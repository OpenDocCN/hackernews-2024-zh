- en: <!--yml
  id: totrans-split-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-split-1
  prefs: []
  type: TYPE_NORMAL
  zh: '类别: 未分类'
- en: 'date: 2024-05-27 14:41:40'
  id: totrans-split-2
  prefs: []
  type: TYPE_NORMAL
  zh: '日期: 2024-05-27 14:41:40'
- en: -->
  id: totrans-split-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: KAIST develops next-generation ultra-low power LLM accelerator | Yonhap News
    Agency
  id: totrans-split-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: KAIST开发了下一代超低功耗LLM加速器 | 韩联社
- en: 来源：[https://en.yna.co.kr/view/AEN20240306003700320](https://en.yna.co.kr/view/AEN20240306003700320)
  id: totrans-split-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://en.yna.co.kr/view/AEN20240306003700320](https://en.yna.co.kr/view/AEN20240306003700320)
- en: By Kim Na-young
  id: totrans-split-6
  prefs: []
  type: TYPE_NORMAL
  zh: '作者: 金娜英'
- en: SEOUL, March 6 (Yonhap) -- A research team at the Korea Advanced Institute of
    Science and Technology (KAIST) has developed the world's first artificial intelligence
    (AI) semiconductor capable of processing a large language model (LLM) with ultra-low
    power consumption, the science ministry said Wednesday.
  id: totrans-split-7
  prefs: []
  type: TYPE_NORMAL
  zh: 首尔，3月6日（韩联社）-- 韩国科学技术院（KAIST）的研究团队开发出世界首款能够以超低功耗处理大语言模型（LLM）的人工智能（AI）半导体，科技部周三表示。
- en: The team, led by Professor Yoo Hoi-jun at the KAIST PIM Semiconductor Research
    Center, developed a "Complementary-Transformer" AI chip, which processes GPT-2
    with an ultra-low power consumption of 400 milliwatts and a high speed of 0.4
    seconds, according to the Ministry of Science and ICT.
  id: totrans-split-8
  prefs: []
  type: TYPE_NORMAL
  zh: 由KAIST PIM半导体研究中心的尤海俊教授领导的团队开发了一款名为“互补变压器”的AI芯片，据科技与信息通信部称，该芯片仅消耗400毫瓦的超低功耗，并以0.4秒的高速运行GPT-2。
- en: The 4.5-mm-square chip, developed using Korean tech giant Samsung Electronics
    Co.'s 28 nanometer process, has 625 times less power consumption compared with
    global AI chip giant Nvidia's A-100 GPU, which requires 250 watts of power to
    process LLMs, the ministry explained.
  id: totrans-split-9
  prefs: []
  type: TYPE_NORMAL
  zh: 这款4.5毫米²的芯片采用了韩国科技巨头三星电子公司的28纳米工艺，与全球AI芯片巨头Nvidia的A-100 GPU相比，功耗降低了625倍，后者需要250瓦的功耗来处理LLMs，科技部解释道。
- en: The chip is also 41 times smaller in area than the Nvidia model, enabling it
    to be used on devices like mobile phones.
  id: totrans-split-10
  prefs: []
  type: TYPE_NORMAL
  zh: 该芯片的面积还比Nvidia模型小41倍，使其可以用于手机等设备。
- en: The ministry said the utilization of neuromorphic computing technology, specifically
    spiking neural networks (SNNs), is essential to the achievement.
  id: totrans-split-11
  prefs: []
  type: TYPE_NORMAL
  zh: 科技部表示，利用神经形态计算技术，特别是脉冲神经网络（SNNs），对于这一成就至关重要。
- en: Previously, the technology was less accurate than deep neural networks (DNNs)
    and mainly capable of simple image classifications, but the research team succeeded
    in improving the accuracy of the technology to match that of DNNs to apply it
    to LLMs.
  id: totrans-split-12
  prefs: []
  type: TYPE_NORMAL
  zh: 此前，这项技术的准确性不如深度神经网络（DNNs），主要能够进行简单的图像分类，但研究团队成功提高了技术的准确性，使其达到了DNNs的水平，从而应用于LLMs。
- en: The team said its new AI chip optimizes computational energy consumption while
    maintaining accuracy by using unique neural network architecture that fuses DNNs
    and SNNs, and effectively compresses the large parameters of LLMs.
  id: totrans-split-13
  prefs: []
  type: TYPE_NORMAL
  zh: 该团队表示，他们的新AI芯片通过采用独特的神经网络架构（融合了DNN和SNN），并有效压缩LLM的大参数，优化了计算能耗，同时保持了精度。
- en: '**A photo describing an artificial intelligence chip which processes a large
    language model with neuromorphic computing technology provided by the Ministry
    of Science and ICT on March 6, 2024 (PHOTO NOT FOR SALE) (Yonhap)**'
  id: totrans-split-14
  prefs: []
  type: TYPE_NORMAL
  zh: '**一张描述使用神经形态计算技术处理大语言模型的人工智能芯片的照片，由科技与信息通信部于2024年3月6日提供（照片不出售）（韩联社）**'
- en: '**nyway@yna.co.kr'
  id: totrans-split-15
  prefs: []
  type: TYPE_NORMAL
  zh: '**nyway@yna.co.kr'
- en: (END)**
  id: totrans-split-16
  prefs: []
  type: TYPE_NORMAL
  zh: （完）**
