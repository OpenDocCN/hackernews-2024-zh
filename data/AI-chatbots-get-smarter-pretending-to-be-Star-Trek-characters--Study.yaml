- en: <!--yml
  id: totrans-split-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-split-1
  prefs: []
  type: TYPE_NORMAL
  zh: 类别：未分类
- en: 'date: 2024-05-27 14:34:18'
  id: totrans-split-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-05-27 14:34:18
- en: -->
  id: totrans-split-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: 'AI chatbots get smarter pretending to be Star Trek characters: Study'
  id: totrans-split-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: AI 聊天机器人更聪明，假装是 *Star Trek* 中的角色：研究
- en: 来源：[https://qz.com/ai-chatbots-math-study-star-trek-1851301719](https://qz.com/ai-chatbots-math-study-star-trek-1851301719)
  id: totrans-split-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://qz.com/ai-chatbots-math-study-star-trek-1851301719](https://qz.com/ai-chatbots-math-study-star-trek-1851301719)
- en: '**Read more**: [The biggest AI chatbot blunders (so far)](https://qz.com/ai-chatbot-blunders-openai-chatgpt-google-gemini-micros-1851301005)'
  id: totrans-split-6
  prefs: []
  type: TYPE_NORMAL
  zh: '**了解更多**：[迄今为止最大的 AI 聊天机器人失误](https://qz.com/ai-chatbot-blunders-openai-chatgpt-google-gemini-micros-1851301005)'
- en: For chatbots, math is the final frontier. AI language models generate responses
    using statistics, spitting out an answer that’s mostly likely to be satisfying.
    That works great when the goal is a passable sentence, but it means chatbots struggle
    with questions like math where there’s exactly one right answer.
  id: totrans-split-7
  prefs: []
  type: TYPE_NORMAL
  zh: 对于聊天机器人而言，数学是最后的边界。AI 语言模型使用统计数据生成响应，输出的答案通常是令人满意的。这在目标是合格句子时效果很好，但对于数学等只有一个正确答案的问题，聊天机器人则面临挑战。
- en: A growing body of evidence suggests you can get better results if you give AI
    some friendly encouragement, but a new study pushes that strange reality further.
    Research from the software company VMware shows chatbots perform better on math
    questions when you tell models to pretend they’re on [*Star Trek*](https://gizmodo.com/io9/television/star-trek).
  id: totrans-split-8
  prefs: []
  type: TYPE_NORMAL
  zh: 越来越多的证据表明，如果你给予 AI 一些友好的鼓励，你可以获得更好的结果，但一项新的研究推动了这种奇怪的现实更远。VMware 软件公司的研究显示，当你告诉模型假装自己在[*Star
    Trek*](https://gizmodo.com/io9/television/star-trek)上时，聊天机器人在数学问题上表现更好。
- en: “It’s both surprising and irritating that trivial modifications to the prompt
    can exhibit such dramatic swings in performance,” the authors wrote in the paper,
    first spotted by [New Scientist](https://www.newscientist.com/article/2419531-ais-get-better-at-maths-if-you-tell-them-to-pretend-to-be-in-star-trek/?utm_source=rakuten&utm_medium=affiliate&utm_campaign=2116208:Skimlinks.com&utm_content=10&ranMID=47192&ranEAID=TnL5HPStwNw&ranSiteID=TnL5HPStwNw-NyvIOKKQrrJWQz0jP7hWiw).
  id: totrans-split-9
  prefs: []
  type: TYPE_NORMAL
  zh: “修改提示的微小变化如此明显地影响性能，既令人惊讶又让人恼火”，作者在论文中写道，首次由[新科学家](https://www.newscientist.com/article/2419531-ais-get-better-at-maths-if-you-tell-them-to-pretend-to-be-in-star-trek/?utm_source=rakuten&utm_medium=affiliate&utm_campaign=2116208:Skimlinks.com&utm_content=10&ranMID=47192&ranEAID=TnL5HPStwNw&ranSiteID=TnL5HPStwNw-NyvIOKKQrrJWQz0jP7hWiw)发现。
- en: Nvidia earnings stun the street and the stock is soaring
  id: totrans-split-10
  prefs: []
  type: TYPE_NORMAL
  zh: Nvidia 的收益令华尔街震惊，股价飙升。
- en: <track kind="captions" label="English" src="https://kinja.com/api/videoupload/caption/22546.vtt"
    srclang="en">
  id: totrans-split-11
  prefs: []
  type: TYPE_NORMAL
  zh: <track kind="captions" label="English" src="https://kinja.com/api/videoupload/caption/22546.vtt"
    srclang="en">
- en: '[The study](https://arxiv.org/html/2402.10949v2#bib.bib9), published on arXiv,
    didn’t set out with *Star Trek* as its prime directive. Previous research found
    that chatbots answer math problems more accurately when you offer [friendly motivation](https://www.businessinsider.com/ai-google-researchers-deepmind-tell-take-deep-breath-improve-accuracy-2023-9)
    like “take a deep breath and work on this step by step.” Others found you can
    trick [ChatGPT](https://gizmodo.com/chatgpt-gone-berserk-giving-nonsensical-responses-1851273889)
    into breaking its own safety guidelines if you [threaten to kill it](https://www.cnbc.com/2023/02/06/chatgpt-jailbreak-forces-it-to-break-its-own-rules.html)
    or offer the AI money.'
  id: totrans-split-12
  prefs: []
  type: TYPE_NORMAL
  zh: '[该研究](https://arxiv.org/html/2402.10949v2#bib.bib9)，发表在 arXiv 上，没有将 *Star Trek*
    视为其主要指令。先前的研究发现，聊天机器人在面对数学问题时，如果给予“友好的鼓励”如“深呼吸并一步步进行”，其回答更准确。其他人发现，如果你威胁要杀死它或者向
    AI 提供金钱，你可以迫使[ChatGPT](https://gizmodo.com/chatgpt-gone-berserk-giving-nonsensical-responses-1851273889)打破其自身的安全指南。'
- en: Rick Battle and Teja Gollapudi from WMWare’s Natural Language Processing Lab
    set out to test the effects of framing their questions with “positive thinking.”
    The study looked at three AI tools, including two versions of [Meta’s Llama 2](https://gizmodo.com/meta-and-microsoft-introduce-open-source-llama-2-ai-1850652165)
    and a model from the French company [Mistral AI](https://gizmodo.com/mistral-artificial-intelligence-gpt-3-openai-1851091217).
  id: totrans-split-13
  prefs: []
  type: TYPE_NORMAL
  zh: Rick Battle 和 Teja Gollapudi 来自 WMWare 的自然语言处理实验室，旨在测试将问题框定在“积极思维”下的影响。该研究涉及三种
    AI 工具，包括两个版本的[Meta's Llama 2](https://gizmodo.com/meta-and-microsoft-introduce-open-source-llama-2-ai-1850652165)和法国公司[Mistral
    AI](https://gizmodo.com/mistral-artificial-intelligence-gpt-3-openai-1851091217)的模型。
- en: They developed a list of encouraging ways to frame questions, including starting
    prompts with phrases such as “You are as smart as ChatGPT” and “You are an expert
    mathematician,” and closing prompts with “This will be fun!” and
  id: totrans-split-14
  prefs: []
  type: TYPE_NORMAL
  zh: 他们开发了一系列鼓舞人心的问题框架，包括以“你和ChatGPT一样聪明”和“你是一位专业的数学家”开头的提示，以及以“这会很有趣！”结尾。
- en: “Take a deep breath and think carefully.” The researchers then used GSM8K, a
    standard set of grade-school math problems, and tested the results.
  id: totrans-split-15
  prefs: []
  type: TYPE_NORMAL
  zh: “深呼吸，仔细思考。” 研究人员随后使用了GSM8K，一个标准的小学数学问题集，并测试了结果。
- en: In the first phase, the results were mixed. Some prompts improved answers, others
    had insignificant effects, and there was no consistent pattern across the board.
    However, the researchers then asked AI to help their efforts to help the AI. There,
    the results got more interesting.
  id: totrans-split-16
  prefs: []
  type: TYPE_NORMAL
  zh: 在第一阶段，结果是参差不齐的。有些提示改善了答案，其他的影响微乎其微，并且在整个数据集上没有一致的模式。然而，研究人员随后要求AI帮助他们的努力。在那里，结果变得更加有趣。
- en: The study used an automated process to try numerous variations of prompts and
    tweak the language based on how much it improved the chatbots’ accuracy. Unsurprisingly,
    this automated process was more effective than the researchers’ hand-written attempts
    to frame questions with positive thinking. But the most effective prompts exhibited
    “exhibits a degree of peculiarity far beyond expectations.”
  id: totrans-split-17
  prefs: []
  type: TYPE_NORMAL
  zh: 这项研究使用自动化流程尝试了许多提示的变化，并根据这些变化如何提高聊天机器人的准确性来调整语言。毫不奇怪，这种自动化过程比研究人员手写的试图用积极思维构建问题更为有效。但最有效的提示表现出了“远远超出预期的特异性”。
- en: For one of the models, asking the AI to start its response with the phrases
    “Captain’s Log, Stardate [insert date here]:.” yielded the most accurate answers.
  id: totrans-split-18
  prefs: []
  type: TYPE_NORMAL
  zh: 对于其中一个模型，要求AI在其回答中以“舰长的日志，星日期[插入日期]：”开头，产生了最准确的答案。
- en: “Surprisingly, it appears that the model’s proficiency in mathematical reasoning
    can be enhanced by the expression of an affinity for *Star Trek*,” the researchers
    wrote.
  id: totrans-split-19
  prefs: []
  type: TYPE_NORMAL
  zh: “令人惊讶的是，似乎表达对*星际迷航*的喜爱可以增强模型在数学推理方面的熟练度，”研究人员写道。
- en: The authors wrote they have no idea what *Star Trek* references improved the
    AI’s performance. There’s some logic to the fact that positive thinking or a threat
    leads to better answers. These chatbots are trained on billions of lines of text
    gathered from the real world. It’s possible that out in the wild, human beings
    who wrote the language used to build AI gave more accurate responses to questions
    when they were pressured with violence or offered encouragement. The same goes
    for bribes; people are more likely to follow instructions when there’s money on
    the line. It could be that large language models picked up on that kind of phenomenon,
    so they behave the same way.
  id: totrans-split-20
  prefs: []
  type: TYPE_NORMAL
  zh: 作者们写道，他们不知道*星际迷航*的参考对提升AI的表现有何影响。认为积极思维或威胁会导致更好的答案有一定的逻辑性。这些聊天机器人是在收集自真实世界的数十亿行文本基础上训练的。可能在野外，建造AI所用语言的人类在面对暴力威胁或鼓励时，对问题给出更准确的回答。贿赂也是一样；当利益受到威胁时，人们更可能遵循指令。大型语言模型可能也注意到了这种现象，所以它们的行为方式也相似。
- en: 'But it’s hard to imagine that in the data sets that trained the chatbots, the
    most accurate answers began with the phrase “Captain’s Log.” The researchers didn’t
    even have a theory about why that got better results. It speaks to one of the
    strangest facts about AI language models: Even the people who build and study
    them don’t really understand how they work.'
  id: totrans-split-21
  prefs: []
  type: TYPE_NORMAL
  zh: 但很难想象在训练聊天机器人的数据集中，最准确的答案竟然以“舰长的日志”这个短语开头。研究人员甚至对为什么这能得到更好的结果没有理论。这反映了关于AI语言模型最奇怪的事实之一：即使是建造和研究它们的人，也不真正理解它们的工作原理。
- en: '[*A version of this article originally appeared on Gizmodo*](https://gizmodo.com/ai-chatbots-are-better-at-math-when-they-pretend-to-be-1851300787).'
  id: totrans-split-22
  prefs: []
  type: TYPE_NORMAL
  zh: '[*本文的一个版本最初发表在Gizmodo上*](https://gizmodo.com/ai-chatbots-are-better-at-math-when-they-pretend-to-be-1851300787)。'
