- en: <!--yml
  id: totrans-split-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-split-1
  prefs: []
  type: TYPE_NORMAL
  zh: 'category: 未分类'
- en: 'date: 2024-05-29 12:29:42'
  id: totrans-split-2
  prefs: []
  type: TYPE_NORMAL
  zh: 'date: 2024-05-29 12:29:42'
- en: -->
  id: totrans-split-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: Understanding Google’s File System
  id: totrans-split-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解谷歌文件系统
- en: 来源：[https://www.micahlerner.com/2020/03/22/understanding-googles-file-system.html](https://www.micahlerner.com/2020/03/22/understanding-googles-file-system.html)
  id: totrans-split-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 'Source: [https://www.micahlerner.com/2020/03/22/understanding-googles-file-system.html](https://www.micahlerner.com/2020/03/22/understanding-googles-file-system.html)'
- en: Discussion on [Hacker News](https://news.ycombinator.com/item?id=39756262)
  id: totrans-split-6
  prefs: []
  type: TYPE_NORMAL
  zh: '[Hacker News的讨论](https://news.ycombinator.com/item?id=39756262)'
- en: '*These paper reviews can [be delivered weekly to your inbox](https://newsletter.micahlerner.com/),
    or you can subscribe to the [Atom feed](https://www.micahlerner.com/feed.xml).
    As always, feel free to reach out on [Twitter](https://twitter.com/micahlerner)
    with feedback or suggestions!*'
  id: totrans-split-7
  prefs: []
  type: TYPE_NORMAL
  zh: '*这些论文评论可以[每周发送到您的收件箱](https://newsletter.micahlerner.com/)，或者您可以订阅[Atom feed](https://www.micahlerner.com/feed.xml)。如往常一样，欢迎通过[Twitter](https://twitter.com/micahlerner)提供反馈或建议！*'
- en: Today I read [the original paper](https://static.googleusercontent.com/media/research.google.com/en//archive/gfs-sosp2003.pdf)
    about the Google File System (GFS), a system that provided the storage layer for
    many of Google’s applications in the company’s early days. The original implementation
    has reportedly been replaced by a newer version called Colossus, but reading about
    the original approach was still illuminating and I thought I’d do a quick write
    up about it.
  id: totrans-split-8
  prefs: []
  type: TYPE_NORMAL
  zh: 今天我阅读了[谷歌文件系统（GFS）的原始论文](https://static.googleusercontent.com/media/research.google.com/en//archive/gfs-sosp2003.pdf)，这是谷歌早期公司应用中提供存储层的系统。据报道，原始实现已被称为Colossus的新版本取代，但阅读原始方法仍然很有启发性，我想快速写一篇关于它的简要总结。
- en: Why is/was GFS such a big deal?
  id: totrans-split-9
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: GFS为何如此重要？
- en: The original paper was published in 2003 at SOSP (Symposium on Operating Systems
    Principles - one of, if not the, best conferences for operating systems research).
  id: totrans-split-10
  prefs: []
  type: TYPE_NORMAL
  zh: 原始论文于2003年在SOSP（操作系统原理研讨会 - 是操作系统研究中最好的会议之一）上发表。
- en: GFS made it onto the program because of how revolutionary it was at the time
    - the accompanying paper detailed how Google had successfully implemented academic
    ideas of weak consistency and reliance on a single master controller (more on
    this later) at tremendous scale in an industry application.
  id: totrans-split-11
  prefs: []
  type: TYPE_NORMAL
  zh: GFS之所以如此革命性，是因为它成功地在产业应用中以极大规模实现了学术上弱一致性和对单个主控制器的依赖的思想，附带的论文详细说明了当时的情况。
- en: The ultimate goal of GFS was to provide a replicated storage layer (redundant
    copies of data are kept across many machines) across the commodity level machines
    in a Google datacenter. The original motivation for developing such a system was
    to power batch jobs, although the system eventually powered other projects.
  id: totrans-split-12
  prefs: []
  type: TYPE_NORMAL
  zh: GFS的最终目标是在谷歌数据中心的商品级机器上提供复制的存储层（数据的冗余副本分布在多台机器上）。开发这样一个系统的最初动机是为了支持批处理作业，尽管最终该系统也为其他项目提供了动力。
- en: Because GFS was designed for batch jobs, it primarily optimized for appending
    to, rather than modifying, files. Users of the program were generally writing
    large files out at once rather than making modifications to specific parts of
    a file.
  id: totrans-split-13
  prefs: []
  type: TYPE_NORMAL
  zh: 因为GFS设计用于批处理作业，主要优化的是文件的追加，而不是修改。程序的用户通常一次性写入大文件，而不是对文件的特定部分进行修改。
- en: What are the abstractions that power GFS?
  id: totrans-split-14
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: GFS的驱动抽象是什么？
- en: 'At the core of GFS is a concept called **chunks**. Chunks are used to split
    up files into fixed-size 64MB segments that are then replicated around the datacenter
    [†](#footnotes). Chunks are referred to by **chunk handles**, basically unique
    ids for a chunk. Splitting a large file into many chunks, then replicating those
    chunks across many machines accomplished two goals: improving performance (as
    there could now be many readers and writers of a single file), and allowing huge
    files to exist behind a simple abstraction.'
  id: totrans-split-15
  prefs: []
  type: TYPE_NORMAL
  zh: GFS的核心概念是**chunk**（块）。Chunks用于将文件分割成固定大小的64MB段，然后在数据中心周围复制这些段[†](#footnotes)。Chunks由**chunk
    handle**（块句柄）引用，基本上是块的唯一标识符。将大文件分割成许多块，然后在多台机器上复制这些块实现了两个目标：提高性能（因为现在可以有多个读者和写者访问单个文件）并允许大文件存在于简单的抽象之后。
- en: To make the idea of how this abstraction works more concrete, imagine using
    a library to open a file on a disk. Behind the scenes, that library now goes out
    and fetches all of the different pieces of the file you requested from computers
    all around your datacenter, then provides a transparent way to interact with the
    stitched together data [†](#footnotes).
  id: totrans-split-16
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更具体地说明这种抽象工作原理的想法，请想象使用一个库来打开磁盘上的文件。在幕后，该库现在会从数据中心各处的计算机上获取您请求的文件的所有不同部分，然后提供一种透明的方式与拼接在一起的数据进行交互
    [†](#footnotes)。
- en: 'The aforementioned library (called by your user program, the Client) performs
    fetching and writing operations by interacting with several components of GFS:'
  id: totrans-split-17
  prefs: []
  type: TYPE_NORMAL
  zh: 上述库（由您的用户程序调用，称为客户端）通过与GFS的几个组件交互来执行获取和写入操作：
- en: '**Master**: The master has a few responsibilties. To start, it is the first
    point of contact for a client when they want to interact with GFS. In addition
    to that function, the master is also responsible for communicating with a set
    of **chunk servers** that host chunks. To perform its functions, the master stores
    a few tables in RAM:'
  id: totrans-split-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**主节点**：主节点有几个职责。首先，当客户端希望与GFS交互时，它是客户端的第一个接触点。除了这个功能之外，主节点还负责与托管分块的一组**分块服务器**进行通信。为了执行其功能，主节点在RAM中存储了几个表：'
- en: A mapping from filenames to **chunk handles** (chunk handles are basically IDs
    for chunks).
  id: totrans-split-19
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从文件名到**分块句柄**的映射（分块句柄基本上是分块的ID）。
- en: A mapping from **chunk handles** to a list of the machines that the chunk is
    on, versioning information about the chunk (a piece of data to help with managing
    multiple writes to the same chunk), and two pieces of information related to managing
    writes to that chunk - the primary and the lease. I’ll cover the primary and the
    lease in the next section.
  id: totrans-split-20
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从**分块句柄**到存储该分块的机器列表的映射，分块的版本信息（用于管理对同一分块的多次写入的数据片段），以及两个与管理对该分块的写入相关的信息 - 主要和租约。我将在下一节讨论主要和租约。
- en: '**Chunk Server**: Chunk servers handle work around writing to and reading from
    disk. A client starts talking to them after being told to do so by the master.'
  id: totrans-split-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**分块服务器**：分块服务器处理与磁盘写入和读取相关的工作。客户端在主节点告知后开始与它们通信。'
- en: How does writing and reading to GFS work?
  id: totrans-split-22
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 写入和从GFS中读取的工作原理是怎样的？
- en: Reading from GFS
  id: totrans-split-23
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 从GFS中读取
- en: To read a file to GFS, a client says to the master, “I would like to read this
    byte offset in this file”, where the file looks like a regular file system path.
  id: totrans-split-24
  prefs: []
  type: TYPE_NORMAL
  zh: 要从GFS中读取文件，客户端对主节点说：“我想读取此文件中的这个字节偏移量”，文件看起来像是一个常规的文件系统路径。
- en: The master then receives the request from the client and calculates which chunk
    corresponds to the associated file and byte offset. Using the chunk handle of
    the calculated chunk, the master then gets the list of chunk servers that store
    the aforementioned chunk and provides it to the client. The client then chooses
    a chunk server, contacting it with the chunk and offset it wants, then is provided
    with the requested data.
  id: totrans-split-25
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，主节点接收来自客户端的请求，并计算与关联文件和字节偏移量对应的分块。使用计算出的分块的分块句柄，主节点然后获取存储上述分块的分块服务器列表，并将其提供给客户端。然后客户端选择一个分块服务器，与其联系，并获取它想要的分块和偏移量，然后获取请求的数据。
- en: Along the way, the client also caches information about the chunk and the chunkservers
    it can find that chunk on if it needs to rerequest the chunk.
  id: totrans-split-26
  prefs: []
  type: TYPE_NORMAL
  zh: 在整个过程中，客户端还会缓存有关分块和它能找到的分块服务器的信息，以便在需要时重新请求该分块。
- en: Writing to GFS
  id: totrans-split-27
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 写入到GFS中
- en: Writing (in this case, appending) to files in GFS is significantly more complicated
    than reading from GFS.
  id: totrans-split-28
  prefs: []
  type: TYPE_NORMAL
  zh: 在GFS中写入（在这种情况下，追加）文件比从GFS中读取要复杂得多。
- en: To start a client, asks the master for a specific file’s last chunk (the end
    of the file is necessary because we are appending). The master then checks its
    tables for information on that chunk, using the returned chunk handle (the chunk
    handle is essentially the ID of the chunk).
  id: totrans-split-29
  prefs: []
  type: TYPE_NORMAL
  zh: 要启动客户端，客户端向主节点请求特定文件的最后一个分块（由于我们正在附加，所以文件的末尾是必要的）。然后，主节点检查其表以获取有关该分块的信息，使用返回的分块句柄（分块句柄实质上是分块的ID）。
- en: The master then inspects two pieces of information that it is storing about
    each chunk - the primary and lease fields.
  id: totrans-split-30
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，主节点检查它关于每个分块存储的两个信息 - 主要字段和租约字段。
- en: The **primary** is a reference to a chunk server that has been assigned to coordinate
    writes among chunk servers. This assignment is short lived, and is governed by
    the expiration of the **lease**. When the lease runs out, the master can assign
    a new chunk server to coordinate writes.
  id: totrans-split-31
  prefs: []
  type: TYPE_NORMAL
  zh: '**主服务器**是指被分配协调块服务器之间写入的块服务器的引用。此分配是短暂的，并受**租约**到期的管理。租约到期后，主服务器可以分配新的块服务器来协调写入操作。'
- en: If the chunk that a client requests does not have a **primary** assigned, the
    master assigns one, and increments the version of the data. Incrementing the version
    number allows the master to keep track of which data is the most recent. If the
    chunk already has a primary, this step is skipped.
  id: totrans-split-32
  prefs: []
  type: TYPE_NORMAL
  zh: 如果客户端请求的块没有分配主服务器，主服务器将分配一个并增加数据的版本号。增加版本号允许主服务器跟踪最新的数据。如果块已经有了主服务器，则跳过此步骤。
- en: The next step is to transmit information about the primary and secondaries (chunk
    servers that have the chunk, but aren’t the primary) to the client. From there,
    the client sends the data it wants to write to the respective chunk servers. After
    all chunk servers have the data, the client tells the primary to write it. The
    primary chunk server chooses a byte offset in the chunk (whatever the end of the
    file is), and sends it to all of the secondaries, after which all of them perform
    the right.
  id: totrans-split-33
  prefs: []
  type: TYPE_NORMAL
  zh: 下一步是向客户端传输关于主服务器和副本（具有块但不是主服务器的块服务器）的信息。然后，客户端将要写入的数据发送到各个块服务器。当所有块服务器都有数据后，客户端告诉主服务器进行写入。主块服务器选择块中的字节偏移量（文件末尾），并将其发送给所有副本，之后它们都执行写入操作。
- en: If the primary and all secondaries write, the client receives a success! If
    not all secondaries write, the client receives a failure, at which point it needs
    to recontact the master and repeat the process from the beginning.
  id: totrans-split-34
  prefs: []
  type: TYPE_NORMAL
  zh: 如果主服务器和所有副本都写入了，客户端会收到成功的消息！如果没有所有副本都写入，客户端会收到失败的消息，此时需要重新联系主服务器并从头开始重复此过程。
- en: Wrapping up
  id: totrans-split-35
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 总结
- en: I [found an interview](https://queue.acm.org/detail.cfm?id=1594206) with one
    of the engineers who worked on GFS to be fairly interesting. GFS was very successful
    for the applications it was designed for and reached wide adoption within Google.
  id: totrans-split-36
  prefs: []
  type: TYPE_NORMAL
  zh: 我发现一篇[采访](https://queue.acm.org/detail.cfm?id=1594206)其中一个工程师讨论了他在 GFS 上的工作，觉得非常有趣。GFS
    对其设计的应用非常成功，并在谷歌内部广泛采用。
- en: Unfortunately, it didn’t scale as well to new use cases for a few reasons. First
    off, the system used a single master process to store of chunk servers in addition
    to other metadata. Having all of this information in RAM on a single machine only
    went so far.
  id: totrans-split-37
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，由于几个原因，它对新用例的扩展能力不是很好。首先，系统使用单个主进程存储块服务器以及其他元数据。将所有这些信息存储在单台机器的 RAM 中只能做到这么多。
- en: Another issue that GFS ran into was in storing small files. For example, if
    a user wanted to store many files smaller than the chunk size, the master needed
    to store an entry for each file, and allocate the full chunk size on disk. Google
    ended up working on other systems and making tweaks to GFS to solve this problem
    (in particular, one of the systems that is discusses is BigTable).
  id: totrans-split-38
  prefs: []
  type: TYPE_NORMAL
  zh: GFS 遇到的另一个问题是存储小文件。例如，如果用户想要存储许多小于块大小的文件，主服务器需要为每个文件存储一个条目，并在磁盘上分配完整的块大小。谷歌最终着手开发其他系统，并对
    GFS 进行了调整以解决这个问题（特别是其中讨论到的一个系统是 BigTable）。
- en: '[1] Google’s new storage system would try to decrease the chunk size for reasons
    that I talk about at the end of this post.'
  id: totrans-split-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[1] 谷歌的新存储系统试图减少块大小，原因我在本文末尾有讨论。'
- en: '[2] Whether the data is actually stitched together or not is somewhat of an
    implementation detail'
  id: totrans-split-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[2] 数据是否实际上被拼接在一起，这在一定程度上是一个实现细节。'
- en: 'References:'
  id: totrans-split-41
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 参考资料：
