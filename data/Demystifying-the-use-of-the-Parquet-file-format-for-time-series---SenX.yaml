- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 类别：未分类
- en: 'date: 2024-05-27 14:52:14'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-05-27 14:52:14
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: Demystifying the use of the Parquet file format for time series - SenX
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 解密用于时间序列的 Parquet 文件格式 - SenX
- en: 来源：[https://blog.senx.io/demystifying-the-use-of-the-parquet-file-format-for-time-series/](https://blog.senx.io/demystifying-the-use-of-the-parquet-file-format-for-time-series/)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://blog.senx.io/demystifying-the-use-of-the-parquet-file-format-for-time-series/](https://blog.senx.io/demystifying-the-use-of-the-parquet-file-format-for-time-series/)
- en: As the maker of [Warp 10](https://warp10.io/), the most advanced Time Series
    Platform, we are regularly in contact with people who have been storing time series
    data for quite some time. The diversity of technologies that have employed would
    surprise you. Among those technologies we often encounter the [Parquet](https://parquet.apache.org)
    file format which is usually chosen *because it is a columnar format and therefore
    it is performant for time series data*. When we ask further questions to better
    understand the schema adopted, how those files are used and what storage and access
    performance they allow, the answers often show the way the Parquet format works
    is not very well understood. This post aims at giving you a better understanding
    of the internals of the Parquet format and of its suitability for time series
    data.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 作为 [Warp 10](https://warp10.io/) 的开发者，最先进的时间序列平台，我们经常与存储时间序列数据已经相当长时间的人联系。他们所采用的技术的多样性会让你感到惊讶。在这些技术中，我们经常遇到通常被选择*因为它是列式格式，因此对于时间序列数据而言性能良好*的[Parquet](https://parquet.apache.org)文件格式。当我们进一步询问以更好地了解采用的架构，这些文件的使用方式以及它们允许的存储和访问性能时，答案通常显示
    Parquet 格式的工作方式并不是很好理解。本文旨在让您更好地了解 Parquet 格式的内部结构以及其适用于时间序列数据的性能。
- en: The Parquet file format
  id: totrans-7
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Parquet 文件格式
- en: History
  id: totrans-8
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 历史
- en: 'In 2010 at the [VLDB Conference](https://vldb.org/), people from [Google](https://google.com/)
    presented a paper titled [Dremel: Interactive Analysis of Web-Scale Datasets](https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/36632.pdf)
    which described an internal tool named [*Dremel*](https://en.wikipedia.org/wiki/Dremel_(software))
    and the way it organized data.'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '在 2010 年的[VLDB 会议](https://vldb.org/)上，[Google](https://google.com/) 的人员发表了一篇论文，题为[Dremel:
    Web 规模数据的交互式分析](https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/36632.pdf)，描述了一种名为[*Dremel*](https://en.wikipedia.org/wiki/Dremel_(software))的内部工具以及它组织数据的方式。'
- en: In the summer of 2012, [Julien Le Dem](https://twitter.com/J_) who was then
    working at Twitter, [tweeted](https://twitter.com/J_/status/239247373581316096)
    that he had found an error in the Dremel paper, the reason was that he had started
    working on an open source implementation of the nested record format used by Dremel
    for use in Hadoop, a project that would a little later become known as [Parquet](https://web.archive.org/web/20130415111841/http://parquet.io/).
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在 2012 年夏天，[Julien Le Dem](https://twitter.com/J_) 当时在 Twitter 工作，[发推文](https://twitter.com/J_/status/239247373581316096)说他在
    Dremel 论文中发现了一个错误，原因是他开始着手开发一个用于 Hadoop 中的 Dremel 使用的嵌套记录格式的开源实现，这个项目稍后会成为[Parquet](https://web.archive.org/web/20130415111841/http://parquet.io/)。
- en: Fundamentals
  id: totrans-11
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 基础知识
- en: The novelty described in the *Dremel* paper was a clever way of enabling complex
    records to be stored in a columnar format.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在 *Dremel* 论文中描述的新奇之处是一种巧妙的方法，可以将复杂记录存储在列式格式中。
- en: The idea behind columnar formats is that by grouping data in *columns* instead
    or *rows*, similar values end up together and lead to better compression, and
    at retrieval time only the needed data can be read if not all columns are needed.
    Systems such as [HBase](https://hbbase.apache.org) had long made use of the columnar
    format paradigm.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 列式格式背后的理念是，通过将数据分组到*列*而不是*行*中，相似的值会聚在一起，导致更好的压缩，在检索时，如果不需要所有列，则只需读取所需数据。诸如[HBase](https://hbbase.apache.org)之类的系统长期以来一直在使用列式格式范例。
- en: But if the columnar format was well suited for *table like* data, it was not
    adapted to nested data structures which often would end up in a single column
    hence annihilating the benefit of the columnar format if only specific fields
    of such a structure are needed.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，如果列式格式非常适用于*类似表格*的数据，那么对于嵌套数据结构来说就不适用，因为这些数据结构通常会最终汇聚在单个列中，因此如果只需要此类结构的特定字段，则会消除列式格式的好处。
- en: This is the issue that the Dremel paper addressed and solved with a clever mechanism
    for describing how fields are nested and repeated. We let you deep dive into section
    4 of the original Dremel paper to learn the intrinsics of how records are split
    in columns.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是 Dremel 论文解决的问题，并通过一种巧妙的机制来描述字段如何嵌套和重复进行了解决。我们让您深入阅读原始 Dremel 论文第 4 部分，以了解记录如何在列中分割的内在机制。
- en: Implementation
  id: totrans-16
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 实现
- en: 'The Parquet implementation uses the *flattening* principles described in the
    Dremel paper and organises a file in the following manner:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: Parquet 实现采用了在 Dremel 论文中描述的*展平*原则，并按以下方式组织文件：
- en: A *file* is divided in *row groups* which contain values for various columns
    stored in *column chunks*. The actual data is packed in *pages* within those column
    chunks. Pages are compressed using a mechanism adapted to the type of values they
    store. At the end of the file a *footer* contains various informations, such as
    the file schema, the index of the row groups and column chunks and information
    such as statistics for each column chunk.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 一个*文件*被划分为包含在*列块*中存储的各列值的*行组*。实际数据被打包在这些列块内的*页*中。页使用适应其存储值类型的机制进行压缩。文件末尾的*页脚*包含各种信息，如文件架构、行组和列块的索引以及每个列块的统计信息。
- en: When reading data from a Parquet file, the footer is read to identify the offsets
    where the column chunks can be read, the chunks matching the requested data are
    then accessed and the records reconstructed from the data for the individual columns.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 从 Parquet 文件读取数据时，会读取页脚以识别可以读取列块的偏移量，然后访问与请求数据匹配的块，并从各个列的数据重构记录。
- en: A predicate pushdown mechanism in the Parquet read API can exploit the statistics
    in the file footer to only read the row groups containing values matching a predicate.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: Parquet 读取 API 中的谓词下推机制可以利用文件页脚中的统计信息，仅读取包含与谓词匹配值的行组。
- en: Essential takeaway
  id: totrans-21
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 核心要点
- en: The Parquet format is an intelligent *columnar* format with the ability to store
    complex nested or repeated data structures as columns. So in short, every field,
    or every element of repeated fields in the stored data structures end up in its
    own column. This is fundamental to understand and has important implications when
    storing time series in Parquet.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: Parquet 格式是一种智能的*列式*格式，具有存储复杂嵌套或重复数据结构作为列的能力。简而言之，存储的数据结构中的每个字段或重复字段的每个元素最终都以自己的列形式存在。这是理解
    Parquet 中存储时间序列时的基础，对于存储时间序列具有重要的影响。
- en: Storing Time Series in Parquet
  id: totrans-23
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在 Parquet 文件中存储时间序列
- en: The motivation behind the selection of the Parquet format to store time series
    could be that those data need to be processed using a parallel processing framework
    such as [Spark](https://spark.apache.org/), that they need to be read by a query
    engine such as [Impala](https://impala.apache.org) or [Drill](https://drill.apache.org),
    or it could be that you are considering using Parquet for the storage layer of
    the [next rewrite](https://www.influxdata.com/blog/announcing-influxdb-iox/) of
    your time series database or of your [cloud service](https://docs.microsoft.com/en-us/azure/time-series-insights/concepts-storage#parquet-file-format-and-folder-structure).
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 存储时间序列选用 Parquet 格式的动机可能是需要使用诸如[Spark](https://spark.apache.org/)这样的并行处理框架处理这些数据，需要由查询引擎（如[Impala](https://impala.apache.org)或[Drill](https://drill.apache.org)）读取，或者考虑将
    Parquet 作为[下一次重写](https://www.influxdata.com/blog/announcing-influxdb-iox/)时间序列数据库或[云服务](https://docs.microsoft.com/en-us/azure/time-series-insights/concepts-storage#parquet-file-format-and-folder-structure)的存储层。
- en: Data model
  id: totrans-25
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 数据模型
- en: Before you can actually store your data in a Parquet file you need to have a
    clear idea of how you want to model your data as this model will directly reflect
    in the structure of the records which will be stored.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 在实际存储数据到 Parquet 文件之前，您需要清楚地了解如何对数据建模，因为这个模型将直接反映在将要存储的记录结构中。
- en: When working with time series you basically have two ways to model the data.
    The first one, the most flexible, is to consider series as individual entities
    which will be treated independently but may be manipulated jointly with other
    series. The second way, more naive, is to consider series as columns of a table.
    The records which are stored are therefore really rows which, at a given timestamp,
    contain values for various columns. This is model typical of IT monitoring use
    cases where each record is a photograph of several metrics. It is less suited
    for IoT use cases where sensors may differ from device to device and sampling
    frequency from sensor to sensor.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 当处理时间序列时，基本上有两种方法来建模数据。第一种，即最灵活的方法，是将系列视为独立实体，它们将被独立处理，但可以与其他系列一起进行操作。第二种方法，更为天真，是将系列视为表的列。因此，存储的记录实际上是在给定时间戳下包含各列数值的行。这是
    IT 监控用例中的典型模型，其中每个记录都是几个指标的快照。对于物联网用例来说，这种模型不太适用，因为传感器可能因设备而异，采样频率也可能因传感器而异。
- en: The *table* model of time series maps naturally to a Parquet schema with one
    field per series, one field for the timestamp and maybe a set of fields for metadata
    such as labels, *i.e.* key/value pairs which add context to a series. This model
    is sometimes also called the *observations* model since each record is just that,
    a set of observations at a given timestamp.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 时间序列的*表*模型自然地映射到了一个 Parquet 架构，每个系列对应一个字段，一个字段对应时间戳，可能还有一组用于元数据的字段，*即*为系列添加上下文的键/值对。有时这个模型也被称为*观察*模型，因为每条记录只是一个在给定时间戳下的一组观察。
- en: The *series* model differs in that each series, or a chunk of time of a series,
    lies in a row. The columns that Parquet will create would then be the occurrences
    of the timestamps and values. This approach is described in [a chapter](http://what-when-how.com/Tutorial/topic-406tdru0g/Time-Series-Databases-32.html)
    of the [Time Series Book](https://apprize.best/data/series/4.html) by [Ted Dunning](https://twitter.com/ted_dunning)
    and [Ellen Friedman](https://twitter.com/Ellen_Friedman).
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: '*系列*模型的不同之处在于每个系列或系列的时间段位于一行中。Parquet 将创建的列将成为时间戳和值的出现次数。这一方法在 [一章](http://what-when-how.com/Tutorial/topic-406tdru0g/Time-Series-Databases-32.html)
    以及 [时间序列书](https://apprize.best/data/series/4.html) 中有描述，作者是 [特德·邓宁](https://twitter.com/ted_dunning)
    和 [埃伦·弗里德曼](https://twitter.com/Ellen_Friedman)。'
- en: We see that in both cases the number of columns may explode if you want to store
    many series (in the table model) or many data points (in the series model). In
    this latter case the compression would also most probably be rather poor since
    a given column (the i*th* tick or i*th* value) would not have anything in common
    with the same tick in the next series.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 我们注意到，在这两种情况下，如果要存储多个系列（在表模型中）或多个数据点（在系列模型中），则列的数量可能会激增。在后一种情况下，压缩很可能也会相当差，因为给定的列（第
    i 个刻度或第 i 个数值）与下一个系列中的相同刻度没有任何相似之处。
- en: Performance
  id: totrans-31
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 性能
- en: There are two performance aspects to consider when using Parquet files. The
    first one is the efficiency of the compression of the data. As we have just seen,
    the *table* model leads to better compression that the *series* approach.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 Parquet 文件时需要考虑两个性能方面。第一个是数据压缩的效率。正如我们刚才所见，*表*模型比*序列*方法具有更好的压缩性能。
- en: The second performance aspect to consider is the speed at which data can be
    retrieved. As we already mentioned, Parquet has the ability to do *predicate pushdowns*
    where predicates on data are used to determine what data to read. The Parquet
    format does so at the *column chunk* + *row group* level and considers columns
    independently of each other. That is to say a predicate on a column will use the
    columns statistics to determine if a given row group contains valid data for the
    column given the predicate. If multiple predicates are pushed down, the row groups
    matching all predicates will be selected. This could seem to be a victory for
    performance, but unfortunately it is not completely so, as the only guarantee
    is that the selected row groups contain data which match the predicate for various
    columns, but there is no guarantee that the matching values for those columns
    belong to the same rows. You might therefore read many data which you will end
    up discarding. Luckily Parquet will drop records before reassembling them when
    a column ultimately does not verify a pushed down predicate, but that will not
    prevent you from paying for the transfer of those useless data.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 要考虑的第二个性能方面是数据检索的速度。正如我们之前提到的，Parquet 具有 *谓词推送* 的功能，其中对数据的谓词用于确定要读取的数据。Parquet
    格式在 *列块* + *行组* 级别执行此操作，并独立考虑列。也就是说，对列的谓词将使用列的统计信息来确定给定行组是否包含符合谓词的列的有效数据。如果推送多个谓词，将选择所有谓词都匹配的行组。这似乎对性能来说是一种胜利，但不幸的是，唯一的保证是所选的行组包含符合各列谓词的数据，但不能保证这些列的匹配值属于相同的行。因此，您可能会读取许多最终会丢弃的数据。幸运的是，Parquet
    在重新组装时会在最终不验证推送下的谓词的列时删除记录，但这并不能阻止您支付传输这些无用数据的费用。
- en: 'Predicates on values are seldom used for time series, but predicates on timestamp
    and on metadata (labels) are very common. Imagine you have stored the following
    data in your Parquet file:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 值的谓词在时间序列中很少使用，但时间戳和元数据（标签）的谓词非常常见。想象一下，您已经将以下数据存储在您的 Parquet 文件中：
- en: '| row group | timestamp | room | house | temperature | humidity |'
  id: totrans-35
  prefs: []
  type: TYPE_TB
  zh: '| row group | timestamp | room | house | temperature | humidity |'
- en: '| --- | --- | --- | --- | --- | --- |'
  id: totrans-36
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- |'
- en: '| 0 | 10:00:00Z | `kitchen` | `main` | 20.0 | 75.0 |'
  id: totrans-37
  prefs: []
  type: TYPE_TB
  zh: '| 0 | 10:00:00Z | `kitchen` | `main` | 20.0 | 75.0 |'
- en: '| 0 | 10:00:00Z | `kitchen` | `vacation` | 15.0 | 50.1 |'
  id: totrans-38
  prefs: []
  type: TYPE_TB
  zh: '| 0 | 10:00:00Z | `kitchen` | `vacation` | 15.0 | 50.1 |'
- en: '| 1 | 10:01:00Z | `bathroom` | `main` | 20.1 | 80.0 |'
  id: totrans-39
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 10:01:00Z | `bathroom` | `main` | 20.1 | 80.0 |'
- en: '| 1 | 10:01:10Z | `den` | `vacation` | 17.0 | 60.0 |'
  id: totrans-40
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 10:01:10Z | `den` | `vacation` | 17.0 | 60.0 |'
- en: Imagine that you want the data from your vacation house between 10:00:00Z and
    10:01:00Z, you see that the row which matches is the second one which is in row
    group `0`, but as per the independence of the columns, both row groups will match
    as they contain timestamps within the requested range **AND** an occurrence of
    `vacation` in the *house* column. So you will end up reading the second row group
    even though it does not contain any data matching your criteria.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 假设您想要在 10:00:00Z 到 10:01:00Z 之间的假期房屋数据，您会发现匹配的行是第二行，位于行组 `0` 中，但由于列的独立性，两个行组都会匹配，因为它们包含请求范围内的时间戳
    **和** 在 *house* 列中出现 `vacation`。因此，尽管第二行组不包含符合您条件的数据，您最终将读取第二行组。
- en: Mitigating this would require creation of custom row groups, ideally with a
    single series being stored per row group to guarantee that row group selection
    would not lead to extraneous data being read. This would create small row groups
    in most cases which is opposite to the purpose of row groups. This is rather cumbersome,
    but unfortunately this is the only solution as the Parquet format lacks provision
    for indexing the content of Parquet files for efficient random access to specific
    rows. Note also that predicates cannot be applied to nested structures, so grouping
    labels in a *MAP* column would not work for being more efficient at selecting
    matching rows.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 减轻这种情况需要创建定制的行组，理想情况下每个行组存储一个单一系列，以确保行组选择不会导致读取多余的数据。这将在大多数情况下创建小的行组，这与行组的目的相反。这相当繁琐，但不幸的是这是唯一的解决方案，因为
    Parquet 格式缺乏为了有效随机访问特定行而索引 Parquet 文件内容的规定。还要注意，谓词不能应用于嵌套结构，因此在 *MAP* 列中分组标签无法提高选择匹配行的效率。
- en: Another issue we have briefly mentioned is the explosion in the number of columns.
    It is not wise to have too many columns in a Parquet file as it makes the footer
    grow quite consequently and some problems may arise with clients doing code generation.
    This constraints both the *tables* and *series* approaches to either a limited
    number of series, or a limited size of time chunks.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 我们曾简要提及的另一个问题是列数的激增。在 Parquet 文件中拥有过多列并不明智，因为这会导致页脚的增长相当大，并且可能会出现一些与客户端执行代码生成相关的问题。这不仅限制了*表格*和*系列*方法的列数，还限制了时间块的大小。
- en: '*Read also [Build a Complete Application with Warp 10, from TCP Stream to Dashboard](https://blog.senx.io/complete-application-warp-10-tcp-stream-dashboard/)*'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '*还可阅读 [使用 Warp 10 从 TCP 流到仪表盘构建完整应用程序](https://blog.senx.io/complete-application-warp-10-tcp-stream-dashboard/)*'
- en: Conclusion
  id: totrans-45
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 结论
- en: While Parquet is a very good format, when it comes to Time Series data the columnar
    approach it provides may not be very well suited and the access performance advertised
    by Parquet aficionados might just not be there in some contexts. The need for
    a fixed schema in a Parquet file also limits the variety of time series that can
    be stored in a single file. Bear that in mind when choosing Parquet or a tool
    using it for accessing time series data in an interactive way.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管 Parquet 是一个非常好的格式，但是当涉及到时间序列数据时，它提供的列式方法可能并不是非常适用，并且 Parquet 爱好者所宣传的访问性能在某些情况下可能并不存在。Parquet
    文件中需要固定的模式也限制了可以存储在单个文件中的时间序列的种类。在选择 Parquet 或使用它来交互方式访问时间序列数据的工具时，请记住这一点。
- en: At SenX we have come to that conclusion several years ago and we designed a
    way of storing time series in files which lead to extreme compression and speed
    of access. Those files are readable in tools such as [Spark](https://spark.apache.org)
    and [Flink](https://flink.apache.org/) but more importantly can be mounted in
    a Warp 10 instance so the data they contain can be `FETCH`*ed* almost like if
    they were stored in the Warp 10 storage engine. Those files can also be stored
    in a cloud object storage service.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 在 SenX，几年前我们就得出了这个结论，并且设计了一种在文件中存储时间序列的方式，使得极端压缩和访问速度成为可能。这些文件可以在诸如[Spark](https://spark.apache.org)和[Flink](https://flink.apache.org/)之类的工具中读取，但更重要的是，它们可以在
    Warp 10 实例中挂载，以便其中包含的数据几乎可以像在 Warp 10 存储引擎中存储一样进行`FETCH`。这些文件也可以存储在云对象存储服务中。
- en: In some cases we have experienced compression ratios approaching 1:40, with
    each timestamp and value occupying less than 0.4 bytes, and read performance of
    hundreds of millions of data points per second on a single thread.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 在某些情况下，我们经历过接近 1:40 的压缩比，每个时间戳和值占用少于 0.4 个字节，并且在单个线程上每秒读取数亿个数据点的性能。
- en: This approach, named *History File Stores*, is also very well suited for distributing
    data sets. For example a complete yearly dataset of [AIS data](https://blog.senx.io/ais-data-made-easy/)
    from [AISHub](https://www.aishub.net/) fits in a single file less than 100Gb,
    including the [spatio-temporal indexing](https://blog.senx.io/spatio-temporal-indexing-in-warp-10/)
    of the ships movements, this single file can be served by a Warp 10 instance and
    applications can interact with that data with blazing fast performance.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 这种称为*History File Stores*的方法也非常适合分发数据集。例如，来自[AISHub](https://www.aishub.net/)的[船舶自动识别系统数据](https://blog.senx.io/ais-data-made-easy/)的完整年度数据集可以容纳在小于
    100Gb 的单个文件中，其中包括船舶运动的[时空索引](https://blog.senx.io/spatio-temporal-indexing-in-warp-10/)，这个单个文件可以由
    Warp 10 实例提供，并且应用程序可以以极快的性能与该数据交互。
- en: The History File Store format is currently available via our *Technology Preview*
    program, don't hesitate to [reach out](mailto:contact@senx.io) if you are interested.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: History File Store 格式目前通过我们的*技术预览*计划提供，请不要犹豫，如果您感兴趣，请[联系我们](mailto:contact@senx.io)。
