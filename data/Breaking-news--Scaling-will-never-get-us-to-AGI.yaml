- en: <!--yml
  id: totrans-split-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-split-1
  prefs: []
  type: TYPE_NORMAL
  zh: 'category: 未分类'
- en: 'date: 2024-05-27 13:00:56'
  id: totrans-split-2
  prefs: []
  type: TYPE_NORMAL
  zh: 'date: 2024-05-27 13:00:56'
- en: -->
  id: totrans-split-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: 'Breaking news: Scaling will never get us to AGI'
  id: totrans-split-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 突发新闻：扩展永远无法让我们达到通用人工智能
- en: 来源：[https://garymarcus.substack.com/p/breaking-news-scaling-will-never](https://garymarcus.substack.com/p/breaking-news-scaling-will-never)
  id: totrans-split-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://garymarcus.substack.com/p/breaking-news-scaling-will-never](https://garymarcus.substack.com/p/breaking-news-scaling-will-never)
- en: Neural networks (at least in the configurations that have been dominant over
    the last three decades) have trouble generalizing beyond the multidimensional
    space that surrounds their training examples. That limits their ability to reason
    and plan reliably. It also drives their greediness with data, and even the ethical
    choices their developers have been making. There will never be enough data; there
    will always be outliers. This is why driverless cars are still just demos, and
    why LLMs will never be reliable.
  id: totrans-split-6
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络（至少是过去三十年中主导的配置）在超出其训练示例周围的多维空间进行泛化时存在困难。这限制了它们进行可靠推理和规划的能力。这也促使它们对数据的贪婪，甚至影响到它们开发者的伦理选择。永远不会有足够的数据；永远会有异常值。这就是为什么无人驾驶汽车仍然只是演示，以及为什么大型语言模型永远不会可靠的原因。
- en: I have said this so often in so many ways, going back to [1998](https://www.sciencedirect.com/science/article/pii/S0010028598906946),
    that today I am going to let someone else, Chomba Bupe, a sharp-thinking tech
    entrepreneur/computer vision researcher from Zambia, take a shot.
  id: totrans-split-7
  prefs: []
  type: TYPE_NORMAL
  zh: 我已经在很多场合多次如此说过，回溯到[1998](https://www.sciencedirect.com/science/article/pii/S0010028598906946)，今天我要让来自赞比亚的敏锐思维技术企业家/计算机视觉研究员
    Chomba Bupe 来试试看。
- en: 'An important [new preprint](https://arxiv.org/abs/2404.04125) that just came
    out on data and scaling. Bupe explains it well:'
  id: totrans-split-8
  prefs: []
  type: TYPE_NORMAL
  zh: 一个重要的[新预印本](https://arxiv.org/abs/2404.04125)关于数据和扩展刚刚发布。Bupe 很好地解释了：
- en: 'In short, diminishing returns are what we can expect. (Asterisk: current models
    aren’t literally look up tables; they can generalize to some degree, but not enough.
    As I explained in the 1998 paper and in The Algebraic Mind they can generalize
    within a space of training examples but face considerable trouble beyond that
    space.)'
  id: totrans-split-9
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，我们可以期待边际效益递减。（星号：当前模型并非字面上的查找表；它们可以在某种程度上泛化，但不够。正如我在1998年的论文和《代数心灵》中所解释的，它们可以在训练示例空间内泛化，但在那个空间之外遇到了相当大的困难。）
- en: Sooner or later, the exponential greed for data will exceed what is available.
  id: totrans-split-10
  prefs: []
  type: TYPE_NORMAL
  zh: 迟早，对数据的指数级需求将超过可获得的量。
- en: To get to AGI, we need alternative approaches that can generalize better beyond
    the data on which they have been trained. End of story.
  id: totrans-split-11
  prefs: []
  type: TYPE_NORMAL
  zh: 要达到通用人工智能，我们需要能够在它们被训练的数据之外更好地泛化的替代方法。故事的结尾。
- en: '***Gary Marcus*** **looks forward to watching what happens as people begin
    to take in the implications.**'
  id: totrans-split-12
  prefs: []
  type: TYPE_NORMAL
  zh: '***Gary Marcus*** **期待着看看人们开始接受这些含义时会发生什么。**'
