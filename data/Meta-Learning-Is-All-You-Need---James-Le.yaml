- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 类别：未分类
- en: 'date: 2024-05-27 15:03:08'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-05-27 15:03:08
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: Meta-Learning Is All You Need — James Le
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 元学习就是你所需要的一切 —— 詹姆斯·勒
- en: 来源：[https://jameskle.com/writes/meta-learning-is-all-you-need](https://jameskle.com/writes/meta-learning-is-all-you-need)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://jameskle.com/writes/meta-learning-is-all-you-need](https://jameskle.com/writes/meta-learning-is-all-you-need)
- en: Neural networks have been highly influential in the past decades in the machine
    learning community, thanks to the rise of computing power, the abundance of unstructured
    data, and the advancement of algorithmic solutions. However, it is still a long
    way for researchers to completely use neural networks in real-world settings where
    the data is scarce and requirements for model accuracy/speed are critical.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 在过去几十年中，由于计算能力的提升、非结构化数据的丰富以及算法解决方案的进步，神经网络在机器学习社区中具有极高的影响力。然而，对于研究人员来说，要在数据稀缺且模型准确度/速度要求严格的真实世界环境中完全使用神经网络仍然还有很长的路要走。
- en: '**Meta-learning**, also known as *learning how to learn*, has recently emerged
    as a potential learning paradigm that can learn information from one task and
    generalize that information to unseen tasks proficiently. During this quarantine
    time, I started watching lectures on [Stanford’s CS 330 class on Deep Multi-Task
    and Meta-Learning](http://cs330.stanford.edu/) taught by the brilliant [Chelsea
    Finn](https://ai.stanford.edu/~cbfinn/). As a courtesy of her lectures, this blog
    post attempts to answer these key questions:'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '**元学习**，也被称为*学习如何学习*，最近已经成为一个潜在的学习范式，它可以从一个任务中学习信息，并将该信息有效地推广到未见过的任务中。在这段隔离时间里，我开始观看由杰出的[切尔西·芬恩](https://ai.stanford.edu/~cbfinn/)教授的[斯坦福大学
    CS 330 深度多任务与元学习课程](http://cs330.stanford.edu/)的讲座。作为对她讲座的一种礼貌，这篇博客尝试回答以下关键问题：'
- en: Why do we need meta-learning?
  id: totrans-8
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们为什么需要元学习？
- en: How does the math of meta-learning work?
  id: totrans-9
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 元学习的数学原理是如何工作的？
- en: What are the different approaches to design a meta-learning algorithm?
  id: totrans-10
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设计元学习算法的不同方法有哪些？
- en: '**Note:** *The content of this post is largely based on CS330''s* [*lecture
    1 on problem definitions*](https://www.youtube.com/watch?v=0rZtSwNOTQo&list=PLoROMvodv4rMC6zfYmnD7UG3LVvwaITY5&index=1)*,*
    [*lecture 2 on supervised and black-box meta-learning*](https://www.youtube.com/watch?v=6stKGH6zI8g&list=PLoROMvodv4rMC6zfYmnD7UG3LVvwaITY5&index=2)*,*
    [*lecture 3 on optimization-based meta-learning*](https://www.youtube.com/watch?v=v7otSgpTc0Q&list=PLoROMvodv4rMC6zfYmnD7UG3LVvwaITY5&index=3)*,
    and* [*lecture 4 on few-shot learning via metric learning*](https://www.youtube.com/watch?v=bc-6tzTyYcM&list=PLoROMvodv4rMC6zfYmnD7UG3LVvwaITY5&index=4)*.
    They are all accessible to the public.*'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '**注意：** *本文的内容主要基于 CS330 的* [*问题定义讲座 1*](https://www.youtube.com/watch?v=0rZtSwNOTQo&list=PLoROMvodv4rMC6zfYmnD7UG3LVvwaITY5&index=1)*,*
    [*监督式和黑盒元学习讲座 2*](https://www.youtube.com/watch?v=6stKGH6zI8g&list=PLoROMvodv4rMC6zfYmnD7UG3LVvwaITY5&index=2)*,*
    [*基于优化的元学习讲座 3*](https://www.youtube.com/watch?v=v7otSgpTc0Q&list=PLoROMvodv4rMC6zfYmnD7UG3LVvwaITY5&index=3)*,*
    和* [*通过度量学习进行少样本学习讲座 4*](https://www.youtube.com/watch?v=bc-6tzTyYcM&list=PLoROMvodv4rMC6zfYmnD7UG3LVvwaITY5&index=4)*。它们都是公开可访问的。*'
- en: '**1 - Motivation for Meta-Learning**'
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**1 - 元学习的动机**'
- en: Thanks to the advancement in algorithms, data, and compute power in the past
    decade, deep neural networks have allowed us to handle unstructured data (such
    as images, text, audio, video, etc.) very well without the need to engineer features
    by hand. Empirical research has shown that if neural networks can generalize very
    well if we feed them large and diverse inputs. For example, [Transformers](https://arxiv.org/abs/1706.03762)
    and [GPT-2](https://openai.com/blog/better-language-models/) have made the wave
    in the Natural Language Processing research community last year with their wide
    applicability in various tasks.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 由于过去十年算法、数据和计算能力的进步，深度神经网络已经使我们能够非常好地处理非结构化数据（例如图像、文本、音频、视频等），而无需手工设计特征。经验研究表明，如果我们为神经网络提供大量且多样化的输入，它们可以非常好地推广。例如，[变压器](https://arxiv.org/abs/1706.03762)和[GPT-2](https://openai.com/blog/better-language-models/)去年在自然语言处理研究社区掀起了风浪，因为它们在各种任务中的广泛适用性。
- en: 'However, there is a catch with using neural networks in the real-world setting
    where:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，在使用神经网络的真实世界设置中存在一个问题，即：
- en: '**Large datasets are unavailable**: This issue is common in many domains ranging
    from classification of rare diseases to translation of rare languages. It is clearly
    impractical to learn from scratch for each task in these scenarios.'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**大型数据集不可用**：这个问题在许多领域都很常见，从罕见疾病的分类到罕见语言的翻译都是如此。在这些场景中，对每个任务从头开始学习显然是不可行的。'
- en: '**Data has a long tail**: This issue can easily break the standard machine
    learning paradigm. For example, in the self-driving car setting, an autonomous
    vehicle can be trained to handle common situations very well, but it often struggles
    with uncommon situations (such as people jay-walking, animals crossing, traffic
    lines not working) where humans can easily handle. This can lead to very bad outcomes,
    such as [the Uber''s accident in Arizona](https://en.wikipedia.org/wiki/Death_of_Elaine_Herzberg)
    a few years ago.'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据呈长尾分布**：这个问题很容易打破标准的机器学习范式。例如，在自动驾驶汽车设置中，自主车辆可以很好地训练以处理常见情况，但在不常见情况下（如人们横穿马路、动物穿越、交通线不起作用等），它经常遇到困难，而人类可以轻松处理。这可能导致非常糟糕的结果，比如[几年前亚利桑那州优步的事故](https://en.wikipedia.org/wiki/Death_of_Elaine_Herzberg)。'
- en: '**We want to quickly learn something about a new task without training our
    model from scratch:** Humans can do this quite easily by leveraging our prior
    experience. For example, if I know a bit of Spanish, then it should not be too
    difficult for me to learn Italian, as these two languages are quite similar linguistically.'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**我们希望能够快速学习新任务的一些知识，而不必从头开始训练我们的模型：**人类可以通过利用我们的先前经验轻松做到这一点。例如，如果我了解一点西班牙语，那么学习意大利语就不应该太困难，因为这两种语言在语言学上相似。'
- en: In this article, I would like to give an introductory overview of meta-learning,
    which is a learning framework that can help our neural network become more effective
    in the settings mentioned above. In this setup, we want our network to learn a
    new task more proficiently - assuming that it is given access to data on previous
    tasks.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇文章中，我想对元学习进行初步概述，这是一种学习框架，可以帮助我们的神经网络在上述设置中变得更加有效。在这种设置中，我们希望我们的网络更加熟练地学习新任务-假设它可以访问以前任务的数据。
- en: Historically, there have been a few papers thinking in this direction.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 从历史上看，有一些论文在这个方向上进行了思考。
- en: Back in 1992, [Bengio et al.](https://www.researchgate.net/publication/2389122_On_the_Optimization_of_a_Synaptic_Learning_Rule) looked
    at the possibility of a learning rule that can solve new tasks.
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[1992年](https://www.researchgate.net/publication/2389122_On_the_Optimization_of_a_Synaptic_Learning_Rule)，[Bengio等人](https://www.researchgate.net/publication/2389122_On_the_Optimization_of_a_Synaptic_Learning_Rule)研究了一种可以解决新任务的学习规则的可能性。'
- en: In 1997, [Rich Caruana](https://link.springer.com/article/10.1023/A:1007379606734)
    wrote a survey about **multi-task learning**, which is a variant of meta-learning.
    He explained how tasks can be learned in parallel using a shared representation
    between models and also presented a multi-task inductive transfer notion that
    uses back-propagation to handle additional tasks.
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[1997年](https://link.springer.com/article/10.1023/A:1007379606734)，[Rich Caruana](https://link.springer.com/article/10.1023/A:1007379606734)撰写了一份关于**多任务学习**的调查报告，这是元学习的一种变体。他解释了如何通过在模型之间共享表示来并行学习任务，并提出了一种多任务归纳转移概念，该概念使用反向传播来处理额外的任务。'
- en: In 1998, [Sebastian Thrun](https://papers.nips.cc/paper/1034-is-learning-the-n-th-thing-any-easier-than-learning-the-first.pdf)
    explored the problem of **lifelong learning**, which is inspired by the ability
    of humans to exploit experiences that come from related learning tasks to generalize
    to new tasks.
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[1998年](https://papers.nips.cc/paper/1034-is-learning-the-n-th-thing-any-easier-than-learning-the-first.pdf)，[Sebastian
    Thrun](https://papers.nips.cc/paper/1034-is-learning-the-n-th-thing-any-easier-than-learning-the-first.pdf)探讨了**终身学习**的问题，灵感来自人类利用来自相关学习任务的经验以概括新任务的能力。'
