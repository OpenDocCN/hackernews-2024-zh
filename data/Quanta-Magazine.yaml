- en: <!--yml
  id: totrans-split-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-split-1
  prefs: []
  type: TYPE_NORMAL
  zh: 'category: 未分类'
- en: 'date: 2024-05-27 13:12:06'
  id: totrans-split-2
  prefs: []
  type: TYPE_NORMAL
  zh: 'date: 2024-05-27 13:12:06'
- en: -->
  id: totrans-split-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: Quanta Magazine
  id: totrans-split-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Quanta Magazine
- en: 来源：[https://www.quantamagazine.org/how-do-machines-grok-data-20240412/](https://www.quantamagazine.org/how-do-machines-grok-data-20240412/)
  id: totrans-split-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://www.quantamagazine.org/how-do-machines-grok-data-20240412/](https://www.quantamagazine.org/how-do-machines-grok-data-20240412/)
- en: As a network trains, it tends to learn more complex functions, and the discrepancy
    between the expected output and the actual one starts falling for training data.
    Even better, this discrepancy, known as loss, also starts going down for test
    data, which is new data not used in training. But at some point, the model starts
    to overfit, and while the loss on training data keeps falling, the test data’s
    loss starts to rise. So, typically, that’s when researchers stop training the
    network.
  id: totrans-split-6
  prefs: []
  type: TYPE_NORMAL
  zh: 随着网络的训练，它往往会学习更复杂的函数，训练数据中预期输出与实际输出之间的差异开始减少。更好的是，这种差异，即损失，对于测试数据也开始减少，测试数据是未在训练中使用的新数据。但是在某个时刻，模型开始过拟合，虽然训练数据上的损失持续下降，但测试数据上的损失开始上升。因此，通常研究人员会在这时停止训练网络。
- en: 'That was the prevailing wisdom when the team at OpenAI began exploring how
    a neural network could do math. They were using a small [transformer](https://www.quantamagazine.org/will-transformers-take-over-artificial-intelligence-20220310/)
    — a network architecture that’s recently revolutionized large language models
    — to do different kinds of modular arithmetic, in which you work with a limited
    set numbers that loop back on themselves. Modulo 12, for example, can be done
    on a clock face: 11 + 2 = 1\. The team showed the network examples of adding two
    numbers, *a* and *b*, to produce an output, *c,* in modulo 97 (equivalent to a
    clock face with 97 numbers). They then tested the transformer on unseen combinations
    of *a* and *b* to see if it could correctly predict *c*.'
  id: totrans-split-7
  prefs: []
  type: TYPE_NORMAL
  zh: 这是当 OpenAI 的团队开始探索神经网络如何进行数学运算时的普遍智慧。他们使用了一个小型[transformer](https://www.quantamagazine.org/will-transformers-take-over-artificial-intelligence-20220310/)
    —— 这种网络架构最近已经彻底改变了大型语言模型的方式 —— 来进行不同类型的模块化算术运算，在这些运算中，你使用的数字集会循环回到起点。例如，在模 12
    中，你可以把它想象成一个时钟：11 + 2 = 1。团队向网络展示了如何将两个数字 *a* 和 *b* 相加得到一个输出 *c*，在模 97 中（相当于一个有
    97 个数字的时钟面）。然后，他们在未曾见过的 *a* 和 *b* 的组合上测试了 transformer，以查看它是否能够正确预测 *c*。
- en: As expected, when the network entered the overfitting regime, the loss on the
    training data came close to zero (it had begun memorizing what it had seen), and
    the loss on the test data began climbing. It wasn’t generalizing. “And then one
    day, we got lucky,” said team leader Alethea Power, [speaking in September 2022](https://youtu.be/gYGWFjMf9JA?t=1236)
    at a conference in San Francisco. “And by lucky, I mean forgetful.”
  id: totrans-split-8
  prefs: []
  type: TYPE_NORMAL
  zh: 正如预期的那样，当网络进入过拟合阶段时，训练数据上的损失接近零（它开始记住它所见到的内容），而测试数据上的损失开始上升。它没有泛化。“然后有一天，我们走运了，”
    团队负责人 Alethea Power 在[2022 年九月的一个会议上说道](https://youtu.be/gYGWFjMf9JA?t=1236)，“我说的走运，意思是健忘。”
- en: The team member who was training the network went on vacation and forgot to
    stop the training. As this version of the network continued to train, it suddenly
    became accurate on unseen data. Automatic testing revealed this unexpected accuracy
    to the rest of the team, and they soon realized that the network had found clever
    ways of arranging the numbers *a* and *b*. Internally, the network represents
    the numbers in some high-dimensional space, but when the researchers projected
    these numbers down to 2D space and mapped them, the numbers formed a circle.
  id: totrans-split-9
  prefs: []
  type: TYPE_NORMAL
  zh: 负责训练网络的团队成员去度假时忘记停止训练。当这个版本的网络继续训练时，它突然在未见过的数据上变得精确。自动测试向团队的其他成员展示了这种意外的准确性，他们很快意识到网络找到了安排数字
    *a* 和 *b* 的巧妙方式。在内部，网络以某种高维空间表示这些数字，但当研究人员将这些数字投射到二维空间并进行映射时，这些数字形成了一个圆形。
- en: This was astonishing. The team never told the model it was doing modulo 97 math,
    or even what modulo meant — they just showed it examples of arithmetic. The model
    seemed to have stumbled upon some deeper, analytical solution — an equation that
    generalized to all combinations of *a* and *b*, even beyond the training data.
    The network had grokked, and the accuracy on test data shot up to 100%. “This
    is weird,” Power told her audience.
  id: totrans-split-10
  prefs: []
  type: TYPE_NORMAL
  zh: 这是令人惊讶的。团队从未告诉模型它在进行模 97 的数学运算，甚至什么是模运算 —— 他们只是向它展示了算术的例子。模型似乎偶然发现了一些更深层次的分析解决方案
    —— 一个可以推广到所有 *a* 和 *b* 组合的方程式，甚至超出了训练数据。网络已经理解，测试数据的准确率飙升到了100%。“这很奇怪，” Power
    在听众面前说道。
- en: The team verified the results using different tasks and different networks.
    The discovery held up.
  id: totrans-split-11
  prefs: []
  type: TYPE_NORMAL
  zh: 团队使用不同任务和不同网络验证了结果。这一发现经得住考验。
- en: '**Of Clocks and Pizzas**'
  id: totrans-split-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**关于时钟和比萨**'
- en: But what was the equation the network had found? The OpenAI paper didn’t say,
    but the result caught Nanda’s attention. “One of the core mysteries and annoying
    things about neural networks is that they’re very good at what they do, but that
    by default, we have no idea how they work,” said Nanda, whose work focuses on
    reverse-engineering a trained network to figure out what algorithms it learned.
  id: totrans-split-13
  prefs: []
  type: TYPE_NORMAL
  zh: 但网络找到的方程是什么呢？OpenAI 的论文没有说，但结果引起了南达的注意。“神经网络的一个核心谜团和让人恼火的事情是，它们擅长自己的工作，但默认情况下，我们不知道它们是如何工作的，”南达说道，他的工作重点是逆向工程训练好的网络，以找出它学到了什么算法。
- en: 'Nanda was fascinated by the OpenAI discovery, and he decided to pick apart
    a neural network that had grokked. He designed an even simpler version of the
    OpenAI neural network so that he could closely examine the model’s parameters
    as it learned to do modular arithmetic. He saw the same behavior: overfitting
    that gave way to generalization and an abrupt improvement in test accuracy. His
    network was also arranging numbers in a circle. It took some effort, but Nanda
    eventually figured out why.'
  id: totrans-split-14
  prefs: []
  type: TYPE_NORMAL
  zh: 南达对OpenAI的发现深感兴趣，决定拆解一个已经理解的神经网络。他设计了一个更简单的OpenAI神经网络版本，以便他可以仔细检查模型学习模数算术时的参数。他看到了相同的行为：过度拟合导致泛化和测试精度的突然提高。他的网络也在圆圈里安排数字。尽管需要一些努力，但南达最终弄清楚了原因。
- en: 'While it was representing the numbers on a circle, the network wasn’t simply
    counting off digits like a kindergartner watching a clock: It was doing some sophisticated
    mathematical manipulations. By studying the values of the network’s parameters,
    [Nanda and colleagues revealed](https://arxiv.org/abs/2301.05217) that it was
    adding the clock numbers by performing “discrete Fourier transforms” on them —
    transforming the numbers using trigonometric functions such as sines and cosines
    and then manipulating these values using trigonometric identities to arrive at
    the solution. At least, this was what his particular network was doing.'
  id: totrans-split-15
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然它在圆圈上表示数字，但这个网络并不像幼儿园小朋友看时钟一样简单地数数：它进行了一些复杂的数学操作。通过研究网络参数的值，[南达和同事们揭示](https://arxiv.org/abs/2301.05217)，它是通过对它们执行“离散傅里叶变换”来添加时钟数字
    —— 使用三角函数如正弦和余弦来变换数字，然后使用三角恒等式来操作这些值以得到解。至少，这是他特定的网络正在做的事情。
- en: 'When a team at MIT [followed up](https://arxiv.org/abs/2306.17844) on Nanda’s
    work, they showed that the grokking neural networks don’t always discover this
    “clock” algorithm. Sometimes, the networks instead find what the researchers call
    the “pizza” algorithm. This approach imagines a pizza divided into slices and
    numbered in order. To add two numbers, imagine drawing arrows from the center
    of the pizza to the numbers in question, then calculating the line that bisects
    the angle formed by the first two arrows. This line passes through the middle
    of some slice of the pizza: The number of the slice is the sum of the two numbers.
    These operations can also be written down in terms of trigonometric and algebraic
    manipulations of the sines and cosines of *a* and *b*, and they’re theoretically
    just as accurate as the clock approach.'
  id: totrans-split-16
  prefs: []
  type: TYPE_NORMAL
  zh: 当麻省理工学院的一个团队[跟进](https://arxiv.org/abs/2306.17844)南达的工作时，他们表明，理解神经网络不总是发现这种“时钟”算法。有时，网络反而会找到研究人员称之为“比萨”算法。这种方法想象一个被切成片并按顺序编号的比萨饼。要添加两个数字，想象从比萨的中心到所讨论数字的箭头，然后计算通过第一对箭头形成的角度的中分线。这条线经过比萨的某个片的中心：片的编号就是两个数字的和。这些操作也可以用三角函数和代数操作来写出，理论上与时钟方法一样准确。
