- en: <!--yml
  id: totrans-split-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-split-1
  prefs: []
  type: TYPE_NORMAL
  zh: 'category: 未分类'
- en: 'date: 2024-05-27 14:37:52'
  id: totrans-split-2
  prefs: []
  type: TYPE_NORMAL
  zh: 'date: 2024-05-27 14:37:52'
- en: -->
  id: totrans-split-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: Lessons From Our 8 Years Of Kubernetes In Production — Two Major Cluster Crashes,
    Ditching Self-Managed, Cutting Cluster Costs, Tooling, And More | by Anders Jönsson
    | Medium
  id: totrans-split-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 从我们在生产中使用 Kubernetes 的 8 年经验中学到的教训 —— 两次主要集群崩溃，放弃自管理，降低集群成本，工具化，以及更多 | 作者：Anders
    Jönsson | Medium
- en: 来源：[https://medium.com/@.anders/learnings-from-our-8-years-of-kubernetes-in-production-two-major-cluster-crashes-ditching-self-0257c09d36cd](https://medium.com/@.anders/learnings-from-our-8-years-of-kubernetes-in-production-two-major-cluster-crashes-ditching-self-0257c09d36cd)
  id: totrans-split-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://medium.com/@.anders/learnings-from-our-8-years-of-kubernetes-in-production-two-major-cluster-crashes-ditching-self-0257c09d36cd](https://medium.com/@.anders/learnings-from-our-8-years-of-kubernetes-in-production-two-major-cluster-crashes-ditching-self-0257c09d36cd)
- en: Lessons From Our 8 Years Of Kubernetes In Production — Two Major Cluster Crashes,
    Ditching Self-Managed, Cutting Cluster Costs, Tooling, And More
  id: totrans-split-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 从我们在生产中使用 Kubernetes 的 8 年经验中学到的教训 —— 两次主要集群崩溃，放弃自管理，降低集群成本，工具化，以及更多
- en: Early on at Urb-it, before I joined, we decided to use Kubernetes as a cornerstone
    for our cloud-native strategy. The thinking behind this choice was our anticipated
    rapid scaling, coupled with the desire to leverage container orchestration capabilities
    to get a more dynamic, resilient, and efficient environment for our applications.
    And with our microservice architecture, Kubernetes fitted well.
  id: totrans-split-7
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Urb-it 的早期，我加入之前，我们决定将 Kubernetes 作为云原生战略的核心。这一选择的背后是我们预期的快速扩展，以及利用容器编排能力为我们的应用程序提供更动态、更弹性和更高效的环境。而且，考虑到我们的微服务架构，Kubernetes
    很好地适配了我们的需求。
- en: Early Decision
  id: totrans-split-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 早期决策
- en: The decision was made early, which, of course, should be questioned since it
    represents a significant dependency and a substantial amount of knowledge to carry
    for a startup (or any company). Also, did we even face the problems Kubernetes
    solves at that stage? One might argue that we could have initially gone with a
    sizable monolith and relied on that until scaling and other issues became painful,
    and then we made a move to Kubernetes (or something else). Also, Kubernetes was
    still in early development. But let’s go deep on this another time.
  id: totrans-split-9
  prefs: []
  type: TYPE_NORMAL
  zh: 我们早早地做出了这个决策，当然，这个决策应该受到质疑，因为它代表了对一个初创公司（或者任何公司）的重大依赖和大量的知识负担。此外，在那个阶段，我们是否真正面临
    Kubernetes 可以解决的问题？有人可能会认为，最初我们可以选择一个规模较大的单体应用，并依靠它直到扩展和其他问题变得痛苦时，再考虑迁移到 Kubernetes（或其他方案）。此外，Kubernetes
    当时仍处于早期开发阶段。但是，让我们另外深入探讨这个问题。
- en: '**8 Years In Production**'
  id: totrans-split-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**8 Years In Production**'
- en: Having run Kubernetes for over eight years in production *(separate cluster
    for each environment)*, we’ve made a mix of good and not-so-good decisions. Some
    mistakes were simply a result of “otur när vi tänkte” (bad luck in our decision-making),
    while others originated from us not entirely (or not even partly) understanding
    the underlying technology itself. Kubernetes is powerful, but it also has layers
    of complexity.
  id: totrans-split-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在生产环境中运行 Kubernetes 已经超过八年的时间（每个环境单独一个集群），我们做出了一些好的和不那么好的决策。有些错误仅仅是“otur när
    vi tänkte”（我们决策时的不好运气）的结果，而另一些则源于我们并没有完全（甚至根本没有）理解这项技术本身。Kubernetes 强大，但也非常复杂。
- en: We went head-on without any previous experience of running it at scale.
  id: totrans-split-12
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 我们毫无经验地迎头而上，试图在规模上运行它。
- en: '**Migrating From Self-Managed On AWS To Managed On Azure (AKS)**'
  id: totrans-split-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**从AWS自管理迁移到Azure托管（AKS）**'
- en: For the first years, we ran a self-managed cluster on AWS. If my memory serves
    me well, we didn’t have the option initially to use [Azure Kubernetes Service
    (AKS)](https://azure.microsoft.com/en-us/products/kubernetes-service), [Google
    Kubernetes Engine (GKE)](https://cloud.google.com/kubernetes-engine), [Amazon
    Elastic Kubernetes Service (EKS)](https://aws.amazon.com/eks/) since they didn’t
    provide a official managed solution yet. It was on [Amazon Web Services (AWS)](https://aws.amazon.com/)
    self-hosted we had our first and most horrible cluster crash in Urb-it history,
    but more on that later.
  id: totrans-split-14
  prefs: []
  type: TYPE_NORMAL
  zh: 在最初的几年里，我们在AWS上运行了一个自管理的集群。如果我没记错的话，最开始我们并没有选择使用 [Azure Kubernetes Service (AKS)](https://azure.microsoft.com/en-us/products/kubernetes-service)，[Google
    Kubernetes Engine (GKE)](https://cloud.google.com/kubernetes-engine)，[Amazon Elastic
    Kubernetes Service (EKS)](https://aws.amazon.com/eks/)，因为它们还没有提供官方的托管解决方案。我们的第一个也是最严重的集群崩溃发生在
    [Amazon Web Services (AWS)](https://aws.amazon.com/) 的自托管环境中，稍后我们会详细讲述这件事情。
- en: Since we were a small team, it was challenging to keep up with all the new capabilities
    we needed. At the same time, managing a self-hosted cluster required constant
    attention and care, which added to our workload.
  id: totrans-split-15
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们是一个小团队，跟上我们所需的所有新能力是具有挑战性的。同时，管理自托管集群需要不断的关注和维护，这增加了我们的工作量。
- en: When managed solutions became generally available, we took some time to evaluate
    [AKS](https://azure.microsoft.com/en-us/products/kubernetes-service), [GKE](https://cloud.google.com/kubernetes-engine),
    and [EKS](https://aws.amazon.com/eks/). All of them were multiple times better
    for us than managing it ourselves, and we could easily see the quick ROI with
    moving.
  id: totrans-split-16
  prefs: []
  type: TYPE_NORMAL
  zh: 当托管解决方案普遍可用时，我们花了一些时间评估[Azure Kubernetes 服务 (AKS)](https://azure.microsoft.com/en-us/products/kubernetes-service)、[Google
    Kubernetes Engine (GKE)](https://cloud.google.com/kubernetes-engine)和[Amazon EKS](https://aws.amazon.com/eks/)。对于我们来说，它们都比自己管理好几倍，并且我们可以很容易地看到通过迁移带来的快速投资回报率。
- en: Our platform back then was 50% .Net and 50% Python, and we were already using
    Azure Service Bus, Azure SQL Server, and other Azure services. Therefore, moving
    our cluster to Azure would not only make it easier to use them in an integrated
    fashion but also benefit us by utilizing the Azure Backbone Networking Infrastructure,
    avoiding the costs associated with leaving/entering external networks and VNETs,
    which we had between our mixed AWS and Azure setup. Also, many of our engineers
    were familiar with Azure and its ecosystem.
  id: totrans-split-17
  prefs: []
  type: TYPE_NORMAL
  zh: 当时，我们的平台是50% 的 .Net 和 50% 的 Python，我们已经在使用 Azure Service Bus、Azure SQL Server
    和其他 Azure 服务。因此，将我们的集群迁移到 Azure 不仅可以更轻松地集成使用它们，还可以通过利用 Azure 主干网络基础设施来使我们受益，避免与离开/进入外部网络和
    VNETs 相关的成本，这些成本是我们在混合 AWS 和 Azure 设置中存在的。此外，我们的许多工程师都熟悉 Azure 及其生态系统。
- en: We should also mention that for our initial setup on AKS, we didn’t have to
    pay for the control plane nodes (master nodes) — which, was an extra bonus (saving
    money on nodes).
  id: totrans-split-18
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还应该提到，对于我们在 AKS 上的初始设置，我们不必为控制平面节点（主节点）付费 — 这是一个额外的好处（节省节点费用）。
- en: '*We migrated during the winter of 2018, and even though we have encountered
    some issues with AKS over the years, we have never regretted the move.*'
  id: totrans-split-19
  prefs: []
  type: TYPE_NORMAL
  zh: '*我们在2018年冬天迁移了，尽管多年来我们遇到了一些 AKS 的问题，但我们从未后悔过这次迁移。*'
- en: '**Cluster Crash #1**'
  id: totrans-split-20
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**集群崩溃 #1**'
- en: During our self-managed time on AWS, we experienced a massive cluster crash
    that resulted in the majority of our systems and products going down. The Root
    CA certificate, etcd certificate, and API server certificate expired, which caused
    the cluster to stop working and prevented our management of it. The support to
    resolve this, at that time, in kube-aws was limited. We brought in an expert,
    but in the end, we had to rebuild the entire cluster from scratch.
  id: totrans-split-21
  prefs: []
  type: TYPE_NORMAL
  zh: 在 AWS 上自我管理期间，我们遇到了一个严重的集群崩溃，导致我们大多数系统和产品宕机。根 CA 证书、etcd 证书和 API 服务器证书过期，导致集群停止工作并阻止我们对其进行管理。那时，在
    kube-aws 中解决这个问题的支持是有限的。我们请来了一位专家，但最终我们不得不从头开始重建整个集群。
- en: 'We thought we had all the values and Helm charts in each git repository, but,
    surprise, surprise, that wasn’t the case for all services. On top of this, none
    of the configurations for creating the cluster were stored. It became a race against
    time to set up the cluster again and populate it with all the services and products
    we had. Some of them required reinventing the Helm charts to create the missing
    configurations. There were moments like Dev1 to Dev2: “Do you remember how much
    CPU or RAM this service should have, or what network and port access it should
    have?”. Not to mention all the secrets that were gone with the wind.'
  id: totrans-split-22
  prefs: []
  type: TYPE_NORMAL
  zh: 我们以为所有的值和 Helm charts 都在每个 git 仓库里，但是，出乎意料的是，并非所有服务都是这样。更重要的是，用于创建集群的所有配置都没有存储。这变成了一场与时间赛跑，要重新设置集群并填充我们拥有的所有服务和产品。其中一些需要重新发明
    Helm charts 来创建缺失的配置。像 Dev1 对 Dev2 说：“你还记得这个服务应该有多少 CPU 或内存，或者它应该有什么网络和端口访问权限吗？”。更不用说所有随风而逝的秘密了。
- en: It took us days to get it up and running again. Not our proudest moment, to
    say the least.
  id: totrans-split-23
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 我们花了几天时间让它重新运行起来。这绝不是我们最自豪的时刻。
- en: Thanks to our proactive communication, by maintaining transparency, honesty,
    and nurturing our relationships, we didn’t lose any business or customers.
  id: totrans-split-24
  prefs: []
  type: TYPE_NORMAL
  zh: 多亏了我们的积极沟通，通过保持透明、诚实和培养我们的关系，我们没有失去任何业务或客户。
- en: '**Cluster Crash #2**'
  id: totrans-split-25
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**集群崩溃 #2**'
- en: 'And now you might say: the second crash couldn’t have been due to a certificate,
    since you must have learned your lesson from the first crash, right? Yes and no.
    When recreating the cluster from crash #1, unfortunately, the specific version
    of kube-aws that we used had an [issue](https://github.com/kubernetes-retired/kube-aws/issues/892).
    When it created the new clusters, it didn’t set the expiration of the etcd certificate
    to the provided expiry date; it defaulted to one year. So exactly one year after
    the first cluster crash, the certificate expired, and we experienced another cluster
    crash. However, this one was easier to recover from; we didn’t have to rebuild
    everything. But it was still a weekend from hell.'
  id: totrans-split-26
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你可能会说：第二次崩溃不可能是因为证书问题，因为你应该从第一次崩溃中吸取了教训，对吧？是和否。重新创建从第一次崩溃中的集群时，不幸的是，我们使用的
    kube-aws 版本具有一个 [问题](https://github.com/kubernetes-retired/kube-aws/issues/892)。当它创建新集群时，它没有将
    etcd 证书的过期日期设置为提供的到期日期；它默认为一年。因此，在第一次集群崩溃后的一年，证书过期了，我们经历了另一次集群崩溃。然而，这次更容易恢复；我们不必重新构建一切。但仍然是一个糟糕的周末。
- en: '*Side note 1: Other companies were also affected by this* [*bug*](https://github.com/kubernetes-incubator/kube-aws/issues/892)
    *the same way we were, not that it helped our customers…*'
  id: totrans-split-27
  prefs: []
  type: TYPE_NORMAL
  zh: '*附注 1：其他公司也受到了这个* [*bug*](https://github.com/kubernetes-incubator/kube-aws/issues/892)
    *的影响，就像我们一样，这并没有帮助我们的客户……*'
- en: '*Side note 2: Our plan was to update all the certificates after a year, but
    to give ourselves some margin, we set the expiration to two years (if I remember
    it correctly). So we had plans to update the certificates, but the bug beat us
    to it.*'
  id: totrans-split-28
  prefs: []
  type: TYPE_NORMAL
  zh: '*附注 2：我们的计划是在一年后更新所有证书，但为了给自己留有余地，我们将到期时间设置为两年（如果我记得没错的话）。因此，我们有更新证书的计划，但是
    bug 比我们还快一步。*'
- en: '*Since 2018, we have not had any more cluster crashes… Jinxing it? Yes.*'
  id: totrans-split-29
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*自 2018 年以来，我们再也没有遇到过集群崩溃…… 这是在自吹自擂吗？是的。*'
- en: Learnings
  id: totrans-split-30
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 学到的东西
- en: '**Kubernetes Is Complex** You need engineers who are interested in and want
    to work with the infrastructure and operations aspects of Kubernetes. In our case,
    we needed a couple of engineers who, in addition to their regular duties, would
    devote their time to Kubernetes as the “go-to” experts whenever necessary. The
    workload for Kubernetes-specific tasks varied, as you might imagine. Some weeks
    there was almost nothing to do, while others required more attention, such as
    during a cluster upgrade.'
  id: totrans-split-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Kubernetes Is Complex** 您需要对 Kubernetes 基础设施和运维方面感兴趣并愿意从事相关工作的工程师。在我们的情况下，除了正常职责外，我们需要几名工程师随时担任
    Kubernetes 的“问题专家”。Kubernetes 特定任务的工作量各不相同，这一点您可能已经可以想象到了。有些周几乎没有什么要做的，而在其他时候，例如在集群升级期间，需要更多的关注。'
- en: It was impossible for us to rotate and split the work over the entire team;
    the technology is too complex to “jump in and out of” every second week. Of course,
    everyone needs to know how to use it (deploy, debugging, etc.) — but to excel
    in the more challenging aspects, dedicated time is necessary. Additionally, it’s
    important to have someone who leads with a vision and has a strategy for evolving
    the cluster.
  id: totrans-split-32
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 对我们来说，不可能轮换和分配整个团队的工作；技术过于复杂，无法每两周“跳进和跳出”。当然，每个人都需要知道如何使用它（部署、调试等） — 但要在更具挑战性的方面出色，需要专门的时间。此外，重要的是要有人以远见领导，并制定演化集群的策略。
- en: '**Kubernetes Certificates**'
  id: totrans-split-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Kubernetes 证书**'
- en: Having experienced two cluster crashes, both due to certificates expiring, it’s
    crucial to be well-versed in the details of internal Kubernetes certificates and
    their expiration dates.
  id: totrans-split-34
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 经历了两次集群崩溃，都是因为证书过期，深入了解 Kubernetes 内部证书及其过期日期的细节至关重要。
- en: '**Keep Kubernetes & Helm Up To Date** When you fall behind, it becomes expensive
    and tedious. We always waited a couple of months before jumping on the latest
    version to ensure that others would face any new version issues first. But even
    with keeping it up to date, we faced many time-consuming rewrites of configurations
    files and charts due to new versions of Kubernetes and Helm *(Kubernetes API’s
    going from alfa to beta, beta to 1.0, etc.).* I know Simon and Martin loved all
    the Ingress changes.'
  id: totrans-split-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**保持 Kubernetes 和 Helm 的最新状态** 落后一点就变得既昂贵又乏味。我们总是等待几个月才跳到最新版本，以确保其他人先面对任何新版本的问题。但即使保持更新，由于
    Kubernetes 和 Helm 的新版本（例如 Kubernetes API 从 alpha 到 beta，beta 到 1.0 等），我们仍然面临许多耗时的配置文件和图表重写。我知道
    Simon 和 Martin 都喜欢所有的 Ingress 更改。'
- en: '**Centralized Helm Charts**'
  id: totrans-split-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**集中管理的 Helm 图表**'
- en: When it came to the Helm charts, we grew tired of updating all 70+ charts for
    each version change, so we adopted a more generic *“one chart to rule them all”*
    approach. There are many pros and cons to a centralized Helm charts approach,
    but in the end, this suited our needs better.
  id: totrans-split-37
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 当涉及到Helm图表时，我们厌倦了为每个版本更改更新所有70个以上的图表，因此我们采用了更通用的*“一张图表统治它们所有”*的方法。集中式Helm图表方法有许多利弊，但最终，这更符合我们的需求。
- en: '**Disaster Recovery Plan**'
  id: totrans-split-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**灾难恢复计划**'
- en: 'I can’t emphasize this enough: make sure to have ways to recreate the cluster
    if needed. Yes, you can click around in a UI to create new clusters, but that
    approach will never work at scale or in a timely manner.'
  id: totrans-split-39
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我再也不能强调这一点了：务必确保有方法可以在需要时重新创建集群。是的，你可以在用户界面中点击来创建新的集群，但这种方法永远不适用于大规模或及时地工作。
- en: There are different ways to handle this, ranging from simple shell scripts to
    more advanced methods like using Terraform (or similar). Crossplane can also be
    used to manage Infrastructure as Code (IaC) and more.
  id: totrans-split-40
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 有不同的方法来处理这个问题，从简单的shell脚本到更高级的方法，如使用Terraform（或类似的）。Crossplane还可以用来管理基础设施即代码（IaC）等。
- en: For us, due to limited team bandwidth, we settled on storing and using shell
    scripts.
  id: totrans-split-41
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 对于我们来说，由于团队的带宽有限，我们决定存储和使用shell脚本。
- en: '**Regardless of the method you select, make sure to test the flow from time
    to time to ensure you can recreate the cluster if needed.**'
  id: totrans-split-42
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '**无论你选择的方法如何，请务必定期测试流程，以确保在需要时可以重新创建集群。**'
- en: '**Backup Of Secrets**'
  id: totrans-split-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**备份密钥**'
- en: Have a strategy for backing up and storing secrets. If your cluster goes away,
    all your secrets will be gone. And trust me, we experienced this first-hand; it
    takes a lot of time to get everything right again when you have multiple different
    microservices and external dependencies.
  id: totrans-split-44
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 有一个备份和存储密钥的策略。如果你的集群消失了，你所有的密钥也会消失。相信我，我们亲身经历过这一点；当你拥有多个不同的微服务和外部依赖时，要再次正确地获取所有内容需要花费很多时间。
- en: '**Vendor-Agnostic VS “Go All In”**'
  id: totrans-split-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**与供应商无关与“全面采用”**'
- en: In the beginning, after moving to AKS, we tried to keep our cluster vendor-agnostic,
    meaning that we would continue to use other services for container registry, auth,
    key vaults, etc. The idea was that we could easily move to another manage solution
    one day. While being vendor-agnostic is a great idea, for us, it came with a high
    opportunity cost. After a while, we decided to go all-in on AKS-related Azure
    products, like the container registry, security scanning, auth, etc. For us, this
    resulted in an improved developer experience, simplified security ( centralized
    access management with Azure Entra Id), and more, which led to faster time-to-market
    and reduced costs (volume benefits).
  id: totrans-split-46
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 刚开始时，我们在转移到AKS后，试图使我们的集群与供应商无关，意味着我们将继续使用其他服务进行容器注册表、认证、密钥保管库等功能。我们的想法是，我们将来可以轻松地转移到其他的托管解决方案。尽管保持与供应商无关是一个很好的想法，但对我们来说，它带来了很高的机会成本。过了一段时间，我们决定全面使用与AKS相关的Azure产品，比如容器注册表、安全扫描、认证等。对我们来说，这带来了更好的开发体验，简化的安全性（与Azure
    Entra Id集中式访问管理）等，从而导致了更快的上市时间和降低成本（量产效益）。
- en: '**Customer Resource Definitions** Yes, we went all in on the Azure products,
    but our guiding star was to have as few Custom Resource Definitions as possible,
    and instead use the built-in Kubernetes resources. However, we had some exceptions,
    like Traefik, since the Ingress API didn’t fulfill all our needs.'
  id: totrans-split-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**客户资源定义**是的，我们全面使用了Azure产品，但我们的指导要点是尽可能少地使用自定义资源定义，而是使用内置的Kubernetes资源。然而，我们也有一些例外，比如Traefik，因为Ingress
    API并未满足我们所有的需求。'
- en: '**Security** See below.'
  id: totrans-split-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**安全**见下文。'
- en: '**Observability**'
  id: totrans-split-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**可观察性**'
- en: See below.
  id: totrans-split-50
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 见下文。
- en: '**Pre-Scaling During Known Peaks** Even with the auto-scaler, we sometimes
    scaled too slowly. By using traffic data and common knowledge (we are a logistics
    company and have peaks at holidays), we scaled up the cluster manually (ReplicaSet)
    a day before the peak arrived, then scaled it down the day after (slowly to handle
    any second peak wave that might occur).'
  id: totrans-split-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**已知高峰期间的预扩展**即使使用了自动扩展器，我们有时也扩展得太慢。通过使用流量数据和常识（我们是一个物流公司，节日期间有高峰），我们手动扩展了集群（ReplicaSet）前一天到达高峰之前，然后在高峰后一天将其缩减（慢慢地来应对可能发生的第二波高峰）。'
- en: '**Drone Inside The Cluster**'
  id: totrans-split-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**集群内的飞行器**'
- en: We kept the [Drone](https://www.drone.io/) build system in the stage cluster;
    it had some benefits but also some drawbacks. It was easy to scale and use since
    it was in the same cluster. However, when building too much at the same time,
    it consumed almost all the resources, leading to a rush in Kubernetes to spin
    up new nodes. The best solution would probably be to have it as a pure SaaS solution,
    not having to worry about hosting and maintaining the product itself.
  id: totrans-split-53
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们将构建系统[Drone](https://www.drone.io/)保留在舞台集群中；它有一些好处，但也有一些缺点。由于它在同一集群中，很容易进行扩展和使用。然而，当同时构建太多时，它几乎消耗了所有资源，导致Kubernetes急于启动新节点。最好的解决方案可能是将其作为纯SaaS解决方案，无需担心托管和维护产品本身。
- en: '**Pick The Right Node Type** This is very context-specific, but depending on
    the node type, AKS reserves about ~10-30% of the available memory *(for internal
    AKS services).* So for us, we found it beneficial to use fewer but larger node
    types. Also, since we were running .Net on many of the services, we needed to
    choose node types with efficient and sizable IO. *(.Net frequently writes to disk
    for JIT and logging, and if this requires network access, it becomes slow. We
    also made sure that the node disk/cache had at least the same size as the total
    configured node disk size, to again, prevent the need for network jumps).*'
  id: totrans-split-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**选择正确的节点类型** 这非常依赖于具体的上下文，但根据节点类型不同，AKS会为可用内存的大约~10-30%预留（用于内部AKS服务）。因此，对我们来说，使用较少但更大的节点类型是有益的。另外，由于我们在许多服务上运行.Net，我们需要选择具有高效和可靠IO的节点类型。（.Net经常对JIT和日志进行磁盘写入，如果这需要网络访问，则会变慢。我们还确保节点磁盘/缓存至少与总配置的节点磁盘大小相同，以避免需要进行网络跳跃。）'
- en: '**Reserved Instances**'
  id: totrans-split-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**保留实例**'
- en: You can debate that this approach goes a bit against the flexibility of the
    cloud, but for us, reserving key instances for a year or two resulted in massive
    cost savings. In many cases, we would save 50–60% compared to the “pay as you
    go” approach. And yes, that’s plenty of cake for the team.
  id: totrans-split-56
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你可以争论说这种方法在某种程度上违背了云的灵活性，但对我们来说，将关键实例保留一年或两年实现了巨大的成本节约。在许多情况下，与“按需付费”方式相比，我们可以节省50-60%的费用。是的，这对团队来说是丰厚的蛋糕。
- en: '**k9s**'
  id: totrans-split-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**k9s**'
- en: '[https://k9scli.io/](https://k9scli.io/) is great tool for anyone who wants
    one level higher abstraction than pure *kubectl*'
  id: totrans-split-58
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[https://k9scli.io/](https://k9scli.io/) 是一个很棒的工具，适合任何希望比纯*kubectl*更高抽象层次的人。'
- en: Observability
  id: totrans-split-59
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**可观测性**'
- en: '**Monitoring**'
  id: totrans-split-60
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**监控**'
- en: Ensure you track the usage of memory, CPU, etc., over time so you can observe
    how your cluster is performing and determine if new capabilities are improving
    or worsening its performance. With this, it’s easier to find and set the “correct”
    limits for different pods (finding the right balance is important, since the pod
    is killed if it runs out of memory).
  id: totrans-split-61
  prefs: []
  type: TYPE_NORMAL
  zh: 确保随时间跟踪内存、CPU等的使用情况，以便观察您的集群的性能如何，并确定新功能是否改善或恶化了其性能。通过这样做，更容易找到并设置不同Pod的“正确”限制（找到正确的平衡非常重要，因为如果Pod内存不足时会被杀死）。
- en: '**Alerting**'
  id: totrans-split-62
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**警报**'
- en: Refining our alerting system was a process, but eventually, we directed all
    alerts to our Slack channels. This approach made it convenient to receive notifications
    whenever the cluster was not functioning as expected or if any unforeseen issues
    arose.
  id: totrans-split-63
  prefs: []
  type: TYPE_NORMAL
  zh: 优化我们的警报系统是一个过程，但最终，我们将所有警报都指向了我们的Slack频道。这种方法使得在集群出现预期外问题或发生任何意外问题时接收通知变得非常方便。
- en: '**Logging**'
  id: totrans-split-64
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**日志**'
- en: Having all logs consolidated in one place, along with a robust trace ID strategy
    (e.g. [OpenTelemetry](https://opentelemetry.io/) or similar), is crucial for any
    microservices architecture. It took us 2–3 years to get this right. If we had
    implemented it earlier, it would have saved us a considerable amount of time.
  id: totrans-split-65
  prefs: []
  type: TYPE_NORMAL
  zh: 将所有日志集中在一个地方，并配备健壮的跟踪ID策略（例如[OpenTelemetry](https://opentelemetry.io/)或类似的），对于任何微服务架构都至关重要。我们花了2-3年时间才做到这一点。如果我们早些实施它，将会节省大量时间。
- en: Security
  id: totrans-split-66
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**安全性**'
- en: Security in Kubernetes is a vast topic, and I highly recommend researching it
    thoroughly to understand all the nuances (e.g. see [NSA, CISA release Kubernetes
    Hardening Guidance](https://www.nsa.gov/Press-Room/News-Highlights/Article/Article/2716980/nsa-cisa-release-kubernetes-hardening-guidance/)).
    Below are some key points from our experience, but please note, this is by no
    means a complete picture of the challenges.
  id: totrans-split-67
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes中的安全性是一个广阔的主题，我强烈建议彻底研究以理解所有细微差别（例如参见[NSA，CISA发布的Kubernetes加固指南](https://www.nsa.gov/Press-Room/News-Highlights/Article/Article/2716980/nsa-cisa-release-kubernetes-hardening-guidance/)）。以下是我们经验中的一些关键点，但请注意，这绝不是挑战的完整图景。
- en: Access Control
  id: totrans-split-68
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 访问控制
- en: In brief, Kubernetes isn’t overly restrictive by default. Therefore, we invested
    considerable time in tightening access, implementing least privilege principles
    for pods and containers. Additionally, due to specific vulnerabilities, it was
    possible that an unprivileged attacker, could potentially escalate their privileges
    to root, circumventing Linux namespace restrictions, and in some cases, even escape
    the container to gain root access on the host node. Not good to say the least.
  id: totrans-split-69
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，Kubernetes默认情况下并不过于严格。因此，我们花了大量时间 tightening access，为pod和容器实施最小权限原则。此外，由于特定漏洞，未授权的攻击者有可能将其权限提升至root，绕过Linux命名空间限制，并且在某些情况下甚至逃逸容器以获取主机节点的root访问权限。这是非常不好的。
- en: You should set read only root filesystem, disable service account token auto
    mount, disable privilege escalation, drop all unnecessary capabilities, and more.
    In our specific setup, we use Azure Policy and Gatekeeper to make sure we didn’t
    deploy unsecure containers.
  id: totrans-split-70
  prefs: []
  type: TYPE_NORMAL
  zh: 您应该设置只读根文件系统，禁用服务账户令牌自动挂载，禁用特权升级，放弃所有不必要的能力，等等。在我们特定的设置中，我们使用Azure Policy和Gatekeeper来确保我们没有部署不安全的容器。
- en: In our Kubernetes setup within AKS, we leveraged the robustness of Role-Based
    Access Control (RBAC) to further enhance security and access management.
  id: totrans-split-71
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的AKS Kubernetes设置中，我们利用了基于角色的访问控制（RBAC）的强大性能，进一步增强了安全性和访问管理。
- en: Container Vulnerability
  id: totrans-split-72
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 容器漏洞
- en: There are many good tools out there that can scan and validate containers and
    other parts of Kubernetes. We used Azure Defender and Azure Defender for Containers
    to target some of our needs.
  id: totrans-split-73
  prefs: []
  type: TYPE_NORMAL
  zh: 有许多优秀的工具可以扫描和验证Kubernetes中的容器和其他部分。我们使用Azure Defender和Azure Defender for Containers来满足部分需求。
- en: '*Note: Instead of getting stuck in “*[*analysis paralysis*](https://en.wikipedia.org/wiki/Analysis_paralysis)*”
    trying to find the perfect tool, the one with all the “bells and whistles”, just
    pick something and let the learning begin.*'
  id: totrans-split-74
  prefs: []
  type: TYPE_NORMAL
  zh: '*注：与其在寻找完美的工具时“*[*分析麻痹*](https://en.wikipedia.org/wiki/Analysis_paralysis)*”，试图找到带有所有“花哨功能”的工具，不如选择一个，让学习开始吧。*'
- en: '**Our Setup Over The Years**'
  id: totrans-split-75
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**我们多年来的设置**'
- en: '**Deployments** As with many others, we use Helm to manage and streamline the
    deployment and packaging of our applications on Kubernetes. Since we started using
    Helm a long time ago and initially had a mix of .Net/Go/Java/Python/PHP, we have
    rewritten the Helm charts more times than I dare to remember.'
  id: totrans-split-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**部署** 与许多其他人一样，我们使用Helm来管理和简化我们在Kubernetes上的应用部署和打包。自从很久以前开始使用Helm以来，我们最初混合了.Net/Go/Java/Python/PHP，我们重写了Helm图表的次数多到我不敢记数。'
- en: '**Observability** We started using [Loggly](https://www.loggly.com/) together
    with [FluentD](https://www.fluentd.org/) for centralized logging, but after a
    couple of years, we moved over to Elastic and Kibana (ELK stack). It was easier
    for us to work with Elastic and Kibana since they are more widely used, and also,
    in our setup, it was cheaper.'
  id: totrans-split-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**可观测性** 我们最开始使用[Loggly](https://www.loggly.com/)与[FluentD](https://www.fluentd.org/)进行集中日志管理，但几年后，我们转向了Elastic和Kibana（ELK堆栈）。对我们来说，使用Elastic和Kibana更容易，因为它们被广泛使用，并且在我们的设置中更加经济实惠。'
- en: '**Container Registries** We started with [Quay](https://quay.io/), which was
    a good product. But with the migration to Azure, it became natural to use [Azure
    Container Registry](https://azure.microsoft.com/en-us/products/container-registry)
    instead since it was integrated and thus a more “native” solution for us. (We
    also then got our containers under the Azure Security Advisor).'
  id: totrans-split-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**容器注册表** 我们最初使用[Quay](https://quay.io/)，这是一个不错的产品。但随着迁移到Azure，自然而然地开始使用[Azure
    Container Registry](https://azure.microsoft.com/en-us/products/container-registry)，因为它集成度高，对我们而言更“本地化”。（我们也因此将容器纳入Azure安全顾问的管理）。'
- en: '**Pipelines** From the start, we have been using [Drone](https://www.drone.io/)
    for building our containers. When we first began, there weren’t many CI systems
    that supported containers and Docker, nor did they offer configurations as code.
    Drone has served us well over the years. It became a bit messy when Harness acquired
    it, but after we caved in and moved to the premium version, we had all the features
    we needed.'
  id: totrans-split-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**流水线** 从一开始，我们就使用[Drone](https://www.drone.io/)构建我们的容器。当我们开始时，没有多少CI系统支持容器和Docker，也没有提供配置作为代码的选项。多年来，Drone一直为我们提供良好的服务。在Harness收购它后变得有些混乱，但是在我们妥协并迁移到高级版本之后，我们得到了所有所需的功能。'
- en: '**Game Changer**'
  id: totrans-split-80
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**游戏改变者**'
- en: In the last few years, Kubernetes has been a game-changer for us. It has unlocked
    capabilities that enable us to **scale more efficiently (**volatile traffic volumes**),
    optimize our infrastructure costs, improved our developer experiences, made it
    easier test new ideas, and thus significantly reduce the time-to-market/time-to-money**
    for new products and services.
  id: totrans-split-81
  prefs: []
  type: TYPE_NORMAL
  zh: 在过去几年中，Kubernetes对我们来说是一个**革命性的变革者**。它解锁了能力，使我们能够更高效地**扩展（**波动的流量量**）**，优化我们的基础设施成本，改进我们的开发者体验，使测试新思路更加容易，从而显著缩短新产品和服务的上市时间/投产时间。
- en: We started with Kubernetes a bit too early, before we really had the problems
    it would solve. But in the long run, and especially in the latest years, **it
    has proven to provide great value for us.**
  id: totrans-split-82
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在早期就开始使用Kubernetes，可能比我们真正面临它可以解决的问题要早一些。但从长远来看，特别是在最近几年，**它已被证明为我们带来了巨大的价值。**
- en: '**Final Words**'
  id: totrans-split-83
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**最后的话**'
- en: Reflecting on eight years of experience, there’s an abundance of stories to
    share, with many already fading into memory. I hope you enjoyed reading about
    our setup, the mistakes we made, and the lessons we learned along the way.
  id: totrans-split-84
  prefs: []
  type: TYPE_NORMAL
  zh: 回顾八年的经验，有许多故事可以分享，很多已经在记忆中淡去。希望您喜欢了解我们的设置，我们犯过的错误，以及我们在这条路上学到的教训。
- en: Thanks for reading.
  id: totrans-split-85
  prefs: []
  type: TYPE_NORMAL
  zh: 谢谢您的阅读。
