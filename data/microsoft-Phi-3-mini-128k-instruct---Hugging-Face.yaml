- en: <!--yml
  id: totrans-split-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-split-1
  prefs: []
  type: TYPE_NORMAL
  zh: 类别：未分类
- en: 'date: 2024-05-27 13:28:05'
  id: totrans-split-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-05-27 13:28:05
- en: -->
  id: totrans-split-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: microsoft/Phi-3-mini-128k-instruct · Hugging Face
  id: totrans-split-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: microsoft/Phi-3-mini-128k-instruct · Hugging Face
- en: 来源：[https://huggingface.co/microsoft/Phi-3-mini-128k-instruct](https://huggingface.co/microsoft/Phi-3-mini-128k-instruct)
  id: totrans-split-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://huggingface.co/microsoft/Phi-3-mini-128k-instruct](https://huggingface.co/microsoft/Phi-3-mini-128k-instruct)
- en: '[](#model-summary)Model Summary'
  id: totrans-split-6
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '[](#model-summary)模型摘要'
- en: The Phi-3-Mini-128K-Instruct is a 3.8 billion-parameter, lightweight, state-of-the-art
    open model trained using the Phi-3 datasets. This dataset includes both synthetic
    data and filtered publicly available website data, with an emphasis on high-quality
    and reasoning-dense properties. The model belongs to the Phi-3 family with the
    Mini version in two variants [4K](https://huggingface.co/microsoft/Phi-3-mini-4k-instruct)
    and [128K](https://huggingface.co/microsoft/Phi-3-mini-128k-instruct) which is
    the context length (in tokens) that it can support.
  id: totrans-split-7
  prefs: []
  type: TYPE_NORMAL
  zh: Phi-3-Mini-128K-Instruct 是一个38亿参数、轻量级、最先进的开放模型，使用了Phi-3数据集进行训练。该数据集包括合成数据和经过滤的公开可用网站数据，重点是高质量和推理密集属性。该模型属于Phi-3系列，Mini版本有两个变体
    [4K](https://huggingface.co/microsoft/Phi-3-mini-4k-instruct) 和 [128K](https://huggingface.co/microsoft/Phi-3-mini-128k-instruct)，其支持的上下文长度（以令牌计）。
- en: 'After initial training, the model underwent a post-training process that involved
    supervised fine-tuning and direct preference optimization to enhance its ability
    to follow instructions and adhere to safety measures. When evaluated against benchmarks
    that test common sense, language understanding, mathematics, coding, long-term
    context, and logical reasoning, the Phi-3 Mini-128K-Instruct demonstrated robust
    and state-of-the-art performance among models with fewer than 13 billion parameters.
    Resources and Technical Documentation:'
  id: totrans-split-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在初步训练后，模型经历了一个后训练过程，其中包括监督微调和直接偏好优化，以增强其遵循指令和遵守安全措施的能力。在针对测试常识、语言理解、数学、编码、长期上下文和逻辑推理的基准进行评估时，Phi-3
    Mini-128K-Instruct 在少于130亿参数的模型中表现出了强大和最先进的性能。资源和技术文档：
- en: '[](#intended-uses)Intended Uses'
  id: totrans-split-9
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '[](#intended-uses)预期用途'
- en: '**Primary use cases**'
  id: totrans-split-10
  prefs: []
  type: TYPE_NORMAL
  zh: '**主要用例**'
- en: 'The model is intended for commercial and research use in English. The model
    provides uses for applications which require:'
  id: totrans-split-11
  prefs: []
  type: TYPE_NORMAL
  zh: 该模型旨在用于英语的商业和研究用途。该模型为需要以下应用程序提供了使用：
- en: Memory/compute constrained environments
  id: totrans-split-12
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 内存/计算受限环境
- en: Latency bound scenarios
  id: totrans-split-13
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 延迟边界场景
- en: Strong reasoning (especially code, math and logic)
  id: totrans-split-14
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 强大的推理能力（尤其是代码、数学和逻辑）
- en: Our model is designed to accelerate research on language and multimodal models,
    for use as a building block for generative AI powered features.
  id: totrans-split-15
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的模型旨在加速语言和多模型的研究，用作生成AI功能的构建模块。
- en: '**Use case considerations**'
  id: totrans-split-16
  prefs: []
  type: TYPE_NORMAL
  zh: '**用例注意事项**'
- en: Our models are not specifically designed or evaluated for all downstream purposes.
    Developers should consider common limitations of language models as they select
    use cases, and evaluate and mitigate for accuracy, safety, and fariness before
    using within a specific downstream use case, particularly for high risk scenarios.
    Developers should be aware of and adhere to applicable laws or regulations (including
    privacy, trade compliance laws, etc.) that are relevant to their use case.
  id: totrans-split-17
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的模型并非专门设计或评估所有下游用途。开发人员在选择用例时应考虑语言模型的常见限制，并在使用特定下游用例之前评估和减轻其准确性、安全性和公平性，尤其是对于高风险场景。开发人员应注意并遵守适用的法律或法规（包括隐私、贸易合规法律等），这些法律或法规与他们的用例相关。
- en: Nothing contained in this Model Card should be interpreted as or deemed a restriction
    or modification to the license the model is released under.
  id: totrans-split-18
  prefs: []
  type: TYPE_NORMAL
  zh: 本模型卡中包含的任何内容都不应被解释为或视为限制或修改模型发布的许可证。
- en: '[](#how-to-use)How to Use'
  id: totrans-split-19
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '[](#how-to-use)使用方法'
- en: 'Phi-3 Mini-128K-Instruct has been integrated in the development version (4.41.0.dev0)
    of `transformers`. Until the official version is released through `pip`, ensure
    that you are doing one of the following:'
  id: totrans-split-20
  prefs: []
  type: TYPE_NORMAL
  zh: Phi-3 Mini-128K-Instruct 已集成到 `transformers` 的开发版本（4.41.0.dev0）。在通过 `pip` 发布官方版本之前，请确保您正在执行以下操作之一：
- en: When loading the model, ensure that `trust_remote_code=True` is passed as an
    argument of the `from_pretrained()` function.
  id: totrans-split-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 加载模型时，请确保将`trust_remote_code=True`作为`from_pretrained()`函数的参数传递。
- en: 'Update your local `transformers` to the development version: `pip uninstall
    -y transformers && pip install git+https://github.com/huggingface/transformers`.
    The previous command is an alternative to cloning and installing from the source.'
  id: totrans-split-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 更新您的本地 `transformers` 到开发版本：`pip uninstall -y transformers && pip install git+https://github.com/huggingface/transformers`。前述命令是从源代码克隆和安装的替代方法。
- en: 'The current `transformers` version can be verified with: `pip list | grep transformers`.'
  id: totrans-split-23
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以通过以下命令验证当前的 `transformers` 版本：`pip list | grep transformers`。
- en: '[](#tokenizer)Tokenizer'
  id: totrans-split-24
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '[](#tokenizer)分词器'
- en: Phi-3 Mini-128K-Instruct supports a vocabulary size of up to `32064` tokens.
    The [tokenizer files](https://huggingface.co/microsoft/Phi-3-mini-128k-instruct/blob/main/added_tokens.json)
    already provide placeholder tokens that can be used for downstream fine-tuning,
    but they can also be extended up to the model's vocabulary size.
  id: totrans-split-25
  prefs: []
  type: TYPE_NORMAL
  zh: Phi-3 Mini-128K-Instruct 支持多达 `32064` 个标记的词汇量。[分词器文件](https://huggingface.co/microsoft/Phi-3-mini-128k-instruct/blob/main/added_tokens.json)已经提供了可用于下游微调的占位符标记，但它们也可以扩展到模型的词汇量。
- en: '[](#chat-format)Chat Format'
  id: totrans-split-26
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '[](#chat-format)对话格式'
- en: 'Given the nature of the training data, the Phi-3 Mini-128K-Instruct model is
    best suited for prompts using the chat format as follows. You can provide the
    prompt as a question with a generic template as follow:'
  id: totrans-split-27
  prefs: []
  type: TYPE_NORMAL
  zh: 鉴于训练数据的特性，Phi-3 Mini-128K-Instruct 模型最适合采用以下对话格式的提示。您可以将提示提供为以下通用模板的问题：
- en: '[PRE0]'
  id: totrans-split-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'For example:'
  id: totrans-split-29
  prefs: []
  type: TYPE_NORMAL
  zh: 例如：
- en: '[PRE1]'
  id: totrans-split-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'where the model generates the text after `<|assistant|>`. In case of few-shots
    prompt, the prompt can be formatted as the following:'
  id: totrans-split-31
  prefs: []
  type: TYPE_NORMAL
  zh: 在模型生成文本之后 `<|assistant|>`。在少量样本提示情况下，提示可以格式化如下：
- en: '[PRE2]'
  id: totrans-split-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '[](#sample-inference-code)Sample inference code'
  id: totrans-split-33
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '[](#sample-inference-code)示例推理代码'
- en: 'This code snippets show how to get quickly started with running the model on
    a GPU:'
  id: totrans-split-34
  prefs: []
  type: TYPE_NORMAL
  zh: 这些代码片段展示了如何快速启动在 GPU 上运行模型：
- en: '[PRE3]'
  id: totrans-split-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '*Some applications/frameworks might not include a BOS token (`<s>`) at the
    start of the conversation. Please ensure that it is included since it provides
    more reliable results.*'
  id: totrans-split-36
  prefs: []
  type: TYPE_NORMAL
  zh: '*一些应用/框架可能不包括对话开始的 BOS 标记 (`<s>`)。请确保它被包含，因为它能提供更可靠的结果。*'
- en: '[](#responsible-ai-considerations)Responsible AI Considerations'
  id: totrans-split-37
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '[](#responsible-ai-considerations)负责任的 AI 考虑'
- en: 'Like other language models, the Phi series models can potentially behave in
    ways that are unfair, unreliable, or offensive. Some of the limiting behaviors
    to be aware of include:'
  id: totrans-split-38
  prefs: []
  type: TYPE_NORMAL
  zh: 与其他语言模型一样，Phi 系列模型可能表现出不公平、不可靠或令人反感的方式。一些需注意的限制行为包括：
- en: 'Quality of Service: the Phi models are trained primarily on English text. Languages
    other than English will experience worse performance. English language varieties
    with less representation in the training data might experience worse performance
    than standard American English.'
  id: totrans-split-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 服务质量：Phi 系列模型主要是在英文文本上进行训练的。除英语外的其他语言将会表现得更差。训练数据中较少代表性的英语语言变体可能比标准美式英语表现更差。
- en: 'Representation of Harms & Perpetuation of Stereotypes: These models can over-
    or under-represent groups of people, erase representation of some groups, or reinforce
    demeaning or negative stereotypes. Despite safety post-training, these limitations
    may still be present due to differing levels of representation of different groups
    or prevalence of examples of negative stereotypes in training data that reflect
    real-world patterns and societal biases.'
  id: totrans-split-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于伤害的表达与刻板印象的延续：这些模型可能会过度或不足地代表某些群体，擦除某些群体的代表性，或者加强贬低或负面刻板印象。尽管安全后训练，由于不同群体的代表性水平或负面刻板印象的例子在训练数据中的普遍性和社会偏见的反映，这些限制可能仍然存在。
- en: 'Inappropriate or Offensive Content: these models may produce other types of
    inappropriate or offensive content, which may make it inappropriate to deploy
    for sensitive contexts without additional mitigations that are specific to the
    use case.'
  id: totrans-split-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 不恰当或冒犯性内容：这些模型可能生成其他类型的不恰当或冒犯性内容，这可能使其在没有针对具体使用情境的额外缓解措施的情况下不适合部署于敏感环境中。
- en: 'Information Reliability: Language models can generate nonsensical content or
    fabricate content that might sound reasonable but is inaccurate or outdated.'
  id: totrans-split-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 信息可靠性：语言模型可以生成荒谬的内容或捏造可能听起来合理但不准确或过时的内容。
- en: 'Limited Scope for Code: Majority of Phi-3 training data is based in Python
    and use common packages such as "typing, math, random, collections, datetime,
    itertools". If the model generates Python scripts that utilize other packages
    or scripts in other languages, we strongly recommend users manually verify all
    API uses.'
  id: totrans-split-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 代码的使用范围有限：大部分Phi-3训练数据基于Python，并使用常见的包，例如"typing, math, random, collections,
    datetime, itertools"。如果模型生成利用其他包或其他语言的Python脚本，我们强烈建议用户手动验证所有API使用情况。
- en: 'Developers should apply responsible AI best practices and are responsible for
    ensuring that a specific use case complies with relevant laws and regulations
    (e.g. privacy, trade, etc.). Important areas for consideration include:'
  id: totrans-split-44
  prefs: []
  type: TYPE_NORMAL
  zh: 开发者应当应用负责任的人工智能最佳实践，并确保特定用例符合相关法律和法规（例如隐私、贸易等）。需要重点考虑的领域包括：
- en: 'Allocation: Models may not be suitable for scenarios that could have consequential
    impact on legal status or the allocation of resources or life opportunities (ex:
    housing, employment, credit, etc.) without further assessments and additional
    debiasing techniques.'
  id: totrans-split-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分配：模型可能不适用于可能对法律地位或资源分配或生活机会分配（例如住房、就业、信贷等）产生重大影响的场景，除非进一步评估和额外去偏见技术。
- en: 'High-Risk Scenarios: Developers should assess suitability of using models in
    high-risk scenarios where unfair, unreliable or offensive outputs might be extremely
    costly or lead to harm. This includes providing advice in sensitive or expert
    domains where accuracy and reliability are critical (ex: legal or health advice).
    Additional safeguards should be implemented at the application level according
    to the deployment context.'
  id: totrans-split-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 高风险场景：开发者应评估在高风险场景中使用模型的适宜性，其中不公平、不可靠或冒犯性输出可能会造成极大的成本或导致危害。这包括在敏感或专业领域提供建议，其中准确性和可靠性至关重要（例如法律或健康建议）。根据部署上下文，应在应用程序级别实施额外的安全保障措施。
- en: 'Misinformation: Models may produce inaccurate information. Developers should
    follow transparency best practices and inform end-users they are interacting with
    an AI system. At the application level, developers can build feedback mechanisms
    and pipelines to ground responses in use-case specific, contextual information,
    a technique known as Retrieval Augmented Generation (RAG).'
  id: totrans-split-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 误信息：模型可能生成不准确的信息。开发者应遵循透明度最佳实践，并告知最终用户他们正在与AI系统交互。在应用程序级别，开发者可以构建反馈机制和管道，将响应根据特定用例的上下文信息实时化，这是一种称为检索增强生成（RAG）的技术。
- en: 'Generation of Harmful Content: Developers should assess outputs for their context
    and use available safety classifiers or custom solutions appropriate for their
    use case.'
  id: totrans-split-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 生成有害内容：开发者应评估输出的上下文，并使用适合其用例的安全分类器或自定义解决方案。
- en: 'Misuse: Other forms of misuse such as fraud, spam, or malware production may
    be possible, and developers should ensure that their applications do not violate
    applicable laws and regulations.'
  id: totrans-split-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 滥用：可能存在其他形式的滥用，如欺诈、垃圾邮件或恶意软件生产，并且开发者应确保其应用程序不违反适用的法律和法规。
- en: '[](#training)Training'
  id: totrans-split-50
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '[](#training)训练'
- en: '[](#model)Model'
  id: totrans-split-51
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '[](#model)模型'
- en: 'Architecture: Phi-3 Mini-128K-Instruct has 3.8B parameters and is a dense decoder-only
    Transformer model. The model is fine-tuned with Supervised fine-tuning (SFT) and
    Direct Preference Optimization (DPO) to ensure alignment with human preferences
    and safety guidlines.'
  id: totrans-split-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 架构：Phi-3 Mini-128K-Instruct有38亿参数，是一个密集的仅解码Transformer模型。模型经过监督微调（SFT）和直接偏好优化（DPO），以确保与人类偏好和安全准则的一致性。
- en: 'Inputs: Text. It is best suited for prompts using chat format.'
  id: totrans-split-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 输入：文本。最适合使用聊天格式进行提示。
- en: 'Context length: 128K tokens'
  id: totrans-split-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 上下文长度：128K标记
- en: 'GPUs: 512 H100-80G'
  id: totrans-split-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: GPU：512个H100-80G
- en: 'Training time: 7 days'
  id: totrans-split-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练时间：7天
- en: 'Training data: 3.3T tokens'
  id: totrans-split-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练数据：3.3万亿标记
- en: 'Outputs: Generated text in response to the input'
  id: totrans-split-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 输出：对输入生成的文本
- en: 'Dates: Our models were trained between February and April 2024'
  id: totrans-split-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 日期：我们的模型在2024年2月至4月之间进行了训练
- en: 'Status: This is a static model trained on an offline dataset with cutoff date
    October 2023\. Future versions of the tuned models may be released as we improve
    models.'
  id: totrans-split-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 状态：这是一个静态模型，使用截至2023年10月的离线数据集进行训练。随着模型的改进，可能会发布调优版本的未来模型。
- en: '[](#datasets)Datasets'
  id: totrans-split-61
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '[](#datasets)数据集'
- en: Our training data includes a wide variety of sources, totaling 3.3 trillion
    tokens, and is a combination of
  id: totrans-split-62
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的训练数据包括多种来源，总计3.3万亿标记，是高质量教育数据和代码的结合；
- en: Publicly available documents filtered rigorously for quality, selected high-quality
    educational data, and code;
  id: totrans-split-63
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 经过严格过滤的公开文档，选择高质量的教育数据和代码；
- en: Newly created synthetic, “textbook-like” data for the purpose of teaching math,
    coding, common sense reasoning, general knowledge of the world (science, daily
    activities, theory of mind, etc.);
  id: totrans-split-64
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为了教授数学、编程、常识推理、世界通识（科学、日常活动、心理理论等），新创建了类似“教科书”的合成数据；
- en: High quality chat format supervised data covering various topics to reflect
    human preferences on different aspects such as instruct-following, truthfulness,
    honesty and helpfulness.
  id: totrans-split-65
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 高质量聊天格式的监督数据，涵盖各种主题，反映人类在不同方面（如指示-跟随、真实性、诚实和乐于助人）上的偏好。
- en: '[](#fine-tuning)Fine-tuning'
  id: totrans-split-66
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '[](#fine-tuning)微调'
- en: A basic example of multi-GPUs supervised fine-tuning (SFT) with TRL and Accelerate
    modules is provided [here](https://huggingface.co/microsoft/Phi-3-mini-128k-instruct/resolve/main/sample_finetune.py).
  id: totrans-split-67
  prefs: []
  type: TYPE_NORMAL
  zh: 提供了关于多GPU监督微调（SFT）的基本示例，包括TRL和Accelerate模块，可在[这里](https://huggingface.co/microsoft/Phi-3-mini-128k-instruct/resolve/main/sample_finetune.py)找到。
- en: '[](#benchmarks)Benchmarks'
  id: totrans-split-68
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '[](#benchmarks)基准'
- en: We report the results for Phi-3-Mini-128K-Instruct on standard open-source benchmarks
    measuring the model's reasoning ability (both common sense reasoning and logical
    reasoning). We compare to Phi-2, Mistral-7b-v0.1, Mixtral-8x7b, Gemma 7B, Llama-3-8B-Instruct,
    and GPT-3.5.
  id: totrans-split-69
  prefs: []
  type: TYPE_NORMAL
  zh: 我们报告Phi-3-Mini-128K-Instruct在衡量模型推理能力（常识推理和逻辑推理）的标准开源基准上的结果。我们与Phi-2、Mistral-7b-v0.1、Mixtral-8x7b、Gemma
    7B、Llama-3-8B-Instruct和GPT-3.5进行比较。
- en: All the reported numbers are produced with the exact same pipeline to ensure
    that the numbers are comparable. These numbers might differ from other published
    numbers due to slightly different choices in the evaluation.
  id: totrans-split-70
  prefs: []
  type: TYPE_NORMAL
  zh: 所有报告的数字都是使用完全相同的流程生成的，以确保数字是可比较的。由于在评估中选择略有不同，这些数字可能与其他发布的数字有所不同。
- en: As is now standard, we use few-shot prompts to evaluate the models, at temperature
    0\. The prompts and number of shots are part of a Microsoft internal tool to evaluate
    language models, and in particular we did no optimization to the pipeline for
    Phi-3. More specifically, we do not change prompts, pick different few-shot examples,
    change prompt format, or do any other form of optimization for the model.
  id: totrans-split-71
  prefs: []
  type: TYPE_NORMAL
  zh: 正如现在的标准，我们使用少量示例来评估模型，在温度为0。提示和射击次数是微软内部工具的一部分，用于评估语言模型，特别是我们对Phi-3没有进行管道优化。更具体地说，我们不改变提示，选择不同的少量示例，改变提示格式，或进行任何其他形式的模型优化。
- en: The number of k–shot examples is listed per-benchmark.
  id: totrans-split-72
  prefs: []
  type: TYPE_NORMAL
  zh: 每个基准的k-shot示例数量列在括号内。
- en: '|  | Phi-3-Mini-128K-In 3.8b | Phi-3-Small 7b (preview) | Phi-3-Medium 14b
    (preview) | Phi-2 2.7b | Mistral 7b | Gemma 7b | Llama-3-In 8b | Mixtral 8x7b
    | GPT-3.5 version 1106 |'
  id: totrans-split-73
  prefs: []
  type: TYPE_TB
  zh: '|  | Phi-3-Mini-128K-In 3.8b | Phi-3-Small 7b（预览）| Phi-3-Medium 14b（预览）| Phi-2
    2.7b | Mistral 7b | Gemma 7b | Llama-3-In 8b | Mixtral 8x7b | GPT-3.5版本1106 |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-split-74
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
- en: '| MMLU 5-Shot | 68.1 | 75.3 | 78.2 | 56.3 | 61.7 | 63.6 | 66.5 | 68.4 | 71.4
    |'
  id: totrans-split-75
  prefs: []
  type: TYPE_TB
  zh: '| MMLU 5-Shot | 68.1 | 75.3 | 78.2 | 56.3 | 61.7 | 63.6 | 66.5 | 68.4 | 71.4
    |'
- en: '| HellaSwag 5-Shot | 74.5 | 78.7 | 83.2 | 53.6 | 58.5 | 49.8 | 71.1 | 70.4
    | 78.8 |'
  id: totrans-split-76
  prefs: []
  type: TYPE_TB
  zh: '| HellaSwag 5-Shot | 74.5 | 78.7 | 83.2 | 53.6 | 58.5 | 49.8 | 71.1 | 70.4
    | 78.8 |'
- en: '| ANLI 7-Shot | 52.8 | 55.0 | 58.7 | 42.5 | 47.1 | 48.7 | 57.3 | 55.2 | 58.1
    |'
  id: totrans-split-77
  prefs: []
  type: TYPE_TB
  zh: '| ANLI 7-Shot | 52.8 | 55.0 | 58.7 | 42.5 | 47.1 | 48.7 | 57.3 | 55.2 | 58.1
    |'
- en: '| GSM-8K 0-Shot; CoT | 83.6 | 86.4 | 90.8 | 61.1 | 46.4 | 59.8 | 77.4 | 64.7
    | 78.1 |'
  id: totrans-split-78
  prefs: []
  type: TYPE_TB
  zh: '| GSM-8K 0-Shot; CoT | 83.6 | 86.4 | 90.8 | 61.1 | 46.4 | 59.8 | 77.4 | 64.7
    | 78.1 |'
- en: '| MedQA 2-Shot | 55.3 | 58.2 | 69.8 | 40.9 | 49.6 | 50.0 | 60.5 | 62.2 | 63.4
    |'
  id: totrans-split-79
  prefs: []
  type: TYPE_TB
  zh: '| MedQA 2-Shot | 55.3 | 58.2 | 69.8 | 40.9 | 49.6 | 50.0 | 60.5 | 62.2 | 63.4
    |'
- en: '| AGIEval 0-Shot | 36.9 | 45.0 | 49.7 | 29.8 | 35.1 | 42.1 | 42.0 | 45.2 |
    48.4 |'
  id: totrans-split-80
  prefs: []
  type: TYPE_TB
  zh: '| AGIEval 0-Shot | 36.9 | 45.0 | 49.7 | 29.8 | 35.1 | 42.1 | 42.0 | 45.2 |
    48.4 |'
- en: '| TriviaQA 5-Shot | 57.1 | 59.1 | 73.3 | 45.2 | 72.3 | 75.2 | 67.7 | 82.2 |
    85.8 |'
  id: totrans-split-81
  prefs: []
  type: TYPE_TB
  zh: '| TriviaQA 5-Shot | 57.1 | 59.1 | 73.3 | 45.2 | 72.3 | 75.2 | 67.7 | 82.2 |
    85.8 |'
- en: '| Arc-C 10-Shot | 84.0 | 90.7 | 91.9 | 75.9 | 78.6 | 78.3 | 82.8 | 87.3 | 87.4
    |'
  id: totrans-split-82
  prefs: []
  type: TYPE_TB
  zh: '| Arc-C 10-Shot | 84.0 | 90.7 | 91.9 | 75.9 | 78.6 | 78.3 | 82.8 | 87.3 | 87.4
    |'
- en: '| Arc-E 10-Shot | 95.2 | 97.1 | 98.0 | 88.5 | 90.6 | 91.4 | 93.4 | 95.6 | 96.3
    |'
  id: totrans-split-83
  prefs: []
  type: TYPE_TB
  zh: '| Arc-E 10-Shot | 95.2 | 97.1 | 98.0 | 88.5 | 90.6 | 91.4 | 93.4 | 95.6 | 96.3
    |'
- en: '| PIQA 5-Shot | 83.6 | 87.8 | 88.2 | 60.2 | 77.7 | 78.1 | 75.7 | 86.0 | 86.6
    |'
  id: totrans-split-84
  prefs: []
  type: TYPE_TB
  zh: '| PIQA 5-Shot | 83.6 | 87.8 | 88.2 | 60.2 | 77.7 | 78.1 | 75.7 | 86.0 | 86.6
    |'
- en: '| SociQA 5-Shot | 76.1 | 79.0 | 79.4 | 68.3 | 74.6 | 65.5 | 73.9 | 75.9 | 68.3
    |'
  id: totrans-split-85
  prefs: []
  type: TYPE_TB
  zh: '| SociQA 5-Shot | 76.1 | 79.0 | 79.4 | 68.3 | 74.6 | 65.5 | 73.9 | 75.9 | 68.3
    |'
- en: '| BigBench-Hard 0-Shot | 71.5 | 75.0 | 82.5 | 59.4 | 57.3 | 59.6 | 51.5 | 69.7
    | 68.32 |'
  id: totrans-split-86
  prefs: []
  type: TYPE_TB
  zh: '| BigBench-Hard 0-Shot | 71.5 | 75.0 | 82.5 | 59.4 | 57.3 | 59.6 | 51.5 | 69.7
    | 68.32 |'
- en: '| WinoGrande 5-Shot | 72.5 | 82.5 | 81.2 | 54.7 | 54.2 | 55.6 | 65.0 | 62.0
    | 68.8 |'
  id: totrans-split-87
  prefs: []
  type: TYPE_TB
  zh: '| WinoGrande 5-Shot | 72.5 | 82.5 | 81.2 | 54.7 | 54.2 | 55.6 | 65.0 | 62.0
    | 68.8 |'
- en: '| OpenBookQA 10-Shot | 80.6 | 88.4 | 86.6 | 73.6 | 79.8 | 78.6 | 82.6 | 85.8
    | 86.0 |'
  id: totrans-split-88
  prefs: []
  type: TYPE_TB
  zh: '| OpenBookQA 10-Shot | 80.6 | 88.4 | 86.6 | 73.6 | 79.8 | 78.6 | 82.6 | 85.8
    | 86.0 |'
- en: '| BoolQ 0-Shot | 78.7 | 82.9 | 86.5 | -- | 72.2 | 66.0 | 80.9 | 77.6 | 79.1
    |'
  id: totrans-split-89
  prefs: []
  type: TYPE_TB
  zh: '| BoolQ 0-Shot | 78.7 | 82.9 | 86.5 | -- | 72.2 | 66.0 | 80.9 | 77.6 | 79.1
    |'
- en: '| CommonSenseQA 10-Shot | 78.0 | 80.3 | 82.6 | 69.3 | 72.6 | 76.2 | 79 | 78.1
    | 79.6 |'
  id: totrans-split-90
  prefs: []
  type: TYPE_TB
  zh: '| CommonSenseQA 10-Shot | 78.0 | 80.3 | 82.6 | 69.3 | 72.6 | 76.2 | 79 | 78.1
    | 79.6 |'
- en: '| TruthfulQA 10-Shot | 63.2 | 68.1 | 74.8 | -- | 52.1 | 53.0 | 63.2 | 60.1
    | 85.8 |'
  id: totrans-split-91
  prefs: []
  type: TYPE_TB
  zh: '| TruthfulQA 10-Shot | 63.2 | 68.1 | 74.8 | -- | 52.1 | 53.0 | 63.2 | 60.1
    | 85.8 |'
- en: '| HumanEval 0-Shot | 57.9 | 59.1 | 54.7 | 47.0 | 28.0 | 34.1 | 60.4 | 37.8
    | 62.2 |'
  id: totrans-split-92
  prefs: []
  type: TYPE_TB
  zh: '| HumanEval 0-Shot | 57.9 | 59.1 | 54.7 | 47.0 | 28.0 | 34.1 | 60.4 | 37.8
    | 62.2 |'
- en: '| MBPP 3-Shot | 62.5 | 71.4 | 73.7 | 60.6 | 50.8 | 51.5 | 67.7 | 60.2 | 77.8
    |'
  id: totrans-split-93
  prefs: []
  type: TYPE_TB
  zh: '| MBPP 3-Shot | 62.5 | 71.4 | 73.7 | 60.6 | 50.8 | 51.5 | 67.7 | 60.2 | 77.8
    |'
- en: '[](#software)Software'
  id: totrans-split-94
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '[](#software)软件'
- en: '[](#hardware)Hardware'
  id: totrans-split-95
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '[](#hardware)硬件'
- en: 'Note that by default, the Phi-3-mini model uses flash attention, which requires
    certain types of GPU hardware to run. We have tested on the following GPU types:'
  id: totrans-split-96
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，Phi-3-mini 模型使用闪存注意力，需要特定类型的 GPU 硬件才能运行。我们已在以下 GPU 类型上进行了测试：
- en: NVIDIA A100
  id: totrans-split-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: NVIDIA A100
- en: NVIDIA A6000
  id: totrans-split-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: NVIDIA A6000
- en: NVIDIA H100
  id: totrans-split-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: NVIDIA H100
- en: 'If you want to run the model on:'
  id: totrans-split-100
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您想在以下设备上运行该模型：
- en: 'NVIDIA V100 or earlier generation GPUs: call AutoModelForCausalLM.from_pretrained()
    with attn_implementation="eager"'
  id: totrans-split-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: NVIDIA V100 或早期的 GPU：调用 AutoModelForCausalLM.from_pretrained() 并设置 attn_implementation="eager"
- en: 'Optimized inference on GPU, CPU, and Mobile: use the **ONNX** models [128K](https://aka.ms/phi3-mini-128k-instruct-onnx)'
  id: totrans-split-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: GPU、CPU 和移动设备的优化推断：使用 **ONNX** 模型 [128K](https://aka.ms/phi3-mini-128k-instruct-onnx)
- en: '[](#cross-platform-support)Cross Platform Support'
  id: totrans-split-103
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '[](#cross-platform-support)跨平台支持'
- en: ONNX runtime ecosystem now supports Phi-3 Mini models across platforms and hardware.
    You can find the optimized Phi-3 Mini-128K-Instruct ONNX model [here](https://aka.ms/phi3-mini-128k-instruct-onnx).
  id: totrans-split-104
  prefs: []
  type: TYPE_NORMAL
  zh: ONNX 运行时生态系统现在跨平台和硬件支持 Phi-3 Mini 模型。你可以在[这里](https://aka.ms/phi3-mini-128k-instruct-onnx)找到优化的
    Phi-3 Mini-128K-Instruct ONNX 模型。
- en: Optimized Phi-3 models are also published here in ONNX format, to run with ONNX
    Runtime on CPU and GPU across devices, including server platforms, Windows, Linux
    and Mac desktops, and mobile CPUs, with the precision best suited to each of these
    targets. DirectML support lets developers bring hardware acceleration to Windows
    devices at scale across AMD, Intel, and NVIDIA GPUs.
  id: totrans-split-105
  prefs: []
  type: TYPE_NORMAL
  zh: 优化的 Phi-3 模型也以 ONNX 格式发布，可在 CPU 和 GPU 上使用 ONNX Runtime 在各种设备上运行，包括服务器平台、Windows、Linux
    和 Mac 桌面以及移动 CPU，适应每个目标的最佳精度。DirectML 支持让开发人员在 AMD、Intel 和 NVIDIA GPU 上为 Windows
    设备带来规模化的硬件加速。
- en: Along with DirectML, ONNX Runtime provides cross platform support for Phi-3
    across a range of devices CPU, GPU, and mobile.
  id: totrans-split-106
  prefs: []
  type: TYPE_NORMAL
  zh: 除了 DirectML，ONNX Runtime 还为 Phi-3 在一系列设备上提供跨平台支持，包括 CPU、GPU 和移动设备。
- en: 'Here are some of the optimized configurations we have added:'
  id: totrans-split-107
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是我们添加的一些优化配置：
- en: 'ONNX models for int4 DML: Quantized to int4 via AWQ'
  id: totrans-split-108
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: int4 DML 的 ONNX 模型：通过 AWQ 进行整数 4 位量化
- en: ONNX model for fp16 CUDA
  id: totrans-split-109
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: fp16 CUDA 的 ONNX 模型
- en: 'ONNX model for int4 CUDA: Quantized to int4 via RTN'
  id: totrans-split-110
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: int4 CUDA 的 ONNX 模型：通过 RTN 进行整数 4 位量化
- en: 'ONNX model for int4 CPU and Mobile: Quantized to int4 via RTN'
  id: totrans-split-111
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: int4 CPU 和移动设备的 ONNX 模型：通过 RTN 进行整数 4 位量化
- en: '[](#license)License'
  id: totrans-split-112
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '[](#license)许可证'
- en: The model is licensed under the [MIT license](https://huggingface.co/microsoft/Phi-3-mini-128k/resolve/main/LICENSE).
  id: totrans-split-113
  prefs: []
  type: TYPE_NORMAL
  zh: 该模型在 [MIT 许可证](https://huggingface.co/microsoft/Phi-3-mini-128k/resolve/main/LICENSE)下许可。
- en: '[](#trademarks)Trademarks'
  id: totrans-split-114
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '[](#trademarks)商标'
- en: This project may contain trademarks or logos for projects, products, or services.
    Authorized use of Microsoft trademarks or logos is subject to and must follow [Microsoft’s
    Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks).
    Use of Microsoft trademarks or logos in modified versions of this project must
    not cause confusion or imply Microsoft sponsorship. Any use of third-party trademarks
    or logos are subject to those third-party’s policies.
  id: totrans-split-115
  prefs: []
  type: TYPE_NORMAL
  zh: 本项目可能包含项目、产品或服务的商标或标识。使用 Microsoft 商标或标识必须遵守并遵循 [Microsoft 商标和品牌指南](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks)。在此项目的修改版本中使用
    Microsoft 商标或标识不得引起混淆或暗示 Microsoft 赞助。任何第三方商标或标识的使用需遵守该第三方的政策。
