- en: <!--yml
  id: totrans-split-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-split-1
  prefs: []
  type: TYPE_NORMAL
  zh: 类别：未分类
- en: 'date: 2024-05-27 14:35:08'
  id: totrans-split-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-05-27 14:35:08
- en: -->
  id: totrans-split-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: How do transformers work?+Design a Multi-class Sentiment Analysis for Customer
    Reviews
  id: totrans-split-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: transformers如何工作？+设计一个多类情感分析用于客户评论
- en: 来源：[https://nintyzeros.substack.com/p/how-do-transformer-workdesign-a-multi](https://nintyzeros.substack.com/p/how-do-transformer-workdesign-a-multi)
  id: totrans-split-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://nintyzeros.substack.com/p/how-do-transformer-workdesign-a-multi](https://nintyzeros.substack.com/p/how-do-transformer-workdesign-a-multi)
- en: '*👋 Hi, this is [](https://twitter.com/gergelyorosz) Venkat and here with a
    free, full issue of the The ZenMode Engineer Newsletter. In every issue, I cover
    one topic explained in a simpler terms in areas related to computer technologies
    and beyond.*'
  id: totrans-split-6
  prefs: []
  type: TYPE_NORMAL
  zh: '*👋 嗨，这里是[](https://twitter.com/gergelyorosz) Venkat，并且在这里免费提供《ZenMode工程师通讯》的完整内容。在每一期中，我涵盖一个关于计算机技术及其它领域的主题，用更简单的术语解释。*'
- en: '* * *'
  id: totrans-split-7
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Transformers have become synonymous with cutting-edge AI, particularly in the
    realm of natural language processing (NLP).
  id: totrans-split-8
  prefs: []
  type: TYPE_NORMAL
  zh: Transformers已经成为尤其在自然语言处理（NLP）领域的前沿AI的代名词。
- en: But what exactly makes them tick? How do these models navigate the intricacies
    of language with such remarkable efficiency and accuracy?
  id: totrans-split-9
  prefs: []
  type: TYPE_NORMAL
  zh: 但究竟是什么让它们如此高效准确地工作？这些模型如何在语言的复杂性中航行？
- en: Buckle up, because we're about to learn the heart of the transformer architecture.
  id: totrans-split-10
  prefs: []
  type: TYPE_NORMAL
  zh: 系好安全带，因为我们即将了解transformer架构的核心。
- en: But.. Before we deep dive into it lets understand where its been used.. if you
    have used google translate/ ChatGPT both rely on these.
  id: totrans-split-11
  prefs: []
  type: TYPE_NORMAL
  zh: 但是…… 在我们深入探讨之前，让我们了解它被使用在哪里…… 如果你使用过Google翻译/ChatGPT，它们都依赖于这些。
- en: '***Google Translate:** This widely used platform relies heavily on transformers
    to achieve fast and accurate translations across over 100 languages. It considers
    the entire sentence context, not just individual words, leading to more natural-sounding
    translations.*'
  id: totrans-split-12
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '***谷歌翻译：** 这一广泛使用的平台依赖于transformers，实现快速且准确的跨100多种语言的翻译。它考虑整个句子的语境，而不仅仅是单个单词，从而产生更自然的翻译。*'
- en: '***Netflix Recommendation System:** Ever wondered how Netflix suggests shows
    and movies you might enjoy? Transformers analyze your viewing history and other
    users'' data to identify patterns and connections, ultimately recommending content
    tailored to your preferences.*'
  id: totrans-split-13
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '***Netflix推荐系统：** 你是否想知道Netflix是如何推荐你可能喜欢的节目和电影的？Transformers分析你的观看历史和其他用户的数据，识别模式和连接，最终推荐根据你偏好量身定制的内容。*'
- en: '**The Big Picture: Encoder and Decoder Dance**'
  id: totrans-split-14
  prefs: []
  type: TYPE_NORMAL
  zh: '**整体情况：编码器和解码器的舞蹈**'
- en: 'Imagine a factory, but instead of assembling physical objects, it processes
    language. This factory has two main departments:'
  id: totrans-split-15
  prefs: []
  type: TYPE_NORMAL
  zh: 想象一个工厂，但不是用来组装物理物体的，而是处理语言。这个工厂有两个主要部门：
- en: '**The Encoder:** This is the information extractor, meticulously dissecting
    the input text, understanding its individual elements, and uncovering the hidden
    connections between them.'
  id: totrans-split-16
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**编码器：** 这是信息提取器，精细解剖输入文本，理解其各个元素，并揭示它们之间隐藏的连接。'
- en: '**The Decoder:** Armed with the encoder''s insights, the decoder crafts the
    desired output, be it a translated sentence, a concise summary, or even a brand
    new poem.'
  id: totrans-split-17
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**解码器：** 在编码器的洞察力指导下，解码器制定所需的输出，无论是翻译句子、简洁摘要，甚至是全新的诗歌。'
- en: '**Encoder: Decoding the Input Labyrinth**'
  id: totrans-split-18
  prefs: []
  type: TYPE_NORMAL
  zh: '**编码器：解码输入迷宫**'
- en: The encoder's journey begins with **Input Embedding**, where each word is transformed
    from its textual form into a numerical representation (vector). Think of it as
    assigning each word a unique identifier.
  id: totrans-split-19
  prefs: []
  type: TYPE_NORMAL
  zh: 编码器的旅程始于**输入嵌入**，在这里，每个单词从其文本形式转换为数值表示（向量）。可以将其看作是为每个单词分配一个唯一标识符。
- en: '* * *'
  id: totrans-split-20
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Consider this example.
  id: totrans-split-21
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑这个例子。
- en: '**Input Text:** The process begins with the raw text sentence, such as "The
    cat sat on the mat."'
  id: totrans-split-22
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**输入文本：** 这个过程从原始文本句子开始，比如“猫坐在垫子上。”'
- en: '**Input Embedding Layer:**'
  id: totrans-split-23
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**输入嵌入层：**'
- en: This layer acts as a translator, converting each word into a numerical vector.
  id: totrans-split-24
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这一层充当翻译器的角色，将每个单词转换为数值向量。
- en: Imagine a large dictionary where each word has a corresponding vector address.
  id: totrans-split-25
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 想象一个大型词典，其中每个单词都有对应的向量地址。
- en: 'These vectors capture various aspects of word meaning:'
  id: totrans-split-26
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这些向量捕捉单词含义的各种方面：
- en: Semantic relationships (e.g., "cat" is closer to "pet" than "chair").
  id: totrans-split-27
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 语义关系（例如，“cat”更接近“pet”而不是“chair”）。
- en: Syntactic roles (e.g., "cat" is often a noun, while "sat" is a verb).
  id: totrans-split-28
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 句法角色（例如，“cat”通常是名词，而“sat”是动词）。
- en: Context within the sentence (e.g., "mat" here likely refers to a floor mat).
  id: totrans-split-29
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 句子内上下文（例如，“mat”这里可能指地毯）。
- en: '**Vector Representation:**'
  id: totrans-split-30
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**向量表示：**'
- en: '* * *'
  id: totrans-split-31
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '* * *'
- en: 'But the encoder doesn''t stop there. It employs the following key mechanisms
    to delve deeper:'
  id: totrans-split-32
  prefs: []
  type: TYPE_NORMAL
  zh: 但编码器并不止于此。它采用以下关键机制深入探索：
- en: '**Self-Attention Layer:** This is the game-changer. Imagine shining a spotlight
    on each word, but instead of illuminating it in isolation, you also highlight
    how it connects to all other words in the sentence. This allows the encoder to
    grasp the context, nuances, and relationships within the text, not just the individual
    words.'
  id: totrans-split-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**自注意力层：** 这是改变游戏规则的地方。想象一下，对每个词都投射出聚光灯，但不仅照亮它本身，还突显它与句子中所有其他词的连接方式。这使得编码器能够理解文本中的上下文、细微差别和关系，而不仅仅是个别词语。'
- en: ref from Raimi Karim blog (used only to refernce)
  id: totrans-split-34
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 引用自 Raimi Karim 博客（仅供参考）
- en: '* * *'
  id: totrans-split-35
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '* * *'
- en: Consider this example sentence again "***The quick brown fox jumps over the
    lazy dog.***"
  id: totrans-split-36
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 再次考虑这个例句：“***The quick brown fox jumps over the lazy dog.***”
- en: '**Word Embeddings:** First, each word is transformed into a numerical representation
    called a "word embedding." Think of it as assigning each word a unique identifier
    in a giant vocabulary map.'
  id: totrans-split-37
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**词嵌入：** 首先，每个词被转换为一个称为“词嵌入”的数值表示。可以将其视为在一个巨大的词汇映射中为每个词分配一个唯一的标识符。'
- en: '**Query, Key, Value:** Next, the Self-Attention mechanism creates three special
    vectors for each word:'
  id: totrans-split-38
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**查询、键、值：** 接下来，自注意力机制为每个词创建三个特殊向量：'
- en: '**Query (Q):** This vector asks "What information do I need from other words?"'
  id: totrans-split-39
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**查询（Q）：** 这个向量询问“我需要从其他词语中获取什么信息？”'
- en: '**Key (K):** This vector acts like a label, saying "This is the information
    I have to offer."'
  id: totrans-split-40
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**键（K）：** 这个向量充当标签，表示“这是我要提供的信息”。'
- en: '**Value (V):** This vector holds the actual information, like the word''s meaning
    and context.'
  id: totrans-split-41
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**值（V）：** 这个向量保存实际信息，比如词的含义和上下文。'
- en: '**Attention Scores:** Now comes the interesting part. The Self-Attention layer
    compares the Query vector of each word with the Key vectors of all other words
    in the sentence.'
  id: totrans-split-42
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**注意力分数：** 现在来到有趣的部分。自注意力层将每个词的查询向量与句子中所有其他词的键向量进行比较。'
- en: This helps it understand how relevant each word is to the current word. Based
    on this comparison, it calculates an **attention score** for each pair of words.
  id: totrans-split-43
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这有助于理解每个词与当前词的相关性。基于这种比较，它为每对词计算一个**注意力分数**。
- en: Imagine shining a spotlight on each word. The brighter the spotlight on another
    word, the higher the attention score, meaning the more relevant that word is to
    the current word.
  id: totrans-split-44
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 想象一下，对每个词都投射出聚光灯。其他词上的聚光灯越亮，注意力分数越高，意味着该词对当前词的相关性越高。
- en: '**Weighted Values:** Finally, the Self-Attention layer uses the attention scores
    to weigh the Value vectors of all other words. Words with higher attention scores
    get more weight, contributing more to the final representation of the current
    word.'
  id: totrans-split-45
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**加权值：** 最后，自注意力层使用注意力分数来加权所有其他词的值向量。注意力分数较高的词获得更大的权重，对当前词的最终表示贡献更大。'
- en: Think of it like taking a weighted average of the information from other words,
    where the weights are determined by how relevant they are.
  id: totrans-split-46
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 可以将其视为从其他词语获取信息的加权平均值，其中权重由它们的相关性决定。
- en: '**New Word Representation:** By considering the context provided by other words,
    the Self-Attention layer creates a new, enriched representation of each word.
    This new representation captures not just the word''s own meaning, but also how
    it relates to and is influenced by other words in the sentence.'
  id: totrans-split-47
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**新词表示：** 通过考虑其他词提供的上下文，自注意力层为每个词创建了一个新的、丰富的表示。这种新的表示不仅捕捉了词语本身的含义，还捕捉了它与句子中其他词语的关系及其受影响的方式。'
- en: '* * *'
  id: totrans-split-48
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: '* * *'
- en: '**Multi-Head Attention:** This is like having multiple teams of analysts, each
    focusing on different aspects of the connections between words. It allows the
    encoder to capture various facets of the relationships, enriching its understanding.'
  id: totrans-split-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**多头注意力：** 这就像有多个分析团队，每个团队关注词语之间连接的不同方面。它使编码器能够捕捉关系的多个方面，丰富其理解。'
- en: '* * *'
  id: totrans-split-50
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '* * *'
- en: '**Sentence:** "The quick brown fox jumps over the lazy dog."'
  id: totrans-split-51
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '**句子：** “The quick brown fox jumps over the lazy dog.”'
- en: '**Individual Heads:** Instead of one Self-Attention mechanism, Multi-Head Attention
    uses several independent "heads" (often 4-8). Each head has its own set of Query,
    Key, and Value vectors for each word.'
  id: totrans-split-52
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**单独的头部：** 多头注意力不使用单一的自注意力机制，而是使用几个独立的“头部”（通常为4-8个）。每个头部为每个词都有自己的查询、键和值向量集。'
- en: '**Diverse Attention:** Each head computes attention scores differently, focusing
    on various aspects of word relationships:'
  id: totrans-split-53
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**多样化的注意力：** 每个头部以不同的方式计算注意力分数，关注词语关系的各个方面：'
- en: One head might attend to grammatical roles (e.g., "fox" and "jumps").
  id: totrans-split-54
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个头可能关注语法角色（例如，“狐狸”和“跳”）。
- en: Another might focus on word order (e.g., "the" and "quick").
  id: totrans-split-55
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 另一个可能专注于词序（例如，“the”和“quick”）。
- en: Another might capture synonyms or related concepts (e.g., "quick" and "fast").
  id: totrans-split-56
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 另一个可能捕捉同义词或相关概念（例如，“quick”和“fast”）。
- en: '**Combining Perspectives:** After each head generates its own weighted values,
    their outputs are concatenated. This combines the diverse insights from different
    attention mechanisms.'
  id: totrans-split-57
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**结合多个视角：** 每个头生成其加权值后，它们的输出被串联起来。这结合了来自不同注意机制的多样洞察。'
- en: '**Final Representation:** This combined representation holds a richer understanding
    of the sentence, incorporating various relationships between words, not just a
    single focus.'
  id: totrans-split-58
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**最终表示：** 这种结合表示持有句子更丰富的理解，包括词之间的各种关系，而不是单一的焦点。'
- en: '* * *'
  id: totrans-split-59
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: '* * *'
- en: '**Positional Encoding:** Since transformers don''t process word order directly,
    this layer injects information about each word''s position in the sentence. It''s
    like giving the analysts a map so they know the order in which to consider the
    words.'
  id: totrans-split-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**位置编码：** 由于transformer不直接处理词序，这一层注入了关于每个单词在句子中位置的信息。这就像给分析人员一张地图，让他们知道考虑单词的顺序。'
- en: '* * *'
  id: totrans-split-61
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '* * *'
- en: 'Sure, let''s delve into positional encoding using an example sentence:'
  id: totrans-split-62
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 当然，让我们通过一个示例句子深入探讨位置编码：
- en: '**Sentence:** "The quick brown fox jumps over the lazy dog."'
  id: totrans-split-63
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '**句子：** "The quick brown fox jumps over the lazy dog."'
- en: '**Here''s how positional encoding works step-by-step:**'
  id: totrans-split-64
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '**以下是位置编码逐步工作的方式：**'
- en: '**Word Embeddings:**'
  id: totrans-split-65
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**词嵌入：**'
- en: Each word ("The", "quick", etc.) is converted into a numerical representation
    called a word embedding, like a unique identifier in a vast vocabulary map.
  id: totrans-split-66
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个单词（“The”，“quick”等）被转换为数值表示，称为词嵌入，就像在庞大词汇表中的唯一标识符。
- en: 'Imagine these embeddings as vectors:'
  id: totrans-split-67
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 想象这些嵌入向量为向量：
- en: '"The": [0.2, 0.5, -0.1, ...]'
  id: totrans-split-68
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '"The": [0.2, 0.5, -0.1, ...]'
- en: '"quick": [0.8, -0.3, 0.4, ...]'
  id: totrans-split-69
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '"quick": [0.8, -0.3, 0.4, ...]'
- en: '"brown": [..., ...]'
  id: totrans-split-70
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '"brown": [..., ...]'
- en: '...'
  id: totrans-split-71
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '...'
- en: '**Positional Information:**'
  id: totrans-split-72
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**位置信息：**'
- en: Each word's embedding is combined with additional values based on its position
    in the sentence.
  id: totrans-split-73
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个单词的嵌入与基于句子中位置的附加值结合。
- en: 'These values are calculated using sine and cosine functions at different frequencies:'
  id: totrans-split-74
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这些值是使用不同频率的正弦和余弦函数计算的：
- en: Lower frequencies capture long-range dependencies (e.g., "quick" and "fox" are
    related).
  id: totrans-split-75
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 低频捕捉长距离依赖关系（例如，“quick”和“fox”相关）。
- en: Higher frequencies encode short-range relationships (e.g., "jumps" and "over"
    are close).
  id: totrans-split-76
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 高频编码短距离关系（例如，“jumps”和“over”接近）。
- en: 'Think of these additional values as "position vectors":'
  id: totrans-split-77
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 把这些附加值想象成“位置向量”：
- en: '"The": [position 1 vector]'
  id: totrans-split-78
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '"The": [第1位置向量]'
- en: '"quick": [position 2 vector]'
  id: totrans-split-79
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '"quick": [第2位置向量]'
- en: '"brown": [position 3 vector]'
  id: totrans-split-80
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '"brown": [第3位置向量]'
- en: '...'
  id: totrans-split-81
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '...'
- en: '**Combining Embeddings and Positions:**'
  id: totrans-split-82
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**结合嵌入和位置：**'
- en: '**Understanding Order:**'
  id: totrans-split-83
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**理解顺序：**'
- en: Even if the sentence order changes (e.g., "Dog lazy jumps..."), the position
    vectors ensure relative positions are maintained.
  id: totrans-split-84
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 即使句子顺序改变（例如，“狗 懒 跳...”），位置向量确保相对位置保持不变。
- en: The model can still learn that "jumps" is more related to "over" than, say,
    "The".
  id: totrans-split-85
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型仍然可以学习到“jumps”与“over”之间的关系大于与“The”之间的关系。
- en: '* * *'
  id: totrans-split-86
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '* * *'
- en: '**Feed Forward Network(FFN):** This adds a layer of non-linearity, enabling
    the model to learn more complex relationships that might not be easily captured
    by attention mechanisms alone.'
  id: totrans-split-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**前馈网络（FFN）：** 这增加了一层非线性，使模型能够学习到单纯通过注意机制难以捕捉的更复杂关系。'
- en: '* * *'
  id: totrans-split-88
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '* * *'
- en: You've already delved into the sentence through previous layers. You understand
    individual words, their relationships, and their positions. Now, the FFN arrives
    like a detective magnifying glass, ready to uncover intricate details not immediately
    visible.
  id: totrans-split-89
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你已经通过之前的层深入探讨了这个句子。你理解单词，它们之间的关系以及它们的位置。现在，前馈网络（FFN）像侦探的放大镜一样，准备揭示不易看到的复杂细节。
- en: '**The FFN does this through three key steps:**'
  id: totrans-split-90
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '**FFN通过三个关键步骤完成这一过程：**'
- en: '**Non-linear Transformation:** Instead of straightforward calculations, the
    FFN uses non-linear functions like ReLU to add complexity. Think of it as applying
    a special filter to the existing information, revealing hidden patterns and connections
    that simple arithmetic might miss. This allows the FFN to capture more nuanced
    relationships between words.'
  id: totrans-split-91
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**非线性转换：** FFN不使用直接的计算，而是使用ReLU等非线性函数增加复杂性。把它看作是对现有信息应用特殊过滤器，揭示隐藏的模式和连接，这使得FFN能够捕捉简单算术可能忽略的更微妙的单词之间关系。'
- en: '**Multi-layered Analysis:** The FFN isn''t just one step; it''s typically a
    chain of two or more fully connected layers. Each layer builds upon the previous
    one, transforming the information step-by-step. Imagine you''re examining the
    sentence under increasing magnification, uncovering finer details with each layer.'
  id: totrans-split-92
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**多层分析：** FFN不只是一个步骤；它通常是两个或更多全连接层的链条。每一层都建立在前一层之上，逐步转换信息。想象你在逐渐增大的放大倍率下审视句子，每一层揭示出更精细的细节。'
- en: '**Dimensionality Shift:** The FFN expands the information''s size (e.g., from
    512 dimensions to 2048) in the first layer. This allows it to analyze a wider
    range of features and capture more complex patterns. Think of it as spreading
    out the information on a larger canvas for deeper examination. Then, it contracts
    it back to the original size (e.g., 512 again) in the final layer to ensure compatibility
    with subsequent layers.'
  id: totrans-split-93
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**维度转换：** FFN在第一层扩展信息的大小（例如，从512维到2048维）。这使得它能够分析更广泛的特征并捕捉更复杂的模式。可以将其想象为在更大的画布上展开信息进行深入检查。然后，在最后一层将其收缩回原始大小（例如，再次为512维），以确保与后续层兼容。'
- en: '**Applying this to our sentence:**'
  id: totrans-split-94
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '**将此应用到我们的句子中：**'
- en: Imagine the FFN helps identify that "quick" and "brown" not only describe the
    "fox" but also subtly connect to its perceived speed through their combined meaning.
  id: totrans-split-95
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 想象一下，前馈网络（FFN）帮助识别“quick”和“brown”不仅描述“fox”，而且通过它们的联合意义微妙地连接到它的感知速度。
- en: Or, it might delve deeper into the relationship between "jumps" and "over,"
    understanding the action and spatial context beyond just their individual definitions.
  id: totrans-split-96
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 或者，它可能深入探讨“jumps”和“over”之间的关系，理解动作和空间背景，不仅仅是它们各自的定义。
- en: '* * *'
  id: totrans-split-97
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '* * *'
- en: '**Repeat, Refine, Repeat:** These layers (self-attention, multi-head attention,
    etc.) are stacked and repeated multiple times. With each iteration, the encoder
    refines its understanding, building a comprehensive representation of the input
    text.'
  id: totrans-split-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**重复、精炼、再重复：** 这些层（自注意力、多头注意力等）会被堆叠和重复多次。每次迭代时，编码器都会精炼其理解，构建输入文本的全面表示。'
- en: 'image source: pillow lab blog'
  id: totrans-split-99
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来源：pillow lab 博客
- en: '**Decoder: Weaving the Output Tapestry**'
  id: totrans-split-100
  prefs: []
  type: TYPE_NORMAL
  zh: '**解码器：编织输出的图景**'
- en: 'Now, the decoder takes the baton. But unlike the encoder, it has an additional
    challenge: generating the output word by word without peeking at the future. To
    achieve this, it utilizes:'
  id: totrans-split-101
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，解码器接过接力棒。但与编码器不同的是，它面临额外的挑战：逐字生成输出，不能窥视未来。为了实现这一点，它利用以下机制：
- en: '**Masked Self-Attention:** Similar to the encoder''s self-attention, but with
    a twist. The decoder only attends to previously generated words, ensuring it doesn''t
    cheat and use future information. It''s like writing a story one sentence at a
    time, without knowing how it ends.'
  id: totrans-split-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**掩码自注意力：** 类似于编码器的自注意力，但有所不同。解码器只关注先前生成的单词，确保不作弊和使用未来信息。这就像一次只写一句话的故事，不知道它如何结束。'
- en: '**Encoder-Decoder Attention:** This mechanism allows the decoder to consult
    the encoded input, like referring back to a reference document while writing.
    It ensures the generated output stays coherent and aligned with the original text.'
  id: totrans-split-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**编码器-解码器注意力：** 这个机制允许解码器参考编码的输入，就像写作时参考参考文献一样。它确保生成的输出保持连贯并与原始文本对齐。'
- en: '**Multi-Head Attention and Feed Forward Network:** Just like the encoder, these
    layers help the decoder refine its understanding of the context and relationships
    within the text.'
  id: totrans-split-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**多头注意力和前馈网络：** 就像编码器一样，这些层帮助解码器精炼其对文本内上下文和关系的理解。'
- en: '**Output Layer:** Finally, the decoder translates its internal representation
    into the actual output word, one by one. It''s like the final assembly line, putting
    the pieces together to form the desired outcome.'
  id: totrans-split-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**输出层：** 最后，解码器将其内部表示逐个翻译为实际输出词。这就像最终的装配线，将各个部件组装起来形成期望的结果。'
- en: '**Beyond the Basics:**'
  id: totrans-split-106
  prefs: []
  type: TYPE_NORMAL
  zh: '**超越基础：**'
- en: Remember, this is just a glimpse into the fascinating world of transformers.
    The specific architecture can vary depending on the task and dataset, with different
    numbers of layers and configurations.
  id: totrans-split-107
  prefs: []
  type: TYPE_NORMAL
  zh: 记住，这只是转换器引人入胜世界的一瞥。具体的架构可能因任务和数据集而异，具有不同数量的层和配置。
- en: Additionally, each layer involves complex mathematical operations that go beyond
    the scope of this explanation.
  id: totrans-split-108
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，每一层都涉及复杂的数学操作，超出本解释的范围。
- en: But hopefully, this has equipped you with a fundamental understanding of how
    transformers work and why they have revolutionized the field of NLP.
  id: totrans-split-109
  prefs: []
  type: TYPE_NORMAL
  zh: 但希望这让你对转换器如何工作及其为何革新自然语言处理领域有了基本理解。
- en: So, the next time you encounter a seamless machine translation or marvel at
    the creativity of an AI-powered text generator, remember the intricate dance of
    the encoder and decoder within the transformer, weaving magic with the power of
    attention and parallel processing.
  id: totrans-split-110
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，下次你遇到无缝的机器翻译或者对AI驱动的文本生成器的创造力感到惊叹时，请记住变压器内编码器和解码器的复杂舞蹈，用注意力和并行处理的力量编织魔法。
- en: '*Paper: https://arxiv.org/abs/1706.03762*'
  id: totrans-split-111
  prefs: []
  type: TYPE_NORMAL
  zh: '*论文：https://arxiv.org/abs/1706.03762*'
- en: Thank you for reading The ZenMode. This post is public so feel free to share
    it.
  id: totrans-split-112
  prefs: []
  type: TYPE_NORMAL
  zh: 感谢您阅读《ZenMode》。本文是公开的，所以请随意分享。
- en: '[Share](https://nintyzeros.substack.com/p/how-do-transformer-workdesign-a-multi?utm_source=substack&utm_medium=email&utm_content=share&action=share)'
  id: totrans-split-113
  prefs: []
  type: TYPE_NORMAL
  zh: '[分享](https://nintyzeros.substack.com/p/how-do-transformer-workdesign-a-multi?utm_source=substack&utm_medium=email&utm_content=share&action=share)'
- en: '* * *'
  id: totrans-split-114
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
