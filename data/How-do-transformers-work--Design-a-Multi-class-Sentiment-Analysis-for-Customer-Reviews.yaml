- en: <!--yml
  id: totrans-split-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: æœªåˆ†ç±»'
  id: totrans-split-1
  prefs: []
  type: TYPE_NORMAL
  zh: ç±»åˆ«ï¼šæœªåˆ†ç±»
- en: 'date: 2024-05-27 14:35:08'
  id: totrans-split-2
  prefs: []
  type: TYPE_NORMAL
  zh: æ—¥æœŸï¼š2024-05-27 14:35:08
- en: -->
  id: totrans-split-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: How do transformers work?+Design a Multi-class Sentiment Analysis for Customer
    Reviews
  id: totrans-split-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: transformerså¦‚ä½•å·¥ä½œï¼Ÿ+è®¾è®¡ä¸€ä¸ªå¤šç±»æƒ…æ„Ÿåˆ†æç”¨äºå®¢æˆ·è¯„è®º
- en: æ¥æºï¼š[https://nintyzeros.substack.com/p/how-do-transformer-workdesign-a-multi](https://nintyzeros.substack.com/p/how-do-transformer-workdesign-a-multi)
  id: totrans-split-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: æ¥æºï¼š[https://nintyzeros.substack.com/p/how-do-transformer-workdesign-a-multi](https://nintyzeros.substack.com/p/how-do-transformer-workdesign-a-multi)
- en: '*ğŸ‘‹ Hi, this is [](https://twitter.com/gergelyorosz) Venkat and here with a
    free, full issue of the The ZenMode Engineer Newsletter. In every issue, I cover
    one topic explained in a simpler terms in areas related to computer technologies
    and beyond.*'
  id: totrans-split-6
  prefs: []
  type: TYPE_NORMAL
  zh: '*ğŸ‘‹ å—¨ï¼Œè¿™é‡Œæ˜¯[](https://twitter.com/gergelyorosz) Venkatï¼Œå¹¶ä¸”åœ¨è¿™é‡Œå…è´¹æä¾›ã€ŠZenModeå·¥ç¨‹å¸ˆé€šè®¯ã€‹çš„å®Œæ•´å†…å®¹ã€‚åœ¨æ¯ä¸€æœŸä¸­ï¼Œæˆ‘æ¶µç›–ä¸€ä¸ªå…³äºè®¡ç®—æœºæŠ€æœ¯åŠå…¶å®ƒé¢†åŸŸçš„ä¸»é¢˜ï¼Œç”¨æ›´ç®€å•çš„æœ¯è¯­è§£é‡Šã€‚*'
- en: '* * *'
  id: totrans-split-7
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Transformers have become synonymous with cutting-edge AI, particularly in the
    realm of natural language processing (NLP).
  id: totrans-split-8
  prefs: []
  type: TYPE_NORMAL
  zh: Transformerså·²ç»æˆä¸ºå°¤å…¶åœ¨è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰é¢†åŸŸçš„å‰æ²¿AIçš„ä»£åè¯ã€‚
- en: But what exactly makes them tick? How do these models navigate the intricacies
    of language with such remarkable efficiency and accuracy?
  id: totrans-split-9
  prefs: []
  type: TYPE_NORMAL
  zh: ä½†ç©¶ç«Ÿæ˜¯ä»€ä¹ˆè®©å®ƒä»¬å¦‚æ­¤é«˜æ•ˆå‡†ç¡®åœ°å·¥ä½œï¼Ÿè¿™äº›æ¨¡å‹å¦‚ä½•åœ¨è¯­è¨€çš„å¤æ‚æ€§ä¸­èˆªè¡Œï¼Ÿ
- en: Buckle up, because we're about to learn the heart of the transformer architecture.
  id: totrans-split-10
  prefs: []
  type: TYPE_NORMAL
  zh: ç³»å¥½å®‰å…¨å¸¦ï¼Œå› ä¸ºæˆ‘ä»¬å³å°†äº†è§£transformeræ¶æ„çš„æ ¸å¿ƒã€‚
- en: But.. Before we deep dive into it lets understand where its been used.. if you
    have used google translate/ ChatGPT both rely on these.
  id: totrans-split-11
  prefs: []
  type: TYPE_NORMAL
  zh: ä½†æ˜¯â€¦â€¦ åœ¨æˆ‘ä»¬æ·±å…¥æ¢è®¨ä¹‹å‰ï¼Œè®©æˆ‘ä»¬äº†è§£å®ƒè¢«ä½¿ç”¨åœ¨å“ªé‡Œâ€¦â€¦ å¦‚æœä½ ä½¿ç”¨è¿‡Googleç¿»è¯‘/ChatGPTï¼Œå®ƒä»¬éƒ½ä¾èµ–äºè¿™äº›ã€‚
- en: '***Google Translate:** This widely used platform relies heavily on transformers
    to achieve fast and accurate translations across over 100 languages. It considers
    the entire sentence context, not just individual words, leading to more natural-sounding
    translations.*'
  id: totrans-split-12
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '***è°·æ­Œç¿»è¯‘ï¼š** è¿™ä¸€å¹¿æ³›ä½¿ç”¨çš„å¹³å°ä¾èµ–äºtransformersï¼Œå®ç°å¿«é€Ÿä¸”å‡†ç¡®çš„è·¨100å¤šç§è¯­è¨€çš„ç¿»è¯‘ã€‚å®ƒè€ƒè™‘æ•´ä¸ªå¥å­çš„è¯­å¢ƒï¼Œè€Œä¸ä»…ä»…æ˜¯å•ä¸ªå•è¯ï¼Œä»è€Œäº§ç”Ÿæ›´è‡ªç„¶çš„ç¿»è¯‘ã€‚*'
- en: '***Netflix Recommendation System:** Ever wondered how Netflix suggests shows
    and movies you might enjoy? Transformers analyze your viewing history and other
    users'' data to identify patterns and connections, ultimately recommending content
    tailored to your preferences.*'
  id: totrans-split-13
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '***Netflixæ¨èç³»ç»Ÿï¼š** ä½ æ˜¯å¦æƒ³çŸ¥é“Netflixæ˜¯å¦‚ä½•æ¨èä½ å¯èƒ½å–œæ¬¢çš„èŠ‚ç›®å’Œç”µå½±çš„ï¼ŸTransformersåˆ†æä½ çš„è§‚çœ‹å†å²å’Œå…¶ä»–ç”¨æˆ·çš„æ•°æ®ï¼Œè¯†åˆ«æ¨¡å¼å’Œè¿æ¥ï¼Œæœ€ç»ˆæ¨èæ ¹æ®ä½ åå¥½é‡èº«å®šåˆ¶çš„å†…å®¹ã€‚*'
- en: '**The Big Picture: Encoder and Decoder Dance**'
  id: totrans-split-14
  prefs: []
  type: TYPE_NORMAL
  zh: '**æ•´ä½“æƒ…å†µï¼šç¼–ç å™¨å’Œè§£ç å™¨çš„èˆè¹ˆ**'
- en: 'Imagine a factory, but instead of assembling physical objects, it processes
    language. This factory has two main departments:'
  id: totrans-split-15
  prefs: []
  type: TYPE_NORMAL
  zh: æƒ³è±¡ä¸€ä¸ªå·¥å‚ï¼Œä½†ä¸æ˜¯ç”¨æ¥ç»„è£…ç‰©ç†ç‰©ä½“çš„ï¼Œè€Œæ˜¯å¤„ç†è¯­è¨€ã€‚è¿™ä¸ªå·¥å‚æœ‰ä¸¤ä¸ªä¸»è¦éƒ¨é—¨ï¼š
- en: '**The Encoder:** This is the information extractor, meticulously dissecting
    the input text, understanding its individual elements, and uncovering the hidden
    connections between them.'
  id: totrans-split-16
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**ç¼–ç å™¨ï¼š** è¿™æ˜¯ä¿¡æ¯æå–å™¨ï¼Œç²¾ç»†è§£å‰–è¾“å…¥æ–‡æœ¬ï¼Œç†è§£å…¶å„ä¸ªå…ƒç´ ï¼Œå¹¶æ­ç¤ºå®ƒä»¬ä¹‹é—´éšè—çš„è¿æ¥ã€‚'
- en: '**The Decoder:** Armed with the encoder''s insights, the decoder crafts the
    desired output, be it a translated sentence, a concise summary, or even a brand
    new poem.'
  id: totrans-split-17
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**è§£ç å™¨ï¼š** åœ¨ç¼–ç å™¨çš„æ´å¯ŸåŠ›æŒ‡å¯¼ä¸‹ï¼Œè§£ç å™¨åˆ¶å®šæ‰€éœ€çš„è¾“å‡ºï¼Œæ— è®ºæ˜¯ç¿»è¯‘å¥å­ã€ç®€æ´æ‘˜è¦ï¼Œç”šè‡³æ˜¯å…¨æ–°çš„è¯—æ­Œã€‚'
- en: '**Encoder: Decoding the Input Labyrinth**'
  id: totrans-split-18
  prefs: []
  type: TYPE_NORMAL
  zh: '**ç¼–ç å™¨ï¼šè§£ç è¾“å…¥è¿·å®«**'
- en: The encoder's journey begins with **Input Embedding**, where each word is transformed
    from its textual form into a numerical representation (vector). Think of it as
    assigning each word a unique identifier.
  id: totrans-split-19
  prefs: []
  type: TYPE_NORMAL
  zh: ç¼–ç å™¨çš„æ—…ç¨‹å§‹äº**è¾“å…¥åµŒå…¥**ï¼Œåœ¨è¿™é‡Œï¼Œæ¯ä¸ªå•è¯ä»å…¶æ–‡æœ¬å½¢å¼è½¬æ¢ä¸ºæ•°å€¼è¡¨ç¤ºï¼ˆå‘é‡ï¼‰ã€‚å¯ä»¥å°†å…¶çœ‹ä½œæ˜¯ä¸ºæ¯ä¸ªå•è¯åˆ†é…ä¸€ä¸ªå”¯ä¸€æ ‡è¯†ç¬¦ã€‚
- en: '* * *'
  id: totrans-split-20
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Consider this example.
  id: totrans-split-21
  prefs: []
  type: TYPE_NORMAL
  zh: è€ƒè™‘è¿™ä¸ªä¾‹å­ã€‚
- en: '**Input Text:** The process begins with the raw text sentence, such as "The
    cat sat on the mat."'
  id: totrans-split-22
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**è¾“å…¥æ–‡æœ¬ï¼š** è¿™ä¸ªè¿‡ç¨‹ä»åŸå§‹æ–‡æœ¬å¥å­å¼€å§‹ï¼Œæ¯”å¦‚â€œçŒ«ååœ¨å«å­ä¸Šã€‚â€'
- en: '**Input Embedding Layer:**'
  id: totrans-split-23
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**è¾“å…¥åµŒå…¥å±‚ï¼š**'
- en: This layer acts as a translator, converting each word into a numerical vector.
  id: totrans-split-24
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: è¿™ä¸€å±‚å……å½“ç¿»è¯‘å™¨çš„è§’è‰²ï¼Œå°†æ¯ä¸ªå•è¯è½¬æ¢ä¸ºæ•°å€¼å‘é‡ã€‚
- en: Imagine a large dictionary where each word has a corresponding vector address.
  id: totrans-split-25
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: æƒ³è±¡ä¸€ä¸ªå¤§å‹è¯å…¸ï¼Œå…¶ä¸­æ¯ä¸ªå•è¯éƒ½æœ‰å¯¹åº”çš„å‘é‡åœ°å€ã€‚
- en: 'These vectors capture various aspects of word meaning:'
  id: totrans-split-26
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: è¿™äº›å‘é‡æ•æ‰å•è¯å«ä¹‰çš„å„ç§æ–¹é¢ï¼š
- en: Semantic relationships (e.g., "cat" is closer to "pet" than "chair").
  id: totrans-split-27
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: è¯­ä¹‰å…³ç³»ï¼ˆä¾‹å¦‚ï¼Œâ€œcatâ€æ›´æ¥è¿‘â€œpetâ€è€Œä¸æ˜¯â€œchairâ€ï¼‰ã€‚
- en: Syntactic roles (e.g., "cat" is often a noun, while "sat" is a verb).
  id: totrans-split-28
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: å¥æ³•è§’è‰²ï¼ˆä¾‹å¦‚ï¼Œâ€œcatâ€é€šå¸¸æ˜¯åè¯ï¼Œè€Œâ€œsatâ€æ˜¯åŠ¨è¯ï¼‰ã€‚
- en: Context within the sentence (e.g., "mat" here likely refers to a floor mat).
  id: totrans-split-29
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: å¥å­å†…ä¸Šä¸‹æ–‡ï¼ˆä¾‹å¦‚ï¼Œâ€œmatâ€è¿™é‡Œå¯èƒ½æŒ‡åœ°æ¯¯ï¼‰ã€‚
- en: '**Vector Representation:**'
  id: totrans-split-30
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**å‘é‡è¡¨ç¤ºï¼š**'
- en: '* * *'
  id: totrans-split-31
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '* * *'
- en: 'But the encoder doesn''t stop there. It employs the following key mechanisms
    to delve deeper:'
  id: totrans-split-32
  prefs: []
  type: TYPE_NORMAL
  zh: ä½†ç¼–ç å™¨å¹¶ä¸æ­¢äºæ­¤ã€‚å®ƒé‡‡ç”¨ä»¥ä¸‹å…³é”®æœºåˆ¶æ·±å…¥æ¢ç´¢ï¼š
- en: '**Self-Attention Layer:** This is the game-changer. Imagine shining a spotlight
    on each word, but instead of illuminating it in isolation, you also highlight
    how it connects to all other words in the sentence. This allows the encoder to
    grasp the context, nuances, and relationships within the text, not just the individual
    words.'
  id: totrans-split-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**è‡ªæ³¨æ„åŠ›å±‚ï¼š** è¿™æ˜¯æ”¹å˜æ¸¸æˆè§„åˆ™çš„åœ°æ–¹ã€‚æƒ³è±¡ä¸€ä¸‹ï¼Œå¯¹æ¯ä¸ªè¯éƒ½æŠ•å°„å‡ºèšå…‰ç¯ï¼Œä½†ä¸ä»…ç…§äº®å®ƒæœ¬èº«ï¼Œè¿˜çªæ˜¾å®ƒä¸å¥å­ä¸­æ‰€æœ‰å…¶ä»–è¯çš„è¿æ¥æ–¹å¼ã€‚è¿™ä½¿å¾—ç¼–ç å™¨èƒ½å¤Ÿç†è§£æ–‡æœ¬ä¸­çš„ä¸Šä¸‹æ–‡ã€ç»†å¾®å·®åˆ«å’Œå…³ç³»ï¼Œè€Œä¸ä»…ä»…æ˜¯ä¸ªåˆ«è¯è¯­ã€‚'
- en: ref from Raimi Karim blog (used only to refernce)
  id: totrans-split-34
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: å¼•ç”¨è‡ª Raimi Karim åšå®¢ï¼ˆä»…ä¾›å‚è€ƒï¼‰
- en: '* * *'
  id: totrans-split-35
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '* * *'
- en: Consider this example sentence again "***The quick brown fox jumps over the
    lazy dog.***"
  id: totrans-split-36
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: å†æ¬¡è€ƒè™‘è¿™ä¸ªä¾‹å¥ï¼šâ€œ***The quick brown fox jumps over the lazy dog.***â€
- en: '**Word Embeddings:** First, each word is transformed into a numerical representation
    called a "word embedding." Think of it as assigning each word a unique identifier
    in a giant vocabulary map.'
  id: totrans-split-37
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**è¯åµŒå…¥ï¼š** é¦–å…ˆï¼Œæ¯ä¸ªè¯è¢«è½¬æ¢ä¸ºä¸€ä¸ªç§°ä¸ºâ€œè¯åµŒå…¥â€çš„æ•°å€¼è¡¨ç¤ºã€‚å¯ä»¥å°†å…¶è§†ä¸ºåœ¨ä¸€ä¸ªå·¨å¤§çš„è¯æ±‡æ˜ å°„ä¸­ä¸ºæ¯ä¸ªè¯åˆ†é…ä¸€ä¸ªå”¯ä¸€çš„æ ‡è¯†ç¬¦ã€‚'
- en: '**Query, Key, Value:** Next, the Self-Attention mechanism creates three special
    vectors for each word:'
  id: totrans-split-38
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**æŸ¥è¯¢ã€é”®ã€å€¼ï¼š** æ¥ä¸‹æ¥ï¼Œè‡ªæ³¨æ„åŠ›æœºåˆ¶ä¸ºæ¯ä¸ªè¯åˆ›å»ºä¸‰ä¸ªç‰¹æ®Šå‘é‡ï¼š'
- en: '**Query (Q):** This vector asks "What information do I need from other words?"'
  id: totrans-split-39
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**æŸ¥è¯¢ï¼ˆQï¼‰ï¼š** è¿™ä¸ªå‘é‡è¯¢é—®â€œæˆ‘éœ€è¦ä»å…¶ä»–è¯è¯­ä¸­è·å–ä»€ä¹ˆä¿¡æ¯ï¼Ÿâ€'
- en: '**Key (K):** This vector acts like a label, saying "This is the information
    I have to offer."'
  id: totrans-split-40
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**é”®ï¼ˆKï¼‰ï¼š** è¿™ä¸ªå‘é‡å……å½“æ ‡ç­¾ï¼Œè¡¨ç¤ºâ€œè¿™æ˜¯æˆ‘è¦æä¾›çš„ä¿¡æ¯â€ã€‚'
- en: '**Value (V):** This vector holds the actual information, like the word''s meaning
    and context.'
  id: totrans-split-41
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**å€¼ï¼ˆVï¼‰ï¼š** è¿™ä¸ªå‘é‡ä¿å­˜å®é™…ä¿¡æ¯ï¼Œæ¯”å¦‚è¯çš„å«ä¹‰å’Œä¸Šä¸‹æ–‡ã€‚'
- en: '**Attention Scores:** Now comes the interesting part. The Self-Attention layer
    compares the Query vector of each word with the Key vectors of all other words
    in the sentence.'
  id: totrans-split-42
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**æ³¨æ„åŠ›åˆ†æ•°ï¼š** ç°åœ¨æ¥åˆ°æœ‰è¶£çš„éƒ¨åˆ†ã€‚è‡ªæ³¨æ„åŠ›å±‚å°†æ¯ä¸ªè¯çš„æŸ¥è¯¢å‘é‡ä¸å¥å­ä¸­æ‰€æœ‰å…¶ä»–è¯çš„é”®å‘é‡è¿›è¡Œæ¯”è¾ƒã€‚'
- en: This helps it understand how relevant each word is to the current word. Based
    on this comparison, it calculates an **attention score** for each pair of words.
  id: totrans-split-43
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: è¿™æœ‰åŠ©äºç†è§£æ¯ä¸ªè¯ä¸å½“å‰è¯çš„ç›¸å…³æ€§ã€‚åŸºäºè¿™ç§æ¯”è¾ƒï¼Œå®ƒä¸ºæ¯å¯¹è¯è®¡ç®—ä¸€ä¸ª**æ³¨æ„åŠ›åˆ†æ•°**ã€‚
- en: Imagine shining a spotlight on each word. The brighter the spotlight on another
    word, the higher the attention score, meaning the more relevant that word is to
    the current word.
  id: totrans-split-44
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: æƒ³è±¡ä¸€ä¸‹ï¼Œå¯¹æ¯ä¸ªè¯éƒ½æŠ•å°„å‡ºèšå…‰ç¯ã€‚å…¶ä»–è¯ä¸Šçš„èšå…‰ç¯è¶Šäº®ï¼Œæ³¨æ„åŠ›åˆ†æ•°è¶Šé«˜ï¼Œæ„å‘³ç€è¯¥è¯å¯¹å½“å‰è¯çš„ç›¸å…³æ€§è¶Šé«˜ã€‚
- en: '**Weighted Values:** Finally, the Self-Attention layer uses the attention scores
    to weigh the Value vectors of all other words. Words with higher attention scores
    get more weight, contributing more to the final representation of the current
    word.'
  id: totrans-split-45
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**åŠ æƒå€¼ï¼š** æœ€åï¼Œè‡ªæ³¨æ„åŠ›å±‚ä½¿ç”¨æ³¨æ„åŠ›åˆ†æ•°æ¥åŠ æƒæ‰€æœ‰å…¶ä»–è¯çš„å€¼å‘é‡ã€‚æ³¨æ„åŠ›åˆ†æ•°è¾ƒé«˜çš„è¯è·å¾—æ›´å¤§çš„æƒé‡ï¼Œå¯¹å½“å‰è¯çš„æœ€ç»ˆè¡¨ç¤ºè´¡çŒ®æ›´å¤§ã€‚'
- en: Think of it like taking a weighted average of the information from other words,
    where the weights are determined by how relevant they are.
  id: totrans-split-46
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: å¯ä»¥å°†å…¶è§†ä¸ºä»å…¶ä»–è¯è¯­è·å–ä¿¡æ¯çš„åŠ æƒå¹³å‡å€¼ï¼Œå…¶ä¸­æƒé‡ç”±å®ƒä»¬çš„ç›¸å…³æ€§å†³å®šã€‚
- en: '**New Word Representation:** By considering the context provided by other words,
    the Self-Attention layer creates a new, enriched representation of each word.
    This new representation captures not just the word''s own meaning, but also how
    it relates to and is influenced by other words in the sentence.'
  id: totrans-split-47
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**æ–°è¯è¡¨ç¤ºï¼š** é€šè¿‡è€ƒè™‘å…¶ä»–è¯æä¾›çš„ä¸Šä¸‹æ–‡ï¼Œè‡ªæ³¨æ„åŠ›å±‚ä¸ºæ¯ä¸ªè¯åˆ›å»ºäº†ä¸€ä¸ªæ–°çš„ã€ä¸°å¯Œçš„è¡¨ç¤ºã€‚è¿™ç§æ–°çš„è¡¨ç¤ºä¸ä»…æ•æ‰äº†è¯è¯­æœ¬èº«çš„å«ä¹‰ï¼Œè¿˜æ•æ‰äº†å®ƒä¸å¥å­ä¸­å…¶ä»–è¯è¯­çš„å…³ç³»åŠå…¶å—å½±å“çš„æ–¹å¼ã€‚'
- en: '* * *'
  id: totrans-split-48
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: '* * *'
- en: '**Multi-Head Attention:** This is like having multiple teams of analysts, each
    focusing on different aspects of the connections between words. It allows the
    encoder to capture various facets of the relationships, enriching its understanding.'
  id: totrans-split-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**å¤šå¤´æ³¨æ„åŠ›ï¼š** è¿™å°±åƒæœ‰å¤šä¸ªåˆ†æå›¢é˜Ÿï¼Œæ¯ä¸ªå›¢é˜Ÿå…³æ³¨è¯è¯­ä¹‹é—´è¿æ¥çš„ä¸åŒæ–¹é¢ã€‚å®ƒä½¿ç¼–ç å™¨èƒ½å¤Ÿæ•æ‰å…³ç³»çš„å¤šä¸ªæ–¹é¢ï¼Œä¸°å¯Œå…¶ç†è§£ã€‚'
- en: '* * *'
  id: totrans-split-50
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '* * *'
- en: '**Sentence:** "The quick brown fox jumps over the lazy dog."'
  id: totrans-split-51
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '**å¥å­ï¼š** â€œThe quick brown fox jumps over the lazy dog.â€'
- en: '**Individual Heads:** Instead of one Self-Attention mechanism, Multi-Head Attention
    uses several independent "heads" (often 4-8). Each head has its own set of Query,
    Key, and Value vectors for each word.'
  id: totrans-split-52
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**å•ç‹¬çš„å¤´éƒ¨ï¼š** å¤šå¤´æ³¨æ„åŠ›ä¸ä½¿ç”¨å•ä¸€çš„è‡ªæ³¨æ„åŠ›æœºåˆ¶ï¼Œè€Œæ˜¯ä½¿ç”¨å‡ ä¸ªç‹¬ç«‹çš„â€œå¤´éƒ¨â€ï¼ˆé€šå¸¸ä¸º4-8ä¸ªï¼‰ã€‚æ¯ä¸ªå¤´éƒ¨ä¸ºæ¯ä¸ªè¯éƒ½æœ‰è‡ªå·±çš„æŸ¥è¯¢ã€é”®å’Œå€¼å‘é‡é›†ã€‚'
- en: '**Diverse Attention:** Each head computes attention scores differently, focusing
    on various aspects of word relationships:'
  id: totrans-split-53
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**å¤šæ ·åŒ–çš„æ³¨æ„åŠ›ï¼š** æ¯ä¸ªå¤´éƒ¨ä»¥ä¸åŒçš„æ–¹å¼è®¡ç®—æ³¨æ„åŠ›åˆ†æ•°ï¼Œå…³æ³¨è¯è¯­å…³ç³»çš„å„ä¸ªæ–¹é¢ï¼š'
- en: One head might attend to grammatical roles (e.g., "fox" and "jumps").
  id: totrans-split-54
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä¸€ä¸ªå¤´å¯èƒ½å…³æ³¨è¯­æ³•è§’è‰²ï¼ˆä¾‹å¦‚ï¼Œâ€œç‹ç‹¸â€å’Œâ€œè·³â€ï¼‰ã€‚
- en: Another might focus on word order (e.g., "the" and "quick").
  id: totrans-split-55
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: å¦ä¸€ä¸ªå¯èƒ½ä¸“æ³¨äºè¯åºï¼ˆä¾‹å¦‚ï¼Œâ€œtheâ€å’Œâ€œquickâ€ï¼‰ã€‚
- en: Another might capture synonyms or related concepts (e.g., "quick" and "fast").
  id: totrans-split-56
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: å¦ä¸€ä¸ªå¯èƒ½æ•æ‰åŒä¹‰è¯æˆ–ç›¸å…³æ¦‚å¿µï¼ˆä¾‹å¦‚ï¼Œâ€œquickâ€å’Œâ€œfastâ€ï¼‰ã€‚
- en: '**Combining Perspectives:** After each head generates its own weighted values,
    their outputs are concatenated. This combines the diverse insights from different
    attention mechanisms.'
  id: totrans-split-57
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**ç»“åˆå¤šä¸ªè§†è§’ï¼š** æ¯ä¸ªå¤´ç”Ÿæˆå…¶åŠ æƒå€¼åï¼Œå®ƒä»¬çš„è¾“å‡ºè¢«ä¸²è”èµ·æ¥ã€‚è¿™ç»“åˆäº†æ¥è‡ªä¸åŒæ³¨æ„æœºåˆ¶çš„å¤šæ ·æ´å¯Ÿã€‚'
- en: '**Final Representation:** This combined representation holds a richer understanding
    of the sentence, incorporating various relationships between words, not just a
    single focus.'
  id: totrans-split-58
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**æœ€ç»ˆè¡¨ç¤ºï¼š** è¿™ç§ç»“åˆè¡¨ç¤ºæŒæœ‰å¥å­æ›´ä¸°å¯Œçš„ç†è§£ï¼ŒåŒ…æ‹¬è¯ä¹‹é—´çš„å„ç§å…³ç³»ï¼Œè€Œä¸æ˜¯å•ä¸€çš„ç„¦ç‚¹ã€‚'
- en: '* * *'
  id: totrans-split-59
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: '* * *'
- en: '**Positional Encoding:** Since transformers don''t process word order directly,
    this layer injects information about each word''s position in the sentence. It''s
    like giving the analysts a map so they know the order in which to consider the
    words.'
  id: totrans-split-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**ä½ç½®ç¼–ç ï¼š** ç”±äºtransformerä¸ç›´æ¥å¤„ç†è¯åºï¼Œè¿™ä¸€å±‚æ³¨å…¥äº†å…³äºæ¯ä¸ªå•è¯åœ¨å¥å­ä¸­ä½ç½®çš„ä¿¡æ¯ã€‚è¿™å°±åƒç»™åˆ†æäººå‘˜ä¸€å¼ åœ°å›¾ï¼Œè®©ä»–ä»¬çŸ¥é“è€ƒè™‘å•è¯çš„é¡ºåºã€‚'
- en: '* * *'
  id: totrans-split-61
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '* * *'
- en: 'Sure, let''s delve into positional encoding using an example sentence:'
  id: totrans-split-62
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: å½“ç„¶ï¼Œè®©æˆ‘ä»¬é€šè¿‡ä¸€ä¸ªç¤ºä¾‹å¥å­æ·±å…¥æ¢è®¨ä½ç½®ç¼–ç ï¼š
- en: '**Sentence:** "The quick brown fox jumps over the lazy dog."'
  id: totrans-split-63
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '**å¥å­ï¼š** "The quick brown fox jumps over the lazy dog."'
- en: '**Here''s how positional encoding works step-by-step:**'
  id: totrans-split-64
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '**ä»¥ä¸‹æ˜¯ä½ç½®ç¼–ç é€æ­¥å·¥ä½œçš„æ–¹å¼ï¼š**'
- en: '**Word Embeddings:**'
  id: totrans-split-65
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**è¯åµŒå…¥ï¼š**'
- en: Each word ("The", "quick", etc.) is converted into a numerical representation
    called a word embedding, like a unique identifier in a vast vocabulary map.
  id: totrans-split-66
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: æ¯ä¸ªå•è¯ï¼ˆâ€œTheâ€ï¼Œâ€œquickâ€ç­‰ï¼‰è¢«è½¬æ¢ä¸ºæ•°å€¼è¡¨ç¤ºï¼Œç§°ä¸ºè¯åµŒå…¥ï¼Œå°±åƒåœ¨åºå¤§è¯æ±‡è¡¨ä¸­çš„å”¯ä¸€æ ‡è¯†ç¬¦ã€‚
- en: 'Imagine these embeddings as vectors:'
  id: totrans-split-67
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: æƒ³è±¡è¿™äº›åµŒå…¥å‘é‡ä¸ºå‘é‡ï¼š
- en: '"The": [0.2, 0.5, -0.1, ...]'
  id: totrans-split-68
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '"The": [0.2, 0.5, -0.1, ...]'
- en: '"quick": [0.8, -0.3, 0.4, ...]'
  id: totrans-split-69
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '"quick": [0.8, -0.3, 0.4, ...]'
- en: '"brown": [..., ...]'
  id: totrans-split-70
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '"brown": [..., ...]'
- en: '...'
  id: totrans-split-71
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '...'
- en: '**Positional Information:**'
  id: totrans-split-72
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**ä½ç½®ä¿¡æ¯ï¼š**'
- en: Each word's embedding is combined with additional values based on its position
    in the sentence.
  id: totrans-split-73
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: æ¯ä¸ªå•è¯çš„åµŒå…¥ä¸åŸºäºå¥å­ä¸­ä½ç½®çš„é™„åŠ å€¼ç»“åˆã€‚
- en: 'These values are calculated using sine and cosine functions at different frequencies:'
  id: totrans-split-74
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: è¿™äº›å€¼æ˜¯ä½¿ç”¨ä¸åŒé¢‘ç‡çš„æ­£å¼¦å’Œä½™å¼¦å‡½æ•°è®¡ç®—çš„ï¼š
- en: Lower frequencies capture long-range dependencies (e.g., "quick" and "fox" are
    related).
  id: totrans-split-75
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä½é¢‘æ•æ‰é•¿è·ç¦»ä¾èµ–å…³ç³»ï¼ˆä¾‹å¦‚ï¼Œâ€œquickâ€å’Œâ€œfoxâ€ç›¸å…³ï¼‰ã€‚
- en: Higher frequencies encode short-range relationships (e.g., "jumps" and "over"
    are close).
  id: totrans-split-76
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: é«˜é¢‘ç¼–ç çŸ­è·ç¦»å…³ç³»ï¼ˆä¾‹å¦‚ï¼Œâ€œjumpsâ€å’Œâ€œoverâ€æ¥è¿‘ï¼‰ã€‚
- en: 'Think of these additional values as "position vectors":'
  id: totrans-split-77
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: æŠŠè¿™äº›é™„åŠ å€¼æƒ³è±¡æˆâ€œä½ç½®å‘é‡â€ï¼š
- en: '"The": [position 1 vector]'
  id: totrans-split-78
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '"The": [ç¬¬1ä½ç½®å‘é‡]'
- en: '"quick": [position 2 vector]'
  id: totrans-split-79
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '"quick": [ç¬¬2ä½ç½®å‘é‡]'
- en: '"brown": [position 3 vector]'
  id: totrans-split-80
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '"brown": [ç¬¬3ä½ç½®å‘é‡]'
- en: '...'
  id: totrans-split-81
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '...'
- en: '**Combining Embeddings and Positions:**'
  id: totrans-split-82
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**ç»“åˆåµŒå…¥å’Œä½ç½®ï¼š**'
- en: '**Understanding Order:**'
  id: totrans-split-83
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**ç†è§£é¡ºåºï¼š**'
- en: Even if the sentence order changes (e.g., "Dog lazy jumps..."), the position
    vectors ensure relative positions are maintained.
  id: totrans-split-84
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: å³ä½¿å¥å­é¡ºåºæ”¹å˜ï¼ˆä¾‹å¦‚ï¼Œâ€œç‹— æ‡’ è·³...â€ï¼‰ï¼Œä½ç½®å‘é‡ç¡®ä¿ç›¸å¯¹ä½ç½®ä¿æŒä¸å˜ã€‚
- en: The model can still learn that "jumps" is more related to "over" than, say,
    "The".
  id: totrans-split-85
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: æ¨¡å‹ä»ç„¶å¯ä»¥å­¦ä¹ åˆ°â€œjumpsâ€ä¸â€œoverâ€ä¹‹é—´çš„å…³ç³»å¤§äºä¸â€œTheâ€ä¹‹é—´çš„å…³ç³»ã€‚
- en: '* * *'
  id: totrans-split-86
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '* * *'
- en: '**Feed Forward Network(FFN):** This adds a layer of non-linearity, enabling
    the model to learn more complex relationships that might not be easily captured
    by attention mechanisms alone.'
  id: totrans-split-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**å‰é¦ˆç½‘ç»œï¼ˆFFNï¼‰ï¼š** è¿™å¢åŠ äº†ä¸€å±‚éçº¿æ€§ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿå­¦ä¹ åˆ°å•çº¯é€šè¿‡æ³¨æ„æœºåˆ¶éš¾ä»¥æ•æ‰çš„æ›´å¤æ‚å…³ç³»ã€‚'
- en: '* * *'
  id: totrans-split-88
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '* * *'
- en: You've already delved into the sentence through previous layers. You understand
    individual words, their relationships, and their positions. Now, the FFN arrives
    like a detective magnifying glass, ready to uncover intricate details not immediately
    visible.
  id: totrans-split-89
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: ä½ å·²ç»é€šè¿‡ä¹‹å‰çš„å±‚æ·±å…¥æ¢è®¨äº†è¿™ä¸ªå¥å­ã€‚ä½ ç†è§£å•è¯ï¼Œå®ƒä»¬ä¹‹é—´çš„å…³ç³»ä»¥åŠå®ƒä»¬çš„ä½ç½®ã€‚ç°åœ¨ï¼Œå‰é¦ˆç½‘ç»œï¼ˆFFNï¼‰åƒä¾¦æ¢çš„æ”¾å¤§é•œä¸€æ ·ï¼Œå‡†å¤‡æ­ç¤ºä¸æ˜“çœ‹åˆ°çš„å¤æ‚ç»†èŠ‚ã€‚
- en: '**The FFN does this through three key steps:**'
  id: totrans-split-90
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '**FFNé€šè¿‡ä¸‰ä¸ªå…³é”®æ­¥éª¤å®Œæˆè¿™ä¸€è¿‡ç¨‹ï¼š**'
- en: '**Non-linear Transformation:** Instead of straightforward calculations, the
    FFN uses non-linear functions like ReLU to add complexity. Think of it as applying
    a special filter to the existing information, revealing hidden patterns and connections
    that simple arithmetic might miss. This allows the FFN to capture more nuanced
    relationships between words.'
  id: totrans-split-91
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**éçº¿æ€§è½¬æ¢ï¼š** FFNä¸ä½¿ç”¨ç›´æ¥çš„è®¡ç®—ï¼Œè€Œæ˜¯ä½¿ç”¨ReLUç­‰éçº¿æ€§å‡½æ•°å¢åŠ å¤æ‚æ€§ã€‚æŠŠå®ƒçœ‹ä½œæ˜¯å¯¹ç°æœ‰ä¿¡æ¯åº”ç”¨ç‰¹æ®Šè¿‡æ»¤å™¨ï¼Œæ­ç¤ºéšè—çš„æ¨¡å¼å’Œè¿æ¥ï¼Œè¿™ä½¿å¾—FFNèƒ½å¤Ÿæ•æ‰ç®€å•ç®—æœ¯å¯èƒ½å¿½ç•¥çš„æ›´å¾®å¦™çš„å•è¯ä¹‹é—´å…³ç³»ã€‚'
- en: '**Multi-layered Analysis:** The FFN isn''t just one step; it''s typically a
    chain of two or more fully connected layers. Each layer builds upon the previous
    one, transforming the information step-by-step. Imagine you''re examining the
    sentence under increasing magnification, uncovering finer details with each layer.'
  id: totrans-split-92
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**å¤šå±‚åˆ†æï¼š** FFNä¸åªæ˜¯ä¸€ä¸ªæ­¥éª¤ï¼›å®ƒé€šå¸¸æ˜¯ä¸¤ä¸ªæˆ–æ›´å¤šå…¨è¿æ¥å±‚çš„é“¾æ¡ã€‚æ¯ä¸€å±‚éƒ½å»ºç«‹åœ¨å‰ä¸€å±‚ä¹‹ä¸Šï¼Œé€æ­¥è½¬æ¢ä¿¡æ¯ã€‚æƒ³è±¡ä½ åœ¨é€æ¸å¢å¤§çš„æ”¾å¤§å€ç‡ä¸‹å®¡è§†å¥å­ï¼Œæ¯ä¸€å±‚æ­ç¤ºå‡ºæ›´ç²¾ç»†çš„ç»†èŠ‚ã€‚'
- en: '**Dimensionality Shift:** The FFN expands the information''s size (e.g., from
    512 dimensions to 2048) in the first layer. This allows it to analyze a wider
    range of features and capture more complex patterns. Think of it as spreading
    out the information on a larger canvas for deeper examination. Then, it contracts
    it back to the original size (e.g., 512 again) in the final layer to ensure compatibility
    with subsequent layers.'
  id: totrans-split-93
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**ç»´åº¦è½¬æ¢ï¼š** FFNåœ¨ç¬¬ä¸€å±‚æ‰©å±•ä¿¡æ¯çš„å¤§å°ï¼ˆä¾‹å¦‚ï¼Œä»512ç»´åˆ°2048ç»´ï¼‰ã€‚è¿™ä½¿å¾—å®ƒèƒ½å¤Ÿåˆ†ææ›´å¹¿æ³›çš„ç‰¹å¾å¹¶æ•æ‰æ›´å¤æ‚çš„æ¨¡å¼ã€‚å¯ä»¥å°†å…¶æƒ³è±¡ä¸ºåœ¨æ›´å¤§çš„ç”»å¸ƒä¸Šå±•å¼€ä¿¡æ¯è¿›è¡Œæ·±å…¥æ£€æŸ¥ã€‚ç„¶åï¼Œåœ¨æœ€åä¸€å±‚å°†å…¶æ”¶ç¼©å›åŸå§‹å¤§å°ï¼ˆä¾‹å¦‚ï¼Œå†æ¬¡ä¸º512ç»´ï¼‰ï¼Œä»¥ç¡®ä¿ä¸åç»­å±‚å…¼å®¹ã€‚'
- en: '**Applying this to our sentence:**'
  id: totrans-split-94
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '**å°†æ­¤åº”ç”¨åˆ°æˆ‘ä»¬çš„å¥å­ä¸­ï¼š**'
- en: Imagine the FFN helps identify that "quick" and "brown" not only describe the
    "fox" but also subtly connect to its perceived speed through their combined meaning.
  id: totrans-split-95
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: æƒ³è±¡ä¸€ä¸‹ï¼Œå‰é¦ˆç½‘ç»œï¼ˆFFNï¼‰å¸®åŠ©è¯†åˆ«â€œquickâ€å’Œâ€œbrownâ€ä¸ä»…æè¿°â€œfoxâ€ï¼Œè€Œä¸”é€šè¿‡å®ƒä»¬çš„è”åˆæ„ä¹‰å¾®å¦™åœ°è¿æ¥åˆ°å®ƒçš„æ„ŸçŸ¥é€Ÿåº¦ã€‚
- en: Or, it might delve deeper into the relationship between "jumps" and "over,"
    understanding the action and spatial context beyond just their individual definitions.
  id: totrans-split-96
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: æˆ–è€…ï¼Œå®ƒå¯èƒ½æ·±å…¥æ¢è®¨â€œjumpsâ€å’Œâ€œoverâ€ä¹‹é—´çš„å…³ç³»ï¼Œç†è§£åŠ¨ä½œå’Œç©ºé—´èƒŒæ™¯ï¼Œä¸ä»…ä»…æ˜¯å®ƒä»¬å„è‡ªçš„å®šä¹‰ã€‚
- en: '* * *'
  id: totrans-split-97
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '* * *'
- en: '**Repeat, Refine, Repeat:** These layers (self-attention, multi-head attention,
    etc.) are stacked and repeated multiple times. With each iteration, the encoder
    refines its understanding, building a comprehensive representation of the input
    text.'
  id: totrans-split-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**é‡å¤ã€ç²¾ç‚¼ã€å†é‡å¤ï¼š** è¿™äº›å±‚ï¼ˆè‡ªæ³¨æ„åŠ›ã€å¤šå¤´æ³¨æ„åŠ›ç­‰ï¼‰ä¼šè¢«å †å å’Œé‡å¤å¤šæ¬¡ã€‚æ¯æ¬¡è¿­ä»£æ—¶ï¼Œç¼–ç å™¨éƒ½ä¼šç²¾ç‚¼å…¶ç†è§£ï¼Œæ„å»ºè¾“å…¥æ–‡æœ¬çš„å…¨é¢è¡¨ç¤ºã€‚'
- en: 'image source: pillow lab blog'
  id: totrans-split-99
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ç‰‡æ¥æºï¼špillow lab åšå®¢
- en: '**Decoder: Weaving the Output Tapestry**'
  id: totrans-split-100
  prefs: []
  type: TYPE_NORMAL
  zh: '**è§£ç å™¨ï¼šç¼–ç»‡è¾“å‡ºçš„å›¾æ™¯**'
- en: 'Now, the decoder takes the baton. But unlike the encoder, it has an additional
    challenge: generating the output word by word without peeking at the future. To
    achieve this, it utilizes:'
  id: totrans-split-101
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨ï¼Œè§£ç å™¨æ¥è¿‡æ¥åŠ›æ£’ã€‚ä½†ä¸ç¼–ç å™¨ä¸åŒçš„æ˜¯ï¼Œå®ƒé¢ä¸´é¢å¤–çš„æŒ‘æˆ˜ï¼šé€å­—ç”Ÿæˆè¾“å‡ºï¼Œä¸èƒ½çª¥è§†æœªæ¥ã€‚ä¸ºäº†å®ç°è¿™ä¸€ç‚¹ï¼Œå®ƒåˆ©ç”¨ä»¥ä¸‹æœºåˆ¶ï¼š
- en: '**Masked Self-Attention:** Similar to the encoder''s self-attention, but with
    a twist. The decoder only attends to previously generated words, ensuring it doesn''t
    cheat and use future information. It''s like writing a story one sentence at a
    time, without knowing how it ends.'
  id: totrans-split-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**æ©ç è‡ªæ³¨æ„åŠ›ï¼š** ç±»ä¼¼äºç¼–ç å™¨çš„è‡ªæ³¨æ„åŠ›ï¼Œä½†æœ‰æ‰€ä¸åŒã€‚è§£ç å™¨åªå…³æ³¨å…ˆå‰ç”Ÿæˆçš„å•è¯ï¼Œç¡®ä¿ä¸ä½œå¼Šå’Œä½¿ç”¨æœªæ¥ä¿¡æ¯ã€‚è¿™å°±åƒä¸€æ¬¡åªå†™ä¸€å¥è¯çš„æ•…äº‹ï¼Œä¸çŸ¥é“å®ƒå¦‚ä½•ç»“æŸã€‚'
- en: '**Encoder-Decoder Attention:** This mechanism allows the decoder to consult
    the encoded input, like referring back to a reference document while writing.
    It ensures the generated output stays coherent and aligned with the original text.'
  id: totrans-split-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**ç¼–ç å™¨-è§£ç å™¨æ³¨æ„åŠ›ï¼š** è¿™ä¸ªæœºåˆ¶å…è®¸è§£ç å™¨å‚è€ƒç¼–ç çš„è¾“å…¥ï¼Œå°±åƒå†™ä½œæ—¶å‚è€ƒå‚è€ƒæ–‡çŒ®ä¸€æ ·ã€‚å®ƒç¡®ä¿ç”Ÿæˆçš„è¾“å‡ºä¿æŒè¿è´¯å¹¶ä¸åŸå§‹æ–‡æœ¬å¯¹é½ã€‚'
- en: '**Multi-Head Attention and Feed Forward Network:** Just like the encoder, these
    layers help the decoder refine its understanding of the context and relationships
    within the text.'
  id: totrans-split-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**å¤šå¤´æ³¨æ„åŠ›å’Œå‰é¦ˆç½‘ç»œï¼š** å°±åƒç¼–ç å™¨ä¸€æ ·ï¼Œè¿™äº›å±‚å¸®åŠ©è§£ç å™¨ç²¾ç‚¼å…¶å¯¹æ–‡æœ¬å†…ä¸Šä¸‹æ–‡å’Œå…³ç³»çš„ç†è§£ã€‚'
- en: '**Output Layer:** Finally, the decoder translates its internal representation
    into the actual output word, one by one. It''s like the final assembly line, putting
    the pieces together to form the desired outcome.'
  id: totrans-split-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**è¾“å‡ºå±‚ï¼š** æœ€åï¼Œè§£ç å™¨å°†å…¶å†…éƒ¨è¡¨ç¤ºé€ä¸ªç¿»è¯‘ä¸ºå®é™…è¾“å‡ºè¯ã€‚è¿™å°±åƒæœ€ç»ˆçš„è£…é…çº¿ï¼Œå°†å„ä¸ªéƒ¨ä»¶ç»„è£…èµ·æ¥å½¢æˆæœŸæœ›çš„ç»“æœã€‚'
- en: '**Beyond the Basics:**'
  id: totrans-split-106
  prefs: []
  type: TYPE_NORMAL
  zh: '**è¶…è¶ŠåŸºç¡€ï¼š**'
- en: Remember, this is just a glimpse into the fascinating world of transformers.
    The specific architecture can vary depending on the task and dataset, with different
    numbers of layers and configurations.
  id: totrans-split-107
  prefs: []
  type: TYPE_NORMAL
  zh: è®°ä½ï¼Œè¿™åªæ˜¯è½¬æ¢å™¨å¼•äººå…¥èƒœä¸–ç•Œçš„ä¸€ç¥ã€‚å…·ä½“çš„æ¶æ„å¯èƒ½å› ä»»åŠ¡å’Œæ•°æ®é›†è€Œå¼‚ï¼Œå…·æœ‰ä¸åŒæ•°é‡çš„å±‚å’Œé…ç½®ã€‚
- en: Additionally, each layer involves complex mathematical operations that go beyond
    the scope of this explanation.
  id: totrans-split-108
  prefs: []
  type: TYPE_NORMAL
  zh: æ­¤å¤–ï¼Œæ¯ä¸€å±‚éƒ½æ¶‰åŠå¤æ‚çš„æ•°å­¦æ“ä½œï¼Œè¶…å‡ºæœ¬è§£é‡Šçš„èŒƒå›´ã€‚
- en: But hopefully, this has equipped you with a fundamental understanding of how
    transformers work and why they have revolutionized the field of NLP.
  id: totrans-split-109
  prefs: []
  type: TYPE_NORMAL
  zh: ä½†å¸Œæœ›è¿™è®©ä½ å¯¹è½¬æ¢å™¨å¦‚ä½•å·¥ä½œåŠå…¶ä¸ºä½•é©æ–°è‡ªç„¶è¯­è¨€å¤„ç†é¢†åŸŸæœ‰äº†åŸºæœ¬ç†è§£ã€‚
- en: So, the next time you encounter a seamless machine translation or marvel at
    the creativity of an AI-powered text generator, remember the intricate dance of
    the encoder and decoder within the transformer, weaving magic with the power of
    attention and parallel processing.
  id: totrans-split-110
  prefs: []
  type: TYPE_NORMAL
  zh: å› æ­¤ï¼Œä¸‹æ¬¡ä½ é‡åˆ°æ— ç¼çš„æœºå™¨ç¿»è¯‘æˆ–è€…å¯¹AIé©±åŠ¨çš„æ–‡æœ¬ç”Ÿæˆå™¨çš„åˆ›é€ åŠ›æ„Ÿåˆ°æƒŠå¹æ—¶ï¼Œè¯·è®°ä½å˜å‹å™¨å†…ç¼–ç å™¨å’Œè§£ç å™¨çš„å¤æ‚èˆè¹ˆï¼Œç”¨æ³¨æ„åŠ›å’Œå¹¶è¡Œå¤„ç†çš„åŠ›é‡ç¼–ç»‡é­”æ³•ã€‚
- en: '*Paper: https://arxiv.org/abs/1706.03762*'
  id: totrans-split-111
  prefs: []
  type: TYPE_NORMAL
  zh: '*è®ºæ–‡ï¼šhttps://arxiv.org/abs/1706.03762*'
- en: Thank you for reading The ZenMode. This post is public so feel free to share
    it.
  id: totrans-split-112
  prefs: []
  type: TYPE_NORMAL
  zh: æ„Ÿè°¢æ‚¨é˜…è¯»ã€ŠZenModeã€‹ã€‚æœ¬æ–‡æ˜¯å…¬å¼€çš„ï¼Œæ‰€ä»¥è¯·éšæ„åˆ†äº«ã€‚
- en: '[Share](https://nintyzeros.substack.com/p/how-do-transformer-workdesign-a-multi?utm_source=substack&utm_medium=email&utm_content=share&action=share)'
  id: totrans-split-113
  prefs: []
  type: TYPE_NORMAL
  zh: '[åˆ†äº«](https://nintyzeros.substack.com/p/how-do-transformer-workdesign-a-multi?utm_source=substack&utm_medium=email&utm_content=share&action=share)'
- en: '* * *'
  id: totrans-split-114
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
