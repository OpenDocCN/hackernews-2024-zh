- en: <!--yml
  id: totrans-split-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-split-1
  prefs: []
  type: TYPE_NORMAL
  zh: 类别：未分类
- en: 'date: 2024-05-27 13:27:36'
  id: totrans-split-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-05-27 13:27:36
- en: -->
  id: totrans-split-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: microsoft/Phi-3-mini-128k-instruct-onnx · Hugging Face
  id: totrans-split-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: microsoft/Phi-3-mini-128k-instruct-onnx · Hugging Face
- en: 来源：[https://huggingface.co/microsoft/Phi-3-mini-128k-instruct-onnx](https://huggingface.co/microsoft/Phi-3-mini-128k-instruct-onnx)
  id: totrans-split-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://huggingface.co/microsoft/Phi-3-mini-128k-instruct-onnx](https://huggingface.co/microsoft/Phi-3-mini-128k-instruct-onnx)
- en: '[](#phi-3-mini-128k-instruct-onnx-models)Phi-3 Mini-128K-Instruct ONNX models'
  id: totrans-split-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '[](#phi-3-mini-128k-instruct-onnx-models)Phi-3 Mini-128K-Instruct ONNX 模型'
- en: This repository hosts the optimized versions of [Phi-3-mini-128k-instruct](https://aka.ms/phi3-mini-128k-instruct)
    to accelerate inference with ONNX Runtime.
  id: totrans-split-7
  prefs: []
  type: TYPE_NORMAL
  zh: 该存储库托管了优化版本的 [Phi-3-mini-128k-instruct](https://aka.ms/phi3-mini-128k-instruct)，以加速使用
    ONNX Runtime 的推理过程。
- en: 'Phi-3 Mini is a lightweight, state-of-the-art open model built upon datasets
    used for Phi-2 - synthetic data and filtered websites - with a focus on very high-quality,
    reasoning dense data. The model belongs to the Phi-3 model family, and the mini
    version comes in two variants: 4K and 128K which is the context length (in tokens)
    it can support. The model underwent a rigorous enhancement process, incorporating
    both supervised fine-tuning and direct preference optimization to ensure precise
    instruction adherence and robust safety measures.'
  id: totrans-split-8
  prefs: []
  type: TYPE_NORMAL
  zh: Phi-3 Mini 是一个轻量级、最新开放模型，基于 Phi-2 使用的数据集构建 - 合成数据和经过筛选的网站 - 重点关注非常高质量、推理密集的数据。该模型属于
    Phi-3 模型系列，迷你版本有两个变体：4K 和 128K（这是它能支持的上下文长度（以标记计））。该模型经过严格的增强过程，包括监督的微调和直接的偏好优化，以确保精确的指令遵循和稳健的安全措施。
- en: Optimized Phi-3 Mini models are published here in [ONNX](https://onnx.ai) format
    to run with [ONNX Runtime](https://onnxruntime.ai/) on CPU and GPU across devices,
    including server platforms, Windows, Linux and Mac desktops, and mobile CPUs,
    with the precision best suited to each of these targets.
  id: totrans-split-9
  prefs: []
  type: TYPE_NORMAL
  zh: 优化的 Phi-3 Mini 模型以 [ONNX](https://onnx.ai) 格式发布，以便在包括服务器平台、Windows、Linux 和 Mac
    桌面以及移动 CPU 在内的设备上通过 [ONNX Runtime](https://onnxruntime.ai/) 运行，每个目标都选择最适合的精度。
- en: '[DirectML](https://aka.ms/directml) support lets developers bring hardware
    acceleration to Windows devices at scale across AMD, Intel, and NVIDIA GPUs. Along
    with DirectML, ONNX Runtime provides cross platform support for Phi-3 Mini across
    a range of devices for CPU, GPU, and mobile.'
  id: totrans-split-10
  prefs: []
  type: TYPE_NORMAL
  zh: '[DirectML](https://aka.ms/directml) 支持使开发人员能够在 Windows 设备上扩展硬件加速，覆盖 AMD、Intel
    和 NVIDIA GPU。除 DirectML 外，ONNX Runtime 还为各种设备提供跨平台支持，适用于 CPU、GPU 和移动设备上的 Phi-3
    Mini。'
- en: To easily get started with Phi-3, you can use our newly introduced ONNX Runtime
    Generate() API. See [here](https://aka.ms/generate-tutorial) for instructions
    on how to run it.
  id: totrans-split-11
  prefs: []
  type: TYPE_NORMAL
  zh: 要轻松开始使用 Phi-3，您可以使用我们新推出的 ONNX Runtime Generate() API。请参阅 [此处](https://aka.ms/generate-tutorial)
    获取有关如何运行的说明。
- en: '[](#onnx-models)ONNX Models'
  id: totrans-split-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '[](#onnx-models)ONNX 模型'
- en: 'Here are some of the optimized configurations we have added:'
  id: totrans-split-13
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是我们添加的一些优化配置：
- en: 'ONNX model for int4 DML: ONNX model for AMD, Intel, and NVIDIA GPUs on Windows,
    quantized to int4 using [AWQ](https://arxiv.org/abs/2306.00978).'
  id: totrans-split-14
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: int4 DML 的 ONNX 模型：适用于 AMD、Intel 和 NVIDIA GPU 上的 Windows，使用 [AWQ](https://arxiv.org/abs/2306.00978)
    进行 int4 量化。
- en: 'ONNX model for fp16 CUDA: ONNX model you can use to run for your NVIDIA GPUs.'
  id: totrans-split-15
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: fp16 CUDA 的 ONNX 模型：你可以用于 NVIDIA GPU 运行的 ONNX 模型。
- en: 'ONNX model for int4 CUDA: ONNX model for NVIDIA GPUs using int4 quantization
    via RTN.'
  id: totrans-split-16
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: int4 CUDA 的 ONNX 模型：使用 RTN 进行 NVIDIA GPU 的 int4 量化。
- en: 'ONNX model for int4 CPU and Mobile: ONNX model for your CPU and Mobile, using
    int4 quantization via RTN. There are two versions uploaded to balance latency
    vs. accuracy. Acc=1 is targeted at improved accuracy, while Acc=4 is for improved
    perf. For mobile devices, we recommend using the model with acc-level-4.'
  id: totrans-split-17
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: int4 CPU 和移动设备的 ONNX 模型：用于 CPU 和移动设备的 ONNX 模型，使用 RTN 进行 int4 量化。已上传两个版本，以平衡延迟和准确性。Acc=1
    旨在提高准确性，而 Acc=4 则是为了提高性能。对于移动设备，我们建议使用具有 acc-level-4 的模型。
- en: More updates on AMD, and additional optimizations on CPU and Mobile will be
    added with the official ORT 1.18 release in early May. Stay tuned!
  id: totrans-split-18
  prefs: []
  type: TYPE_NORMAL
  zh: 更多关于 AMD 的更新，以及在正式发布 ORT 1.18 版本时对 CPU 和移动设备的额外优化将会添加进来。请关注！
- en: '[](#hardware-supported)Hardware Supported'
  id: totrans-split-19
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '[](#hardware-supported)硬件支持'
- en: 'The models are tested on:'
  id: totrans-split-20
  prefs: []
  type: TYPE_NORMAL
  zh: 这些模型已在以下设备上进行了测试：
- en: 'GPU SKU: RTX 4090 (DirectML)'
  id: totrans-split-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: GPU SKU：RTX 4090（DirectML）
- en: 'GPU SKU: 1 A100 80GB GPU, SKU: Standard_ND96amsr_A100_v4 (CUDA)'
  id: totrans-split-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: GPU SKU：1 A100 80GB GPU，SKU：Standard_ND96amsr_A100_v4（CUDA）
- en: 'CPU SKU: Standard F64s v2 (64 vcpus, 128 GiB memory)'
  id: totrans-split-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: CPU SKU：标准 F64s v2（64 个 vCPU，128 GiB 内存）
- en: 'Mobile SKU: Samsung Galaxy S21'
  id: totrans-split-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 移动 SKU：三星 Galaxy S21
- en: 'Minimum Configuration Required:'
  id: totrans-split-25
  prefs: []
  type: TYPE_NORMAL
  zh: 最低配置要求：
- en: 'Windows: DirectX 12-capable GPU and a minimum of 4GB of combined RAM'
  id: totrans-split-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Windows：需要 DirectX 12 兼容 GPU 和至少 4GB 的组合内存
- en: 'CUDA: NVIDIA GPU with [Compute Capability](https://developer.nvidia.com/cuda-gpus)
    >= 7.0'
  id: totrans-split-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: CUDA：具有[Compute Capability](https://developer.nvidia.com/cuda-gpus) >= 7.0的NVIDIA
    GPU
- en: '[](#model-description)Model Description'
  id: totrans-split-28
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '[](#model-description)模型描述'
- en: '**Developed by:** Microsoft'
  id: totrans-split-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**开发者：** Microsoft'
- en: '**Model type:** ONNX'
  id: totrans-split-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**模型类型：** ONNX'
- en: '**Language(s) (NLP):** Python, C, C++'
  id: totrans-split-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**语言（NLP）：** Python，C，C++'
- en: '**License:** MIT'
  id: totrans-split-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**许可证：** MIT'
- en: '**Model Description:** This is a conversion of the Phi-3 Mini-4K-Instruct model
    for ONNX Runtime inference.'
  id: totrans-split-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**模型描述：** 这是Phi-3 Mini-4K-Instruct模型为ONNX Runtime推理转换的结果。'
- en: '[](#additional-details)Additional Details'
  id: totrans-split-34
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '[](#additional-details)附加细节'
- en: '[](#how-to-get-started-with-the-model)How to Get Started with the Model'
  id: totrans-split-35
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '[](#how-to-get-started-with-the-model)如何开始使用该模型'
- en: To make running of the Phi-3 models across a range of devices and platforms
    across various execution provider backends possible, we introduce a new API to
    wrap several aspects of generative AI inferencing. This API make it easy to drag
    and drop LLMs straight into your app. For running the early version of these models
    with ONNX Runtime, follow the steps [here](http://aka.ms/generate-tutorial).
  id: totrans-split-36
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使Phi-3模型在多种设备和平台上使用各种执行提供程序后端变得可能，我们引入了一个新的API来包装生成式AI推理的几个方面。该API使得将LLM直接拖放到您的应用程序中变得非常容易。要在ONNX
    Runtime中运行这些模型的早期版本，请按照这里的步骤进行操作[here](http://aka.ms/generate-tutorial)。
- en: 'For example:'
  id: totrans-split-37
  prefs: []
  type: TYPE_NORMAL
  zh: 例如：
- en: '[PRE0]'
  id: totrans-split-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '[PRE1]'
  id: totrans-split-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '[](#performance-metrics)Performance Metrics'
  id: totrans-split-40
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '[](#performance-metrics)性能指标'
- en: Phi-3 Mini-128K-Instruct performs better in ONNX Runtime than PyTorch for all
    batch size, prompt length combinations. For FP16 CUDA, ORT performs up to 5X faster
    than PyTorch, while with INT4 CUDA it's up to 9X faster than PyTorch.
  id: totrans-split-41
  prefs: []
  type: TYPE_NORMAL
  zh: Phi-3 Mini-128K-Instruct在所有批次大小和提示长度组合中，比PyTorch在ONNX Runtime中表现更好。对于FP16 CUDA，ORT的性能比PyTorch快了多达5倍，而对于INT4
    CUDA，它比PyTorch快了多达9倍。
- en: 'The table below shows the average throughput of the first 256 tokens generated
    (tps) for FP16 and INT4 precisions on CUDA as measured on [1 A100 80GB GPU, SKU:
    Standard_ND96amsr_A100_v4](https://learn.microsoft.com/en-us/azure/virtual-machines/ndm-a100-v4-series).'
  id: totrans-split-42
  prefs: []
  type: TYPE_NORMAL
  zh: 下表显示了在CUDA上使用FP16和INT4精度测量的前256个生成的平均吞吐量（tps），使用的硬件是[1 A100 80GB GPU，SKU：Standard_ND96amsr_A100_v4](https://learn.microsoft.com/en-us/azure/virtual-machines/ndm-a100-v4-series)。
- en: '| Batch Size, Prompt Length | ORT FP16 CUDA | PyTorch Eager FP16 CUDA | FP16
    CUDA Speed Up (ORT/PyTorch) |'
  id: totrans-split-43
  prefs: []
  type: TYPE_TB
  zh: '| 批次大小，提示长度 | ORT FP16 CUDA | PyTorch Eager FP16 CUDA | FP16 CUDA加速比（ORT/PyTorch）
    |'
- en: '| --- | --- | --- | --- |'
  id: totrans-split-44
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| 1, 16 | 134.46 | 25.35 | 5.30 |'
  id: totrans-split-45
  prefs: []
  type: TYPE_TB
  zh: '| 1, 16 | 134.46 | 25.35 | 5.30 |'
- en: '| 1, 64 | 132.21 | 25.69 | 5.15 |'
  id: totrans-split-46
  prefs: []
  type: TYPE_TB
  zh: '| 1, 64 | 132.21 | 25.69 | 5.15 |'
- en: '| 1, 256 | 124.51 | 25.77 | 4.83 |'
  id: totrans-split-47
  prefs: []
  type: TYPE_TB
  zh: '| 1, 256 | 124.51 | 25.77 | 4.83 |'
- en: '| 1, 1024 | 110.03 | 25.73 | 4.28 |'
  id: totrans-split-48
  prefs: []
  type: TYPE_TB
  zh: '| 1, 1024 | 110.03 | 25.73 | 4.28 |'
- en: '| 1, 2048 | 96.93 | 25.72 | 3.77 |'
  id: totrans-split-49
  prefs: []
  type: TYPE_TB
  zh: '| 1, 2048 | 96.93 | 25.72 | 3.77 |'
- en: '| 1, 4096 | 62.12 | 25.66 | 2.42 |'
  id: totrans-split-50
  prefs: []
  type: TYPE_TB
  zh: '| 1, 4096 | 62.12 | 25.66 | 2.42 |'
- en: '| 4, 16 | 521.10 | 101.31 | 5.14 |'
  id: totrans-split-51
  prefs: []
  type: TYPE_TB
  zh: '| 4, 16 | 521.10 | 101.31 | 5.14 |'
- en: '| 4, 64 | 507.03 | 101.66 | 4.99 |'
  id: totrans-split-52
  prefs: []
  type: TYPE_TB
  zh: '| 4, 64 | 507.03 | 101.66 | 4.99 |'
- en: '| 4, 256 | 459.47 | 101.15 | 4.54 |'
  id: totrans-split-53
  prefs: []
  type: TYPE_TB
  zh: '| 4, 256 | 459.47 | 101.15 | 4.54 |'
- en: '| 4, 1024 | 343.60 | 101.09 | 3.40 |'
  id: totrans-split-54
  prefs: []
  type: TYPE_TB
  zh: '| 4, 1024 | 343.60 | 101.09 | 3.40 |'
- en: '| 4, 2048 | 264.81 | 100.78 | 2.63 |'
  id: totrans-split-55
  prefs: []
  type: TYPE_TB
  zh: '| 4, 2048 | 264.81 | 100.78 | 2.63 |'
- en: '| 4, 4096 | 158.00 | 77.98 | 2.03 |'
  id: totrans-split-56
  prefs: []
  type: TYPE_TB
  zh: '| 4, 4096 | 158.00 | 77.98 | 2.03 |'
- en: '| 16, 16 | 1689.08 | 394.19 | 4.28 |'
  id: totrans-split-57
  prefs: []
  type: TYPE_TB
  zh: '| 16, 16 | 1689.08 | 394.19 | 4.28 |'
- en: '| 16, 64 | 1567.13 | 394.29 | 3.97 |'
  id: totrans-split-58
  prefs: []
  type: TYPE_TB
  zh: '| 16, 64 | 1567.13 | 394.29 | 3.97 |'
- en: '| 16, 256 | 1232.10 | 405.30 | 3.04 |'
  id: totrans-split-59
  prefs: []
  type: TYPE_TB
  zh: '| 16, 256 | 1232.10 | 405.30 | 3.04 |'
- en: '| 16, 1024 | 680.61 | 294.79 | 2.31 |'
  id: totrans-split-60
  prefs: []
  type: TYPE_TB
  zh: '| 16, 1024 | 680.61 | 294.79 | 2.31 |'
- en: '| 16, 2048 | 350.77 | 203.02 | 1.73 |'
  id: totrans-split-61
  prefs: []
  type: TYPE_TB
  zh: '| 16, 2048 | 350.77 | 203.02 | 1.73 |'
- en: '| 16, 4096 | 192.36 | OOM |  |'
  id: totrans-split-62
  prefs: []
  type: TYPE_TB
  zh: '| 16, 4096 | 192.36 | OOM |  |'
- en: '| Batch Size, Prompt Length | PyTorch Eager INT4 CUDA | INT4 CUDA Speed Up
    (ORT/PyTorch) |'
  id: totrans-split-63
  prefs: []
  type: TYPE_TB
  zh: '| 批次大小，提示长度 | PyTorch Eager INT4 CUDA | INT4 CUDA加速比（ORT/PyTorch） |'
- en: '| --- | --- | --- |'
  id: totrans-split-64
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| 1, 16 | 25.35 | 8.89 |'
  id: totrans-split-65
  prefs: []
  type: TYPE_TB
  zh: '| 1, 16 | 25.35 | 8.89 |'
- en: '| 1, 64 | 25.69 | 8.58 |'
  id: totrans-split-66
  prefs: []
  type: TYPE_TB
  zh: '| 1, 64 | 25.69 | 8.58 |'
- en: '| 1, 256 | 25.77 | 7.69 |'
  id: totrans-split-67
  prefs: []
  type: TYPE_TB
  zh: '| 1, 256 | 25.77 | 7.69 |'
- en: '| 1, 1024 | 25.73 | 6.34 |'
  id: totrans-split-68
  prefs: []
  type: TYPE_TB
  zh: '| 1, 1024 | 25.73 | 6.34 |'
- en: '| 1, 2048 | 25.72 | 5.24 |'
  id: totrans-split-69
  prefs: []
  type: TYPE_TB
  zh: '| 1, 2048 | 25.72 | 5.24 |'
- en: '| 1, 4096 | 25.66 | 2.97 |'
  id: totrans-split-70
  prefs: []
  type: TYPE_TB
  zh: '| 1, 4096 | 25.66 | 2.97 |'
- en: '| 4, 16 | 101.31 | 2.82 |'
  id: totrans-split-71
  prefs: []
  type: TYPE_TB
  zh: '| 4, 16 | 101.31 | 2.82 |'
- en: '| 4, 64 | 101.66 | 2.77 |'
  id: totrans-split-72
  prefs: []
  type: TYPE_TB
  zh: '| 4, 64 | 101.66 | 2.77 |'
- en: '| 4, 256 | 101.15 | 2.64 |'
  id: totrans-split-73
  prefs: []
  type: TYPE_TB
  zh: '| 4, 256 | 101.15 | 2.64 |'
- en: '| 4, 1024 | 101.09 | 2.20 |'
  id: totrans-split-74
  prefs: []
  type: TYPE_TB
  zh: '| 4, 1024 | 101.09 | 2.20 |'
- en: '| 4, 2048 | 100.78 | 1.84 |'
  id: totrans-split-75
  prefs: []
  type: TYPE_TB
  zh: '| 4, 2048 | 100.78 | 1.84 |'
- en: '| 4, 4096 | 77.98 | 1.62 |'
  id: totrans-split-76
  prefs: []
  type: TYPE_TB
  zh: '| 4, 4096 | 77.98 | 1.62 |'
- en: '| 16, 16 | 394.19 | 2.52 |'
  id: totrans-split-77
  prefs: []
  type: TYPE_TB
  zh: '| 16, 16 | 394.19 | 2.52 |'
- en: '| 16, 64 | 394.29 | 2.41 |'
  id: totrans-split-78
  prefs: []
  type: TYPE_TB
  zh: '| 16, 64 | 394.29 | 2.41 |'
- en: '| 16, 256 | 405.30 | 2.00 |'
  id: totrans-split-79
  prefs: []
  type: TYPE_TB
  zh: '| 16, 256 | 405.30 | 2.00 |'
- en: '| 16, 1024 | 294.79 | 1.79 |'
  id: totrans-split-80
  prefs: []
  type: TYPE_TB
  zh: '| 16, 1024 | 294.79 | 1.79 |'
- en: '| 16, 2048 | 203.02 | 1.81 |'
  id: totrans-split-81
  prefs: []
  type: TYPE_TB
  zh: '| 16, 2048 | 203.02 | 1.81 |'
- en: '| 16, 4096 | OOM |  |'
  id: totrans-split-82
  prefs: []
  type: TYPE_TB
  zh: '| 16, 4096 | OOM |  |'
- en: 'Note: PyTorch compile and Llama.cpp currently do not support the Phi-3 Mini-128K-Instruct
    model.'
  id: totrans-split-83
  prefs: []
  type: TYPE_NORMAL
  zh: 注：PyTorch编译和Llama.cpp当前不支持Phi-3 Mini-128K-Instruct模型。
- en: '[](#package-versions)Package Versions'
  id: totrans-split-84
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '[](#package-versions)包版本'
- en: '| Pip package name | Version |'
  id: totrans-split-85
  prefs: []
  type: TYPE_TB
  zh: '| Pip软件包名称 | 版本 |'
- en: '| --- | --- |'
  id: totrans-split-86
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| torch | 2.2.0 |'
  id: totrans-split-87
  prefs: []
  type: TYPE_TB
  zh: '| torch | 2.2.0 |'
- en: '| triton | 2.2.0 |'
  id: totrans-split-88
  prefs: []
  type: TYPE_TB
  zh: '| triton | 2.2.0 |'
- en: '| onnxruntime-gpu | 1.18.0 |'
  id: totrans-split-89
  prefs: []
  type: TYPE_TB
  zh: '| onnxruntime-gpu | 1.18.0 |'
- en: '| onnxruntime-genai | 0.2.0 |'
  id: totrans-split-90
  prefs: []
  type: TYPE_TB
  zh: '| onnxruntime-genai | 0.2.0 |'
- en: '| onnxruntime-genai-cuda | 0.2.0 |'
  id: totrans-split-91
  prefs: []
  type: TYPE_TB
  zh: '| onnxruntime-genai-cuda | 0.2.0 |'
- en: '| onnxruntime-genai-directml | 0.2.0 |'
  id: totrans-split-92
  prefs: []
  type: TYPE_TB
  zh: '| onnxruntime-genai-directml | 0.2.0 |'
- en: '| transformers | 4.39.0 |'
  id: totrans-split-93
  prefs: []
  type: TYPE_TB
  zh: '| transformers | 4.39.0 |'
- en: '| bitsandbytes | 0.42.0 |'
  id: totrans-split-94
  prefs: []
  type: TYPE_TB
  zh: '| bitsandbytes | 0.42.0 |'
- en: '[](#appendix)Appendix'
  id: totrans-split-95
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '[](#appendix)附录'
- en: '[](#activation-aware-quantization)Activation Aware Quantization'
  id: totrans-split-96
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '[](#activation-aware-quantization)激活感知量化'
- en: AWQ works by identifying the top 1% most salient weights that are most important
    for maintaining accuracy and quantizing the remaining 99% of weights. This leads
    to less accuracy loss from quantization compared to many other quantization techniques.
    For more on AWQ, see [here](https://arxiv.org/abs/2306.00978).
  id: totrans-split-97
  prefs: []
  type: TYPE_NORMAL
  zh: AWQ 通过识别对维持准确性最重要的前1%的显著权重，并对其余99%的权重进行量化来工作。与许多其他量化技术相比，这导致了从量化中损失的准确性较少。有关AWQ的更多信息，请参见[这里](https://arxiv.org/abs/2306.00978)。
- en: '[](#model-card-contact)Model Card Contact'
  id: totrans-split-98
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '[](#model-card-contact)模型卡联系人'
- en: parinitarahi, kvaishnavi, natke
  id: totrans-split-99
  prefs: []
  type: TYPE_NORMAL
  zh: parinitarahi, kvaishnavi, natke
- en: '[](#contributors)Contributors'
  id: totrans-split-100
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '[](#contributors)贡献者'
- en: Kunal Vaishnavi, Sunghoon Choi, Yufeng Li, Akshay Sonawane, Sheetal Arun Kadam,
    Rui Ren, Edward Chen, Scott McKay, Ryan Hill, Emma Ning, Natalie Kershaw, Parinita
    Rahi, Patrice Vignola, Chai Chaoweeraprasit, Logan Iyer, Vicente Rivera, Jacques
    Van Rhyn
  id: totrans-split-101
  prefs: []
  type: TYPE_NORMAL
  zh: Kunal Vaishnavi, Sunghoon Choi, Yufeng Li, Akshay Sonawane, Sheetal Arun Kadam,
    Rui Ren, Edward Chen, Scott McKay, Ryan Hill, Emma Ning, Natalie Kershaw, Parinita
    Rahi, Patrice Vignola, Chai Chaoweeraprasit, Logan Iyer, Vicente Rivera, Jacques
    Van Rhyn
