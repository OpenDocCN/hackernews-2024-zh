- en: <!--yml
  id: totrans-split-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-split-1
  prefs: []
  type: TYPE_NORMAL
  zh: 'category: 未分类'
- en: 'date: 2024-05-27 14:58:11'
  id: totrans-split-2
  prefs: []
  type: TYPE_NORMAL
  zh: 'date: 2024-05-27 14:58:11'
- en: -->
  id: totrans-split-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: With the rise of AI, web crawlers are suddenly controversial - The Verge
  id: totrans-split-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 随着人工智能的兴起，网络爬虫突然成为了有争议的话题 - The Verge
- en: 来源：[https://www.theverge.com/24067997/robots-txt-ai-text-file-web-crawlers-spiders](https://www.theverge.com/24067997/robots-txt-ai-text-file-web-crawlers-spiders)
  id: totrans-split-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://www.theverge.com/24067997/robots-txt-ai-text-file-web-crawlers-spiders](https://www.theverge.com/24067997/robots-txt-ai-text-file-web-crawlers-spiders)
- en: For three decades, a tiny text file has kept the internet from chaos. This text
    file has no particular legal or technical authority, and it’s not even particularly
    complicated. It represents a handshake deal between some of the earliest pioneers
    of the internet to respect each other’s wishes and build the internet in a way
    that benefitted everybody. It’s a mini constitution for the internet, written
    in code.
  id: totrans-split-6
  prefs: []
  type: TYPE_NORMAL
  zh: 三十年来，一个微小的文本文件阻止了互联网陷入混乱。这个文本文件没有特定的法律或技术权威，甚至并不特别复杂。它代表了互联网的早期先驱之间的一种握手协议，以尊重彼此的意愿，并以有利于每个人的方式构建互联网。它是互联网的迷你宪法，以代码形式书写。
- en: It’s called robots.txt and is usually located at yourwebsite.com/robots.txt.
    That file allows anyone who runs a website — big or small, cooking blog or multinational
    corporation — to tell the web who’s allowed in and who isn’t. Which search engines
    can index your site? What archival projects can grab a version of your page and
    save it? Can competitors keep tabs on your pages for their own files? You get
    to decide and declare that to the web.
  id: totrans-split-7
  prefs: []
  type: TYPE_NORMAL
  zh: 它被称为robots.txt，通常位于yourwebsite.com/robots.txt。这个文件允许任何运行网站的人 —— 不论是大公司还是小博客，烹饪博客还是跨国企业
    —— 告诉网络谁可以进入，谁不可以。哪些搜索引擎可以索引你的网站？什么存档项目可以抓取你页面的版本并保存它？竞争对手可以监视你的页面用于他们自己的文件吗？这些都由你决定，并向网络宣告。
- en: 'It’s not a perfect system, but it works. Used to, anyway. For decades, the
    main focus of robots.txt was on search engines; you’d let them scrape your site
    and in exchange they’d promise to send people back to you. Now AI has changed
    the equation: companies around the web are using your site and its data to build
    massive sets of training data, in order to build models and products that may
    not acknowledge your existence at all.'
  id: totrans-split-8
  prefs: []
  type: TYPE_NORMAL
  zh: 这不是一个完美的系统，但它确实有效。不过以前是这样。数十年来，robots.txt的主要焦点是搜索引擎；你允许它们抓取你的网站，作为回报，它们会承诺将人们带回你的网站。现在AI改变了这个方程式：全网的公司都在使用你的网站和其数据来构建大量的训练数据，以便建立可能完全不承认你存在的模型和产品。
- en: The robots.txt file governs a give and take; AI feels to many like all take
    and no give. But there’s now so much money in AI, and the technological state
    of the art is changing so fast that many site owners can’t keep up. And the fundamental
    agreement behind robots.txt, and the web as a whole — which for so long amounted
    to “everybody just be cool” — may not be able to keep up either.
  id: totrans-split-9
  prefs: []
  type: TYPE_NORMAL
  zh: robots.txt文件管理的是一种互惠互利的关系；对许多人来说，AI感觉就像是只会索取而不会付出。但现在AI带来了如此多的资金，技术前沿变化如此之快，以至于许多网站所有者无法跟上。而robots.txt背后的基本协议，以及整个网络
    —— 长期以来的“大家都冷静点”的共识 —— 也许也无法跟得上了。
- en: '* * *'
  id: totrans-split-10
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: 'In the early days of the internet, robots went by many names: spiders, crawlers,
    worms, WebAnts, web crawlers. Most of the time, they were built with good intentions.
    Usually it was a developer trying to build a directory of cool new websites, make
    sure their own site was working properly, or build a research database — this
    was 1993 or so, long before search engines were everywhere and in the days when
    you could fit most of the internet on your computer’s hard drive.'
  id: totrans-split-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在互联网早期，机器人有许多名称：蜘蛛、爬虫、虫子、WebAnts、网络爬虫。大多数情况下，它们都是出于善意构建的。通常是开发者试图建立一个酷炫新网站的目录，确保他们自己的网站正常运行，或者建立一个研究数据库
    —— 那时大约是1993年，远在搜索引擎遍地开花之前，当你的电脑硬盘几乎可以装下整个互联网的时代。
- en: 'The only real problem then was the traffic: accessing the internet was slow
    and expensive both for the person seeing a website and the one hosting it. If
    you hosted your website on your computer, as many people did, or on hastily constructed
    server software run through your home internet connection, all it took was a few
    robots overzealously downloading your pages for things to break and the phone
    bill to spike.'
  id: totrans-split-12
  prefs: []
  type: TYPE_NORMAL
  zh: 当时唯一的真正问题是流量：访问互联网对于查看网站的人和托管网站的人来说都很慢且昂贵。如果你把网站托管在自己的电脑上，就像许多人那样，或者通过家庭互联网连接运行匆忙搭建的服务器软件，只需几个过于热心下载你网页的机器人就足以导致事情崩溃，电话费飙升。
- en: 'Over the course of a few months in 1994, a software engineer and developer
    named Martijn Koster, along with a group of other web administrators and developers,
    came up with a solution they called the Robots Exclusion Protocol. The proposal
    was straightforward enough: it asked web developers to add a plain-text file to
    their domain specifying which robots were not allowed to scour their site, or
    listing pages that are off limits to all robots. (Again, this was a time when
    you could maintain a list of every single robot in existence — Koster and a few
    others helpfully did just that.) For robot makers, the deal was even simpler:
    respect the wishes of the text file.'
  id: totrans-split-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在1994年的几个月里，一位名叫马蒂茹恩·科斯特（Martijn Koster）的软件工程师和开发者，与其他一群网络管理员和开发者一起，提出了一种解决方案，他们称之为机器人排除协议（Robots
    Exclusion Protocol）。这个提议相当简单：要求网络开发者在其域中添加一个纯文本文件，指定哪些机器人不被允许扫描他们的网站，或列出对所有机器人禁止访问的页面。（再次强调，那时候你可以维护每一个现存的机器人列表
    —— 科斯特和其他几个人友好地做到了。）对于机器人制造商来说，协议更简单：遵守文本文件的规定。
- en: From the beginning, Koster made clear that he didn’t hate robots, nor did he
    intend to get rid of them. “Robots are one of the few aspects of the web that
    cause operational problems and cause people grief,” he said in an initial email
    to a mailing list called WWW-Talk (which included early-internet pioneers like
    Tim Berners-Lee and Marc Andreessen) in early 1994\. “At the same time they do
    provide useful services.” Koster cautioned against arguing about whether robots
    are good or bad — because it doesn’t matter, they’re here and not going away.
    He was simply trying to design a system that might “minimise the problems and
    may well maximize the benefits.”
  id: totrans-split-14
  prefs: []
  type: TYPE_NORMAL
  zh: 从一开始，科斯特就明确表示他并不讨厌机器人，也不打算摆脱它们。“机器人是网络中少数几个会引起运行问题和给人们带来困扰的方面之一，”他在1994年初向一个名为WWW-Talk的邮件列表（其中包括互联网早期先驱如Tim
    Berners-Lee和Marc Andreessen）发送的一封初始电子邮件中说。“同时它们也提供有用的服务。”科斯特警告说不要争论机器人是好还是坏 ——
    因为这并不重要，它们已经存在，而且不会消失。他只是试图设计一个可能“最大程度地减少问题并可能最大化好处”的系统。
- en: “Robots are one of the few aspects of the web that cause operational problems
    and cause people grief. At the same time, they do provide useful services.”
  id: totrans-split-15
  prefs: []
  type: TYPE_NORMAL
  zh: “机器人是网络中少数几个会引起运行问题和给人们带来困扰的方面之一。同时，它们也提供有用的服务。”
- en: By the summer of that year, his proposal had become a standard — not an official
    one, but more or less a universally accepted one. Koster pinged the WWW-Talk group
    again in June with an update. “In short it is a method of guiding robots away
    from certain areas in a Web server’s URL space, by providing a simple text file
    on the server,” he wrote. “This is especially handy if you have large archives,
    CGI scripts with massive URL subtrees, temporary information, or you simply don’t
    want to serve robots.” He’d set up a topic-specific mailing list, where its members
    had agreed on some basic syntax and structure for those text files, changed the
    file’s name from RobotsNotWanted.txt to a simple robots.txt, and pretty much all
    agreed to support it.
  id: totrans-split-16
  prefs: []
  type: TYPE_NORMAL
  zh: 到了那年夏天，他的提议已经成为一种标准 —— 虽然不是官方标准，但基本上是被普遍接受的。科斯特在六月再次向WWW-Talk组发送了更新信息。“简而言之，这是一种通过在服务器上提供简单的文本文件来指导机器人远离Web服务器URL空间中某些区域的方法，”他写道。“如果你有大型档案、带有大量URL子树的CGI脚本、临时信息或者你根本不想为机器人服务，这将特别方便。”他建立了一个专门的邮件列表，其成员已经就这些文本文件的基本语法和结构达成了一致，将文件的名称从RobotsNotWanted.txt更改为简单的robots.txt，并且几乎所有人都同意支持它。
- en: And for most of the next 30 years, that worked pretty well.
  id: totrans-split-17
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的30年中，这种方法运行得相当不错。
- en: But the internet doesn’t fit on a hard drive anymore, and the robots are vastly
    more powerful. Google uses them to crawl and index the entire web for its search
    engine, which has become the interface to the web and brings the company billions
    of dollars a year. Bing’s crawlers do the same, and Microsoft licenses its database
    to other search engines and companies. The Internet Archive uses a crawler to
    store webpages for posterity. Amazon’s crawlers traipse the web looking for product
    information, and according to a recent antitrust suit, the company uses that information
    to punish sellers who offer better deals away from Amazon. AI companies like OpenAI
    are crawling the web in order to train large language models that could once again
    fundamentally change the way we access and share information.
  id: totrans-split-18
  prefs: []
  type: TYPE_NORMAL
  zh: 但是互联网已经不能再容纳在硬盘中了，而且机器人的能力也大大增强。谷歌利用它们来爬取和索引整个网络，作为其搜索引擎的接口，这使得公司每年赚取数十亿美元。必应的爬虫也是如此，微软将其数据库授权给其他搜索引擎和公司使用。互联网档案馆利用爬虫保存网页供后人参考。亚马逊的爬虫在网络上寻找产品信息，根据最近的反垄断诉讼，该公司利用这些信息惩罚那些在亚马逊之外提供更好交易的卖家。像
    OpenAI 这样的 AI 公司正在爬取网络，以训练大型语言模型，这可能再次从根本上改变我们获取和分享信息的方式。
- en: The ability to download, store, organize, and query the modern internet gives
    any company or developer something like the world’s accumulated knowledge to work
    with. In the last year or so, the rise of AI products like ChatGPT, and the large
    language models underlying them, have made high-quality training data one of the
    internet’s most valuable commodities. That has caused internet providers of all
    sorts to reconsider the value of the data on their servers, and rethink who gets
    access to what. Being too permissive can bleed your website of all its value;
    being too restrictive can make you invisible. And you have to keep making that
    choice with new companies, new partners, and new stakes all the time.
  id: totrans-split-19
  prefs: []
  type: TYPE_NORMAL
  zh: 下载、存储、组织和查询现代互联网的能力使任何公司或开发者能够利用世界积累的知识。在过去的一年左右，像 ChatGPT 这样的 AI 产品及其背后的大型语言模型已经使高质量的训练数据成为互联网最有价值的商品之一。这导致各种互联网服务提供商重新评估其服务器上数据的价值，并重新思考谁能访问这些数据。太放任可能会使你的网站失去所有价值；太严格可能会使你变得无形。你必须在新公司、新合作伙伴和新利益上不断做出这样的选择。
- en: '* * *'
  id: totrans-split-20
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: There are a few breeds of internet robot. You might build a totally innocent
    one to crawl around and make sure all your on-page links still lead to other live
    pages; you might send a much sketchier one around the web harvesting every email
    address or phone number you can find. But the most common one, and the most currently
    controversial, is a simple web crawler. Its job is to find, and download, as much
    of the internet as it possibly can.
  id: totrans-split-21
  prefs: []
  type: TYPE_NORMAL
  zh: 有几种类型的网络机器人。你可能会构建一个完全无害的机器人来爬行并确保所有页面上的链接仍然指向其他活跃页面；你可能会派遣一个更为可疑的机器人在网络上收集每一个电子邮件地址或电话号码。但最常见的，也是当前最具争议的，是简单的网络爬虫。它的任务是尽可能多地找到和下载互联网的内容。
- en: Web crawlers are generally fairly simple. They start on a well-known website,
    like cnn.com or wikipedia.org or health.gov. (If you’re running a general search
    engine, you’ll start with lots of high-quality domains across various subjects;
    if all you care about is sports or cars, you’ll just start with car sites.) The
    crawler downloads that first page and stores it somewhere, then automatically
    clicks on every link on that page, downloads all those, clicks all the links on
    every one, and spreads around the web that way. With enough time and enough computing
    resources, a crawler will eventually find and download billions of webpages.
  id: totrans-split-22
  prefs: []
  type: TYPE_NORMAL
  zh: 网络爬虫通常相当简单。它们从知名网站开始，如 cnn.com 或 wikipedia.org 或 health.gov。（如果你运行一个通用搜索引擎，会从各种主题的高质量域名开始；如果只关心体育或汽车，就直接从汽车网站开始。）爬虫下载第一页并存储在某处，然后自动点击页面上的每个链接，下载所有这些页面，点击每个页面上的所有链接，如此在网络中传播。有足够的时间和计算资源，爬虫最终会找到并下载数十亿个网页。
- en: 'The tradeoff is fairly straightforward: if Google can crawl your page, it can
    index it and show it in search results.'
  id: totrans-split-23
  prefs: []
  type: TYPE_NORMAL
  zh: 这种权衡相当简单：如果谷歌可以爬取你的页面，它就可以对其进行索引并在搜索结果中显示出来。
- en: 'Google estimated in 2019 that more than 500 million websites had a robots.txt
    page dictating whether and what these crawlers are allowed to access. The structure
    of those pages is usually roughly the same: it names a “User-agent,” which refers
    to the name a crawler uses when it identifies itself to a server. Google’s agent
    is Googlebot; Amazon’s is Amazonbot; Bing’s is Bingbot; OpenAI’s is GPTBot. Pinterest,
    LinkedIn, Twitter, and many other sites and services have bots of their own, not
    all of which get mentioned on every page. ([Wikipedia](https://en.wikipedia.org/robots.txt)
    and [Facebook](https://facebook.com/robots.txt) are two platforms with particularly
    thorough robot accounting.) Underneath, the robots.txt page lists sections or
    pages of the site that a given agent is not allowed to access, along with specific
    exceptions that are allowed. If the line just reads “Disallow: /” the crawler
    is not welcome at all.'
  id: totrans-split-24
  prefs: []
  type: TYPE_NORMAL
  zh: 'Google 在2019年估计，超过5亿个网站有一个 robots.txt 页面，规定这些爬虫是否可以访问以及可以访问什么内容。这些页面的结构通常大致相同：它命名了一个“User-agent”，这是爬虫在向服务器标识自己时使用的名称。Google
    的代理是 Googlebot；亚马逊的是 Amazonbot；必应的是 Bingbot；OpenAI 的是 GPTBot。Pinterest、LinkedIn、Twitter
    和许多其他站点和服务都有自己的机器人，不一定在每个页面上都提到。（[维基百科](https://zh.wikipedia.org/robots.txt) 和
    [Facebook](https://facebook.com/robots.txt) 是两个具有特别详细的机器人记录的平台。）在下面，robots.txt
    页面列出了某个代理不允许访问的站点的部分或页面，以及允许的特定例外。如果行中只写“Disallow: /”，则完全不欢迎爬虫。'
- en: It’s been a while since “overloaded servers” were a real concern for most people.
    “Nowadays, it’s usually less about the resources that are used on the website
    and more about personal preferences,” says John Mueller, a search advocate at
    Google. “What do you want to have crawled and indexed and whatnot?”
  id: totrans-split-25
  prefs: []
  type: TYPE_NORMAL
  zh: 对于大多数人来说，“服务器超载”已经不再是一个真正的问题。“现在，通常更多地是关于个人偏好，而不是网站上使用的资源，” Google 的搜索倡导者 John
    Mueller 表示。“你想让哪些内容被爬取和索引？”
- en: 'The biggest question most website owners historically had to answer was whether
    to allow Googlebot to crawl their site. The tradeoff is fairly straightforward:
    if Google can crawl your page, it can index it and show it in search results.
    Any page you want to be Googleable, Googlebot needs to see. (How and where Google
    actually displays that page in search results is of course a completely different
    story.) The question is whether you’re willing to let Google eat some of your
    bandwidth and download a copy of your site in exchange for the visibility that
    comes with search.'
  id: totrans-split-26
  prefs: []
  type: TYPE_NORMAL
  zh: 对大多数网站所有者历史上最大的问题是是否允许 Googlebot 爬取他们的网站。这种权衡相当简单：如果 Google 可以爬取你的页面，它可以索引并在搜索结果中显示。任何你希望在
    Google 中可见的页面，Googlebot 都需要看到。（Google 如何在搜索结果中实际显示该页面是完全不同的故事。）问题在于，你是否愿意让 Google
    消耗一些带宽并下载你网站的副本，以换取搜索带来的可见性。
- en: For most websites, this was an easy trade. “Google is our most important spider,”
    says Medium CEO Tony Stubblebine. Google gets to download all of Medium’s pages,
    “and in exchange we get a significant amount of traffic. It’s win-win. Everyone
    thinks that.” This is the bargain Google made with the internet as a whole, to
    funnel traffic to other websites while selling ads against the search results.
    And Google has, by all accounts, been a good citizen of robots.txt. “Pretty much
    all of the well-known search engines comply with it,” Google’s Mueller says. “They’re
    happy to be able to crawl the web, but they don’t want to annoy people with it…
    it just makes life easier for everyone.”
  id: totrans-split-27
  prefs: []
  type: TYPE_NORMAL
  zh: 对于大多数网站来说，这是一笔易于交易的生意。Medium 的 CEO Tony Stubblebine 表示：“Google 是我们最重要的蜘蛛。” Google
    可以下载 Medium 的所有页面，“作为交换，我们获得了大量的流量。这是双赢。每个人都这么认为。” 这就是 Google 与整个互联网达成的协议，将流量引导到其他网站，同时在搜索结果中销售广告。据所有报告显示，Google
    在 robots.txt 方面表现良好。“几乎所有知名的搜索引擎都遵守它，” Google 的 Mueller 表示。“他们乐于爬取网络，但不想用它来打扰人们……这只是为了让每个人的生活更轻松。”
- en: '* * *'
  id: totrans-split-28
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: In the last year or so, though, the rise of AI has upended that equation. For
    many publishers and platforms, having their data crawled for training data felt
    less like trading and more like stealing. “What we found pretty quickly with the
    AI companies,” Stubblebine says, “is not only was it not an exchange of value,
    we’re getting nothing in return. Literally zero.” When Stubblebine announced last
    fall that Medium [would be blocking AI crawlers](https://blog.medium.com/default-no-to-ai-training-on-your-stories-abb5b4589c8),
    he wrote that “AI companies have leached value from writers in order to spam Internet
    readers.”
  id: totrans-split-29
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，在过去一年左右，人工智能的兴起颠覆了这一方程式。对于许多出版商和平台来说，他们的数据被用于训练数据感觉更像是偷窃而不是交换。“我们很快发现AI公司”，斯塔布尔宾说，“不仅没有交换价值，我们一无所获。真的是零。”当斯塔布尔宾在去年秋天宣布Medium将[屏蔽AI爬虫](https://blog.medium.com/default-no-to-ai-training-on-your-stories-abb5b4589c8)时，他写道，“AI公司已经从作家身上吸取了价值，以便向互联网读者发送垃圾信息。”
- en: Over the last year, a large chunk of the media industry has echoed Stubblebine’s
    sentiment. “We do not believe the current ‘scraping’ of BBC data without our permission
    in order to train Gen AI models is in the public interest,” BBC director of nations
    Rhodri Talfan Davies [wrote last fall](https://www.bbc.co.uk/mediacentre/articles/2023/generative-ai-at-the-bbc/),
    announcing that the BBC would also be blocking OpenAI’s crawler. *The New York
    Times* blocked GPTBot as well, months before launching a suit against OpenAI alleging
    that OpenAI’s models “were built by copying and using millions of *The Times*’s
    copyrighted news articles, in-depth investigations, opinion pieces, reviews, how-to
    guides, and more.” [A study by Ben Welsh](https://palewi.re/docs/news-homepages/openai-gptbot-robotstxt.html),
    the news applications editor at *Reuters*, found that 606 of 1,156 surveyed publishers
    had blocked GPTBot in their robots.txt file.
  id: totrans-split-30
  prefs: []
  type: TYPE_NORMAL
  zh: 在过去的一年里，大部分媒体行业都在回应斯塔布尔宾的观点。BBC国家总监罗德里·塔尔凡·戴维斯在去年秋天写道，[“我们不认为当前未经我们许可而‘爬取’BBC数据以训练Gen
    AI模型符合公共利益”](https://www.bbc.co.uk/mediacentre/articles/2023/generative-ai-at-the-bbc/)，宣布BBC也将屏蔽OpenAI的爬虫。*纽约时报*几个月前也屏蔽了GPTBot，并在之后对OpenAI提起诉讼，指控OpenAI的模型“通过复制和使用数百万*时报*的受版权保护的新闻文章、深度调查、观点文章、评论、操作指南等建立”。[由本·威尔士进行的一项研究](https://palewi.re/docs/news-homepages/openai-gptbot-robotstxt.html)，*路透社*的新闻应用编辑，发现在1,156家受调查的出版商中，有606家在其robots.txt文件中屏蔽了GPTBot。
- en: It’s not just publishers, either. Amazon, Facebook, Pinterest, WikiHow, WebMD,
    and many other platforms explicitly block GPTBot from accessing some or all of
    their websites. On most of these robots.txt pages, OpenAI’s GPTBot is the only
    crawler explicitly and completely disallowed. But there are plenty of other AI-specific
    bots beginning to crawl the web, like Anthropic’s anthropic-ai and Google’s new
    Google-Extended. According to a study from last fall by Originality.AI, 306 of
    the top 1,000 sites on the web blocked GPTBot, but only 85 blocked Google-Extended
    and 28 blocked anthropic-ai.
  id: totrans-split-31
  prefs: []
  type: TYPE_NORMAL
  zh: 不仅仅是出版商，亚马逊、Facebook、Pinterest、WikiHow、WebMD等许多平台明确禁止GPTBot访问它们的一些或所有网站。在这些robots.txt页面中，OpenAI的GPTBot是唯一明确和完全不允许的爬虫。但是还有许多其他专门用于AI的机器人开始爬行网络，如Anthropic的anthropic-ai和Google的新Google-Extended。根据Originality.AI去年秋天的一项研究，排名前1,000的网站中有306家屏蔽了GPTBot，但只有85家屏蔽了Google-Extended，28家屏蔽了anthropic-ai。
- en: There are also crawlers used for both web search and AI. CCBot, which is run
    by the organization Common Crawl, scours the web for search engine purposes, but
    its data is also used by OpenAI, Google, and others to train their models. Microsoft’s
    Bingbot is both a search crawler and an AI crawler. And those are just the crawlers
    that identify themselves — many others attempt to operate in relative secrecy,
    making it hard to stop or even find them in a sea of other web traffic. For any
    sufficiently popular website, finding a sneaky crawler is needle-in-haystack stuff.
  id: totrans-split-32
  prefs: []
  type: TYPE_NORMAL
  zh: 还有用于网页搜索和AI的爬虫。由Common Crawl组织运行的CCBot为搜索引擎目的在网络上搜索，但其数据也被OpenAI、Google和其他公司用于训练模型。Microsoft的Bingbot既是搜索爬虫又是AI爬虫。这些只是自我标识的爬虫，还有许多试图以相对隐秘的方式运行，使其难以停止甚至找到它们在其他网络流量中的存在。对于任何足够受欢迎的网站来说，找到一个偷偷摸摸的爬虫就像大海中的一根针一样困难。
- en: In large part, GPTBot has become the main villain of robots.txt because OpenAI
    allowed it to happen. The company published and promoted a page about how to block
    GPTBot and built its crawler to loudly identify itself every time it approaches
    a website. Of course, it did all of this *after* training the underlying models
    that have made it so powerful, and only once it became an important part of the
    tech ecosystem. But OpenAI’s chief strategy officer Jason Kwon says that’s sort
    of the point. “We are a player in an ecosystem,” he says. “If you want to participate
    in this ecosystem in a way that is open, then this is the reciprocal trade that
    everybody’s interested in.” Without this trade, he says, the web begins to retract,
    to close — and that’s bad for OpenAI and everyone. “We do all this so the web
    can stay open.”
  id: totrans-split-33
  prefs: []
  type: TYPE_NORMAL
  zh: 在很大程度上，GPTBot 已经成为 robots.txt 的主要对手，因为 OpenAI 允许它存在。该公司发布并推广了一篇关于如何阻止 GPTBot
    的页面，并且构建了其爬虫，每次接近一个网站时都会大声自识别。当然，这些都是在训练了使其如此强大的基础模型之后才进行的，并且只有在它成为技术生态系统中重要组成部分之后才这样做。但是，OpenAI
    的首席战略官 Jason Kwon 表示，这正是重点所在。“我们是生态系统中的一员，”他说，“如果你想以开放的方式参与这个生态系统，那么这就是每个人都感兴趣的互惠交易。”
    他说，没有这种交易，网络就会开始收缩，关闭 —— 这对 OpenAI 和所有人来说都是不利的。“我们做所有这些是为了让网络保持开放。”
- en: By default, the Robots Exclusion Protocol has always been permissive. It believes,
    as Koster did 30 years ago, that most robots are good and are made by good people,
    and thus allows them by default. That was, by and large, the right call. “I think
    the internet is fundamentally a social creature,” OpenAI’s Kwon says, “and this
    handshake that has persisted over many decades seems to have worked.” OpenAI’s
    role in keeping that agreement, he says, includes keeping ChatGPT free to most
    users — thus delivering that value back — and respecting the rules of the robots.
  id: totrans-split-34
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，机器人排除协议始终是宽容的。它认为，正如 30 年前科斯特所认为的那样，大多数机器人都是好的，并且是由好人制造的，因此默认情况下允许它们。总体而言，这是正确的决定。“我认为互联网在本质上是一个社会性生物，”OpenAI
    的 Kwon 说道，“而这种几十年来持续存在的握手似乎是有效的。” 他说，OpenAI 在维护该协议方面的角色包括保持 ChatGPT 对大多数用户的免费使用
    — 从而返还这种价值 — 并尊重机器人的规则。
- en: But robots.txt is not a legal document — and 30 years after its creation, it
    still relies on the good will of all parties involved.
  id: totrans-split-35
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，robots.txt 并不是法律文件 —— 30 年后的今天，它仍然依赖所有相关方的善意。
- en: But robots.txt is not a legal document — and 30 years after its creation, it
    still relies on the good will of all parties involved. Disallowing a bot on your
    robots.txt page is like putting up a “No Girls Allowed” sign on your treehouse
    — it sends a message, but it’s not going to stand up in court. Any crawler that
    wants to ignore robots.txt can simply do so, with little fear of repercussions.
    (There is some legal precedent around web scraping in general, though even that
    can be complicated and mostly lands on crawling and scraping being allowed.) The
    Internet Archive, for example, simply announced in 2017 that it was no longer
    abiding by the rules of robots.txt. “Over time we have observed that the robots.txt
    files that are geared toward search engine crawlers do not necessarily serve our
    archival purposes,” Mark Graham, the director of the Internet Archive’s Wayback
    Machine, [wrote at the time](https://blog.archive.org/2017/04/17/robots-txt-meant-for-search-engines-dont-work-well-for-web-archives/).
    And that was that.
  id: totrans-split-36
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，robots.txt 并不是法律文件 —— 30 年后的今天，它仍然依赖所有相关方的善意。在 robots.txt 页面上禁止机器人就像在你的树屋上挂上“禁止女孩入内”的牌子
    —— 它传达了一种信息，但在法庭上不会站得住脚。任何想要忽略 robots.txt 的爬虫都可以轻易地这样做，几乎不用担心后果。（关于网络抓取的一般法律先例也有些许，尽管其中大部分都是允许爬行和抓取的。）例如，互联网档案馆在2017年宣布不再遵守
    robots.txt 的规则。“随着时间的推移，我们观察到专门针对搜索引擎爬虫的 robots.txt 文件并不一定符合我们的档案目的”，互联网档案馆的 Wayback
    Machine 主管 Mark Graham 在[当时写道](https://blog.archive.org/2017/04/17/robots-txt-meant-for-search-engines-dont-work-well-for-web-archives/)。就这样。
- en: As the AI companies continue to multiply, and their crawlers grow more unscrupulous,
    anyone wanting to sit out or wait out the AI takeover has to take on an endless
    game of whac-a-mole. They have to stop each robot and crawler individually, if
    that’s even possible, while also reckoning with the side effects. If AI is in
    fact the future of search, as Google and others have predicted, blocking AI crawlers
    could be a short-term win but a long-term disaster.
  id: totrans-split-37
  prefs: []
  type: TYPE_NORMAL
  zh: 随着人工智能公司的不断增加，它们的网络爬虫变得越来越不道德，任何想要置身事外或等待人工智能接管的人都必须进行无休止的抵抗。如果AI确实是搜索的未来，就像Google和其他公司预测的那样，阻止AI爬虫可能是一个短期的胜利，但长期可能是一场灾难。
- en: There are people on both sides who believe we need better, stronger, more rigid
    tools for managing crawlers. They argue that there’s too much money at stake,
    and too many new and unregulated use cases, to rely on everyone just agreeing
    to do the right thing. “Though many actors have some rules self-governing their
    use of crawlers,” two tech-focused attorneys wrote in [a 2019 paper](https://digitalcommons.law.uw.edu/wjlta/vol13/iss3/4/)
    on the legality of web crawlers, “the rules as a whole are too weak, and holding
    them accountable is too difficult.”
  id: totrans-split-38
  prefs: []
  type: TYPE_NORMAL
  zh: 有人认为我们需要更好、更强、更严格的工具来管理网络爬虫。他们认为，涉及的利益太大，而且出现了太多新的未受监管的使用案例，不能仅仅依靠所有人自觉遵守规则。“尽管许多行为者在使用爬虫时有一些自我管理的规则，”两位专注于科技的律师在[2019年的一篇论文](https://digitalcommons.law.uw.edu/wjlta/vol13/iss3/4/)中写道，“但总体上规则还是太弱，而且很难追究其责任。”
- en: Some publishers would like more detailed controls over both what is crawled
    and what it’s used for, instead of robots.txt’s blanket yes-or-no permissions.
    Google, which a few years ago made an effort to make the Robots Exclusion Protocol
    an official formalized standard, has also pushed to deemphasize robots.txt on
    the grounds that it’s an old standard and too many sites don’t pay attention to
    it. “We recognize that existing web publisher controls were developed before new
    AI and research use cases,” Google’s VP of trust Danielle Romain [wrote last year](https://blog.google/technology/ai/ai-web-publisher-controls-sign-up/).
    “We believe it’s time for the web and AI communities to explore additional machine-readable
    means for web publisher choice and control for emerging AI and research use cases.”
  id: totrans-split-39
  prefs: []
  type: TYPE_NORMAL
  zh: 一些出版商希望对爬取的内容及其使用有更详细的控制，而不是仅仅依赖于 robots.txt 的全盘允许或拒绝权限。几年前，Google 努力使 Robots
    Exclusion Protocol 成为官方正式标准，但也主张减少对 robots.txt 的重视，理由是这是一个过时的标准，而且太多网站没有注意到它的存在。Google
    的信任副总裁 Danielle Romain [去年写道](https://blog.google/technology/ai/ai-web-publisher-controls-sign-up/)：“我们意识到现有的网络发布控制机制是在新的人工智能和研究用例出现之前开发的。”“我们认为现在是网络和人工智能社区探索额外的可机读方式，以便对新兴的人工智能和研究用例进行选择和控制的时候了。”
- en: 'Even as AI companies face regulatory and legal questions over how they build
    and train their models, those models continue to improve and new companies seem
    to start every day. Websites large and small are faced with a decision: submit
    to the AI revolution or stand their ground against it. For those that choose to
    opt out, their most powerful weapon is an agreement made three decades ago by
    some of the web’s earliest and most optimistic true believers. They believed that
    the internet was a good place, filled with good people, who above all wanted the
    internet to be a good thing. In that world, and on that internet, explaining your
    wishes in a text file was governance enough. Now, as AI stands to reshape the
    culture and economy of the internet all over again, a humble plain-text file is
    starting to look a little old-fashioned.'
  id: totrans-split-40
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管人工智能公司在如何构建和训练其模型方面面临着监管和法律问题，这些模型仍在不断改进，并且每天都有新公司涌现。无论大小网站都面临一个抉择：是顺应人工智能革命还是对抗它。“对于选择退出的人来说，他们最强大的武器是三十年前由一些最早和最乐观的真正信仰者制定的协议。”他们相信互联网是一个良好的地方，充满了善良的人，最重要的是，希望互联网成为一个好东西。在那个世界和互联网上，在一个简单的纯文本文件中表达你的意愿已经足够管理。现在，随着人工智能再次重新塑造互联网的文化和经济，一个朴素的纯文本文件开始显得有点过时。
