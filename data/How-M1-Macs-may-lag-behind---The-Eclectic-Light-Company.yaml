- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 类别：未分类
- en: 'date: 2024-05-27 14:45:24'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024年05月27日14:45:24
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: How M1 Macs may lag behind – The Eclectic Light Company
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: M1 Mac可能会落后 - 《The Eclectic Light Company》
- en: 来源：[https://eclecticlight.co/2024/01/13/how-m1-macs-may-lag-behind/](https://eclecticlight.co/2024/01/13/how-m1-macs-may-lag-behind/)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://eclecticlight.co/2024/01/13/how-m1-macs-may-lag-behind/](https://eclecticlight.co/2024/01/13/how-m1-macs-may-lag-behind/)
- en: Like it or loathe it, AI seems here to stay, and in the form of machine learning
    (ML) has already been changing our Macs. Use Spotlight, Siri, word completion,
    or any image processing tools, and you’ll be benefitting from them. Apple silicon
    chips contain sophisticated hardware support for both AI and ML in their GPUs
    and ANE, a dedicated neural engine. While the latter is probably the least-used
    part of the chips at present, that’s changing rapidly, and Apple is set to release
    more support in the near future. One clue has dropped in the appearance of a new
    Private Framework named DeepThought in Sonoma 14.2.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 不管你喜欢还是讨厌，AI似乎已经来了，并且以机器学习（ML）的形式已经改变了我们的Mac。使用Spotlight、Siri、文字自动补全或任何图像处理工具，你都会受益于它们。苹果硅片芯片在其GPU和ANE中包含了对AI和ML的复杂硬件支持，ANE是专用的神经引擎。虽然后者目前可能是芯片中使用最少的部分，但这一情况正在迅速改变，而且苹果计划在不久的将来发布更多的支持。一个线索出现在Sonoma
    14.2中的一个名为DeepThought的新私有框架中。
- en: 'But not all M-series chips are equal in this respect: M1 chips have more limited
    support for recent AI/ML features, including what has become a near-universal
    format for floating-point numbers, bfloat16\. Without that, Macs with M1 chips
    are likely to remain at a significant disadvantage when running AI and ML functions.'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 但并非所有的M系列芯片在这方面都是相等的：M1芯片对最近的AI/ML功能的支持更有限，包括已成为近乎普遍的浮点数格式，bfloat16。没有这个，搭载M1芯片的Mac在运行AI和ML功能时很可能仍然处于显著劣势。
- en: 'Representing integers (whole numbers) in binary formats used by computers is
    relatively straightforward: the more digits, the larger the numbers that can be
    represented. With one hex digit, you get 0-15 in decimal; double that to two hex
    digits, and the range goes from 0-255\. If you want negative numbers, then just
    set a bit to indicate that, and the range can go from -128 to +127.'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在计算机使用的二进制格式中表示整数（整数）相对简单：位数越多，可以表示的数字越大。用一个十六进制数位，你可以得到0-15的十进制数；将两个十六进制数位加倍，范围就从0到255。如果你想要负数，那么只需设置一个位来指示，范围可以从-128到+127。
- en: 'The most common way of representing floating-point numbers is to express them
    in a similar format to scientific or engineering format in decimal. The latter
    uses a sign (+ or -), a fraction, and an exponent. For example, the number -1,234,567.89
    might be expressed as 1.23456789 x 10^6 (ten to the power of six) with the negative
    sign: that has a fraction of 1.23456789 and an exponent of 6\. Being computers,
    rather than using powers of ten for the exponent, powers of 2 are used instead.'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 表示浮点数的最常见方法是将它们表示为类似于十进制科学或工程格式的格式。后者使用一个符号（+或-）、一个小数部分和一个指数。例如，数字-1,234,567.89可以表示为1.23456789
    x 10^6（10的六次方）带有负号：这个数的小数部分为1.23456789，指数为6。由于是计算机，因此不是使用十的幂作为指数，而是使用二的幂。
- en: The most common floating-point formats are those of the IEEE 754 standard, where
    a single-precision 32-bit float has a sign bit, an 8-bit exponent, and 23 bits
    to contain the fraction. The size allowed for the exponent determines the range
    of floating-point numbers that can be represented in that format, while the size
    allowed for the fraction determines how precise any number can be.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 最常见的浮点数格式是IEEE 754标准的格式，其中单精度32位浮点数有一个符号位，一个8位的指数，和23位用于包含小数部分。允许的指数大小确定了该格式中可以表示的浮点数的范围，而允许的小数部分大小确定了任何数字可以有多精确。
- en: With recent rapid developments in AI and ML, several new floating-point number
    formats have come into use, among them what’s known as bfloat16, with a sign bit,
    an 8-bit exponent just like the single-precision 32-bit float, but only 7 bits
    to contain the fraction. Compared with the 32-bit standard, in half the number
    of bits, bfloat16 numbers cover the same range at lower precision. That’s claimed
    to be ideal for AI, ML, and use with smart sensor technology.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 随着人工智能和机器学习的最新快速发展，出现了几种新的浮点数格式，其中包括被称为bfloat16的格式，它具有一个符号位，一个8位的指数，与单精度32位浮点数一样，但只有7位用于包含小数部分。与32位标准相比，在半数的比特数中，bfloat16数字以较低的精度覆盖相同的范围。据称，这对于人工智能、机器学习和智能传感器技术的使用是理想的。
- en: bfloat16 was developed as part of Google Brain, and has been adopted quickly
    over the last couple of years across Intel, AMD and Arm processors, and is widely
    supported in the tools and libraries used for AI and ML. As far as Apple’s M-series
    chips go, M2 and M3 CPUs support the ARMv8.6A instruction set, which includes
    bfloat16 support, but the M1 only supports ARMv8.5A, which doesn’t. Support by
    GPUs and Apple’s neural engine (ANE) is less clear, although [work on the M1 ANE](https://github.com/hollance/neural-engine/blob/master/docs/16-bit.md)
    suggests that it uses float16 (presumably IEEE half-precision 16-bit float) throughout.
    Given that the first M1 chips were being delivered in M1 Macs in late 2020, it
    seems most unlikely that Apple could have incorporated support for bfloat16 in
    their design.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: bfloat16是作为谷歌Brain项目的一部分开发的，在过去几年里迅速被Intel、AMD和Arm处理器广泛采用，并且在用于人工智能和机器学习的工具和库中得到广泛支持。至于苹果的M系列芯片，M2和M3
    CPU支持ARMv8.6A指令集，其中包括bfloat16支持，但M1只支持ARMv8.5A，不包括此支持。GPU和苹果的神经引擎（ANE）对bfloat16的支持程度不太清楚，尽管[M1
    ANE的工作](https://github.com/hollance/neural-engine/blob/master/docs/16-bit.md)表明它全程使用float16（可能是IEEE半精度16位浮点数）。考虑到2020年底首批M1芯片已交付至M1
    Mac，苹果很可能无法在设计中加入对bfloat16的支持。
- en: If the use of bfloat16 is as advantageous as is generally claimed, it looks
    like M1 Macs will remain at a significant disadvantage compared with M2 and later
    models. As Apple and third-parties roll out more products with AI and ML at their
    heart, don’t be surprised if their performance on M1 Macs proves disappointing
    compared with their M2 and M3 successors.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 如果bfloat16的使用像通常所宣称的那样有优势，那么M1 Mac与M2及更高型号相比仍然存在显著劣势。随着苹果和第三方推出更多以人工智能和机器学习为核心的产品，如果它们在M1
    Mac上的性能与其M2和M3后继机型相比令人失望，那也不足为奇。
- en: This situation is starker with Intel Macs, though, as they lack any hardware
    support for AI and ML, and are already being left in the past.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 与此相比，英特尔的Mac则更为严峻，因为它们缺乏任何硬件支持人工智能和机器学习的功能，并且已经被淘汰。
- en: '**Reference**'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: '**参考**'
- en: '[Wikipedia](https://en.wikipedia.org/wiki/Bfloat16_floating-point_format)'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: '[维基百科](https://en.wikipedia.org/wiki/Bfloat16_floating-point_format)'
