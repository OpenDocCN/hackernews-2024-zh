- en: <!--yml
  id: totrans-split-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-split-1
  prefs: []
  type: TYPE_NORMAL
  zh: 'category: 未分类'
- en: 'date: 2024-05-27 13:32:06'
  id: totrans-split-2
  prefs: []
  type: TYPE_NORMAL
  zh: 'date: 2024-05-27 13:32:06'
- en: -->
  id: totrans-split-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: Desperately Trying To Fathom The Coffeepocalypse Argument
  id: totrans-split-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 拼命试图理解咖啡末日论证
- en: 来源：[https://www.astralcodexten.com/p/desperately-trying-to-fathom-the](https://www.astralcodexten.com/p/desperately-trying-to-fathom-the)
  id: totrans-split-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://www.astralcodexten.com/p/desperately-trying-to-fathom-the](https://www.astralcodexten.com/p/desperately-trying-to-fathom-the)
- en: 'One of the most common arguments against AI safety is:'
  id: totrans-split-6
  prefs: []
  type: TYPE_NORMAL
  zh: 对人工智能安全性的最常见的论点之一是：
- en: Here’s an example of a time someone was worried about something, but it didn’t
    happen. Therefore, AI, which you are worried about, also won’t happen.
  id: totrans-split-7
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 这里有一个例子，有人担心某事情，但没发生。因此，你担心的人工智能也不会发生。
- en: 'I always give the obvious answer: “Okay, but there are other examples of times
    someone was worried about something, and it *did* happen, right? How do we know
    AI isn’t more like those?” The people I’m arguing with always seem so surprised
    by this response, as if I’m committing some sort of betrayal by destroying their
    beautiful argument.'
  id: totrans-split-8
  prefs: []
  type: TYPE_NORMAL
  zh: 我总是给出显而易见的答案：“好吧，但还有其他例子，有时候有人担心某事情，*结果*发生了，对吧？我们怎么知道人工智能不更像那些情况呢？” 我与之争论的人总是对这个回答感到非常惊讶，仿佛我在摧毁他们美好的论点时犯了某种背叛行为。
- en: The first hundred times this happened, I thought I must be misunderstanding
    something. Surely “I can think of one thing that didn’t happen, therefore nothing
    happens” is such a dramatic logical fallacy that no human is dumb enough to fall
    for it. But people keep bringing it up, again and again. Very smart people, people
    who I otherwise respect, make this argument and genuinely expect it to convince
    people!
  id: totrans-split-9
  prefs: []
  type: TYPE_NORMAL
  zh: 前一百次这种情况发生时，我以为我一定是误解了什么。毕竟，“我能想到一件事情没发生，所以什么也不会发生”是如此明显的逻辑谬误，以至于没有人会愚蠢到会中招。但人们一遍又一遍地提出这个论点。非常聪明的人，我其他方面尊重的人，他们提出这个论点，并真诚地期望它能说服人们！
- en: 'Usually the thing that didn’t happen is overpopulation, global cooling, etc.
    But most recently it was some kind of coffeepocalypse:'
  id: totrans-split-10
  prefs: []
  type: TYPE_NORMAL
  zh: 通常那些没有发生的事情是人口过剩、全球变冷等等。但最近大多数是某种咖啡末日的情况：
- en: You can [read the full thread here](https://twitter.com/Dan_Jeffries1/status/1741445839053025450),
    but I’m warning you, it’s just going to be “once people were worried about coffee,
    but now we know coffee is safe. Therefore AI will also be safe.”
  id: totrans-split-11
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以[在这里阅读完整的讨论](https://twitter.com/Dan_Jeffries1/status/1741445839053025450)，但我警告你，内容基本上是“有一次人们担心咖啡，但现在我们知道咖啡是安全的。因此人工智能也会安全无虞。”
- en: 'I keep trying to steelman this argument, and it keeps resisting my steelmanning.
    For example:'
  id: totrans-split-12
  prefs: []
  type: TYPE_NORMAL
  zh: 我一直试图加强这个论点，但它总是抵抗我的加强。例如：
- en: Maybe the argument is a failed attempt to gesture at a principle of “most technologies
    don’t go wrong”? But people make the same argument with things that aren’t technologies,
    like global cooling or overpopulation.
  id: totrans-split-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 或许这个论点是对“大多数技术不会出错”的一个失败尝试？但人们用不是技术的东西做同样的论据，比如全球变冷或人口过剩。
- en: Maybe the argument is a failed attempt to gesture at a principle of “the world
    is never destroyed, so doomsday prophecies have an abysmal track record”? But
    overpopulation and global cooling don’t claim that everyone will die - just that
    a lot of people will. And plenty of prophecies about mass death events have come
    true (eg Black Plague, WWII, AIDS). And none of this explains coffee!
  id: totrans-split-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 或许这个论点是对“世界从未被毁灭过，所以末日预言的记录很糟糕”原则的失败尝试？但人口过剩和全球变冷并没有声称每个人都会死 - 只是会有很多人。并且关于大规模死亡事件的许多预言已经成真（例如黑死病、第二次世界大战、艾滋病）。但这一切都无法解释咖啡啊！
- en: 'So my literal, non-rhetorical question, is “how can anyone be stupid enough
    to think this makes sense?” I’m not (just) trying to insult the people who say
    this; I consider their existence a genuine philosophical mystery. Isn’t this,
    in some sense, no different from saying (for example):'
  id: totrans-split-15
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我字面上的、非修辞的问题是：“有谁会愚蠢到认为这有道理呢？” 我并不（只）是想侮辱说这些话的人；我认为他们的存在是一个真正的哲学之谜。在某种意义上，这难道不和说（例如）没有不同吗：
- en: I once heard about a dumb person who thought halibut weren’t a kind of fish
    - but boy, that person sure was wrong. Therefore, AI is also a kind of fish.
  id: totrans-split-16
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 我曾经听说过一个愚蠢的人认为大比目鱼不是一种鱼 - 但是天哪，那个人肯定是错的。因此，人工智能也是一种鱼。
- en: 'The coffee version is:'
  id: totrans-split-17
  prefs: []
  type: TYPE_NORMAL
  zh: 咖啡版本是：
- en: I once heard about a dumb person who thought coffee would cause lots of problems
    - but boy, that person sure was wrong. Therefore, AI also won’t cause lots of
    problems.
  id: totrans-split-18
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 我曾经听说过一个愚蠢的人认为咖啡会引发很多问题 - 但是天哪，那个人肯定是错的。因此，人工智能也不会引发很多问题。
- en: Nobody would ever take it seriously in its halibut form. So what part of reskinning
    it as about coffee makes it more credible?
  id: totrans-split-19
  prefs: []
  type: TYPE_NORMAL
  zh: 没有人会认真对待它的比目鱼形式。那么将它重塑为关于咖啡的部分是什么让它更可信？
- en: Whenever I wonder how anyone can be so stupid, I start by asking if I myself
    am exactly this stupid in some other situation. This time, I remembered an argument
    from one of Stuart Russell’s pro-AI-risk arguments. [He pointed out](https://www.edge.org/response-detail/26157)
    that physicist Ernest Rutherford declared nuclear chain reactions impossible *less
    than twenty-four hours* before Szilard discovered the secret of the nuclear chain
    reaction. At the time, I thought this was a cute and helpful warning against being
    too sure that superintelligence was impossible.
  id: totrans-split-20
  prefs: []
  type: TYPE_NORMAL
  zh: 每当我想知道有人怎么能如此愚蠢时，我首先会问自己，是否在其他情况下我自己也正是如此愚蠢。这次，我记起了斯图尔特·拉塞尔（Stuart Russell）支持AI风险的论据之一。[他指出](https://www.edge.org/response-detail/26157)，物理学家欧内斯特·卢瑟福在亚历山大·西拉德（Szilard）发现核链反应秘密不到二十四小时前宣称核链反应不可能。当时，我认为这是一个有趣而有用的警告，提醒我们不要太过肯定超智能不可能。
- en: 'But isn’t this the same argument as the coffeepocalypse? A hostile rephrasing
    might be:'
  id: totrans-split-21
  prefs: []
  type: TYPE_NORMAL
  zh: 但这难道不是与咖啡末日论证相同的论点吗？一个敌对的重新表述可能是：
- en: There is at least one thing that was possible. Therefore, superintelligent AI
    is also possible.
  id: totrans-split-22
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 至少有一件事是可能的。因此，超智能AI也是可能的。
- en: 'And an only slightly less hostile rephrasing:'
  id: totrans-split-23
  prefs: []
  type: TYPE_NORMAL
  zh: 并且稍微少一些敌对的重新表述：
- en: People were wrong when they said nuclear reactions were impossible. Therefore,
    they might also be wrong when they say superintelligent AI is possible.
  id: totrans-split-24
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 人们在宣称核反应不可能时是错误的。因此，他们在宣称超智能AI可能时也可能是错误的。
- en: How is this better than the coffeepocalypse argument? In fact, how is it even
    better than the halibut argument? What are we doing when we make arguments like
    these?
  id: totrans-split-25
  prefs: []
  type: TYPE_NORMAL
  zh: 这比咖啡末日论证有何优势？事实上，这比比目鱼论证又更好在哪里？我们在做这些论证时究竟在做什么？
- en: 'Some thoughts:'
  id: totrans-split-26
  prefs: []
  type: TYPE_NORMAL
  zh: 一些想法：
- en: '**As An Existence Proof?**'
  id: totrans-split-27
  prefs: []
  type: TYPE_NORMAL
  zh: '**作为存在的证明？**'
- en: When I think of why I appreciated Prof. Russell’s argument, it wasn’t because
    it was a complete proof that superintelligence was possible. It was more like
    an argument for humility. “You may think it’s impossible. But given that there’s
    at least one case where people thought that and were proven wrong, you should
    believe it’s at least possible.”
  id: totrans-split-28
  prefs: []
  type: TYPE_NORMAL
  zh: 当我思考为什么我欣赏拉塞尔教授的论点时，不是因为它是超智能可能性的完全证明。它更像是一个谦卑的论据。“你可能认为这是不可能的。但考虑到至少有一个案例是人们认为不可能却被证明错了，你应该相信它至少是可能的。”
- en: But first of all, one case shouldn’t prove anything. If you doubt you will win
    the lottery, I can’t prove you wrong - even in a weak, probabilistic way - by
    bringing up a case of someone who did. I can’t even prove you should be humble
    - you are definitely allowed to be arrogant and very confident in your belief
    you won’t win!
  id: totrans-split-29
  prefs: []
  type: TYPE_NORMAL
  zh: 但首先，一个案例不应证明任何事情。如果你怀疑自己会中彩票，我无法通过举出一个中奖者的案例来证明你错了 - 即使在弱、概率化的方式上也不行。我甚至不能证明你应该谦卑
    - 你完全可以自负、非常自信地相信你不会中奖！
- en: And second of all, existence proofs can only make you *slightly* more humble.
    They can refute the claim “I am absolutely, 100% certain that AI is/isn’t dangerous”.
    But not many people make this claim, and it’s [uncharitable](https://slatestarcodex.com/2013/06/13/arguments-from-my-opponent-believes-something/)
    to suspect your opponent of doing so.
  id: totrans-split-30
  prefs: []
  type: TYPE_NORMAL
  zh: 而且，存在的证明只能让你*稍微*更谦卑一点。它们可以驳斥“我绝对、百分之百确定AI是/不是危险”的说法。但并不是很多人会这样说，怀疑你的对手这样做是[不友善的](https://slatestarcodex.com/2013/06/13/arguments-from-my-opponent-believes-something/)。
- en: Maybe this debate collapses into the debate around the [Safe Uncertainty Fallacy](https://www.astralcodexten.com/p/mr-tries-the-safe-uncertainty-fallacy),
    where some people think if there’s any uncertainty at all about something, you
    have to assume it will be totally safe and fine (no, I don’t get it either), and
    other people think if there’s even a 1% chance of disaster, you have to multiply
    out by the size of the disaster and end up very concerned (at the tails, this
    becomes Pascalian reasoning, but nobody has a good theory of where the tails begin).
  id: totrans-split-31
  prefs: []
  type: TYPE_NORMAL
  zh: 或许这场辩论会演变成关于[安全不确定性谬误](https://www.astralcodexten.com/p/mr-tries-the-safe-uncertainty-fallacy)的辩论，一些人认为，如果某事有任何不确定性，你必须假设它完全安全和良好（不，我也不理解），而另一些人认为，如果灾难发生的可能性仅有1%，你就必须将其乘以灾难的规模，最终会非常担忧（在极端情况下，这变成了帕斯卡推理，但没有人能清楚定义何时开始极端情况）。
- en: I still don’t think an existence proof that it’s theoretically possible for
    your opponent to be wrong goes very far. Still, this is sort of what I was trying
    to do [with the diphyllic dam example here](https://www.astralcodexten.com/p/ye-olde-bay-area-house-party)
    - show that a line of argument can sometimes be wrong, in a way that forces people
    to try something more sophisticated.
  id: totrans-split-32
  prefs: []
  type: TYPE_NORMAL
  zh: 我仍然认为，证明你的对手在理论上可能是错的存在证明并不远。不过，这基本上是我试图通过这里的双轨水坝例子来做的事情 - 展示一种论证线路有时可能是错误的，这种方式迫使人们尝试更复杂的方法。
- en: '**As An Attempt To Trigger A Heuristic?**'
  id: totrans-split-33
  prefs: []
  type: TYPE_NORMAL
  zh: '**作为触发启发式的尝试？**'
- en: Maybe Prof. Russell’s argument implicitly assumes that everyone has a large
    store of knowledge about failed predictions - no heavier-than-air flying machine
    is possible, there is a world market for maybe five computers. You could think
    of this particular example of a prediction being false as trying to trigger people’s
    existing stock of memories that *very often* people’s predictions are false.
  id: totrans-split-34
  prefs: []
  type: TYPE_NORMAL
  zh: 也许罗素教授的论点暗示着每个人都有关于失败预测的大量知识 - 比如没有重于空气的飞行器是可能的，可能世界市场上只有五台计算机。你可以将这个特定的预测错误的例子视为试图激活人们已有的记忆库，*很多时候*人们的预测都是错误的。
- en: You could make the same argument about the coffeepocalypse. “People worried
    about coffee but it was fine” is intended to activate a long list of stored moral
    panics in your mind - the one around marijuana, the one around violent video games
    - enough to remind you that *very often* people worry about something and it’s
    nothing.
  id: totrans-split-35
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以对咖啡末日做出同样的论点。“人们担心咖啡，但其实没事”，意在激活你心中存储的一长串道德恐慌案例 - 如大麻、暴力视频游戏等，足以提醒你，*很多时候*人们担心的事情其实都是虚惊一场。
- en: But - even granting that there are many cases of both - are these useful? There
    are many cases of moral panics turning out to be nothing. But there are many other
    cases of moral panics proving true, or of people not worrying about things they
    should worry about. People didn’t worry enough about tobacco, and then it killed
    lots of people. People didn’t worry enough about lead in gasoline, and then it
    poisoned lots of children. People didn’t worry enough about global warming, OxyContin,
    al-Qaeda, growing international tension in the pre-WWI European system, etc, until
    after those things had already gotten out of control and hurt lots of people.
    We even have words and idioms for this kind of failure to listen to warnings -
    like the ostrich burying its head in the sand.
  id: totrans-split-36
  prefs: []
  type: TYPE_NORMAL
  zh: 但是 - 即使承认这两者有很多案例 - 这些有用吗？有很多道德恐慌最终证明是无中生有。但也有很多道德恐慌最终证明是真实的，或者人们没有担心他们应该担心的事情。人们对烟草没怎么担心，然后它杀死了很多人。人们对汽油中的铅没怎么担心，然后它毒害了很多儿童。人们对全球变暖、奥斯康丁、基地组织、前第一次世界大战欧洲体系的国际紧张局势等没怎么担心，直到这些事情已经失控并伤害了很多人。我们甚至有专门描述这种不听警告的失败的词语和成语
    - 就像鸵鸟把头埋在沙子里一样。
- en: (and there are many examples of people predicting that things were impossible,
    and they really were impossible, eg perpetual motion).
  id: totrans-split-37
  prefs: []
  type: TYPE_NORMAL
  zh: （而有很多例子表明，人们预测某些事情是不可能的，而这确实是不可能的，如永动机）。
- en: It would seem like in order to usefully invoke a heuristic (“remember all these
    cases of moral panic we all agree were bad? Then you should assume this is probably
    also a moral panic”), you need to establish that moral panics are more common
    than ostrich-head-burying. And in order to usefully invoke a heuristic against
    predicting something is impossible, you need to establish that failed impossibility
    proofs are more common than accurate ones.
  id: totrans-split-38
  prefs: []
  type: TYPE_NORMAL
  zh: 为了有效地调用启发式（“记住我们都同意是坏的那些道德恐慌的所有案例？那么你应该假设这也可能是一种道德恐慌”），你需要建立道德恐慌比鸵鸟把头埋进沙子更普遍的事实。并且为了有效地反驳预测某事是不可能的启发式，你需要建立失败的不可能性证明比准确的更普遍的事实。
- en: This seems somewhere between “nobody has done it” and “impossible in principle”.
    Insisting on it would eliminate 90%+ of discourse.
  id: totrans-split-39
  prefs: []
  type: TYPE_NORMAL
  zh: 这似乎介于“没有人做到”和“从原理上讲不可能”的地方。坚持这一点将消除90%以上的言论。
- en: See also [Caution On Bias Arguments](https://slatestarcodex.com/2019/07/17/caution-on-bias-arguments/),
    where I try to make the same point. I think you can rewrite this section to be
    about proposed bias arguments (“People have a known bias to worry about things
    excessively, so we should correct for it”). But as always, you can posit an opposite
    bias (“People have a known bias to put their heads in the sand and ignore problems
    that it would be scary to think about or expensive to fix”), and figuring out
    which of these dueling biases you need to correct for, is the same problem as
    figuring out which of the dueling heuristics you need to invoke.
  id: totrans-split-40
  prefs: []
  type: TYPE_NORMAL
  zh: 参见也[关于偏见论证的警告](https://slatestarcodex.com/2019/07/17/caution-on-bias-arguments/)，在那里我试图表达同样的观点。我认为你可以重新写这一节，让它关于提出的偏见论点（“人们对过度担心的事情有已知的偏见，所以我们应该进行修正”）。但像往常一样，你可以假设一个相反的偏见（“人们有一个已知的偏见，他们把头埋在沙子里，忽视那些令人害怕或昂贵修复的问题”），弄清楚你需要纠正哪一个对立的偏见，这与决定你需要调用哪一个对立的启发式是相同的问题。
- en: '**What Is Evidence, Anyway?**'
  id: totrans-split-41
  prefs: []
  type: TYPE_NORMAL
  zh: '**什么是证据？**'
- en: Suppose someone’s trying to argue for some specific point, like “Russia will
    win the war with Ukraine”. They bring up some evidence, like “Russia has some
    very good tanks.”
  id: totrans-split-42
  prefs: []
  type: TYPE_NORMAL
  zh: 假设有人试图为某个特定观点辩护，比如“俄罗斯将赢得与乌克兰的战争”。他们提出了一些证据，比如“俄罗斯有一些非常好的坦克。”
- en: Obviously this on its own proves nothing. Russia could have good tanks, but
    Ukraine could be better at other things.
  id: totrans-split-43
  prefs: []
  type: TYPE_NORMAL
  zh: 很明显，仅凭这一点并不能证明什么。俄罗斯可能有好的坦克，但乌克兰在其他方面可能更出色。
- en: 'But then how does *any* amount of evidence prove an argument? You could make
    a hundred similar statements: “Russia has good tanks”, “Russia has good troop
    transport ships”, “the Russian general in the 4th District of the Western Theater
    is very skilled” […], and run into exactly the same problem. But an argument that
    Russia will win the war has to be made up of some number of pieces of evidence.
    So how can it ever work?'
  id: totrans-split-44
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，*任何*量的证据又如何证明一个论点呢？你可以做一百个类似的声明：“俄罗斯有好坦克”，“俄罗斯有好的运兵船”，“西部战区第四区的俄罗斯将军非常有能力”[...]，并且遇到完全相同的问题。但是一个论证俄罗斯会赢得战争的论点必须由若干证据片段组成。那么它如何能够奏效呢？
- en: I think it has to carry an implicit assumption of “…and you’re pretty good at
    weighing how much evidence it would take to prove something, and everything else
    is pretty equal, so this is enough evidence to push you over the edge into believing
    my point.”
  id: totrans-split-45
  prefs: []
  type: TYPE_NORMAL
  zh: 我认为它必须携带一个隐含的假设“…而且你很擅长权衡证据足以证明某事的量，其他的事情也相当，所以这足以让你相信我的观点。”
- en: For example, if someone said “Russia will win because they outnumber Ukraine
    3 to 1 and have better generals” (and then proved this was true), that at least
    seems like a plausible argument that shouldn’t be immediately ignored. Everyone
    knows that having a 3:1 advantage, and having good generals, are both big advantages
    in war. It carries an implied “and surely Ukraine doesn’t have some other advantage
    that counterbalances both of those”. But this could be so plausible that we accept
    it (it’s hard to counterbalance a 3:1 manpower advantage). Or it could be a challenge
    to pro-Ukraine people (if you can’t name some advantage of your side that sounds
    as convincing as these, then we win).
  id: totrans-split-46
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，如果有人说“俄罗斯会赢，因为他们的兵力是乌克兰的三倍，并且有更好的将军”（然后证明这是真的），这至少似乎是一个不应立即被忽视的合理论据。大家都知道兵力优势是一个很大的优势，有优秀的将军也是如此。这带有一个暗示，“当然，乌克兰没有其他优势来抵消这两个”。但这可能是如此合理，以至于我们接受它（很难抵消三倍的人力优势）。或者它可能对亲乌克兰的人构成挑战（如果你不能说出一些你一方的优势，听起来像这些一样令人信服，那么我们就赢了）。
- en: And it’s legitimate for someone who believes Russia will win, and has talked
    about it at length, to write one article about the good tanks, without explicitly
    saying “Obviously this is only one part of my case that Russia will win, and won’t
    convince anyone on its own; still, please update a little on this one, and maybe
    as you keep going and run into other things, you’ll update more.”
  id: totrans-split-47
  prefs: []
  type: TYPE_NORMAL
  zh: 有人认为俄罗斯会赢，并且长时间讨论过这个问题，所以写一篇关于好坦克的文章是合理的，而不明言“显然这只是我认为俄罗斯会赢的一个部分，不能单独说服任何人；不过，请在这一点上更新一下，并且当你继续前进并遇到其他问题时，你可能会更新更多。”
- en: Is this what the people talking about coffee are doing?
  id: totrans-split-48
  prefs: []
  type: TYPE_NORMAL
  zh: 那些谈论咖啡的人是这么做的吗？
- en: 'An argument against: you should at least update a *little* on the good tanks,
    right? But the coffee thing proves *literally* nothing. It proves that there was
    *one time* when people worried about a bad thing, and then it didn’t happen. Surely
    you already knew this must have happened at least once!'
  id: totrans-split-49
  prefs: []
  type: TYPE_NORMAL
  zh: 一个反对的论点：你至少应该稍微更新一下对好坦克的看法，对吧？但是咖啡的事情实际上什么也没证明。这只证明了有一次人们担心了一个坏事情，然后它没有发生。你肯定已经知道至少会有一次这样的情况发生！
- en: 'An argument in favor: suppose there are a hundred different facets of war as
    important as “has good tanks”. It would be very implausible if, of two relatively
    evenly-matched competitors, one of them was better at all 100, and the other at
    0\. So all that “Russia has good tanks” is telling you is that Russia is better
    on at least one axis, which you could have already predicted. Is this more of
    an update than the coffee situation?'
  id: totrans-split-50
  prefs: []
  type: TYPE_NORMAL
  zh: 一个支持的论点：假设有一百种与“拥有好坦克”同等重要的战争要素。如果两个竞争对手相对均衡，其中一个在所有100个要素上都更优秀，另一个都为0，这将是不太可能的。因此，“俄罗斯拥有好坦克”告诉你的只是俄罗斯在至少一个方面更优秀，这是你本来就可以预测到的。这比咖啡情况更像是一个更新吗？
- en: 'My proposed answer: if you knew the person making the argument was deliberately
    looking for pro-Russia arguments, then “has good tanks” updates you almost zero
    - it would only convince you that Russia was better in at least 1 of 100 domains.
    If you thought they were relatively unbiased and just happened to stumble across
    this information, it would update you slightly (we have chosen a randomly selected
    facet, and Russia is better).'
  id: totrans-split-51
  prefs: []
  type: TYPE_NORMAL
  zh: 我的提议答案：如果你知道提出论点的人刻意寻找亲俄论据，那么“拥有好坦克”几乎不会给你带来任何更新 - 它只会让你相信俄罗斯在100个领域中至少在一个方面更优秀。如果你认为他们相对公正，只是碰巧发现了这些信息，那么它会稍微更新你的看法（我们已选择一个随机选中的要素，而俄罗斯更胜一筹）。
- en: If you thought the person making the coffee argument was doing an unbiased survey
    of all times people had been worried, then the coffee fact (in this particular
    time people worried, it was unnecessary) might feel like sampling a random point.
    But we have so much more evidence about whether things are dangerous or safe that
    I don’t think sampling a random point (even if we could do so fairly) would mean
    much.
  id: totrans-split-52
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你认为提出咖啡论点的人正在做一个关于所有人们何时担心过的不偏不倚的调查，那么咖啡的事实（在这个特定时间人们担心了，但其实是多余的）可能会感觉像是取样一个随机点。但是我们对于事物是危险还是安全有更多的证据，我不认为取样一个随机点（即使我们可以公正地这样做）会有多大意义。
- en: '**Conclusion: I Genuinely Don’t Know What These People Are Thinking**'
  id: totrans-split-53
  prefs: []
  type: TYPE_NORMAL
  zh: '**结论：我真的不知道这些人在想什么**'
- en: I would like to understand the mindset of people who make arguments like this,
    but I’m not sure I’ve succeeded. The best I can say is that sometimes people on
    my side make similar arguments (the nuclear chain reaction one) which I don’t
    immediately flag as dumb, and maybe I can follow this thread to figure out why
    they seem tempting sometimes.
  id: totrans-split-54
  prefs: []
  type: TYPE_NORMAL
  zh: 我希望能理解那些提出这种论点的人的思维方式，但我不确定我是否成功了。我能说的最好的是，有时候我这边的人也会提出类似的论点（核链反应那个），我并不会立即认为它们愚蠢，也许我可以沿着这个思路来弄清楚为什么有时候它们看起来很诱人。
- en: If you see me making an argument that you think is like coffeepocalypse, please
    let me know, so I can think about what factors led me to think it was a reasonable
    thing to do, and see if they also apply to the coffee case.
  id: totrans-split-55
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你看到我提出的论点，你觉得它像咖啡末日一样，请告诉我，这样我可以思考是什么因素导致我认为这是一个合理的做法，并看看它们是否也适用于咖啡情况。
- en: . . . although I have to admit, I’m a little nervous asking for this, though.
    Douglas Adams once said that if anyone ever understood the Universe, it would
    immediately disappear and be replaced by something even more incomprehensible.
    I worry that if I ever understand why anti-AI-safety people think the things they
    say count as good arguments, the same thing might happen.
  id: totrans-split-56
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然我不得不承认，我有点紧张地提出这个要求。道格拉斯·亚当斯曾经说过，如果有人理解了宇宙，它会立即消失，并被更加难以理解的东西替代。我担心如果我真的理解了为什么反对AI安全的人认为他们说的东西算是好论点，同样的事情可能会发生。
