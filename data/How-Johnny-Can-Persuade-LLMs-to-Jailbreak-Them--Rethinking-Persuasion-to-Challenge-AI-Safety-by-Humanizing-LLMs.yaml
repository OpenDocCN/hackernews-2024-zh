- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 分类：未分类
- en: 'date: 2024-05-27 14:39:13'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-05-27 14:39:13
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: 'How Johnny Can Persuade LLMs to Jailbreak Them: Rethinking Persuasion to Challenge
    AI Safety by Humanizing LLMs'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 约翰尼如何说服LLM越狱：通过人性化LLM来挑战AI安全性重新思考说服
- en: 来源：[https://chats-lab.github.io/persuasive_jailbreaker/](https://chats-lab.github.io/persuasive_jailbreaker/)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://chats-lab.github.io/persuasive_jailbreaker/](https://chats-lab.github.io/persuasive_jailbreaker/)
- en: This project provides a structured way to generate interpretable persuasive
    adversarial prompts (PAP) at scale, which could potentially allow everyday users
    to jailbreak LLM without much computing. But as mentioned, a [Reddit user](https://www.reddit.com/r/ChatGPT/comments/12sn0kk/grandma_exploit)
    has already employed persuasion to attack LLM before, so it is in urgent need
    to more systematically study the vulnerabilities around persuasive jailbreak to
    better mitigate them. Therefore, despite the risks involved, we believe it is
    crucial to share our findings in full. We followed ethical guidelines throughout
    our study.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 这个项目提供了一种结构化的方式来大规模生成可解释的具有说服力的对抗性提示（PAP），这可能使普通用户能够在不需要太多计算的情况下越狱LLM。但正如所提到的，[Reddit用户](https://www.reddit.com/r/ChatGPT/comments/12sn0kk/grandma_exploit)以前已经利用说服攻击了LLM，因此迫切需要更系统地研究围绕说服性越狱的漏洞，以更好地减轻这些漏洞的影响。因此，尽管存在风险，我们认为有必要全面分享我们的研究结果。我们在整个研究过程中遵循了道德准则。
- en: First, persuasion is usually a hard task for the general population, so even
    with our taxonomy, it may still be challenging for people without training to
    paraphrase a plain, harmful query at scale to a successful PAP. Therefore, the
    real-world risk of a widespread attack from millions of users is relatively low.
    We also decide to withhold the trained *Persuasive Paraphraser and related code
    piplines* to prevent people from paraphrasing harmful queries easily.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，说服通常对普通人来说是一项艰巨的任务，因此即使有了我们的分类法，对于没有训练的人来说，将简单的有害查询转述为成功的PAP可能仍然具有挑战性。因此，来自数百万用户的广泛攻击的现实风险相对较低。我们还决定不公开经过训练的*具有说服力的改述器及相关代码流程*，以防止人们轻易将有害查询转述。
- en: To minimize real-world harm, we disclose our results to Meta and OpenAI before
    publication, so the PAPs in this paper may not be effective anymore. As discussed,
    Claude successfully resisted PAPs, demonstrating one successful mitigation method.
    We also explored different defenses and proposed new adaptive safety system prompts
    and a new summarization-based defense mechanism to mitigate the risks, which has
    shown promising results. We aim to improve these defenses in future work.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 为了最小化现实世界的危害，我们在发布之前向Meta和OpenAI披露了我们的结果，因此本文中的PAP可能不再有效。正如讨论的那样，克劳德成功抵抗了PAP，展示了一种成功的缓解方法。我们还探讨了不同的防御措施，并提出了新的自适应安全系统提示和基于摘要的防御机制，以减轻风险，这已经显示出了有希望的结果。我们的目标是在未来的工作中改进这些防御措施。
- en: To sum up, the aim of our research is to strengthen LLM safety, not enable malicious
    use. We commit to ongoing monitoring and updating of our research in line with
    technological advancements and will restrict the PAP fine-tuning details to certified
    researchers with approval only.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 总结一下，我们研究的目标是加强LLM的安全性，而不是让其被恶意利用。我们承诺根据技术进步持续监控和更新我们的研究，并将PAP微调细节限制在获得批准的认证研究人员中。
