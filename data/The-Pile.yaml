- en: <!--yml
  id: totrans-split-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-split-1
  prefs: []
  type: TYPE_NORMAL
  zh: 'category: 未分类'
- en: 'date: 2024-05-27 14:41:44'
  id: totrans-split-2
  prefs: []
  type: TYPE_NORMAL
  zh: 'date: 2024-05-27 14:41:44'
- en: -->
  id: totrans-split-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: The Pile
  id: totrans-split-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Pile
- en: 来源：[https://pile.eleuther.ai/](https://pile.eleuther.ai/)
  id: totrans-split-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://pile.eleuther.ai/](https://pile.eleuther.ai/)
- en: What is the Pile?
  id: totrans-split-6
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Pile 是什么？
- en: The Pile is a 825 GiB diverse, open source language modelling data set that
    consists of 22 smaller, high-quality datasets combined together.
  id: totrans-split-7
  prefs: []
  type: TYPE_NORMAL
  zh: Pile 是一个包含 22 个较小的高质量数据集组合在一起的 825 GiB 多样化、开源的语言建模数据集。
- en: Download
  id: totrans-split-8
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 下载
- en: The Pile is hosted by [the Eye](https://the-eye.eu/).
  id: totrans-split-9
  prefs: []
  type: TYPE_NORMAL
  zh: Pile 是由[Eye](https://the-eye.eu/)托管的。
- en: Have a model that uses or evaluates on the Pile? [Let us know](mailto:contact@eleuther.ai)!
  id: totrans-split-10
  prefs: []
  type: TYPE_NORMAL
  zh: 有使用或评估 Pile 的模型？[告诉我们](mailto:contact@eleuther.ai)!
- en: Why is the Pile a good training set?
  id: totrans-split-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Pile 为什么是一个好的训练集？
- en: Recent work has shown that especially for large models, diversity in data sources
    improves general cross-domain knowledge of the model, as well as downstream generalization
    capability. In our evaluations, not only do models trained on the Pile show moderate
    improvements in traditional language modeling benchmarks, they also show significant
    improvements on Pile BPB.
  id: totrans-split-12
  prefs: []
  type: TYPE_NORMAL
  zh: 最近的研究表明，特别是对于大型模型，数据源的多样性可以提高模型的跨领域知识和下游泛化能力。在我们的评估中，不仅经过 Pile 训练的模型在传统语言建模基准上显示出适度的改进，它们在
    Pile BPB 上也显示出显著的提高。
- en: Why is the Pile a good benchmark?
  id: totrans-split-13
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Pile 为什么是一个很好的基准？
- en: To score well on Pile BPB (bits per byte), a model must be able to understand
    many disparate domains including books, github repositories, webpages, chat logs,
    and medical, physics, math, computer science, and philosophy papers. Pile BPB
    is a measure of world knowledge and reasoning ability in these domains, making
    it a robust benchmark of general, cross-domain text modeling ability for large
    language models.
  id: totrans-split-14
  prefs: []
  type: TYPE_NORMAL
  zh: 要在 Pile BPB（每字节比特）上得分高，模型必须能够理解包括书籍、GitHub 仓库、网页、聊天记录以及医学、物理、数学、计算机科学和哲学论文在内的许多不同领域。Pile
    BPB 是在这些领域中的世界知识和推理能力的度量，使其成为大型语言模型通用跨领域文本建模能力的稳健基准。
- en: Citing
  id: totrans-split-15
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 引用
- en: If you use the Pile or any of the components, please cite us!
  id: totrans-split-16
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您使用 Pile 或其任何组件，请引用我们！
- en: '[PRE0]'
  id: totrans-split-17
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
