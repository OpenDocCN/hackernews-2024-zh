- en: <!--yml
  id: totrans-split-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-split-1
  prefs: []
  type: TYPE_NORMAL
  zh: 'category: 未分类'
- en: 'date: 2024-05-27 15:01:17'
  id: totrans-split-2
  prefs: []
  type: TYPE_NORMAL
  zh: 'date: 2024-05-27 15:01:17'
- en: -->
  id: totrans-split-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: Researchers, please replace SQLite with DuckDB now | by Dirk Petersen | Medium
  id: totrans-split-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 研究人员，请立即使用 DuckDB 替代 SQLite | 作者 Dirk Petersen | Medium
- en: 来源：[https://dirk-petersen.medium.com/researchers-please-replace-sqlite-with-duckdb-now-f038044a2702](https://dirk-petersen.medium.com/researchers-please-replace-sqlite-with-duckdb-now-f038044a2702)
  id: totrans-split-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://dirk-petersen.medium.com/researchers-please-replace-sqlite-with-duckdb-now-f038044a2702](https://dirk-petersen.medium.com/researchers-please-replace-sqlite-with-duckdb-now-f038044a2702)
- en: Researchers, please replace SQLite with DuckDB now
  id: totrans-split-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 研究人员，请立即使用 DuckDB 替代 SQLite
- en: If you are a researcher with computational workloads, you might be using SQLite
    for some tasks. Please drop it now and switch to DuckDB, it is much faster and
    easier to use
  id: totrans-split-7
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如果您是一个有计算工作负载的研究人员，您可能会在某些任务中使用 SQLite。请立即放弃它，转而使用 DuckDB，它更快速且更易于使用。
- en: You are a researcher, potentially in the life or social sciences, and your organization
    offers you access to a fast machine with many CPUs and lots of memory, potentially
    as part of a CPU cluster. You are familiar with Linux and use Python/Pandas and/or
    R to manipulate and analyze your data. You sometimes use SQLite, for example,
    to pre-filter large datasets but also as an option for storing datasets longer
    term.
  id: totrans-split-8
  prefs: []
  type: TYPE_NORMAL
  zh: 您是一名研究人员，可能从事生命或社会科学，您的组织为您提供了访问高性能机器的权限，该机器拥有多个 CPU 和大量内存，可能作为 CPU 群集的一部分。您熟悉
    Linux，并使用 Python/Pandas 和/或 R 来处理和分析数据。有时，您会使用 SQLite，例如对大型数据集进行预过滤，也作为长期存储数据集的选项。
- en: Some colleagues and IT folks have told you that SQLite is a toy database engine
    for testing (which isn’t true) and that you should switch to something faster
    and more mature, such as PostgreSQL or other databases. But you like the fact
    that SQLite does not require its own server, and you can just create a SQL database
    in your project folder along with all the other files you are using.
  id: totrans-split-9
  prefs: []
  type: TYPE_NORMAL
  zh: 一些同事和 IT 专家告诉您，SQLite 是一个仅用于测试的玩具数据库引擎（这是不正确的），并建议您切换到速度更快、更成熟的 PostgreSQL 或其他数据库。但您喜欢
    SQLite 不需要自己的服务器，您可以在项目文件夹中创建一个 SQL 数据库，并与您正在使用的所有其他文件一起使用。
- en: 'Don’t change that approach, even though a modern PostgreSQL on a modern computer
    will be faster. Instead, you should switch from SQLite to DuckDB because it is
    much faster and easier to use; these are the main benefits:'
  id: totrans-split-10
  prefs: []
  type: TYPE_NORMAL
  zh: 不要改变这种方法，即使现代计算机上的现代 PostgreSQL 速度更快。相反，您应该从 SQLite 切换到 DuckDB，因为它更快且更易于使用；这些是其主要优点：
- en: DuckDB is designed from day one to use all the CPU cores in your machine
  id: totrans-split-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: DuckDB 从设计之初就考虑了在机器的所有 CPU 核心上进行利用。
- en: DuckDB is optimized for complex queries, while SQLite and most other SQL databases
    are more optimized for writing multiple datasets at the same time. You can read
    up on the [details here](https://medium.com/scalecapacity/columnar-database-and-row-database-how-are-they-different-2efa8e4c38a3).
  id: totrans-split-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: DuckDB 优化了复杂查询，而 SQLite 和大多数其他 SQL 数据库更倾向于同时写入多个数据集。您可以在[这里](https://medium.com/scalecapacity/columnar-database-and-row-database-how-are-they-different-2efa8e4c38a3)详细了解。
- en: DuckDB supports multiple fast data formats out of the box and can read many
    database files in parallel.
  id: totrans-split-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: DuckDB 支持多种快速数据格式，并可以并行读取多个数据库文件。
- en: As test system we use a middle of the road 32 core server (Intel Gold 6326 CPU
    @ 2.90GHz) with 768 GB Memory and fast local flash storage. The system is connected
    to a fast shared Posix file system with more than 4GB/s read/write throughput
    as shown by the [scratch-dna benchmark](https://www.delltechnologies.com/asset/en-au/products/storage/industry-market/white-paper-esg-technical-review-performance-testing-onefs-google-cloud.pdf).
  id: totrans-split-14
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用一台中等配置的 32 核服务器（Intel Gold 6326 CPU @ 2.90GHz），内存为 768 GB，并配备快速本地闪存存储。该系统连接到一个快速的共享
    Posix 文件系统，读写吞吐量超过 4GB/s，如[scratch-dna benchmark](https://www.delltechnologies.com/asset/en-au/products/storage/industry-market/white-paper-esg-technical-review-performance-testing-onefs-google-cloud.pdf)所示。
- en: '[PRE0]'
  id: totrans-split-15
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: and a local flash disk with at least 1.2 GB/s read/write throughput
  id: totrans-split-16
  prefs: []
  type: TYPE_NORMAL
  zh: 以及至少 1.2 GB/s 读写吞吐量的本地闪存磁盘
- en: '[PRE1]'
  id: totrans-split-17
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Now, let’s test SQLite and DuckDB with a reasonably sized CSV file — nothing
    too huge, but also large enough to demonstrate the difference. If you work in
    research, you will likely have access to a fast POSIX filesystem connected to
    your Linux machine. This filesystem may contain many millions or billions of files,
    and we would like to harvest that information to do some analysis of your file
    metadata, such as sizes, file types, etc. You can use [pwalk](https://github.com/fizwit/filesystem-reporting-tools)
    to generate a CSV file of your folder metadata and then ensure with ‘iconv’ that
    it has the right format to be used with a database engine.
  id: totrans-split-18
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们使用一个合理大小的 CSV 文件来测试 SQLite 和 DuckDB — 不要太大，但也足够大以展示差异。如果您从事研究工作，可能会访问连接到
    Linux 机器的快速 POSIX 文件系统。此文件系统可能包含许多百万或十亿个文件，我们希望利用这些信息来分析文件的元数据，如大小、文件类型等。您可以使用
    [pwalk](https://github.com/fizwit/filesystem-reporting-tools) 生成一个包含您文件夹元数据的 CSV
    文件，然后使用 'iconv' 确保它具有正确的格式以便与数据库引擎一起使用。
- en: '[PRE2]'
  id: totrans-split-19
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Testing SQLite
  id: totrans-split-20
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 测试 SQLite
- en: Let’s import this CSV file into SQLite. We’ll use standalone SQLite version
    3.38 and Python version 3.10, which comes with SQLite version 3.39\. Additionally,
    we’ll use Pandas as a helper to detect if the columns of the CSV files are numbers,
    text, or dates — a functionality SQLite lacks. Pandas operates in memory and can
    insert its table directly into a SQLite database. However, Pandas does not come
    with Python and needs to be installed separately.
  id: totrans-split-21
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们将这个 CSV 文件导入 SQLite。我们将使用独立的 SQLite 版本 3.38 和 Python 版本 3.10，它带有 SQLite 版本
    3.39。此外，我们将使用 Pandas 作为辅助工具，以检测 CSV 文件的列是数字、文本还是日期 — 这是 SQLite 缺乏的功能。Pandas 在内存中操作，并可以直接将其表插入
    SQLite 数据库。但是，Pandas 不随 Python 一起提供，需要单独安装。
- en: '[PRE3]'
  id: totrans-split-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Then we execute the script in our shared file system and it takes more than
    43 minutes.
  id: totrans-split-23
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们在共享文件系统中执行脚本，这需要超过43分钟。
- en: '[PRE4]'
  id: totrans-split-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'That is a lot ….. but all the columns seem to have been imported correctly:'
  id: totrans-split-25
  prefs: []
  type: TYPE_NORMAL
  zh: 那真是太多了……但所有列似乎都已正确导入：
- en: '[PRE5]'
  id: totrans-split-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Now, let’s run a simple query on our shared POSIX file system to check if all
    283 million records were inserted. We don’t need Python for this; it’s simple
    enough that we can just execute it in our Bash shell. We just count the number
    of records in the table:'
  id: totrans-split-27
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们在我们的共享 POSIX 文件系统上运行一个简单的查询，检查是否已插入所有 2.83 亿条记录。我们不需要 Python 来执行此操作；它足够简单，我们可以在
    Bash shell 中执行它。我们只需计算表中记录的数量：
- en: '[PRE6]'
  id: totrans-split-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'More than 20 min is quite long, there may be something wrong with our fast
    shared filesystem? Let’s copy the database to local disk and try again:'
  id: totrans-split-29
  prefs: []
  type: TYPE_NORMAL
  zh: 超过20分钟相当长，我们的快速共享文件系统可能出了问题？让我们将数据库复制到本地磁盘，然后再试一次：
- en: '[PRE7]'
  id: totrans-split-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Well, since we’ve copied the file, it may have been cached, but that alone
    cannot explain the difference. Re-running the data from the shared file system,
    even when cached, is faster, but it’s still slow and takes almost 11 minutes:'
  id: totrans-split-31
  prefs: []
  type: TYPE_NORMAL
  zh: 好吧，既然我们复制了文件，它可能已被缓存，但这本身不能解释差异。即使在缓存的情况下，从共享文件系统重新运行数据也更快，但仍然很慢，需要将近11分钟：
- en: '[PRE8]'
  id: totrans-split-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Let’s return to our local file and run another simple yet slightly more complex
    query. We’d like to know the total disk consumption of all files in bytes and
    generate a sum() of the ‘st_size’ column:'
  id: totrans-split-33
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们回到本地文件并运行另一个简单但稍微复杂的查询。我们想知道所有文件的总磁盘消耗量（以字节为单位），并生成 'st_size' 列的 sum()：
- en: '[PRE9]'
  id: totrans-split-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: The slightly more complex query takes more than 50% longer than the simple one
    which makes sense and returns about 538 TiB (591263908311685 bytes) of disk consumption
  id: totrans-split-35
  prefs: []
  type: TYPE_NORMAL
  zh: 稍微复杂的查询所花费的时间比简单查询长了超过50%，这是合理的，并返回约 538 TiB（591263908311685 字节）的磁盘消耗量
- en: Testing DuckDB
  id: totrans-split-36
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 测试 DuckDB
- en: 'One of the first things we notice is that DuckDB can use CSV files directly
    without importing them first. We can use the ‘read_csv_auto’ function to automatically
    figure out the data types in each column.:'
  id: totrans-split-37
  prefs: []
  type: TYPE_NORMAL
  zh: 我们注意到的第一件事是，DuckDB 可以直接使用 CSV 文件而无需先导入它们。我们可以使用 'read_csv_auto' 函数自动确定每列中的数据类型：
- en: '[PRE10]'
  id: totrans-split-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: It took 3 minutes to run a sql query directly on a CSV file and we can see that
    the CPU utilization is more than 800% which means that 8–9 CPU cores were busy.
    We also see that DuckDB has a nice progress bar.
  id: totrans-split-39
  prefs: []
  type: TYPE_NORMAL
  zh: 在一个 CSV 文件上直接运行 SQL 查询花了3分钟，并且我们可以看到 CPU 利用率超过 800%，这意味着 8-9 个 CPU 核心在忙碌。我们还看到
    DuckDB 有一个漂亮的进度条。
- en: 'Now, working with CSV files directly is slow, as queries cannot be optimized
    when using this data interchange format, but we can counteract that with raw compute
    power. However, let’s explore other options we have. We can import the CSV file
    into a native DuckDB format that is probably very fast… but DuckDB is quite new,
    and no other tools will be able to read the DuckDB format, so let’s check if other
    formats are supported. We notice Apache Arrow, which is gaining a lot of popularity,
    but we also see the good old Parquet format, which can be read by many tools out
    there. Let’s try that:'
  id: totrans-split-40
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，直接使用CSV文件的工作速度较慢，因为在使用这种数据交换格式时无法优化查询，但我们可以通过原始计算能力来抵消这一点。然而，让我们探索一下其他选择。我们可以将CSV文件导入到可能非常快速的本地DuckDB格式...但是DuckDB相当新，其他工具将无法读取DuckDB格式，因此让我们检查是否支持其他格式。我们注意到Apache
    Arrow正在获得很多人气，但我们也看到了受欢迎的旧Parquet格式，可以被许多工具读取。让我们试试这个：
- en: '[PRE11]'
  id: totrans-split-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'OK, this conversion took just over 5 minutes, which is not bad. Now let’s see
    what we can do with ‘x-dept.parquet’. If we read it through our shared POSIX file
    system, let’s run our previous queries:'
  id: totrans-split-42
  prefs: []
  type: TYPE_NORMAL
  zh: 好吧，这个转换花了超过5分钟，这还算不错。现在让我们看看我们可以如何处理‘x-dept.parquet’。如果我们通过共享的POSIX文件系统读取它，让我们运行我们之前的查询：
- en: '[PRE12]'
  id: totrans-split-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Oops, 0.6 seconds instead of more than 20 minutes ? That is 2000 times faster
    on a shared filesystem and more than 100 times faster when comparing to SQLite
    on local flash/SSD! And we did not even use DuckDB’s internal database format
    yet. Let’s confirm these results with our second query:'
  id: totrans-split-44
  prefs: []
  type: TYPE_NORMAL
  zh: 哎呀，0.6秒而不是20多分钟？在共享文件系统上快了2000倍，在本地闪存/SSD上与SQLite比较快了100多倍！而且我们甚至还没有使用DuckDB的内部数据库格式。让我们通过我们的第二个查询确认这些结果：
- en: '[PRE13]'
  id: totrans-split-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Equally impressive. Now, let’s try something a bit more complex. We would like
    to know the percentage share of each file type, as identified by the file extension,
    as well as the total disk space each file type consumes. In this case, we’ll put
    the SQL statement in a text file called ‘extension-summary.sql’ :'
  id: totrans-split-46
  prefs: []
  type: TYPE_NORMAL
  zh: 同样令人印象深刻。现在，让我们尝试一些更复杂的内容。我们想知道每种文件类型的百分比份额（由文件扩展名识别），以及每种文件类型占用的总磁盘空间。在这种情况下，我们将SQL语句放在一个名为‘extension-summary.sql’的文本文件中：
- en: '[PRE14]'
  id: totrans-split-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'and then we ask DuckDB to run it against our Parquet file:'
  id: totrans-split-48
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们让DuckDB对我们的Parquet文件运行它：
- en: '[PRE15]'
  id: totrans-split-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: This was executed on a different server from the previous one to ensure that
    the cache was cold. 3.3 seconds is impressive, but obviously, we need to find
    an even more complex query to test the true capabilities of DuckDB. Let’s try
    to identify all files that may be duplicates, as they have the same name, file
    size, and modification date, but are stored in different directories. We don’t
    really have in-depth knowledge of SQL, so let’s ask ChatGPT for some help. Given
    that DuckDB is compatible with PostgreSQL syntax, once we describe the table structure
    and explain what we want to achieve, it provides a suitable solution that we then
    copy into the ‘dedup.sql’ text file.
  id: totrans-split-50
  prefs: []
  type: TYPE_NORMAL
  zh: 这是在不同的服务器上执行的，以确保缓存是冷的。3.3秒是令人印象深刻的，但显然，我们需要找到一个更复杂的查询来测试DuckDB的真正能力。让我们尝试识别所有可能重复的文件，因为它们具有相同的名称、文件大小和修改日期，但存储在不同的目录中。我们对SQL并不是很了解，所以让我们向ChatGPT寻求帮助。鉴于DuckDB兼容PostgreSQL语法，一旦我们描述了表结构并解释了我们想要实现的目标，它就提供了一个合适的解决方案，然后我们将其复制到‘dedup.sql’文本文件中。
- en: '[PRE16]'
  id: totrans-split-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: '[PRE17]'
  id: totrans-split-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'In this case 13 CPU cores ran for 22 seconds to execute a pretty complex query.
    Rerunning this query after copying the parquet file to a local disk gives us this:'
  id: totrans-split-53
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，13个CPU核心运行了22秒来执行一个相当复杂的查询。将parquet文件复制到本地磁盘后重新运行此查询，结果如下：
- en: '[PRE18]'
  id: totrans-split-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: In other words, there is no performance difference between the shared file system
    and the local flash disk.
  id: totrans-split-55
  prefs: []
  type: TYPE_NORMAL
  zh: 换句话说，共享文件系统和本地闪存磁盘之间没有性能差异。
- en: 'Now let’s compare this again with SQLite and our file ‘extentions-summary.sql’:'
  id: totrans-split-56
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们再次与SQLite和我们的文件‘extentions-summary.sql’进行比较：
- en: '[PRE19]'
  id: totrans-split-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: OK, 6 minutes and 15 seconds versus 3.3 seconds means that DuckDB is 113 times
    faster than SQLite, at least in this example.
  id: totrans-split-58
  prefs: []
  type: TYPE_NORMAL
  zh: 好吧，6分钟15秒对比3.3秒意味着DuckDB比SQLite至少快113倍，至少在这个例子中是这样。
- en: 'And how does our complex ‘dedup.sql’ query perform? After asking ChatGPT again
    to convert the PostgreSQL-compatible query to one that works with SQLite, we can
    execute it:'
  id: totrans-split-59
  prefs: []
  type: TYPE_NORMAL
  zh: 我们复杂的‘dedup.sql’查询的表现如何？在再次询问ChatGPT将兼容PostgreSQL的查询转换为SQLite的查询后，我们可以执行它：
- en: '[PRE20]'
  id: totrans-split-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: In this case DuckDB is only 15 times faster than SQLite and the performance
    difference can perhaps be mostly attributed to the multiple cpu cores, that DuckDB
    uses.
  id: totrans-split-61
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，DuckDB只比SQLite快15倍，性能差异可能主要归因于DuckDB使用的多个CPU核心。
- en: And finally, a word about wildcards
  id: totrans-split-62
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 最后，谈一下通配符
- en: 'Research datasets are often dispersed across multiple files. This is sometimes
    because the datasets are too large to be stored in a single file; other times,
    it’s because researchers, working globally and distributed, manage their own datasets,
    and the data only needs to be merged occasionally for analytics purposes. In many
    database systems, this would mean you have multiple tables that would need to
    be combined with a ‘union’ query, but with DuckDB, this process is very straightforward:
    Imagine you had three files: ‘./data/prj-asia.csv’, ‘./data/prj-africa.csv’, and
    ‘./data/prj-europe.csv’, all with the same schema (column structure). You can
    simply use a wildcard to read all three files as a single table:'
  id: totrans-split-63
  prefs: []
  type: TYPE_NORMAL
  zh: 研究数据集通常分散在多个文件中。这有时是因为数据集过大而无法存储在单个文件中；另一些情况下，是因为全球和分布式工作的研究人员管理自己的数据集，数据仅需偶尔合并以进行分析。在许多数据库系统中，这意味着您可能有多个需要使用“union”查询合并的表，但使用DuckDB，这个过程非常简单：假设您有三个文件：‘./data/prj-asia.csv’、‘./data/prj-africa.csv’和‘./data/prj-europe.csv’，所有文件具有相同的模式（列结构）。您可以简单地使用通配符将所有三个文件读取为单个表：
- en: '[PRE21]'
  id: totrans-split-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: Summary of Benefits of DuckDB vs SQLite for Analytics
  id: totrans-split-65
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: DuckDB vs SQLite在分析中的优势总结
- en: DuckDB is much faster than SQLite, in some cases orders of magnitude
  id: totrans-split-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: DuckDB在某些情况下比SQLite快得多
- en: DuckDB has much more data import functionality built-in, no external python
    packages needed
  id: totrans-split-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: DuckDB具有更多内置的数据导入功能，无需外部Python包
- en: DuckDB does not experience any performance bottlenecks with shared Posix filesystems
    which are common in most research environments
  id: totrans-split-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: DuckDB在共享的Posix文件系统中没有任何性能瓶颈，这在大多数研究环境中是常见的
- en: DuckDB uses PostgreSQL syntax, the most prevalent SQL slang among data scientists
    and new open source database projects
  id: totrans-split-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: DuckDB使用PostgreSQL语法，这是数据科学家和新的开源数据库项目中最普遍的SQL行话
- en: DuckDB has built-in support for writing and reading Parquet and Apache Arrow
    data formats
  id: totrans-split-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: DuckDB内置支持写入和读取Parquet和Apache Arrow数据格式
- en: Python and R packages are integral part of the DuckDB project
  id: totrans-split-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Python和R包是DuckDB项目的一个重要组成部分
- en: Support of wildcards allows researchers to work with many files in parallel
  id: totrans-split-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通配符的支持允许研究人员并行处理多个文件
- en: Where you should continue to use SQLite
  id: totrans-split-73
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在应继续使用SQLite的情况下
- en: SQLite is the most used database on planet Earth and is also one of the most
    stable, with millions of smartphone apps using it internally. There are many good
    reasons to use SQLite, and they are laid out on the [SQLite website](https://www.sqlite.org/whentouse.html).
    Researchers appreciate SQLite because of its flexibility and the fact that no
    database server is needed. However, as DuckDB does a better job of supporting
    this use case, researchers should consider switching today.
  id: totrans-split-74
  prefs: []
  type: TYPE_NORMAL
  zh: SQLite是地球上使用最多的数据库之一，也是最稳定的之一，数百万智能手机应用程序在内部使用它。有许多使用SQLite的好理由，这些理由在[SQLite网站](https://www.sqlite.org/whentouse.html)上已经详细说明。研究人员喜欢SQLite因为其灵活性和无需数据库服务器的特性。然而，由于DuckDB在支持这种用例方面做得更好，研究人员应考虑立即转换。
- en: Other resources
  id: totrans-split-75
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 其他资源
- en: I hope to write a second article about DuckDB soon, now more geared towards
    use of DuckDB in High Performance Computing (HPC) systems. A discussion of the
    wildcard feature will be part of this.
  id: totrans-split-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我希望很快写一篇关于DuckDB的第二篇文章，现在更加关注DuckDB在高性能计算（HPC）系统中的使用。这篇文章将讨论通配符功能。
- en: There is a similar [article for data scientists here](https://towardsdatascience.com/forget-about-sqlite-use-duckdb-instead-and-thank-me-later-df76ee9bb777),
    but that one has more techie language and I felt the need to write something,
    that is more inclusive towards all researchers.
  id: totrans-split-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 有一篇类似的[数据科学家文章在这里](https://towardsdatascience.com/forget-about-sqlite-use-duckdb-instead-and-thank-me-later-df76ee9bb777)，但那篇文章使用了更多技术性语言，我觉得有必要写一些更为包容所有研究人员的内容。
- en: If you are interested in analyzing your filesystem metadata with a more professional
    and feature rich tool, you should consider [https://starfishstorage.com/](https://starfishstorage.com/)
  id: totrans-split-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果您有兴趣使用更专业和功能丰富的工具分析您的文件系统元数据，您应考虑[https://starfishstorage.com/](https://starfishstorage.com/)
