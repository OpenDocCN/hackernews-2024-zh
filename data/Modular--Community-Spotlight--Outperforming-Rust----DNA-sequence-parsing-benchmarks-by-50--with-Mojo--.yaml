- en: <!--yml
  id: totrans-split-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: æœªåˆ†ç±»'
  id: totrans-split-1
  prefs: []
  type: TYPE_NORMAL
  zh: 'category: æœªåˆ†ç±»'
- en: 'date: 2024-05-27 14:40:20'
  id: totrans-split-2
  prefs: []
  type: TYPE_NORMAL
  zh: 'date: 2024-05-27 14:40:20'
- en: -->
  id: totrans-split-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: 'Modular: Community Spotlight: Outperforming Rust âš™ï¸ DNA sequence parsing benchmarks
    by 50% with Mojo ğŸ”¥'
  id: totrans-split-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 'Modular: ç¤¾åŒºèšç„¦ï¼šç”¨ Mojo ğŸ”¥ æˆ˜èƒœ Rust âš™ï¸ DNA åºåˆ—è§£æåŸºå‡†æµ‹è¯•ï¼Œæ€§èƒ½æå‡50%'
- en: æ¥æºï¼š[https://www.modular.com/blog/outperforming-rust-benchmarks-with-mojo](https://www.modular.com/blog/outperforming-rust-benchmarks-with-mojo)
  id: totrans-split-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: æ¥æºï¼š[https://www.modular.com/blog/outperforming-rust-benchmarks-with-mojo](https://www.modular.com/blog/outperforming-rust-benchmarks-with-mojo)
- en: This is a guest blog post by Mohamed Mabrouk. Mohamed is the creator of the
    [MojoFastTrim](https://github.com/MoSafi2/MojoFastTrim), a Mojo ğŸ”¥ community project.
    Mohamed achieved 100x benchmark improvements over Python, and a 50% improvement
    over the fastest implementation in Rust. He quickly learned the language, and
    it took only 200 lines of code for the first implementation. Read on for details
    about the extra optimizations he applied, to beat the fastest existing benchmarks!
  id: totrans-split-6
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: è¿™æ˜¯ Mohamed Mabrouk çš„å®¢åº§åšå®¢æ–‡ç« ã€‚Mohamed æ˜¯ [MojoFastTrim](https://github.com/MoSafi2/MojoFastTrim)
    çš„åˆ›ä½œè€…ï¼Œä¸€ä¸ª Mojo ğŸ”¥ ç¤¾åŒºé¡¹ç›®ã€‚Mohamed åœ¨Pythonä¸Šå–å¾—äº†100å€çš„æ€§èƒ½æå‡ï¼Œå¹¶æ¯”Rustæœ€å¿«çš„å®ç°æé«˜äº†50%ã€‚ä»–å¾ˆå¿«å­¦ä¼šäº†è¿™é—¨è¯­è¨€ï¼Œç¬¬ä¸€æ¬¡å®ç°ä»…ç”¨äº†200è¡Œä»£ç ã€‚ç»§ç»­é˜…è¯»äº†è§£ä»–åº”ç”¨çš„é¢å¤–ä¼˜åŒ–ç»†èŠ‚ï¼Œä»¥æˆ˜èƒœç°æœ‰æœ€å¿«åŸºå‡†æµ‹è¯•ï¼
- en: The era of big-data in bioinformatics
  id: totrans-split-7
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: ç”Ÿç‰©ä¿¡æ¯å­¦çš„å¤§æ•°æ®æ—¶ä»£
- en: The challenges for bioinformatics in the modern day, are rooted in big-data
    manipulation. Thousands of multi-million dollar DNA-sequencing machines are working
    non-stop in all fields of biotechnology, medicine, and biomedical research. The
    annual sequencing data size is expected to be up to [40 exabytes](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4494865/)
    of raw sequences by 2025\. That's **20x** the data uploaded to YouTube every year.
  id: totrans-split-8
  prefs: []
  type: TYPE_NORMAL
  zh: å½“ä»Šç”Ÿç‰©ä¿¡æ¯å­¦é¢ä¸´çš„æŒ‘æˆ˜æ ¹æºäºå¤§æ•°æ®å¤„ç†ã€‚æˆåƒä¸Šä¸‡å°ä»·å€¼æ•°ç™¾ä¸‡ç¾å…ƒçš„DNAæµ‹åºæœºå™¨åœ¨ç”Ÿç‰©æŠ€æœ¯ã€åŒ»å­¦å’Œç”Ÿç‰©åŒ»å­¦ç ”ç©¶çš„æ‰€æœ‰é¢†åŸŸä¸­ä¸åœåœ°å·¥ä½œã€‚é¢„è®¡åˆ°2025å¹´ï¼Œæ¯å¹´çš„æµ‹åºæ•°æ®è§„æ¨¡å°†è¾¾åˆ°[40è‰¾å­—èŠ‚](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4494865/)çš„åŸå§‹åºåˆ—æ•°æ®ã€‚è¿™ç›¸å½“äºYouTubeæ¯å¹´ä¸Šä¼ çš„æ•°æ®é‡çš„**20å€**ã€‚
- en: While most of the final analysis is carried out in high-level languages like
    Python and R, the world of bioinformatics is powered by an underlayer of black
    magic! Highly-optimized tools written in C, C++, and Java that pre-process and
    summarize large amount of raw data. This creates a two-world problem where bioinformaticians
    who are not skilled in low-level languages, are prohibited from understanding,
    customizing, and implementing low-level operations. In addition, typical bioinformatic
    pipelines are a mixture of Bash and Python scripts calling into pre-compiled binaries,
    along with the analysis logic itself. It's becoming increasingly complex and frustrating
    for new and experienced bioinformaticians. This is the same issue that the AIÂ community
    is facing.
  id: totrans-split-9
  prefs: []
  type: TYPE_NORMAL
  zh: å¤§å¤šæ•°æœ€ç»ˆåˆ†ææ˜¯åœ¨é«˜çº§è¯­è¨€å¦‚Pythonå’ŒRä¸­è¿›è¡Œçš„ï¼Œä½†ç”Ÿç‰©ä¿¡æ¯å­¦çš„ä¸–ç•Œå´æ˜¯ç”±Cã€C++å’ŒJavaç­‰ä½å±‚é»‘é­”æ³•é©±åŠ¨çš„ï¼è¿™äº›é«˜åº¦ä¼˜åŒ–çš„å·¥å…·ç”¨äºé¢„å¤„ç†å’Œæ€»ç»“å¤§é‡åŸå§‹æ•°æ®ã€‚è¿™å°±äº§ç”Ÿäº†ä¸€ä¸ªåŒé‡ä¸–ç•Œçš„é—®é¢˜ï¼Œé‚£äº›ä¸ç†Ÿæ‚‰ä½çº§è¯­è¨€çš„ç”Ÿç‰©ä¿¡æ¯å­¦å®¶æ— æ³•ç†è§£ã€å®šåˆ¶å’Œå®ç°ä½çº§æ“ä½œã€‚æ­¤å¤–ï¼Œå…¸å‹çš„ç”Ÿç‰©ä¿¡æ¯å­¦æµæ°´çº¿æ˜¯Bashå’ŒPythonè„šæœ¬ä¸é¢„ç¼–è¯‘äºŒè¿›åˆ¶æ–‡ä»¶æ··åˆï¼Œè¿˜åŒ…æ‹¬åˆ†æé€»è¾‘æœ¬èº«ã€‚å¯¹æ–°è€ç”Ÿç‰©ä¿¡æ¯å­¦å®¶æ¥è¯´ï¼Œè¿™ç§å¤æ‚æ€§å’ŒæŒ«æŠ˜æ„Ÿè¶Šæ¥è¶Šä¸¥é‡ã€‚è¿™ä¸äººå·¥æ™ºèƒ½ç¤¾åŒºæ‰€é¢ä¸´çš„é—®é¢˜ç›¸åŒã€‚
- en: Mojo ğŸ”¥ one tool to rule them all
  id: totrans-split-10
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Mojo ğŸ”¥ ç»Ÿæ²»ä¸€åˆ‡çš„å·¥å…·
- en: 'I first heard about Mojo from the demo video by [Jeremy Howard](https://www.youtube.com/watch?v=6GvB5lZJqcE).
    Its value offer is simple, a Pythonic language that allows the programmer to optimize
    at a much lower level, to unify the fragmentation in fields such as AI. Learning
    Mojo was relatively easy for me, coming from the Python world, I got used to the
    extra syntax in only a few days. I decided to try Mojo ğŸ”¥ in a serious project
    for a low-level bioinformatic task; FASTQ parsing and quality trimming. [FASTQ](https://en.wikipedia.org/wiki/FASTQ_format)
    is a basic format for most DNA sequencing operations, incorporating both the genomic
    sequence and confidence scores of the machine in each base call. It is a simple
    format to parse, with most records looking like this:'
  id: totrans-split-11
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ç¬¬ä¸€æ¬¡ä»[Jeremy Howard](https://www.youtube.com/watch?v=6GvB5lZJqcE)çš„æ¼”ç¤ºè§†é¢‘ä¸­äº†è§£åˆ°Mojoã€‚å®ƒçš„ä»·å€¼æè®®å¾ˆç®€å•ï¼Œå®ƒæ˜¯ä¸€ç§Pythonicè¯­è¨€ï¼Œå…è®¸ç¨‹åºå‘˜åœ¨æ›´ä½çš„å±‚æ¬¡ä¸Šè¿›è¡Œä¼˜åŒ–ï¼Œä»¥ç»Ÿä¸€äººå·¥æ™ºèƒ½ç­‰é¢†åŸŸçš„ç¢ç‰‡åŒ–ã€‚å¯¹æˆ‘æ¥è¯´ï¼Œä»Pythonä¸–ç•Œæ¥çœ‹ï¼Œå­¦ä¹ Mojoç›¸å¯¹å®¹æ˜“ï¼Œåªç”¨äº†å‡ å¤©å°±ä¹ æƒ¯äº†é¢å¤–çš„è¯­æ³•ã€‚æˆ‘å†³å®šåœ¨ä¸€ä¸ªä¸¥è‚ƒçš„é¡¹ç›®ä¸­å°è¯•Mojo
    ğŸ”¥ï¼Œç”¨äºä½çº§ç”Ÿç‰©ä¿¡æ¯å­¦ä»»åŠ¡ï¼šFASTQè§£æå’Œè´¨é‡ä¿®å‰ªã€‚[FASTQ](https://en.wikipedia.org/wiki/FASTQ_format)æ˜¯å¤§å¤šæ•°DNAæµ‹åºæ“ä½œçš„åŸºæœ¬æ ¼å¼ï¼ŒåŒ…å«äº†æ¯ä¸ªç¢±åŸºå‘¼å«çš„åŸºå› ç»„åºåˆ—å’Œæœºå™¨çš„ç½®ä¿¡åº¦åˆ†æ•°ã€‚è¿™æ˜¯ä¸€ä¸ªç®€å•çš„è§£ææ ¼å¼ï¼Œå¤§å¤šæ•°è®°å½•çœ‹èµ·æ¥åƒè¿™æ ·ï¼š
- en: FASTQ
  id: totrans-split-12
  prefs: []
  type: TYPE_NORMAL
  zh: FASTQ
- en: '@SEQ_ID # New read head + read_id GATTTGGGGTTCAAAGCAGTATCGATCAAATAGTAAA...
    # DNA sequence + # Quality header + id (optional) !''''*((((***+))%%%++)(%%%%).1***-+*'''')...
    # Phred quality score'
  id: totrans-split-13
  prefs: []
  type: TYPE_NORMAL
  zh: '@SEQ_ID # æ–°çš„è¯»å–å¤´ + read_id GATTTGGGGTTCAAAGCAGTATCGATCAAATAGTAAA... # DNA åºåˆ—
    + # è´¨é‡æ ‡å¤´ + idï¼ˆå¯é€‰ï¼‰ï¼''''*((((***+))%%%++)(%%%%).1***-+*'''')... # Phred è´¨é‡åˆ†æ•°'
- en: However, typical uncompressed file sizes are 1-50 GB, an average sequence-heavy
    study could generate north of 1 TB for a single file.Â Performance is critical
    in parsing and data manipulation.
  id: totrans-split-14
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶è€Œï¼Œå…¸å‹çš„æœªå‹ç¼©æ–‡ä»¶å¤§å°ä¸º1-50 GBï¼Œä¸€ä¸ªå¹³å‡ä»¥åºåˆ—ä¸ºä¸»çš„ç ”ç©¶å¯èƒ½ä¸ºå•ä¸ªæ–‡ä»¶ç”Ÿæˆè¶…è¿‡1 TB çš„æ•°æ®ã€‚åœ¨è§£æå’Œæ•°æ®å¤„ç†ä¸­ï¼Œæ€§èƒ½è‡³å…³é‡è¦ã€‚
- en: 'I tried to write a simple parser that would:'
  id: totrans-split-15
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘å°è¯•ç¼–å†™ä¸€ä¸ªç®€å•çš„è§£æå™¨ï¼Œå®ƒå¯ä»¥ï¼š
- en: 1\. Read a chunk of the file as a *String*.
  id: totrans-split-16
  prefs: []
  type: TYPE_NORMAL
  zh: 1\. ä½œä¸º *String* è¯»å–æ–‡ä»¶çš„ä¸€ä¸ªå—ã€‚
- en: 2\. Split the string on the newline *\n* separator.
  id: totrans-split-17
  prefs: []
  type: TYPE_NORMAL
  zh: 2\. æ ¹æ®æ¢è¡Œç¬¦ *\n* åˆ†éš”å­—ç¬¦ä¸²ã€‚
- en: 3\. Take each 4 lines, validate that they are a consistent and correct FASTQ
    record, and return it.
  id: totrans-split-18
  prefs: []
  type: TYPE_NORMAL
  zh: 3\. è·å–æ¯4è¡Œï¼ŒéªŒè¯å®ƒä»¬æ˜¯å¦æ˜¯ä¸€è‡´ä¸”æ­£ç¡®çš„ FASTQ è®°å½•ï¼Œå¹¶è¿”å›å®ƒã€‚
- en: 4\. Rinse and repeat until reaching *EOF*.
  id: totrans-split-19
  prefs: []
  type: TYPE_NORMAL
  zh: 4\. åå¤æ‰§è¡Œç›´åˆ°è¾¾åˆ° *EOF*ã€‚
- en: On the [first try](https://github.com/MoSafi2/MojoFastTrim/tree/v0.1), *MojoFastTrim
    ğŸ”¥* achieved **8x** the performance of python's [SeqIO](https://biopython.org/wiki/SeqIO).
    I was pleasantly surprised with the development time. My code was still Pythonic,
    concise at around 200 lines, and using features the average python developer would
    understand. In quality trimming, where low quality bases are removed from each
    read, it achieved 50%-80% of the industry standard tool [Cutadapt](https://github.com/marcelm/cutadapt).
    This was a surprising level of performance for development time I put into the
    project.
  id: totrans-split-20
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ [ç¬¬ä¸€æ¬¡å°è¯•](https://github.com/MoSafi2/MojoFastTrim/tree/v0.1)ï¼Œ*MojoFastTrim ğŸ”¥*
    çš„æ€§èƒ½è¾¾åˆ°äº† Python çš„ [SeqIO](https://biopython.org/wiki/SeqIO) çš„ **8x**ã€‚æˆ‘å¯¹å¼€å‘æ—¶é—´æ„Ÿåˆ°æ„‰å¿«çš„æƒŠè®¶ã€‚æˆ‘çš„ä»£ç ä»ç„¶ä¿æŒ
    Python é£æ ¼ï¼Œç®€æ´åœ°çº¦200è¡Œï¼Œå¹¶ä½¿ç”¨æ™®é€š Python å¼€å‘è€…èƒ½ç†è§£çš„åŠŸèƒ½ã€‚åœ¨è´¨é‡ä¿®å‰ªä¸­ï¼Œä»æ¯ä¸ªè¯»å–ä¸­åˆ é™¤ä½è´¨é‡ç¢±åŸºï¼Œå®ƒè¾¾åˆ°äº†è¡Œä¸šæ ‡å‡†å·¥å…· [Cutadapt](https://github.com/marcelm/cutadapt)
    çš„50%-80%ã€‚å¯¹æˆ‘æŠ•å…¥åˆ°é¡¹ç›®ä¸­çš„å¼€å‘æ—¶é—´æ¥è¯´ï¼Œè¿™æ˜¯æƒŠäººçš„æ€§èƒ½æ°´å¹³ã€‚
- en: Going down the optimization rabbit hole
  id: totrans-split-21
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: æ²‰è¿·äºä¼˜åŒ–çš„å…”å­æ´
- en: 'The most powerful benefit of Mojo ğŸ”¥ is that it gives you access to low-level
    optimizations. The nascent state of the Mojo standard library meant that I had
    to write, test, and benchmark some functions from the ground up. Mojo''s first-class
    support of *SIMD* vectorization was really helpful and surprisingly intuitive.
    Here is the implementation of the vectorized version of a function to find the
    index of the newline separator in Mojo:'
  id: totrans-split-22
  prefs: []
  type: TYPE_NORMAL
  zh: Mojo ğŸ”¥ æœ€å¼ºå¤§çš„å¥½å¤„åœ¨äºå®ƒä¸ºæ‚¨æä¾›äº†è®¿é—®ä½çº§ä¼˜åŒ–çš„èƒ½åŠ›ã€‚Mojo æ ‡å‡†åº“çš„åˆæœŸçŠ¶æ€æ„å‘³ç€æˆ‘ä¸å¾—ä¸ä»å¤´å¼€å§‹ç¼–å†™ã€æµ‹è¯•å’ŒåŸºå‡†ä¸€äº›å‡½æ•°ã€‚Mojo å¯¹ *SIMD*
    å‘é‡åŒ–çš„ä¸€æµæ”¯æŒéå¸¸æœ‰å¸®åŠ©ï¼Œè€Œä¸”ä»¤äººæƒŠè®¶åœ°ç›´è§‚ã€‚è¿™é‡Œæ˜¯åœ¨ Mojo ä¸­å®ç°å‘é‡åŒ–ç‰ˆæœ¬çš„å‡½æ•°ï¼Œç”¨äºæ‰¾åˆ°æ¢è¡Œç¬¦ç´¢å¼•çš„å®ç°ï¼š
- en: Iterative
  id: totrans-split-23
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: è¿­ä»£çš„
- en: Mojo
  id: totrans-split-24
  prefs: []
  type: TYPE_NORMAL
  zh: Mojo
- en: '@always_inline fn find_chr_next_occurance_iter[ T: DType ](in_tensor: Tensor[T],
    chr: Int = 10, start: Int = 0) -> Int: for i in range(start, in_tensor.num_elements()):
    if in_tensor[i] == chr: return i return -1'
  id: totrans-split-25
  prefs: []
  type: TYPE_NORMAL
  zh: '@always_inline fn find_chr_next_occurance_iter[ T: DType ](in_tensor: Tensor[T],
    chr: Int = 10, start: Int = 0) -> Int: for i in range(start, in_tensor.num_elements()):
    if in_tensor[i] == chr: return i return -1'
- en: SIMDÂ Vectorized
  id: totrans-split-26
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: SIMDÂ Vectorized
- en: Mojo
  id: totrans-split-27
  prefs: []
  type: TYPE_NORMAL
  zh: Mojo
- en: '@always_inline fn find_chr_next_occurance_simd[ T: DType ](in_tensor: Tensor[T],
    chr: Int = 10, start: Int = 0) -> Int: alias simd_width = simdwidthof[T]() let
    len = in_tensor.num_elements() - start let aligned = start + math.align_down(len,
    simd_width) for s in range(start, aligned, simd_width): let v = in_tensor.simd_load[simd_width](s)
    let mask = v == chr if mask.reduce_or(): return s + arg_true(mask) # Handling
    other elements iteratively return -1'
  id: totrans-split-28
  prefs: []
  type: TYPE_NORMAL
  zh: '@always_inline fn find_chr_next_occurance_simd[ T: DType ](in_tensor: Tensor[T],
    chr: Int = 10, start: Int = 0) -> Int: alias simd_width = simdwidthof[T]() let
    len = in_tensor.num_elements() - start let aligned = start + math.align_down(len,
    simd_width) for s in range(start, aligned, simd_width): let v = in_tensor.simd_load[simd_width](s)
    let mask = v == chr if mask.reduce_or(): return s + arg_true(mask) # Handling
    other elements iteratively return -1'
- en: The vectorized version loads 32-elements of *Int8* and checks the presence of
    a new-line separator using fewer operations.
  id: totrans-split-29
  prefs: []
  type: TYPE_NORMAL
  zh: å‘é‡åŒ–ç‰ˆæœ¬åŠ è½½32ä¸ª *Int8* å…ƒç´ ï¼Œå¹¶ä½¿ç”¨æ›´å°‘çš„æ“ä½œæ£€æŸ¥æ¢è¡Œç¬¦çš„å­˜åœ¨ã€‚
- en: In the following graph, you can see the effect of SIMD vectorization. It provides
    up to **4x** speed up, with average speed up of **3.2x**. Similarly, *SIMD* storing
    and loading from tensors providers substantial performance gains.
  id: totrans-split-30
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ä»¥ä¸‹å›¾è¡¨ä¸­ï¼Œæ‚¨å¯ä»¥çœ‹åˆ° SIMD å‘é‡åŒ–çš„æ•ˆæœã€‚å®ƒæä¾›äº†é«˜è¾¾ **4x** çš„åŠ é€Ÿï¼Œå¹³å‡åŠ é€Ÿåº¦ä¸º **3.2x**ã€‚åŒæ ·ï¼Œ*SIMD* ä»å¼ é‡ä¸­çš„å­˜å‚¨å’ŒåŠ è½½æä¾›äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚
- en: 'In addition, I explored optimizations from C/C++ implementations. I was concerned
    that no explicit memory buffer was allocated for the loaded chunks, but the Mojo
    compiler was already taking care of that and avoiding new memory allocations:'
  id: totrans-split-31
  prefs: []
  type: TYPE_NORMAL
  zh: æ­¤å¤–ï¼Œæˆ‘æ¢ç´¢äº†æ¥è‡ªC/C++å®ç°çš„ä¼˜åŒ–ã€‚æˆ‘æ‹…å¿ƒåŠ è½½çš„å—æ²¡æœ‰ä¸ºæ˜ç¡®çš„å†…å­˜ç¼“å†²åŒºè¿›è¡Œåˆ†é…ï¼Œä½†Mojoç¼–è¯‘å™¨å·²ç»ç…§é¡¾åˆ°äº†è¿™ä¸€ç‚¹ï¼Œå¹¶é¿å…äº†æ–°çš„å†…å­˜åˆ†é…ï¼š
- en: Mojo
  id: totrans-split-32
  prefs: []
  type: TYPE_NORMAL
  zh: Mojo
- en: 'fn test_chunk_ptr_index() raises: let f = open("large_fastq_file.fastq", "r")
    var offset = 0 var chunk_no = 0 while True: let t = f.read_bytes(Size) print(t._ptr.__as_index())
    offset += Size chunk_no += 1 _ = f.seek(offset) if t.num_elements() == 0: break
    # memory adddress of chunk 0 : 139711892942912 # memory adddress of chunk 1 :
    139711892942912 # memory adddress of chunk 2 : 139711892942912 # memory adddress
    of chunk 3 : 139711892942912 # memory adddress of chunk 4 : 139711892942912'
  id: totrans-split-33
  prefs: []
  type: TYPE_NORMAL
  zh: 'fn test_chunk_ptr_index() raises: let f = open("large_fastq_file.fastq", "r")
    var offset = 0 var chunk_no = 0 while True: let t = f.read_bytes(Size) print(t._ptr.__as_index())
    offset += Size chunk_no += 1 _ = f.seek(offset) if t.num_elements() == 0: break
    # chunk 0çš„å†…å­˜åœ°å€ï¼š139711892942912 # chunk 1çš„å†…å­˜åœ°å€ï¼š139711892942912 # chunk 2çš„å†…å­˜åœ°å€ï¼š139711892942912
    # chunk 3çš„å†…å­˜åœ°å€ï¼š139711892942912 # chunk 4çš„å†…å­˜åœ°å€ï¼š139711892942912'
- en: Implementing those optimizations resulted in an extra **3x** speedup, and *MojoFastTrim
    ğŸ”¥* was on average **24x** more performant than Python's *SeqIO*. In addition,
    due to control over reference and value semantics in Mojo ğŸ”¥ I applied a *FastParser*
    version of the parser. No memory copies are made during parsing and the individual
    reads are passed around as references to the loaded chunk in memory. This approach
    is implemented in Rust's [needletail](https://github.com/onecodex/needletail)
    parser. Although Mojo is still a young language, my implementation was **50%**
    faster than the Rust implementation on Apple Silicon, and **100x** faster than
    *SeqIO*. In quality trimming, *MojoFastTrim ğŸ”¥* was on average **2x** faster than
    the highly-optimized Python/Cython *Cutadapt*.
  id: totrans-split-34
  prefs: []
  type: TYPE_NORMAL
  zh: å®ç°è¿™äº›ä¼˜åŒ–å¯¼è‡´é¢å¤–çš„**3x**é€Ÿåº¦æå‡ï¼Œè€Œ*MojoFastTrim ğŸ”¥*å¹³å‡æ¯”Pythonçš„*SeqIO*å¿«äº†**24x**ã€‚æ­¤å¤–ï¼Œç”±äºMojoå¯¹å¼•ç”¨å’Œå€¼è¯­ä¹‰çš„æ§åˆ¶ğŸ”¥ï¼Œæˆ‘åº”ç”¨äº†è§£æå™¨çš„*FastParser*ç‰ˆæœ¬ã€‚åœ¨è§£ææœŸé—´ä¸ä¼šè¿›è¡Œå†…å­˜å¤åˆ¶ï¼Œå¹¶ä¸”å„ä¸ªè¯»å–éƒ½è¢«ä¼ é€’ä¸ºå¯¹å†…å­˜ä¸­åŠ è½½çš„å—çš„å¼•ç”¨ã€‚è¿™ç§æ–¹æ³•åœ¨Rustçš„[needletail](https://github.com/onecodex/needletail)è§£æå™¨ä¸­å®ç°ã€‚å°½ç®¡Mojoä»ç„¶æ˜¯ä¸€é—¨å¹´è½»çš„è¯­è¨€ï¼Œä½†æˆ‘çš„å®ç°åœ¨Apple
    Siliconä¸Šæ¯”Rustå®ç°å¿«äº†**50%**ï¼Œæ¯”*SeqIO*å¿«äº†**100x**ã€‚åœ¨è´¨é‡ä¿®å‰ªæ–¹é¢ï¼Œ*MojoFastTrim ğŸ”¥*å¹³å‡æ¯”é«˜åº¦ä¼˜åŒ–çš„Python/Cythonçš„*Cutadapt*å¿«äº†**2x**ã€‚
- en: This benchmark can be reproduced by [following the instructions here.](https://github.com/MoSafi2/MojoFastTrim?tab=readme-ov-file#setup)
  id: totrans-split-35
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¸ªåŸºå‡†å¯ä»¥é€šè¿‡[æŒ‰ç…§è¿™é‡Œçš„è¯´æ˜](https://github.com/MoSafi2/MojoFastTrim?tab=readme-ov-file#setup)æ¥é‡ç°ã€‚
- en: Final thoughts
  id: totrans-split-36
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: æœ€åçš„æ€è€ƒ
- en: For Python programmers wanting to write more performant code, Mojo is a great
    tool to try, and easy to learn. However, the language and the ecosystem is still
    growing, I had to use print debugging to gain insight into the bugs I was encountering.
    The debugger is still in preview and undocumented, although they tell me it will
    be officially launching soon!
  id: totrans-split-37
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹äºæƒ³è¦ç¼–å†™æ›´å…·æ€§èƒ½çš„Pythonç¨‹åºå‘˜æ¥è¯´ï¼ŒMojoæ˜¯ä¸€ä¸ªå€¼å¾—å°è¯•å’Œå­¦ä¹ çš„å¥½å·¥å…·ã€‚ç„¶è€Œï¼Œè¯­è¨€å’Œç”Ÿæ€ç³»ç»Ÿä»åœ¨æˆé•¿ï¼Œæˆ‘ä¸å¾—ä¸ä½¿ç”¨æ‰“å°è°ƒè¯•æ¥æ·±å…¥äº†è§£æˆ‘é‡åˆ°çš„é”™è¯¯ã€‚è°ƒè¯•å™¨ä»å¤„äºé¢„è§ˆçŠ¶æ€ä¸”æœªç»è®°å½•ï¼Œå°½ç®¡ä»–ä»¬å‘Šè¯‰æˆ‘å®ƒå¾ˆå¿«å°†æ­£å¼æ¨å‡ºï¼
- en: In conclusion, I think that Mojo can be a radical change for a wide range of
    Python trained scientists and researchers across many fields. It can enable them
    to have a level of performance and control, that was previously unachievable.
  id: totrans-split-38
  prefs: []
  type: TYPE_NORMAL
  zh: æ€»ä¹‹ï¼Œæˆ‘è®¤ä¸ºMojoå¯ä»¥æˆä¸ºPythonè®­ç»ƒçš„ç§‘å­¦å®¶å’Œç ”ç©¶äººå‘˜åœ¨è®¸å¤šé¢†åŸŸè¿›è¡Œæ¿€è¿›æ”¹å˜çš„å·¥å…·ã€‚å®ƒå¯ä»¥è®©ä»–ä»¬åœ¨æ€§èƒ½å’Œæ§åˆ¶æ–¹é¢è¾¾åˆ°ä»¥å‰æ— æ³•å®ç°çš„æ°´å¹³ã€‚
- en: Thanks for reading!
  id: totrans-split-39
  prefs: []
  type: TYPE_NORMAL
  zh: æ„Ÿè°¢é˜…è¯»ï¼
