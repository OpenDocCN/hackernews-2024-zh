- en: <!--yml
  id: totrans-split-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-split-1
  prefs: []
  type: TYPE_NORMAL
  zh: 'category: 未分类'
- en: 'date: 2024-05-27 14:39:40'
  id: totrans-split-2
  prefs: []
  type: TYPE_NORMAL
  zh: 'date: 2024-05-27 14:39:40'
- en: -->
  id: totrans-split-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: New localllm lets you develop gen AI apps locally, without GPUs | Google Cloud
    Blog
  id: totrans-split-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 新的localllm让您在没有GPU的情况下本地开发通用AI应用 | 谷歌云博客
- en: 来源：[https://cloud.google.com/blog/products/application-development/new-localllm-lets-you-develop-gen-ai-apps-locally-without-gpus](https://cloud.google.com/blog/products/application-development/new-localllm-lets-you-develop-gen-ai-apps-locally-without-gpus)
  id: totrans-split-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://cloud.google.com/blog/products/application-development/new-localllm-lets-you-develop-gen-ai-apps-locally-without-gpus](https://cloud.google.com/blog/products/application-development/new-localllm-lets-you-develop-gen-ai-apps-locally-without-gpus)
- en: In today's fast-paced AI landscape, developers face numerous challenges when
    it comes to building applications that use large language models (LLMs). In particular,
    the scarcity of GPUs, which are traditionally required for running LLMs, poses
    a significant hurdle. In this post, we introduce you to a novel solution that
    allows developers to harness the power of LLMs locally on CPU and memory, right
    within [Cloud Workstations](https://cloud.google.com/workstations), Google Cloud’s
    fully managed development environment. The models we use in this walkthrough are
    located on [Hugging Face](https://huggingface.co/) and are specifically in a repo
    from “The Bloke” and are compatible with the quantization method used to allow
    them to run on CPUs or low power GPUs. This innovative approach not only eliminates
    the need for GPUs but also opens up a world of possibilities for seamless and
    efficient application development. By using a combination of “quantized models,”
    Cloud Workstations, a new open-source tool named [localllm](https://github.com/googlecloudplatform/localllm),
    and generally available resources, you can develop AI-based applications on a
    well-equipped development workstation, leveraging existing processes and workflows.
  id: totrans-split-6
  prefs: []
  type: TYPE_NORMAL
  zh: 在当今快节奏的人工智能领域，开发者在构建使用大语言模型（LLMs）的应用程序时面临诸多挑战。特别是，传统上用于运行LLMs的GPU稀缺性构成了重要障碍。在本篇文章中，我们为您介绍一种新颖的解决方案，允许开发者在本地CPU和内存上利用LLMs的强大功能，正好适用于[Cloud
    Workstations](https://cloud.google.com/workstations)，谷歌云的完全托管开发环境。我们在本文中使用的模型存储在[Hugging
    Face](https://huggingface.co/)上，特别来自“The Bloke”的一个repo，并且与用于在CPU或低功耗GPU上运行它们的量化方法兼容。这种创新方法不仅消除了对GPU的需求，还为无缝高效的应用程序开发打开了无限可能。通过使用“量化模型”、Cloud
    Workstations、一个名为[localllm](https://github.com/googlecloudplatform/localllm)的新开源工具和普遍可用的资源的组合，您可以在配备良好的开发工作站上开发基于人工智能的应用程序，利用现有的流程和工作流程。
- en: '**Quantized models + Cloud Workstations == Productivity**'
  id: totrans-split-7
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**量化模型 + 云工作站 == 生产力**'
- en: 'Quantized models are AI models that have been optimized to run on local devices
    with limited computational resources. These models are designed to be more efficient
    in terms of memory usage and processing power, allowing them to run smoothly on
    devices such as smartphones, laptops, and other edge devices. In this case, we
    are running them on Cloud Workstations with ample available resources. Here are
    some great examples of why leveraging quantized models in your development loop
    may unblock your efforts:'
  id: totrans-split-8
  prefs: []
  type: TYPE_NORMAL
  zh: 量化模型是经过优化以在具有有限计算资源的本地设备上运行的AI模型。这些模型在内存使用和处理能力方面设计更高效，使其能够在智能手机、笔记本电脑和其他边缘设备等设备上平稳运行。在本例中，我们在具有充足资源的Cloud
    Workstations上运行它们。以下是一些优秀的例子，说明为什么在开发过程中利用量化模型可能会解除您的障碍：
- en: '**Improved performance:** Quantized models are optimized to perform computations
    using lower-precision data types such as 8-bit integers, instead of standard 32-bit
    floating-point numbers. This reduction in precision allows for faster computations
    and improved performance on devices with limited resources.'
  id: totrans-split-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**性能提升:** 量化模型经过优化，使用低精度数据类型（如8位整数）进行计算，而不是标准的32位浮点数。这种精度降低可以实现更快的计算速度和在资源有限设备上的改进性能。'
- en: '**Reduced memory footprint:** Quantization techniques help reduce the memory
    requirements of AI models. By representing weights and activations with fewer
    bits, the overall size of the model is reduced, making it easier to fit on devices
    with limited storage capacity.'
  id: totrans-split-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**减少内存占用:** 量化技术有助于减少AI模型的内存需求。通过使用更少的位数表示权重和激活，模型的整体大小减小，更容易适配存储容量有限的设备。'
- en: '**Faster inference:** Quantized models can perform computations more quickly
    due to their reduced precision and smaller model size. This enables faster inference
    times, allowing AI applications to run more smoothly and responsively on local
    devices.'
  id: totrans-split-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**更快的推断速度**：量化模型由于其较低的精度和更小的模型尺寸，可以更快地执行计算。这使得推断时间更短，使得AI应用能够在本地设备上更加流畅和响应迅速。'
- en: Combining quantized models with Cloud Workstations allows you to take advantage
    of the flexibility, scalability and cost effectiveness of Cloud Workstations.
    Moreover, the traditional approach of relying on remote servers or cloud-based
    GPU instances for LLM-based application development can introduce latency, security
    concerns, and dependency on third-party services. A solution that lets you leverage
    LLMs locally, within your Cloud Workstations, without compromising performance,
    security, or control over your data, can have a lot of benefits.
  id: totrans-split-12
  prefs: []
  type: TYPE_NORMAL
  zh: 将量化模型与Cloud Workstations结合使用，使您可以充分利用Cloud Workstations的灵活性、可扩展性和成本效益。此外，依赖远程服务器或基于云的GPU实例进行基于LLM的应用程序开发的传统方法可能会引入延迟、安全问题和对第三方服务的依赖。在您的Cloud
    Workstations内部本地使用LLMs的解决方案，不会牺牲性能、安全性或数据控制权，具有很多好处。
- en: '**Introducing** **localllm**'
  id: totrans-split-13
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**介绍** **localllm**'
- en: Today, we’re introducing `localllm`, a set of tools and libraries that provides
    easy access to quantized models from HuggingFace through a command-line utility.
    `localllm` can be a game-changer for developers seeking to leverage LLMs without
    the constraints of GPU availability. This repository provides a comprehensive
    framework and tools to run LLMs locally on CPU and memory, right within the Google
    Cloud Workstation, using this method (though you can also run LLM models on your
    local machine or anywhere with sufficient CPU). By eliminating the dependency
    on GPUs, you can unlock the full potential of LLMs for your application development
    needs.
  id: totrans-split-14
  prefs: []
  type: TYPE_NORMAL
  zh: 今天，我们推出了`localllm`，这是一套工具和库，通过命令行实用程序提供了对HuggingFace的量化模型的便捷访问。对于希望利用LLMs而无需依赖GPU可用性的开发者来说，`localllm`可以是一个改变游戏规则的工具。该代码库提供了在CPU和内存上本地运行LLMs的全面框架和工具，可以在Google
    Cloud Workstation内部使用此方法（尽管您也可以在本地机器或任何具有足够CPU的地方运行LLM模型）。通过消除对GPU的依赖，您可以释放LLMs在应用开发需求中的全部潜力。
- en: '**Key features and benefits**'
  id: totrans-split-15
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**主要功能和好处**'
- en: '**GPU-free LLM execution**: `localllm` lets you execute LLMs on CPU and memory,
    removing the need for scarce GPU resources, so you can integrate LLMs into your
    application development workflows, without compromising performance or productivity.'
  id: totrans-split-16
  prefs: []
  type: TYPE_NORMAL
  zh: '**无需GPU的LLM执行**：`localllm`允许您在CPU和内存上执行LLMs，无需稀缺的GPU资源，因此可以将LLMs集成到应用程序开发工作流中，而无需牺牲性能或生产力。'
- en: '**Enhanced productivity**: With `localllm`, you use LLMs directly within the
    Google Cloud ecosystem. This integration streamlines the development process,
    reducing the complexities associated with remote server setups or reliance on
    external services. Now, you can focus on building innovative applications without
    managing GPUs.'
  id: totrans-split-17
  prefs: []
  type: TYPE_NORMAL
  zh: '**增强生产力**：通过`localllm`，您可以直接在Google Cloud生态系统内使用LLMs。这种集成简化了开发过程，减少了与远程服务器设置或依赖外部服务相关的复杂性。现在，您可以专注于构建创新应用，而不必管理GPU。  '
- en: '**Cost efficiency**: By leveraging `localllm`, you can significantly reduce
    infrastructure costs associated with GPU provisioning. The ability to run LLMs
    on CPU and memory within the Google Cloud environment lets you optimize resource
    utilization, resulting in cost savings and improved return on investment.'
  id: totrans-split-18
  prefs: []
  type: TYPE_NORMAL
  zh: '**成本效益**：通过利用`localllm`，您可以显著降低与GPU配置相关的基础设施成本。在Google Cloud环境中在CPU和内存上运行LLMs的能力，使您可以优化资源利用率，实现成本节约和投资回报率的提升。'
- en: '**Improved data security**: Running LLMs locally on CPU and memory helps keep
    sensitive data within your control. With `localllm`, you can mitigate the risks
    associated with data transfer and third-party access, enhancing data security
    and privacy.'
  id: totrans-split-19
  prefs: []
  type: TYPE_NORMAL
  zh: '**提升数据安全性**：在CPU和内存上运行LLMs有助于保护敏感数据。通过`localllm`，您可以减少数据传输和第三方访问的风险，增强数据安全性和隐私保护。'
- en: '**Seamless integration with Google Cloud services**: `localllm` integrates
    with various Google Cloud services, including data storage, machine learning APIs,
    or other Google Cloud services, so you can leverage the full potential of the
    Google Cloud ecosystem.'
  id: totrans-split-20
  prefs: []
  type: TYPE_NORMAL
  zh: '**与Google Cloud服务的无缝集成**：`localllm`与各种Google Cloud服务集成，包括数据存储、机器学习API或其他Google
    Cloud服务，因此您可以充分利用Google Cloud生态系统的全部潜力。'
- en: '**Getting started with** **localllm**'
  id: totrans-split-21
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**开始使用** **localllm**'
- en: To get started with the `localllm`, visit the GitHub repository at [https://github.com/googlecloudplatform/localllm](https://github.com/googlecloudplatform/localllm).
    The repository provides detailed documentation, code samples, and step-by-step
    instructions to set up and utilize LLMs locally on CPU and memory within the Google
    Cloud environment. You can explore the repository, contribute to its development,
    and leverage its capabilities to enhance your application development workflows.
  id: totrans-split-22
  prefs: []
  type: TYPE_NORMAL
  zh: 要开始使用`localllm`，请访问GitHub仓库[https://github.com/googlecloudplatform/localllm](https://github.com/googlecloudplatform/localllm)。该仓库提供了详细的文档、代码示例和逐步说明，以便在Google
    Cloud环境中在CPU和内存上设置和利用LLMs。您可以探索该仓库，为其开发贡献力量，并利用其功能来增强您的应用程序开发工作流程。
- en: Once you’ve cloned the repo locally, the following simple steps will run localllm
    with a quantized model of your choice from the HuggingFace repo “The Bloke,” then
    execute an initial sample prompt query. For example we are using Llama.
  id: totrans-split-23
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦您在本地克隆了该仓库，以下简单步骤将运行localllm，使用HuggingFace仓库中您选择的量化模型“The Bloke”，然后执行初始样本提示查询。例如，我们正在使用Llama。
