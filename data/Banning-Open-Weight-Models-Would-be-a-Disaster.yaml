- en: <!--yml
  id: totrans-split-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-split-1
  prefs: []
  type: TYPE_NORMAL
  zh: 'category: 未分类'
- en: 'date: 2024-05-27 12:51:03'
  id: totrans-split-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024年5月27日 12:51:03
- en: -->
  id: totrans-split-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: Banning Open-Weight Models Would be a Disaster
  id: totrans-split-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 禁止开放权重模型将是一场灾难。
- en: 来源：[https://rbren.substack.com/p/banning-open-weight-models-would](https://rbren.substack.com/p/banning-open-weight-models-would)
  id: totrans-split-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://rbren.substack.com/p/banning-open-weight-models-would](https://rbren.substack.com/p/banning-open-weight-models-would)
- en: In response to an [Executive Order](https://www.whitehouse.gov/briefing-room/presidential-actions/2023/10/30/executive-order-on-the-safe-secure-and-trustworthy-development-and-use-of-artificial-intelligence/)
    from President Biden, The U.S. Department of Commerce (DoC) has asked the public
    for comments on “Open-Weight AI Models”—models like LLaMa, Stable Diffusion, and
    Mixtral—which are freely distributed to the public. They are considering blocking
    access these models in order to prevent abuse.
  id: totrans-split-6
  prefs: []
  type: TYPE_NORMAL
  zh: 响应拜登总统的[行政命令](https://www.whitehouse.gov/briefing-room/presidential-actions/2023/10/30/executive-order-on-the-safe-secure-and-trustworthy-development-and-use-of-artificial-intelligence/)，美国商务部（DoC）已要求公众对“开放权重AI模型”（如LLaMa、稳定扩散和Mixtral等）发表意见。他们正在考虑阻止公众访问这些模型，以防止滥用。
- en: This would be a terrible mistake.
  id: totrans-split-7
  prefs: []
  type: TYPE_NORMAL
  zh: 这将是一个可怕的错误。
- en: '* * *'
  id: totrans-split-8
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: 'PSA: You can send the DoC your comments by clicking the “Comment” button [on
    regulations.gov](https://www.regulations.gov/document/NTIA-2023-0009-0001). Feel
    free to copy or paraphrase my comments below.'
  id: totrans-split-9
  prefs: []
  type: TYPE_NORMAL
  zh: PSA：您可以通过点击[regulations.gov](https://www.regulations.gov/document/NTIA-2023-0009-0001)上的“评论”按钮向美国商务部提交您的评论。请随意复制或改写以下我的评论。
- en: The deadline is March 27, 2024.
  id: totrans-split-10
  prefs: []
  type: TYPE_NORMAL
  zh: 截止日期为2024年3月27日。
- en: '* * *'
  id: totrans-split-11
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: The most sophisticated AI being developed today is closed-source (including,
    ironically, most of OpenAI’s work). Scientists, engineers, and the public have
    no view into the inner workings of these algorithms. If we want to use them, we’re
    forced to enter into a legal and/or financial agreement with the creator. Our
    activity is tightly restricted and monitored.
  id: totrans-split-12
  prefs: []
  type: TYPE_NORMAL
  zh: 如今开发的最复杂的AI大多是闭源的（包括讽刺的是OpenAI的大部分工作）。科学家、工程师和公众无法看到这些算法的内部运作。如果我们想使用它们，就必须与创建者签订法律和/或财务协议。我们的活动受到严格限制和监控。
- en: This is mostly normal. Corporations develop new technology, then control its
    use. ChatGPT’s closed-source nature isn’t meaningfully different from that of
    Google Search or Snapchat.
  id: totrans-split-13
  prefs: []
  type: TYPE_NORMAL
  zh: 这大多是正常现象。公司开发新技术，然后控制其使用。ChatGPT的闭源性质与Google Search或Snapchat并没有实质性区别。
- en: But competitors are on the rise, and many of them are taking a more open approach.
    Meta, Stability AI, Mistral, and other companies are giving their work away for
    free, allowing us to run state-of-the-art models on our own hardware, outside
    the reach of anyone’s oversight or control.
  id: totrans-split-14
  prefs: []
  type: TYPE_NORMAL
  zh: 但竞争对手正在崛起，并且其中许多公司采取更加开放的方式。Meta、Stability AI、Mistral等公司都免费提供他们的工作，使我们能够在自己的硬件上运行最先进的模型，超出任何监督或控制的范围。
- en: This, too, is mostly normal. People develop new technology, then give it away
    for free, since it doesn’t cost anything to distribute. The open nature of Mixtral
    or LLaMa isn’t meaningfully different from Linux or Signal.
  id: totrans-split-15
  prefs: []
  type: TYPE_NORMAL
  zh: 这也大多是正常现象。人们开发新技术，然后免费发布，因为发布不需要任何成本。Mixtral或LLaMa的开放性与Linux或Signal并没有实质性区别。
- en: The only difference with AI, is that AI is immensely powerful. The government
    is rightfully concerned about what will happen when advanced AI is widely available.
  id: totrans-split-16
  prefs: []
  type: TYPE_NORMAL
  zh: 与AI唯一的区别在于，AI具有巨大的力量。政府对广泛使用先进AI的后果感到担忧是正当的。
- en: 'To put it bluntly: closed models allow for centralized control. Open systems,
    meanwhile, are free from oversight, and any controls can be trivially bypassed
    (see, e.g. [How to Remove Stable Diffusion’s Safety Filter in 5 Seconds](https://www.reddit.com/r/StableDiffusion/comments/wv2nw0/tutorial_how_to_remove_the_safety_filter_in_5/)).'
  id: totrans-split-17
  prefs: []
  type: TYPE_NORMAL
  zh: 直白地说：封闭模型允许集中控制。与此相反，开放系统则无需监督，可以轻松绕过任何控制（见例如[如何在5秒内移除Stable Diffusion的安全过滤器](https://www.reddit.com/r/StableDiffusion/comments/wv2nw0/tutorial_how_to_remove_the_safety_filter_in_5/)）。
- en: So it’s easy to see why the government might want to prevent the distribution
    of open models.
  id: totrans-split-18
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，政府希望阻止开放模型的分发，这一点很容易理解。
- en: The biggest threats posed by AI come not from individuals, but from corporations
    and state-level actors. And they will have unfettered access to state-of-the-art
    AI no matter what.
  id: totrans-split-19
  prefs: []
  type: TYPE_NORMAL
  zh: AI带来的最大威胁并非来自个人，而是来自企业和国家级行为者。无论如何，他们都将不受限制地获得最先进的AI技术。
- en: Well-funded organizations (e.g. corporations and intelligence agencies) can
    afford to build and train their own custom models. No matter what regulatory approach
    the U.S. takes, this is a reality we will have to contend with—at the very least,
    adversarial governments will be adding advanced AI to their arsenals. We should
    expect to see a surge in coordinated disinformation campaigns, astroturfing, addiction-driven
    media, and sophisticated cybersecurity attacks.
  id: totrans-split-20
  prefs: []
  type: TYPE_NORMAL
  zh: 资金充裕的组织（例如公司和情报机构）能够负担得起建立和训练他们自己的定制模型。无论美国采取何种监管方法，这都是我们必须应对的现实——至少，敌对国家将向他们的军火库中添加先进的AI。我们应该预料到将出现协调一致的虚假信息宣传活动、伪草根运动、媒体驱动的成瘾以及复杂的网络安全攻击。
- en: Open models give the public a chance to fight back. Open models allow security
    researchers, academics, NGOs, and regulatory bodies to experiment with state-of-the-art
    technology. We can find attack patterns and build technology for detecting and
    preventing abuse. We can create good AI to combat nefarious AI.
  id: totrans-split-21
  prefs: []
  type: TYPE_NORMAL
  zh: 开放模型使公众有机会进行反击。开放模型使安全研究人员、学术界、非政府组织和监管机构能够尝试使用最先进的技术。我们可以发现攻击模式并构建技术来检测和预防滥用。我们可以创建良好的AI来对抗邪恶的AI。
- en: Open models level the playing field.
  id: totrans-split-22
  prefs: []
  type: TYPE_NORMAL
  zh: 开放模型平衡了竞争环境。
- en: Secondarily, banning open models would be a massive impediment to innovation
    and economic growth. Open models democratize access to AI technology, enabling
    use cases that are financially unviable with closed models (e.g. a local high
    school could purchase a single computer to give students unlimited access to an
    open LLM, but would need to pay perpetual fees to a closed model provider like
    OpenAI). Open models allow academics and startups to build and distribute new
    applications, undreamt of by the creators of foundation models. Open models can
    be easily built into existing workflows and applications (see e.g. community-built
    integrations for Stable Diffusion in [Photoshop](https://github.com/AbdullahAlfaraj/Auto-Photoshop-StableDiffusion-Plugin)
    and [Blender](https://github.com/benrugg/AI-Render), while DALL-E remains mostly
    walled off).
  id: totrans-split-23
  prefs: []
  type: TYPE_NORMAL
  zh: 另外，禁止开放模型将极大地阻碍创新和经济增长。开放模型使AI技术的获取民主化，使得使用闭源模型不可行的用例成为可能（例如，当地的高中可以购买一台计算机，给学生提供无限制访问开放的LLM，而使用闭源模型如OpenAI则需支付永久性费用）。开放模型使学术界和初创公司能够构建和分发新应用，这些应用连基础模型的创作者都未曾梦想过。开放模型可以轻松地集成到现有的工作流程和应用中（例如在[Photoshop](https://github.com/AbdullahAlfaraj/Auto-Photoshop-StableDiffusion-Plugin)和[Blender](https://github.com/benrugg/AI-Render)中使用的社区构建的Stable
    Diffusion集成，而DALL-E则基本上被封闭在墙内）。
- en: AI is not just a new industry, it’s an economic accelerant. Curbing access would
    exacerbate inequality, and make the U.S. less competitive in the global economy.
  id: totrans-split-24
  prefs: []
  type: TYPE_NORMAL
  zh: AI 不仅是一个新的产业，它还是经济的催化剂。限制获取将加剧不平等，并使美国在全球经济中竞争力下降。
- en: To be sure, some harm will be done by individuals with unfettered access to
    AI, mostly to other individuals. We will need laws and regulations to mitigate
    the damage. But these laws should punish people and organizations who abuse AI—they
    shouldn’t ban access to the technology entirely.
  id: totrans-split-25
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，个人对AI拥有无限制访问可能会对其他个人造成一些伤害。我们需要法律和法规来减轻损害。但这些法律应该惩罚滥用AI的人和组织，而不是完全禁止技术的访问。
- en: 'In general, there are two approaches to social harm: we can disincentivize
    it through punishment, or we can try to eradicate it through surveillance and
    control. The former is the norm in free societies, while the latter is a hallmark
    of oppression.'
  id: totrans-split-26
  prefs: []
  type: TYPE_NORMAL
  zh: 总的来说，对社会伤害有两种处理方式：我们可以通过惩罚来减少它，或者通过监视和控制来根除它。前者是自由社会的规范，而后者是压制的标志。
- en: A ban on open models might prevent some harm at the individual level, but it
    would expose us to existential threat at the societal level.
  id: totrans-split-27
  prefs: []
  type: TYPE_NORMAL
  zh: 禁止开放模型可能会在个人层面上防止一些伤害，但这将使我们在社会层面上面临存在威胁。
- en: 'Here’s an abridged version of the DoC’s summary of their RFC (emphasis mine):'
  id: totrans-split-28
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是DoC对其RFC摘要的简要版本（重点在于我自己）：
- en: Artificial intelligence (AI) has had, and will have, a significant effect on
    society, the economy, and scientific progress. **Many of the most prominent models…are
    ‘‘fully closed’’ or ‘‘highly restricted,’’**…however…an ecosystem of increasingly
    ‘‘open’’ advanced AI models [is] allowing developers and others to fine-tune models
    using widely available computing.
  id: totrans-split-29
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 人工智能（AI）对社会、经济和科学进步产生了重大影响，**许多最重要的模型……是‘‘完全封闭’’或‘‘高度受限’’**……然而……越来越‘‘开放’’的先进AI模型生态系统，允许开发者和其他人使用广泛可用的计算资源来优化模型。
- en: '**[Open models] could play a key role in fostering growth among…[s]mall businesses,
    academic institutions, underfunded entrepreneurs, and even legacy businesses**…The
    concentration of access to foundation models…poses the risk of hindering such
    innovation and advancements…These open foundation models have the potential to
    help scientists make new medical discoveries or even make mundane, time-consuming
    activities more efficient.'
  id: totrans-split-30
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**[开放模型] 在促进…小型企业、学术机构、资金不足的企业家甚至传统企业的增长中可能起到关键作用**……对基础模型访问的集中可能会阻碍这种创新和进步……这些开放基础模型有潜力帮助科学家做出新的医学发现，甚至使枯燥且耗时的活动更加高效。'
- en: '**Open foundation models have the potential to transform…medicine, pharmaceutical,
    and scientific research…**'
  id: totrans-split-31
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**开放基础模型有可能转变…医学、制药和科学研究…**'
- en: Open foundation models can allow for more transparency and enable broader access
    to allow greater oversight by technical experts, researchers, academics, and those
    from the security community…The accessibility of **open foundation models also
    provides tools for individuals and civil society groups to resist authoritarian
    regimes**, furthering democratic values and U.S. foreign policy goals.
  id: totrans-split-32
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 公开基础模型能够提供更多透明度，使技术专家、研究人员、学者以及来自安全社区的人士能够更广泛地访问和进行更广泛的监督......**公开基础模型的可访问性还为个人和公民社会团体提供了工具，以抵抗威权政权**，进一步推动民主价值观和美国外交政策目标。
- en: …**open foundation models…may pose risks as well**…such as risks to security,
    equity, civil rights, or other harms due to, for instance, affirmative misuse,
    failures of effective oversight, or lack of clear accountability mechanisms…The
    lack of monitoring of open foundation models may worsen existing challenges, for
    example, by easing **creation of synthetic non-consensual intimate images or enabling
    mass disinformation campaigns**.
  id: totrans-split-33
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: …**开放基础模型也可能带来风险**…例如由于主动滥用、有效监督的失败或缺乏明确的问责机制而造成的安全、公平、民权或其他损害……对开放基础模型的监控不足可能加剧现有挑战，例如便于**创建合成的非自愿亲密图像或实施大规模虚假信息宣传**。
- en: …the Executive order asks NTIA to consider risks and benefits of dual-use foundation
    models with weights that are ‘‘widely available.’’…
  id: totrans-split-34
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '...总统令要求NTIA考虑具有“广泛可用”权重的双用途基础模型的风险和好处......'
- en: 'The National Telecommunications and Information Administration (NTIA) has listed
    out a few dozen questions to help guide their policy here. They’re mostly great
    questions, though a few are naive or nonsensical. There are nine major sections:'
  id: totrans-split-35
  prefs: []
  type: TYPE_NORMAL
  zh: 国家电信和信息管理局（NTIA）列出了几十个问题，以帮助指导他们在此政策方面的决策。这些大部分是很好的问题，尽管有一些显得天真或荒谬。总共有九个主要部分：
- en: How should we define “open”?
  id: totrans-split-36
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: “开放”应该如何定义？
- en: How do the risks compare between open and closed models?
  id: totrans-split-37
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 开放和封闭模型之间的风险比较是如何的？
- en: What are the benefits of open models?
  id: totrans-split-38
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 开放模型有哪些好处？
- en: Besides weights, what other components matter?
  id: totrans-split-39
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 除了权重，还有哪些其他组件很重要？
- en: What are the technical issues involved in managing risk?
  id: totrans-split-40
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如何处理与管理风险相关的技术问题？
- en: What are the legal and business issues?
  id: totrans-split-41
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 法律和商业问题
- en: What regulatory mechanisms can be leveraged?
  id: totrans-split-42
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 可以利用哪些监管机制？
- en: How do we future-proof our strategy?
  id: totrans-split-43
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们如何未雨绸缪我们的战略？
- en: Other issues
  id: totrans-split-44
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 其他问题
- en: Below are my responses, which will be sent to the NTIA via the link at the top
    of this essay. I’ve bolded the most important questions if you want to skim.
  id: totrans-split-45
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是我的回答，将通过上面的链接发送给NTIA。如果你想快速浏览，我已经用粗体标记了最重要的问题。
- en: 1\. How should NTIA define ‘‘open’’ or ‘‘widely available’’ when thinking about
    foundation models and model weights?
  id: totrans-split-46
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 1\. 当思考基础模型和模型权重时，NTIA应该如何定义“开放”或“广泛可用”？
- en: A model is only “widely available” if its weights are available. Training requires
    access to immense amounts of compute and data, effectively rendering an “open-source,
    closed-weight” model useless.
  id: totrans-split-47
  prefs: []
  type: TYPE_NORMAL
  zh: 如果一个模型的权重可用，则该模型才能被称为“广泛可用”。训练需要大量的计算资源和数据，否则一个“开源但权重封闭”的模型将毫无用处。
- en: Licensing restrictions can make the availability more or less wide, but enforcing
    the license is hard. Restrictions are generally respected by risk-averse institutions,
    but can easily be ignored by individuals and state actors.
  id: totrans-split-48
  prefs: []
  type: TYPE_NORMAL
  zh: 许可限制可能会影响其可用性的广度，但强制执行许可却很难。风险规避的机构通常尊重这些限制，但个人和国家行为者则很容易无视。
- en: If anyone can download and run the model on their own hardware, the model should
    be considered “open.”
  id: totrans-split-49
  prefs: []
  type: TYPE_NORMAL
  zh: 如果任何人都可以下载并在自己的硬件上运行模型，则该模型应被视为“开放”。
- en: '**1a. Is there evidence or historical examples suggesting that weights of models
    similar to currently-closed AI systems will, or will not, likely become widely
    available? If so, what are they?**'
  id: totrans-split-50
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**1a. 是否有证据或历史例子表明类似当前闭源AI系统的模型权重可能会或不会广泛可用？如果有，它们是什么？**'
- en: Yes—software always follows this flow, thanks to low marginal cost of distribution.
    Someone creates something great, keeps it private, and enjoys a temporary monopoly.
    Eventually, individuals or a would-be competitor release not-quite-as-good open
    source, in hopes of undercutting that monopoly.
  id: totrans-split-51
  prefs: []
  type: TYPE_NORMAL
  zh: 是的——软件始终遵循这种流程，这要归功于分发的边际成本低。有人创造了一些伟大的东西，将其保密，并享受临时的垄断地位。最终，个人或潜在竞争者发布了不那么好的开源版本，希望打破那种垄断地位。
- en: While proprietary software may still find ways to maintain a large market share,
    the *technical* gap between open and closed shrinks over time.
  id: totrans-split-52
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然专有软件可能仍然找到维持大市场份额的方法，但开放和闭源之间的*技术*差距随时间缩小。
- en: Historical examples include operating systems (Windows → Linux), cloud computing
    (AWS → Kubernetes), social media (Twitter → Mastodon), and chat (WhatsApp → Signal).
  id: totrans-split-53
  prefs: []
  type: TYPE_NORMAL
  zh: 历史上的例子包括操作系统（Windows → Linux）、云计算（AWS → Kubernetes）、社交媒体（Twitter → Mastodon）和聊天（WhatsApp
    → Signal）。
- en: 1b. Is it possible to generally estimate the timeframe between the deployment
    of a closed model and the deployment of an open foundation model of similar performance
    on relevant tasks? How do you expect that timeframe to change? Based on what variables?
    How do you expect those variables to change in the coming months and years?
  id: totrans-split-54
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 1b. 一般来说，我们能估计封闭模型和开放基础模型在相关任务上部署之间的时间间隔吗？你预计这个时间间隔会如何变化？基于哪些变量？你预计这些变量在未来几个月和年份会如何变化？
- en: All I can say here is that open models have caught up to GPT incredibly fast.
    I suspect the gap will remain small. Generally the delay shrinks over time as
    proprietary knowledge gets distributed.
  id: totrans-split-55
  prefs: []
  type: TYPE_NORMAL
  zh: 我在这里可以说的是，开放模型在非常快的时间内追赶上了GPT。我怀疑这个差距会保持很小。通常，随着专有知识的分布，延迟会随时间缩短。
- en: Releasing a model openly gives an AI creator a huge competitive advantage, and
    helps to undercut more capable proprietary models. Given the immense competition
    in this space, we should expect to see more companies opening their models in
    an attempt to gain market share.
  id: totrans-split-56
  prefs: []
  type: TYPE_NORMAL
  zh: 公开发布模型为AI创作者带来了巨大的竞争优势，并帮助削弱更有能力的专有模型。鉴于这个领域的激烈竞争，我们应该预期看到更多公司公开他们的模型，以试图获得市场份额。
- en: 1c. Should ‘‘wide availability’’ of model weights be defined by level of distribution?
    If so, at what level of distribution (*e.g.,* 10,000 entities; 1 million entities;
    open publication; etc.) should model weights be presumed to be ‘‘widely available’’?
    If not, how should NTIA define ‘‘wide availability?’’
  id: totrans-split-57
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 1c. 应该根据分发水平来定义模型权重的“广泛可用性”吗？如果是，模型权重应被视为何种分发水平下的“广泛可用性”（*例如*，10,000个实体；100万个实体；开放出版等）？如果不是，NTIA应该如何定义“广泛可用性”？
- en: I don’t think this question makes sense. It’s not clear how you’d even count
    the number of entities with access to the model, unless the weights are kept closed.
  id: totrans-split-58
  prefs: []
  type: TYPE_NORMAL
  zh: 我不认为这个问题有意义。不清楚如何计算有权访问模型的实体数量，除非权重被保持关闭。
- en: 'For example: Meta could force anyone who downloads LLaMa weights to provide
    an ID and sign a restrictive license, but any bad actor can just break the license,
    redistribute the weights, etc.'
  id: totrans-split-59
  prefs: []
  type: TYPE_NORMAL
  zh: 例如：Meta可能会强制下载LLaMa权重的任何人提供ID并签署限制性许可，但任何不良行为者都可以打破许可证，重新分发权重等等。
- en: Or OpenAI could enter into private agreements to share their weights with other
    trustworthy organizations (e.g. Microsoft), but this is certainly not “wide availability”—even
    if they have thousands of such agreements.
  id: totrans-split-60
  prefs: []
  type: TYPE_NORMAL
  zh: 或者OpenAI可以与其他值得信赖的组织（例如Microsoft）达成私下协议，分享他们的权重，但这肯定不是“广泛可用性”——即使他们有成千上万这样的协议。
- en: “Wide availability” is binary—either anyone can download and run the model,
    or the model provider gates access, giving it only to highly trustworthy entities.
  id: totrans-split-61
  prefs: []
  type: TYPE_NORMAL
  zh: “广泛可用性”是二元的——要么任何人都可以下载和运行模型，要么模型提供者对接入进行控制，只授予高度信任的实体。
- en: 1d. Do certain forms of access to an open foundation model (web applications,
    Application Programming Interfaces (API), local hosting, edge deployment) provide
    more or less benefit or more or less risk than others? Are these risks dependent
    on other details of the system or application enabling access?
  id: totrans-split-62
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 1d. 对于开放基础模型的某些形式的访问（Web应用程序、API、本地托管、边缘部署），提供更多或更少的利益或风险？这些风险是否取决于系统或应用程序启用访问的其他细节？
- en: Usage can only be monitored and controlled if the weights are kept closed, and
    all usage is driven through a networked interface (e.g. a web app or REST API).
  id: totrans-split-63
  prefs: []
  type: TYPE_NORMAL
  zh: 只有将权重保持闭源，并且所有使用通过网络接口驱动（例如 Web 应用程序或 REST API），才能监控和控制使用情况。
- en: There is no middle-ground between an uncontrolled, freely distributed model,
    and a fully-controlled closed model.
  id: totrans-split-64
  prefs: []
  type: TYPE_NORMAL
  zh: 在不受控制、自由分发的模型与完全受控制的闭源模型之间没有中间地带。
- en: '**1d i. Are there promising** ***prospective*** **forms or modes of access
    that could strike a more favorable benefit-risk balance? If so, what are they?**'
  id: totrans-split-65
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**1d i. 是否存在可能能够在利益与风险之间取得更有利平衡的有前途的访问形式或方式？如果有，它们是什么？**'
- en: I could imagine something analogous to the App Store—ostensibly everyone has
    access, but there’s an application process, and a team of human reviewers have
    to get involved.
  id: totrans-split-66
  prefs: []
  type: TYPE_NORMAL
  zh: 我可以想象类似于应用商店的东西——表面上每个人都有访问权限，但需要申请流程，并且需要人工审查团队参与。
- en: But gating access to a publishing platform like the App Store is very different
    from gating a data download—once someone has downloaded the model, that access
    cannot be revoked. And they can easily redistribute the model, circumventing the
    review process.
  id: totrans-split-67
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，像应用商店这样的发布平台限制访问与限制数据下载完全不同——一旦有人下载了模型，访问权限就无法撤销。而且他们可以轻松地重新分发模型，绕过审查过程。
- en: So again, no—model access is binary.
  id: totrans-split-68
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，再次强调，模型访问是二元的。
- en: '**2\. How do the risks associated with making model weights widely available
    compare to the risks associated with non-public model weights?**'
  id: totrans-split-69
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**2\. 让模型权重广泛可用的风险与保持非公开模型权重相关的风险相比如何？**'
- en: The potential for abuse is similar in both cases—what differs is *who* is able
    to exploit that potential, and what their goals might be.
  id: totrans-split-70
  prefs: []
  type: TYPE_NORMAL
  zh: 在两种情况下滥用的潜力相似——不同之处在于能够利用这一潜力的*人*及其可能的目标。
- en: With both open and closed models, state and corporate actors will be able to
    use these models in ways that harm society. Keeping models closed only adds a
    small barrier here—large organizations have the resources to train their own models
    using publicly available information. We should expect things like coordinated
    disinformation campaigns, astroturfing, addiction-driven media, and sophisticated
    cybersecurity attacks.
  id: totrans-split-71
  prefs: []
  type: TYPE_NORMAL
  zh: 无论是开放还是闭源模型，国家和企业行为者都能够以损害社会为目的使用这些模型。将模型保持闭源只会在这里增加一道小障碍——大型组织有资源使用公开信息训练自己的模型。我们应该预期像协调的虚假信息宣传、草根模拟运动、以媒体为驱动的成瘾和复杂的网络安全攻击等事件发生。
- en: With public models, individual actors enter the fray. Mostly they will cause
    harm to other individuals, e.g. by generating fake images of private citizens.
    Public models don’t add additional existential threats to society at large, but
    will likely cause some additional harm to isolated individuals.
  id: totrans-split-72
  prefs: []
  type: TYPE_NORMAL
  zh: 对于公共模型，个体行为者参与其中。大多数情况下，他们将对其他个人造成伤害，例如通过生成的虚假私人公民图像。公共模型不会给整个社会增加额外的存在威胁，但可能会对个别个人造成一些额外伤害。
- en: 2a. What, if any, are the risks associated with widely available model weights?
    How do these risks change, if at all, when the training data or source code associated
    with fine tuning, pretraining, or deploying a model is simultaneously widely available?
  id: totrans-split-73
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 2a. 与广泛可用的模型权重相关的风险是什么？这些风险是否随着培训数据或与微调、预训练或部署模型相关的源代码同时广泛可用而改变？
- en: Widely available weights allow all individuals to use the technology in any
    way they want. Distributing source code and other assets empowers that a bit (e.g.
    making fine-tuning easier), but doesn’t meaningfully change the risk.
  id: totrans-split-74
  prefs: []
  type: TYPE_NORMAL
  zh: 广泛可用的权重允许所有个人以任何方式使用技术。分发源代码和其他资产会使微调变得更容易，但并不会实质性改变风险。
- en: The main risk here is harm to other individuals—e.g. through generated images
    of private citizens. There’s also a risk of online discourse degrading further
    than it has already, as generated images and text start to dominate the conversation.
  id: totrans-split-75
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的主要风险是对其他个人造成伤害，例如通过生成的私人公民图像。此外，还存在在线讨论可能进一步恶化的风险，因为生成的图像和文本开始主导对话。
- en: 2b. Could open foundation models reduce equity in rights and safety-impacting
    AI systems (*e.g.,* healthcare, education, criminal justice, housing, online platforms,
    etc.)?
  id: totrans-split-76
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 2b. 开放基础模型是否可能会减少在影响权利和安全的人工智能系统（如医疗、教育、刑事司法、住房、在线平台等）中的公平性？
- en: Open models are much more likely to improve equity than to reduce it.
  id: totrans-split-77
  prefs: []
  type: TYPE_NORMAL
  zh: 开放模型更可能提高公平性，而不是减少它。
- en: Low levels of equity are caused by unequal access to technology, opportunity,
    resources, etc. When large, for-profit entities capture a system, they push the
    system out of an equitable equilibrium, and into a state where they can capture
    more profit at the expense of other stakeholders.
  id: totrans-split-78
  prefs: []
  type: TYPE_NORMAL
  zh: 低平等水平是由于科技、机会、资源等不平等的获取造成的。当大型营利实体掌控系统时，他们将系统推向不公平的失衡状态，并以牺牲其他利益相关者为代价获取更多利润。
- en: Open models will prevent this sort of oligopoly from forming around AI.
  id: totrans-split-79
  prefs: []
  type: TYPE_NORMAL
  zh: 开放模型将防止围绕AI形成这种寡头垄断。
- en: '**2c. What, if any, risks related to privacy could result from the wide availability
    of model weights?**'
  id: totrans-split-80
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**2c. 宽泛可用的模型权重是否可能导致与隐私相关的风险？**'
- en: The same risks that are present with any publicly available information.
  id: totrans-split-81
  prefs: []
  type: TYPE_NORMAL
  zh: 任何公开可用信息都存在同样的风险。
- en: If private/proprietary data is discovered in a set of weights, the parties hosting
    the weights (e.g. GitHub) can easily be notified by the same mechanisms available
    today (e.g. DMCA notices).
  id: totrans-split-82
  prefs: []
  type: TYPE_NORMAL
  zh: 如果在一组权重中发现了私有/专有数据，托管这些权重的各方（例如GitHub）可以通过今天可用的相同机制（例如DMCA通知）轻松通知。
- en: With an open model, this process will be much more transparent, making compliance
    more likely. With a closed model, it’s up to the controlling organization how
    it’s handled.
  id: totrans-split-83
  prefs: []
  type: TYPE_NORMAL
  zh: 开放模型将使这一过程更加透明，从而更可能符合合规要求。而封闭模型则由控制组织决定如何处理。
- en: We see this pattern today with open and closed source software. Private software
    companies often choose not to disclose security flaws and breaches. Open source
    software is forced to disclose, since the code—along with any security patches—is
    public.
  id: totrans-split-84
  prefs: []
  type: TYPE_NORMAL
  zh: 我们今天在开源和闭源软件中看到了这一模式。私有软件公司通常选择不披露安全漏洞和违规行为。开源软件则被迫披露，因为代码及其任何安全补丁都是公开的。
- en: 2d. Are there novel ways that state or non-state actors could use widely available
    model weights to create or exacerbate security risks, including but not limited
    to threats to infrastructure, public health, human and civil rights, democracy,
    defense, and the economy?
  id: totrans-split-85
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**2d. 国家或非国家行为者是否可以利用广泛可用的模型权重创建或加剧安全风险，包括但不限于对基础设施、公共健康、人权和民权、民主、国防和经济的威胁？**'
- en: State-level actors are not much empowered by open models—they have the resources
    to build and train their own closed models.
  id: totrans-split-86
  prefs: []
  type: TYPE_NORMAL
  zh: 国家级行为者并没有因开放模型而大为增强其能力——他们有资源建立和训练自己的封闭模型。
- en: Nefarious state-level actors are mostly *hindered* by open models, which put
    security researchers, NGOs, activists, and startups on a level playing field.
  id: totrans-split-87
  prefs: []
  type: TYPE_NORMAL
  zh: 险恶的国家级行为者在很大程度上受到开放模型的*阻碍*，这使得安全研究人员、非政府组织、活动家和初创企业处于公平竞争的水平。
- en: 2d i. How do these risks compare to those associated with closed models?
  id: totrans-split-88
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**2d i. 这些风险与封闭模型相关的风险相比如何？**'
- en: The risk of state-level abuse is larger if the world only has closed models.
    Open models give the public tools to fight back against state-level actors.
  id: totrans-split-89
  prefs: []
  type: TYPE_NORMAL
  zh: 如果全世界只有封闭模型，国家级滥用的风险将更大。开放模型为公众提供了反击国家级行为者的工具。
- en: '**2d ii. How do these risks compare to those associated with other types of
    software systems and information resources?**'
  id: totrans-split-90
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**2d ii. 这些风险与其他类型的软件系统和信息资源相关的风险相比如何？**'
- en: This is a great question. The difference is not one of kind, but of magnitude.
  id: totrans-split-91
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个很好的问题。差异不是种类的不同，而是规模的不同。
- en: Encryption technology, security tooling, internet access, etc are all things
    a government might be tempted to limit the availability of, whether through regulation
    or export controls. In each case, we completely prevent small actors from accessing
    the technology, while only inconveniencing large actors.
  id: totrans-split-92
  prefs: []
  type: TYPE_NORMAL
  zh: 加密技术、安全工具、互联网接入等都是政府可能想要限制可用性的东西，无论是通过法规还是出口管制。在每种情况下，我们都完全阻止了小行为者访问技术的可能性，而只是给大行为者带来了不便。
- en: AI is more powerful than these technologies, but it follows the same dynamic.
  id: totrans-split-93
  prefs: []
  type: TYPE_NORMAL
  zh: AI比这些技术更强大，但遵循同样的动态。
- en: e. What, if any, risks could result from differences in access to widely available
    models across different jurisdictions?
  id: totrans-split-94
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**e. 不同司法管辖区对广泛可用模型的访问差异可能会导致什么风险？**'
- en: Making models available in one jurisdiction, but not another, would likely worsen
    global inequality. But it’s not a difficult regulation to skirt—you only need
    one person to smuggle the data behind the jurisdiction firewall. And VPNs make
    that easy.
  id: totrans-split-95
  prefs: []
  type: TYPE_NORMAL
  zh: 在一个司法管辖区内提供模型，而在另一个司法管辖区内不提供，可能会加剧全球不平等。但规避这种法规并不难——您只需有一个人将数据走私到管辖防火墙之后即可。而VPN使这一过程变得简单。
- en: '**f. Which are the most severe, and which the most likely risks described in
    answering the questions above? How do these set of risks relate to each other,
    if at all?**'
  id: totrans-split-96
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**f. 在回答上述问题时，哪些风险最严重，哪些最有可能？这些风险如何相互关联（如果有的话）？**'
- en: 'The most severe risks are societal-level disruptions: coordinated disinformation
    campaigns, state-scale cybersecurity attacks, AI-enhanced warfare, etc. These
    are unfortunately likely to happen to some degree. But their magnitude can be
    mitigated by ensuring security researchers and academics have open access to state-of-the-art
    models.'
  id: totrans-split-97
  prefs: []
  type: TYPE_NORMAL
  zh: 最严重的风险是社会级别的破坏：协调的虚假信息宣传活动、国家规模的网络安全攻击、增强型AI战争等。不幸的是，这些事情可能会在某种程度上发生。但可以通过确保安全研究人员和学术界可以开放获取最先进的模型来减轻其影响。
- en: Less severe—but still noteworthy—are those risks that cause individual harm.
    Generated images of private citizens could hurt reputations and cause psychological
    damage; LLMs could be used to automatically harass people via social media; people
    might be duped by fake imagery or fake news sites. These risks are slightly elevated
    in a world with open models—it becomes easier for individuals to use the software
    in harmful ways, which might have been blocked by a closed model behind an API.
  id: totrans-split-98
  prefs: []
  type: TYPE_NORMAL
  zh: 较不严重但仍值得注意的是那些会给个人带来伤害的风险。生成的私人公民图像可能会损害声誉并造成心理伤害；LLM可以用于通过社交媒体自动骚扰人们；人们可能会被虚假图像或虚假新闻网站欺骗。在开放模型的世界中，这些风险略有增加——个人更容易利用软件进行有害行为，而这可能会被API后面的封闭模型所阻止。
- en: 'So we have a tradeoff: a world with open models entails more individual risk,
    but less societal risk; a world without open models keeps individuals a bit safer,
    but puts society as a whole in danger.'
  id: totrans-split-99
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们面临一个抉择：开放模型的世界带来更多个体风险，但社会风险较少；没有开放模型的世界可以使个人稍微更安全，但会使整个社会处于危险之中。
- en: '**3\. What are the benefits of foundation models with model weights that are
    widely available as compared to fully closed models?**'
  id: totrans-split-100
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**3\. 开放模型权重与完全封闭模型相比有哪些好处？**'
- en: 'The main benefits are:'
  id: totrans-split-101
  prefs: []
  type: TYPE_NORMAL
  zh: 主要好处包括：
- en: Greater transparency
  id: totrans-split-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 更大的透明度
- en: Better security
  id: totrans-split-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 更好的安全性
- en: Lower barriers to innovation
  id: totrans-split-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 降低创新壁垒
- en: Lower barriers to market participation
  id: totrans-split-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 降低市场参与门槛
- en: Lower usage costs
  id: totrans-split-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 降低使用成本
- en: Better user experiences
  id: totrans-split-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 更好的用户体验
- en: Lower financial and political inequality
  id: totrans-split-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 降低财政和政治不平等
- en: '**a. What benefits do open model weights offer for competition and innovation,
    both in the AI marketplace and in other areas of the economy?** In what ways can
    open dual-use foundation models enable or enhance scientific research, as well
    as education/ training in computer science and related fields?'
  id: totrans-split-109
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**a. 开放模型权重在人工智能市场以及经济其他领域中对竞争和创新带来了哪些好处？开放双用基础模型如何能够促进或增强科学研究以及计算机科学及相关领域的教育/培训？**'
- en: Open models greatly reduce the barriers to entry for small- and medium-sized
    business, academic institutions, and non-profits.
  id: totrans-split-110
  prefs: []
  type: TYPE_NORMAL
  zh: 开放模型极大地降低了小型和中型企业、学术机构和非营利组织的进入门槛。
- en: It takes millions (and soon, potentially billions) of dollars to train a state-of-the-art
    LLM. Corporations will naturally try to defend their investment, preventing startups
    from using their models in ways they see as competitive.
  id: totrans-split-111
  prefs: []
  type: TYPE_NORMAL
  zh: 训练一流的LLM需要数百万美元（不久之后可能是数十亿美元）。企业自然会保护它们的投资，阻止初创企业以竞争性方式使用其模型。
- en: Open models allow any individual or startup to build products, services, and
    open source projects that leverage AI, and to distribute them without needing
    permission from a corporate entity.
  id: totrans-split-112
  prefs: []
  type: TYPE_NORMAL
  zh: 开放模型允许任何个人或初创企业构建利用人工智能的产品、服务和开源项目，并在无需企业许可的情况下分发它们。
- en: As an example, look at the myriad commercial and open source projects that leverage
    Stable Diffusion, compared to the relatively closed DALL-E. Stable Diffusion has
    driven far more innovation around in-painting/out-painting, parameter tweaking,
    animation, etc. The community has incorporated Stable Diffusion into existing
    image editing software, like Photoshop and Blender, while DALL-E remains mostly
    walled off.
  id: totrans-split-113
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，看看利用稳定扩散的众多商业和开源项目，与相对封闭的DALL-E相比。稳定扩散在修补/修复、参数调整、动画等方面带来了更多创新。社区已将稳定扩散整合到像Photoshop和Blender这样的现有图像编辑软件中，而DALL-E仍然大多封闭。
- en: '**b. How can making model weights widely available improve the safety, security,
    and trustworthiness of AI and the robustness of public preparedness against potential
    AI risks?**'
  id: totrans-split-114
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**b. 如何通过使模型权重广泛可用来提高人工智能的安全性、可信度和公众应对潜在人工智能风险的鲁棒性？**'
- en: Transparency breeds trust.
  id: totrans-split-115
  prefs: []
  type: TYPE_NORMAL
  zh: 透明性培养信任。
- en: When AI is made in private, it can be accused of (and is susceptible to) political
    bias, racial bias, etc—these biases can be inserted intentionally (e.g. by an
    ideological CEO) or accidentally (e.g. from bias in training data). Bad actors
    can insert backdoors (aka “[sleeper agents](https://www.astralcodexten.com/p/ai-sleeper-agents)”).
  id: totrans-split-116
  prefs: []
  type: TYPE_NORMAL
  zh: 当人工智能在私人领域制造时，它可能会被指责（并容易受到）政治偏见、种族偏见等，这些偏见可能是有意插入的（例如由意识形态的CEO）或者是意外的（例如来自训练数据的偏见）。不良行为者可以插入后门（又称“[沉睡间谍](https://www.astralcodexten.com/p/ai-sleeper-agents)”）。
- en: Open models, meanwhile, can be examined by researchers. Every change is public
    and can be audited.
  id: totrans-split-117
  prefs: []
  type: TYPE_NORMAL
  zh: 与此同时，研究人员可以审查开放模型。每一次变更都是公开的，可以接受审计。
- en: Furthermore, giving security researchers and academics unfettered access to
    state-of-the-art models greatly enhances public preparedness for AI risk. We can
    be informed ahead of time as to how adversarial governments might use AI to disrupt
    our economy, attack our infrastructure, or undermine democracy. We can develop
    defensive AI that is able to combat nefarious AI.
  id: totrans-split-118
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，让安全研究人员和学者无限制地接触到最先进的模型极大地增强了公众对人工智能风险的准备。我们可以提前了解对手政府如何利用人工智能来破坏我们的经济、攻击我们的基础设施或者破坏民主。我们可以开发防御性人工智能，以应对恶意人工智能。
- en: '**c. Could open model weights, and in particular the ability to retrain models,
    help advance equity in rights and safety- impacting AI systems (*****e.g.,***
    **healthcare, education, criminal justice, housing, online platforms etc.)?**'
  id: totrans-split-119
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**c. 开放模型权重，特别是重新训练模型的能力，是否有助于促进权利和安全的公平，影响人工智能系统（***例如*** **医疗保健、教育、刑事司法、住房、在线平台等）？**'
- en: Yes. An open model can be retrained by non-profits, NGOs, government agencies,
    etc in order to address public needs.
  id: totrans-split-120
  prefs: []
  type: TYPE_NORMAL
  zh: 是的。非营利组织、非政府组织、政府机构等可以重新训练开放模型，以解决公共需求。
- en: 'Specifically, an open model can help mine and analyze data relevant to socially
    beneficial missions. Examples: a social media startup could use an LLM to detect
    hate and harassment on its platform; a non-profit could use AI to analyze racial
    disparities in real estate listings; LLMs could help the incarcerated better understand
    their legal options; a social scientist could use an LLM to mine through city
    council meeting minutes for signs of corruption.'
  id: totrans-split-121
  prefs: []
  type: TYPE_NORMAL
  zh: 具体而言，开放模型可以帮助挖掘和分析与社会有益使命相关的数据。例如：社交媒体初创公司可以使用语言模型来检测其平台上的仇恨和骚扰；非营利组织可以使用人工智能来分析房地产列表中的种族差异；语言模型可以帮助被监禁者更好地了解他们的法律选择；社会科学家可以使用语言模型来分析市政会议记录中的腐败迹象。
- en: If models are kept private, these tasks may or may not be explicitly disallowed
    by the private entities that control them. Most would be financially unviable.
    Open models drastically reduce the cost of leveraging the technology, shifting
    it from a monopolized resource to a commodity.
  id: totrans-split-122
  prefs: []
  type: TYPE_NORMAL
  zh: 如果模型保持私有，这些任务可能会或可能不会被控制它们的私人实体明确禁止。大多数情况下是经济不可行的。开放模型大幅降低了利用这项技术的成本，将其从垄断资源转变为商品化。
- en: '**d. How can the diffusion of AI models with widely available weights support
    the United States’ national security interests? How could it interfere with, or
    further the enjoyment and protection of human rights within and outside of the
    United States?**'
  id: totrans-split-123
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**d. AI模型的扩散如何支持美国的国家安全利益？它可能如何干扰或促进美国内外人权的享受和保护？**'
- en: Open models are a huge benefit to U.S. national security.
  id: totrans-split-124
  prefs: []
  type: TYPE_NORMAL
  zh: 开放模型对美国国家安全有巨大的益处。
- en: Security researchers and academics can use open models to explore the state-of-the-art
    without restrictions, making vulnerabilities and attack vectors public knowledge
    faster than adversarial governments can exploit them. They can develop defensive
    AI to help us combat abuse. If we limit the ability of these researchers to access
    and study AI, adversarial governments gain a strategic advantage.
  id: totrans-split-125
  prefs: []
  type: TYPE_NORMAL
  zh: 安全研究人员和学者可以使用开放模型探索最先进的技术，没有限制，使漏洞和攻击路径更快地成为公共知识，胜过对手政府能够利用它们。他们可以开发防御性人工智能来帮助我们对抗滥用。如果我们限制这些研究人员访问和研究人工智能的能力，对手政府将获得战略优势。
- en: 'Internationally, open models put dissidents on a level playing field with their
    oppressors. For example: an authoritarian government might use generative AI to
    create propaganda, then spread it on social media. Dissidents might then leverage
    an open model to distinguish fake photographs from real ones, or to spot patterns
    of government-generated social media interactions.'
  id: totrans-split-126
  prefs: []
  type: TYPE_NORMAL
  zh: 在国际上，开放模型使异见人士与压制者处于同一起跑线上。例如：一个专制政府可能会使用生成式人工智能制造宣传，然后在社交媒体上传播。异见人士随后可能利用开放模型来区分真实照片和虚假照片，或者发现政府生成的社交媒体互动模式。
- en: Without access to AI, dissidents are left defenseless.
  id: totrans-split-127
  prefs: []
  type: TYPE_NORMAL
  zh: 如果没有人工智能的访问权限，异见人士就无法自卫。
- en: e. How do these benefits change, if at all, when the training data or the associated
    source code of the model is simultaneously widely available?
  id: totrans-split-128
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: e. 当模型的训练数据或相关源代码同时广泛可用时，这些益处如何改变（如果有的话）？
- en: Data and source code can help in fine-tuning models. But the weights are where
    most of the value is.
  id: totrans-split-129
  prefs: []
  type: TYPE_NORMAL
  zh: 数据和源代码可以帮助微调模型。但是权重是价值大部分所在。
- en: 4\. Are there other relevant components of open foundation models that, if simultaneously
    widely available, would change the risks or benefits presented by widely available
    model weights? If so, please list them and explain their impact.
  id: totrans-split-130
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 4\. 如果同时广泛公开的开放基础模型还有其他相关组件，会改变广泛公开模型权重带来的风险或益处吗？如果有，请列出并解释其影响。
- en: 'Weights, as well as the code needed to load the weights into a working model,
    are by far the most important piece. But there are a few other components that
    are helpful. Roughly in order of importance:'
  id: totrans-split-131
  prefs: []
  type: TYPE_NORMAL
  zh: 权重以及将权重加载到工作模型中所需的代码，远远是最重要的部分。但还有一些其他有用的组件。大致按重要性排序：
- en: 'Data: the data used to train the model can be evaluated for statistical bias,
    or reused in fine-tuning. It can be used to train new, competing models with improved
    performance.'
  id: totrans-split-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据：用于训练模型的数据可以评估是否存在统计偏差，或者重新用于微调。它可以用来训练新的、性能更好的竞争模型。
- en: 'Source code: Any source code beyond what’s needed to run the model itself (e.g.
    training scripts, deployment configuration, fine-tuning code, etc.) is helpful
    for folks who want to use or improve the model.'
  id: totrans-split-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 源代码：除了运行模型本身所需的内容（例如训练脚本、部署配置、微调代码等）外，任何源代码都对希望使用或改进模型的人有帮助。
- en: 'Methodology: A human-language description of the model, including motivations
    in technical choices, tradeoffs made, and references to prior work, can help others
    improve upon the model.'
  id: totrans-split-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 方法论：模型的人类语言描述，包括技术选择的动机、所做的权衡以及对先前工作的引用，可以帮助其他人改进模型。
- en: 5\. What are the safety-related or broader technical issues involved in managing
    risks and amplifying benefits of dual-use foundation models with widely available
    model weights?
  id: totrans-split-135
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 5\. 管理风险和放大双用基础模型广泛可用模型权重的益处涉及哪些与安全相关或更广泛的技术问题？
- en: 'There are two categories of risk here:'
  id: totrans-split-136
  prefs: []
  type: TYPE_NORMAL
  zh: 这里存在两类风险：
- en: People using the model in nefarious ways
  id: totrans-split-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用模型进行恶意操作的人
- en: People deploying the model in insecure ways, enabling others to abuse the model
  id: totrans-split-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将模型以不安全方式部署的人，使其他人能够滥用模型
- en: Nefarious usage can be mitigated through license agreements, but is hard to
    enforce at a technical level. Models can be trained not to generate certain types
    of images or text, but it’s hard to make any guarantees here, and fine-tuning
    can undo that work.
  id: totrans-split-139
  prefs: []
  type: TYPE_NORMAL
  zh: 通过许可协议可以减轻恶意使用，但在技术层面上难以强制执行。模型可以经过训练，不生成某些类型的图像或文本，但在这里很难做出任何保证，微调可能会撤销这些工作。
- en: Insecure deployment can be mitigated by providing reference code for deployment,
    security checklists, and add-on software for e.g. detecting attack prompts and
    harmful output.
  id: totrans-split-140
  prefs: []
  type: TYPE_NORMAL
  zh: 不安全的部署可以通过提供部署参考代码、安全检查清单以及用于检测攻击提示和有害输出的附加软件来减轻。
- en: a. What model evaluations, if any, can help determine the risks or benefits
    associated with making weights of a foundation model widely available?
  id: totrans-split-141
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: a. 如果有的话，哪些模型评估可以帮助确定广泛公开基础模型权重的风险或益处？
- en: Frameworks like the [OWASP LLM Top 10](https://owasp.org/www-project-top-10-for-large-language-model-applications/llm-top-10-governance-doc/LLM_AI_Security_and_Governance_Checklist-v1.pdf)
    are helpful for anyone releasing or deploying an LLM. Much of the advice is applicable
    to other generative AI systems.
  id: totrans-split-142
  prefs: []
  type: TYPE_NORMAL
  zh: 像[OWASP LLM Top 10](https://owasp.org/www-project-top-10-for-large-language-model-applications/llm-top-10-governance-doc/LLM_AI_Security_and_Governance_Checklist-v1.pdf)
    这样的框架对于任何发布或部署LLM的人都是有帮助的。大部分建议也适用于其他生成式人工智能系统。
- en: '[This paper](https://arxiv.org/pdf/2302.08500.pdf) suggests auditing three
    layers:'
  id: totrans-split-143
  prefs: []
  type: TYPE_NORMAL
  zh: '[这篇论文](https://arxiv.org/pdf/2302.08500.pdf) 建议审计三个层面：'
- en: For widely available models, the last step becomes harder, as there is no limit
    to the number and scope of applications. Ideally the disseminating company or
    a third-party would provide instructions for self-auditing.
  id: totrans-split-144
  prefs: []
  type: TYPE_NORMAL
  zh: 对于广泛可用的模型，最后一步变得更加困难，因为应用程序的数量和范围没有限制。理想情况下，发布公司或第三方应提供自审计的指导。
- en: '**b. Are there effective ways to create safeguards around foundation models,
    either to ensure that model weights do not become available, or to protect system
    integrity or human well-being (including privacy) and reduce security risks in
    those cases where weights are widely available?**'
  id: totrans-split-145
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**b. 是否有有效的方法来围绕基础模型创建防护措施，以确保模型权重不可用，或者保护系统完整性或人类福祉（包括隐私），并减少在权重广泛可用的情况下的安全风险？**'
- en: It’s important to understand that, at a technical level, restricting access
    to a model is a binary choice.
  id: totrans-split-146
  prefs: []
  type: TYPE_NORMAL
  zh: 在技术层面上，重要的是理解对模型访问的限制是一个二元选择。
- en: If a model is hidden behind a web API, it can be fully controlled by a single
    entity. Preventing the disclosure of the model’s weights is as trivial as keeping
    source code private, though there’s always chance of a breach (especially by a
    motivated state-level actor).
  id: totrans-split-147
  prefs: []
  type: TYPE_NORMAL
  zh: 如果一个模型被隐藏在一个 web API 后面，它可以完全由单一实体控制。防止模型权重泄露和保持源代码私有一样简单，尽管总有被入侵的风险（特别是由积极的国家级行动者）。
- en: If the weights are distributed publicly, however, anyone can download and share
    those weights with others, and can use the model however they see fit. They can
    modify the model to remove any safeguards, and redistribute those modifications.
    Any controls can be bypassed, more or less trivially.
  id: totrans-split-148
  prefs: []
  type: TYPE_NORMAL
  zh: 如果权重被公开分发，任何人都可以下载并与他人分享这些权重，并可以随意使用模型。他们可以修改模型以删除任何防护措施，并重新分发这些修改版。任何控制都可以相对容易地被绕过。
- en: You can add restrictions to an open model with a *license*, but you can’t enforce
    those restrictions technically.
  id: totrans-split-149
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以通过*许可证*对一个开放模型添加限制，但你无法在技术上强制执行这些限制。
- en: '**c. What are the prospects for developing effective safeguards in the future?**'
  id: totrans-split-150
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**c. 未来开发有效保护措施的前景如何？**'
- en: We might take some inspiration from DRM strategies, and find ways to make it
    harder to copy and redistribute LLMs. But it’s important to understand that this
    would only be an *impediment*, not a guarantee.
  id: totrans-split-151
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可能可以从 DRM 策略中获得一些灵感，并找到使复制和重新分发 LLMs 变得更加困难的方法。但重要的是要理解，这只会是一种*阻碍*，而不是保证。
- en: 'To use a metaphor: it’s like putting a lock on your front door. It’ll keep
    honest people honest, but a motivated attacker can break a window.'
  id: totrans-split-152
  prefs: []
  type: TYPE_NORMAL
  zh: 用一个比喻来说：这就像给你的前门加锁。它会让诚实的人保持诚实，但一个积极进攻的人可以打破窗户。
- en: 'This is an issue with DRM too: if you let a user watch a video, there’s nothing
    stopping them from recording that video with a camera and making copies, even
    if you keep them from the raw data of the original file.'
  id: totrans-split-153
  prefs: []
  type: TYPE_NORMAL
  zh: 这也是 DRM 的问题：如果让用户观看视频，没有什么能阻止他们用摄像机录制视频并制作副本，即使你阻止他们获取原始文件的原始数据。
- en: DRM is moderately successful because the hope is to keep the average person
    from sharing a file with their friends. But the audience for AI models is small,
    highly technical, and highly motivated—the value of the model is much higher than
    the value of a movie. A DRM-like strategy is unlikely to work.
  id: totrans-split-154
  prefs: []
  type: TYPE_NORMAL
  zh: DRM 相对成功是因为希望阻止普通人与朋友分享文件。但 AI 模型的受众小而且高度技术化、高度积极—模型的价值远高于电影。类似 DRM 的策略不太可能奏效。
- en: d. **Are there ways to regain control over and/or restrict access to and/or
    limit use of weights of an open foundation model** that, either inadvertently
    or purposely, have already become widely available? What are the approximate costs
    of these methods today? How reliable are they?
  id: totrans-split-155
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: d. **是否有办法重新获得对一个已广泛公开的基础模型权重的控制、限制对其使用？这些方法的大致成本是多少？它们的可靠性如何？**
- en: There are not. The weights represent a very large mathematical equation—once
    the equation is known, anyone can instantiate it and use it.
  id: totrans-split-156
  prefs: []
  type: TYPE_NORMAL
  zh: 没有。这些权重代表一个非常庞大的数学方程—一旦方程被知晓，任何人都可以实例化它并使用它。
- en: e. What if any secure storage techniques or practices could be considered necessary
    to prevent unintentional distribution of model weights?
  id: totrans-split-157
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: e. 如果有的话，可以考虑哪些安全存储技术或实践来防止模型权重的意外分发？
- en: 'The same storage techniques that are used for any high-value intellectual property.
    Specifically:'
  id: totrans-split-158
  prefs: []
  type: TYPE_NORMAL
  zh: 用于任何高价值知识产权的相同存储技术。具体来说：
- en: '**f. Which components of a foundation model need to be available, and to whom,
    in order to analyze, evaluate, certify, or red-team the model?** To the extent
    possible, please identify specific evaluations or types of evaluations and the
    component(s) that need to be available for each.'
  id: totrans-split-159
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**f. 基础模型的哪些组成部分需要提供，并且需要提供给谁，以便分析、评估、认证或红队测试模型？** 在可能的情况下，请具体标识每种评估或评估类型以及需要提供的组件。'
- en: The more access that’s given, the better the analysis can be.
  id: totrans-split-160
  prefs: []
  type: TYPE_NORMAL
  zh: 给予更多的访问权限，分析效果就越好。
- en: At the most restrictive level, gated API access could be given to a penetration
    tester or analyst. This will allow them to red-team the AI, e.g. attempting prompt
    injection techniques, attempting to extract personal information, testing for
    bias and hallucinations, etc.
  id: totrans-split-161
  prefs: []
  type: TYPE_NORMAL
  zh: 在最严格的级别，可以给予带有API门控访问权限的渗透测试员或分析员。这将允许他们红队测试AI，例如尝试注入提示技术、尝试提取个人信息，测试偏见和幻觉等。
- en: Granting a small team access to source code, data, and model weights can improve
    this testing. The source code can be subjected to static analysis, data can be
    analyzed for statistical bias, and the model can be probed directly, with different
    parts isolated or analyzed. This deeper level of access also allows the analyst
    to understand where certain controls are put in place. E.g. is the model itself
    trained not to output dangerous text, or is there just a thin layer on top blocking
    prompts with certain keywords?
  id: totrans-split-162
  prefs: []
  type: TYPE_NORMAL
  zh: 允许小团队访问源代码、数据和模型权重可以改善此测试。源代码可以接受静态分析，数据可以分析统计偏差，模型可以直接进行探索，不同部分可以被隔离或分析。这种更深入的访问水平还允许分析人员了解特定控制放置的位置。例如，模型本身是否经过训练以不输出危险文本，或者是否只是在顶部有一层薄膜阻止带有特定关键词的提示？
- en: Granting full, public access to all these things is best. It allows unaffiliated
    teams of researchers and academics to probe the model. For open models, failure
    patterns and flaws will be discovered and disclosed at a much higher rate.
  id: totrans-split-163
  prefs: []
  type: TYPE_NORMAL
  zh: 全面向公众开放所有这些内容是最好的。它允许独立的研究团队和学者探索该模型。对于开放模型，故障模式和缺陷的发现和披露率将大大提高。
- en: g. Are there means by which to test or verify model weights? What methodology
    or methodologies exist to audit model weights and/or foundation models?
  id: totrans-split-164
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: g. 是否有方法可以测试或验证模型权重？存在哪些方法论来审计模型权重和/或基础模型？
- en: 'There are four dimensions we should audit:'
  id: totrans-split-165
  prefs: []
  type: TYPE_NORMAL
  zh: 我们应该审计四个维度：
- en: 'Performance: how well does the model accomplish its intended task?'
  id: totrans-split-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 性能：模型如何完成其预期任务？
- en: 'Robustness: how well does the model respond to unexpected inputs?'
  id: totrans-split-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 鲁棒性：模型如何应对意外输入？
- en: 'Truthfulness: how often does the model respond with misleading output?'
  id: totrans-split-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 真实性：模型多频率回应误导性输出？
- en: 'Security: how well does the model resist outputting harmful content?'
  id: totrans-split-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 安全性：模型抵抗输出有害内容的能力如何？
- en: Each of these can be evaluated with automated standards (i.e. a common benchmark)
    but should also be subject to ad-hoc analysis.
  id: totrans-split-170
  prefs: []
  type: TYPE_NORMAL
  zh: 每个都可以通过自动化标准（即共同基准）进行评估，但也应该接受特别分析。
- en: 6\. What are the legal or business issues or effects related to open foundation
    models?
  id: totrans-split-171
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 6\. 与开放基础模型相关的法律或商业问题或影响是什么？
- en: Legal issues center on licensing and enforcement, as well as the potential for
    models to be used in illegal ways.
  id: totrans-split-172
  prefs: []
  type: TYPE_NORMAL
  zh: 法律问题集中在许可和执行上，以及模型可能被非法使用的潜力。
- en: Business issues center on usage costs, innovation, competitive advantage, and
    barriers to market participation.
  id: totrans-split-173
  prefs: []
  type: TYPE_NORMAL
  zh: 商业问题集中在使用成本、创新、竞争优势和市场参与障碍上。
- en: a. In which ways is open-source software policy analogous (or not) to the availability
    of model weights? **Are there lessons we can learn from the history and ecosystem
    of open-source software**, open data, and other ‘‘open’’ initiatives for open
    foundation models, particularly the availability of model weights?
  id: totrans-split-174
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: a. 开源软件政策与模型权重可用性的类比（或不类比）。**我们可以从开源软件、开放数据和其他“开放”倡议的历史和生态系统中学到哪些教训**，特别是基础模型的权重可用性？
- en: The history of open source software will be highly instructive here. The only
    difference between AI models and traditional software is how easy it is for a
    human to introspect the logic.
  id: totrans-split-175
  prefs: []
  type: TYPE_NORMAL
  zh: 开源软件的历史在这里将非常有教益。人工智能模型与传统软件的唯一区别是人类如何审视其逻辑。
- en: 'In particular, we should expect open models to:'
  id: totrans-split-176
  prefs: []
  type: TYPE_NORMAL
  zh: 特别是，我们应该期望开放模型能够：
- en: Deliver huge amounts of economic value
  id: totrans-split-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 提供巨大的经济价值
- en: Allow startups and small companies to compete with large enterprises
  id: totrans-split-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 允许初创企业和小公司与大企业竞争
- en: Complement and support commercial solutions
  id: totrans-split-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 补充和支持商业解决方案
- en: Find large commercial backers, who add value through support and complementary
    software
  id: totrans-split-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 找到大型商业支持者，通过支持和补充软件增加价值。
- en: Set common, industry-wide standards for distribution, deployment, APIs, etc
  id: totrans-split-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为分发、部署、API等制定共同的行业标准。
- en: Provide a higher degree of transparency to users and other stakeholders
  id: totrans-split-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 向用户和其他利益相关者提供更高程度的透明度。
- en: b. **How, if at all, does the wide availability of model weights change the
    competition dynamics in the broader economy**, specifically looking at industries
    such as but not limited to healthcare, marketing, and education?
  id: totrans-split-183
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 'b. **模型权重的广泛可用如何（如果有）改变更广泛经济的竞争动态**，特别是在医疗保健、市场营销和教育等行业？ '
- en: Openly available weights will make the market far more competitive and efficient.
  id: totrans-split-184
  prefs: []
  type: TYPE_NORMAL
  zh: 公开可用的权重将使市场更加竞争和高效。
- en: It’s extremely expensive and difficult to train a state-of-the-art model. If
    all models were kept closed, an oligopoly would form very quickly—similar to the
    market for cloud computing.
  id: totrans-split-185
  prefs: []
  type: TYPE_NORMAL
  zh: 训练一款最先进的模型是极其昂贵且困难的。如果所有模型都保持封闭状态，那么很快就会形成寡头垄断，类似于云计算市场。
- en: Furthermore, closed models must run on hardware owned by the distributor (unless
    there’s a high degree of trust between distributor and licensee). This can be
    prohibitively expensive for many use cases.
  id: totrans-split-186
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，封闭模型必须在分销商拥有的硬件上运行（除非分销商和许可证持有者之间有很高的信任度）。对于许多用例来说，这可能成本过高。
- en: For example, a local high school could easily make an LLM available to students
    and teachers by running the LLM on its own hardware; there would only be a fixed,
    initial cost for purchasing the hardware. But to used a closed LLM, they’d need
    to pay ongoing licensing fees to the distributor. The same goes for hospitals
    and other businesses.
  id: totrans-split-187
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，本地高中可以通过在自己的硬件上运行LLM轻松地为学生和教师提供LLM；只需支付购买硬件的固定初始成本。但是，如果使用封闭的LLM，则需要向分销商支付持续的许可费用。医院和其他企业也是如此。
- en: c. **How, if at all, do intellectual property-related issues—such as the license
    terms under which foundation model weights are made publicly available—influence
    competition, benefits, and risks?** Which licenses are most prominent in the context
    of making model weights widely available? What are the tradeoffs associated with
    each of these licenses?
  id: totrans-split-188
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: c. **知识产权相关问题——例如，公开基础模型权重的许可条款如何（如果有）影响竞争、利益和风险？** 在公开模型权重使其广泛可用的背景下，哪些许可证最为突出？每种许可证的权衡利弊是什么？
- en: License terms are a great way to mitigate the risk of misuse.
  id: totrans-split-189
  prefs: []
  type: TYPE_NORMAL
  zh: 许可条款是减少误用风险的重要途径。
- en: A restrictive license—e.g. that disallows using the model to represent public
    figures, or to generate violent/sexual imagery—does stop many people from engaging
    in those behaviors.
  id: totrans-split-190
  prefs: []
  type: TYPE_NORMAL
  zh: 限制性许可证——例如，禁止使用模型来代表公众人物，或生成暴力/性暗示图像——确实阻止了许多人从事这些行为。
- en: It especially prevents people from making harmful functionality available in
    a public, user-friendly way, without exposing themselves to litigation.
  id: totrans-split-191
  prefs: []
  type: TYPE_NORMAL
  zh: 它特别阻止人们以公开、用户友好的方式提供有害功能，而不会让自己卷入诉讼。
- en: That said, restrictive licenses can also reduce competition by disallowing use
    cases that compete with the creator company. But it’s perfectly within the rights
    of the creator company to add those kinds of terms (e.g. Meta’s restriction of
    LLaMa to products with fewer than 700M users).
  id: totrans-split-192
  prefs: []
  type: TYPE_NORMAL
  zh: 话虽如此，限制性许可证也会通过禁止与创建公司竞争的用例来减少竞争。但是，创建公司完全有权添加这些条款（例如Meta将LLaMa限制为少于7亿用户的产品）。
- en: It’s important to note that restrictive licenses for models work much like they
    do for open source and entertainment media—they can greatly *reduce* the amount
    of harm, but a bad actor can always ignore them.
  id: totrans-split-193
  prefs: []
  type: TYPE_NORMAL
  zh: 需要注意的是，模型的限制性许可证与开源和娱乐媒体的许可证工作方式非常相似——它们可以极大地*减少*伤害，但是坏演员总是可以忽视它们。
- en: d. Are there concerns about potential barriers to interoperability stemming
    from different incompatible ‘‘open’’ licenses, *e.g.,* licenses with conflicting
    requirements, applied to AI components? Would standardizing license terms specifically
    for foundation model weights be beneficial? Are there particular examples in existence
    that could be useful?
  id: totrans-split-194
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: d. 是否担心由不同不兼容的“开放”许可证引起的互操作性障碍，*例如*，对应用于AI组件的许可证具有冲突要求？是否有必要专门为基础模型权重标准化许可条款？现有的特定例子是否有用？
- en: Yes, standard licenses would be a huge help. I expect this will happen organically,
    as it has in the open source ecosystem.
  id: totrans-split-195
  prefs: []
  type: TYPE_NORMAL
  zh: 是的，标准许可证将是很大的帮助。我预计这将自然而然地发生，就像在开源生态系统中一样。
- en: The main concern with having many individual licenses is that legal departments
    have to get involved to read and accept each one. Having standards (like MIT,
    Apache 2.0, AGPL, etc in open source) allows organizations and lawyers to issue
    blanket guidance, so engineers can adopt new technology without getting the legal
    department with every decision.
  id: totrans-split-196
  prefs: []
  type: TYPE_NORMAL
  zh: 拥有许多个体许可证的主要问题在于，法律部门必须参与每一个许可证的阅读和接受。制定标准（例如MIT、Apache 2.0、AGPL等在开源中的标准）允许组织和律师发布全面的指导意见，使工程师可以在不需要每一次决策都牵涉法律部门的情况下采用新技术。
- en: As the market evolves, it will naturally standardize on a few different licenses.
  id: totrans-split-197
  prefs: []
  type: TYPE_NORMAL
  zh: 随着市场的发展，自然会统一几种不同的许可证。
- en: 7\. **What are current or potential voluntary, domestic regulatory, and international
    mechanisms to manage the risks and maximize the benefits of foundation models
    with widely available weights?** What kind of entities should take a leadership
    role across which features of governance?
  id: totrans-split-198
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 7\. **当前或潜在的自愿、国内监管和国际机制是什么，以管理基础模型的风险并最大化其效益，尤其是具有广泛权重的基础模型？** 哪些实体应在治理的哪些方面发挥领导作用？
- en: 'Regulations and laws around AI come in two flavors:'
  id: totrans-split-199
  prefs: []
  type: TYPE_NORMAL
  zh: 围绕AI的法规和法律有两种风味：
- en: Restricting certain usage of AI is common sense. Many current laws already apply
    to nefarious usage of AI (e.g. libel laws prevent the dissemination of fake images
    of private citizens). But new legislation here will be helpful (e.g. disallowing
    AI-generated images in advertising, or clarifying copyright law for works that
    imitate the style of a living artist).
  id: totrans-split-200
  prefs: []
  type: TYPE_NORMAL
  zh: 限制AI的某些使用是常识。许多现行法律已经适用于恶意使用AI（例如诽谤法阻止传播私人公民的虚假图像）。但在这里制定新立法将会有所帮助（例如禁止在广告中使用AI生成的图像，或者明确生效于模仿现代艺术家风格的作品的版权法）。
- en: Restricting distribution is more fraught. It would prevent law-abiding entities
    from accessing the technology, while criminals would continue to find ways to
    access it. It would completely stop positive uses, while only putting a surmountable
    hurdle in front of malicious uses.
  id: totrans-split-201
  prefs: []
  type: TYPE_NORMAL
  zh: 限制分发更加棘手。这将阻止守法实体访问技术，而罪犯则会继续找到获取技术的途径。这将完全阻止积极使用，却只在恶意使用面前设置了可逾越的障碍。
- en: That said, we can still regulate platforms and products that make AI widely
    available to end-users. We can ensure they have mechanisms in place for preventing
    abuse and for responding to security issues.
  id: totrans-split-202
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管如此，我们仍然可以监管使AI广泛可用于最终用户的平台和产品。我们可以确保它们设有防止滥用和响应安全问题的机制。
- en: a. What security, legal, or other measures can reasonably be employed to reliably
    prevent wide availability of access to a foundation model’s weights, or limit
    their end use?
  id: totrans-split-203
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: a. 可以合理采取哪些安全、法律或其他措施来可靠防止基础模型权重的广泛获取，或限制它们的最终使用？
- en: 'To regulate the distribution of models, we’d need to regulate two separate
    types of entities:'
  id: totrans-split-204
  prefs: []
  type: TYPE_NORMAL
  zh: 要监管模型的分发，我们需要监管两种不同类型的实体：
- en: Creator organizations, like OpenAI, Stability AI, Mistral AI, Meta, etc.
  id: totrans-split-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 创作者组织，如OpenAI、Stability AI、Mistral AI、Meta等。
- en: Disseminating organizations, like GitHub, Hugging Face, Dropbox, etc.
  id: totrans-split-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 传播组织，如GitHub、Hugging Face、Dropbox等。
- en: Creator organizations would need to disclose who was given access to the weights,
    and what security controls were put in place to keep them private.
  id: totrans-split-207
  prefs: []
  type: TYPE_NORMAL
  zh: 创作者组织需要披露谁被授予权重的访问权限，以及为保持这些权重私密性所采取的安全控制措施。
- en: Disseminating organizations would need to respond to takedown notices, just
    as they do today with copyright violations.
  id: totrans-split-208
  prefs: []
  type: TYPE_NORMAL
  zh: 传播组织需要对下架通知做出回应，就像今天他们对侵犯版权行为的处理一样。
- en: b. How might the wide availability of open foundation model weights facilitate,
    or else frustrate, government action in AI regulation?
  id: totrans-split-209
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: b. 开放基础模型权重的广泛可用性可能如何促进或阻碍政府在AI监管方面的行动？
- en: Open model distribution removes a “chokepoint” for AI, similar to how an open
    internet removes chokepoints.
  id: totrans-split-210
  prefs: []
  type: TYPE_NORMAL
  zh: 开放模型分发消除了AI的“瓶颈”，类似于开放互联网消除了瓶颈。
- en: Oppressive governments often deliberately create internet chokepoints, e.g.
    shutting down DNS when protests start to gather to prevent coordination on social
    media.
  id: totrans-split-211
  prefs: []
  type: TYPE_NORMAL
  zh: 压迫性政府通常会故意创建互联网瓶颈，例如在抗议活动开始集结时关闭DNS，以防止社交媒体上的协调。
- en: In a closed AI world, similar chokepoints are easier to establish. E.g. OpenAI
    can shut down its servers to stop GPT from being used; the same can’t be said
    for open models like LLaMa.
  id: totrans-split-212
  prefs: []
  type: TYPE_NORMAL
  zh: 在封闭的AI世界中，类似的瓶颈更容易建立。例如，OpenAI可以关闭其服务器以阻止GPT的使用；而像LLaMa这样的开放模型则无法做到这一点。
- en: c. When, if ever, should entities deploying AI disclose to users or the general
    public that they are using open foundation models either with or without widely
    available weights?
  id: totrans-split-213
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: c. 何时（如果有的话），部署AI的实体应向用户或公众披露他们是否使用带有或不带有广泛可用权重的开放基础模型？
- en: This shouldn’t be a requirement, any more than disclosing which version of PHP
    runs your website.
  id: totrans-split-214
  prefs: []
  type: TYPE_NORMAL
  zh: 这不应成为必须要求，就像披露您的网站使用哪个PHP版本一样。
- en: Disclosing which AI is driving your product gives attackers an extra piece of
    information they can exploit. If you tell the world you’re using LLaMa, and a
    new prompt injection attack is discovered for LLaMa, attackers can immediately
    start applying it to your app.
  id: totrans-split-215
  prefs: []
  type: TYPE_NORMAL
  zh: 披露驱动您产品的AI给攻击者提供了额外的信息可以利用。如果您告诉世界您正在使用LLaMa，并且发现了LLaMa的新的提示注入攻击，攻击者可以立即开始将其应用于您的应用程序。
- en: That said, organizations might still choose to disclose this, e.g. for marketing
    or recruitment purposes.
  id: totrans-split-216
  prefs: []
  type: TYPE_NORMAL
  zh: 话虽如此，组织可能仍会选择披露这一点，例如出于营销或招聘目的。
- en: '**d. What role, if any, should the U.S. government take in setting metrics
    for risk, creating standards for best practices, and/or supporting or restricting
    the availability of foundation model weights?**'
  id: totrans-split-217
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: d. 美国政府在设置风险指标、制定最佳实践标准、支持或限制基础模型权重的可用性方面应扮演何种角色（如果有的话）？
- en: The U.S. government should provide a robust set of guidelines for deploying
    and distributing AI safely and securely.
  id: totrans-split-218
  prefs: []
  type: TYPE_NORMAL
  zh: 美国政府应提供一套健全的指导方针，以安全和安全地部署和分发人工智能。
- en: We will also need laws restricting how generative AI can be used by individuals
    and organizations. E.g. we might make it illegal to use AI-generated imagery in
    advertisements, or enhance libel laws to punish harmful AI-generated images.
  id: totrans-split-219
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还需要制定法律，限制个人和组织使用生成式人工智能的方式。例如，我们可能会将在广告中使用AI生成的图像视为非法行为，或者加强诽谤法律以惩罚有害的AI生成图像。
- en: The U.S. government should *not* restrict the availability of model weights
    to the public. Doing so would only harm good actors, and add a minor hindrance
    for bad actors. It would hamper security researchers and academics, while giving
    an advantage to nefarious state-level actors.
  id: totrans-split-220
  prefs: []
  type: TYPE_NORMAL
  zh: 美国政府不应限制向公众提供模型权重的可用性。这样做只会损害良好的行为者，对恶意行为者产生轻微的阻碍。它将阻碍安全研究人员和学术界，同时给予不良国家级行为者优势。
- en: The U.S. government should instead encourage and fund the development of open
    models, which will strengthen the U.S. economy and enhance our preparedness for
    an AI-equipped adversary.
  id: totrans-split-221
  prefs: []
  type: TYPE_NORMAL
  zh: 相反，美国政府应鼓励并资助开放模型的发展，这将增强美国经济并增强我们对配备AI的对手的准备能力。
- en: i. Should other government or non- government bodies, currently existing or
    not, support the government in this role? Should this vary by sector?
  id: totrans-split-222
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: i. 其他政府或非政府机构（当前存在或不存在）是否应支持政府在此角色中的作用？在不同部门中是否应有所不同？
- en: 'Yes, the government should rely on non-profits and third-parties to:'
  id: totrans-split-223
  prefs: []
  type: TYPE_NORMAL
  zh: 是的，政府应依赖非营利组织和第三方来：
- en: Create standards for evaluating AI security
  id: totrans-split-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 制定评估AI安全性的标准
- en: Create standards for licensing open and closed models
  id: totrans-split-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 制定开放和闭合模型许可的标准
- en: Audit both open and closed models for performance, robustness, truthfulness,
    and security
  id: totrans-split-226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对开放和闭合模型进行审计，评估其性能、稳健性、真实性和安全性。
- en: Certain sectors (especially healthcare and finance) will naturally come up with
    their own standards and evaluation frameworks.
  id: totrans-split-227
  prefs: []
  type: TYPE_NORMAL
  zh: 某些部门（特别是医疗保健和金融）将自然地制定自己的标准和评估框架。
- en: e. What should the role of model hosting services (*e.g.,* HuggingFace, GitHub,
    etc.) be in making dual-use models with open weights more or less available? **Should
    hosting services host models that do not meet certain safety standards? By whom
    should those standards be prescribed?**
  id: totrans-split-228
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: e. 模型托管服务（例如，HuggingFace、GitHub等）在使带有开放权重的双重用途模型更或更少可用方面应承担什么角色？托管服务是否应该托管不符合某些安全标准的模型？这些标准应由谁制定？
- en: There should be a system for notifying the hosting service of models that have
    security flaws, backdoors, or private information—just as there is today for source
    code and other media.
  id: totrans-split-229
  prefs: []
  type: TYPE_NORMAL
  zh: 应当建立一个系统，用于通知托管服务存在安全漏洞、后门或私人信息的模型，就像今天针对源代码和其他媒体一样。
- en: Users, government officials, and security researchers should be able to report
    particular models to the host, indicating the problem and its severity. E.g. if
    a model is found to have memorized individual social security numbers, or if it
    is regurgitating CSAM from its training set, the hosting organization should take
    down the model immediately, and notify the maintainer. If a model has less severe
    security flaws (e.g. a prompt injection vulnerability) the maintainer should be
    informed.
  id: totrans-split-230
  prefs: []
  type: TYPE_NORMAL
  zh: 用户、政府官员和安全研究人员应能够向托管方报告特定模型，指出问题及其严重性。例如，如果发现模型已记住个人社保号码，或者如果它从其训练集中回响CSAM，托管组织应立即撤下模型，并通知维护者。如果模型存在较轻微的安全漏洞（例如提示注入漏洞），应通知维护者。
- en: GitHub today does a good job of this—they highlight a list of known vulnerabilities
    in every repository (private to the repository owner), and quickly take down projects
    that violate copyright or other laws.
  id: totrans-split-231
  prefs: []
  type: TYPE_NORMAL
  zh: GitHub 今天在这方面做得很好——他们突出显示每个存储库中已知的漏洞列表（对存储库所有者保密），并迅速撤下侵犯版权或其他法律的项目。
- en: The government should set a high bar for problems that require a project to
    be taken down immediately (e.g. leaking highly sensitive information), and allow
    hosting providers to self-regulate below that bar.
  id: totrans-split-232
  prefs: []
  type: TYPE_NORMAL
  zh: 政府应设立高标准，对需要立即撤下项目的问题设定一个门槛（例如泄露高度敏感信息），并允许托管提供者在此门槛以下进行自我管理。
- en: f. Should there be different standards for government as opposed to private
    industry when it comes to sharing model weights of open foundation models or contracting
    with companies who use them?
  id: totrans-split-233
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: f. 当涉及共享开放基础模型的模型权重或与使用这些模型的公司签订合同时，政府是否应该设定不同的标准？
- en: It might be tempting to restrict public/private access to models, while relaxing
    those restrictions for government organizations. But this would impede a great
    deal of economic and technological progress. It would make the U.S. less competitive
    in the global economy, and more susceptible to attack.
  id: totrans-split-234
  prefs: []
  type: TYPE_NORMAL
  zh: 也许会有诱惑，将对模型的公共/私人访问限制，同时放宽对政府机构的限制。但这将阻碍大量经济和技术进步。这将使美国在全球经济中竞争力下降，并且更容易受到攻击。
- en: It might also be tempting for the government to restrict the distribution of
    open models, and instead provide its own government-approved models to the public.
    But the U.S. government has little history or experience with building, maintaining,
    and deploying open source software at that level of scale. The quality of government-managed
    models would likely be far worse than privately managed models.
  id: totrans-split-235
  prefs: []
  type: TYPE_NORMAL
  zh: 政府可能也会有诱惑，限制开放模型的分发，而是向公众提供其自己批准的政府模型。但美国政府在建立、维护和部署如此规模的开源软件方面几乎没有历史或经验。政府管理的模型质量可能远不如私人管理的模型。
- en: g. What should the U.S. prioritize in working with other countries on this topic,
    and which countries are most important to work with?
  id: totrans-split-236
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: g. 在这个问题上，美国应该与其他国家合作的重点是什么，哪些国家最重要？
- en: 'There are two categories of countries we need to discuss: Allies and Adversaries.'
  id: totrans-split-237
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要讨论两类国家：盟友和对手。
- en: We should encourage allies to adopt similar regulations to our own. Whichever
    country takes the laxest approach to model regulation will likely outcompete other
    nations. Furthermore, lax laws in one country can effectively be exploited by
    users in another country—a U.S. citizen could use a VPN or fly to the EU to download
    a model that is restricted in the U.S.
  id: totrans-split-238
  prefs: []
  type: TYPE_NORMAL
  zh: 我们应鼓励盟友采纳类似于我们自己的法规。无论哪个国家在模型监管方面采取最宽松的方法，都可能会超越其他国家。此外，一个国家的宽松法律可以被另一个国家的用户有效地利用——例如，一个美国公民可以使用VPN或飞到欧盟下载在美国受限制的模型。
- en: We will need to work with adversaries as well. Building collaborative relationships
    between our research teams and theirs, our security teams and theirs, etc, will
    help keep the world aligned when it comes to AI safety.
  id: totrans-split-239
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还需要与对手合作。建立我们的研究团队与他们的、我们的安全团队与他们的等之间的合作关系，将有助于在涉及人工智能安全时保持世界的一致性。
- en: Collaboration between e.g. U.S. and Chinese citizens is already significant
    in the open source world. The U.S. government should avoid getting in the way
    of this kind of collaboration.
  id: totrans-split-240
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，美国与中国公民在开源世界中的合作已经非常显著。美国政府应避免干扰这种合作。
- en: h. What insights from other countries or other societal systems are most useful
    to consider?
  id: totrans-split-241
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: h. 从其他国家或其他社会体系中获得的哪些见解最有用？
- en: 'There are two major regulatory pushes internationally:'
  id: totrans-split-242
  prefs: []
  type: TYPE_NORMAL
  zh: 在国际上有两个主要的监管推动：
- en: The US should follow the EU more than China. We should work to mitigate the
    harm done to society, rather than trying to centralize control of a promising
    new technology.
  id: totrans-split-243
  prefs: []
  type: TYPE_NORMAL
  zh: 美国应该更多地效仿欧盟而不是中国。我们应该努力减少对社会造成的伤害，而不是试图集中控制一个有前景的新技术。
- en: i. Are there effective mechanisms or procedures that can be used by the government
    or companies to make decisions regarding an appropriate degree of availability
    of model weights in a dual-use foundation model or the dual-use foundation model
    ecosystem? Are there methods for making effective decisions about open AI deployment
    that balance both benefits and risks? This may include responsible capability
    scaling policies, preparedness frameworks, et cetera.
  id: totrans-split-244
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: i. 政府或公司是否有有效的机制或程序可以用来决定双重用途基础模型或双重用途基础模型生态系统中适当的模型权重可用性程度？是否有办法有效地决定平衡开放AI部署的利益与风险？这可能包括负责任的能力扩展政策、准备框架等。
- en: As stated above, “degree of availability” is not a very sensible idea. Availability
    is either “on” or “off”, with only some minor gradations in either extreme.
  id: totrans-split-245
  prefs: []
  type: TYPE_NORMAL
  zh: 正如上文所述，“可用性程度”不是一个非常明智的想法。可用性要么是“开启”的，要么是“关闭”的，只有在这两个极端中有一些微小的变化。
- en: 'That said, government and creator companies can mitigate risk by:'
  id: totrans-split-246
  prefs: []
  type: TYPE_NORMAL
  zh: 也就是说，政府和创作者公司可以通过以下方式减少风险：
- en: Working with researchers and independent auditors to assess the model
  id: totrans-split-247
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 与研究人员和独立审计员合作评估模型
- en: Candidly acknowledging the limitations of models
  id: totrans-split-248
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 坦率地承认模型的局限性
- en: Shipping models with licenses that restrict socially harmful usage
  id: totrans-split-249
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 发布限制社会有害使用的许可证的模型
- en: Disclosing risks and vulnerabilities as they’re found
  id: totrans-split-250
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 发现风险和漏洞时要坦率披露
- en: j. **Are there particular individuals/ entities who should or should not have
    access** to open-weight foundation models? If so, why and under what circumstances?
  id: totrans-split-251
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: j. **是否有特定的个人/实体应该或不应该访问**开放权重基础模型？如果是，为什么？在什么情况下？
- en: No. We shouldn’t restrict an individual’s access to an open model any more than
    we should restrict their access to the internet or to certain books. And unless
    the person is incarcerated, doing so is technically infeasible.
  id: totrans-split-252
  prefs: []
  type: TYPE_NORMAL
  zh: 不。我们不应该限制个人访问开放模型，就像我们不应该限制他们访问互联网或某些书籍一样。除非这个人被关押，否则这样做在技术上是不可行的。
- en: 8\. In the face of continually changing technology, and given unforeseen risks
    and benefits, how can governments, companies, and individuals make decisions or
    plans today about open foundation models that will be useful in the future?
  id: totrans-split-253
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 8\. 面对不断变化的技术，并考虑到未预见的风险和收益，政府、公司和个人如何能够今天就对未来有用的开放基础模型做出决策或计划？
- en: Legislating against nefarious *usage* of AI, and interpreting existing laws
    in light of the new technology, will be the most scalable way to future-proof
    AI regulation.
  id: totrans-split-254
  prefs: []
  type: TYPE_NORMAL
  zh: 立法反对人工智能的不良*使用*，并根据新技术的光芒解释现有法律，将是未来证明人工智能监管最具规模化的方式。
- en: Legislation that focuses on particular technologies (e.g. “large language models”)
    or on specs (e.g. number of weights or floating-point operations per second) is
    almost guaranteed to become obsolete within months to years.
  id: totrans-split-255
  prefs: []
  type: TYPE_NORMAL
  zh: 专门关注特定技术（例如“大型语言模型”）或规格（例如每秒的权重数量或浮点运算）的立法几乎肯定会在几个月到几年内过时。
- en: '**a. How should these potentially competing interests of innovation, competition,
    and security be addressed or balanced?**'
  id: totrans-split-256
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**a. 如何平衡创新、竞争和安全等潜在的竞争利益？**'
- en: Striking a balance between innovation and security will be crucial.
  id: totrans-split-257
  prefs: []
  type: TYPE_NORMAL
  zh: 在创新和安全之间取得平衡将至关重要。
- en: Enacting and enforcing laws that punish the abuses of AI is crucial. If we don’t
    regulate at all, we will likely see a large increase in harm done to individuals.
    Media will become increasingly “bait” driven as companies tune images, video,
    and text to our personal psychology. Fake images, including libelous and non-consensual
    intimate images, will proliferate. Fake news will become easier to churn out.
    Public trust will erode.
  id: totrans-split-258
  prefs: []
  type: TYPE_NORMAL
  zh: 颁布和执行惩罚滥用人工智能的法律至关重要。如果我们根本不加管制，可能会看到对个人造成的伤害大幅增加。媒体将越来越“诱饵”驱动，因为公司调整图像、视频和文本以符合我们的个人心理。伪造的图像，包括诽谤和非同意的亲密图像，将会大量传播。虚假新闻将更容易被推出。公众信任将会侵蚀。
- en: Conversely, if we regulate too heavily out of fear, other countries will quickly
    outcompete us. New AI applications will flourish in the EU and China, while U.S.
    citizens are forced to stick to a few walled gardens. Given the potential for
    AI not just as a new industry, but as an economic accelerant, this would be disastrous.
  id: totrans-split-259
  prefs: []
  type: TYPE_NORMAL
  zh: 相反，如果我们因恐惧而过度规范，其他国家将迅速超越我们。新的AI应用将在欧盟和中国蓬勃发展，而美国公民则被迫局限于少数的封闭园地。考虑到AI不仅是新兴行业，而且是经济的加速剂，这将是灾难性的。
- en: Ironically, over-regulating access to AI would harm *both* innovation *and*
    security, as it prevents academics and researchers from fully studying AI. Adversarial
    governments and other bad actors would have an information advantage, which they
    could use to attack the US or steal intellectual property.
  id: totrans-split-260
  prefs: []
  type: TYPE_NORMAL
  zh: 具有讽刺意味的是，过度规管AI的访问将损害*创新*和*安全*，因为它阻止学术界和研究人员对AI进行全面研究。对抗性政府和其他不良行为者将具有信息优势，他们可以利用此优势攻击美国或窃取知识产权。
- en: b. Noting that E.O. 14110 grants the Secretary of Commerce the capacity to adapt
    the threshold, is the amount of computational resources required to build a model,
    such as the cutoff of 1026 integer or floating-point operations used in the Executive
    order, a useful metric for thresholds to mitigate risk in the long-term, particularly
    for risks associated with wide availability of model weights?
  id: totrans-split-261
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: b. 注意到E.O. 14110授予商务部长调整门槛的能力，例如在行政命令中使用的1026个整数或浮点运算的截止点，这是否是一个有用的度量标准，用于长期缓解与广泛使用的模型权重相关的风险，特别是计算资源需求的风险？
- en: Hard technical cutoffs here are naive. As the technology evolves, we will find
    ways to get the same performance out of smaller models. The stated limitations
    will quickly become obsolete.
  id: totrans-split-262
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里进行严格的技术截止日期是幼稚的。随着技术的发展，我们将找到方法使较小的模型达到相同的性能。所述的限制很快就会过时。
- en: c. Are there more robust risk metrics for foundation models with widely available
    weights that will stand the test of time? Should we look at models that fall outside
    of the dual-use foundation model definition?
  id: totrans-split-263
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: c. 对于具有广泛可用模型权重的基础模型，是否有更强大的风险度量标准，以经受时间考验？我们应该考虑那些不符合双重用途基础模型定义的模型吗？
- en: Any cutoffs should be based on evaluation metrics. For example, you could limit
    access to LLMs that score above 90% on HumanEval pass@1—this would be robust to
    future technological developments. These cutoffs would also be applicable for
    a wider array of AI technologies.
  id: totrans-split-264
  prefs: []
  type: TYPE_NORMAL
  zh: 任何截止日期都应基于评估指标。例如，您可以限制对在HumanEval pass@1上得分超过90%的LLMs的访问——这将对未来技术发展具有鲁棒性。这些截止日期也适用于更广泛的AI技术。
- en: However, this sort of cutoff would be catastrophically bad for the competitive
    landscape, and would greatly hinder technological progress.
  id: totrans-split-265
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，这种截止日期对竞争格局来说将是灾难性的，将极大地阻碍技术进步。
- en: 9\. What other issues, topics, or adjacent technological advancements should
    we consider when analyzing risks and benefits of dual-use foundation models with
    widely available model weights?
  id: totrans-split-266
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 9\. 在分析具有广泛可用模型权重的双重用途基础模型的风险和利益时，我们还应考虑哪些其他问题、主题或相关技术进展？
- en: Generative AI ties in deeply with virtual and augmented reality, as well as
    with traditional media. It gives media companies the potential to craft videos,
    images, and text that are maximally alluring, and to tailor media to individual
    psychological profiles. We need to regulate the use of AI in entertainment and
    advertising.
  id: totrans-split-267
  prefs: []
  type: TYPE_NORMAL
  zh: 生成式AI与虚拟和增强现实密切相关，也与传统媒体密切相关。它使媒体公司有能力制作极具吸引力的视频、图像和文本，并根据个体心理特征量身定制媒体。我们需要规范AI在娱乐和广告中的使用。
- en: Copyright is another concern. Models which are trained on a corpus of music,
    painting, writing, etc can then imitate the styles of particular creators, creating
    competing media on demand. We’ll need to clarify copyright law when it comes to
    disseminating derivative AI-generated works.
  id: totrans-split-268
  prefs: []
  type: TYPE_NORMAL
  zh: 版权问题是另一个关注点。那些在音乐、绘画、写作等语料库上训练的模型可以模仿特定创作者的风格，根据需求创建竞争性媒体。在传播衍生的AI生成作品时，我们需要澄清版权法律。
- en: I’m happy to see the US government taking this topic seriously. The new generation
    of AI will be transformative, and will certainly cause both economic and social
    disruption. Intelligent, thoughtful regulation can help to mitigate the harms
    and amplify the benefits.
  id: totrans-split-269
  prefs: []
  type: TYPE_NORMAL
  zh: 我很高兴看到美国政府认真对待这个问题。新一代AI将是变革性的，并肯定会带来经济和社会动荡。智能而审慎的监管可以帮助减轻伤害并放大利益。
- en: The focus, as always, should be on regulating large, profit-driven organizations—not
    individuals, academics, and researchers. If AI technology poses an existential
    threat, that threat comes from state- and enterprise-level actors—not PhD students
    and open source developers.
  id: totrans-split-270
  prefs: []
  type: TYPE_NORMAL
  zh: 重点始终应该是对大型、以利润为驱动的组织进行监管，而不是个人、学者和研究人员。如果人工智能技术构成存在威胁，这种威胁来自国家和企业级行为者，而不是博士生和开源开发者。
- en: To be sure, some individuals will find harmful ways to use AI. They might generate
    non-consensual intimate imagery, post incendiary fake images on social media,
    or use AI to amplify their political views. These issues are best mitigated through
    existing laws (e.g. libel/slander/defamation laws) and platform regulation (e.g.
    holding Facebook responsible for disseminating false information).
  id: totrans-split-271
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，有些个人可能会以有害方式使用人工智能。他们可能会生成未经同意的亲密影像，发布社交媒体上的挑衅性虚假图像，或者利用人工智能放大他们的政治观点。这些问题最好通过现有法律（如诽谤法）和平台监管（如要求Facebook对传播虚假信息负责）来缓解。
- en: 'Again, there are two approaches to social harm: we can disincentivize it through
    punishment, or we can try to eradicate it through surveillance and control. The
    former is the norm in free societies, while the latter is a hallmark of oppression.'
  id: totrans-split-272
  prefs: []
  type: TYPE_NORMAL
  zh: 再次强调，对社会伤害有两种方法：我们可以通过惩罚来减少它，或者可以通过监视和控制来尝试根除它。前者是自由社会的常态，而后者是压制的标志。
- en: I understand why the government is tempted to ban open models. But doing so
    would be disastrous for national security, for our economy, and for individual
    liberty.
  id: totrans-split-273
  prefs: []
  type: TYPE_NORMAL
  zh: 我理解政府为何有意禁止开放模型。但这样做对国家安全、经济和个人自由都将是灾难性的。
- en: If you agree, please [send your comments](https://www.regulations.gov/document/NTIA-2023-0009-0001)
    to the DoC by March 27th, 2024.
  id: totrans-split-274
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您同意，请于2024年3月27日之前向商务部[发送您的评论](https://www.regulations.gov/document/NTIA-2023-0009-0001)。
- en: '[Share](https://rbren.substack.com/p/banning-open-weight-models-would?utm_source=substack&utm_medium=email&utm_content=share&action=share)'
  id: totrans-split-275
  prefs: []
  type: TYPE_NORMAL
  zh: '[分享](https://rbren.substack.com/p/banning-open-weight-models-would?utm_source=substack&utm_medium=email&utm_content=share&action=share)'
