- en: <!--yml
  id: totrans-split-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: æœªåˆ†ç±»'
  id: totrans-split-1
  prefs: []
  type: TYPE_NORMAL
  zh: 'category: æœªåˆ†ç±»'
- en: 'date: 2024-05-27 12:49:58'
  id: totrans-split-2
  prefs: []
  type: TYPE_NORMAL
  zh: 'date: 2024-05-27 12:49:58'
- en: -->
  id: totrans-split-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: How to stream PostgreSQL CDC to Kafka and use Propel to get an instant API
  id: totrans-split-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: å¦‚ä½•å°† PostgreSQL CDC æµå¼ä¼ è¾“åˆ° Kafka å¹¶ä½¿ç”¨ Propel å®ç°å³æ—¶ API
- en: æ¥æºï¼š[https://www.propeldata.com/blog/postgresql-cdc-to-kafka](https://www.propeldata.com/blog/postgresql-cdc-to-kafka)
  id: totrans-split-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: æ¥æºï¼š[https://www.propeldata.com/blog/postgresql-cdc-to-kafka](https://www.propeldata.com/blog/postgresql-cdc-to-kafka)
- en: Learn how to stream PostgreSQL Change Data Capture (CDC) to a Kafka topic and
    serve the data via an API with Propel. This guide provides step-by-step instructions,
    from setting up your PostgreSQL database and Kafka broker to deploying the Debezium
    PostgreSQL connector and creating a Propel Kafka Datapool.
  id: totrans-split-6
  prefs: []
  type: TYPE_NORMAL
  zh: å­¦ä¹ å¦‚ä½•å°† PostgreSQL å˜æ›´æ•°æ®æ•è·ï¼ˆCDCï¼‰æµå¼ä¼ è¾“åˆ° Kafka ä¸»é¢˜ï¼Œå¹¶ä½¿ç”¨ Propel æä¾› API æœåŠ¡æ•°æ®ã€‚æœ¬æŒ‡å—æä¾›é€æ­¥è¯´æ˜ï¼Œä»è®¾ç½®
    PostgreSQL æ•°æ®åº“å’Œ Kafka Broker åˆ°éƒ¨ç½² Debezium PostgreSQL è¿æ¥å™¨å’Œåˆ›å»º Propel Kafka æ•°æ®æ± ã€‚
- en: Change Data Capture (CDC) is the process of tracking and capturing changes to
    your data. In PostgreSQL, we can implement CDC efficiently using the transaction
    log so that, rather than running batch jobs to gather data, we can capture data
    changes as they occur continuously. This has numerous benefits, including allowing
    your applications to see and react to changes in near real-time with lower impact
    on your source systems. In this article, we'll walk you step-by-step through how
    to stream CDC from PostgreSQL to a [Propel Kafka Data Pool](https://www.propeldata.com/docs/connect-your-data/kafka)
    to get an instant data-serving API.
  id: totrans-split-7
  prefs: []
  type: TYPE_NORMAL
  zh: å˜æ›´æ•°æ®æ•è·ï¼ˆCDCï¼‰æ˜¯è·Ÿè¸ªå’Œæ•è·æ•°æ®å˜æ›´çš„è¿‡ç¨‹ã€‚åœ¨ PostgreSQL ä¸­ï¼Œæˆ‘ä»¬å¯ä»¥åˆ©ç”¨äº‹åŠ¡æ—¥å¿—æœ‰æ•ˆåœ°å®ç° CDCï¼Œè¿™æ ·æˆ‘ä»¬å°±ä¸å¿…è¿è¡Œæ‰¹å¤„ç†ä½œä¸šæ¥æ”¶é›†æ•°æ®ï¼Œè€Œæ˜¯å¯ä»¥è¿ç»­åœ°æ•è·æ•°æ®å˜æ›´ã€‚è¿™æœ‰å¾ˆå¤šå¥½å¤„ï¼ŒåŒ…æ‹¬å…è®¸åº”ç”¨ç¨‹åºå‡ ä¹å®æ—¶åœ°æŸ¥çœ‹å’Œå“åº”å˜æ›´ï¼ŒåŒæ—¶å¯¹æºç³»ç»Ÿçš„å½±å“è¾ƒå°ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬å°†é€æ­¥ä»‹ç»å¦‚ä½•å°†
    PostgreSQL çš„ CDC æµå¼ä¼ è¾“åˆ° [Propel Kafka æ•°æ®æ± ](https://www.propeldata.com/docs/connect-your-data/kafka)
    ä»¥è·å–å³æ—¶æ•°æ®æœåŠ¡ APIã€‚
- en: What are we trying to do?
  id: totrans-split-8
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬è¯•å›¾åšä»€ä¹ˆï¼Ÿ
- en: We want to power large-scale analytics applications with data coming from PostgreSQL.
    As we all know, PostgreSQL, as an OLTP database, is notoriously slow in handling
    analytical queries that process and aggregate large amounts of data.
  id: totrans-split-9
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å¸Œæœ›ä½¿ç”¨æ¥è‡ª PostgreSQL çš„æ•°æ®æ¥æ”¯æŒå¤§è§„æ¨¡åˆ†æåº”ç”¨ç¨‹åºã€‚ä¼—æ‰€å‘¨çŸ¥ï¼Œä½œä¸º OLTP æ•°æ®åº“ï¼ŒPostgreSQL åœ¨å¤„ç†å¤„ç†å’Œèšåˆå¤§é‡æ•°æ®çš„åˆ†ææŸ¥è¯¢æ—¶é€Ÿåº¦å¾ˆæ…¢ã€‚
- en: First, we want to capture all changes to our PostgreSQL database table(s) and
    send them as JSON messages (see below) to a [Kafka](https://kafka.apache.org/)
    topic.
  id: totrans-split-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: é¦–å…ˆï¼Œæˆ‘ä»¬å¸Œæœ›æ•è·æˆ‘ä»¬çš„ PostgreSQL æ•°æ®åº“è¡¨çš„æ‰€æœ‰æ›´æ”¹ï¼Œå¹¶å°†å®ƒä»¬ä½œä¸º JSON æ¶ˆæ¯å‘é€åˆ° [Kafka](https://kafka.apache.org/)
    ä¸»é¢˜ã€‚
- en: Second, weâ€™ll need to configure Â [Kafka Connect](https://docs.confluent.io/platform/current/connect/userguide.html#connect-userguide)
    and the [Debezium PostgreSQL connector](https://debezium.io/documentation/reference/2.6/connectors/postgresql.html)
    to stream the CDC to a Kafka topic.
  id: totrans-split-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å…¶æ¬¡ï¼Œæˆ‘ä»¬éœ€è¦é…ç½® [Kafka Connect](https://docs.confluent.io/platform/current/connect/userguide.html#connect-userguide)
    å’Œ [Debezium PostgreSQL connector](https://debezium.io/documentation/reference/2.6/connectors/postgresql.html)
    æ¥å°† CDC æµå¼ä¼ è¾“åˆ° Kafka ä¸»é¢˜ã€‚
- en: Lastly, create a Propel Kafka Data Pool that ingests data from the Kafka topic
    and exposes it via a low latency API.
  id: totrans-split-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æœ€åï¼Œåˆ›å»ºä¸€ä¸ª Propel Kafka æ•°æ®æ± ï¼Œé€šè¿‡ä½å»¶è¿Ÿ API ä» Kafka ä¸»é¢˜æ‘„å–æ•°æ®ã€‚
- en: We will cover how to deploy how to create and deploy all services in a Kubernetes
    cluster.
  id: totrans-split-13
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å°†ä»‹ç»å¦‚ä½•åœ¨ Kubernetes é›†ç¾¤ä¸­åˆ›å»ºå’Œéƒ¨ç½²æ‰€æœ‰æœåŠ¡ã€‚
- en: How PostgreSQL CDC with Debezium works
  id: totrans-split-14
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: PostgreSQL CDC ä¸ Debezium çš„å·¥ä½œåŸç†
- en: In this section, we will provide a quick overview of how PostgreSQL change data
    capture works with Debezium.
  id: totrans-split-15
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æœ¬èŠ‚ä¸­ï¼Œæˆ‘ä»¬å°†ç®€è¦ä»‹ç» PostgreSQL å˜æ›´æ•°æ®æ•è·ä¸ Debezium çš„å·¥ä½œåŸç†ã€‚
- en: 'Imagine you have a simple table:'
  id: totrans-split-16
  prefs: []
  type: TYPE_NORMAL
  zh: æƒ³è±¡ä½ æœ‰ä¸€ä¸ªç®€å•çš„è¡¨æ ¼ï¼š
- en: '[PRE0]'
  id: totrans-split-17
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'You update a row with the following SQL statement:'
  id: totrans-split-18
  prefs: []
  type: TYPE_NORMAL
  zh: ä½ å¯ä»¥ä½¿ç”¨ä»¥ä¸‹ SQL è¯­å¥æ›´æ–°è¡Œï¼š
- en: '[PRE1]'
  id: totrans-split-19
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: The change to the column <span class="code-exp">foo</span> will result in an
    â€œupdateâ€ to the transaction log, which Debezium then transforms into the JSON
    message below and drops into a Kafka topic.
  id: totrans-split-20
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹åˆ— <span class="code-exp">foo</span> çš„æ›´æ”¹å°†å¯¼è‡´â€œupdateâ€åˆ°äº‹åŠ¡æ—¥å¿—ï¼Œç„¶å Debezium å°†å…¶è½¬æ¢ä¸ºä¸‹é¢çš„
    JSON æ¶ˆæ¯ï¼Œå¹¶å°†å…¶æ”¾å…¥ Kafka ä¸»é¢˜ã€‚
- en: 'CDC JSON with <span class="code-exp">before</span> and <span class="code-exp">after</span>:'
  id: totrans-split-21
  prefs: []
  type: TYPE_NORMAL
  zh: CDC JSONï¼ŒåŒ…å« <span class="code-exp">before</span> å’Œ <span class="code-exp">after</span>ï¼š
- en: '[PRE2]'
  id: totrans-split-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'The Debezium PostgreSQL connector generates a data change event for each row-level
    <span class="code-exp">INSERT</span>, <span class="code-exp">UPDATE</span>, and
    <span class="code-exp">DELETE</span> operation. The <span class="code-exp">op</span>
    key describes the operation that caused the connector to generate the event. In
    this example, u indicates that the operation updated a row. Valid values are:'
  id: totrans-split-23
  prefs: []
  type: TYPE_NORMAL
  zh: 'Debezium PostgreSQL è¿æ¥å™¨ä¸ºæ¯ä¸ªè¡Œçº§åˆ«çš„ <span class="code-exp">INSERT</span>ã€<span class="code-exp">UPDATE</span>
    å’Œ <span class="code-exp">DELETE</span> æ“ä½œç”Ÿæˆæ•°æ®å˜æ›´äº‹ä»¶ã€‚ <span class="code-exp">op</span>
    é”®æè¿°äº†å¯¼è‡´è¿æ¥å™¨ç”Ÿæˆäº‹ä»¶çš„æ“ä½œã€‚åœ¨æ­¤ç¤ºä¾‹ä¸­ï¼Œu è¡¨ç¤ºæ“ä½œå·²æ›´æ–°è¡Œã€‚æœ‰æ•ˆå€¼åŒ…æ‹¬:'
- en: <span class="code-exp">c</span> = create
  id: totrans-split-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: <span class="code-exp">c</span> = create
- en: <span class="code-exp">u</span> = update
  id: totrans-split-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: <span class="code-exp">u</span> = update
- en: <span class="code-exp">d</span> = delete
  id: totrans-split-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: <span class="code-exp">d</span> = delete
- en: <span class="code-exp">r</span> = read (applies to only snapshots)
  id: totrans-split-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: <span class="code-exp">r</span> = readï¼ˆä»…é€‚ç”¨äºå¿«ç…§ï¼‰
- en: <span class="code-exp">t</span> = truncate
  id: totrans-split-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: <span class="code-exp">t</span> = truncate
- en: <span class="code-exp">m</span> = message
  id: totrans-split-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: <span class="code-exp">m</span> = message
- en: What is the Propel Kafka Datapool?
  id: totrans-split-30
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ä»€ä¹ˆæ˜¯ Propel Kafka æ•°æ®æ± ï¼Ÿ
- en: The Kafka [Data Pool](https://www.propeldata.com/docs/connect-your-data#key-concept-2-data-pools)
    lets you ingest real-time streaming data into Propel. It provides an instant low-latency
    API on top of Kafka to power real-time dashboards, streaming analytics, and workflows.
  id: totrans-split-31
  prefs: []
  type: TYPE_NORMAL
  zh: Kafka [æ•°æ®æ± ](https://www.propeldata.com/docs/connect-your-data#key-concept-2-data-pools)
    è®©æ‚¨å°†å®æ—¶æµæ•°æ®å¯¼å…¥ Propelã€‚å®ƒåœ¨ Kafka ä¹‹ä¸Šæä¾›å³æ—¶ä½å»¶è¿Ÿçš„ APIï¼Œç”¨äºæ”¯æŒå®æ—¶ä»ªè¡¨æ¿ã€æµå¼åˆ†æå’Œå·¥ä½œæµã€‚
- en: 'Consider using Propel on top of Kafka when:'
  id: totrans-split-32
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ä»¥ä¸‹æƒ…å†µè€ƒè™‘ä½¿ç”¨ Propel åœ¨ Kafka ä¹‹ä¸Šï¼š
- en: You need an API on top of a Kafka topic.
  id: totrans-split-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æ‚¨éœ€è¦åœ¨ Kafka ä¸»é¢˜ä¹‹ä¸Šçš„ APIã€‚
- en: You need to power real-time analytics applications with streaming data from
    Kafka.
  id: totrans-split-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æ‚¨éœ€è¦é€šè¿‡ Kafka çš„æµæ•°æ®æ¥æ”¯æŒå®æ—¶åˆ†æåº”ç”¨ã€‚
- en: You need to ingest Kafka messages into ClickHouse.
  id: totrans-split-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æ‚¨éœ€è¦å°† Kafka æ¶ˆæ¯å¯¼å…¥ ClickHouseã€‚
- en: You need to ingest from [self-hosted Kafka](https://www.propeldata.com/docs/connect-your-data/kafka/guides/how-to-ingest-data-from-Kafka-into-propel?kafka-platform=self-hosted-kafka),
    [Confluent Cloud](https://www.propeldata.com/docs/connect-your-data/kafka/guides/how-to-ingest-data-from-Kafka-into-propel?kafka-platform=confluent),
    [AWS MSK](https://www.propeldata.com/docs/connect-your-data/kafka/guides/how-to-ingest-data-from-Kafka-into-propel?kafka-platform=aws-msk),
    or [Redpanda](https://www.propeldata.com/docs/connect-your-data/kafka/guides/how-to-ingest-data-from-Kafka-into-propel?kafka-platform=redpanda)
    into ClickHouse.
  id: totrans-split-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æ‚¨éœ€è¦ä» [è‡ªæ‰˜ç®¡ Kafka](https://www.propeldata.com/docs/connect-your-data/kafka/guides/how-to-ingest-data-from-Kafka-into-propel?kafka-platform=self-hosted-kafka)ã€[Confluent
    Cloud](https://www.propeldata.com/docs/connect-your-data/kafka/guides/how-to-ingest-data-from-Kafka-into-propel?kafka-platform=confluent)ã€[AWS
    MSK](https://www.propeldata.com/docs/connect-your-data/kafka/guides/how-to-ingest-data-from-Kafka-into-propel?kafka-platform=aws-msk)
    æˆ– [Redpanda](https://www.propeldata.com/docs/connect-your-data/kafka/guides/how-to-ingest-data-from-Kafka-into-propel?kafka-platform=redpanda)
    å¯¼å…¥ ClickHouseã€‚
- en: You need to transform or enrich your streaming data.
  id: totrans-split-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æ‚¨éœ€è¦è½¬æ¢æˆ–ä¸°å¯Œæ‚¨çš„æµæ•°æ®ã€‚
- en: You need to power real-time personalization and recommendations for use cases.
  id: totrans-split-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æ‚¨éœ€è¦ä¸ºå®æ—¶ä¸ªæ€§åŒ–å’Œæ¨èç”¨ä¾‹æä¾›æ”¯æŒã€‚
- en: Once we have the PostgreSQL, Debezium, and Kafka set up, weâ€™ll ingest the data
    into a Propel Kafka Data Pool to expose it via an API.
  id: totrans-split-39
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€æ—¦æˆ‘ä»¬è®¾ç½®å¥½ PostgreSQLã€Debezium å’Œ Kafkaï¼Œæˆ‘ä»¬å°†æŠŠæ•°æ®å¯¼å…¥ Propel Kafka æ•°æ®æ± ï¼Œä»¥ä¾¿é€šè¿‡ API å…¬å¼€ã€‚
- en: Setting up Minikube, PostgreSQL, Kafka, Kafka Connect, and Debezium
  id: totrans-split-40
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: è®¾ç½® Minikubeã€PostgreSQLã€Kafkaã€Kafka Connect å’Œ Debezium
- en: We will walk through setting this up in a Kubernetes cluster on a Mac. This
    can be done in other environments, but Kubernetes is a common platform for hosting
    and running data streaming services in the cloud. Â For this example, we will use
    [Minikube](https://minikube.sigs.k8s.io/docs/start/) to deploy our Kubernetes
    cluster, [Redpanda](https://redpanda.com/) to deploy our Kafka cluster, and [Helm](https://helm.sh/)
    to manage Kubernetes applications like PostgreSQL. Â Iâ€™ll provide the Mac-specific
    commands but links to the installed services if you are running a different OS.
  id: totrans-split-41
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å°†åœ¨ Mac ä¸Šçš„ Kubernetes é›†ç¾¤ä¸­è¿›è¡Œè®¾ç½®ã€‚è¿™ä¹Ÿå¯ä»¥åœ¨å…¶ä»–ç¯å¢ƒä¸­å®Œæˆï¼Œä½† Kubernetes æ˜¯äº‘ä¸­æ‰˜ç®¡å’Œè¿è¡Œæ•°æ®æµæœåŠ¡çš„å¸¸è§å¹³å°ã€‚ä¸ºäº†æœ¬ä¾‹ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨
    [Minikube](https://minikube.sigs.k8s.io/docs/start/) éƒ¨ç½²æˆ‘ä»¬çš„ Kubernetes é›†ç¾¤ï¼Œ[Redpanda](https://redpanda.com/)
    éƒ¨ç½²æˆ‘ä»¬çš„ Kafka é›†ç¾¤ï¼Œå¹¶ä½¿ç”¨ [Helm](https://helm.sh/) ç®¡ç†åƒ PostgreSQL è¿™æ ·çš„ Kubernetes åº”ç”¨ç¨‹åºã€‚å¦‚æœæ‚¨ä½¿ç”¨ä¸åŒçš„æ“ä½œç³»ç»Ÿï¼Œæˆ‘å°†æä¾›
    Mac ç‰¹å®šçš„å‘½ä»¤ä»¥åŠå®‰è£…æœåŠ¡çš„é“¾æ¥ã€‚
- en: Setting up Minikube
  id: totrans-split-42
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: è®¾ç½® Minikube
- en: '**Step 1:** **Install** [**Docker Desktop**](https://www.docker.com/products/docker-desktop/)'
  id: totrans-split-43
  prefs: []
  type: TYPE_NORMAL
  zh: '**æ­¥éª¤ 1:** **å®‰è£…** [**Docker Desktop**](https://www.docker.com/products/docker-desktop/)'
- en: '[PRE3]'
  id: totrans-split-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '**Step 2:** **Install** [**Kubectl**](https://kubernetes.io/docs/tasks/tools/)'
  id: totrans-split-45
  prefs: []
  type: TYPE_NORMAL
  zh: '**æ­¥éª¤ 2:** **å®‰è£…** [**Kubectl**](https://kubernetes.io/docs/tasks/tools/)'
- en: '**Step 3: Install minikube**'
  id: totrans-split-46
  prefs: []
  type: TYPE_NORMAL
  zh: '**æ­¥éª¤ 3: å®‰è£… minikube**'
- en: '[PRE4]'
  id: totrans-split-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '**Step 4:** **Start minikube**'
  id: totrans-split-48
  prefs: []
  type: TYPE_NORMAL
  zh: '**æ­¥éª¤ 4ï¼š** **å¯åŠ¨ minikube**'
- en: '[PRE5]'
  id: totrans-split-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '**Step 5: Interact with your cluster**'
  id: totrans-split-50
  prefs: []
  type: TYPE_NORMAL
  zh: '**æ­¥éª¤ 5ï¼šä¸æ‚¨çš„é›†ç¾¤äº¤äº’**'
- en: '[PRE6]'
  id: totrans-split-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '**Step 6:** **Install helm**'
  id: totrans-split-52
  prefs: []
  type: TYPE_NORMAL
  zh: '**æ­¥éª¤ 6ï¼š** **å®‰è£… helm**'
- en: Setting up PostgreSQL
  id: totrans-split-53
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: è®¾ç½® PostgreSQL
- en: '**Step 1: Install the PostgreSQL Helm Chart with extended configuration**'
  id: totrans-split-54
  prefs: []
  type: TYPE_NORMAL
  zh: '**æ­¥éª¤ 1ï¼šä½¿ç”¨æ‰©å±•é…ç½®å®‰è£… PostgreSQL Helm Chart**'
- en: Extend the configuration to set the <span class="code-exp">wal_level</span>
  id: totrans-split-55
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰©å±•é…ç½®ä»¥è®¾ç½® <span class="code-exp">wal_level</span>
- en: '[PRE7]'
  id: totrans-split-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '**Step 2:** **Connect to PostgreSQL using the PostgreSQL CLI Create the database**'
  id: totrans-split-57
  prefs: []
  type: TYPE_NORMAL
  zh: '**æ­¥éª¤ 2ï¼š** **ä½¿ç”¨ PostgreSQL CLI è¿æ¥åˆ° PostgreSQL åˆ›å»ºæ•°æ®åº“**'
- en: '[PRE8]'
  id: totrans-split-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '**Step 3: Create the table**'
  id: totrans-split-59
  prefs: []
  type: TYPE_NORMAL
  zh: '**æ­¥éª¤ 3ï¼šåˆ›å»ºè¡¨**'
- en: '[PRE9]'
  id: totrans-split-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '**Step 4: Create a PostgreSQL user**'
  id: totrans-split-61
  prefs: []
  type: TYPE_NORMAL
  zh: '**æ­¥éª¤ 4ï¼šåˆ›å»ºä¸€ä¸ª PostgreSQL ç”¨æˆ·**'
- en: 'To create a PostgreSQL user with the necessary privileges for Debezium to stream
    changes from the PostgreSQL source tables, run this statement with superuser privileges:'
  id: totrans-split-62
  prefs: []
  type: TYPE_NORMAL
  zh: è¦ä¸º Debezium ä» PostgreSQL æºè¡¨ä¸­æµå¼ä¼ è¾“æ›´æ”¹è®¾ç½®å…·æœ‰å¿…è¦ç‰¹æƒçš„ PostgreSQL ç”¨æˆ·ï¼Œè¯·ä½¿ç”¨è¶…çº§ç”¨æˆ·æƒé™è¿è¡Œæ­¤è¯­å¥ï¼š
- en: '[PRE10]'
  id: totrans-split-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Setting up Kafka (Redpanda)
  id: totrans-split-64
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: è®¾ç½® Kafkaï¼ˆRedpandaï¼‰
- en: '**Step 1: Add the helm repo and install dependencies**'
  id: totrans-split-65
  prefs: []
  type: TYPE_NORMAL
  zh: '**æ­¥éª¤ 1ï¼šæ·»åŠ  helm ä»“åº“å¹¶å®‰è£…ä¾èµ–é¡¹**'
- en: '[PRE11]'
  id: totrans-split-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '**Step 2: Configure and install Redpanda**'
  id: totrans-split-67
  prefs: []
  type: TYPE_NORMAL
  zh: '**æ­¥éª¤ 2ï¼šé…ç½®å’Œå®‰è£… Redpanda**'
- en: '[PRE12]'
  id: totrans-split-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '**Step 3: Install the Redpanda client <span class="code-exp">rpk</span>**'
  id: totrans-split-69
  prefs: []
  type: TYPE_NORMAL
  zh: '**æ­¥éª¤ 3ï¼šå®‰è£… Redpanda å®¢æˆ·ç«¯ <span class="code-exp">rpk</span>**'
- en: '[PRE13]'
  id: totrans-split-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '**Step 4:** **Create an alias to simplify the <span class="code-exp">rpk</span>
    commands**'
  id: totrans-split-71
  prefs: []
  type: TYPE_NORMAL
  zh: '**æ­¥éª¤ 4ï¼š** **åˆ›å»ºä¸€ä¸ªåˆ«åä»¥ç®€åŒ– <span class="code-exp">rpk</span> å‘½ä»¤**'
- en: '[PRE14]'
  id: totrans-split-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: '**Step 5: Describe the Redpanda Cluster**'
  id: totrans-split-73
  prefs: []
  type: TYPE_NORMAL
  zh: '**æ­¥éª¤ 5ï¼šæè¿° Redpanda é›†ç¾¤**'
- en: '[PRE15]'
  id: totrans-split-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: '**Step 6: Create a user**'
  id: totrans-split-75
  prefs: []
  type: TYPE_NORMAL
  zh: '**æ­¥éª¤ 6ï¼šåˆ›å»ºä¸€ä¸ªç”¨æˆ·**'
- en: '[PRE16]'
  id: totrans-split-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: '**Step 7: Create an** [**ACL**](https://docs.redpanda.com/current/manage/security/authorization/#create-acls)
    **for the user <span class="code-exp">blog</span>**'
  id: totrans-split-77
  prefs: []
  type: TYPE_NORMAL
  zh: '**æ­¥éª¤ 7ï¼šä¸ºç”¨æˆ·** [**ACL**](https://docs.redpanda.com/current/manage/security/authorization/#create-acls)
    **åˆ›å»ºä¸€ä¸ª <span class="code-exp">blog</span>**'
- en: '[PRE17]'
  id: totrans-split-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Setting up Kafka Connect
  id: totrans-split-79
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: è®¾ç½® Kafka Connect
- en: '**Step 1: Â Create a namespace**'
  id: totrans-split-80
  prefs: []
  type: TYPE_NORMAL
  zh: '**æ­¥éª¤ 1ï¼šåˆ›å»ºä¸€ä¸ªå‘½åç©ºé—´**'
- en: '[PRE18]'
  id: totrans-split-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: '**Step 2: Create the Service**'
  id: totrans-split-82
  prefs: []
  type: TYPE_NORMAL
  zh: '**æ­¥éª¤ 2ï¼šåˆ›å»ºæœåŠ¡**'
- en: Below is the manifest youâ€™ll need to apply to set up the Kafka Connect service
    with your Redpanda broker (see above). Â Copy the manifest below and provide the
    needed values to configure the service for your environment. In the example below,
    we are connecting with <span class="code-exp">SASL_PLAINTEXT</span> and <span
    class="code-exp">SCRAM-SHA-256</span>, ensure that <span class="code-exp">CONNECT_SASL_JAAS_CONFIG</span>
    is using <span class="code-exp">org.apache.kafka.common.security.scram.ScramLoginModule</span>
    and that you have set the username and password for both the <span class="code-exp">CONNECT_</span>
    and <span class="code-exp">PRODUCER_</span> keys.
  id: totrans-split-83
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸‹é¢æ˜¯æ‚¨éœ€è¦åº”ç”¨çš„æ¸…å•ï¼Œä»¥è®¾ç½®ä¸æ‚¨çš„ Redpanda broker è¿æ¥çš„ Kafka Connect æœåŠ¡ï¼ˆè§ä¸Šæ–‡ï¼‰ã€‚å¤åˆ¶ä¸‹é¢çš„æ¸…å•å¹¶æä¾›é…ç½®ç¯å¢ƒæ‰€éœ€çš„å€¼ã€‚åœ¨ä¸‹é¢çš„ç¤ºä¾‹ä¸­ï¼Œæˆ‘ä»¬ä½¿ç”¨
    <span class="code-exp">SASL_PLAINTEXT</span> å’Œ <span class="code-exp">SCRAM-SHA-256</span>
    è¿›è¡Œè¿æ¥ï¼Œè¯·ç¡®ä¿ <span class="code-exp">CONNECT_SASL_JAAS_CONFIG</span> ä½¿ç”¨ <span class="code-exp">org.apache.kafka.common.security.scram.ScramLoginModule</span>ï¼Œå¹¶ä¸”å·²ä¸º
    <span class="code-exp">CONNECT_</span> å’Œ <span class="code-exp">PRODUCER_</span>
    é”®è®¾ç½®äº†ç”¨æˆ·åå’Œå¯†ç ã€‚
- en: '[PRE19]'
  id: totrans-split-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: Use <span class="code-exp">kubectl</span> to install the manifest and start
    the Kafka Connect pod in your cluster.
  id: totrans-split-85
  prefs: []
  type: TYPE_NORMAL
  zh: ä½¿ç”¨ <span class="code-exp">kubectl</span> å®‰è£…æ¸…å•å¹¶åœ¨é›†ç¾¤ä¸­å¯åŠ¨ Kafka Connect podã€‚
- en: '[PRE20]'
  id: totrans-split-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: After you apply the manifest, you should get the following output.
  id: totrans-split-87
  prefs: []
  type: TYPE_NORMAL
  zh: åº”ç”¨æ¸…å•åï¼Œæ‚¨åº”è¯¥å¾—åˆ°ä»¥ä¸‹è¾“å‡ºã€‚
- en: '[PRE21]'
  id: totrans-split-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: Check the pod log files to ensure the service can log in to the broker. You
    should see the following in the logs if everything has been configured correctly.
  id: totrans-split-89
  prefs: []
  type: TYPE_NORMAL
  zh: æ£€æŸ¥ pod æ—¥å¿—æ–‡ä»¶ï¼Œç¡®ä¿æœåŠ¡å¯ä»¥ç™»å½•åˆ° brokerã€‚å¦‚æœä¸€åˆ‡é…ç½®æ­£ç¡®ï¼Œæ‚¨åº”è¯¥åœ¨æ—¥å¿—ä¸­çœ‹åˆ°ä»¥ä¸‹å†…å®¹ã€‚
- en: '[PRE22]'
  id: totrans-split-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: Check that the <span class="code-exp">connect-offsets</span> topic has been
    created on the Redpanda broker.
  id: totrans-split-91
  prefs: []
  type: TYPE_NORMAL
  zh: æ£€æŸ¥ <span class="code-exp">connect-offsets</span> ä¸»é¢˜æ˜¯å¦å·²åœ¨ Redpanda broker ä¸Šåˆ›å»ºã€‚
- en: '[PRE23]'
  id: totrans-split-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: Setting up the Debezium PostgreSQL Connector
  id: totrans-split-93
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: è®¾ç½® Debezium PostgreSQL è¿æ¥å™¨
- en: We must create a topic to store the CDC messages before deploying the Debezium
    PostgreSQL connector to the Kafka Connect service. Â  If you take a look at the
    config below weâ€™ve set the property <span class="code-exp">topic.prefix</span>
    to â€œblogâ€ <span class="code-exp">andpublication.autocreate.mode</span> to â€œfilteredâ€
    and set the <span class="code-exp">table.include.list</span> to â€œpublic.simpleâ€.
    Â This configures the connector to write the CDC messages to a topic named <span
    class="code-exp">blog.public.simple</span>, use the command below to create the
    topic, then deploy the connector using the cURL command. Youâ€™ll need to provide
    your environment-specific details for connecting to your PostgreSQL server and
    the username/password for the Redpanda service you used above.
  id: totrans-split-94
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨éƒ¨ç½²Debezium PostgreSQLè¿æ¥å™¨åˆ°Kafka ConnectæœåŠ¡ä¹‹å‰ï¼Œæˆ‘ä»¬å¿…é¡»åˆ›å»ºä¸€ä¸ªä¸»é¢˜æ¥å­˜å‚¨CDCæ¶ˆæ¯ã€‚å¦‚æœæŸ¥çœ‹ä¸‹é¢çš„é…ç½®ï¼Œæˆ‘ä»¬å·²å°†å±æ€§<span
    class="code-exp">topic.prefix</span>è®¾ç½®ä¸ºâ€œblogâ€ï¼Œ<span class="code-exp">publication.autocreate.mode</span>è®¾ç½®ä¸ºâ€œfilteredâ€ï¼Œå¹¶å°†<span
    class="code-exp">table.include.list</span>è®¾ç½®ä¸ºâ€œpublic.simpleâ€ã€‚è¿™å°†é…ç½®è¿æ¥å™¨å°†CDCæ¶ˆæ¯å†™å…¥åä¸º<span
    class="code-exp">blog.public.simple</span>çš„ä¸»é¢˜ã€‚ä½¿ç”¨ä»¥ä¸‹å‘½ä»¤åˆ›å»ºä¸»é¢˜ï¼Œç„¶åä½¿ç”¨cURLå‘½ä»¤éƒ¨ç½²è¿æ¥å™¨ã€‚æ‚¨éœ€è¦æä¾›è¿æ¥åˆ°æ‚¨çš„PostgreSQLæœåŠ¡å™¨çš„ç‰¹å®šç¯å¢ƒè¯¦ç»†ä¿¡æ¯ä»¥åŠæ‚¨ä¸Šé¢ä½¿ç”¨çš„RedpandaæœåŠ¡çš„ç”¨æˆ·å/å¯†ç ã€‚
- en: '**Step 1: Â **Create a topic in Redpanda'
  id: totrans-split-95
  prefs: []
  type: TYPE_NORMAL
  zh: '-   **Step 1: åˆ›å»ºRedpandaä¸­çš„ä¸»é¢˜**'
- en: '[PRE24]'
  id: totrans-split-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: Install the Debezium PostgreSQL connector using cURL. Â Youâ€™ll need to fetch
    the password for the PostgreSQL user and fill it in below. Â We will also set up
    port forwarding from the Kafka Connect service to our local host.
  id: totrans-split-97
  prefs: []
  type: TYPE_NORMAL
  zh: ä½¿ç”¨cURLå®‰è£…Debezium PostgreSQLè¿æ¥å™¨ã€‚æ‚¨éœ€è¦è·å–PostgreSQLç”¨æˆ·çš„å¯†ç å¹¶åœ¨ä¸‹é¢å¡«å†™å®ƒã€‚æˆ‘ä»¬è¿˜å°†è®¾ç½®ä»Kafka ConnectæœåŠ¡åˆ°æˆ‘ä»¬çš„æœ¬åœ°ä¸»æœºçš„ç«¯å£è½¬å‘ã€‚
- en: '**Step 2: Get the PostgreSQL password**'
  id: totrans-split-98
  prefs: []
  type: TYPE_NORMAL
  zh: '-   **Step 2: è·å–PostgreSQLå¯†ç **'
- en: '[PRE25]'
  id: totrans-split-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: '**Step 3: Forward the Kafka-Connect service port (8083)**'
  id: totrans-split-100
  prefs: []
  type: TYPE_NORMAL
  zh: '-   **Step 3: è½¬å‘Kafka-ConnectæœåŠ¡ç«¯å£ï¼ˆ8083ï¼‰**'
- en: '[PRE26]'
  id: totrans-split-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: '**Step 4: Deploy the Debezium Postgres Connector with cURL**'
  id: totrans-split-102
  prefs: []
  type: TYPE_NORMAL
  zh: '-   **Step 4: ä½¿ç”¨cURLéƒ¨ç½²Debezium Postgresè¿æ¥å™¨**'
- en: '[PRE27]'
  id: totrans-split-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: Insert data and validate the setup
  id: totrans-split-104
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: æ’å…¥æ•°æ®å¹¶éªŒè¯è®¾ç½®
- en: Letâ€™s validate the CDC messages are landing in the topic we created; weâ€™ll insert
    some data into our PostgreSQL table and then consume a message from the topic.
  id: totrans-split-105
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬éªŒè¯CDCæ¶ˆæ¯æ˜¯å¦è½åœ¨æˆ‘ä»¬åˆ›å»ºçš„ä¸»é¢˜ä¸­ï¼›æˆ‘ä»¬å°†å‘PostgreSQLè¡¨ä¸­æ’å…¥ä¸€äº›æ•°æ®ï¼Œç„¶åä»ä¸»é¢˜ä¸­æ¶ˆè´¹ä¸€æ¡æ¶ˆæ¯ã€‚
- en: '**Step 1: Connect to the database**'
  id: totrans-split-106
  prefs: []
  type: TYPE_NORMAL
  zh: '-   **Step 1: è¿æ¥åˆ°æ•°æ®åº“**'
- en: '[PRE28]'
  id: totrans-split-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: '**Step 2: Insert data into the table**'
  id: totrans-split-108
  prefs: []
  type: TYPE_NORMAL
  zh: '-   **Step 2: æ’å…¥æ•°æ®åˆ°è¡¨ä¸­**'
- en: '[PRE29]'
  id: totrans-split-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: '**Step 3: Consume the CDC message from the topic**'
  id: totrans-split-110
  prefs: []
  type: TYPE_NORMAL
  zh: '-   **Step 3: æ¶ˆè´¹æ¥è‡ªä¸»é¢˜çš„CDCæ¶ˆæ¯**'
- en: '[PRE30]'
  id: totrans-split-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: Weâ€™ve validated that the Debezium CDC messages with <span class="code-exp">before</span>-and-<span
    class="code-exp">after</span> values are landing in the topic. Â Donâ€™t be concerned
    if <span class="code-exp">before</span> is not filled out in this example; if
    you take a look, the â€œopâ€ is <span class="code-exp">c</span>, which means a new
    row was created in the table.
  id: totrans-split-112
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬éªŒè¯äº†Debezium CDCæ¶ˆæ¯çš„<span class="code-exp">before</span>å’Œ<span class="code-exp">after</span>å€¼æ˜¯å¦è½åœ¨ä¸»é¢˜ä¸­ã€‚å¦‚æœåœ¨æ­¤ç¤ºä¾‹ä¸­<span
    class="code-exp">before</span>æœªå¡«å†™ï¼Œä¸å¿…æ‹…å¿ƒï¼›å¦‚æœæŸ¥çœ‹ï¼Œâ€œopâ€ä¸º<span class="code-exp">c</span>ï¼Œè¿™æ„å‘³ç€åœ¨è¡¨ä¸­åˆ›å»ºäº†æ–°è¡Œã€‚
- en: Expose Redpanda to the internet
  id: totrans-split-113
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: å°†Redpandaæš´éœ²åˆ°äº’è”ç½‘
- en: Deploying all of the services locally using <span class="code-exp">minikube</span>
    is a great way to do local development and get hands-on experience without paying
    a cloud provider. However, for us to use Propel and create a Kafka Data Pool,
    Propel has to connect to our Redpanda topic. There are a number of ways that this
    can be accomplished. For the sake of simplicity, we are going to install a reverse
    proxy and create a TCP Tunnel to expose the Redpanda port to the internet. Â The
    reverse proxy we are going to use is called [LocalXpose](https://localxpose.io/).
  id: totrans-split-114
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æœ¬åœ°ä½¿ç”¨<span class="code-exp">minikube</span>éƒ¨ç½²æ‰€æœ‰æœåŠ¡æ˜¯è¿›è¡Œæœ¬åœ°å¼€å‘å’Œè·å¾—å®é™…æ“ä½œç»éªŒçš„å¥½æ–¹æ³•ï¼Œè€Œæ— éœ€æ”¯ä»˜äº‘æœåŠ¡æä¾›å•†ã€‚ä½†æ˜¯ï¼Œä¸ºäº†ä½¿ç”¨Propelå¹¶åˆ›å»ºKafkaæ•°æ®æ± ï¼ŒPropelå¿…é¡»è¿æ¥åˆ°æˆ‘ä»¬çš„Redpandaä¸»é¢˜ã€‚æœ‰å¤šç§æ–¹æ³•å¯ä»¥å®ç°è¿™ä¸€ç‚¹ã€‚ä¸ºäº†ç®€å•èµ·è§ï¼Œæˆ‘ä»¬å°†å®‰è£…ä¸€ä¸ªåå‘ä»£ç†ï¼Œå¹¶åˆ›å»ºä¸€ä¸ªTCPéš§é“å°†Redpandaç«¯å£æš´éœ²åˆ°äº’è”ç½‘ã€‚æˆ‘ä»¬å°†ä½¿ç”¨çš„åå‘ä»£ç†ç§°ä¸º[LocalXpose](https://localxpose.io/)ã€‚
- en: ğŸ’¡ To use the LocalXpose TCP Tunnel with a [dedicated port](https://localxpose.io/docs/tunnels/tcp),
    I had to pay $6\. Â Other free services also do this, but you canâ€™t control which
    port the tunnel is on. Â Redpanda has a limitation when creating the external NodePort
    that restricts it to ports <span class="code-exp">30000-32767</span>.
  id: totrans-split-115
  prefs: []
  type: TYPE_NORMAL
  zh: ğŸ’¡ è¦ä½¿ç”¨LocalXpose TCPéš§é“åŠå…¶[ä¸“ç”¨ç«¯å£](https://localxpose.io/docs/tunnels/tcp)ï¼Œæˆ‘ä¸å¾—ä¸æ”¯ä»˜$6ã€‚å…¶ä»–å…è´¹æœåŠ¡ä¹Ÿè¿™æ ·åšï¼Œä½†æ‚¨æ— æ³•æ§åˆ¶éš§é“æ‰€åœ¨çš„ç«¯å£ã€‚åˆ›å»ºå¤–éƒ¨NodePortæ—¶ï¼ŒRedpandaå­˜åœ¨ä¸€ä¸ªé™åˆ¶ï¼Œä»…é™äºç«¯å£<span
    class="code-exp">30000-32767</span>ã€‚
- en: '**Step 1: Install LocalXpose and <span class="code-exp">kcat</span>**'
  id: totrans-split-116
  prefs: []
  type: TYPE_NORMAL
  zh: '-   **Step 1: å®‰è£…LocalXposeå’Œ<span class="code-exp">kcat</span>**'
- en: '[https://localxpose.io/docs](https://localxpose.io/docs)'
  id: totrans-split-117
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://localxpose.io/docs](https://localxpose.io/docs)'
- en: '[PRE31]'
  id: totrans-split-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: '**Step 2: Start the TCP Tunnel**'
  id: totrans-split-119
  prefs: []
  type: TYPE_NORMAL
  zh: '-   **Step 2: å¯åŠ¨TCPéš§é“**'
- en: '[PRE32]'
  id: totrans-split-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: You can now see the running tunnels. Copy the domain name in <span class="code-exp">From</span>,
    e.g., <span class="code-exp">us.localx.io</span>. This will be used in step 3
    and later when creating the Kafka Data Pool.
  id: totrans-split-121
  prefs: []
  type: TYPE_NORMAL
  zh: æ‚¨ç°åœ¨å¯ä»¥çœ‹åˆ°æ­£åœ¨è¿è¡Œçš„éš§é“ã€‚å¤åˆ¶ <span class="code-exp">From</span> ä¸­çš„åŸŸåï¼Œä¾‹å¦‚ <span class="code-exp">us.localx.io</span>ã€‚è¿™å°†åœ¨æ­¥éª¤
    3 å’Œåˆ›å»º Kafka æ•°æ®æ± æ—¶ä½¿ç”¨ã€‚
- en: '**Step 3: Upgrade the Redpanda install and expose an external NodePort**'
  id: totrans-split-122
  prefs: []
  type: TYPE_NORMAL
  zh: '**æ­¥éª¤ 3ï¼šå‡çº§ Redpanda å®‰è£…å¹¶å…¬å¼€ä¸€ä¸ªå¤–éƒ¨ NodePort**'
- en: Save the yaml below to <span class="code-exp">external-dns.yaml</span>, and
    replace the <span class="code-exp-bracket">domain</span> with the value from step
    2.
  id: totrans-split-123
  prefs: []
  type: TYPE_NORMAL
  zh: å°†ä¸‹é¢çš„ yaml ä¿å­˜åˆ° <span class="code-exp">external-dns.yaml</span>ï¼Œå¹¶ç”¨æ­¥éª¤ 2 çš„å€¼æ›¿æ¢ <span
    class="code-exp-bracket">domain</span>ã€‚
- en: '[PRE33]'
  id: totrans-split-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: Upgrade Redpanda.
  id: totrans-split-125
  prefs: []
  type: TYPE_NORMAL
  zh: å‡çº§ Redpandaã€‚
- en: '[PRE34]'
  id: totrans-split-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: '**Step 4: Forward the traffic to <span class="code-exp">minikube</span>**'
  id: totrans-split-127
  prefs: []
  type: TYPE_NORMAL
  zh: '**æ­¥éª¤ 4ï¼šå°†æµé‡è½¬å‘åˆ° <span class="code-exp">minikube</span>**'
- en: '[PRE35]'
  id: totrans-split-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: '**Step 5: Validate you can connect externally using <span class="code-exp">kcat</span>**'
  id: totrans-split-129
  prefs: []
  type: TYPE_NORMAL
  zh: '**æ­¥éª¤ 5ï¼šéªŒè¯æ‚¨å¯ä»¥ä½¿ç”¨ <span class="code-exp">kcat</span> è¿›è¡Œå¤–éƒ¨è¿æ¥**'
- en: '[PRE36]'
  id: totrans-split-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: You can now tunnel traffic to Redpanda running in <span class="code-exp">minikube</span>,
    allowing Propelâ€™s backend to connect and consume the CDC messages from the topic.
  id: totrans-split-131
  prefs: []
  type: TYPE_NORMAL
  zh: æ‚¨ç°åœ¨å¯ä»¥å°†æµé‡éš§é“åˆ°è¿è¡Œåœ¨ <span class="code-exp">minikube</span> ä¸Šçš„ Redpandaï¼Œå…è®¸ Propel
    çš„åç«¯è¿æ¥å¹¶æ¶ˆè´¹æ¥è‡ªä¸»é¢˜çš„ CDC æ¶ˆæ¯ã€‚
- en: Create a Propel Kafka Data Pool
  id: totrans-split-132
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: åˆ›å»º Propel Kafka æ•°æ®æ± ã€‚
- en: Login to the [Propel console](https://console.propeldata.com) or create a new
    account. Â Then choose your environment and create a new Kafka Datapool, for this
    example Iâ€™m using the <span class="code-exp">development</span> environment.
  id: totrans-split-133
  prefs: []
  type: TYPE_NORMAL
  zh: ç™»å½•åˆ° [Propel æ§åˆ¶å°](https://console.propeldata.com) æˆ–åˆ›å»ºä¸€ä¸ªæ–°å¸æˆ·ã€‚ç„¶åé€‰æ‹©æ‚¨çš„ç¯å¢ƒå¹¶åˆ›å»ºä¸€ä¸ªæ–°çš„ Kafka
    æ•°æ®æ± ï¼Œä¾‹å¦‚æˆ‘åœ¨ <span class="code-exp">development</span> ç¯å¢ƒä¸­ä½¿ç”¨ã€‚
- en: Next, choose Add new credentials to configure the connection to our Redpanda
    cluster. (use the domain and port from the TCP Tunnel that you created above for
    the Bootstrap Servers)
  id: totrans-split-134
  prefs: []
  type: TYPE_NORMAL
  zh: æ¥ä¸‹æ¥ï¼Œé€‰æ‹©æ·»åŠ æ–°å‡­æ®ä»¥é…ç½®åˆ°æˆ‘ä»¬çš„ Redpanda é›†ç¾¤çš„è¿æ¥ã€‚ï¼ˆä½¿ç”¨ä¸Šé¢ä¸º Bootstrap æœåŠ¡å™¨åˆ›å»ºçš„ TCP éš§é“ä¸­çš„åŸŸå’Œç«¯å£ï¼‰
- en: Click Create and test Credentials
  id: totrans-split-135
  prefs: []
  type: TYPE_NORMAL
  zh: ç‚¹å‡»åˆ›å»ºå¹¶æµ‹è¯•å‡­æ®ã€‚
- en: Ensure the topics are visible
  id: totrans-split-136
  prefs: []
  type: TYPE_NORMAL
  zh: ç¡®ä¿ä¸»é¢˜å¯è§ã€‚
- en: Next, choose the topic <span class="code-exp">blog.public.simple</span> to start
    pulling in the CDC data.
  id: totrans-split-137
  prefs: []
  type: TYPE_NORMAL
  zh: æ¥ä¸‹æ¥ï¼Œé€‰æ‹©ä¸»é¢˜ <span class="code-exp">blog.public.simple</span>ï¼Œå¼€å§‹æ‹‰å– CDC æ•°æ®ã€‚
- en: Click Next to choose the primary timestamp.
  id: totrans-split-138
  prefs: []
  type: TYPE_NORMAL
  zh: ç‚¹å‡»ä¸‹ä¸€æ­¥é€‰æ‹©ä¸»è¦æ—¶é—´æˆ³ã€‚
- en: Give the Data Pool a unique name.
  id: totrans-split-139
  prefs: []
  type: TYPE_NORMAL
  zh: ç»™æ•°æ®æ± ä¸€ä¸ªå”¯ä¸€çš„åç§°ã€‚
- en: Click Next to start syncing the data from the topic into Propel.
  id: totrans-split-140
  prefs: []
  type: TYPE_NORMAL
  zh: ç‚¹å‡»ä¸‹ä¸€æ­¥å¼€å§‹å°†ä¸»é¢˜æ•°æ®åŒæ­¥åˆ° Propel ä¸­ã€‚
- en: Once you see two records in the Datapool, you can click Preview Data.
  id: totrans-split-141
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€æ—¦æ‚¨åœ¨æ•°æ®æ± ä¸­çœ‹åˆ°ä¸¤æ¡è®°å½•ï¼Œæ‚¨å¯ä»¥ç‚¹å‡»é¢„è§ˆæ•°æ®ã€‚
- en: Query the data
  id: totrans-split-142
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æŸ¥è¯¢æ•°æ®ã€‚
- en: To query the data we have in Propel, youâ€™ll need to create an Application. Follow
    the guide in the [docs](https://www.propeldata.com/docs/applications) to create
    an application. Once youâ€™ve created an Application, you can get a token, as seen
    in the image below. Weâ€™ll use that token in the subsequent API calls below.
  id: totrans-split-143
  prefs: []
  type: TYPE_NORMAL
  zh: è¦æŸ¥è¯¢åœ¨ Propel ä¸­æ‹¥æœ‰çš„æ•°æ®ï¼Œæ‚¨éœ€è¦åˆ›å»ºä¸€ä¸ªåº”ç”¨ç¨‹åºã€‚è¯·æŒ‰ç…§ [æ–‡æ¡£](https://www.propeldata.com/docs/applications)
    ä¸­çš„æŒ‡å—åˆ›å»ºä¸€ä¸ªåº”ç”¨ç¨‹åºã€‚åˆ›å»ºåº”ç”¨ç¨‹åºåï¼Œæ‚¨å¯ä»¥è·å–ä¸€ä¸ªä»¤ç‰Œï¼Œå¦‚ä¸‹å›¾æ‰€ç¤ºã€‚æˆ‘ä»¬å°†åœ¨ä¸‹é¢çš„åç»­ API è°ƒç”¨ä¸­ä½¿ç”¨è¯¥ä»¤ç‰Œã€‚
- en: Use the [Data Grid API](https://www.propeldata.com/docs/query-your-data/data-grid)
    to query the data in the newly created Kafka Data Pool. Â Make sure to copy the
    token you created above and replace <span class="code-exp-bracket">TOKEN</span>
    with the value. Â You will also need to replace <span class="code-exp-bracket">DATAPOOL_ID</span>
    with the ID of the newly created Kafka Data Pool. Copy the <span class="code-exp">curl</span>
    command below and execute it from the command line.
  id: totrans-split-144
  prefs: []
  type: TYPE_NORMAL
  zh: ä½¿ç”¨ [Data Grid API](https://www.propeldata.com/docs/query-your-data/data-grid)
    æŸ¥è¯¢æ–°åˆ›å»ºçš„ Kafka æ•°æ®æ± ä¸­çš„æ•°æ®ã€‚ç¡®ä¿å¤åˆ¶æ‚¨åˆ›å»ºçš„ä»¤ç‰Œï¼Œå¹¶ç”¨å€¼æ›¿æ¢ <span class="code-exp-bracket">TOKEN</span>ã€‚æ‚¨è¿˜éœ€è¦ç”¨æ–°åˆ›å»ºçš„
    Kafka æ•°æ®æ± çš„ ID æ›¿æ¢ <span class="code-exp-bracket">DATAPOOL_ID</span>ã€‚å¤åˆ¶ä¸‹é¢çš„ <span
    class="code-exp">curl</span> å‘½ä»¤ï¼Œå¹¶åœ¨å‘½ä»¤è¡Œä¸­æ‰§è¡Œå®ƒã€‚
- en: '[PRE37]'
  id: totrans-split-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: This will return a Data Grid with headers and rows.
  id: totrans-split-146
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™å°†è¿”å›ä¸€ä¸ªå¸¦æœ‰æ ‡é¢˜å’Œè¡Œçš„æ•°æ®ç½‘æ ¼ã€‚
- en: '[PRE38]'
  id: totrans-split-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: Update the source PostgreSQL database
  id: totrans-split-148
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: æ›´æ–°æº PostgreSQL æ•°æ®åº“ã€‚
- en: If we update the PostgreSQL database with a new value for one of the rows, weâ€™ll
    see that change reflected in the Propel Data Pool. Letâ€™s execute the SQL statement
    below and re-run the GraphQL query we ran above.
  id: totrans-split-149
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœæˆ‘ä»¬æ›´æ–° PostgreSQL æ•°æ®åº“ä¸­æŸè¡Œçš„æ–°å€¼ï¼Œæˆ‘ä»¬å°†çœ‹åˆ°åœ¨ Propel æ•°æ®æ± ä¸­åæ˜ å‡ºè¿™ç§å˜åŒ–ã€‚è®©æˆ‘ä»¬æ‰§è¡Œä¸‹é¢çš„ SQL è¯­å¥ï¼Œå¹¶é‡æ–°è¿è¡Œä¸Šé¢è¿è¡Œçš„
    GraphQL æŸ¥è¯¢ã€‚
- en: '[PRE39]'
  id: totrans-split-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: When we re-run the GraphQL query, we get the following response.
  id: totrans-split-151
  prefs: []
  type: TYPE_NORMAL
  zh: å½“æˆ‘ä»¬é‡æ–°è¿è¡Œ GraphQL æŸ¥è¯¢æ—¶ï¼Œæˆ‘ä»¬å¾—åˆ°ä»¥ä¸‹å“åº”ã€‚
- en: '[PRE40]'
  id: totrans-split-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: This shows that we have three rows in our Data Pool; it reflects the raw CDC
    messages that have been sent to the Kafka Data Pool. In order to use the data
    for analytics, we need to transform it to correctly represent the current state
    in the source database.
  id: totrans-split-153
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ˜¾ç¤ºæˆ‘ä»¬çš„æ•°æ®æ± ä¸­æœ‰ä¸‰è¡Œæ•°æ®ï¼›å®ƒåæ˜ äº†å·²å‘é€åˆ° Kafka æ•°æ®æ± çš„åŸå§‹ CDC æ¶ˆæ¯ã€‚ä¸ºäº†å°†æ•°æ®ç”¨äºåˆ†æï¼Œæˆ‘ä»¬éœ€è¦å¯¹å…¶è¿›è¡Œè½¬æ¢ï¼Œä»¥æ­£ç¡®è¡¨ç¤ºæºæ•°æ®åº“ä¸­çš„å½“å‰çŠ¶æ€ã€‚
- en: Create a real-time transformation to unpack the CDC JSON message
  id: totrans-split-154
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: åˆ›å»ºä¸€ä¸ªå®æ—¶è½¬æ¢æ¥è§£åŒ… CDC JSON æ¶ˆæ¯
- en: Now that weâ€™re getting the stream of CDC messages to Propel and landing them
    in a Kafka Data Pool, we need to write some SQL to transform the messages based
    on the â€œopâ€ type. Â The columns we are interested in using are <span class="code-exp">id</span>,
    <span class="code-exp">_timestamp</span>, and <span class="code-exp">foo</span>.
    Â Our <span class="code-exp">SELECT</span> statement below will query the source
    table <span class="code-exp">blog-post</span> and extract <span class="code-exp">id</span>
    and <span class="code-exp">foo</span> from <span class="code-exp">_propel_payload.after</span>.
    Â This will handle both the create and update operations. Â In the case of an update
    operation a duplicate row will be inserted and then de-duped by Propel. Â This
    will ensure that when we set <span class="code-exp">id</span> as the <span class="code-exp">uniqueId</span>
    column in the MaterializedView only one version of the <span class="code-exp">id</span>
    exists with the latest value from the source table.
  id: totrans-split-155
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨æˆ‘ä»¬å·²ç»å°† CDC æ¶ˆæ¯æµå‘é€åˆ° Propel å¹¶å°†å…¶è½å…¥ Kafka æ•°æ®æ± ï¼Œæˆ‘ä»¬éœ€è¦ç¼–å†™ä¸€äº› SQL æ¥æ ¹æ®â€œopâ€ç±»å‹è½¬æ¢æ¶ˆæ¯ã€‚æˆ‘ä»¬æ„Ÿå…´è¶£çš„åˆ—æ˜¯
    <span class="code-exp">id</span>ã€<span class="code-exp">_timestamp</span> å’Œ <span
    class="code-exp">foo</span>ã€‚ä¸‹é¢çš„ <span class="code-exp">SELECT</span> è¯­å¥å°†æŸ¥è¯¢æºè¡¨ <span
    class="code-exp">blog-post</span> å¹¶ä» <span class="code-exp">_propel_payload.after</span>
    ä¸­æå– <span class="code-exp">id</span> å’Œ <span class="code-exp">foo</span>ã€‚è¿™å°†å¤„ç†åˆ›å»ºå’Œæ›´æ–°æ“ä½œã€‚åœ¨æ›´æ–°æ“ä½œçš„æƒ…å†µä¸‹ï¼Œå°†æ’å…¥ä¸€ä¸ªé‡å¤çš„è¡Œï¼Œç„¶åç”±
    Propel è¿›è¡Œå»é‡ã€‚è¿™å°†ç¡®ä¿åœ¨æˆ‘ä»¬å°† <span class="code-exp">id</span> è®¾ç½®ä¸º MaterializedView ä¸­çš„
    <span class="code-exp">uniqueId</span> åˆ—æ—¶ï¼Œåªå­˜åœ¨ä¸€ä¸ªç‰ˆæœ¬çš„ <span class="code-exp">id</span>ï¼Œå…¶å€¼æ¥è‡ªæºè¡¨çš„æœ€æ–°å€¼ã€‚
- en: '[PRE41]'
  id: totrans-split-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: You should get a response like the one below.
  id: totrans-split-157
  prefs: []
  type: TYPE_NORMAL
  zh: ä½ åº”è¯¥å¾—åˆ°å¦‚ä¸‹çš„å“åº”ã€‚
- en: '[PRE42]'
  id: totrans-split-158
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: Query the Materialized View Data Pool
  id: totrans-split-159
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: æŸ¥è¯¢ Materialized View æ•°æ®æ± 
- en: Now that weâ€™ve created the MaterialzedView letâ€™s query the Data Pool that was
    created along with it. Â Replace <span class="code-exp-bracket">token</span> with
    the token you created earlier, and use the response from the above <span class="code-exp">curl</span>
    command to get the destination data pool id. Â This is available at <span class="code-exp">data.createMaterializedView.materializedView.destination.id</span>
    in the JSON. Â Replace <span class="code-exp">datapool_id</span>with that value
    and run the <span class="code-exp">curl</span> command from the command line.
  id: totrans-split-160
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨æˆ‘ä»¬å·²ç»åˆ›å»ºäº† MaterializedViewï¼Œè®©æˆ‘ä»¬æŸ¥è¯¢ä¸å…¶ä¸€åŒåˆ›å»ºçš„ Data Poolã€‚ç”¨ä¹‹å‰åˆ›å»ºçš„ä»¤ç‰Œæ›¿æ¢ <span class="code-exp-bracket">token</span>ï¼Œå¹¶ä½¿ç”¨ä¸Šè¿°
    <span class="code-exp">curl</span> å‘½ä»¤çš„å“åº”æ¥è·å–ç›®æ ‡æ•°æ®æ±  IDã€‚è¿™åœ¨ JSON ä¸­å¯ä»¥é€šè¿‡ <span class="code-exp">data.createMaterializedView.materializedView.destination.id</span>
    æ‰¾åˆ°ã€‚å°† <span class="code-exp">datapool_id</span> æ›¿æ¢ä¸ºè¯¥å€¼ï¼Œç„¶åä»å‘½ä»¤è¡Œè¿è¡Œ <span class="code-exp">curl</span>
    å‘½ä»¤ã€‚
- en: '[PRE43]'
  id: totrans-split-161
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: The response you get should only have two rows and the updated value of <span
    class="code-exp-bracket">baz</span> for <span class="code-exp">9cb52b2a-8ef2-4987-8856-c79a1b2c2f73</span>.
  id: totrans-split-162
  prefs: []
  type: TYPE_NORMAL
  zh: ä½ æ‰€å¾—åˆ°çš„å“åº”åº”è¯¥åªæœ‰ä¸¤è¡Œï¼Œå¹¶ä¸”æ›´æ–°äº† <span class="code-exp-bracket">baz</span> çš„å€¼ä¸º <span class="code-exp">9cb52b2a-8ef2-4987-8856-c79a1b2c2f73</span>ã€‚
- en: '[PRE44]'
  id: totrans-split-163
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: Visualize the results
  id: totrans-split-164
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: å¯è§†åŒ–ç»“æœ
- en: Letâ€™s write a small <span class="code-exp">node.js</span> application that combines
    all of the Propel concepts and renders a table with the results from the MaterializedView.
    Â Instead of providing a token, weâ€™ll use the Propel API to mint a token using
    the OAuth2 client credentials from the Propel Application we created. Â These are
    found in the Propel console within the Applications section, as seen below.
  id: totrans-split-165
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬ç¼–å†™ä¸€ä¸ªå°çš„ <span class="code-exp">node.js</span> åº”ç”¨ç¨‹åºï¼Œç»“åˆ Propel çš„æ‰€æœ‰æ¦‚å¿µï¼Œå¹¶ä½¿ç”¨æˆ‘ä»¬åˆ›å»ºçš„
    Propel åº”ç”¨ç¨‹åºçš„ OAuth2 å®¢æˆ·ç«¯å‡­æ®æ¥å‘ˆç°åŒ…å« MaterializedView ç»“æœçš„è¡¨æ ¼ã€‚è¿™äº›å‡­æ®å¯ä»¥åœ¨ Propel æ§åˆ¶å°çš„åº”ç”¨ç¨‹åºéƒ¨åˆ†ä¸­æ‰¾åˆ°ï¼Œå¦‚ä¸‹æ‰€ç¤ºã€‚
- en: '**Step 1: OAuth2 client credentials**'
  id: totrans-split-166
  prefs: []
  type: TYPE_NORMAL
  zh: '**æ­¥éª¤ 1ï¼šOAuth2 å®¢æˆ·ç«¯å‡­æ®**'
- en: '**Step 2: Set environment variables**'
  id: totrans-split-167
  prefs: []
  type: TYPE_NORMAL
  zh: '**æ­¥éª¤ 2ï¼šè®¾ç½®ç¯å¢ƒå˜é‡**'
- en: Replace the tokens with the values from the console, then paste and execute
    the commands.
  id: totrans-split-168
  prefs: []
  type: TYPE_NORMAL
  zh: ç”¨æ§åˆ¶å°ä¸­çš„å€¼æ›¿æ¢ä»¤ç‰Œï¼Œç„¶åç²˜è´´å¹¶æ‰§è¡Œå‘½ä»¤ã€‚
- en: '[PRE45]'
  id: totrans-split-169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: '**Step 3: Install the dependencies**'
  id: totrans-split-170
  prefs: []
  type: TYPE_NORMAL
  zh: '**æ­¥éª¤ 3ï¼šå®‰è£…ä¾èµ–é¡¹**'
- en: '[PRE46]'
  id: totrans-split-171
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: '**Step 4: Copy the code**â€'
  id: totrans-split-172
  prefs: []
  type: TYPE_NORMAL
  zh: '**æ­¥éª¤ 4ï¼šå¤åˆ¶ä»£ç **â€'
- en: Copy the code below and replace <span class="code-exp-bracket">DATAPOOL_ID</span>
    with the data pool ID for the MaterializedView from above.
  id: totrans-split-173
  prefs: []
  type: TYPE_NORMAL
  zh: å¤åˆ¶ä¸‹é¢çš„ä»£ç ï¼Œå¹¶å°† <span class="code-exp-bracket">DATAPOOL_ID</span> æ›¿æ¢ä¸ºä¸Šé¢ MaterializedView
    çš„æ•°æ®æ±  IDã€‚
- en: '[PRE47]'
  id: totrans-split-174
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: '**Step 5: Run the script**'
  id: totrans-split-175
  prefs: []
  type: TYPE_NORMAL
  zh: '**ç¬¬äº”æ­¥ï¼šè¿è¡Œè„šæœ¬**'
- en: '[PRE48]'
  id: totrans-split-176
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: Wrap up
  id: totrans-split-177
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æ€»ç»“
- en: In wrapping up this post, it's important to recap the key skills and knowledge
    you've acquired. You have learned how to effectively stream PostgreSQL Change
    Data Capture (CDC) to a Propel Kafka DataPool, a significant step in managing
    real-time data.
  id: totrans-split-178
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ç»“æŸæœ¬ç¯‡æ–‡ç« æ—¶ï¼Œé‡ç‚¹æ˜¯å›é¡¾æ‚¨å·²ç»æŒæ¡çš„å…³é”®æŠ€èƒ½å’ŒçŸ¥è¯†ã€‚æ‚¨å·²ç»å­¦ä¼šäº†å¦‚ä½•æœ‰æ•ˆåœ°æµå¼ä¼ è¾“PostgreSQLå˜æ›´æ•°æ®æ•è·ï¼ˆCDCï¼‰åˆ°Propel Kafka
    DataPoolï¼Œè¿™æ˜¯ç®¡ç†å®æ—¶æ•°æ®çš„é‡è¦ä¸€æ­¥ã€‚
- en: Further, you've gained the ability to query this data, which opens up a wealth
    of opportunities for analysis and insights. This guide also walked you through
    the process of updating the source PostgreSQL database, ensuring that you can
    maintain the accuracy and relevance of your data.
  id: totrans-split-179
  prefs: []
  type: TYPE_NORMAL
  zh: æ­¤å¤–ï¼Œæ‚¨å·²ç»æŒæ¡äº†æŸ¥è¯¢è¿™äº›æ•°æ®çš„èƒ½åŠ›ï¼Œè¿™ä¸ºåˆ†æå’Œæ´å¯ŸåŠ›å¼€è¾Ÿäº†å¤§é‡æœºä¼šã€‚æœ¬æŒ‡å—è¿˜æŒ‡å¯¼æ‚¨å¦‚ä½•æ›´æ–°æºPostgreSQLæ•°æ®åº“ï¼Œç¡®ä¿æ‚¨å¯ä»¥ä¿æŒæ•°æ®çš„å‡†ç¡®æ€§å’Œç›¸å…³æ€§ã€‚
- en: One of the highlights of this guide was teaching you how to create a real-time
    transformation to unpack the CDC JSON message. This is a fundamental skill that
    elevates your data management capabilities, enabling you to manipulate and interpret
    data for various applications.
  id: totrans-split-180
  prefs: []
  type: TYPE_NORMAL
  zh: æœ¬æŒ‡å—çš„äº®ç‚¹ä¹‹ä¸€æ˜¯æ•™æ‚¨å¦‚ä½•åˆ›å»ºå®æ—¶è½¬æ¢ä»¥è§£æCDC JSONæ¶ˆæ¯ã€‚è¿™æ˜¯æå‡æ‚¨æ•°æ®ç®¡ç†èƒ½åŠ›çš„åŸºæœ¬æŠ€èƒ½ï¼Œä½¿æ‚¨èƒ½å¤Ÿä¸ºå„ç§åº”ç”¨ç¨‹åºæ“ä½œå’Œè§£é‡Šæ•°æ®ã€‚
- en: Finally, you've learned how to visualize the results. Visualization plays a
    crucial role in data analysis, making complex data more understandable, revealing
    trends and outliers, and contributing to more effective communication.
  id: totrans-split-181
  prefs: []
  type: TYPE_NORMAL
  zh: æœ€åï¼Œæ‚¨å­¦ä¼šäº†å¦‚ä½•å¯è§†åŒ–ç»“æœã€‚å¯è§†åŒ–åœ¨æ•°æ®åˆ†æä¸­èµ·ç€å…³é”®ä½œç”¨ï¼Œä½¿å¤æ‚æ•°æ®æ›´æ˜“ç†è§£ï¼Œæ­ç¤ºè¶‹åŠ¿å’Œå¼‚å¸¸å€¼ï¼Œå¹¶æœ‰åŠ©äºæ›´æœ‰æ•ˆçš„æ²Ÿé€šã€‚
- en: We hope you found the content both informative and practical and that you enjoyed
    the learning process. Propel is dedicated to empowering you with the tools and
    knowledge to transform your data into valuable insights. We look forward to supporting
    you in your future endeavors with Propel. Keep exploring and transforming your
    data!
  id: totrans-split-182
  prefs: []
  type: TYPE_NORMAL
  zh: å¸Œæœ›æ‚¨è§‰å¾—è¿™äº›å†…å®¹æ—¢å…·ä¿¡æ¯æ€§åˆå®ç”¨ï¼Œå¹¶ä¸”äº«å—å­¦ä¹ è¿‡ç¨‹ã€‚Propelè‡´åŠ›äºä¸ºæ‚¨æä¾›å·¥å…·å’ŒçŸ¥è¯†ï¼Œå°†æ‚¨çš„æ•°æ®è½¬åŒ–ä¸ºæœ‰ä»·å€¼çš„æ´å¯ŸåŠ›ã€‚æˆ‘ä»¬æœŸå¾…ç€åœ¨æ‚¨ä½¿ç”¨Propelçš„æœªæ¥æ—…ç¨‹ä¸­æ”¯æŒæ‚¨ã€‚ç»§ç»­æ¢ç´¢å’Œè½¬åŒ–æ‚¨çš„æ•°æ®ï¼
