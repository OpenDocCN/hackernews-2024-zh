- en: <!--yml
  id: totrans-split-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-split-1
  prefs: []
  type: TYPE_NORMAL
  zh: 类别：未分类
- en: 'date: 2024-05-27 15:00:09'
  id: totrans-split-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-05-27 15:00:09
- en: -->
  id: totrans-split-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: CXL Is Dead In The AI Era
  id: totrans-split-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: CXL在AI时代已经名存实亡
- en: 来源：[https://www.semianalysis.com/p/cxl-is-dead-in-the-ai-era](https://www.semianalysis.com/p/cxl-is-dead-in-the-ai-era)
  id: totrans-split-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://www.semianalysis.com/p/cxl-is-dead-in-the-ai-era](https://www.semianalysis.com/p/cxl-is-dead-in-the-ai-era)
- en: If we travel back 2 years in time, before the rapid rise in AI, much of the
    datacenter hardware world was chasing CXL. It was promised as the messiah to bring
    heterogenous compute, memory pooling, and composable server architectures. Existing
    players and a whole new host of startups were rushing to integrate CXL into their
    products, or create new CXL based products such as memory expanders, poolers,
    and switches. Fast forward to 2023 and early 2024, and many projects have been
    quietly shelved and many of the hyperscalers and large semiconductor companies
    have almost entirely pivoted away.
  id: totrans-split-6
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们回到两年前，在AI的快速崛起之前，数据中心硬件世界大多数是在追逐CXL。它被承诺为带来异构计算、内存池和可组合服务器架构的救世主。现有的参与者和一大批新创企业都在赶着将CXL集成到他们的产品中，或者创建基于CXL的新产品，例如内存扩展器、池化器和交换机。快进到2023年和2024年初，许多项目已经悄然搁置，许多超大规模和大型半导体公司几乎完全转向其他方向。
- en: 'With the upcoming Astera Labs IPO and product launches, CXL discussions are
    coming back to the front row at least for a short while. We have written [extensively
    about the technology](https://www.semianalysis.com/p/cxl-deep-dive-future-of-composable),
    its [cost savings potential for Cloud Service Providers](https://www.semianalysis.com/p/cxl-enables-microsoft-azure-to-cut),
    and [the ecosystem & hardware stack](https://www.semianalysis.com/p/cxl-deep-dive-future-of-composable).
    While very promising on paper, the datacenter landscape has evolved a lot, but
    one thing hasn’t changed: CXL hardware such as controllers and switches are still
    not shipping in meaningful volumes. Despite this, there is still a lot of noise
    and research around CXL, with certain professionals in the industry now pushing
    the narrative of CXL as an ''enabler'' of AI.'
  id: totrans-split-7
  prefs: []
  type: TYPE_NORMAL
  zh: 随着Astera Labs的即将上市和产品发布，CXL讨论至少在短期内重新引起关注。我们已经对技术进行了[广泛的介绍](https://www.semianalysis.com/p/cxl-deep-dive-future-of-composable)，它对[云服务提供商的成本节约潜力](https://www.semianalysis.com/p/cxl-enables-microsoft-azure-to-cut)，以及[生态系统和硬件堆栈](https://www.semianalysis.com/p/cxl-deep-dive-future-of-composable)。尽管在纸面上非常有前景，数据中心的格局已经发生了很大变化，但有一件事情仍然没有改变：像控制器和交换机这样的CXL硬件仍然没有大量发货。尽管如此，围绕CXL仍然存在大量噪音和研究，某些行业专业人士现在正推动CXL作为AI的“推动者”的叙述。
- en: Is the broader CXL market ready to take off and deliver on its promise? Can
    CXL become the interconnect for AI applications? What is the role in CPU attach
    expansion and pooling? We will answer these questions in the subscriber section
    of this report.
  id: totrans-split-8
  prefs: []
  type: TYPE_NORMAL
  zh: 更广泛的CXL市场是否已准备好起飞并兑现其承诺？CXL能否成为AI应用的互连？在CPU附加扩展和池化中的作用是什么？我们将在本报告的订阅者部分回答这些问题。
- en: The simple answer is no - the people pushing CXL for AI are dead wrong. Let’s
    start with a quick reminder of the main use cases and promises of CXL.
  id: totrans-split-9
  prefs: []
  type: TYPE_NORMAL
  zh: 简单来说，推动CXL用于AI的人完全错了。让我们先快速回顾一下CXL的主要用例和承诺。
- en: 'CXL is a protocol built on top of the PCIe physical layer, enabling cache and
    memory coherency across devices. Leveraging the wide availability of PCIe interfaces,
    CXL allows memory to be shared across various kinds of hardware: CPUs, NICs &
    DPUs, GPUs & other accelerators, SSDs, and Memory devices.'
  id: totrans-split-10
  prefs: []
  type: TYPE_NORMAL
  zh: CXL是建立在PCIe物理层之上的协议，实现设备间的缓存和内存一致性。利用广泛的PCIe接口可用性，CXL允许在各种硬件上共享内存：CPU、NIC和DPU、GPU及其他加速器、SSD和内存设备。
- en: 'This enables the following use cases:'
  id: totrans-split-11
  prefs: []
  type: TYPE_NORMAL
  zh: 这样可以实现以下用例：
- en: 'Memory expansion: CXL can help increase memory bandwidth and capacity for servers.'
  id: totrans-split-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 内存扩展：CXL可以帮助增加服务器的内存带宽和容量。
- en: 'Memory pooling: CXL can create memory pools where memory is disaggregated from
    the CPU, which, in theory, could massively increase DRAM utilization rates. On
    paper, this could save billions for each Cloud Service Providers.'
  id: totrans-split-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 内存池：CXL可以创建内存池，将内存从CPU中分离，理论上可以大幅提高DRAM的利用率。从理论上讲，这可以为每个云服务提供商节省数十亿美元。
- en: 'Heterogenous compute: ASIC’s are far more efficient than general purpose CPUs.
    CXL can help enable heterogenous compute by providing a low latency cache coherent
    interconnect between ASIC’s and general purpose compute so applications can integrate
    them into existing code bases more easily.'
  id: totrans-split-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 异构计算：ASIC比通用CPU效率高得多。CXL可以通过提供低延迟高速缓存一致性互连，将ASIC与通用计算整合，使应用更容易集成到现有代码库中。
- en: 'Composable server architectures: Servers are broken apart into their various
    components and placed in groups where these resources can be dynamically assigned
    to workloads on the fly, improving resource stranding and utilization rates while
    also better matching application needs.'
  id: totrans-split-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可组合的服务器架构：服务器被分解成各种组件，并放置在可以动态分配工作负载的组中，从而改善资源的利用率，同时更好地匹配应用程序的需求，减少资源的浪费。
- en: 'The below figure tells part of the story: CXL could solve the latency and bandwidth
    gaps between main system memory, and storage, to enable a new memory tier.'
  id: totrans-split-16
  prefs: []
  type: TYPE_NORMAL
  zh: 下图讲述了故事的一部分：CXL可以解决主系统内存和存储之间的延迟和带宽差距，从而实现一个新的内存层。
- en: Some folks are now [forecasting CXL sales of up to $15 billion](https://www.yolegroup.com/press-release/cxl-technology-unlocks-memory-performance/)
    by 2028 versus a few millions today, so we felt it was time for a proper update
    on the CXL market, because that is an outright ridiculous claim. Let’s start by
    addressing the case of CXL for Artificial Intelligence.
  id: totrans-split-17
  prefs: []
  type: TYPE_NORMAL
  zh: 一些人现在预测到2028年，CXL的销售额将达到高达150亿美元，而目前仅为几百万美元，因此我们认为是时候对CXL市场进行适当的更新了，因为这是一个完全荒谬的说法。让我们首先讨论人工智能领域的CXL案例。
- en: Currently, CXL availability is the main issue, as Nvidia GPUs don't support
    it, and with AMD the technology is limited to the MI300A. While the MI300X can
    theoretically support CXL in hardware, it is not exposed properly. Availability
    of CXL IP will improve in the future, but there exist deeper issues than just
    availability that render CXL irrelevant in the era of accelerated computing.
  id: totrans-split-18
  prefs: []
  type: TYPE_NORMAL
  zh: 目前，CXL的可用性是主要问题，因为Nvidia的GPU不支持它，而AMD的技术仅限于MI300A。虽然MI300X理论上可以在硬件上支持CXL，但并未正确暴露。CXL
    IP的可用性将在未来得到改善，但存在使CXL在加速计算时代变得无关紧要的更深层次问题。
- en: The two main issues are related to PCIe SerDes and beachfront or shoreline area.
    IO for chips generally must come from the edges of the chip. The below picture
    from Nvidia shows the H100 in a cartoonized format. The center has all the compute.
    The top and and bottom sides are 100% dedicated to HBM. As we move forward from
    the H100 to B100, the number of HBM grows to 8, requiring even more shore line
    area on it. Nvidia continues to consume two entire sides of their 2 die package
    with HBM.
  id: totrans-split-19
  prefs: []
  type: TYPE_NORMAL
  zh: 两个主要问题与PCIe SerDes、海滨或海岸线区域有关。芯片的IO通常必须来自芯片的边缘。下面的图片来自Nvidia，以卡通化的方式展示了H100。中心有所有的计算能力。顶部和底部完全专用于HBM。随着我们从H100过渡到B100，HBM的数量增加到8个，这需要更多的海岸线面积。Nvidia继续用其2片芯片包装的两侧完全用于HBM。
- en: 'The remaining two sides are dedicated to other chip to chip IO, and this is
    where standards and proprietary interconnects compete for die area. A H100 GPU
    has 3 IO formats: PCIe, NVlink, and C2C (for connecting to Grace). Nvidia has
    decided to include only the minimum 16 PCIe lanes, as Nvidia largely prefers the
    latter NVLink and C2C. Note that server CPUs, such as AMD’s Genoa, go up to 128
    lanes of PCIe.'
  id: totrans-split-20
  prefs: []
  type: TYPE_NORMAL
  zh: 剩余的两侧专用于其他芯片到芯片的IO，并且这是标准和专有互连在芯片面积上竞争的地方。H100 GPU具有3种IO格式：PCIe、NVlink和C2C（用于连接到Grace）。Nvidia决定仅包括最少的16条PCIe通道，因为Nvidia更偏爱后者NVLink和C2C。请注意，诸如AMD的Genoa等服务器CPU可达到128条PCIe通道。
- en: The main reason for this choice is bandwidth. A 16-lane PCIe interface has 64GB/s
    of bandwidth in each direction. Nvidia’s NVlink brings 450 GB/s bandwidth in each
    direction to other GPUs, roughly 7x higher. Nvidia’s C2C also brings 450GB/s bandwidth
    in each direction with the Grace CPU. To be fair, Nvidia dedicates far more beachfront
    area to NVLink, we so need to include silicon area in that equation; but even
    so, we estimate that per mm² across a large variety of SOCs, Ethernet-style SerDes
    such as Nvidia NVLink, Google ICI, etc have 3x more bandwidth per unit of shoreline
    area.
  id: totrans-split-21
  prefs: []
  type: TYPE_NORMAL
  zh: 选择这一方案的主要原因是带宽。16通道的PCIe接口在每个方向上有64GB/s的带宽。Nvidia的NVlink为其他GPU带来了每个方向450 GB/s的带宽，大约高出7倍。Nvidia的C2C与Grace
    CPU也带来了每个方向450GB/s的带宽。公平地说，Nvidia为NVLink分配了更多的海滨区域，所以我们需要在这个方程式中包括硅面积；但即使如此，我们估计，在各种各样的SOC中，以太网风格的SerDes，如Nvidia
    NVLink，Google ICI等每平方毫米的带宽都是海岸线面积的3倍。
- en: Therefore, if you are a chip designer in a bandwidth constrained world, you
    are making your chip roughly 3x worse when you chose to go with PCIe 5.0 instead
    of 112G Ethernet-style SerDes. This gap remains with [next generation GPUs and
    AI accelerators adopting 224G SerDes](https://www.semianalysis.com/p/nvidias-plans-to-crush-competition),
    keeping this 3x gap with PCIe 6.0 / CXL 3.0\. We are in a pad limited world, and
    throwing away IO efficiency is an insane tradeoff.
  id: totrans-split-22
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，如果您是处于带宽受限世界中的芯片设计师，选择使用PCIe 5.0而不是112G Ethernet风格的SerDes会使您的芯片大约差3倍。这种差距将随着下一代GPU和AI加速器采用224G
    SerDes而保持，维持与PCIe 6.0 / CXL 3.0的这种3倍差距。我们正处于一个受限于引脚的世界，放弃IO效率是一个不理智的权衡。
- en: The main scale up and scale out interconnects for AI Clusters will be proprietary
    protocols such as Nvidia NVlink and Google ICI, or Ethernet and Infiniband. This
    is due to [intrinsic PCIe SerDes limitations](https://www.semianalysis.com/i/137826061/roadmap-b-x-h-hbme-g-serdes-pcie-co-packaged-optics-optical-switch)
    even in scale up formats. Due to diverging latency goals, PCIe and Ethernet SerDes
    have dramatically different bit-error-rate (BER) requirements.
  id: totrans-split-23
  prefs: []
  type: TYPE_NORMAL
  zh: AI集群的主要扩展和扩展互连协议将是专有协议，如Nvidia NVlink和Google ICI，或者Ethernet和Infiniband。即使在扩展格式中也是如此，这是由于PCIe
    SerDes在固有的限制下。由于延迟目标的分歧，PCIe和Ethernet SerDes具有极其不同的比特错误率（BER）要求。
- en: PCIe 6 requires a BER of < 1e-12 while Ethernet requires 1e-4\. This massive
    8 orders of magnitude difference is due to PCIe's strict latency requirements,
    necessitating an extremely light forward error correction (FEC) scheme. FECs digitally
    add redundant parity bits/information at the transmitter which is used by the
    receiver to detect and correct errors (bit flips) much like ECC in memory systems.
    Heavier FECs add more overhead, occupying space that could be used for data bits
    instead. More importantly, FECs add a large amount of latency on the receiver.
    This is why PCIe has avoided having any FEC up until Gen6.
  id: totrans-split-24
  prefs: []
  type: TYPE_NORMAL
  zh: PCIe 6要求BER < 1e-12，而Ethernet要求1e-4。这种8个数量级的巨大差异是由于PCIe的严格延迟要求，需要一种极轻的前向纠错（FEC）方案。FEC在发送端数字添加冗余的奇偶校验位/信息，接收端用于检测和纠正错误（位翻转），类似于内存系统中的ECC。更重的FEC会增加更多的开销，在本应用于数据位的空间上占用。更重要的是，FEC会在接收端增加大量延迟。这就是为什么直到第六代PCIe，PCIe才避免使用任何FEC。
- en: Ethernet-style SerDes are much less constrained by stringent PCIe specifications,
    allowing it to be much faster and have higher bandwidth. As a result, NVlink has
    higher latency, but this doesn’t matter as much in the AI world of massively parallel
    workloads, where ~100ns vs ~30ns is not a worthy consideration.
  id: totrans-split-25
  prefs: []
  type: TYPE_NORMAL
  zh: Ethernet风格的SerDes受到严格的PCIe规范限制较少，因此它可以更快速、具有更高的带宽。因此，NVlink具有更高的延迟，但在大规模并行工作负载的AI世界中，约100纳秒与约30纳秒并不是一个值得考虑的问题。
- en: First off, the MI300 AID uses most of its beachfront area for PCIe SerDes instead
    of Ethernet-style SerDes. While this gives AMD more configurability in terms of
    IFIS, CXL, and PCIe connectivity, it results in the total IO being about 1/3 that
    of Ethernet-style SerDes. AMD needs to immediately move off PCIe-style SerDes
    for their AI accelerator if they want to have any hopes at competing with Nvidia’s
    B100\. We believe they will be for MI400.
  id: totrans-split-26
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 首先，MI300 AID主要利用其海滨区域进行PCIe SerDes而非Ethernet风格的SerDes。虽然这使得AMD在IFIS、CXL和PCIe连接方面具有更多的配置灵活性，但结果是总IO大约只有Ethernet风格SerDes的三分之一。如果AMD希望与Nvidia的B100竞争，他们需要立即摆脱PCIe风格的SerDes。我们相信他们将会在MI400上实现这一点。
- en: '[Nvidia’s Plans To Crush Competition – B100, “X100”, H200, 224G SerDes, OCS,
    CPO, PCIe 7.0, HBM3E](https://www.semianalysis.com/p/nvidias-plans-to-crush-competition)'
  id: totrans-split-27
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '[Nvidia的计划以碾压竞争为目标 – B100，“X100”，H200，224G SerDes，OCS，CPO，PCIe 7.0，HBM3E](https://www.semianalysis.com/p/nvidias-plans-to-crush-competition)'
- en: AMD’s lack of high quality SerDes are severely limiting them in the competitiveness
    of their products long term. They have come up with the Open xGMI / Open Infinity
    Fabric / Accelerated Fabric Link, because CXL is not the correct protocol for
    AI. While it is primarily based on PCIe, it does eschew some of the standard features
    of PCIe 7.0 and CXL for time to market, performance, coherence, and reach reasons.
  id: totrans-split-28
  prefs: []
  type: TYPE_NORMAL
  zh: AMD缺乏高质量的SerDes严重限制了他们长期产品的竞争力。他们提出了Open xGMI / Open Infinity Fabric / Accelerated
    Fabric Link，因为CXL不是AI的正确协议。虽然它主要基于PCIe，但出于时间市场、性能、一致性和覆盖范围等原因，它放弃了PCIe 7.0和CXL的一些标准功能。
- en: What about CXL memory bandwidth expansion for AI? What about the custom AI hyperscaler
    chips adoption? What about the other custom silicon chips from other vendors such
    as the Marvell Google CXL chip? We will address those questions and review the
    more classic Memory Pooling and Memory Expansion use-cases.
  id: totrans-split-29
  prefs: []
  type: TYPE_NORMAL
  zh: CXL 内存带宽扩展对 AI 有何影响？定制 AI 超级计算机芯片采用情况如何？其他供应商如 Marvell Google 推出的定制硅芯片又如何？我们将回答这些问题，并审视更经典的内存池化和内存扩展用例。
- en: '[Get 20% off a group subscription](https://www.semianalysis.com/subscribe?group=true&coupon=fe141654)'
  id: totrans-split-30
  prefs: []
  type: TYPE_NORMAL
  zh: '[团体订阅享 20% 折扣](https://www.semianalysis.com/subscribe?group=true&coupon=fe141654)'
