- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 'category: 未分类'
- en: 'date: 2024-05-27 14:45:03'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 'date: 2024-05-27 14:45:03'
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: The Global Project to Make a General Robotic Brain - IEEE Spectrum
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**通用机器人大脑计划** - IEEE Spectrum'
- en: 来源：[https://spectrum.ieee.org/global-robotic-brain](https://spectrum.ieee.org/global-robotic-brain)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://spectrum.ieee.org/global-robotic-brain](https://spectrum.ieee.org/global-robotic-brain)
- en: '**The generative AI revolution** embodied in tools like [ChatGPT](https://chat.openai.com/),
    [Midjourney](https://www.midjourney.com/), and many others is at its core based
    on a simple formula: Take a very large neural network, train it on a huge dataset
    scraped from the Web, and then use it to fulfill a broad range of user requests.
    Large language models ([LLM](https://spectrum.ieee.org/tag/llms)s) can answer
    questions, write code, and spout poetry, while image-generating systems can create
    convincing cave paintings or contemporary art.'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: '**生成式人工智能革命**体现在诸如[ChatGPT](https://chat.openai.com/)、[Midjourney](https://www.midjourney.com/)等工具中，其核心基于一个简单的公式：利用一个非常庞大的神经网络，在从网络上抓取的大量数据集上进行训练，然后使用它来满足广泛的用户请求。大型语言模型([LLM](https://spectrum.ieee.org/tag/llms))可以回答问题、编写代码和吟诗作对，而图像生成系统可以创建逼真的洞穴壁画或当代艺术作品。'
- en: So why haven’t these amazing AI capabilities translated into the kinds of helpful
    and broadly useful robots we’ve seen in science fiction? Where are the robots
    that can clean off the table, fold your laundry, and make you breakfast?
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 那么为什么这些惊人的人工智能能力没有转化为科幻小说中所见的那种有用和广泛使用的机器人呢？那些可以清理桌子、折叠衣服和为你做早餐的机器人在哪里？
- en: Unfortunately, the highly successful generative AI formula—big models trained
    on lots of Internet-sourced data—doesn’t easily carry over into [robotics](https://spectrum.ieee.org/topic/robotics/),
    because the Internet is not full of robotic-interaction data in the same way that
    it’s full of text and images. Robots need robot data to learn from, and this data
    is typically created slowly and tediously by researchers in laboratory environments
    for very specific tasks. Despite tremendous progress on robot-learning algorithms,
    without abundant data we still can’t enable robots to perform real-world tasks
    (like making breakfast) outside the lab. The most impressive results typically
    only work in a single laboratory, on a single robot, and often involve only a
    handful of behaviors.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，高度成功的生成式人工智能公式——在大量互联网数据上训练的大模型——并不容易转化为[机器人技术](https://spectrum.ieee.org/topic/robotics/)，因为互联网并不像文本和图像那样充满机器人交互数据。机器人需要机器人数据进行学习，而这些数据通常由研究人员在实验室环境中缓慢而费力地创建，用于非常具体的任务。尽管机器人学习算法取得了巨大进步，但没有丰富的数据，我们仍然无法使机器人在实验室外执行现实任务（比如做早餐）。最令人印象深刻的结果通常只在单个实验室、单个机器人上工作，并且通常只涉及少量行为。
- en: If the abilities of each robot are limited by the time and effort it takes to
    manually teach it to perform a new task, what if we were to pool together the
    experiences of many robots, so a new robot could learn from all of them at once?
    We decided to give it a try. In 2023, our labs at [Google](https://spectrum.ieee.org/tag/google)
    and the University of California, Berkeley came together with 32 other robotics
    laboratories in North America, Europe, and Asia to undertake the [RT-X project](https://robotics-transformer-x.github.io/),
    with the goal of assembling data, resources, and code to make general-purpose
    robots a reality.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 如果每个机器人的能力都受到手动教导它执行新任务所需的时间和精力的限制，那么如果我们将许多机器人的经验汇集起来，使一个新机器人可以同时从所有这些经验中学习呢？我们决定试一试。2023年，我们在[Google](https://spectrum.ieee.org/tag/google)和加州大学伯克利分校的实验室与北美、欧洲和亚洲的其他32个机器人实验室一起，开始了[RT-X
    项目](https://robotics-transformer-x.github.io/)，旨在整合数据、资源和代码，使通用目的的机器人成为现实。
- en: Here is what we learned from the first phase of this effort.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 这是我们从这一努力的第一阶段中学到的内容。
- en: How to create a generalist robot
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何创建通用机器人
- en: 'Humans are far better at this kind of learning. Our brains can, with a little
    practice, handle what are essentially changes to our body plan, which happens
    when we pick up a tool, ride a bicycle, or get in a car. That is, our “embodiment”
    changes, but our brains adapt. RT-X is aiming for something similar in robots:
    to enable a single deep neural network to control many different [types of robots](https://robotsguide.com/learn/types-of-robots),
    a capability called cross-embodiment. The question is whether a deep neural network
    trained on data from a sufficiently large number of different robots can learn
    to “drive” all of them—even robots with very different appearances, physical properties,
    and capabilities. If so, this approach could potentially unlock the power of large
    datasets for robotic learning.'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 人类在这种学习方面要好得多。经过一点练习，我们的大脑可以处理基本上是对我们身体结构的改变，当我们拿起工具、骑自行车或开车时就会发生这种情况。也就是说，我们的“体现”会改变，但我们的大脑会适应。RT-X的目标是使机器人实现类似的功能：启用一个单一的深度神经网络来控制许多不同类型的[机器人](https://robotsguide.com/learn/types-of-robots)，这种能力称为跨体现。问题在于，一个经过足够多不同机器人数据训练的深度神经网络是否能够学会“驾驶”所有这些机器人，甚至是外观、物理特性和功能非常不同的机器人。如果是这样，这种方法可能会解锁机器学习的大型数据集的潜力。
- en: The scale of this project is very large because it has to be. The RT-X dataset
    currently contains nearly a million robotic trials for 22 types of robots, including
    many of the most commonly used robotic arms on the market. The robots in this
    dataset perform a huge range of behaviors, including picking and placing objects,
    assembly, and specialized tasks like cable routing. In total, there are about
    500 different skills and interactions with thousands of different objects. It’s
    the largest open-source dataset of real robotic actions in existence.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 这个项目的规模非常大，因为必须如此。RT-X数据集目前包含了近百万次针对22种不同类型机器人的试验，其中包括市场上许多常用的机器人臂。这个数据集中的机器人执行着各种各样的行为，包括拾取和放置物体、装配以及诸如电缆布线等专业任务。总共有大约500种不同的技能和与数千种不同物体的交互。这是现有的最大的开源真实机器人动作数据集。
- en: Surprisingly, we found that our multirobot data could be used with relatively
    simple machine-learning methods, provided that we follow the recipe of using large
    neural-network models with large datasets. Leveraging the same kinds of models
    used in current LLMs like [ChatGPT](https://spectrum.ieee.org/tag/chatgpt), we
    were able to train robot-control algorithms that do not require any special features
    for cross-embodiment. Much like a person can drive a car or ride a bicycle using
    the same brain, a model trained on the RT-X dataset can simply recognize what
    kind of robot it’s controlling from what it sees in the robot’s own camera observations.
    If the robot’s camera sees a [UR10 industrial arm](https://www.universal-robots.com/products/ur10-robot/),
    the model sends commands appropriate to a UR10\. If the model instead sees a low-cost
    [WidowX hobbyist arm](https://www.trossenrobotics.com/widowxrobotarm), the model
    moves it accordingly.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 令人惊讶的是，我们发现我们的多机器人数据可以使用相对简单的机器学习方法，只要我们遵循使用大型神经网络模型和大型数据集的配方。利用类似于当前LLMs（如[ChatGPT](https://spectrum.ieee.org/tag/chatgpt)）中使用的模型，我们能够训练出不需要任何特殊特征进行跨体现的机器人控制算法。就像一个人可以使用同一大脑驾驶汽车或骑自行车一样，一个在RT-X数据集上训练的模型可以简单地从机器人自己的摄像头观察中识别出它正在控制的是什么类型的机器人。如果机器人的摄像头看到一个[UR10工业机械臂](https://www.universal-robots.com/products/ur10-robot/)，模型会发送适用于UR10的命令。如果模型看到一个低成本的[WidowX爱好者机械臂](https://www.trossenrobotics.com/widowxrobotarm)，模型会相应地移动它。
- en: To test the capabilities of our model, five of the laboratories involved in
    the RT-X collaboration each tested it in a head-to-head comparison against the
    best control system they had developed independently for their own robot. Each
    lab’s test involved the tasks it was using for its own research, which included
    things like picking up and moving objects, opening doors, and routing cables through
    clips. Remarkably, the single unified model provided improved performance over
    each laboratory’s own best method, succeeding at the tasks about 50 percent more
    often on average.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 为了测试我们模型的能力，RT-X合作中的五个实验室各自对其进行了与各自为其自己的机器人开发的最佳控制系统进行对比的头对头比较测试。每个实验室的测试都涉及到其自己研究中使用的任务，其中包括拾取和移动物体、打开门以及通过夹子进行电缆布线等事项。值得注意的是，这个单一的统一模型在每个实验室自己的最佳方法上提供了更好的性能，平均成功率提高了大约50%。
- en: 'While this result might seem surprising, we found that the RT-X controller
    could leverage the diverse experiences of other robots to improve robustness in
    different settings. Even within the same laboratory, every time a robot attempts
    a task, it finds itself in a slightly different situation, and so drawing on the
    experiences of other robots in other situations helped the RT-X controller with
    natural variability and edge cases. Here are a few examples of the range of these
    tasks:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管这个结果可能看起来令人惊讶，但我们发现 RT-X 控制器可以利用其他机器人的多样经验来提高在不同环境下的稳健性。即使在同一个实验室内，每次机器人尝试一个任务，它都会处于稍微不同的情境中，所以借鉴其他机器人在其他情境中的经验有助于
    RT-X 控制器处理自然变异和边缘情况。以下是一些这些任务范围的例子：
- en: Building robots that can reason
  id: totrans-17
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 建造能够推理的机器人
- en: Encouraged by our success with combining data from many [robot types](https://robotsguide.com/learn/types-of-robots),
    we next sought to investigate how such data can be incorporated into a system
    with more in-depth reasoning capabilities. Complex semantic reasoning is hard
    to learn from robot data alone. While the robot data can provide a range of *physical*
    capabilities, more complex tasks like “Move apple between can and orange” also
    require understanding the semantic relationships between objects in an image,
    basic common sense, and other symbolic knowledge that is not directly related
    to the robot’s physical capabilities.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 鼓舞着我们成功地将许多[机器人类型](https://robotsguide.com/learn/types-of-robots)的数据结合起来，接下来我们寻求调查如何将这些数据纳入具有更深层次推理能力的系统中。从仅有机器人数据学习复杂的语义推理是困难的。虽然机器人数据可以提供一系列*物理*能力，但像“将苹果从罐头移到橙子之间”这样的更复杂任务还需要理解图像中物体之间的语义关系，基本常识以及其他与机器人的物理能力无直接关联的符号知识。
- en: 'So we decided to add another massive source of data to the mix: Internet-scale
    image and text data. We used an existing large vision-language model that is already
    proficient at many tasks that require some understanding of the connection between
    natural language and images. The model is similar to the ones available to the
    public such as ChatGPT or [Bard](https://bard.google.com/chat). These models are
    trained to output text in response to prompts containing images, allowing them
    to solve problems such as visual question-answering, captioning, and other open-ended
    visual understanding tasks. We discovered that such models can be adapted to robotic
    control simply by training them to also output robot actions in response to prompts
    framed as robotic commands (such as “Put the banana on the plate”). We applied
    this approach to the robotics data from the RT-X collaboration.'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们决定将另一个大规模数据源添加到混合中：互联网规模的图像和文本数据。我们使用了一个现有的大型视觉语言模型，该模型已经擅长于许多需要一定程度理解自然语言与图像之间关联的任务。该模型类似于公开可用的模型，如
    ChatGPT 或 [Bard](https://bard.google.com/chat)。这些模型经过训练，以响应包含图像的提示输出文本，使它们能够解决诸如视觉问答、字幕制作和其他开放式视觉理解任务等问题。我们发现这样的模型可以被调整为适应机器人控制，只需训练它们也以机器人命令的形式框架的提示输出机器人动作（例如“将香蕉放在盘子上”）。我们将这种方法应用于来自
    RT-X 合作的机器人数据。
- en: The RT-X model uses images or text descriptions of specific robot arms doing
    different tasks to output a series of discrete actions that will allow any robot
    arm to do those tasks. By collecting data from many robots doing many tasks from
    robotics labs around the world, we are building an open-source dataset that can
    be used to teach robots to be generally useful.Chris Philpot
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: RT-X 模型使用特定机器人手臂的图像或文本描述来输出一系列离散动作，从而使任何机器人手臂能够执行这些任务。通过收集来自世界各地机器人实验室的许多机器人执行许多任务的数据，我们正在构建一个可以用来教导机器人成为普遍有用的开源数据集。Chris
    Philpot
- en: To evaluate the combination of Internet-acquired smarts and multirobot data,
    we tested our RT-X model with Google’s mobile manipulator robot. We gave it our
    hardest generalization benchmark tests. The robot had to recognize objects and
    successfully manipulate them, and it also had to respond to complex text commands
    by making logical inferences that required integrating information from both text
    and images. The latter is one of the things that make humans such good generalists.
    Could we give our robots at least a hint of such capabilities?
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 为了评估因特网获取的智能和多机器人数据的组合，我们使用谷歌的移动操纵机器人测试了我们的 RT-X 模型。我们对其进行了最难的泛化基准测试。机器人必须识别对象并成功操纵它们，还必须通过进行逻辑推理来回应复杂的文本命令，这需要整合来自文本和图像的信息。后者是使人类成为如此优秀的泛化者之一的原因。我们能否给我们的机器人至少一点这样的能力的暗示呢？
- en: We conducted two sets of evaluations. As a baseline, we used a model that excluded
    all of the generalized multirobot RT-X data that didn’t involve Google’s robot.
    Google’s robot-specific dataset is in fact the largest part of the RT-X dataset,
    with over 100,000 demonstrations, so the question of whether all the other multirobot
    data would actually help in this case was very much open. Then we tried again
    with all that multirobot data included.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 我们进行了两组评估。 作为基准，我们使用了一个模型，该模型排除了所有不涉及谷歌机器人的泛化多机器人 RT-X 数据。 实际上，谷歌的机器人特定数据集是
    RT-X 数据集的最大部分，拥有超过 100,000 个演示，因此是否所有其它多机器人数据实际上会在这种情况下有所帮助这个问题是非常值得商榷的。 然后我们再次尝试包含所有多机器人数据。
- en: In one of the most difficult evaluation scenarios, the Google robot needed to
    accomplish a task that involved reasoning about spatial relations (“Move apple
    between can and orange”); in another task it had to solve rudimentary math problems
    (“Place an object on top of a paper with the solution to ‘2+3’”). These challenges
    were meant to test the crucial capabilities of reasoning and drawing conclusions.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 在最困难的评估场景之一中，谷歌机器人需要完成一个涉及推理空间关系（“将苹果移动到罐子和橙子之间”）的任务； 在另一个任务中，它必须解决基本的数学问题（“将一个物体放在解答‘2+3’的纸上”）。
    这些挑战旨在测试推理和得出结论的关键能力。
- en: In this case, the reasoning capabilities (such as the meaning of “between” and
    “on top of”) came from the Web-scale data included in the training of the vision-language
    model, while the ability to ground the reasoning outputs in robotic behaviors—commands
    that actually moved the robot arm in the right direction—came from training on
    cross-embodiment robot data from RT-X. An example of an evaluation where we asked
    the robot to perform a task not included in its training data is shown in the
    video below.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，推理能力（例如“之间”和“在……上面”的含义）来自训练视觉语言模型的 Web 规模数据，而将推理输出与机器人行为（实际上将机器人手臂移动到正确位置的命令）联系起来的能力来自
    RT-X 的跨体现机器人数据的训练。 我们要求机器人执行其训练数据中未包含的任务的一个示例评估在下面的视频中显示。
- en: Even without specific training, this Google research robot is able to follow
    the instruction “move apple between can and orange.” This capability is enabled
    by RT-X, a large robotic manipulation dataset and the first step towards a general
    robotic brain.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 即使没有特定的训练，这个谷歌研究机器人也能够遵循“将苹果移动到罐子和橙子之间”的指令。 这一能力是由 RT-X 提供的，这是一个大型的机器人操纵数据集，也是通往通用机器人大脑的第一步。
- en: While these tasks are rudimentary for humans, they present a major challenge
    for general-purpose robots. Without robotic demonstration data that clearly illustrates
    concepts like “between,” “near,” and “on top of,” even a system trained on data
    from many different robots would not be able to figure out what these commands
    mean. By integrating Web-scale knowledge from the vision-language model, our complete
    system was able to solve such tasks, deriving the semantic concepts (in this case,
    spatial relations) from Internet-scale training, and the physical behaviors (picking
    up and moving objects) from multirobot RT-X data. To our surprise, we found that
    the inclusion of the multirobot data improved the Google robot’s ability to generalize
    to such tasks by a factor of three. This result suggests that not only was the
    multirobot RT-X data useful for acquiring a variety of physical skills, it could
    also help to better connect such skills to the semantic and symbolic knowledge
    in vision-language models. These connections give the robot a degree of common
    sense, which could one day enable robots to understand the meaning of complex
    and nuanced user commands like “Bring me my breakfast” while carrying out the
    actions to make it happen.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管对于人类来说，这些任务是基本的，但对于通用用途的机器人来说，它们构成了一个重大挑战。 没有清晰地说明诸如“之间”、“附近”和“在……上面”等概念的机器人示范数据，即使是在许多不同机器人的数据上训练过的系统也无法弄清楚这些命令的含义。
    通过将视觉语言模型的 Web 规模知识与其它数据集集成，我们的完整系统能够解决这些任务，从互联网规模的训练中得出语义概念（在这种情况下，是空间关系），并从多机器人
    RT-X 数据中得出物理行为（拾起和移动物体）。 令我们惊讶的是，我们发现多机器人数据的加入使 Google 机器人的泛化能力提高了三倍。 这一结果表明，多机器人
    RT-X 数据不仅对于获得各种物理技能有用，而且还有助于更好地将这些技能与视觉语言模型中的语义和符号知识联系起来。 这些联系赋予了机器人一定程度的常识，这可能有一天使机器人能够理解像“把我的早餐给我拿来”这样的复杂而微妙的用户命令，同时执行使之发生的操作。
- en: The next steps for RT-X
  id: totrans-27
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: RT-X 的下一步
- en: The RT-X project shows what is possible when the robot-learning community acts
    together. Because of this cross-institutional effort, we were able to put together
    a diverse robotic dataset and carry out comprehensive multirobot evaluations that
    wouldn’t be possible at any single institution. Since the robotics community can’t
    rely on scraping the Internet for training data, we need to create that data ourselves.
    We hope that more researchers will contribute their data to the [RT-X database](https://robotics-transformer-x.github.io/)
    and join this collaborative effort. We also hope to provide tools, models, and
    infrastructure to support cross-embodiment research. We plan to go beyond sharing
    data across labs, and we hope that RT-X will grow into a collaborative effort
    to develop data standards, reusable models, and new techniques and algorithms.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: RT-X 项目展示了当机器人学习社区共同行动时可能实现的成果。由于这种跨机构的努力，我们能够汇集多样化的机器人数据集，并进行全面的多机器人评估，这是任何单一机构都无法做到的。由于机器人学界不能依赖于从互联网上获取训练数据，我们需要自己创建这些数据。我们希望更多的研究人员能将他们的数据贡献给[RT-X
    数据库](https://robotics-transformer-x.github.io/)并加入这一合作努力。我们也希望提供工具、模型和基础设施来支持跨载体研究。我们计划超越跨实验室共享数据，希望
    RT-X 能发展成一个协作的努力，以开发数据标准、可重用模型以及新的技术和算法。
- en: Our early results hint at how large cross-embodiment robotics models could transform
    the field. Much as large language models have mastered a wide range of language-based
    tasks, in the future we might use the same foundation model as the basis for many
    real-world robotic tasks. Perhaps new robotic skills could be enabled by fine-tuning
    or even prompting a pretrained foundation model. In a similar way to how you can
    prompt ChatGPT to tell a story without first training it on that particular story,
    you could ask a robot to write “Happy Birthday” on a cake without having to tell
    it how to use a piping bag or what handwritten text looks like. Of course, much
    more research is needed for these models to take on that kind of general capability,
    as our experiments have focused on single arms with two-finger grippers doing
    simple manipulation tasks.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的早期结果暗示了大型跨载体机器人模型如何改变该领域。就像大型语言模型掌握了各种基于语言的任务一样，在未来我们可能会使用同样的基础模型作为许多实际机器人任务的基础。也许新的机器人技能可以通过微调或甚至提示预训练的基础模型来实现。类似于您如何提示
    ChatGPT 来讲述一个故事，而无需首先对它进行特定故事的训练，您可以要求一个机器人在蛋糕上写“生日快乐”，而无需告诉它如何使用裱花袋或手写文本是什么样子。当然，这种通用能力的实现需要进行更多的研究，因为我们的实验主要集中在带有两指夹具的单臂进行简单操作任务上。
- en: As more labs engage in cross-embodiment research, we hope to further push the
    frontier on what is possible with a single neural network that can control many
    robots. These advances might include adding diverse simulated data from generated
    environments, handling robots with different numbers of arms or fingers, using
    different sensor suites (such as depth cameras and tactile sensing), and even
    combining manipulation and locomotion behaviors. RT-X has opened the door for
    such work, but the most exciting technical developments are still ahead.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 随着越来越多的实验室参与跨载体研究，我们希望进一步推动单个神经网络控制多个机器人的可能性的前沿。这些进展可能包括添加来自生成环境的多样化模拟数据，处理具有不同数量臂或手指的机器人，使用不同的传感器套件（例如深度摄像头和触觉传感），甚至结合操作和运动行为。RT-X
    打开了这样的工作大门，但最令人兴奋的技术发展仍在前方。
- en: 'This is just the beginning. We hope that with this first step, we can together
    create the future of robotics: where general robotic brains can power any robot,
    benefiting from data shared by all robots around the world.'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 这只是个开始。我们希望通过这一步，我们能共同创造机器人技术的未来：通用机器人大脑可以驱动任何机器人，并受益于全球所有机器人共享的数据。
- en: '*This article appears in the February 2024 print issue as “The Global Project
    to Make a General Robotic Brain.”*'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: '*本文于 2024 年 2 月印刷版上发表，标题为“创建通用机器人大脑的全球项目”。*'
- en: From Your Site Articles
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 来自您站点的文章
- en: Related Articles Around the Web
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 相关文章周围的网络
