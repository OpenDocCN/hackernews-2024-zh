- en: <!--yml
  id: totrans-split-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-split-1
  prefs: []
  type: TYPE_NORMAL
  zh: 'category: 未分类'
- en: 'date: 2024-05-27 14:49:27'
  id: totrans-split-2
  prefs: []
  type: TYPE_NORMAL
  zh: 'date: 2024-05-27 14:49:27'
- en: -->
  id: totrans-split-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: AI Poses Extinction-Level Risk, State-Funded Report Says | TIME
  id: totrans-split-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: AI构成灭绝级风险，国家资助的报告称 | TIME
- en: 来源：[https://time.com/6898967/ai-extinction-national-security-risks-report/](https://time.com/6898967/ai-extinction-national-security-risks-report/)
  id: totrans-split-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://time.com/6898967/ai-extinction-national-security-risks-report/](https://time.com/6898967/ai-extinction-national-security-risks-report/)
- en: The U.S. government must move “quickly and decisively” to avert substantial
    national security risks stemming from artificial intelligence (AI) which could,
    in the worst case, cause an “extinction-level threat to the human species,” says
    a [report](https://www.gladstone.ai/action-plan) commissioned by the U.S. government
    published on Monday.
  id: totrans-split-6
  prefs: []
  type: TYPE_NORMAL
  zh: 美国政府必须“迅速而果断地”采取行动，以避免由人工智能（AI）引起的重大国家安全风险，这些风险在最坏的情况下可能导致对人类物种的“灭绝级威胁”，一份由美国政府委托并于周一发布的[报告](https://www.gladstone.ai/action-plan)称。
- en: “Current frontier AI development poses urgent and growing risks to national
    security,” the report, which TIME obtained ahead of its publication, says. “The
    rise of advanced AI and AGI [artificial general intelligence] has the potential
    to destabilize global security in ways reminiscent of the introduction of nuclear
    weapons.” AGI is a hypothetical technology that could perform most tasks at or
    above the level of a human. Such systems do not currently exist, but the leading
    AI labs are working toward them and [many expect AGI to arrive](https://time.com/6556168/when-ai-outsmart-humans/)
    within the next five years or less.
  id: totrans-split-7
  prefs: []
  type: TYPE_NORMAL
  zh: “当前前沿人工智能发展对国家安全构成迫在眉睫且日益增长的风险，”《时代》提前获取的报告称。“先进人工智能和AGI（通用人工智能）的崛起有可能像核武器的引入一样动摇全球安全。”
    AGI是一种假设技术，可能在或超过人类水平上执行大多数任务。目前并不存在这样的系统，但领先的AI实验室正在努力朝这个方向发展，并且[许多人预计AGI将在](https://time.com/6556168/when-ai-outsmart-humans/)未来五年内或更短时间内到来。
- en: The three authors of the report worked on it for more than a year, speaking
    with more than 200 government employees, experts, and workers at frontier AI companies—like
    [OpenAI](https://time.com/6684266/openai-democracy-artificial-intelligence/),
    [Google DeepMind](https://time.com/6343450/gemini-google-deepmind-ai/), [Anthropic](https://time.com/collection/time100-ai/6309047/daniela-and-dario-amodei/)
    and [Meta](https://time.com/6694432/yann-lecun-meta-ai-interview/)— as part of
    their research. Accounts from some of those conversations [paint a disturbing
    picture](https://time.com/6898961/ai-labs-safety-concerns-report/), suggesting
    that many AI safety workers inside cutting-edge labs are concerned about perverse
    incentives driving decisionmaking by the executives who control their companies.
  id: totrans-split-8
  prefs: []
  type: TYPE_NORMAL
  zh: 报告的三位作者花了一年多的时间来完成它，与200多名政府员工、专家以及像[OpenAI](https://time.com/6684266/openai-democracy-artificial-intelligence/)、[Google
    DeepMind](https://time.com/6343450/gemini-google-deepmind-ai/)、[Anthropic](https://time.com/collection/time100-ai/6309047/daniela-and-dario-amodei/)和[Meta](https://time.com/6694432/yann-lecun-meta-ai-interview/)等前沿AI公司的工作人员进行了交谈，作为他们的研究的一部分。从这些对话中获得的账户[描绘了一个令人不安的画面](https://time.com/6898961/ai-labs-safety-concerns-report/)，表明许多AI安全工作者在前沿实验室内对驱动决策的执行高管存在担忧的逆向激励。
- en: '**Read More:** *[Employees at Top AI Labs Fear Safety Is an Afterthought, Report
    Says](https://time.com/6898961/ai-labs-safety-concerns-report)*'
  id: totrans-split-9
  prefs: []
  type: TYPE_NORMAL
  zh: '**阅读更多：** *[顶尖AI实验室的员工担心安全问题仅是后顾之忧，报告称](https://time.com/6898961/ai-labs-safety-concerns-report)*'
- en: The finished document, titled “An Action Plan to Increase the Safety and Security
    of Advanced AI,” recommends a set of sweeping and unprecedented policy actions
    that, if enacted, would radically disrupt the AI industry. Congress should make
    it illegal, the report recommends, to train AI models using more than a certain
    level of computing power. The threshold, the report recommends, should be set
    by a new federal AI agency, although the report suggests, as an example, that
    the agency could set it just above the levels of computing power used to train
    current cutting-edge models like OpenAI’s GPT-4 and Google’s Gemini. The new AI
    agency should require AI companies on the “frontier” of the industry to obtain
    government permission to train and deploy new models above a certain lower threshold,
    the report adds. Authorities should also “urgently” consider outlawing the publication
    of the “weights,” or inner workings, of powerful AI models, for example under
    open-source licenses, with violations possibly punishable by jail time, the report
    says. And the government should further tighten controls on the manufacture and
    export of AI chips, and channel federal funding toward “alignment” research that
    seeks to make advanced AI safer, it recommends.
  id: totrans-split-10
  prefs: []
  type: TYPE_NORMAL
  zh: 该完成的文档，题为“增加先进人工智能安全性和安全性的行动计划”，建议一系列深远且前所未有的政策行动，如果实施，将彻底颠覆人工智能行业。报告建议，国会应该立法，禁止使用超过一定计算能力水平训练人工智能模型。报告建议，这一门槛应该由一个新的联邦人工智能机构设定，尽管报告举例说，该机构可以将其设定在略高于目前诸如OpenAI的GPT-4和Google的Gemini等最新技术模型所使用的计算能力水平之上。报告补充道，新的人工智能机构应要求处于行业“前沿”的人工智能公司获取政府许可，以训练和部署高于某一较低门槛的新模型。当局还应“紧急”考虑禁止出版强大人工智能模型的“权重”或内部机制，例如在开源许可下，违反者可能面临监禁的惩罚，报告称。并且建议政府进一步加强对人工智能芯片的制造和出口的控制，并将联邦资金引导到“对齐”研究中，这些研究旨在使先进人工智能更加安全。
- en: The report was commissioned by the State Department in November 2022 as part
    of a federal contract worth $250,000, according to public records. It was written
    by Gladstone AI, a four-person company that runs technical briefings on AI for
    government employees. (Parts of the action plan recommend that the government
    invests heavily in educating officials on the technical underpinnings of AI systems
    so they can better understand their risks.) The report was delivered as a 247-page
    document to the State Department on Feb. 26\. The State Department did not respond
    to several requests for comment on the report. The recommendations “do not reflect
    the views of the United States Department of State or the United States Government,”
    the first page of the report says.
  id: totrans-split-11
  prefs: []
  type: TYPE_NORMAL
  zh: 据公开记录显示，这份报告是由国务院委托的，该合同于2022年11月签署，金额为25万美元。它由Gladstone AI编写，这是一家专门为政府雇员提供人工智能技术简报的四人公司。（行动计划的部分建议政府大力投资于教育官员关于人工智能系统技术基础的知识，以便他们更好地理解其风险。）该报告作为一份247页的文件于2月26日提交给国务院。国务院未对有关报告的多次请求发表评论。报告的第一页写道，“这些建议并不代表美国国务院或美国政府的观点”。
- en: The report's recommendations, many of them previously unthinkable, follow a
    dizzying series of major developments in AI that have caused many observers to
    recalibrate their stance on the technology. The chatbot ChatGPT, released in November
    2022, was the first time this pace of change became visible to society at large,
    leading many people to question whether future AIs might pose existential risks
    to humanity. New tools, with more capabilities, have continued to be released
    at a rapid clip since. As governments around the world discuss how best to regulate
    AI, the world’s biggest tech companies have fast been building out the infrastructure
    to train the next generation of more powerful systems—in some cases planning to
    use 10 or 100 times more computing power. Meanwhile, more than 80% of the American
    public believe AI could accidentally cause a catastrophic event, and 77% of voters
    believe the government should be doing more to regulate AI, according to recent
    [polling](https://theaipi.org/) by the AI Policy Institute.
  id: totrans-split-12
  prefs: []
  type: TYPE_NORMAL
  zh: 报告的建议，其中许多以前被认为不可思议，是在 AI 领域一系列重大发展之后提出的，这些发展使许多观察者重新评估他们对技术的立场。2022 年 11 月发布的聊天机器人
    ChatGPT 是这种变化步伐首次显现于社会大众中，导致许多人质疑未来的 AI 是否可能对人类构成存在风险。自那时以来，具备更多功能的新工具继续以快速的节奏发布。随着全球各国政府讨论如何最好地监管
    AI，全球最大的科技公司已经快速建设基础设施，用于训练下一代更强大的系统——在某些情况下，计划使用 10 到 100 倍更多的计算能力。与此同时，超过 80%
    的美国公众认为 AI 可能会意外引发灾难性事件，77% 的选民认为政府应该更多地进行 AI 监管，根据 AI 政策研究所最近的[民意调查](https://theaipi.org/)。
- en: '**Read More:** *[Researchers Develop New Technique to Wipe Dangerous Knowledge
    From AI Systems](https://time.com/6878893/ai-artificial-intelligence-dangerous-knowledge/)*'
  id: totrans-split-13
  prefs: []
  type: TYPE_NORMAL
  zh: '**阅读更多：** *[研究人员开发新技术，从 AI 系统中清除危险知识](https://time.com/6878893/ai-artificial-intelligence-dangerous-knowledge/)*'
- en: Outlawing the training of advanced AI systems above a certain threshold, the
    report states, may “moderate race dynamics between all AI developers” and contribute
    to a reduction in the speed of the chip industry manufacturing faster hardware.
    Over time, a federal AI agency could raise the threshold and allow the training
    of more advanced AI systems once evidence of the safety of cutting-edge models
    is sufficiently proven, the report proposes. Equally, it says, the government
    could lowerthe safety threshold if dangerous capabilities are discovered in existing
    models.
  id: totrans-split-14
  prefs: []
  type: TYPE_NORMAL
  zh: 禁止在某一特定门槛以上训练先进 AI 系统，报告指出，可能会“调节所有 AI 开发者之间的竞争动态”，并有助于减缓芯片行业制造更快硬件的速度。随着时间推移，联邦
    AI 机构可以提高门槛，一旦前沿模型的安全性证据得到充分证明，允许训练更先进的 AI 系统，报告建议。同样，如果发现现有模型具有危险能力，政府可以降低安全门槛。
- en: The proposal is likely to face political difficulties. “I think that this recommendation
    is extremely unlikely to be adopted by the United States government” says Greg
    Allen, director of the Wadhwani Center for AI and Advanced Technologies at the
    Center for Strategic and International Studies (CSIS), in response to a summary
    TIME provided of the report’s recommendation to outlaw AI training runs above
    a certain threshold. Current U.S. government AI policy, he notes, is to set compute
    thresholds above which additional transparency monitoring and regulatory requirements
    apply, but not to set limits above which training runs would be illegal. “Absent
    some kind of exogenous shock, I think they are quite unlikely to change that approach,”
    Allen says.
  id: totrans-split-15
  prefs: []
  type: TYPE_NORMAL
  zh: 这一提议可能面临政治上的困难。“我认为美国政府极不可能采纳这一建议”，沃德瓦尼人工智能与先进技术中心（CSIS）战略与国际研究中心的沃德瓦尼人工智能与先进技术中心（CSIS）主任格雷格·艾伦在回应
    TIME 提供的关于禁止某一门槛以上 AI 训练的报告建议摘要时表示。他指出，目前美国政府的 AI 政策是设定计算门槛，超过该门槛将需要额外的透明监控和监管要求，但不会设定训练超过某一门槛以上的法律限制。“除非发生某种外部冲击，否则我认为他们改变这种方法的可能性很小，”艾伦说。
- en: '* * *'
  id: totrans-split-16
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: '**Jeremie and Edouard Harris**, the CEO and CTO of Gladstone respectively,
    have been briefing the U.S. government on the risks of AI since 2021\. The duo,
    who are brothers, say that government officials who attended many of their earliest
    briefings agreed that the risks of AI were significant, but told them the responsibility
    for dealing with them fell to different teams or departments. In late 2021, the
    Harrises say Gladstone finally found an arm of the government with the responsibility
    to address AI risks: the State Department’s Bureau of International Security and
    Nonproliferation. Teams within the Bureau have an inter-agency mandate to address
    risks from emerging technologies including chemical and biological weapons, and
    radiological and nuclear risks. Following briefings by Jeremie and Gladstone''s
    then-CEO Mark Beall, in October 2022 the Bureau put out a tender for report that
    could inform a decision whether to add AI to the list of other risks it monitors.
    (The State Department did not respond to a request for comment on the outcome
    of that decision.) The Gladstone team won that contract, and the report released
    Monday is the outcome.'
  id: totrans-split-17
  prefs: []
  type: TYPE_NORMAL
  zh: '**Jeremie 和 Edouard Harris**，分别是 Gladstone 的 CEO 和 CTO，自 2021 年以来一直在向美国政府介绍人工智能的风险。这对兄弟说，出席过他们最早的几次简报的政府官员都认为人工智能的风险很大，但告诉他们应对这些风险的责任落在不同的团队或部门上。到了
    2021 年底，Harris 兄弟称 Gladstone 终于找到了一个负责处理人工智能风险的政府部门：国务院国际安全与防扩散局。该局内部的团队有跨机构的授权来应对包括化学和生物武器、放射性和核风险在内的新兴技术风险。在
    Jeremie 和 Gladstone 前 CEO Mark Beall 的简报后，2022 年 10 月，该局发布了一份招标文件，以便决定是否将人工智能加入其监控的其他风险列表中。（国务院未对决策结果做出回应。）Gladstone
    团队赢得了该合同，周一发布了报告作为结果。'
- en: 'The report focuses on two separate categories of risk. Describing the first
    category, which it calls “weaponization risk,” the report states: “such systems
    could potentially be used to design and even execute catastrophic biological,
    chemical, or cyber attacks, or enable unprecedented weaponized applications in
    swarm robotics.” The second category is what the report calls the “loss of control”
    risk, or the possibility that advanced AI systems may outmaneuver their creators.
    There is, the report says, “reason to believe that they may be uncontrollable
    if they are developed using current techniques, and could behave adversarially
    to human beings by default.”'
  id: totrans-split-18
  prefs: []
  type: TYPE_NORMAL
  zh: 报告集中讨论了两种不同的风险类别。描述第一类别，即所谓的“武器化风险”，报告称：“这些系统可能被用于设计甚至执行灾难性的生物、化学或网络攻击，或在群体机器人技术中实现前所未有的武器化应用。”第二类别是报告称为“失控风险”，或者说先进人工智能系统可能超越其创造者的可能性。报告称，有理由相信，如果使用当前技术开发，这些系统可能是不可控的，并且可能默认对人类行为具有敌意。
- en: Both categories of risk, the report says, are exacerbated by “race dynamics”
    in the AI industry. The likelihood that the first company to achieve AGI will
    reap the majority of economic rewards, the report says, incentivizes companies
    to prioritize speed over safety. “Frontier AI labs face an intense and immediate
    incentive to scale their AI systems as fast as they can,” the report says. “They
    do not face an immediate incentive to invest in safety or security measures that
    do not deliver direct economic benefits, even though some do out of genuine concern.”
  id: totrans-split-19
  prefs: []
  type: TYPE_NORMAL
  zh: 报告指出，这两类风险都受人工智能行业的“竞争动态”加剧。报告称，第一家实现通用人工智能（AGI）的公司将获得大部分经济奖励的可能性，激励公司优先速度而非安全。报告称：“前沿人工智能实验室面临强烈和即时的动机，尽可能快速扩展其人工智能系统。”报告称：“尽管一些公司出于真正的关切投资于安全或安全措施，但这些公司并没有立即投资于不提供直接经济利益的安全或安全措施。”
- en: The Gladstone report identifies hardware—specifically the high-end computer
    chips currently used to train AI systems—as a significant bottleneck to increases
    in AI capabilities. Regulating the proliferation of this hardware, the report
    argues, may be the “most important requirement to safeguard long-term global safety
    and security from AI.” It says the government should explore tying chip export
    licenses to the presence of on-chip technologies allowing monitoring of whether
    chips are being used in large AI training runs, as a way of enforcing proposed
    rules against training AI systems larger than GPT-4\. However the report also
    notes that any interventions will need to account for the possibility that overregulation
    could bolster foreign chip industries, eroding the U.S.’s ability to influence
    the supply chain.
  id: totrans-split-20
  prefs: []
  type: TYPE_NORMAL
  zh: Gladstone报告确定硬件——特别是目前用于训练AI系统的高端计算机芯片——作为增加AI能力的重要瓶颈。报告认为，监管这些硬件的传播可能是“保障长期全球安全和安全免受AI侵害的最重要要求”。它建议政府探索将芯片出口许可证与芯片上的监控技术存在关联，以此来执行建议的规则，禁止训练超过GPT-4规模的AI系统。然而，报告还指出，任何干预措施都需要考虑到过度管制可能会增强外国芯片行业，从而削弱美国影响供应链的能力。
- en: '**Read More:** *[What to Know About the U.S. Curbs on AI Chip Exports to China](https://time.com/6324619/us-biden-ai-chips-china/)*'
  id: totrans-split-21
  prefs: []
  type: TYPE_NORMAL
  zh: '**阅读更多：** [*了解美国对中国AI芯片出口的限制*](https://time.com/6324619/us-biden-ai-chips-china/)'
- en: The report also raises the possibility that, ultimately, the physical bounds
    of the universe may not be on the side of those attempting to prevent proliferation
    of advanced AI through chips. “As AI algorithms continue to improve, more AI capabilities
    become available for less total compute. Depending on how far this trend progresses,
    it could ultimately become impractical to mitigate advanced AI proliferation through
    compute concentrations at all.” To account for this possibility, the report says
    a new federal AI agency could explore blocking the publication of research that
    improves algorithmic efficiency, though it concedes this may harm the U.S. AI
    industry and ultimately be unfeasible.
  id: totrans-split-22
  prefs: []
  type: TYPE_NORMAL
  zh: 报告还提出了一个可能性，即从根本上说，宇宙的物理界限可能不利于那些试图通过芯片阻止先进AI扩散的人。“随着AI算法的持续改进，更多的AI能力以更少的总计算量变得可用。根据这一趋势的进展程度，通过计算集中完全防止先进AI扩散可能最终变得不切实际。”为了考虑到这一可能性，报告称，一个新的联邦AI机构可以探索阻止提高算法效率的研究的发布，尽管它承认这可能会损害美国的AI产业，最终是不可行的。
- en: The Harrises recognize in conversation that their recommendations will strike
    many in the AI industry as overly zealous. The recommendation to outlaw the open-sourcing
    of advanced AI model weights, they expect, will not be popular. “Open source is
    generally a wonderful phenomenon and overall massively positive for the world,”
    says Edouard, the chief technology officer of Gladstone. “It’s an extremely challenging
    recommendation to make, and we spent a lot of time looking for ways around suggesting
    measures like this.” Allen, the AI policy expert at CSIS, says he is sympathetic
    to the idea that open-source AI makes it more difficult for policymakers to get
    a handle on the risks. But he says any proposal to outlaw the open-sourcing of
    models above a certain size would need to contend with the fact that U.S. law
    has a limited reach. “Would that just mean that the open source community would
    move to Europe?” he says. “Given that it's a big world, you sort of have to take
    that into account.”
  id: totrans-split-23
  prefs: []
  type: TYPE_NORMAL
  zh: 哈里斯夫妇在谈话中意识到，他们的建议会让许多人在人工智能行业中感到过于热情。他们预计，禁止公开发布先进人工智能模型权重的建议不会受到欢迎。“开源通常是一种美妙的现象，对世界整体都是极其积极的，”Gladstone的首席技术官埃杜尔说。“这是一个极具挑战性的建议，我们花了很多时间寻找绕过此类措施的方法。”CSIS的AI政策专家艾伦说，他对开源AI使政策制定者难以掌控风险的想法表示同情。但他说，任何建议禁止公开发布超过一定规模的模型都必须考虑到美国法律的有限适用性。“这是否意味着开源社区会转移到欧洲？”他说。“考虑到世界如此之大，你必须考虑到这一点。”
- en: '**Read More:** [*The 3 Most Important AI Policy Milestones of 2023*](https://time.com/6513046/ai-policy-developments-2023/)'
  id: totrans-split-24
  prefs: []
  type: TYPE_NORMAL
  zh: '**阅读更多：** [*2023年最重要的三个AI政策里程碑*](https://time.com/6513046/ai-policy-developments-2023/)'
- en: Despite the challenges, the report’s authors say they were swayed by how [easy
    and cheap](https://arxiv.org/abs/2310.20624) it currently is for users to remove
    safety guardrails on an AI model if they have access to its weights. “If you proliferate
    an open source model, even if it looks safe, it could still be dangerous down
    the road,” Edouard says, adding that the decision to open-source a model is irreversible.
    “At that point, good luck, all you can do is just take the damage.”
  id: totrans-split-25
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管存在挑战，报告的作者们表示，他们被当前用户如何[轻松和廉价地](https://arxiv.org/abs/2310.20624)去除AI模型的安全防护栏所动摇。埃杜尔说：“如果你广泛推广一个开源模型，即使看起来很安全，将来仍可能会有危险。”他补充说，开源模型的决定是不可逆转的。“那时候，祝你好运，你唯一能做的就是承受损失。”
- en: The third co-author of the report, former Defense Department official Beall,
    has since left Gladstone in order to start a super PAC aimed at advocating for
    AI policy. The PAC, called Americans for AI Safety, officially launched on Monday.
    It aims to make AI safety and security "a key issue in the 2024 elections, with
    a goal of passing AI safety legislation by the end of 2024," the group said in
    a statement to TIME. The PAC did not disclose its funding commitments, but said
    it has "set a goal of raising millions of dollars to accomplish its mission."
  id: totrans-split-26
  prefs: []
  type: TYPE_NORMAL
  zh: 报告的第三位共同作者、前国防部官员比尔已经离开格拉德斯通，开始创办一个旨在倡导人工智能政策的超级政治行动委员会。该政治行动委员会名为“AI安全美国人”，于周一正式启动。该组织在向《TIME》杂志发表的声明中表示，旨在使AI安全和安全性成为“2024年选举的关键问题，并计划在2024年底通过AI安全立法”。政治行动委员会没有透露其资金承诺，但表示已经设定了“筹集数百万美元以完成其使命”的目标。
- en: 'Before co-founding Gladstone with Beall, the Harris brothers ran an AI company
    that went through YCombinator, the famed Silicon Valley incubator, at the time
    when [OpenAI CEO Sam Altman](https://time.com/6342827/ceo-of-the-year-2023-sam-altman/)
    was at the helm. The pair brandish these credentials as evidence they have the
    industry’s interests at heart, even as their recommendations, if implemented,
    would upend it. “Move fast and break things, we love that philosophy, we grew
    up with that philosophy,” Jeremie tells TIME. But the credo, he says, ceases to
    apply when the potential downside of your actions is so massive. “Our default
    trajectory right now,” he says, “seems very much on course to create systems that
    are powerful enough that they either can be weaponized catastrophically, or fail
    to be controlled.” He adds: “One of the worst-case scenarios is you get a catastrophic
    event that completely shuts down AI research for everybody, and we don''t get
    to reap the incredible benefits of this technology.”'
  id: totrans-split-27
  prefs: []
  type: TYPE_NORMAL
  zh: 在与比尔共同创办格拉德斯通之前，哈里斯兄弟经营过一家AI公司，并通过了著名的硅谷孵化器YCombinator，当时[OpenAI CEO Sam Altman](https://time.com/6342827/ceo-of-the-year-2023-sam-altman/)领导着该公司。他们以此作为证据，表明他们关心行业的利益，即使他们的建议如果得到实施将颠覆它。“快速行动并破坏事物，我们喜欢这种哲学，我们成长于这种哲学中，”杰里米告诉《TIME》。但他说，当你的行动潜在风险如此巨大时，这个信条就不再适用。“我们当前的默认轨迹，”他说，“看起来非常有可能创造出强大到可以灾难性武器化或无法控制的系统。”他补充道：“最糟糕的情况之一是发生灾难性事件，彻底关闭了所有人的AI研究，我们将无法享受到这项技术带来的惊人好处。”
- en: More from TIME
  id: totrans-split-28
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 更多来自《TIME》
- en: '***Are you an employee at an AI lab and have concerns that you might consider
    sharing with a journalist? You can contact the author of this piece on Signal
    at billyperrigo.01***'
  id: totrans-split-29
  prefs: []
  type: TYPE_NORMAL
  zh: '***你是一个AI实验室的员工，是否担心可能会与记者分享？你可以通过Signal联系这篇文章的作者，账号为billyperrigo.01***'
