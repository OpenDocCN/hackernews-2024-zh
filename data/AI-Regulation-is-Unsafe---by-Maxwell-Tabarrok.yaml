- en: <!--yml
  id: totrans-split-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-split-1
  prefs: []
  type: TYPE_NORMAL
  zh: 类别：未分类
- en: 'date: 2024-05-27 13:25:11'
  id: totrans-split-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024年05月27日 13:25:11
- en: -->
  id: totrans-split-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: AI Regulation is Unsafe - by Maxwell Tabarrok
  id: totrans-split-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: AI监管不安全 - 马克西姆·塔巴洛克
- en: 来源：[https://www.maximum-progress.com/p/ai-regulation-is-unsafe](https://www.maximum-progress.com/p/ai-regulation-is-unsafe)
  id: totrans-split-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://www.maximum-progress.com/p/ai-regulation-is-unsafe](https://www.maximum-progress.com/p/ai-regulation-is-unsafe)
- en: Concerns over AI safety and calls for government control over the technology
    are highly correlated but they should not be.
  id: totrans-split-6
  prefs: []
  type: TYPE_NORMAL
  zh: AI安全问题和呼吁政府对技术进行管控之间存在高度相关性，但它们本不应如此。
- en: 'There are two major forms of AI risk: misuse and misalignment. Misuse risks
    come from humans using AIs as tools in dangerous ways. Misalignment risks arise
    if AIs take their own actions at the expense of human interests.'
  id: totrans-split-7
  prefs: []
  type: TYPE_NORMAL
  zh: AI风险主要有两种形式：滥用和错位。滥用风险源于人们以危险的方式使用AI作为工具。错位风险是指如果AI以损害人类利益为代价采取自己的行动。
- en: Governments are poor stewards for both types of risk. Misuse regulation is like
    the regulation of any other technology. There are reasonable rules that the government
    might set, but [omission bias](https://en.wikipedia.org/wiki/Omission_bias) and
    incentives to protect small but well organized groups at the expense of everyone
    else will lead to lots of costly ones too. Misalignment regulation is not in the
    Overton window for any government. Governments do not have strong incentives to
    care about long term, global, costs or benefits and they *do* have strong incentives
    to push the development of AI forwards for their own purposes.
  id: totrans-split-8
  prefs: []
  type: TYPE_NORMAL
  zh: 政府不适合管理任何一种风险。滥用监管就像监管其他任何技术一样。政府可能制定合理规则，但[遗漏偏倚](https://en.wikipedia.org/wiki/Omission_bias)和以保护小而组织有序的群体为代价而产生大量代价高昂的规则也会发生。错位规管在任何政府的Overton窗口之外。政府没有强烈的动机去关心长期、全球的成本或收益，而他们*确实*有强烈的动机推动AI的发展以符合他们自己的目的。
- en: Noticing that AI companies put the world at risk is not enough to support greater
    government involvement in the technology. Government involvement is likely to
    exacerbate the most dangerous parts of AI while limiting the upside.
  id: totrans-split-9
  prefs: []
  type: TYPE_NORMAL
  zh: 注意到AI公司将世界置于风险之中并不足以支持政府更多地介入这一技术。政府介入可能会加剧AI最危险的部分，同时限制上行空间。
- en: Governments are not social welfare maximizers. Government actions are an amalgam
    of the actions of thousands of personal welfare maximizers who are loosely aligned
    and constrained. In general, governments have strong incentives for [myopia](https://www.maximum-progress.com/p/spatial-vs-temporal-externalities),
    violent competition with other governments, and negative sum transfers to small,
    well organized groups. These exacerbate existential risk and limit potential upside.
  id: totrans-split-10
  prefs: []
  type: TYPE_NORMAL
  zh: 政府并非社会福利最大化者。政府的行动是成千上万个个人福利最大化者行动的综合，这些人宽松地对齐并受到限制。总的来说，政府对[近视](https://www.maximum-progress.com/p/spatial-vs-temporal-externalities)、与其他政府的激烈竞争，以及对小而组织有序的群体的负和转移等方面都有强烈的动机。这加剧了存在风险并限制了潜在的上行空间。
- en: The vast majority of the costs of existential risk occur outside of the borders
    of any single government and beyond the election cycle for any current decision
    maker, so we should expect governments to ignore them.
  id: totrans-split-11
  prefs: []
  type: TYPE_NORMAL
  zh: 存在风险的绝大部分成本发生在任何单一政府的边界之外，并且超出了任何现任决策者的任期，因此我们应该预计政府会忽视它们。
- en: We see this expectation fulfilled in governments reactions to other long term
    or global externalities e.g debt and climate change. Governments around the world
    are happy to impose trillions of dollars in [direct cost and substantial default
    risk](https://en.wikipedia.org//wiki/National_debt_of_the_United_States#Risks_and_debates)
    on future generations because costs and benefits on these future generations hold
    little sway in the next election. Similarly, governments [spend billions subsidizing
    fossil fuel](https://ourworldindata.org/grapher/fossil-fuel-subsidies?country=Northern+America+%28UN%29~CHN~Europe+%28UN%29)
    production and ignore potential solutions to global warming, like a carbon tax
    or geoengineering, because the [long term or extraterritorial costs and benefits
    of climate change do not enter their optimization function](https://www.maximum-progress.com/p/spatial-vs-temporal-externalities).
  id: totrans-split-12
  prefs: []
  type: TYPE_NORMAL
  zh: 我们看到政府对其他长期或全球外部性问题（如债务和气候变化）的反应符合这种预期。世界各国政府乐于对未来几代人造成数万亿美元的[直接成本和重大违约风险](https://en.wikipedia.org//wiki/National_debt_of_the_United_States#Risks_and_debates)，因为这些未来代人的成本和利益在下次选举中影响甚微。同样，政府[花费数十亿资助化石燃料](https://ourworldindata.org/grapher/fossil-fuel-subsidies?country=Northern+America+%28UN%29~CHN~Europe+%28UN%29)生产，并忽视全球变暖的潜在解决方案，如碳税或地球工程，因为[气候变化的长期或领域外成本和利益未进入其优化函数](https://www.maximum-progress.com/p/spatial-vs-temporal-externalities)。
- en: AI risk is no different. Governments will happily trade off global, long term
    risk for national, short term benefits. The most salient way they will do this
    is through military competition. Government regulations on private AI development
    will not stop them from racing to integrate AI into their militaries. [Autonomous
    drone warfare](https://www.forbes.com/sites/davidhambling/2023/10/17/ukraines-ai-drones-seek-and-attack-russian-forces-without-human-oversight/?sh=70fde60b66da)
    is already happening in Ukraine and Israel. The US military has contracts with
    [Palantir](https://en.wikipedia.org/wiki/Palantir_Technologies#Products) and [Andruil](https://en.wikipedia.org/wiki/Anduril_Industries#Products)
    which use AI to augment military strategy or to power weapons systems. Governments
    will want to use AI for [predictive policing, propaganda](https://www.rfa.org/english/news/china/surveillance-06052023142155.html),
    and other forms of population control.
  id: totrans-split-13
  prefs: []
  type: TYPE_NORMAL
  zh: AI风险并无二致。政府乐意以国家的短期利益来换取全球的长期风险。它们最显著的做法是通过军事竞争。政府对私人AI开发的管制不会阻止它们竞相将AI整合到军事中。[自主无人机战争](https://www.forbes.com/sites/davidhambling/2023/10/17/ukraines-ai-drones-seek-and-attack-russian-forces-without-human-oversight/?sh=70fde60b66da)已在乌克兰和以色列发生。美国军方与[Palantir](https://en.wikipedia.org/wiki/Palantir_Technologies#Products)和[Andruil](https://en.wikipedia.org/wiki/Anduril_Industries#Products)签有合同，利用AI增强军事战略或驱动武器系统。政府将希望利用AI进行[预测性警务、宣传](https://www.rfa.org/english/news/china/surveillance-06052023142155.html)和其他形式的人口控制。
- en: The case of nuclear tech is informative. This technology was strictly regulated
    by governments, but they still raced with each other and used the technology to
    create the most [existentially risky weapons](https://en.wikipedia.org/wiki/Thermonuclear_weapon)
    mankind has ever seen. Simultaneously, they cracked down on civilian use. Now,
    we’re in a world where all the major geopolitical flashpoints have at least one
    side armed with nuclear weapons and where the nuclear power industry is worse
    than stagnant.
  id: totrans-split-14
  prefs: []
  type: TYPE_NORMAL
  zh: 核技术的案例具有启发性。尽管政府严格管制了这项技术，但它们仍互相竞逐并利用技术制造人类有史以来最[存在危险的武器](https://en.wikipedia.org/wiki/Thermonuclear_weapon)。同时，它们打压了民用应用。现在，我们生活在至少一方拥有核武器的主要地缘政治热点且核能产业比停滞还糟的世界中。
- en: Government’s military ambitions mean that their regulation will preserve the
    most dangerous misuse risks from AI. They will also push the AI frontier and train
    larger models, so we will still face misalignment risks. These may be exacerbated
    if governments are less interested or skilled in AI safety techniques. Government
    control over AI development is likely to slow down AI progress overall. Accepting
    the premise that this is good is not sufficient to invite regulation, though,
    because government control will cause a *relative* speed up of the most dystopian
    uses for AI.
  id: totrans-split-15
  prefs: []
  type: TYPE_NORMAL
  zh: 政府的军事野心意味着它们的管制将保留AI最危险的误用风险。它们还将推动AI前沿，并训练更大的模型，因此我们仍将面临错位风险。如果政府在AI安全技术上不那么感兴趣或熟练，这些风险可能会加剧。政府对AI开发的控制可能会总体上减缓AI的进步。但接受这一前提并不足以邀请管制，因为政府的控制将导致最反乌托邦使用AI的*相对*加速。
- en: In short term, governments are primarily interested in protecting well-organized
    groups from the effects of AI. E.g [copyright holders](https://www.axios.com/2024/01/02/copyright-law-violation-artificial-intelligence-courts),
    [drivers unions](https://www2.gov.bc.ca/gov/content/transportation/driving-and-cycling/road-safety-rules-and-consequences/self-drive),
    and [other professional](https://www.bmj.com/company/newsroom/doctors-and-public-health-experts-join-calls-for-halt-to-ai-rd-until-its-regulated/)
    lobby groups. Here’s a summary of last years congressional AI hearing from Zvi
    Mowshowitz.
  id: totrans-split-16
  prefs: []
  type: TYPE_NORMAL
  zh: 短期内，政府主要关注保护那些组织良好的团体免受人工智能影响的影响。例如，[版权持有者](https://www.axios.com/2024/01/02/copyright-law-violation-artificial-intelligence-courts)，[驾驶员联盟](https://www2.gov.bc.ca/gov/content/transportation/driving-and-cycling/road-safety-rules-and-consequences/self-drive)，和[其他专业](https://www.bmj.com/company/newsroom/doctors-and-public-health-experts-join-calls-for-halt-to-ai-rd-until-its-regulated/)游说团体。以下是去年国会人工智能听证会的摘要，来自兹维·莫沃绍维茨。
- en: '[The Senators](https://www.lesswrong.com/posts/5nDxmAvZ9w5CPa9gR/ai-12-the-quest-for-sane-regulations#Deepfaketown_and_Botpocalypse_Soon)
    care deeply about the types of things politicians care deeply about. Klobuchar
    asked about securing royalties for local news media. Blackburn asked about securing
    royalties for Garth Brooks. Lots of concern about copyright violations, about
    using data to train without proper permission, especially in audio models. Graham
    focused on section 230 for some reason, despite numerous reminders it didn’t apply,
    and Howley talked about it a bit too.'
  id: totrans-split-17
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '[参议员们](https://www.lesswrong.com/posts/5nDxmAvZ9w5CPa9gR/ai-12-the-quest-for-sane-regulations#Deepfaketown_and_Botpocalypse_Soon)对政客们关心的类型非常关注。克洛布彻问到如何保护本地新闻媒体的版税。布莱克本问到如何保护加思·布鲁克斯的版税。对版权侵犯，尤其是在音频模型中未经适当许可使用数据进行训练的担忧很大。格雷厄姆专注于第230条，尽管多次提醒它不适用，豪利也稍作提及。'
- en: This kind of regulation has less risk than misaligned killbots, but it does
    limit the potential upside from the technology.
  id: totrans-split-18
  prefs: []
  type: TYPE_NORMAL
  zh: 这种类型的监管比不对齐的杀戮机器人风险小，但它确实限制了技术的潜在上行空间。
- en: Private incentives for AI development are far from perfect. There are still
    large externalities and competitive dynamics that may push progress too fast.
    But identifying this problem is not enough to justify government involvement.
    We need a reason to believe that governments can reliably improve the incentives
    facing private organizations. Government’s strong incentives for myopia, military
    competition, and rent-seeking make it difficult to find such a reason.
  id: totrans-split-19
  prefs: []
  type: TYPE_NORMAL
  zh: 私人发展AI的激励远非完美。仍然存在大量外部性和竞争动态，可能会推动进展过快。但是，仅仅识别这个问题并不足以证明政府介入的必要性。我们需要有理由相信，政府可以可靠地改善私人组织面临的激励。政府对短视、军事竞争和寻租的强大激励使得找到这样的理由变得困难。
- en: The default incentives of both governments and profit seeking companies are
    imperfect. But the whole point of AI safety advocacy is to change these incentives
    or to convince decision makers to act despite them, so you can buy that governments
    are imperfect and still support calls for AI regulation. The problem with this
    is that even extraordinarily successful advocacy in government can be redirected
    into opposite and catastrophic effects.
  id: totrans-split-20
  prefs: []
  type: TYPE_NORMAL
  zh: 政府和追求利润的公司的默认激励是不完善的。但人工智能安全倡导的整点就在于改变这些激励，或者说说服决策者在这些激励下行动，所以你可以认为政府是不完善的，但仍然支持呼吁AI监管。这样做的问题在于，即使在政府中非常成功的倡导也可能被转向相反而灾难性的影响。
- en: 'Consider [Sam Altman’s testimony in congress last May](https://www.youtube.com/watch?v=fP5YdyjTfG0).
    No one was convinced of anything except the power of AI fear for their own pet
    projects. Here is a characteristic quote:'
  id: totrans-split-21
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑一下[山姆·阿尔特曼在去年五月的国会证词](https://www.youtube.com/watch?v=fP5YdyjTfG0)。没有人被任何事情说服，只有对AI恐惧在他们自己的宠物项目中的力量。这里是一个典型的引用：
- en: 'Senator Blumenthal addressing Sam Altman: I think you have said, in fact, and
    I’m gonna quote, ‘Development of superhuman machine intelligence is probably the
    greatest threat to the continued existence of humanity.’ You may have had in mind
    the effect on jobs. Which is really my biggest nightmare in the long term.'
  id: totrans-split-22
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 参议员布卢门瑟尔对山姆·阿尔特曼说：我认为你确实说过，我要引用一下，“超人类机器智能的发展可能是持续存在的最大威胁。”你可能考虑到了对工作的影响。这确实是我长期以来最大的噩梦。
- en: A reasonable upper bound for the potential of AI safety lobbying is the environmental
    movement of the 1970s. It was extraordinarily effective. Their advocacy led to
    a series of laws, including the National Environmental Policy Act (NEPA) that
    are among the most comprehensive and powerful regulations ever passed. These laws
    are not clearly in service of some pre-existing government incentive. Indeed,
    they regulate the federal government more strictly than anything else and [often
    got it in its way](https://en.wikipedia.org/wiki/Tennessee_Valley_Authority_v._Hill#Majority_opinion).
    The cultural and political advocacy of the environmental movement made a large
    counterfactual impact with laws that still have massive influence today.
  id: totrans-split-23
  prefs: []
  type: TYPE_NORMAL
  zh: AI安全游说的潜力的合理上限可以看作是20世纪70年代的环保运动。它是非常有效的。他们的倡导导致了一系列法律的出台，包括国家环境政策法（NEPA），这些法律是有史以来最全面和最有力的法规之一。这些法律并不明确地服务于某些现有的政府激励。事实上，它们比任何其他东西更严格地监管联邦政府，并经常[阻碍其行动](https://en.wikipedia.org/wiki/Tennessee_Valley_Authority_v._Hill#Majority_opinion)。环保运动的文化和政治倡导对法律产生了重大的反事实影响，这些法律至今仍具有巨大的影响力。
- en: This success has turned sour, though, because the massive influence of these
    laws is now a massive barrier to decarbonization. NEPA has exemptions for oil
    and gas but not for solar or windfarms. Exemptions for highways but not highspeed
    rail. The costs of compliance with NEPA’s bureaucratic proceduralism hurts [Terraform
    Industries](https://terraformindustries.com/) a lot more than Shell. The standard
    government incentives for concentrating benefits to large legible groups and diffusing
    costs to large groups and the future redirected the political will and institutional
    power of the environmental movement into some of the most environmentally damaging
    and economically costly laws ever.
  id: totrans-split-24
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管取得了这样的成功，但这已变得苦涩，因为这些法律的巨大影响现在成为减少碳排放的巨大障碍。NEPA对石油和天然气有豁免条款，但对太阳能或风力发电场没有。对高速公路有豁免条款，但对高速铁路没有。遵循NEPA官僚程序主义的成本使[Terraform
    Industries](https://terraformindustries.com/)受到的损害比Shell还大。标准的政府激励措施使利益集中于大型可辨识群体，将成本扩散到更大的群体和未来，政治意愿和环保运动的机构力量被重定向到一些环境破坏最严重、经济成本最高的法律之中。
- en: AI safety advocates should not expect to do much better than this, especially
    since many of their proposals are specifically based on [permitting AI models
    like NEPA permits construction projects](https://twitter.com/AdamThierer/status/1772987264290709987).
  id: totrans-split-25
  prefs: []
  type: TYPE_NORMAL
  zh: AI安全倡导者不应期望做得比这更好，特别是因为他们的许多建议是基于[像NEPA许可建设项目一样允许AI模型](https://twitter.com/AdamThierer/status/1772987264290709987)。
- en: Belief in the potential for existential risk from AI does not imply that governments
    should have greater influence over its development. Government’s incentives make
    them misaligned with the goal of reducing existential risk. They are not rewarded
    or punished for costs or benefits outside of their borders or term limits and
    this is where nearly all of the importance of existential risk lies. Governments
    *are* rewarded for rapid development of military technology that empowers them
    over their rivals. They are also rewarded for providing immediate benefits to
    well-organized, legible groups, even when these rewards come at great expense
    to larger or more remote groups of people. These incentives exacerbate the worst
    misuse and misalignment risks of AI and limit the potential economic upside.
  id: totrans-split-26
  prefs: []
  type: TYPE_NORMAL
  zh: 相信AI可能存在的灭绝风险并不意味着政府应该对其发展具有更大的影响力。政府的激励使其与减少灭绝风险的目标不一致。他们不会因其边界外或任期限制之外的成本或收益而受到奖励或惩罚，而这几乎是灭绝风险重要性的全部所在。政府对于快速发展能增强他们在竞争对手身上的权力的军事技术感到满足。他们还会因为为组织有序、可辨识的群体提供即时好处而受到奖励，即使这些奖励会对更大或更遥远的人群造成巨大损失。这些激励加剧了AI的最严重误用和不一致风险，并限制了潜在的经济上行空间。
