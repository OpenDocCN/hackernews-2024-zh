- en: <!--yml
  id: totrans-split-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-split-1
  prefs: []
  type: TYPE_NORMAL
  zh: 'category: 未分类'
- en: 'date: 2024-05-29 13:28:11'
  id: totrans-split-2
  prefs: []
  type: TYPE_NORMAL
  zh: 'date: 2024-05-29 13:28:11'
- en: -->
  id: totrans-split-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: 'Modular: Announcing MAX Developer Edition Preview'
  id: totrans-split-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Modular：宣布 MAX 开发者版预览
- en: 来源：[https://www.modular.com/blog/announcing-max-developer-edition-preview](https://www.modular.com/blog/announcing-max-developer-edition-preview)
  id: totrans-split-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://www.modular.com/blog/announcing-max-developer-edition-preview](https://www.modular.com/blog/announcing-max-developer-edition-preview)
- en: 'Modular was founded on the [vision](https://www.modular.com/vision) to enable
    AI to be used by anyone, anywhere. We have always believed that to achieve this
    vision, we must first fix the fragmented and disjoint infrastructure upon which
    AI is built today. As we said [2 years ago](https://www.modular.com/blog/the-case-for-a-next-generation-ai-developer-platform),
    we imagine a different future for AI software, one that rings truer now than ever
    before:'
  id: totrans-split-6
  prefs: []
  type: TYPE_NORMAL
  zh: Modular 是基于[愿景](https://www.modular.com/vision)成立的，旨在使任何人任何地方都能使用AI。我们始终认为，要实现这一愿景，我们首先必须修复今天构建AI基础设施的分散和不连贯性。正如我们[两年前所说](https://www.modular.com/blog/the-case-for-a-next-generation-ai-developer-platform)，我们想象一个不同的AI软件未来，现在比以往任何时候更为真实：
- en: Our success means global developers will be empowered by access to AI software
    that is truly usable, portable, and scalable. A world where developers without
    unlimited budgets or access to top talent can be just as efficient as the world’s
    largest technology giants, where the efficiency and total cost of ownership of
    AI hardware are optimized, where organizations can easily plug in custom ASICs
    for their use cases, where deploying to the edge is as easy as deploying to a
    server, where organizations can use any AI framework that best suits their needs,
    and where AI programs seamlessly scale across hardware so that deploying the latest
    AI research into production “just works”.
  id: totrans-split-7
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 我们的成功意味着全球开发者将通过使用真正可用、便携和可扩展的AI软件而得到赋能。一个世界，其中没有无限预算或接触顶级人才的开发者可以和世界上最大的科技巨头一样高效；一个AI硬件效率和总体拥有成本被优化的世界，组织可以轻松地为他们的用例插入定制ASICs；一个将部署到边缘设备与部署到服务器一样简单的世界，组织可以使用最适合他们需求的任何AI框架，以及AI程序可以在各种硬件上无缝扩展，从而将最新的AI研究部署到生产环境中“毫无障碍”。
- en: '**And today, we’re so excited to take a huge step towards that brighter future
    by announcing that the Modular Accelerated Xecution (MAX) Platform is** [**now
    globally available**](https://www.modular.com/max), starting first as a preview
    on Linux systems. MAX is a unified set of tools and libraries that unlock performance,
    programmability and portability for your AI inference pipelines. Each component
    of MAX is designed to simplify the process of deploying models to any hardware,
    delivering the best possible cost-performance ratio for your workloads.'
  id: totrans-split-8
  prefs: []
  type: TYPE_NORMAL
  zh: '**今天，我们非常激动地迈向更加光明的未来，宣布 Modula Accelerated Xecution (MAX) 平台** [**现在全球可用**](https://www.modular.com/max)，首先在
    Linux 系统上提供预览。MAX 是一组统一的工具和库，解锁了您的AI推理管道的性能、可编程性和可移植性。MAX 的每个组件都旨在简化将模型部署到任何硬件的过程，为您的工作负载提供最佳的成本性能比。'
- en: '**What’s included with MAX?**'
  id: totrans-split-9
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**MAX 包含什么？**'
- en: '**MAX is engineered to cater to the demanding needs of AI deployment, giving
    you everything you need to deploy low-latency, high-throughput, real-time inference
    pipelines into production**. This first release of MAX ships with three key components:'
  id: totrans-split-10
  prefs: []
  type: TYPE_NORMAL
  zh: '**MAX 被设计用来满足AI部署的严苛需求，为您提供部署低延迟、高吞吐量、实时推理管道所需的一切**。MAX 的首个发布版本包含三个关键组件：'
- en: '[**MAX Engine**](https://www.modular.com/max/engine): A state-of-the-art AI
    compiler and runtime system supporting PyTorch, TensorFlow, and ONNX models like
    [Mistral](https://performance.modular.com/?task=language&model=mistral-7b-v0.1&instance=c7g.16xlarge&framework=stock-pytorch&view=chart),
    [Stable Diffusion](https://performance.modular.com/?task=computer_vision&model=stable-diffusion-v1.5-unet&instance=c5.9xlarge&framework=stock-pytorch&view=chart),
    [Llama2](https://performance.modular.com/?task=language&model=llama2-7B-MS-context-encoding&instance=c6a.16xlarge&framework=stock-pytorch&view=chart),
    [WavLM](https://performance.modular.com/?task=language&model=wavlm-large&instance=c6i.4xlarge&framework=stock-pytorch&view=chart),
    [DLMR](https://performance.modular.com/?task=recommenders&model=dlrm-rm2&instance=c5a.8xlarge&framework=stock-pytorch&view=chart),
    [ClipVit](https://performance.modular.com/?task=computer_vision&model=clip-vit-large-patch14&instance=c6i.4xlarge&framework=stock-pytorch&view=chart)
    and many more. It delivers unmatched inference speed across diverse hardware platforms,
    and we’re just getting started.'
  id: totrans-split-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[**MAX Engine**](https://www.modular.com/max/engine)：一款支持PyTorch、TensorFlow和ONNX模型的最先进AI编译器和运行时系统，如[Mistral](https://performance.modular.com/?task=language&model=mistral-7b-v0.1&instance=c7g.16xlarge&framework=stock-pytorch&view=chart)，[Stable
    Diffusion](https://performance.modular.com/?task=computer_vision&model=stable-diffusion-v1.5-unet&instance=c5.9xlarge&framework=stock-pytorch&view=chart)，[Llama2](https://performance.modular.com/?task=language&model=llama2-7B-MS-context-encoding&instance=c6a.16xlarge&framework=stock-pytorch&view=chart)，[WavLM](https://performance.modular.com/?task=language&model=wavlm-large&instance=c6i.4xlarge&framework=stock-pytorch&view=chart)，[DLMR](https://performance.modular.com/?task=recommenders&model=dlrm-rm2&instance=c5a.8xlarge&framework=stock-pytorch&view=chart)，[ClipVit](https://performance.modular.com/?task=computer_vision&model=clip-vit-large-patch14&instance=c6i.4xlarge&framework=stock-pytorch&view=chart)等，提供跨多种硬件平台的无与伦比的推理速度，我们仅仅是刚刚开始。'
- en: '[**MAX Serving**](https://www.modular.com/max/serving)**:** An efficient serving
    wrapper for the MAX Engine, ensuring seamless interoperability with current AI
    serving systems like NVIDIA Triton, and smooth deployment to container ecosystems
    such as Kubernetes.'
  id: totrans-split-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[**MAX Serving**](https://www.modular.com/max/serving)**：** MAX引擎的高效服务包装器，确保与当前的AI服务系统（如NVIDIA
    Triton）无缝互操作，并平稳部署到容器生态系统（如Kubernetes）。'
- en: ‍[**Mojo🔥**](https://www.modular.com/max/mojo)**:** The world’s first programming
    language built from the ground up for AI development, with cutting-edge compiler
    technology that delivers unparalleled performance and programmability on any hardware.
  id: totrans-split-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[**Mojo🔥**](https://www.modular.com/max/mojo)**：** 世界上第一个从头开始为AI开发构建的编程语言，具有前沿的编译器技术，在任何硬件上都能提供无与伦比的性能和可编程性。'
- en: MAX gives developers superpowers, providing them with an array of tools and
    libraries for building high-performance AI applications that can be efficiently
    deployed across multiple hardware platforms.
  id: totrans-split-14
  prefs: []
  type: TYPE_NORMAL
  zh: MAX 给开发者带来超凡的力量，为他们提供了一系列工具和库，用于构建高性能的AI应用程序，可以高效地部署在多个硬件平台上。
- en: '**How do I use MAX?**'
  id: totrans-split-15
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**如何使用MAX？**'
- en: MAX is built to meet AI developers where they are today. It provides an instant
    boost in performance for your current models and seamlessly integrates with your
    existing toolchain and serving infrastructure, capturing the immediate value and
    performance improvements of MAX with minimal code changes. Then, when you're ready,
    you can extend MAX to support other parts of your AI pipeline and achieve even
    more performance, programmability, and portability.
  id: totrans-split-16
  prefs: []
  type: TYPE_NORMAL
  zh: MAX 是为了满足当前AI开发者的需求而构建的。它为你现有的模型提供即时的性能提升，并与你现有的工具链和服务基础设施无缝集成，通过最小的代码更改捕获MAX的即时价值和性能改进。然后，当你准备好时，你可以扩展MAX以支持AI流水线的其他部分，实现更多的性能、可编程性和可移植性。
- en: '**Quick performance & portability wins**'
  id: totrans-split-17
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '**快速性能和可移植性优势**'
- en: '**MAX compiles and runs your existing AI models across a wide range of hardware,
    delivering immediate performance gains**. Getting started is as simple as using
    our Python or C API to replace your current PyTorch, TensorFlow, or ONNX inference
    calls with MAX Engine inference calls.'
  id: totrans-split-18
  prefs: []
  type: TYPE_NORMAL
  zh: '**MAX 编译并在各种硬件上运行您现有的AI模型，提供即时的性能增益**。使用我们的Python或C API来开始使用MAX引擎推理调用，替换您当前的PyTorch、TensorFlow或ONNX推理调用就这么简单。'
- en: By changing only a few lines of code, [your models execute up to 5x faster](https://performance.modular.com/?task=recommenders&model=dlrm-rm1-multihot&instance=c7g.4xlarge&framework=stock-pytorch&view=chart),
    supercharging latency so you can productionize larger models while improving efficiency
    to significantly reduce compute costs compared to stock PyTorch, TensorFlow, or
    ONNX Runtime. And the MAX Engine provides full portability across a wide range
    of CPU architectures (Intel, AMD, ARM), with GPU support coming soon. This means
    you can seamlessly move to the best hardware for your use case without rewriting
    your model.
  id: totrans-split-19
  prefs: []
  type: TYPE_NORMAL
  zh: 仅改变几行代码，[您的模型执行速度可提高到5倍](https://performance.modular.com/?task=recommenders&model=dlrm-rm1-multihot&instance=c7g.4xlarge&framework=stock-pytorch&view=chart)，可以极大地减少计算成本，从而使您能够生产更大的模型，同时提高效率，显著降低与标准PyTorch、TensorFlow或ONNX
    Runtime相比的计算成本。并且MAX引擎在广泛的CPU架构（英特尔、AMD、ARM）之间提供了全面的可移植性，GPU支持即将推出。这意味着您可以无缝地将您的模型移动到最适合您使用情况的硬件上，而无需重写你的模型。
- en: Additionally, you can wrap the MAX Engine with MAX Serving as a backend to NVIDIA
    Triton Inference Server for a production-grade deployment stack. Triton provides
    gRPC and HTTP endpoints, efficiently handling incoming input requests.
  id: totrans-split-20
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，你可以将 MAX 引擎与MAX Serving一起作为 NVIDIA Triton推理服务器的后端进行生产级部署。Triton提供gRPC和HTTP端点，高效处理传入的输入请求。
- en: '**Extend & optimize your pipeline**'
  id: totrans-split-21
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '**扩展和优化您的管道**'
- en: '**Beyond executing existing models with MAX Engine, you can further optimize
    your performance with MAX’s unique extensibility and programmability capabilities**.
    MAX Engine is built using Mojo, and it is also fully extensible by you in Mojo.
    Today, the MAX Graph API enables you to build whole inference models in Mojo,
    consolidating the complexity that popular “point solution” inference frameworks
    like llama.cpp provide - delivering better flexibility and better performance.
    [Coming soon](https://docs.modular.com/max/roadmap) you will even be able to write
    custom ops that can be natively fused by the MAX Engine compiler into your existing
    model graph.'
  id: totrans-split-22
  prefs: []
  type: TYPE_NORMAL
  zh: '**除了使用MAX引擎执行现有模型外，你还可以借助 MAX 独特的可扩展性和可编程能力进一步优化性能**。MAX引擎是使用Mojo构建的，并且在Mojo中完全可扩展。如今，MAX图形API使你能够在Mojo中构建整个推理模型，
    consagenbg流行的“点解决方案”推理框架（如llama.cpp）提供的复杂性 - 提供更好的灵活性和性能。 [即将推出](https://docs.modular.com/max/roadmap)，您甚至将能够编写自定义操作，MAX引擎编译器将这些操作原生融入到您现有的模型图中。'
- en: Beyond model optimization with MAX Engine, you can further accelerate the rest
    of your AI pipeline by migrating your data pre/post-processing code and application
    code to Mojo, using native Mojo with the MAX Engine Mojo API. Our north star is
    to provide you with a massive amount of leverage, so you can get innovations in
    AI into your products faster.
  id: totrans-split-23
  prefs: []
  type: TYPE_NORMAL
  zh: 除了使用MAX引擎进行模型优化外，你还可以通过将你的数据预处理/后处理代码和应用程序代码迁移到Mojo，并使用MAX引擎Mojo API的原生Mojo，进一步加速您的AI管道的其余部分。我们的北极星是为您提供大量的杠杆作用，让您能够更快地将AI创新引入您的产品中。
- en: '**How does MAX work?**'
  id: totrans-split-24
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '**MAX 是如何工作的？**'
- en: The foundation of MAX is [Mojo](https://www.modular.com/max/mojo), a common
    programming model for all AI hardware that aims to unify the entire AI stack,
    from graph kernels up to application layer, and provide a portable alternative
    to Python, C, and CUDA.
  id: totrans-split-25
  prefs: []
  type: TYPE_NORMAL
  zh: MAX的基础是[Mojo](https://www.modular.com/max/mojo)，这是一种统一所有AI硬件的通用编程模型，旨在统一整个AI堆栈，从图形内核到应用程序层，并提供一种可移植的替代Python、C和CUDA。
- en: '[MAX Engine](https://www.modular.com/max/engine) uses Mojo transparently to
    accelerate your AI models across a variety of AI hardware via a state-of-the-art
    compiler and runtime stack. Users can load and optimize existing models from PyTorch,
    TensorFlow, and ONNX with MAX Engine’s Python and C bindings, or build their own
    inference graphs using the Mojo Graph API. Additionally, we’ve made it easy to
    use [MAX Serving](https://www.modular.com/max/serving) to quickly integrate and
    support your existing inference serving pipelines.'
  id: totrans-split-26
  prefs: []
  type: TYPE_NORMAL
  zh: '[MAX引擎](https://www.modular.com/max/engine)透明地使用Mojo加速您的AI模型在各种AI硬件上运行，通过先进的编译器和运行时堆栈。用户可以使用MAX引擎的Python和C绑定加载和优化来自PyTorch、TensorFlow和ONNX的现有模型，或使用Mojo图形API构建自己的推理图。此外，我们已经让使用[MAX
    Serving](https://www.modular.com/max/serving)能够快速集成和支持您现有的推理服务管道变得容易。'
- en: Because the internals of MAX are built in Mojo, you can always write native
    Mojo to further optimize the rest of your AI pipelines, including massively accelerating
    your pre- and post-processing logic and finally removing Python out of your production
    AI serving systems.
  id: totrans-split-27
  prefs: []
  type: TYPE_NORMAL
  zh: 由于MAX的内部是在Mojo中构建的，因此您始终可以编写原生Mojo进一步优化您的余下AI管道，包括大大加速您的前处理和后处理逻辑，并最终将Python从您的生产AI服务系统中移除。
- en: '**A better developer experience**'
  id: totrans-split-28
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**更好的开发者体验**'
- en: 'In addition to the preview release of the MAX Platform, we are also excited
    to announce the release of many enhancements to the MAX developer experience.
    These include:'
  id: totrans-split-29
  prefs: []
  type: TYPE_NORMAL
  zh: '除了MAX平台的预览版本，我们还很高兴宣布对MAX开发者体验的许多增强。这些包括:'
- en: '[**MAX Examples Repo**](https://github.com/modularml/max/tree/main/examples):
    The MAX GitHub repository provides plenty of rich examples of how to use MAX in
    practice. This includes inference examples for PyTorch and TensorFlow models with
    the MAX Engine Python and C APIs as well as a llama2.**🔥** model developed with
    the Graph API.'
  id: totrans-split-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[**MAX示例存储库**](https://github.com/modularml/max/tree/main/examples): MAX GitHub存储库提供了丰富的示例，展示了如何在实践中使用MAX。这包括使用MAX引擎Python和C
    API的PyTorch和TensorFlow模型推断示例，以及使用Graph API开发的llama2.**🔥**模型。'
- en: '[**New Documentation Site**](https://docs.modular.com/): Rebuilt from the ground
    up, our new documentation site features blazing fast search, better categorization
    and structure, richer code examples, and enhanced developer usability everywhere.
    Not to mention new tutorials for all the new MAX features!'
  id: totrans-split-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[**新文档站点**](https://docs.modular.com/): 从头重建，我们的新文档站点具有极快的搜索速度，更好的分类和结构，更丰富的代码示例，以及增强的开发者可用性。更不用说为所有新的MAX功能提供的新教程了！'
- en: '[**New “Playground” experience for Mojo**](https://docs.modular.com/mojo/playground):
    Our new Mojo coding playground is perfect for early learnings and quick experimentation.
    You can start executing Mojo code within seconds and easily share it with your
    friends via Gist.'
  id: totrans-split-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[**Mojo的新“Playground”体验**](https://docs.modular.com/mojo/playground): 我们的新Mojo编码沙盒非常适合早期学习和快速实验。您可以在几秒钟内开始执行Mojo代码，并通过Gist轻松与朋友分享。'
- en: '[**New Developer Dashboard**](https://developer.modular.com/dashboard): We’ve
    revamped and updated our Developer Dashboard, enabling you to access updates,
    the performance dashboard, and our MAX Enterprise features soon.'
  id: totrans-split-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[**新开发者仪表板**](https://developer.modular.com/dashboard): 我们已经进行了全面改进和更新我们的开发者仪表板，使您能够很快访问更新内容、性能仪表板，并很快获得我们的MAX企业功能。'
- en: '**… and much more**: There have been product improvements and fixes across
    almost every surface, and we’ll continue rapidly iterating to deliver world class
    AI infrastructure for developers everywhere.'
  id: totrans-split-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**… 以及更多内容**: 几乎每个方面都进行了产品改进和修复，我们将继续快速迭代，为全球开发者提供世界级的AI基础设施。'
- en: '**There’s more to come!**'
  id: totrans-split-35
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**更多精彩内容即将推出！**'
- en: 'All of this is available today in the [MAX Developer Edition preview](https://www.modular.com/max).
    We’re excited to launch this release, have MAX go global, and see what you build
    with it. While this is just our first preview release of MAX, we have many plans
    for subsequent releases in the near future: Mac support, enterprise features,
    and GPU support is all on the way. Please [see our roadmap](https://docs.modular.com/max/roadmap)
    for more information.'
  id: totrans-split-36
  prefs: []
  type: TYPE_NORMAL
  zh: 今天在[**MAX开发者版预览**](https://www.modular.com/max)中全部内容均已提供。我们很高兴推出此版本，让MAX全球化，并期待您用它构建的项目。虽然这只是我们MAX的第一个预览版本，但我们在不久的将来有许多后续版本的计划：包括Mac支持、企业功能和GPU支持。请查看我们的[路线图](https://docs.modular.com/max/roadmap)获取更多信息。
- en: 'To learn more about MAX, and get involved with the Modular community:'
  id: totrans-split-37
  prefs: []
  type: TYPE_NORMAL
  zh: 要了解更多关于MAX的信息，并参与Modular社区：
- en: We know this is just the beginning, and we’re excited to get out this release,
    listen and take in your feedback as we quickly iterate to improve MAX for the
    world. We’re on a mission to help push AI forward for the world, and enable AI
    to be used by anyone, anywhere - join us!
  id: totrans-split-38
  prefs: []
  type: TYPE_NORMAL
  zh: 我们知道这只是一个开始，我们很高兴推出此版本，并期待听取您的反馈，以便我们快速迭代改进MAX，为世界提供帮助推动AI前进，并使AI能够被任何人在任何地方使用
    - 加入我们！
- en: Onwards!
  id: totrans-split-39
  prefs: []
  type: TYPE_NORMAL
  zh: 前进！
- en: The Modular Team
  id: totrans-split-40
  prefs: []
  type: TYPE_NORMAL
  zh: Modular团队
