- en: <!--yml
  id: totrans-split-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: æœªåˆ†ç±»'
  id: totrans-split-1
  prefs: []
  type: TYPE_NORMAL
  zh: 'category: æœªåˆ†ç±»'
- en: 'date: 2024-05-29 13:28:11'
  id: totrans-split-2
  prefs: []
  type: TYPE_NORMAL
  zh: 'date: 2024-05-29 13:28:11'
- en: -->
  id: totrans-split-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: 'Modular: Announcing MAX Developer Edition Preview'
  id: totrans-split-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Modularï¼šå®£å¸ƒ MAX å¼€å‘è€…ç‰ˆé¢„è§ˆ
- en: æ¥æºï¼š[https://www.modular.com/blog/announcing-max-developer-edition-preview](https://www.modular.com/blog/announcing-max-developer-edition-preview)
  id: totrans-split-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: æ¥æºï¼š[https://www.modular.com/blog/announcing-max-developer-edition-preview](https://www.modular.com/blog/announcing-max-developer-edition-preview)
- en: 'Modular was founded on the [vision](https://www.modular.com/vision) to enable
    AI to be used by anyone, anywhere. We have always believed that to achieve this
    vision, we must first fix the fragmented and disjoint infrastructure upon which
    AI is built today. As we said [2 years ago](https://www.modular.com/blog/the-case-for-a-next-generation-ai-developer-platform),
    we imagine a different future for AI software, one that rings truer now than ever
    before:'
  id: totrans-split-6
  prefs: []
  type: TYPE_NORMAL
  zh: Modular æ˜¯åŸºäº[æ„¿æ™¯](https://www.modular.com/vision)æˆç«‹çš„ï¼Œæ—¨åœ¨ä½¿ä»»ä½•äººä»»ä½•åœ°æ–¹éƒ½èƒ½ä½¿ç”¨AIã€‚æˆ‘ä»¬å§‹ç»ˆè®¤ä¸ºï¼Œè¦å®ç°è¿™ä¸€æ„¿æ™¯ï¼Œæˆ‘ä»¬é¦–å…ˆå¿…é¡»ä¿®å¤ä»Šå¤©æ„å»ºAIåŸºç¡€è®¾æ–½çš„åˆ†æ•£å’Œä¸è¿è´¯æ€§ã€‚æ­£å¦‚æˆ‘ä»¬[ä¸¤å¹´å‰æ‰€è¯´](https://www.modular.com/blog/the-case-for-a-next-generation-ai-developer-platform)ï¼Œæˆ‘ä»¬æƒ³è±¡ä¸€ä¸ªä¸åŒçš„AIè½¯ä»¶æœªæ¥ï¼Œç°åœ¨æ¯”ä»¥å¾€ä»»ä½•æ—¶å€™æ›´ä¸ºçœŸå®ï¼š
- en: Our success means global developers will be empowered by access to AI software
    that is truly usable, portable, and scalable. A world where developers without
    unlimited budgets or access to top talent can be just as efficient as the worldâ€™s
    largest technology giants, where the efficiency and total cost of ownership of
    AI hardware are optimized, where organizations can easily plug in custom ASICs
    for their use cases, where deploying to the edge is as easy as deploying to a
    server, where organizations can use any AI framework that best suits their needs,
    and where AI programs seamlessly scale across hardware so that deploying the latest
    AI research into production â€œjust worksâ€.
  id: totrans-split-7
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬çš„æˆåŠŸæ„å‘³ç€å…¨çƒå¼€å‘è€…å°†é€šè¿‡ä½¿ç”¨çœŸæ­£å¯ç”¨ã€ä¾¿æºå’Œå¯æ‰©å±•çš„AIè½¯ä»¶è€Œå¾—åˆ°èµ‹èƒ½ã€‚ä¸€ä¸ªä¸–ç•Œï¼Œå…¶ä¸­æ²¡æœ‰æ— é™é¢„ç®—æˆ–æ¥è§¦é¡¶çº§äººæ‰çš„å¼€å‘è€…å¯ä»¥å’Œä¸–ç•Œä¸Šæœ€å¤§çš„ç§‘æŠ€å·¨å¤´ä¸€æ ·é«˜æ•ˆï¼›ä¸€ä¸ªAIç¡¬ä»¶æ•ˆç‡å’Œæ€»ä½“æ‹¥æœ‰æˆæœ¬è¢«ä¼˜åŒ–çš„ä¸–ç•Œï¼Œç»„ç»‡å¯ä»¥è½»æ¾åœ°ä¸ºä»–ä»¬çš„ç”¨ä¾‹æ’å…¥å®šåˆ¶ASICsï¼›ä¸€ä¸ªå°†éƒ¨ç½²åˆ°è¾¹ç¼˜è®¾å¤‡ä¸éƒ¨ç½²åˆ°æœåŠ¡å™¨ä¸€æ ·ç®€å•çš„ä¸–ç•Œï¼Œç»„ç»‡å¯ä»¥ä½¿ç”¨æœ€é€‚åˆä»–ä»¬éœ€æ±‚çš„ä»»ä½•AIæ¡†æ¶ï¼Œä»¥åŠAIç¨‹åºå¯ä»¥åœ¨å„ç§ç¡¬ä»¶ä¸Šæ— ç¼æ‰©å±•ï¼Œä»è€Œå°†æœ€æ–°çš„AIç ”ç©¶éƒ¨ç½²åˆ°ç”Ÿäº§ç¯å¢ƒä¸­â€œæ¯«æ— éšœç¢â€ã€‚
- en: '**And today, weâ€™re so excited to take a huge step towards that brighter future
    by announcing that the Modular Accelerated Xecution (MAX) Platform is** [**now
    globally available**](https://www.modular.com/max), starting first as a preview
    on Linux systems. MAX is a unified set of tools and libraries that unlock performance,
    programmability and portability for your AI inference pipelines. Each component
    of MAX is designed to simplify the process of deploying models to any hardware,
    delivering the best possible cost-performance ratio for your workloads.'
  id: totrans-split-8
  prefs: []
  type: TYPE_NORMAL
  zh: '**ä»Šå¤©ï¼Œæˆ‘ä»¬éå¸¸æ¿€åŠ¨åœ°è¿ˆå‘æ›´åŠ å…‰æ˜çš„æœªæ¥ï¼Œå®£å¸ƒ Modula Accelerated Xecution (MAX) å¹³å°** [**ç°åœ¨å…¨çƒå¯ç”¨**](https://www.modular.com/max)ï¼Œé¦–å…ˆåœ¨
    Linux ç³»ç»Ÿä¸Šæä¾›é¢„è§ˆã€‚MAX æ˜¯ä¸€ç»„ç»Ÿä¸€çš„å·¥å…·å’Œåº“ï¼Œè§£é”äº†æ‚¨çš„AIæ¨ç†ç®¡é“çš„æ€§èƒ½ã€å¯ç¼–ç¨‹æ€§å’Œå¯ç§»æ¤æ€§ã€‚MAX çš„æ¯ä¸ªç»„ä»¶éƒ½æ—¨åœ¨ç®€åŒ–å°†æ¨¡å‹éƒ¨ç½²åˆ°ä»»ä½•ç¡¬ä»¶çš„è¿‡ç¨‹ï¼Œä¸ºæ‚¨çš„å·¥ä½œè´Ÿè½½æä¾›æœ€ä½³çš„æˆæœ¬æ€§èƒ½æ¯”ã€‚'
- en: '**Whatâ€™s included with MAX?**'
  id: totrans-split-9
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**MAX åŒ…å«ä»€ä¹ˆï¼Ÿ**'
- en: '**MAX is engineered to cater to the demanding needs of AI deployment, giving
    you everything you need to deploy low-latency, high-throughput, real-time inference
    pipelines into production**. This first release of MAX ships with three key components:'
  id: totrans-split-10
  prefs: []
  type: TYPE_NORMAL
  zh: '**MAX è¢«è®¾è®¡ç”¨æ¥æ»¡è¶³AIéƒ¨ç½²çš„ä¸¥è‹›éœ€æ±‚ï¼Œä¸ºæ‚¨æä¾›éƒ¨ç½²ä½å»¶è¿Ÿã€é«˜ååé‡ã€å®æ—¶æ¨ç†ç®¡é“æ‰€éœ€çš„ä¸€åˆ‡**ã€‚MAX çš„é¦–ä¸ªå‘å¸ƒç‰ˆæœ¬åŒ…å«ä¸‰ä¸ªå…³é”®ç»„ä»¶ï¼š'
- en: '[**MAX Engine**](https://www.modular.com/max/engine): A state-of-the-art AI
    compiler and runtime system supporting PyTorch, TensorFlow, and ONNX models like
    [Mistral](https://performance.modular.com/?task=language&model=mistral-7b-v0.1&instance=c7g.16xlarge&framework=stock-pytorch&view=chart),
    [Stable Diffusion](https://performance.modular.com/?task=computer_vision&model=stable-diffusion-v1.5-unet&instance=c5.9xlarge&framework=stock-pytorch&view=chart),
    [Llama2](https://performance.modular.com/?task=language&model=llama2-7B-MS-context-encoding&instance=c6a.16xlarge&framework=stock-pytorch&view=chart),
    [WavLM](https://performance.modular.com/?task=language&model=wavlm-large&instance=c6i.4xlarge&framework=stock-pytorch&view=chart),
    [DLMR](https://performance.modular.com/?task=recommenders&model=dlrm-rm2&instance=c5a.8xlarge&framework=stock-pytorch&view=chart),
    [ClipVit](https://performance.modular.com/?task=computer_vision&model=clip-vit-large-patch14&instance=c6i.4xlarge&framework=stock-pytorch&view=chart)
    and many more. It delivers unmatched inference speed across diverse hardware platforms,
    and weâ€™re just getting started.'
  id: totrans-split-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[**MAX Engine**](https://www.modular.com/max/engine)ï¼šä¸€æ¬¾æ”¯æŒPyTorchã€TensorFlowå’ŒONNXæ¨¡å‹çš„æœ€å…ˆè¿›AIç¼–è¯‘å™¨å’Œè¿è¡Œæ—¶ç³»ç»Ÿï¼Œå¦‚[Mistral](https://performance.modular.com/?task=language&model=mistral-7b-v0.1&instance=c7g.16xlarge&framework=stock-pytorch&view=chart)ï¼Œ[Stable
    Diffusion](https://performance.modular.com/?task=computer_vision&model=stable-diffusion-v1.5-unet&instance=c5.9xlarge&framework=stock-pytorch&view=chart)ï¼Œ[Llama2](https://performance.modular.com/?task=language&model=llama2-7B-MS-context-encoding&instance=c6a.16xlarge&framework=stock-pytorch&view=chart)ï¼Œ[WavLM](https://performance.modular.com/?task=language&model=wavlm-large&instance=c6i.4xlarge&framework=stock-pytorch&view=chart)ï¼Œ[DLMR](https://performance.modular.com/?task=recommenders&model=dlrm-rm2&instance=c5a.8xlarge&framework=stock-pytorch&view=chart)ï¼Œ[ClipVit](https://performance.modular.com/?task=computer_vision&model=clip-vit-large-patch14&instance=c6i.4xlarge&framework=stock-pytorch&view=chart)ç­‰ï¼Œæä¾›è·¨å¤šç§ç¡¬ä»¶å¹³å°çš„æ— ä¸ä¼¦æ¯”çš„æ¨ç†é€Ÿåº¦ï¼Œæˆ‘ä»¬ä»…ä»…æ˜¯åˆšåˆšå¼€å§‹ã€‚'
- en: '[**MAX Serving**](https://www.modular.com/max/serving)**:** An efficient serving
    wrapper for the MAX Engine, ensuring seamless interoperability with current AI
    serving systems like NVIDIA Triton, and smooth deployment to container ecosystems
    such as Kubernetes.'
  id: totrans-split-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[**MAX Serving**](https://www.modular.com/max/serving)**ï¼š** MAXå¼•æ“çš„é«˜æ•ˆæœåŠ¡åŒ…è£…å™¨ï¼Œç¡®ä¿ä¸å½“å‰çš„AIæœåŠ¡ç³»ç»Ÿï¼ˆå¦‚NVIDIA
    Tritonï¼‰æ— ç¼äº’æ“ä½œï¼Œå¹¶å¹³ç¨³éƒ¨ç½²åˆ°å®¹å™¨ç”Ÿæ€ç³»ç»Ÿï¼ˆå¦‚Kubernetesï¼‰ã€‚'
- en: â€[**MojoğŸ”¥**](https://www.modular.com/max/mojo)**:** The worldâ€™s first programming
    language built from the ground up for AI development, with cutting-edge compiler
    technology that delivers unparalleled performance and programmability on any hardware.
  id: totrans-split-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[**MojoğŸ”¥**](https://www.modular.com/max/mojo)**ï¼š** ä¸–ç•Œä¸Šç¬¬ä¸€ä¸ªä»å¤´å¼€å§‹ä¸ºAIå¼€å‘æ„å»ºçš„ç¼–ç¨‹è¯­è¨€ï¼Œå…·æœ‰å‰æ²¿çš„ç¼–è¯‘å™¨æŠ€æœ¯ï¼Œåœ¨ä»»ä½•ç¡¬ä»¶ä¸Šéƒ½èƒ½æä¾›æ— ä¸ä¼¦æ¯”çš„æ€§èƒ½å’Œå¯ç¼–ç¨‹æ€§ã€‚'
- en: MAX gives developers superpowers, providing them with an array of tools and
    libraries for building high-performance AI applications that can be efficiently
    deployed across multiple hardware platforms.
  id: totrans-split-14
  prefs: []
  type: TYPE_NORMAL
  zh: MAX ç»™å¼€å‘è€…å¸¦æ¥è¶…å‡¡çš„åŠ›é‡ï¼Œä¸ºä»–ä»¬æä¾›äº†ä¸€ç³»åˆ—å·¥å…·å’Œåº“ï¼Œç”¨äºæ„å»ºé«˜æ€§èƒ½çš„AIåº”ç”¨ç¨‹åºï¼Œå¯ä»¥é«˜æ•ˆåœ°éƒ¨ç½²åœ¨å¤šä¸ªç¡¬ä»¶å¹³å°ä¸Šã€‚
- en: '**How do I use MAX?**'
  id: totrans-split-15
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**å¦‚ä½•ä½¿ç”¨MAXï¼Ÿ**'
- en: MAX is built to meet AI developers where they are today. It provides an instant
    boost in performance for your current models and seamlessly integrates with your
    existing toolchain and serving infrastructure, capturing the immediate value and
    performance improvements of MAX with minimal code changes. Then, when you're ready,
    you can extend MAX to support other parts of your AI pipeline and achieve even
    more performance, programmability, and portability.
  id: totrans-split-16
  prefs: []
  type: TYPE_NORMAL
  zh: MAX æ˜¯ä¸ºäº†æ»¡è¶³å½“å‰AIå¼€å‘è€…çš„éœ€æ±‚è€Œæ„å»ºçš„ã€‚å®ƒä¸ºä½ ç°æœ‰çš„æ¨¡å‹æä¾›å³æ—¶çš„æ€§èƒ½æå‡ï¼Œå¹¶ä¸ä½ ç°æœ‰çš„å·¥å…·é“¾å’ŒæœåŠ¡åŸºç¡€è®¾æ–½æ— ç¼é›†æˆï¼Œé€šè¿‡æœ€å°çš„ä»£ç æ›´æ”¹æ•è·MAXçš„å³æ—¶ä»·å€¼å’Œæ€§èƒ½æ”¹è¿›ã€‚ç„¶åï¼Œå½“ä½ å‡†å¤‡å¥½æ—¶ï¼Œä½ å¯ä»¥æ‰©å±•MAXä»¥æ”¯æŒAIæµæ°´çº¿çš„å…¶ä»–éƒ¨åˆ†ï¼Œå®ç°æ›´å¤šçš„æ€§èƒ½ã€å¯ç¼–ç¨‹æ€§å’Œå¯ç§»æ¤æ€§ã€‚
- en: '**Quick performance & portability wins**'
  id: totrans-split-17
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '**å¿«é€Ÿæ€§èƒ½å’Œå¯ç§»æ¤æ€§ä¼˜åŠ¿**'
- en: '**MAX compiles and runs your existing AI models across a wide range of hardware,
    delivering immediate performance gains**. Getting started is as simple as using
    our Python or C API to replace your current PyTorch, TensorFlow, or ONNX inference
    calls with MAX Engine inference calls.'
  id: totrans-split-18
  prefs: []
  type: TYPE_NORMAL
  zh: '**MAX ç¼–è¯‘å¹¶åœ¨å„ç§ç¡¬ä»¶ä¸Šè¿è¡Œæ‚¨ç°æœ‰çš„AIæ¨¡å‹ï¼Œæä¾›å³æ—¶çš„æ€§èƒ½å¢ç›Š**ã€‚ä½¿ç”¨æˆ‘ä»¬çš„Pythonæˆ–C APIæ¥å¼€å§‹ä½¿ç”¨MAXå¼•æ“æ¨ç†è°ƒç”¨ï¼Œæ›¿æ¢æ‚¨å½“å‰çš„PyTorchã€TensorFlowæˆ–ONNXæ¨ç†è°ƒç”¨å°±è¿™ä¹ˆç®€å•ã€‚'
- en: By changing only a few lines of code, [your models execute up to 5x faster](https://performance.modular.com/?task=recommenders&model=dlrm-rm1-multihot&instance=c7g.4xlarge&framework=stock-pytorch&view=chart),
    supercharging latency so you can productionize larger models while improving efficiency
    to significantly reduce compute costs compared to stock PyTorch, TensorFlow, or
    ONNX Runtime. And the MAX Engine provides full portability across a wide range
    of CPU architectures (Intel, AMD, ARM), with GPU support coming soon. This means
    you can seamlessly move to the best hardware for your use case without rewriting
    your model.
  id: totrans-split-19
  prefs: []
  type: TYPE_NORMAL
  zh: ä»…æ”¹å˜å‡ è¡Œä»£ç ï¼Œ[æ‚¨çš„æ¨¡å‹æ‰§è¡Œé€Ÿåº¦å¯æé«˜åˆ°5å€](https://performance.modular.com/?task=recommenders&model=dlrm-rm1-multihot&instance=c7g.4xlarge&framework=stock-pytorch&view=chart)ï¼Œå¯ä»¥æå¤§åœ°å‡å°‘è®¡ç®—æˆæœ¬ï¼Œä»è€Œä½¿æ‚¨èƒ½å¤Ÿç”Ÿäº§æ›´å¤§çš„æ¨¡å‹ï¼ŒåŒæ—¶æé«˜æ•ˆç‡ï¼Œæ˜¾è‘—é™ä½ä¸æ ‡å‡†PyTorchã€TensorFlowæˆ–ONNX
    Runtimeç›¸æ¯”çš„è®¡ç®—æˆæœ¬ã€‚å¹¶ä¸”MAXå¼•æ“åœ¨å¹¿æ³›çš„CPUæ¶æ„ï¼ˆè‹±ç‰¹å°”ã€AMDã€ARMï¼‰ä¹‹é—´æä¾›äº†å…¨é¢çš„å¯ç§»æ¤æ€§ï¼ŒGPUæ”¯æŒå³å°†æ¨å‡ºã€‚è¿™æ„å‘³ç€æ‚¨å¯ä»¥æ— ç¼åœ°å°†æ‚¨çš„æ¨¡å‹ç§»åŠ¨åˆ°æœ€é€‚åˆæ‚¨ä½¿ç”¨æƒ…å†µçš„ç¡¬ä»¶ä¸Šï¼Œè€Œæ— éœ€é‡å†™ä½ çš„æ¨¡å‹ã€‚
- en: Additionally, you can wrap the MAX Engine with MAX Serving as a backend to NVIDIA
    Triton Inference Server for a production-grade deployment stack. Triton provides
    gRPC and HTTP endpoints, efficiently handling incoming input requests.
  id: totrans-split-20
  prefs: []
  type: TYPE_NORMAL
  zh: æ­¤å¤–ï¼Œä½ å¯ä»¥å°† MAX å¼•æ“ä¸MAX Servingä¸€èµ·ä½œä¸º NVIDIA Tritonæ¨ç†æœåŠ¡å™¨çš„åç«¯è¿›è¡Œç”Ÿäº§çº§éƒ¨ç½²ã€‚Tritonæä¾›gRPCå’ŒHTTPç«¯ç‚¹ï¼Œé«˜æ•ˆå¤„ç†ä¼ å…¥çš„è¾“å…¥è¯·æ±‚ã€‚
- en: '**Extend & optimize your pipeline**'
  id: totrans-split-21
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '**æ‰©å±•å’Œä¼˜åŒ–æ‚¨çš„ç®¡é“**'
- en: '**Beyond executing existing models with MAX Engine, you can further optimize
    your performance with MAXâ€™s unique extensibility and programmability capabilities**.
    MAX Engine is built using Mojo, and it is also fully extensible by you in Mojo.
    Today, the MAX Graph API enables you to build whole inference models in Mojo,
    consolidating the complexity that popular â€œpoint solutionâ€ inference frameworks
    like llama.cpp provide - delivering better flexibility and better performance.
    [Coming soon](https://docs.modular.com/max/roadmap) you will even be able to write
    custom ops that can be natively fused by the MAX Engine compiler into your existing
    model graph.'
  id: totrans-split-22
  prefs: []
  type: TYPE_NORMAL
  zh: '**é™¤äº†ä½¿ç”¨MAXå¼•æ“æ‰§è¡Œç°æœ‰æ¨¡å‹å¤–ï¼Œä½ è¿˜å¯ä»¥å€ŸåŠ© MAX ç‹¬ç‰¹çš„å¯æ‰©å±•æ€§å’Œå¯ç¼–ç¨‹èƒ½åŠ›è¿›ä¸€æ­¥ä¼˜åŒ–æ€§èƒ½**ã€‚MAXå¼•æ“æ˜¯ä½¿ç”¨Mojoæ„å»ºçš„ï¼Œå¹¶ä¸”åœ¨Mojoä¸­å®Œå…¨å¯æ‰©å±•ã€‚å¦‚ä»Šï¼ŒMAXå›¾å½¢APIä½¿ä½ èƒ½å¤Ÿåœ¨Mojoä¸­æ„å»ºæ•´ä¸ªæ¨ç†æ¨¡å‹ï¼Œ
    consagenbgæµè¡Œçš„â€œç‚¹è§£å†³æ–¹æ¡ˆâ€æ¨ç†æ¡†æ¶ï¼ˆå¦‚llama.cppï¼‰æä¾›çš„å¤æ‚æ€§ - æä¾›æ›´å¥½çš„çµæ´»æ€§å’Œæ€§èƒ½ã€‚ [å³å°†æ¨å‡º](https://docs.modular.com/max/roadmap)ï¼Œæ‚¨ç”šè‡³å°†èƒ½å¤Ÿç¼–å†™è‡ªå®šä¹‰æ“ä½œï¼ŒMAXå¼•æ“ç¼–è¯‘å™¨å°†è¿™äº›æ“ä½œåŸç”Ÿèå…¥åˆ°æ‚¨ç°æœ‰çš„æ¨¡å‹å›¾ä¸­ã€‚'
- en: Beyond model optimization with MAX Engine, you can further accelerate the rest
    of your AI pipeline by migrating your data pre/post-processing code and application
    code to Mojo, using native Mojo with the MAX Engine Mojo API. Our north star is
    to provide you with a massive amount of leverage, so you can get innovations in
    AI into your products faster.
  id: totrans-split-23
  prefs: []
  type: TYPE_NORMAL
  zh: é™¤äº†ä½¿ç”¨MAXå¼•æ“è¿›è¡Œæ¨¡å‹ä¼˜åŒ–å¤–ï¼Œä½ è¿˜å¯ä»¥é€šè¿‡å°†ä½ çš„æ•°æ®é¢„å¤„ç†/åå¤„ç†ä»£ç å’Œåº”ç”¨ç¨‹åºä»£ç è¿ç§»åˆ°Mojoï¼Œå¹¶ä½¿ç”¨MAXå¼•æ“Mojo APIçš„åŸç”ŸMojoï¼Œè¿›ä¸€æ­¥åŠ é€Ÿæ‚¨çš„AIç®¡é“çš„å…¶ä½™éƒ¨åˆ†ã€‚æˆ‘ä»¬çš„åŒ—ææ˜Ÿæ˜¯ä¸ºæ‚¨æä¾›å¤§é‡çš„æ æ†ä½œç”¨ï¼Œè®©æ‚¨èƒ½å¤Ÿæ›´å¿«åœ°å°†AIåˆ›æ–°å¼•å…¥æ‚¨çš„äº§å“ä¸­ã€‚
- en: '**How does MAX work?**'
  id: totrans-split-24
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '**MAX æ˜¯å¦‚ä½•å·¥ä½œçš„ï¼Ÿ**'
- en: The foundation of MAX is [Mojo](https://www.modular.com/max/mojo), a common
    programming model for all AI hardware that aims to unify the entire AI stack,
    from graph kernels up to application layer, and provide a portable alternative
    to Python, C, and CUDA.
  id: totrans-split-25
  prefs: []
  type: TYPE_NORMAL
  zh: MAXçš„åŸºç¡€æ˜¯[Mojo](https://www.modular.com/max/mojo)ï¼Œè¿™æ˜¯ä¸€ç§ç»Ÿä¸€æ‰€æœ‰AIç¡¬ä»¶çš„é€šç”¨ç¼–ç¨‹æ¨¡å‹ï¼Œæ—¨åœ¨ç»Ÿä¸€æ•´ä¸ªAIå †æ ˆï¼Œä»å›¾å½¢å†…æ ¸åˆ°åº”ç”¨ç¨‹åºå±‚ï¼Œå¹¶æä¾›ä¸€ç§å¯ç§»æ¤çš„æ›¿ä»£Pythonã€Cå’ŒCUDAã€‚
- en: '[MAX Engine](https://www.modular.com/max/engine) uses Mojo transparently to
    accelerate your AI models across a variety of AI hardware via a state-of-the-art
    compiler and runtime stack. Users can load and optimize existing models from PyTorch,
    TensorFlow, and ONNX with MAX Engineâ€™s Python and C bindings, or build their own
    inference graphs using the Mojo Graph API. Additionally, weâ€™ve made it easy to
    use [MAX Serving](https://www.modular.com/max/serving) to quickly integrate and
    support your existing inference serving pipelines.'
  id: totrans-split-26
  prefs: []
  type: TYPE_NORMAL
  zh: '[MAXå¼•æ“](https://www.modular.com/max/engine)é€æ˜åœ°ä½¿ç”¨MojoåŠ é€Ÿæ‚¨çš„AIæ¨¡å‹åœ¨å„ç§AIç¡¬ä»¶ä¸Šè¿è¡Œï¼Œé€šè¿‡å…ˆè¿›çš„ç¼–è¯‘å™¨å’Œè¿è¡Œæ—¶å †æ ˆã€‚ç”¨æˆ·å¯ä»¥ä½¿ç”¨MAXå¼•æ“çš„Pythonå’ŒCç»‘å®šåŠ è½½å’Œä¼˜åŒ–æ¥è‡ªPyTorchã€TensorFlowå’ŒONNXçš„ç°æœ‰æ¨¡å‹ï¼Œæˆ–ä½¿ç”¨Mojoå›¾å½¢APIæ„å»ºè‡ªå·±çš„æ¨ç†å›¾ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å·²ç»è®©ä½¿ç”¨[MAX
    Serving](https://www.modular.com/max/serving)èƒ½å¤Ÿå¿«é€Ÿé›†æˆå’Œæ”¯æŒæ‚¨ç°æœ‰çš„æ¨ç†æœåŠ¡ç®¡é“å˜å¾—å®¹æ˜“ã€‚'
- en: Because the internals of MAX are built in Mojo, you can always write native
    Mojo to further optimize the rest of your AI pipelines, including massively accelerating
    your pre- and post-processing logic and finally removing Python out of your production
    AI serving systems.
  id: totrans-split-27
  prefs: []
  type: TYPE_NORMAL
  zh: ç”±äºMAXçš„å†…éƒ¨æ˜¯åœ¨Mojoä¸­æ„å»ºçš„ï¼Œå› æ­¤æ‚¨å§‹ç»ˆå¯ä»¥ç¼–å†™åŸç”ŸMojoè¿›ä¸€æ­¥ä¼˜åŒ–æ‚¨çš„ä½™ä¸‹AIç®¡é“ï¼ŒåŒ…æ‹¬å¤§å¤§åŠ é€Ÿæ‚¨çš„å‰å¤„ç†å’Œåå¤„ç†é€»è¾‘ï¼Œå¹¶æœ€ç»ˆå°†Pythonä»æ‚¨çš„ç”Ÿäº§AIæœåŠ¡ç³»ç»Ÿä¸­ç§»é™¤ã€‚
- en: '**A better developer experience**'
  id: totrans-split-28
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**æ›´å¥½çš„å¼€å‘è€…ä½“éªŒ**'
- en: 'In addition to the preview release of the MAX Platform, we are also excited
    to announce the release of many enhancements to the MAX developer experience.
    These include:'
  id: totrans-split-29
  prefs: []
  type: TYPE_NORMAL
  zh: 'é™¤äº†MAXå¹³å°çš„é¢„è§ˆç‰ˆæœ¬ï¼Œæˆ‘ä»¬è¿˜å¾ˆé«˜å…´å®£å¸ƒå¯¹MAXå¼€å‘è€…ä½“éªŒçš„è®¸å¤šå¢å¼ºã€‚è¿™äº›åŒ…æ‹¬:'
- en: '[**MAX Examples Repo**](https://github.com/modularml/max/tree/main/examples):
    The MAX GitHub repository provides plenty of rich examples of how to use MAX in
    practice. This includes inference examples for PyTorch and TensorFlow models with
    the MAX Engine Python and C APIs as well as a llama2.**ğŸ”¥** model developed with
    the Graph API.'
  id: totrans-split-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[**MAXç¤ºä¾‹å­˜å‚¨åº“**](https://github.com/modularml/max/tree/main/examples): MAX GitHubå­˜å‚¨åº“æä¾›äº†ä¸°å¯Œçš„ç¤ºä¾‹ï¼Œå±•ç¤ºäº†å¦‚ä½•åœ¨å®è·µä¸­ä½¿ç”¨MAXã€‚è¿™åŒ…æ‹¬ä½¿ç”¨MAXå¼•æ“Pythonå’ŒC
    APIçš„PyTorchå’ŒTensorFlowæ¨¡å‹æ¨æ–­ç¤ºä¾‹ï¼Œä»¥åŠä½¿ç”¨Graph APIå¼€å‘çš„llama2.**ğŸ”¥**æ¨¡å‹ã€‚'
- en: '[**New Documentation Site**](https://docs.modular.com/): Rebuilt from the ground
    up, our new documentation site features blazing fast search, better categorization
    and structure, richer code examples, and enhanced developer usability everywhere.
    Not to mention new tutorials for all the new MAX features!'
  id: totrans-split-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[**æ–°æ–‡æ¡£ç«™ç‚¹**](https://docs.modular.com/): ä»å¤´é‡å»ºï¼Œæˆ‘ä»¬çš„æ–°æ–‡æ¡£ç«™ç‚¹å…·æœ‰æå¿«çš„æœç´¢é€Ÿåº¦ï¼Œæ›´å¥½çš„åˆ†ç±»å’Œç»“æ„ï¼Œæ›´ä¸°å¯Œçš„ä»£ç ç¤ºä¾‹ï¼Œä»¥åŠå¢å¼ºçš„å¼€å‘è€…å¯ç”¨æ€§ã€‚æ›´ä¸ç”¨è¯´ä¸ºæ‰€æœ‰æ–°çš„MAXåŠŸèƒ½æä¾›çš„æ–°æ•™ç¨‹äº†ï¼'
- en: '[**New â€œPlaygroundâ€ experience for Mojo**](https://docs.modular.com/mojo/playground):
    Our new Mojo coding playground is perfect for early learnings and quick experimentation.
    You can start executing Mojo code within seconds and easily share it with your
    friends via Gist.'
  id: totrans-split-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[**Mojoçš„æ–°â€œPlaygroundâ€ä½“éªŒ**](https://docs.modular.com/mojo/playground): æˆ‘ä»¬çš„æ–°Mojoç¼–ç æ²™ç›’éå¸¸é€‚åˆæ—©æœŸå­¦ä¹ å’Œå¿«é€Ÿå®éªŒã€‚æ‚¨å¯ä»¥åœ¨å‡ ç§’é’Ÿå†…å¼€å§‹æ‰§è¡ŒMojoä»£ç ï¼Œå¹¶é€šè¿‡Gistè½»æ¾ä¸æœ‹å‹åˆ†äº«ã€‚'
- en: '[**New Developer Dashboard**](https://developer.modular.com/dashboard): Weâ€™ve
    revamped and updated our Developer Dashboard, enabling you to access updates,
    the performance dashboard, and our MAX Enterprise features soon.'
  id: totrans-split-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[**æ–°å¼€å‘è€…ä»ªè¡¨æ¿**](https://developer.modular.com/dashboard): æˆ‘ä»¬å·²ç»è¿›è¡Œäº†å…¨é¢æ”¹è¿›å’Œæ›´æ–°æˆ‘ä»¬çš„å¼€å‘è€…ä»ªè¡¨æ¿ï¼Œä½¿æ‚¨èƒ½å¤Ÿå¾ˆå¿«è®¿é—®æ›´æ–°å†…å®¹ã€æ€§èƒ½ä»ªè¡¨æ¿ï¼Œå¹¶å¾ˆå¿«è·å¾—æˆ‘ä»¬çš„MAXä¼ä¸šåŠŸèƒ½ã€‚'
- en: '**â€¦ and much more**: There have been product improvements and fixes across
    almost every surface, and weâ€™ll continue rapidly iterating to deliver world class
    AI infrastructure for developers everywhere.'
  id: totrans-split-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**â€¦ ä»¥åŠæ›´å¤šå†…å®¹**: å‡ ä¹æ¯ä¸ªæ–¹é¢éƒ½è¿›è¡Œäº†äº§å“æ”¹è¿›å’Œä¿®å¤ï¼Œæˆ‘ä»¬å°†ç»§ç»­å¿«é€Ÿè¿­ä»£ï¼Œä¸ºå…¨çƒå¼€å‘è€…æä¾›ä¸–ç•Œçº§çš„AIåŸºç¡€è®¾æ–½ã€‚'
- en: '**Thereâ€™s more to come!**'
  id: totrans-split-35
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**æ›´å¤šç²¾å½©å†…å®¹å³å°†æ¨å‡ºï¼**'
- en: 'All of this is available today in the [MAX Developer Edition preview](https://www.modular.com/max).
    Weâ€™re excited to launch this release, have MAX go global, and see what you build
    with it. While this is just our first preview release of MAX, we have many plans
    for subsequent releases in the near future: Mac support, enterprise features,
    and GPU support is all on the way. Please [see our roadmap](https://docs.modular.com/max/roadmap)
    for more information.'
  id: totrans-split-36
  prefs: []
  type: TYPE_NORMAL
  zh: ä»Šå¤©åœ¨[**MAXå¼€å‘è€…ç‰ˆé¢„è§ˆ**](https://www.modular.com/max)ä¸­å…¨éƒ¨å†…å®¹å‡å·²æä¾›ã€‚æˆ‘ä»¬å¾ˆé«˜å…´æ¨å‡ºæ­¤ç‰ˆæœ¬ï¼Œè®©MAXå…¨çƒåŒ–ï¼Œå¹¶æœŸå¾…æ‚¨ç”¨å®ƒæ„å»ºçš„é¡¹ç›®ã€‚è™½ç„¶è¿™åªæ˜¯æˆ‘ä»¬MAXçš„ç¬¬ä¸€ä¸ªé¢„è§ˆç‰ˆæœ¬ï¼Œä½†æˆ‘ä»¬åœ¨ä¸ä¹…çš„å°†æ¥æœ‰è®¸å¤šåç»­ç‰ˆæœ¬çš„è®¡åˆ’ï¼šåŒ…æ‹¬Macæ”¯æŒã€ä¼ä¸šåŠŸèƒ½å’ŒGPUæ”¯æŒã€‚è¯·æŸ¥çœ‹æˆ‘ä»¬çš„[è·¯çº¿å›¾](https://docs.modular.com/max/roadmap)è·å–æ›´å¤šä¿¡æ¯ã€‚
- en: 'To learn more about MAX, and get involved with the Modular community:'
  id: totrans-split-37
  prefs: []
  type: TYPE_NORMAL
  zh: è¦äº†è§£æ›´å¤šå…³äºMAXçš„ä¿¡æ¯ï¼Œå¹¶å‚ä¸Modularç¤¾åŒºï¼š
- en: We know this is just the beginning, and weâ€™re excited to get out this release,
    listen and take in your feedback as we quickly iterate to improve MAX for the
    world. Weâ€™re on a mission to help push AI forward for the world, and enable AI
    to be used by anyone, anywhere - join us!
  id: totrans-split-38
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬çŸ¥é“è¿™åªæ˜¯ä¸€ä¸ªå¼€å§‹ï¼Œæˆ‘ä»¬å¾ˆé«˜å…´æ¨å‡ºæ­¤ç‰ˆæœ¬ï¼Œå¹¶æœŸå¾…å¬å–æ‚¨çš„åé¦ˆï¼Œä»¥ä¾¿æˆ‘ä»¬å¿«é€Ÿè¿­ä»£æ”¹è¿›MAXï¼Œä¸ºä¸–ç•Œæä¾›å¸®åŠ©æ¨åŠ¨AIå‰è¿›ï¼Œå¹¶ä½¿AIèƒ½å¤Ÿè¢«ä»»ä½•äººåœ¨ä»»ä½•åœ°æ–¹ä½¿ç”¨
    - åŠ å…¥æˆ‘ä»¬ï¼
- en: Onwards!
  id: totrans-split-39
  prefs: []
  type: TYPE_NORMAL
  zh: å‰è¿›ï¼
- en: The Modular Team
  id: totrans-split-40
  prefs: []
  type: TYPE_NORMAL
  zh: Modularå›¢é˜Ÿ
