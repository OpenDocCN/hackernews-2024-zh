- en: <!--yml
  id: totrans-split-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: æœªåˆ†ç±»'
  id: totrans-split-1
  prefs: []
  type: TYPE_NORMAL
  zh: 'category: æœªåˆ†ç±»'
- en: 'date: 2024-05-27 13:19:22'
  id: totrans-split-2
  prefs: []
  type: TYPE_NORMAL
  zh: 'date: 2024-05-27 13:19:22'
- en: -->
  id: totrans-split-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: Implementing Natural Conversational Agents with Elixir â€“ Sean Moriarity
  id: totrans-split-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ç”¨Elixirå®ç°è‡ªç„¶å¯¹è¯ä»£ç† - Sean Moriarity
- en: æ¥æºï¼š[https://seanmoriarity.com/2024/02/25/implementing-natural-conversational-agents-with-elixir/](https://seanmoriarity.com/2024/02/25/implementing-natural-conversational-agents-with-elixir/)
  id: totrans-split-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: æ¥æºï¼š[https://seanmoriarity.com/2024/02/25/implementing-natural-conversational-agents-with-elixir/](https://seanmoriarity.com/2024/02/25/implementing-natural-conversational-agents-with-elixir/)
- en: In [my last post](https://seanmoriarity.com/2024/02/25/nero-part-1-home-automations/),
    I discussed some work I had done building Nero, the assistant of the future that
    Iâ€™ve always wanted. I ended up creating an end-to-end example which used Nx, OpenAI
    APIs, and ElevenLabs to create an in-browser home automation assistant. For a
    first product, itâ€™s decent. Nero is a neat little party trick that I can use to
    impress my non-tech friends. I am, however, not in this business to impress my
    friends. I want Nero to *actually help me* and *actually feel like an assistant*.
    My previous version is not that.
  id: totrans-split-6
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨[æˆ‘ä¸Šä¸€ç¯‡æ–‡ç« ](https://seanmoriarity.com/2024/02/25/nero-part-1-home-automations/)ä¸­ï¼Œæˆ‘è®¨è®ºäº†æˆ‘æ„å»ºçš„ä¸€äº›å·¥ä½œï¼Œæ„å»ºäº†æˆ‘ä¸€ç›´æƒ³è¦çš„æœªæ¥åŠ©æ‰‹Neroã€‚æœ€ç»ˆï¼Œæˆ‘åˆ›å»ºäº†ä¸€ä¸ªç«¯åˆ°ç«¯ç¤ºä¾‹ï¼Œä½¿ç”¨äº†Nxã€OpenAI
    APIså’ŒElevenLabsæ¥åˆ›å»ºä¸€ä¸ªæµè§ˆå™¨å†…çš„å®¶åº­è‡ªåŠ¨åŒ–åŠ©æ‰‹ã€‚ä½œä¸ºç¬¬ä¸€ä¸ªäº§å“ï¼Œå®ƒè¿˜ä¸é”™ã€‚Neroæ˜¯ä¸€ä¸ªå°å·§çš„èšä¼šæŠŠæˆï¼Œæˆ‘å¯ä»¥ç”¨æ¥ç»™éæŠ€æœ¯æœ‹å‹ç•™ä¸‹æ·±åˆ»å°è±¡ã€‚ç„¶è€Œï¼Œæˆ‘åšè¿™ä¸ªå¹¶ä¸æ˜¯ä¸ºäº†è®©æœ‹å‹ä»¬å°è±¡æ·±åˆ»ã€‚æˆ‘å¸Œæœ›Neroèƒ½å¤Ÿ*çœŸæ­£å¸®åŠ©æˆ‘*ï¼Œ*çœŸæ­£æ„Ÿè§‰åƒä¸€ä¸ªåŠ©æ‰‹*ã€‚æˆ‘çš„ä¸Šä¸€ä¸ªç‰ˆæœ¬å¹¶ä¸æ˜¯é‚£æ ·ã€‚
- en: One missing piece is the ability to converse naturally without browser interaction.
    The first implementation of Neroâ€™s â€œconversationalâ€ abilities relied on user interaction
    with the screen every time we wanted to initiate a response or action. Nero also
    did not retain any conversational history. In short, Nero was not a great conversational
    assistant. It was one of the things I wanted to fix; however, I was motivated
    to do it sooner rather than later after watching [an impressive demo from Retell](https://beta.retellai.com/home-agent).
  id: totrans-split-7
  prefs: []
  type: TYPE_NORMAL
  zh: ç¼ºå°‘çš„ä¸€ç¯æ˜¯åœ¨æ²¡æœ‰æµè§ˆå™¨äº¤äº’çš„æƒ…å†µä¸‹è‡ªç„¶å¯¹è¯çš„èƒ½åŠ›ã€‚Neroçš„ç¬¬ä¸€ä¸ªâ€œå¯¹è¯â€èƒ½åŠ›å®ç°ä¾èµ–äºç”¨æˆ·æ¯æ¬¡æƒ³è¦å¯åŠ¨å“åº”æˆ–åŠ¨ä½œæ—¶ä¸å±å¹•çš„äº¤äº’ã€‚Neroä¹Ÿæ²¡æœ‰ä¿ç•™ä»»ä½•å¯¹è¯å†å²è®°å½•ã€‚ç®€è€Œè¨€ä¹‹ï¼ŒNeroä¸æ˜¯ä¸€ä¸ªå¾ˆå¥½çš„å¯¹è¯åŠ©æ‰‹ã€‚çœ‹å®Œ[Retellçš„ä¸€ä¸ªä»¤äººå°è±¡æ·±åˆ»çš„æ¼”ç¤ºä¹‹å](https://beta.retellai.com/home-agent)ï¼Œæˆ‘ç«‹å³æƒ³è¦å°½å¿«ä¿®å¤è¿™äº›é—®é¢˜ã€‚
- en: 'The Retell demo implements a conversational agent backed by their WebSocket
    API in a browser. The demonstration has:'
  id: totrans-split-8
  prefs: []
  type: TYPE_NORMAL
  zh: Retellæ¼”ç¤ºåœ¨æµè§ˆå™¨ä¸­é€šè¿‡ä»–ä»¬çš„WebSocket APIå®ç°äº†ä¸€ä¸ªæ”¯æŒå¯¹è¯ä»£ç†ã€‚æ¼”ç¤ºåŒ…æ‹¬ï¼š
- en: â€œAlways onâ€ recording
  id: totrans-split-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€œå§‹ç»ˆå¼€å¯â€å½•éŸ³
- en: Low latency
  id: totrans-split-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä½å»¶è¿Ÿ
- en: Support for interrupts
  id: totrans-split-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æ”¯æŒä¸­æ–­
- en: Impressive filtering (e.g. snapping and other non-voice activity doesnâ€™t seem
    to throw off the agent)
  id: totrans-split-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä»¤äººå°è±¡æ·±åˆ»çš„è¿‡æ»¤ï¼ˆä¾‹å¦‚ï¼Œæ•æ‰å’Œå…¶ä»–éè¯­éŸ³æ´»åŠ¨ä¼¼ä¹ä¸ä¼šå¹²æ‰°ä»£ç†ï¼‰
- en: Their documentation suggests they also have support for [backchanneling](https://www.cs.utep.edu/nigel/bc/#:~:text=What%20is%20a%20Backchannel%3F,utterances%20such%20as%20uh%2Dhuh.)
    and intelligent end of turn detectionâ€”two things that are essential to natural
    conversational feel but which are very difficult to express programmatically.
  id: totrans-split-13
  prefs: []
  type: TYPE_NORMAL
  zh: ä»–ä»¬çš„æ–‡æ¡£è¡¨æ˜ä»–ä»¬è¿˜æ”¯æŒ[åæ´åé¦ˆ](https://www.cs.utep.edu/nigel/bc/#:~:text=What%20is%20a%20Backchannel%3F,utterances%20such%20as%20uh%2Dhuh.)å’Œæ™ºèƒ½çš„å¯¹è¯ç»“æŸæ£€æµ‹ï¼Œè¿™ä¸¤ç‚¹å¯¹äºå®ç°è‡ªç„¶å¯¹è¯æ„Ÿè§‰éå¸¸é‡è¦ï¼Œä½†åœ¨ç¨‹åºä¸Šéå¸¸éš¾ä»¥è¡¨è¾¾ã€‚
- en: I had previously convinced myself that I could implement a passable conversational
    agent experience in a short amount of time. So that is what I set out to do.
  id: totrans-split-14
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä¹‹å‰ç¡®ä¿¡è‡ªå·±å¯ä»¥åœ¨å¾ˆçŸ­çš„æ—¶é—´å†…å®ç°ä¸€ä¸ªè¿‡å¾—å»çš„å¯¹è¯ä½“éªŒåŠ©æ‰‹ã€‚æ‰€ä»¥æˆ‘å°±ç€æ‰‹å»åšäº†ã€‚
- en: Always On Recording
  id: totrans-split-15
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: å§‹ç»ˆå¼€å¯å½•éŸ³
- en: The first thing that needed to change about Neroâ€™s design was the speech to
    text pipeline. My original demonstration relied on an [example from Bumblebee](https://github.com/elixir-nx/bumblebee/blob/main/examples/phoenix/speech_to_text.exs)
    which implemented a speech to text pipeline using Whisper. The pipeline uses mouse
    events in a [Phoenix LiveView Hook](https://hexdocs.pm/phoenix_live_view/js-interop.html#client-hooks-via-phx-hook)
    to start and stop recordings before sending them to the server to initiate transcription.
    If youâ€™re not familiar, [Phoenix LiveView](https://hexdocs.pm/phoenix_live_view/Phoenix.LiveView.html)
    is a server-side rendering framework built on top of Elixir. LiveView has support
    for client-side JavaScript hooks which support bidirectional communication between
    client and server.
  id: totrans-split-16
  prefs: []
  type: TYPE_NORMAL
  zh: å…³äºNeroçš„è®¾è®¡éœ€è¦æ”¹å˜çš„ç¬¬ä¸€ä»¶äº‹æ˜¯è¯­éŸ³åˆ°æ–‡æœ¬æµæ°´çº¿ã€‚æˆ‘çš„åˆå§‹æ¼”ç¤ºä¾èµ–äº[Bumblebeeçš„ä¸€ä¸ªç¤ºä¾‹](https://github.com/elixir-nx/bumblebee/blob/main/examples/phoenix/speech_to_text.exs)ï¼Œè¯¥ç¤ºä¾‹ä½¿ç”¨Whisperå®ç°äº†ä¸€ä¸ªä½¿ç”¨Phoenix
    LiveView Hookåœ¨å‘é€åˆ°æœåŠ¡å™¨è¿›è¡Œè½¬å½•ä¹‹å‰å¼€å§‹å’Œåœæ­¢å½•éŸ³çš„è¯­éŸ³åˆ°æ–‡æœ¬æµæ°´çº¿ã€‚å¦‚æœæ‚¨ä¸ç†Ÿæ‚‰ï¼Œ[Phoenix LiveView](https://hexdocs.pm/phoenix_live_view/Phoenix.LiveView.html)æ˜¯å»ºç«‹åœ¨Elixirä¹‹ä¸Šçš„æœåŠ¡å™¨ç«¯æ¸²æŸ“æ¡†æ¶ã€‚LiveViewæ”¯æŒå®¢æˆ·ç«¯JavaScripté’©å­ï¼Œæ”¯æŒå®¢æˆ·ç«¯å’ŒæœåŠ¡å™¨ä¹‹é—´çš„åŒå‘é€šä¿¡ã€‚
- en: The original speech to text implementation used a hook with an event listener
    attached to `mousedown` and `mouseup` on a button to start and stop recording.
    After recording stops, the hook decodes the recorded buffer into a PCM buffer,
    converts the endianness, and then pushes the buffer to the server with an upload.
    The original hook implements most of the functionality we want; however, we need
    to make some minor tweaks. Rather than trigger recordings to stop and start on
    mouse events, we want to trigger recordings to start and stop exactly when a person
    starts and stops speaking. Simple, right?
  id: totrans-split-17
  prefs: []
  type: TYPE_NORMAL
  zh: åˆå§‹çš„è¯­éŸ³è½¬æ–‡å­—å®ç°ä½¿ç”¨äº†ä¸€ä¸ªé’©å­ï¼Œé™„åŠ åˆ°æŒ‰é’®çš„`mousedown`å’Œ`mouseup`äº‹ä»¶ç›‘å¬å™¨ä¸Šï¼Œç”¨äºå¼€å§‹å’Œåœæ­¢å½•éŸ³ã€‚åœ¨å½•éŸ³åœæ­¢åï¼Œé’©å­å°†è®°å½•çš„ç¼“å†²åŒºè§£ç ä¸ºPCMç¼“å†²åŒºï¼Œè½¬æ¢å­—èŠ‚é¡ºåºï¼Œç„¶åå°†ç¼“å†²åŒºæ¨é€åˆ°æœåŠ¡å™¨è¿›è¡Œä¸Šä¼ ã€‚åŸå§‹é’©å­å®ç°äº†å¤§éƒ¨åˆ†æˆ‘ä»¬æƒ³è¦çš„åŠŸèƒ½ï¼›ç„¶è€Œï¼Œæˆ‘ä»¬éœ€è¦è¿›è¡Œä¸€äº›å°çš„è°ƒæ•´ã€‚ä¸å…¶åœ¨é¼ æ ‡äº‹ä»¶ä¸Šè§¦å‘å½•éŸ³å¼€å§‹å’Œåœæ­¢ï¼Œæˆ‘ä»¬å¸Œæœ›åœ¨äººå¼€å§‹è¯´è¯å’Œåœæ­¢è¯´è¯æ—¶å‡†ç¡®åœ°è§¦å‘å½•éŸ³çš„å¼€å§‹å’Œåœæ­¢ã€‚ç®€å•å§ï¼Ÿ
- en: My first idea in implementing what I called â€œalways on recordingâ€ was to monitor
    the microphoneâ€™s volume, and trigger a recording when the volume reached a certain
    threshold. The recording would stop when the volume dipped below that threshold.
    At this point, I learned about [getUserMedia](https://developer.mozilla.org/en-US/docs/Web/API/MediaDevices/getUserMedia).
    `getUserMedia` prompts the user for permission to access media devices such as
    a microphone and/or webcam, and then produces a `MediaStream`. A `MediaStream`
    is a stream of media content containing information about audio and video tracks
    in the stream. We can use data from the `MediaStream` to determine speaker activity
    and thus trigger recordings.
  id: totrans-split-18
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘åœ¨å®ç°æˆ‘ç§°ä¹‹ä¸ºâ€œå§‹ç»ˆå½•éŸ³â€çš„æƒ³æ³•æ—¶ï¼Œç¬¬ä¸€ä¸ªæƒ³åˆ°çš„æ˜¯ç›‘è§†éº¦å…‹é£çš„éŸ³é‡ï¼Œå¹¶ä¸”å½“éŸ³é‡è¾¾åˆ°ä¸€å®šçš„é˜ˆå€¼æ—¶è§¦å‘å½•éŸ³ã€‚å½“éŸ³é‡ä½äºè¯¥é˜ˆå€¼æ—¶åœæ­¢å½•éŸ³ã€‚åœ¨è¿™ä¸€ç‚¹ä¸Šï¼Œæˆ‘äº†è§£äº†[getUserMedia](https://developer.mozilla.org/en-US/docs/Web/API/MediaDevices/getUserMedia)ã€‚`getUserMedia`æç¤ºç”¨æˆ·å…è®¸è®¿é—®åª’ä½“è®¾å¤‡ï¼Œæ¯”å¦‚éº¦å…‹é£å’Œ/æˆ–æ‘„åƒå¤´ï¼Œç„¶åç”Ÿæˆä¸€ä¸ª`MediaStream`ã€‚`MediaStream`æ˜¯ä¸€ä¸ªåŒ…å«æœ‰å…³æµä¸­éŸ³é¢‘å’Œè§†é¢‘è½¨é“ä¿¡æ¯çš„åª’ä½“å†…å®¹æµã€‚æˆ‘ä»¬å¯ä»¥ä½¿ç”¨`MediaStream`çš„æ•°æ®æ¥ç¡®å®šè¯´è¯è€…çš„æ´»åŠ¨ï¼Œä»è€Œè§¦å‘å½•éŸ³ã€‚
- en: 'To determine the volume for a given sample, we can use an [AnalyserNode](https://developer.mozilla.org/en-US/docs/Web/API/AnalyserNode).
    Per the documentation `AnyalyserNode` is designed for processing generated audio
    data for visualization purposes, but we can use it to determine spikes in audio:'
  id: totrans-split-19
  prefs: []
  type: TYPE_NORMAL
  zh: è¦ç¡®å®šç‰¹å®šæ ·æœ¬çš„éŸ³é‡ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨[AnalyserNode](https://developer.mozilla.org/en-US/docs/Web/API/AnalyserNode)ã€‚æ ¹æ®æ–‡æ¡£ï¼Œ`AnyalyserNode`è¢«è®¾è®¡ç”¨äºå¤„ç†ç”Ÿæˆçš„éŸ³é¢‘æ•°æ®ä»¥è¿›è¡Œå¯è§†åŒ–ï¼Œä½†æˆ‘ä»¬å¯ä»¥ä½¿ç”¨å®ƒæ¥ç¡®å®šéŸ³é¢‘ä¸­çš„å³°å€¼ï¼š
- en: '[PRE0]'
  id: totrans-split-20
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: This uses an analyser and repeatedly checks if the volume of the microphone
    at a given frame exceeds the given `VOLUME_THRESHOLD`. If it does, it checks to
    see if we are recording and if not, starts the recording.
  id: totrans-split-21
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä½¿ç”¨ä¸€ä¸ªåˆ†æå™¨å¹¶é‡å¤æ£€æŸ¥ç»™å®šå¸§çš„éº¦å…‹é£éŸ³é‡æ˜¯å¦è¶…è¿‡ç»™å®šçš„`VOLUME_THRESHOLD`ã€‚å¦‚æœæ˜¯ï¼Œåˆ™æ£€æŸ¥æˆ‘ä»¬æ˜¯å¦æ­£åœ¨å½•åˆ¶ï¼Œå¦‚æœä¸æ˜¯ï¼Œåˆ™å¼€å§‹å½•åˆ¶ã€‚
- en: 'After testing a bit, I realized this implementation sucked. Of the many issues
    with this approach, the biggest is that there are many natural dips in a personâ€™s
    volume. Checking a single frame doesnâ€™t account for these natural dips. To fix
    this, I thought it would be a good idea to introduce a timeout which only stopped
    recording after the volume was below a threshold for a certain amount of time:'
  id: totrans-split-22
  prefs: []
  type: TYPE_NORMAL
  zh: æµ‹è¯•äº†ä¸€ä¸‹ä¹‹åï¼Œæˆ‘æ„è¯†åˆ°è¿™ä¸ªå®ç°å¹¶ä¸å¥½ç”¨ã€‚è¿™ç§æ–¹æ³•æœ‰å¾ˆå¤šé—®é¢˜ï¼Œå…¶ä¸­æœ€å¤§çš„é—®é¢˜æ˜¯äººçš„éŸ³é‡ä¼šè‡ªç„¶åœ°æœ‰å¾ˆå¤šæ³¢åŠ¨ã€‚ä»…æ£€æŸ¥å•ä¸ªå¸§æ— æ³•è§£å†³è¿™äº›è‡ªç„¶æ³¢åŠ¨ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘æƒ³å¼•å…¥ä¸€ä¸ªè¶…æ—¶æœºåˆ¶ï¼Œåªæœ‰åœ¨éŸ³é‡ä½äºæŸä¸ªé˜ˆå€¼ä¸€æ®µæ—¶é—´åæ‰åœæ­¢å½•åˆ¶ï¼š
- en: '[PRE1]'
  id: totrans-split-23
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: This actually ended up working decent, but required tuning hyperparameters for
    both `VOLUME_THRESHOLD` and `SILENCE_TIMEOUT`. The challenge here is that higher
    `SILENCE_TIMEOUT` introduces additionally latency in transition time between a
    speaker and Nero; however, lower timeouts might be too sensitive to speakers with
    slower and quieter speaking rhythms. Additionally, a static `VOLUME_THRESHOLD`
    does not account for ambient noise. Now, despite these shortcomings, I found I
    was able to passably detect a single speaker in a quiet room.
  id: totrans-split-24
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™å®é™…ä¸Šæ•ˆæœè¿˜ä¸é”™ï¼Œä½†éœ€è¦è°ƒæ•´ `VOLUME_THRESHOLD` å’Œ `SILENCE_TIMEOUT` çš„è¶…å‚æ•°ã€‚æŒ‘æˆ˜åœ¨äºï¼Œæ›´é«˜çš„ `SILENCE_TIMEOUT`
    ä¼šå¢åŠ è¯´è¯è€…ä¸ Nero ä¹‹é—´è½¬æ¢æ—¶é—´çš„å»¶è¿Ÿï¼Œä½†è¾ƒä½çš„è¶…æ—¶æ—¶é—´å¯èƒ½å¯¹è¯´è¯èŠ‚å¥è¾ƒæ…¢å’ŒéŸ³é‡è¾ƒä½çš„è¯´è¯è€…è¿‡äºæ•æ„Ÿã€‚æ­¤å¤–ï¼Œé™æ€çš„ `VOLUME_THRESHOLD`
    æ— æ³•è€ƒè™‘ç¯å¢ƒå™ªå£°ã€‚å°½ç®¡å­˜åœ¨è¿™äº›ç¼ºç‚¹ï¼Œæˆ‘å‘ç°æˆ‘èƒ½å¤ŸåŸºæœ¬ä¸Šåœ¨å®‰é™çš„æˆ¿é—´é‡Œæ£€æµ‹å‡ºå•ä¸ªè¯´è¯è€…ã€‚
- en: After hooking this up to my existing LiveView and trying some end-to-end conversations,
    I realized something was significantly off. The transcriptions I was getting were
    off. I soon realized that they were always off at the beginning of a transcription.
    Shorter audio sequences were especially affected. It turns out that the detection
    algorithm always resulted in some amount of truncation at the beginning of an
    audio clip. When a speaker starts talking, their volume ramps up â€“ itâ€™s not an
    instantaneous spike. To account for this, I introduced a pre-recording buffer
    which always tracked the previous 150ms of audio. After recording started, I would
    stop the pre-recording buffer and start the actual recording, and then eventually
    splice these 2 together to send to the server for transcription.
  id: totrans-split-25
  prefs: []
  type: TYPE_NORMAL
  zh: å°†è¿™ä¸ªè¿æ¥åˆ°æˆ‘çš„ç°æœ‰ LiveView å¹¶å°è¯•äº†ä¸€äº›ç«¯åˆ°ç«¯å¯¹è¯åï¼Œæˆ‘æ„è¯†åˆ°æœ‰äº›åœ°æ–¹æ˜æ˜¾ä¸å¯¹åŠ²ã€‚æˆ‘å¾—åˆ°çš„è½¬å½•ç»“æœä¸æ­£ç¡®ã€‚å¾ˆå¿«æˆ‘æ„è¯†åˆ°é—®é¢˜æ€»æ˜¯å‡ºç°åœ¨è½¬å½•å¼€å§‹çš„æ—¶å€™ã€‚è¾ƒçŸ­çš„éŸ³é¢‘åºåˆ—å°¤å…¶å—å½±å“ã€‚åŸæ¥æ£€æµ‹ç®—æ³•åœ¨éŸ³é¢‘å‰ªè¾‘çš„å¼€å¤´æ€»æ˜¯ä¼šæœ‰ä¸€å®šçš„æˆªæ–­ã€‚å½“è¯´è¯è€…å¼€å§‹è®²è¯æ—¶ï¼Œä»–ä»¬çš„éŸ³é‡ä¼šé€æ¸å¢åŠ ï¼Œè€Œä¸æ˜¯ç¬é—´çš„å³°å€¼ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘å¼•å…¥äº†ä¸€ä¸ªé¢„å½•éŸ³ç¼“å†²åŒºï¼Œå§‹ç»ˆè·Ÿè¸ªå‰
    150 æ¯«ç§’çš„éŸ³é¢‘ã€‚å½•åˆ¶å¼€å§‹åï¼Œæˆ‘ä¼šåœæ­¢é¢„å½•éŸ³ç¼“å†²å¹¶å¼€å§‹å®é™…å½•åˆ¶ï¼Œç„¶åæœ€ç»ˆå°†è¿™ä¸¤è€…æ‹¼æ¥åœ¨ä¸€èµ·å‘é€åˆ°æœåŠ¡å™¨è¿›è¡Œè½¬å½•ã€‚
- en: Overall, this *actually* worked okay. While there are some obvious failure modes,
    it worked well enough to get a passable demonstration. If you canâ€™t tell by now,
    I am not an audio engineer. I learned later that this is a very naive attempt
    at [voice activity detection](https://en.wikipedia.org/wiki/Voice_activity_detection).
    Later on in this post, Iâ€™ll run through some of the improvements I made based
    on my research into the field of VAD.
  id: totrans-split-26
  prefs: []
  type: TYPE_NORMAL
  zh: æ€»ä½“æ¥è¯´ï¼Œè¿™ *å®é™…ä¸Š* è¿è¡Œå¾—è¿˜å¯ä»¥ã€‚è™½ç„¶å­˜åœ¨ä¸€äº›æ˜æ˜¾çš„å¤±è´¥æ¨¡å¼ï¼Œä½†è¶³å¤Ÿæ¼”ç¤ºäº†ä¸€ä¸‹ã€‚å¦‚æœåˆ°ç›®å‰ä¸ºæ­¢ä½ è¿˜æ²¡å‘ç°ï¼Œæˆ‘ä¸æ˜¯ä¸€ä¸ªéŸ³é¢‘å·¥ç¨‹å¸ˆã€‚åæ¥æˆ‘äº†è§£åˆ°ï¼Œè¿™æ˜¯å¯¹
    [è¯­éŸ³æ´»åŠ¨æ£€æµ‹](https://en.wikipedia.org/wiki/Voice_activity_detection) çš„ä¸€ä¸ªéå¸¸å¹¼ç¨šçš„å°è¯•ã€‚åœ¨æœ¬æ–‡çš„åé¢éƒ¨åˆ†ï¼Œæˆ‘å°†ä»‹ç»ä¸€äº›åŸºäºæˆ‘å¯¹
    VAD é¢†åŸŸç ”ç©¶çš„æ”¹è¿›ã€‚
- en: End-to-End Implementation
  id: totrans-split-27
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ç«¯åˆ°ç«¯å®ç°
- en: 'The demonstration I built for Nero in my [first post](https://seanmoriarity.com/2024/02/25/nero-part-1-home-automations/)
    already contained the scaffolding for an end-to-end transcription -> response
    -> speech pipeline. I only needed to make some slight modifications to get the
    phone call demo to work. The end-to-end the pipeline looks like this:'
  id: totrans-split-28
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘åœ¨æˆ‘çš„ [ç¬¬ä¸€ç¯‡æ–‡ç« ](https://seanmoriarity.com/2024/02/25/nero-part-1-home-automations/)
    ä¸­ä¸º Nero æ„å»ºçš„æ¼”ç¤ºå·²ç»åŒ…å«äº†ç«¯åˆ°ç«¯è½¬å½• -> å“åº” -> è¯­éŸ³ç®¡é“çš„è„šæ‰‹æ¶ã€‚æˆ‘åªéœ€è¦è¿›è¡Œä¸€äº›å¾®å°çš„ä¿®æ”¹å°±èƒ½è®©ç”µè¯æ¼”ç¤ºå·¥ä½œèµ·æ¥ã€‚ç«¯åˆ°ç«¯çš„ç®¡é“å¦‚ä¸‹æ‰€ç¤ºï¼š
- en: 'When our algorithm detects that speech has stopped, it invokes the `stopRecording`
    method. `stopRecording` takes the recorded audio, does some client-side pre-processing,
    and uploads it to the server. The server consumes the uploaded entry as a part
    of [LiveViewâ€™s normal uploads lifecycle](https://hexdocs.pm/phoenix_live_view/uploads.html)
    and then invokes an [async task](https://hexdocs.pm/phoenix_live_view/Phoenix.LiveView.html#module-arbitrary-async-operations)
    to start transcription:'
  id: totrans-split-29
  prefs: []
  type: TYPE_NORMAL
  zh: å½“æˆ‘ä»¬çš„ç®—æ³•æ£€æµ‹åˆ°è¯­éŸ³å·²åœæ­¢æ—¶ï¼Œå®ƒè°ƒç”¨`stopRecording`æ–¹æ³•ã€‚`stopRecording`è·å–å½•åˆ¶çš„éŸ³é¢‘ï¼Œæ‰§è¡Œä¸€äº›å®¢æˆ·ç«¯é¢„å¤„ç†ï¼Œå¹¶å°†å…¶ä¸Šä¼ åˆ°æœåŠ¡å™¨ã€‚æœåŠ¡å™¨ä½œä¸º[LiveViewçš„æ­£å¸¸ä¸Šä¼ ç”Ÿå‘½å‘¨æœŸ](https://hexdocs.pm/phoenix_live_view/uploads.html)çš„ä¸€éƒ¨åˆ†æ¶ˆè€—ä¸Šä¼ çš„æ¡ç›®ï¼Œå¹¶è°ƒç”¨ä¸€ä¸ª[å¼‚æ­¥ä»»åŠ¡](https://hexdocs.pm/phoenix_live_view/Phoenix.LiveView.html#module-arbitrary-async-operations)æ¥å¯åŠ¨è½¬å½•ï¼š
- en: '[PRE2]'
  id: totrans-split-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Note that because we did most of the pre-processing client-side, we can just
    consume the audio binary as an `Nx.Tensor`, without any additional work. The `SpeechToText`
    module implements transcription using [Nx.Serving](https://hexdocs.pm/nx/Nx.Serving.html):'
  id: totrans-split-31
  prefs: []
  type: TYPE_NORMAL
  zh: æ³¨æ„ï¼Œå› ä¸ºæˆ‘ä»¬å¤§éƒ¨åˆ†é¢„å¤„ç†æ˜¯åœ¨å®¢æˆ·ç«¯å®Œæˆçš„ï¼Œæ‰€ä»¥æˆ‘ä»¬å¯ä»¥ç›´æ¥å°†éŸ³é¢‘äºŒè¿›åˆ¶ä½œä¸º`Nx.Tensor`æ¶ˆè€—ï¼Œæ— éœ€é¢å¤–å·¥ä½œã€‚`SpeechToText`æ¨¡å—ä½¿ç”¨[Nx.Serving](https://hexdocs.pm/nx/Nx.Serving.html)å®ç°è½¬å½•ï¼š
- en: '[PRE3]'
  id: totrans-split-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '`Nx.Serving` is an abstraction in the Elixir Nx ecosystem for serving machine
    learning models directly in an Elixir application. It implements dynamic batching,
    encapsulates pre-processing, inference, and post-processing, supports distribution
    and load-balancing between multiple GPUs natively, and in general is an extremely
    easy way to serve machine learning models.'
  id: totrans-split-33
  prefs: []
  type: TYPE_NORMAL
  zh: '`Nx.Serving`æ˜¯Elixir Nxç”Ÿæ€ç³»ç»Ÿä¸­ç”¨äºç›´æ¥ä¸ºElixiråº”ç”¨ç¨‹åºæä¾›æœºå™¨å­¦ä¹ æ¨¡å‹çš„æŠ½è±¡ã€‚å®ƒå®ç°åŠ¨æ€æ‰¹å¤„ç†ï¼Œå°è£…é¢„å¤„ç†ã€æ¨ç†å’Œåå¤„ç†ï¼ŒåŸç”Ÿæ”¯æŒå¤šä¸ªGPUä¹‹é—´çš„åˆ†å¸ƒå’Œè´Ÿè½½å¹³è¡¡ï¼Œå¹¶ä¸”é€šå¸¸æ˜¯ä¸€ç§æå…¶ç®€å•çš„æ–¹æ³•æ¥æä¾›æœºå™¨å­¦ä¹ æ¨¡å‹ã€‚'
- en: 'After transcription completes, we get an async result we can handle to initiate
    a response:'
  id: totrans-split-34
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è½¬å½•å®Œæˆåï¼Œæˆ‘ä»¬è·å¾—äº†ä¸€ä¸ªå¼‚æ­¥ç»“æœï¼Œå¯ä»¥å¤„ç†ä»¥å¯åŠ¨å“åº”ï¼š
- en: '[PRE4]'
  id: totrans-split-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Here `Nero.Agent.respond/1` returns an Elixir `Stream` of text. For my original
    demonstration I just used the Elixir OpenAI library to produce a stream from a
    GPT-3.5 response:'
  id: totrans-split-36
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™é‡Œï¼Œ`Nero.Agent.respond/1`è¿”å›ä¸€ä¸ªElixiræ–‡æœ¬æµã€‚å¯¹äºæˆ‘çš„åŸå§‹æ¼”ç¤ºï¼Œæˆ‘åªæ˜¯ä½¿ç”¨Elixir OpenAIåº“ä»GPT-3.5å“åº”ä¸­ç”Ÿæˆæµï¼š
- en: '[PRE5]'
  id: totrans-split-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'The response stream is consumed by `speak/2`. `speak/2` implements the text
    to speech pipeline:'
  id: totrans-split-38
  prefs: []
  type: TYPE_NORMAL
  zh: å“åº”æµç”±`speak/2`æ¶ˆè€—ã€‚`speak/2`å®ç°äº†æ–‡æœ¬åˆ°è¯­éŸ³çš„æµæ°´çº¿ï¼š
- en: '[PRE6]'
  id: totrans-split-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Where `Nero.TextToSpeech.stream/1` uses the [ElevenLabs WebSocket API](https://elevenlabs.io/docs/api-reference/websockets)
    to stream text in and speech out. You can read a bit more about the implementation
    in my previous post.
  id: totrans-split-40
  prefs: []
  type: TYPE_NORMAL
  zh: '`Nero.TextToSpeech.stream/1`ä½¿ç”¨[ElevenLabs WebSocket API](https://elevenlabs.io/docs/api-reference/websockets)æ¥æµå…¥æ–‡æœ¬å¹¶æµå‡ºè¯­éŸ³ã€‚æ‚¨å¯ä»¥åœ¨æˆ‘çš„ä¸Šä¸€ç¯‡æ–‡ç« ä¸­è¿›ä¸€æ­¥äº†è§£å…¶å®ç°ã€‚'
- en: '`Nero.TextToSpeech.stream/1` returns the consumed response as text so we can
    append that to the chat history after the `:speak` task finishes:'
  id: totrans-split-41
  prefs: []
  type: TYPE_NORMAL
  zh: '`Nero.TextToSpeech.stream/1`å°†æ¶ˆè€—çš„å“åº”è¿”å›ä¸ºæ–‡æœ¬ï¼Œä»¥ä¾¿æˆ‘ä»¬å¯ä»¥åœ¨`:speak`ä»»åŠ¡å®Œæˆåå°†å…¶é™„åŠ åˆ°èŠå¤©å†å²è®°å½•ä¸­ï¼š'
- en: '[PRE7]'
  id: totrans-split-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'This is basically all of the scaffolding needed for an end-to-end demo, but
    I wanted to add a few more features. First, I wanted to support â€œintelligentâ€
    hang-ups. Basically, I wanted to be able to detect when a conversation was finished,
    and stop the recording. To do that, I used [Instructor](https://github.com/thmsmlr/instructor_ex):'
  id: totrans-split-43
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™åŸºæœ¬ä¸Šæ˜¯è¿›è¡Œç«¯åˆ°ç«¯æ¼”ç¤ºæ‰€éœ€çš„æ‰€æœ‰è„šæ‰‹æ¶ï¼Œä½†æˆ‘æƒ³æ·»åŠ æ›´å¤šåŠŸèƒ½ã€‚é¦–å…ˆï¼Œæˆ‘å¸Œæœ›æ”¯æŒâ€œæ™ºèƒ½â€æŒ‚æ–­ç”µè¯ã€‚åŸºæœ¬ä¸Šï¼Œæˆ‘å¸Œæœ›èƒ½å¤Ÿæ£€æµ‹åˆ°å¯¹è¯ä½•æ—¶ç»“æŸï¼Œå¹¶åœæ­¢å½•éŸ³ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä½¿ç”¨äº†[Instructor](https://github.com/thmsmlr/instructor_ex)ï¼š
- en: '[PRE8]'
  id: totrans-split-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Please ignore my wonderfully engineered prompt. This uses GPT-3.5 to determine
    whether or not a given conversation has ended. After every one of Neroâ€™s turns,
    we check the transcript to possibly end the call:'
  id: totrans-split-45
  prefs: []
  type: TYPE_NORMAL
  zh: è¯·å¿½ç•¥æˆ‘ç²¾å¿ƒè®¾è®¡çš„æç¤ºã€‚è¿™ä½¿ç”¨GPT-3.5æ¥ç¡®å®šç»™å®šå¯¹è¯æ˜¯å¦å·²ç»ç»“æŸã€‚åœ¨Neroçš„æ¯æ¬¡å¯¹è¯ä¹‹åï¼Œæˆ‘ä»¬æ£€æŸ¥è½¬å½•ä»¥å¯èƒ½ç»“æŸé€šè¯ï¼š
- en: '[PRE9]'
  id: totrans-split-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'This pushes a `hang_up` event to the socket:'
  id: totrans-split-47
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™å°†å‘å¥—æ¥å­—æ¨é€ä¸€ä¸ª`hang_up`äº‹ä»¶ï¼š
- en: '[PRE10]'
  id: totrans-split-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Which stops the recording, and then pushes an event to `toggle_conversation`
    back to the server. `toggle_conversation` implements the start/stop logic from
    the server:'
  id: totrans-split-49
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™å°†åœæ­¢å½•éŸ³ï¼Œå¹¶å°†ä¸€ä¸ªäº‹ä»¶æ¨é€åˆ°`toggle_conversation`è¿”å›åˆ°æœåŠ¡å™¨ã€‚`toggle_conversation`å®ç°äº†æœåŠ¡å™¨ä¸Šçš„å¯åŠ¨/åœæ­¢é€»è¾‘ï¼š
- en: '[PRE11]'
  id: totrans-split-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Finally, I wanted to implement information extraction from the transcript.
    Again, I used instructor and defined an extraction schema:'
  id: totrans-split-51
  prefs: []
  type: TYPE_NORMAL
  zh: æœ€åï¼Œæˆ‘æƒ³å®ç°ä»è½¬å½•ä¸­æå–ä¿¡æ¯ã€‚åŒæ ·ï¼Œæˆ‘ä½¿ç”¨äº†instructorå¹¶å®šä¹‰äº†ä¸€ä¸ªæå–æ¨¡å¼ï¼š
- en: '[PRE12]'
  id: totrans-split-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'And used GPT-3.5 with a rough prompt to get the necessary information from
    the transcript:'
  id: totrans-split-53
  prefs: []
  type: TYPE_NORMAL
  zh: å¹¶ä½¿ç”¨GPT-3.5ä¸ä¸€ä¸ªç²—ç•¥çš„æç¤ºä»è½¬å½•ä¸­è·å–å¿…è¦çš„ä¿¡æ¯ï¼š
- en: '[PRE13]'
  id: totrans-split-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'And then anytime a conversation ends, we attempt to retrieve appointment information:'
  id: totrans-split-55
  prefs: []
  type: TYPE_NORMAL
  zh: æ¯å½“å¯¹è¯ç»“æŸæ—¶ï¼Œæˆ‘ä»¬å°è¯•æ£€ç´¢çº¦ä¼šä¿¡æ¯ï¼š
- en: '[PRE14]'
  id: totrans-split-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Now this is essentially the exact implementation that produced [this demonstration](https://twitter.com/sean_moriarity/status/1760435005119934862).
    End-to-end this amounted to a couple of hours of work; however, I already had
    most of the basic scaffold implemented from my previous work on Nero. In my biased
    opinion, I think my demo is pretty good, but as others have pointed out Retellâ€™s
    demo kicks my ass in:'
  id: totrans-split-57
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨è¿™åŸºæœ¬ä¸Šå°±æ˜¯ç”Ÿæˆäº†[è¿™ä¸ªæ¼”ç¤º](https://twitter.com/sean_moriarity/status/1760435005119934862)çš„ç¡®åˆ‡å®ç°ã€‚ä»å¤´åˆ°å°¾è¿™å¤§çº¦èŠ±äº†å‡ ä¸ªå°æ—¶çš„å·¥ä½œï¼›ç„¶è€Œï¼Œæˆ‘å·²ç»åœ¨æˆ‘ä¹‹å‰åœ¨
    Nero ä¸Šçš„å·¥ä½œä¸­å®ç°äº†å¤§éƒ¨åˆ†åŸºæœ¬æ¡†æ¶ã€‚åœ¨æˆ‘ä¸»è§‚çš„çœ‹æ³•ä¸­ï¼Œæˆ‘è®¤ä¸ºæˆ‘çš„æ¼”ç¤ºç›¸å½“ä¸é”™ï¼Œä½†æ­£å¦‚å…¶ä»–äººæŒ‡å‡ºçš„ï¼ŒRetell çš„æ¼”ç¤ºæ¯”æˆ‘çš„å¥½å¾—å¤šï¼š
- en: Latency
  id: totrans-split-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å»¶è¿Ÿ
- en: Reliability
  id: totrans-split-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å¯é æ€§
- en: Natural sounding voice
  id: totrans-split-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: è‡ªç„¶çš„å£°éŸ³
- en: And so, I set out to improve my implementation â€“ starting with latency.
  id: totrans-split-61
  prefs: []
  type: TYPE_NORMAL
  zh: å› æ­¤ï¼Œæˆ‘å¼€å§‹æ”¹å–„æˆ‘çš„å®ç°â€”â€”ä»å»¶è¿Ÿå¼€å§‹ã€‚
- en: Reducing Latency
  id: totrans-split-62
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: å‡å°‘å»¶è¿Ÿ
- en: Human conversations have extremely tight â€œtime-to-turn.â€ In-person conversations
    are especially rapid because we rely on visual as well as audio signals to determine
    when itâ€™s our time to participate in a conversation. The â€œaverageâ€ time-to-turn
    in a conversation can be as quick as 200ms. That means for a conversational agent
    to feel realistic, it needs an extremely quick turn around time for â€œtime to first
    spoken word.â€
  id: totrans-split-63
  prefs: []
  type: TYPE_NORMAL
  zh: äººç±»å¯¹è¯æœ‰æå…¶ç´§å¯†çš„â€œå“åº”æ—¶é—´â€ã€‚é¢å¯¹é¢çš„å¯¹è¯å°¤å…¶è¿…é€Ÿï¼Œå› ä¸ºæˆ‘ä»¬ä¾èµ–è§†è§‰å’Œå¬è§‰ä¿¡å·æ¥ç¡®å®šæˆ‘ä»¬ä½•æ—¶å‚ä¸å¯¹è¯ã€‚å¯¹è¯ä¸­çš„â€œå¹³å‡â€å“åº”æ—¶é—´å¯ä»¥éå¸¸å¿«ï¼Œåªæœ‰ 200
    æ¯«ç§’ã€‚è¿™æ„å‘³ç€ä¸ºäº†ä½¿å¯¹è¯ä»£ç†ç¨‹åºæ„Ÿè§‰çœŸå®ï¼Œå®ƒéœ€è¦æå¿«çš„â€œç¬¬ä¸€ä¸ªå‘è¨€è¯çš„æ—¶é—´â€ã€‚
- en: After posting my original demonstration, I already knew there were some very
    easy optimizations I could make, so I set out to improve the average latency of
    my implementation as much as possible in a short amount of time. First, I needed
    at least some method for determining whether an optimization worked. My rudimentary
    approach was to use [JavaScript Performance Timers](https://developer.mozilla.org/en-US/docs/Web/API/Performance/now)
    and logging. Basically, I computed a `startTime` from the exact moment an audio
    recording stopped and an `endTime` from the exact moment an audio output started,
    and then I logged that time to the console.
  id: totrans-split-64
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨å‘å¸ƒæˆ‘çš„åŸå§‹æ¼”ç¤ºä¹‹åï¼Œæˆ‘å·²ç»çŸ¥é“æœ‰ä¸€äº›éå¸¸ç®€å•çš„ä¼˜åŒ–æ–¹æ³•å¯ä»¥å®ç°ï¼Œå› æ­¤æˆ‘ç€æ‰‹å°½å¯èƒ½åœ¨çŸ­æ—¶é—´å†…æ”¹å–„å®ç°çš„å¹³å‡å»¶è¿Ÿã€‚é¦–å…ˆï¼Œæˆ‘éœ€è¦è‡³å°‘ä¸€ç§æ–¹æ³•æ¥ç¡®å®šä¼˜åŒ–æ˜¯å¦å¥æ•ˆã€‚æˆ‘çš„åˆæ­¥æ–¹æ³•æ˜¯ä½¿ç”¨[JavaScript
    æ€§èƒ½è®¡æ—¶å™¨](https://developer.mozilla.org/en-US/docs/Web/API/Performance/now)å’Œæ—¥å¿—è®°å½•ã€‚åŸºæœ¬ä¸Šï¼Œæˆ‘ä»éŸ³é¢‘å½•åˆ¶åœæ­¢çš„ç¡®åˆ‡æ—¶åˆ»è®¡ç®—ä¸€ä¸ª
    `startTime`ï¼Œä»éŸ³é¢‘è¾“å‡ºå¼€å§‹çš„ç¡®åˆ‡æ—¶åˆ»è®¡ç®—ä¸€ä¸ª `endTime`ï¼Œç„¶åå°†è¯¥æ—¶é—´è®°å½•åˆ°æ§åˆ¶å°ã€‚
- en: This is a very unscientific way of doing business. In the future, Iâ€™d like to
    implement a much more involved profiling and benchmarking methodology. For this
    process though, it worked well enough.
  id: totrans-split-65
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ˜¯ä¸€ç§éå¸¸ä¸ç§‘å­¦çš„åšæ³•ã€‚å°†æ¥ï¼Œæˆ‘å¸Œæœ›å®ç°ä¸€ä¸ªæ›´åŠ æ·±å…¥çš„åˆ†æå’ŒåŸºå‡†æµ‹è¯•æ–¹æ³•ã€‚å°½ç®¡å¦‚æ­¤ï¼Œåœ¨è¿™ä¸ªè¿‡ç¨‹ä¸­ï¼Œå®ƒè¡¨ç°å¾—è¶³å¤Ÿå¥½ã€‚
- en: 'Next, I considered all of the areas that could introduce latency into the pipeline.
    From the moment a recording stops, these are all of the steps we take:'
  id: totrans-split-66
  prefs: []
  type: TYPE_NORMAL
  zh: æ¥ä¸‹æ¥ï¼Œæˆ‘è€ƒè™‘äº†å¯èƒ½åœ¨æµæ°´çº¿ä¸­å¼•å…¥å»¶è¿Ÿçš„æ‰€æœ‰é¢†åŸŸã€‚ä»å½•éŸ³åœæ­¢çš„é‚£ä¸€åˆ»èµ·ï¼Œè¿™äº›éƒ½æ˜¯æˆ‘ä»¬æ‰€é‡‡å–çš„æ‰€æœ‰æ­¥éª¤ï¼š
- en: Pre-process recording by converting to PCM buffer, and then converting endianness
    to match server (if necessary)
  id: totrans-split-67
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: é€šè¿‡è½¬æ¢ä¸º PCM ç¼“å†²åŒºé¢„å¤„ç†å½•éŸ³ï¼Œç„¶åè½¬æ¢å­—èŠ‚åºä»¥åŒ¹é…æœåŠ¡å™¨ï¼ˆå¦‚æœéœ€è¦ï¼‰
- en: Upload buffer to server
  id: totrans-split-68
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: å°†ç¼“å†²åŒºä¸Šä¼ åˆ°æœåŠ¡å™¨
- en: Perform speech to text on buffer to produce text
  id: totrans-split-69
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: å¯¹ç¼“å†²åŒºæ‰§è¡Œè¯­éŸ³è½¬æ–‡å­—ä»¥ç”Ÿæˆæ–‡æœ¬
- en: Send text to LLM
  id: totrans-split-70
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: å‘é€æ–‡æœ¬åˆ° LLM
- en: Send streamed text to ElevenLabs
  id: totrans-split-71
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: å‘é€æµå¼æ–‡æœ¬åˆ° ElevenLabs
- en: Receive streamed audio from ElevenLabs
  id: totrans-split-72
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: æ¥æ”¶æ¥è‡ª ElevenLabs çš„æµå¼éŸ³é¢‘
- en: Broadcast audio to client
  id: totrans-split-73
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: å‘å®¢æˆ·ç«¯å¹¿æ’­éŸ³é¢‘
- en: Decode audio on client and play
  id: totrans-split-74
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: åœ¨å®¢æˆ·ç«¯è§£ç éŸ³é¢‘å¹¶æ’­æ”¾
- en: Thatâ€™s a lot of steps that can introduce latency, including potentially 3 (in
    our case 2 because we own the STT pipeline) network calls.
  id: totrans-split-75
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ˜¯ä¸€ä¸ªå¯èƒ½å¼•å…¥å»¶è¿Ÿçš„å¾ˆå¤šæ­¥éª¤ï¼ŒåŒ…æ‹¬æ½œåœ¨çš„ 3 ä¸ªï¼ˆåœ¨æˆ‘ä»¬çš„æƒ…å†µä¸‹æ˜¯ 2 ä¸ªï¼Œå› ä¸ºæˆ‘ä»¬æ‹¥æœ‰ STT æµæ°´çº¿ï¼‰ç½‘ç»œè°ƒç”¨ã€‚
- en: Next, I wanted to esablish a â€œbaselineâ€ of performance. To demonstrate this
    iterative process, I did a baseline example on my M3 Mac CPU. Note that this is
    going to be slow relative to my previous demo because the previous demo runs on
    a GPU. The baseline performance I got from the original demo running on my mac
    was `4537 ms`. 4.5 seconds turn around time. Yikes. Lots of work to do.
  id: totrans-split-76
  prefs: []
  type: TYPE_NORMAL
  zh: æ¥ä¸‹æ¥ï¼Œæˆ‘æƒ³å»ºç«‹ä¸€ä¸ªæ€§èƒ½çš„â€œåŸºçº¿â€ã€‚ä¸ºäº†æ¼”ç¤ºè¿™ä¸ªè¿­ä»£è¿‡ç¨‹ï¼Œæˆ‘åœ¨æˆ‘çš„ M3 Mac CPU ä¸Šåšäº†ä¸€ä¸ªåŸºå‡†ç¤ºä¾‹ã€‚è¯·æ³¨æ„ï¼Œä¸ä¹‹å‰åœ¨ GPU ä¸Šè¿è¡Œçš„æ¼”ç¤ºç›¸æ¯”ï¼Œè¿™å°†ä¼šæ¯”è¾ƒæ…¢ã€‚åœ¨æˆ‘çš„
    Mac ä¸Šè¿è¡ŒåŸå§‹æ¼”ç¤ºçš„åŸºå‡†æ€§èƒ½ä¸º `4537 ms`ã€‚4.5 ç§’çš„åé¦ˆæ—¶é—´ã€‚å¤©å•Šã€‚è¿˜æœ‰å¾ˆå¤šå·¥ä½œè¦åšã€‚
- en: 'To start, I knew that the `SILENCE_TIMEOUT` used to wait for speech to end
    was rather long. For the original demo, I used 1000 ms, which basically means
    a speaker has to stop talking for a full second before weâ€™ll even start the long
    response process. After some trial and error, I figured 500 ms was a â€œpassableâ€
    hyperparameter. After adjusting this down, the latency change was almost exactly
    correlated to the dip: `4079 ms`.'
  id: totrans-split-77
  prefs: []
  type: TYPE_NORMAL
  zh: èµ·åˆï¼Œæˆ‘çŸ¥é“ç­‰å¾…è¯­éŸ³ç»“æŸçš„ `SILENCE_TIMEOUT` æ—¶é—´ç›¸å½“é•¿ã€‚åœ¨æœ€åˆçš„æ¼”ç¤ºä¸­ï¼Œæˆ‘ä½¿ç”¨äº† 1000 æ¯«ç§’ï¼Œè¿™åŸºæœ¬ä¸Šæ„å‘³ç€è¯´è¯è€…å¿…é¡»åœæ­¢è¯´è¯æ•´æ•´ä¸€ç§’é’Ÿï¼Œç„¶åæˆ‘ä»¬æ‰ä¼šå¼€å§‹é•¿æ—¶é—´çš„å“åº”è¿‡ç¨‹ã€‚ç»è¿‡ä¸€äº›è¯•éªŒå’Œé”™è¯¯ï¼Œæˆ‘å‘ç°
    500 æ¯«ç§’æ˜¯ä¸€ä¸ªâ€œå¯ä»¥æ¥å—â€çš„è¶…å‚æ•°ã€‚åœ¨å°†è¿™ä¸ªå€¼è°ƒæ•´ä¸‹æ¥åï¼Œå»¶è¿Ÿå˜åŒ–å‡ ä¹å®Œå…¨ä¸ä¸‹é™çš„ç›¸å…³æ€§æˆæ­£æ¯”ï¼š`4079 ms`ã€‚
- en: 'I had a hunch that my text to speech pipeline was not efficient. Fortunately,
    ElevenLabs gives us a nice [Latency Guide](https://elevenlabs.io/docs/api-reference/reducing-latency).
    The first suggestion is to use their turbo model by specifying `eleven_turbo_v2`.
    I set that and we got a slight performance boost: `4014 ms`.'
  id: totrans-split-78
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘æœ‰ä¸€ç§é¢„æ„Ÿæˆ‘çš„æ–‡æœ¬åˆ°è¯­éŸ³ç®¡é“æ•ˆç‡ä¸é«˜ã€‚å¹¸è¿çš„æ˜¯ï¼ŒElevenLabs ç»™äº†æˆ‘ä»¬ä¸€ä¸ªå¾ˆå¥½çš„ [å»¶è¿ŸæŒ‡å—](https://elevenlabs.io/docs/api-reference/reducing-latency)ã€‚ç¬¬ä¸€ä¸ªå»ºè®®æ˜¯é€šè¿‡æŒ‡å®š
    `eleven_turbo_v2` æ¥ä½¿ç”¨ä»–ä»¬çš„ turbo æ¨¡å‹ã€‚æˆ‘è®¾ç½®äº†è¿™ä¸ªå€¼ï¼Œæˆ‘ä»¬å¾—åˆ°äº†è½»å¾®çš„æ€§èƒ½æå‡ï¼š`4014 ms`ã€‚
- en: 'Next, they suggest adding `optimize_streaming_latency`. I set the value to
    `3` and we get: `3791 ms`. Their next suggestion is to use a pre-made voice. I
    actually didnâ€™t realize until much later that I was not using a pre-made voice
    so I donâ€™t have a comparison for how that change impacted latency.'
  id: totrans-split-79
  prefs: []
  type: TYPE_NORMAL
  zh: æ¥ä¸‹æ¥ï¼Œä»–ä»¬å»ºè®®æ·»åŠ  `optimize_streaming_latency`ã€‚æˆ‘å°†å€¼è®¾ç½®ä¸º `3`ï¼Œæˆ‘ä»¬å¾—åˆ°äº†ï¼š`3791 ms`ã€‚ä»–ä»¬çš„ä¸‹ä¸€ä¸ªå»ºè®®æ˜¯ä½¿ç”¨é¢„åˆ¶çš„è¯­éŸ³ã€‚ç›´åˆ°å¾ˆä¹…ä»¥åæˆ‘æ‰æ„è¯†åˆ°æˆ‘æ²¡æœ‰ä½¿ç”¨é¢„åˆ¶çš„è¯­éŸ³ï¼Œæ‰€ä»¥æˆ‘æ²¡æœ‰æ¯”è¾ƒè¿™ç§å˜åŒ–å¯¹å»¶è¿Ÿçš„å½±å“ã€‚
- en: 'Now it says to limit closing WebSocket connections. my current implementation
    opens a connection everytime it speaks â€“ which is not good. Basically every â€œturnâ€
    has to establish a new websocket connection. Additionally, ElevenLabs has a timeout
    of 20s from when you connect. So you need to send a message at least every 20s.
    I considered 2 options at this point:'
  id: totrans-split-80
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨å»ºè®®é™åˆ¶å…³é—­ WebSocket è¿æ¥ã€‚æˆ‘çš„å½“å‰å®ç°æ¯æ¬¡è®²è¯éƒ½ä¼šæ‰“å¼€ä¸€ä¸ªè¿æ¥ï¼Œè¿™ä¸æ˜¯ä¸€ä¸ªå¥½çš„åšæ³•ã€‚åŸºæœ¬ä¸Šæ¯ä¸€ä¸ªâ€œå›åˆâ€éƒ½éœ€è¦å»ºç«‹ä¸€ä¸ªæ–°çš„ WebSocket
    è¿æ¥ã€‚æ­¤å¤–ï¼ŒElevenLabs ä»è¿æ¥å¼€å§‹æœ‰ 20 ç§’çš„è¶…æ—¶æ—¶é—´ã€‚å› æ­¤ï¼Œä½ éœ€è¦è‡³å°‘æ¯ 20 ç§’å‘é€ä¸€æ¡æ¶ˆæ¯ã€‚åœ¨è¿™ä¸€ç‚¹ä¸Šï¼Œæˆ‘è€ƒè™‘äº†ä¸¤ä¸ªé€‰é¡¹ï¼š
- en: Open a global WebSocket connection, or maybe even a pool of connections, and
    try to keep the connection alive. But that seems really wasteful, and I donâ€™t
    think is the intended use of their API
  id: totrans-split-81
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: æ‰“å¼€ä¸€ä¸ªå…¨å±€ WebSocket è¿æ¥ï¼Œæˆ–è€…ç”šè‡³æ˜¯ä¸€ä¸ªè¿æ¥æ± ï¼Œå¹¶å°è¯•ä¿æŒè¿æ¥æ´»åŠ¨ã€‚ä½†è¿™ä¼¼ä¹éå¸¸æµªè´¹ï¼Œå¹¶ä¸”æˆ‘è®¤ä¸ºè¿™ä¸æ˜¯ä»–ä»¬ API çš„é¢„æœŸä½¿ç”¨æ–¹å¼ã€‚
- en: Open a WebSocket connection when convo starts. We donâ€™t have to worry about
    20s pauses
  id: totrans-split-82
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: å¼€å§‹å¯¹è¯æ—¶æ‰“å¼€ä¸€ä¸ª WebSocket è¿æ¥ã€‚æˆ‘ä»¬ä¸å¿…æ‹…å¿ƒ 20 ç§’çš„æš‚åœã€‚
- en: I decided to go with option 2, but I still think there are some drawbacks and
    considerations for a production system. The implementation I used opens a websocket
    connection on first â€œspeakâ€ and stores the connection PID as an assign in the
    LiveView socket. If you have a system with potentially many concurrent users speaking,
    you run the risk of creating a potentially unbounded number of connections. A
    more robust solution would probably use connection pools; however, Iâ€™m not really
    worried about traffic or scaling here.
  id: totrans-split-83
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘å†³å®šé€‰æ‹©ç¬¬äºŒä¸ªé€‰é¡¹ï¼Œä½†æˆ‘ä»ç„¶è®¤ä¸ºåœ¨ç”Ÿäº§ç³»ç»Ÿä¸­æœ‰ä¸€äº›ç¼ºç‚¹å’Œéœ€è¦è€ƒè™‘çš„åœ°æ–¹ã€‚æˆ‘ä½¿ç”¨çš„å®ç°åœ¨ç¬¬ä¸€æ¬¡â€œè®²è¯â€æ—¶æ‰“å¼€ WebSocket è¿æ¥ï¼Œå¹¶å°†è¿æ¥ PID
    å­˜å‚¨ä¸º LiveView socket ä¸­çš„ä¸€ä¸ªåˆ†é…ã€‚å¦‚æœä½ çš„ç³»ç»Ÿå¯èƒ½æœ‰è®¸å¤šå¹¶å‘ç”¨æˆ·åœ¨è®²è¯ï¼Œä½ å°±æœ‰å¯èƒ½åˆ›å»ºä¸€ä¸ªæ½œåœ¨æ— é™æ•°é‡çš„è¿æ¥çš„é£é™©ã€‚ä¸€ä¸ªæ›´å¥å£®çš„è§£å†³æ–¹æ¡ˆå¯èƒ½ä¼šä½¿ç”¨è¿æ¥æ± ï¼›ç„¶è€Œï¼Œåœ¨è¿™é‡Œï¼Œæˆ‘å¹¶ä¸çœŸçš„æ‹…å¿ƒæµé‡æˆ–æ‰©å±•é—®é¢˜ã€‚
- en: While adding this optimization, I struggled a bit because ElevenLabs would send
    the first frame back, then cut off. Then I realized that it was waiting to generate
    becuase it thought I was going to send more frames. So I needed to â€œflushâ€ the
    generation after I finished sending my tokens. This also seemed to fix unnatural
    audio problems I was having. After applying this optimization, our time to first
    spoken word was slightly lower in the `3700 ms` range.
  id: totrans-split-84
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æ·»åŠ è¿™ä¸ªä¼˜åŒ–æ—¶ï¼Œæˆ‘é‡åˆ°äº†ä¸€äº›å›°éš¾ï¼Œå› ä¸º ElevenLabs ä¼šå‘é€ç¬¬ä¸€å¸§å›æ¥ï¼Œç„¶åå°±ä¸­æ–­äº†ã€‚åæ¥æˆ‘æ„è¯†åˆ°å®ƒæ˜¯åœ¨ç­‰å¾…ç”Ÿæˆï¼Œå› ä¸ºå®ƒä»¥ä¸ºæˆ‘ä¼šå‘é€æ›´å¤šçš„å¸§ã€‚æ‰€ä»¥æˆ‘éœ€è¦åœ¨å‘é€å®Œæˆ‘çš„ä»¤ç‰Œåâ€œåˆ·æ–°â€ç”Ÿæˆã€‚è¿™ä¼¼ä¹ä¹Ÿè§£å†³äº†æˆ‘é‡åˆ°çš„ä¸è‡ªç„¶çš„éŸ³é¢‘é—®é¢˜ã€‚åº”ç”¨è¿™ä¸€ä¼˜åŒ–åï¼Œæˆ‘ä»¬çš„é¦–æ¬¡è¯´è¯æ—¶é—´åœ¨
    `3700 ms` å·¦å³ã€‚
- en: After perusing their docs a bit more, I learned that ElevenLabs will send PCM
    buffers instead of MP3\. Web Browserâ€™s have to decode MP3 to PCM, which potentially
    introduces some overhead. One drawback is that you need to be on the independent
    creator tier to receive PCM instead of MP3\. Now, if youâ€™re wondering if I spent
    $99 to save some milliseconds for a meaningless demo, the answer is absolutely
    yes I did.
  id: totrans-split-85
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿›ä¸€æ­¥ç ”ç©¶ä»–ä»¬çš„æ–‡æ¡£åï¼Œæˆ‘äº†è§£åˆ°ElevenLabså°†å‘é€PCMç¼“å†²è€Œä¸æ˜¯MP3ã€‚Webæµè§ˆå™¨å¿…é¡»å°†MP3è§£ç ä¸ºPCMï¼Œè¿™å¯èƒ½ä¼šå¼•å…¥ä¸€äº›é¢å¤–å¼€é”€ã€‚ä¸€ä¸ªç¼ºç‚¹æ˜¯ï¼Œä½ éœ€è¦å¤„äºç‹¬ç«‹åˆ›ä½œè€…å±‚çº§æ‰èƒ½æ¥æ”¶PCMè€Œä¸æ˜¯MP3ã€‚ç°åœ¨ï¼Œå¦‚æœä½ æƒ³çŸ¥é“æˆ‘æ˜¯å¦èŠ±äº†99ç¾å…ƒæ¥èŠ‚çœä¸€äº›æ¯«ç§’çš„æ— æ„ä¹‰æ¼”ç¤ºï¼Œç­”æ¡ˆæ˜¯ç»å¯¹æ˜¯çš„ã€‚
- en: At this point, I believe Iâ€™ve exhausted a lot of the â€œeasyâ€ optimizations for
    TTS latency. One thing that does bother me about the ElevenLabs Websocket API
    is that thereâ€™s no way to receive binary payloads instead of JSON payloads. This
    is probably because they send alignment data, but Iâ€™m not using the alignment
    data here. When handling an incoming frame from their API we have to first decode
    the JSON, and then decode the Base64 encoded audio buffer. Iâ€™m not sure what the
    latency impact is, but Iâ€™m sure we could shave *some* time by avoiding both of
    these conversions. I also think the Base64 representation results in slightly
    larger buffers which could impact network latency.
  id: totrans-split-86
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™ä¸€ç‚¹ä¸Šï¼Œæˆ‘ç›¸ä¿¡æˆ‘å·²ç»ç”¨å®Œäº†å¾ˆå¤šå…³äºTTSå»¶è¿Ÿçš„â€œç®€å•â€ä¼˜åŒ–ã€‚æœ‰ä¸€ä»¶äº‹è®©æˆ‘ä¸å¤ªé«˜å…´çš„æ˜¯ï¼ŒElevenLabs Websocket APIæ²¡æœ‰åŠæ³•æ¥æ”¶äºŒè¿›åˆ¶è½½è·è€Œä¸æ˜¯JSONè½½è·ã€‚è¿™å¯èƒ½æ˜¯å› ä¸ºä»–ä»¬å‘é€äº†å¯¹é½æ•°æ®ï¼Œä½†æˆ‘åœ¨è¿™é‡Œæ²¡æœ‰ä½¿ç”¨å¯¹é½æ•°æ®ã€‚å½“å¤„ç†æ¥è‡ªä»–ä»¬APIçš„ä¼ å…¥å¸§æ—¶ï¼Œæˆ‘ä»¬å¿…é¡»é¦–å…ˆè§£ç JSONï¼Œç„¶åè§£ç Base64ç¼–ç çš„éŸ³é¢‘ç¼“å†²åŒºã€‚æˆ‘ä¸ç¡®å®šå»¶è¿Ÿçš„å½±å“æ˜¯ä»€ä¹ˆï¼Œä½†æˆ‘ç¡®ä¿¡å¦‚æœé¿å…è¿™ä¸¤ç§è½¬æ¢ï¼Œæˆ‘ä»¬å¯ä»¥èŠ‚çœä¸€äº›æ—¶é—´ã€‚æˆ‘è¿˜è®¤ä¸ºBase64è¡¨ç¤ºä¼šå¯¼è‡´ç¨å¤§çš„ç¼“å†²åŒºï¼Œè¿™å¯èƒ½ä¼šå½±å“ç½‘ç»œå»¶è¿Ÿã€‚
- en: 'The next area I looked to improve was the speech-to-text pipeline. I am using
    `Nx.Serving` specifically for Speech-to-Text. The benefit of this approach is
    that we can avoid an additional network call just for transcription. Of course,
    that assumes our transcription pipeline can run fast enough on our own hardware.
    XLA is notoriously slow on CPUs (itâ€™s getting better). The first â€œoptimizationâ€
    I did was to switch to my GPU: `2050 ms`'
  id: totrans-split-87
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘æ¥ä¸‹æ¥æƒ³è¦æ”¹è¿›çš„æ˜¯è¯­éŸ³è½¬æ–‡æœ¬ç®¡é“ã€‚æˆ‘ç‰¹åˆ«ä½¿ç”¨`Nx.Serving`æ¥è¿›è¡Œè¯­éŸ³è½¬æ–‡æœ¬ã€‚è¿™ç§æ–¹æ³•çš„å¥½å¤„æ˜¯æˆ‘ä»¬å¯ä»¥é¿å…å› ä¸ºè½¬å½•è€Œé¢å¤–è¿›è¡Œç½‘ç»œè°ƒç”¨ã€‚å½“ç„¶ï¼Œè¿™å‡è®¾æˆ‘ä»¬çš„è½¬å½•ç®¡é“åœ¨æˆ‘ä»¬è‡ªå·±çš„ç¡¬ä»¶ä¸Šè¿è¡Œè¶³å¤Ÿå¿«ã€‚XLAåœ¨CPUä¸Šæ˜¯å‡ºäº†åçš„æ…¢ï¼ˆæ­£åœ¨å˜å¾—æ›´å¥½ï¼‰ã€‚æˆ‘åšçš„ç¬¬ä¸€ä¸ªâ€œä¼˜åŒ–â€æ˜¯åˆ‡æ¢åˆ°æˆ‘çš„GPUï¼š`2050
    ms`
- en: And that right there is a bitter lesson, because itâ€™s the largest performance
    boost weâ€™re going to get.
  id: totrans-split-88
  prefs: []
  type: TYPE_NORMAL
  zh: è€Œè¿™æ­£æ˜¯ä¸€ä¸ªç—›è‹¦çš„æ•™è®­ï¼Œå› ä¸ºè¿™æ˜¯æˆ‘ä»¬èƒ½å¾—åˆ°çš„æœ€å¤§çš„æ€§èƒ½æå‡ã€‚
- en: 'Next, I realized the model isnâ€™t using F16, which can introduce some solid
    speed-ups: `1800 ms`. Now, there are probably some additional optimizations we
    could add to Nx and EXLA specifically. For example, we donâ€™t have a flash attention
    implementation. Of course, XLA does a great job of applying similar optimizations
    as a baseline, so Iâ€™m not sure how much it would help. Thereâ€™s also [fast JAX
    implementations of Whisper](https://github.com/sanchit-gandhi/whisper-jax) that
    claim up to 70x speed ups. One issue with a lof of these claimed speed-ups; however,
    is that they are almost **always** for long audio sequences. GPUs and TPUs do
    well with large batch sizes and sequence lengths, but not for batch size 1 and
    short sequence lengths like we care about in this implementation. One day I may
    go down the performance hole of fast batch size 1 transcription, but today is
    not that day.'
  id: totrans-split-89
  prefs: []
  type: TYPE_NORMAL
  zh: æ¥ä¸‹æ¥ï¼Œæˆ‘æ„è¯†åˆ°æ¨¡å‹æ²¡æœ‰ä½¿ç”¨F16ï¼Œè¿™å¯èƒ½ä¼šå¼•å…¥ä¸€äº›æ˜æ˜¾çš„åŠ é€Ÿï¼š`1800 ms`ã€‚ç°åœ¨ï¼Œæˆ‘ä»¬å¯èƒ½è¿˜å¯ä»¥ä¸ºNxå’ŒEXLAæ·»åŠ ä¸€äº›é¢å¤–çš„ä¼˜åŒ–ã€‚ä¾‹å¦‚ï¼Œæˆ‘ä»¬æ²¡æœ‰å¿«é€Ÿçš„æ³¨æ„åŠ›å®ç°ã€‚å½“ç„¶ï¼ŒXLAåœ¨åº”ç”¨ç±»ä¼¼ä¼˜åŒ–ä½œä¸ºåŸºçº¿æ—¶è¡¨ç°å¾—å¾ˆå‡ºè‰²ï¼Œæ‰€ä»¥æˆ‘ä¸ç¡®å®šå®ƒä¼šå¸®åŠ©å¤šå°‘ã€‚è¿˜æœ‰[Whisperçš„å¿«é€ŸJAXå®ç°](https://github.com/sanchit-gandhi/whisper-jax)ï¼Œå£°ç§°å¯ä»¥åŠ é€Ÿè¾¾åˆ°70å€ã€‚ç„¶è€Œï¼Œè¿™äº›å£°ç§°çš„åŠ é€Ÿå‡ ä¹æ€»æ˜¯é’ˆå¯¹é•¿éŸ³é¢‘åºåˆ—ã€‚GPUå’ŒTPUåœ¨å¤§æ‰¹é‡å’Œåºåˆ—é•¿åº¦æ–¹é¢è¡¨ç°è‰¯å¥½ï¼Œä½†ä¸é€‚åˆæˆ‘ä»¬åœ¨è¿™ä¸ªå®ç°ä¸­å…³å¿ƒçš„å•æ‰¹é‡å’ŒçŸ­åºåˆ—é•¿åº¦ã€‚æœ‰ä¸€å¤©æˆ‘å¯èƒ½ä¼šæ·±å…¥ç ”ç©¶æ‰¹é‡å¤§å°ä¸º1çš„å¿«é€Ÿè½¬å½•æ€§èƒ½ï¼Œä½†ä»Šå¤©ä¸æ˜¯é‚£ä¸€å¤©ã€‚
- en: At this point, I had moved on to improving some of the failure modes of my demo.
    While doing so, I learned much more about audio than I had previously known, and
    realized that the configuration I used to record audio can significantly improve
    whisper performance as well. Turns out thereâ€™s a [nice guide](https://dev.to/mxro/optimise-openai-whisper-api-audio-format-sampling-rate-and-quality-29fj)
    of somebody discussing parameters that work. Specifically, you should use 16 kHz
    sample rate for transcriptions. Reducing the sample rate also should reduce network
    overhead because we have less data, but it could reduce quality of the transcription.
    Oh well. Additionally, I realized I wasnâ€™t using a pre-made ElevenLabs voice.
    After introducing both of these optimizations, I was able to achieve `1520 ms`
    turn time.
  id: totrans-split-90
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™ä¸€ç‚¹ä¸Šï¼Œæˆ‘å·²ç»å¼€å§‹æ”¹è¿›æˆ‘çš„æ¼”ç¤ºçš„ä¸€äº›å¤±è´¥æ¨¡å¼ã€‚åœ¨è¿™æ ·åšçš„è¿‡ç¨‹ä¸­ï¼Œæˆ‘å­¦åˆ°äº†æ¯”ä»¥å‰æ›´å¤šçš„å…³äºéŸ³é¢‘çš„çŸ¥è¯†ï¼Œå¹¶æ„è¯†åˆ°æˆ‘ç”¨æ¥å½•åˆ¶éŸ³é¢‘çš„é…ç½®å¯ä»¥æ˜¾è‘—æé«˜Whisperçš„æ€§èƒ½ã€‚ç»“æœå‘ç°æœ‰ä¸€ä¸ª[ä¸é”™çš„æŒ‡å—](https://dev.to/mxro/optimise-openai-whisper-api-audio-format-sampling-rate-and-quality-29fj)ä»‹ç»äº†ä¸€äº›å¯ä»¥å·¥ä½œçš„å‚æ•°ã€‚å…·ä½“æ¥è¯´ï¼Œåº”è¯¥ä½¿ç”¨16kHzçš„é‡‡æ ·ç‡è¿›è¡Œè½¬å½•ã€‚å‡å°‘é‡‡æ ·ç‡è¿˜åº”è¯¥å‡å°‘ç½‘ç»œå¼€é”€ï¼Œå› ä¸ºæ•°æ®é‡å‡å°‘äº†ï¼Œä½†å¯èƒ½ä¼šé™ä½è½¬å½•çš„è´¨é‡ã€‚å“¦ï¼Œå¥½å§ã€‚æ­¤å¤–ï¼Œæˆ‘æ„è¯†åˆ°æˆ‘æ²¡æœ‰ä½¿ç”¨é¢„åˆ¶çš„ElevenLabsè¯­éŸ³ã€‚åœ¨å¼•å…¥äº†è¿™ä¸¤ä¸ªä¼˜åŒ–æªæ–½åï¼Œæˆ‘èƒ½å¤Ÿå®ç°`1520
    ms`çš„è½¬æ¢æ—¶é—´ã€‚
- en: 'Finally, I realized I was doing all of my benchmarks on a development server.
    I switched my phoenix environment from `dev` to `prod` and got: `1375 ms`. So,
    with all of these optimizations weâ€™re sitting at about 1.3s turn around time in
    a conversation. When conversing, it starts to feel somewhat close to natural.
    I should also point out that this is also running over Tailscale, so there is
    about 100 ms ping between my Mac and the server running on my GPU. When I run
    this locally on my GPU, I can consistently get about `1000 ms` and sometimes `900
    ms` turn around time. Still, unfortunately, this does not match Retellâ€™s latency.
    According to them, they are able to achieve `800 ms` consistently. I have some
    musings at the end about how this is possible.'
  id: totrans-split-91
  prefs: []
  type: TYPE_NORMAL
  zh: æœ€åï¼Œæˆ‘æ„è¯†åˆ°æˆ‘æ‰€æœ‰çš„åŸºå‡†æµ‹è¯•éƒ½æ˜¯åœ¨å¼€å‘æœåŠ¡å™¨ä¸Šè¿›è¡Œçš„ã€‚æˆ‘å°†æˆ‘çš„Phoenixç¯å¢ƒä»`dev`åˆ‡æ¢åˆ°`prod`åå¾—åˆ°äº†ï¼š`1375 ms`ã€‚å› æ­¤ï¼Œé€šè¿‡æ‰€æœ‰è¿™äº›ä¼˜åŒ–ï¼Œæˆ‘ä»¬åœ¨å¯¹è¯ä¸­çš„å“åº”æ—¶é—´çº¦ä¸º1.3ç§’ã€‚åœ¨å¯¹è¯ä¸­ï¼Œå¼€å§‹æ„Ÿè§‰æ¥è¿‘è‡ªç„¶äº†ã€‚æˆ‘è¿˜åº”è¯¥æŒ‡å‡ºï¼Œè¿™ä¹Ÿæ˜¯åœ¨Tailscaleä¸Šè¿è¡Œçš„ï¼Œæ‰€ä»¥æˆ‘çš„Macå’Œè¿è¡Œåœ¨GPUä¸Šçš„æœåŠ¡å™¨ä¹‹é—´å¤§çº¦æœ‰100msçš„pingã€‚å½“æˆ‘åœ¨æˆ‘çš„GPUä¸Šæœ¬åœ°è¿è¡Œæ—¶ï¼Œæˆ‘å¯ä»¥ç¨³å®šåœ°å¾—åˆ°çº¦`1000
    ms`ï¼Œæœ‰æ—¶`900 ms`çš„è½¬æ¢æ—¶é—´ã€‚ä¸å¹¸çš„æ˜¯ï¼Œè¿™ä»ç„¶æ— æ³•ä¸Retellçš„å»¶è¿ŸåŒ¹é…ã€‚æ®ä»–ä»¬ç§°ï¼Œä»–ä»¬èƒ½å¤Ÿç¨³å®šåœ°è¾¾åˆ°`800 ms`ã€‚å…³äºè¿™å¦‚ä½•å¯èƒ½çš„é—®é¢˜ï¼Œæˆ‘æœ€åæœ‰äº›æ„Ÿæƒ³ã€‚
- en: I believe the biggest area I could improve the implementation is to use a better
    VAD implementation that relies on small rolling windows of activity rather than
    frames. We could probably get away with using 20-30 ms windows, which could theoretically
    offer a 480 ms latency improvement. I would like to eventually explore this.
  id: totrans-split-92
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘è®¤ä¸ºæˆ‘å¯ä»¥æ”¹è¿›å®ç°çš„æœ€å¤§é¢†åŸŸæ˜¯ä½¿ç”¨ä¸€ä¸ªæ›´å¥½çš„VADå®ç°ï¼Œå®ƒä¾èµ–äºå°çš„æ´»åŠ¨æ»šåŠ¨çª—å£è€Œä¸æ˜¯å¸§ã€‚æˆ‘ä»¬å¯èƒ½å¯ä»¥ä½¿ç”¨20-30æ¯«ç§’çš„çª—å£ï¼Œè¿™ç†è®ºä¸Šå¯ä»¥æä¾›480æ¯«ç§’çš„å»¶è¿Ÿæ”¹è¿›ã€‚æˆ‘æœ€ç»ˆæƒ³æ¢ç´¢è¿™ä¸€ç‚¹ã€‚
- en: In all honesty though, I think that is a significant improvement, and I could
    *probably* stop right here and be done with it.
  id: totrans-split-93
  prefs: []
  type: TYPE_NORMAL
  zh: è¯´å®è¯ï¼Œæˆ‘è®¤ä¸ºè¿™æ˜¯ä¸€ä¸ªæ˜¾è‘—çš„æ”¹è¿›ï¼Œæˆ‘*å¯èƒ½*å¯ä»¥åœ¨è¿™é‡Œåœä¸‹æ¥å¹¶å®Œæˆå®ƒã€‚
- en: If I were to keep going, I would explore using a local LLM with Nx and Bumblebee.
    Nx and Bumblebee support LLMs like Mistral and Llama out-of-the box. And our text
    generation servings support streaming. That means we can possibly eliminate any
    network latency to OpenAI, and instead run 2 of the 3 models locally. One issue
    with this is that Nx currently does not have any quantized inference support (itâ€™s
    coming I promise), so my single 4090 is not sufficient to deploy both Whisper
    and Mistral. Fortunately, the folks at [Fly.io](https://fly.io/gpu) were kind
    enough to give me access to some 80GB A100s. I will post a demo when I get one
    deployed ğŸ™‚
  id: totrans-split-94
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœæˆ‘ç»§ç»­ä¸‹å»ï¼Œæˆ‘å°†æ¢ç´¢ä½¿ç”¨æœ¬åœ°LLMä¸Nxå’ŒBumblebeeã€‚Nxå’ŒBumblebeeæ”¯æŒåƒMistralå’ŒLlamaè¿™æ ·çš„LLMå¼€ç®±å³ç”¨ã€‚æˆ‘ä»¬çš„æ–‡æœ¬ç”ŸæˆæœåŠ¡æ”¯æŒæµå¼å¤„ç†ã€‚è¿™æ„å‘³ç€æˆ‘ä»¬å¯èƒ½å¯ä»¥æ¶ˆé™¤åˆ°OpenAIçš„ä»»ä½•ç½‘ç»œå»¶è¿Ÿï¼Œè€Œæ˜¯åœ¨æœ¬åœ°è¿è¡Œå…¶ä¸­çš„3ä¸ªæ¨¡å‹ä¸­çš„2ä¸ªã€‚å…¶ä¸­ä¸€ä¸ªé—®é¢˜æ˜¯ï¼ŒNxç›®å‰æ²¡æœ‰ä»»ä½•é‡åŒ–æ¨ç†æ”¯æŒï¼ˆæˆ‘æ‰¿è¯ºè¿™æ˜¯åœ¨è¿›è¡Œä¸­ï¼‰ï¼Œæ‰€ä»¥æˆ‘çš„å•ä¸ª4090ä¸è¶³ä»¥éƒ¨ç½²Whisperå’ŒMistralã€‚å¹¸è¿çš„æ˜¯ï¼Œ[Fly.io](https://fly.io/gpu)çš„äººä»¬å¾ˆå‹å¥½åœ°ç»™äº†æˆ‘ä¸€äº›80GBçš„A100ã€‚æˆ‘å°†åœ¨éƒ¨ç½²åå‘å¸ƒæ¼”ç¤º
    ğŸ™‚
- en: Maybe one day I will implement StyleTTS2 and see how efficient we can get with
    an entirely local inference pipeline.
  id: totrans-split-95
  prefs: []
  type: TYPE_NORMAL
  zh: ä¹Ÿè®¸æœ‰ä¸€å¤©æˆ‘ä¼šå®ç°StyleTTS2ï¼Œå¹¶çœ‹çœ‹æˆ‘ä»¬èƒ½é€šè¿‡å®Œå…¨æœ¬åœ°æ¨ç†æµæ°´çº¿æœ‰å¤šé«˜æ•ˆã€‚
- en: Improving the Conversational Experience
  id: totrans-split-96
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æ”¹å–„å¯¹è¯ä½“éªŒ
- en: Some people pointed out that my original demo did not have the same conversational
    experience as Retellâ€™s, and they are absolutely right. Aside from latency, mine
    was prone to failure, picks up system sounds, picks up random noises like keyboard
    and mouse clicks, and doesnâ€™t do well with ambient noise. They also have support
    for backchanneling, fillers and interruptions which introduces some element of
    â€œrealnessâ€ when interacting with their agent.
  id: totrans-split-97
  prefs: []
  type: TYPE_NORMAL
  zh: æœ‰äº›äººæŒ‡å‡ºï¼Œæˆ‘çš„åŸå§‹æ¼”ç¤ºä¸ Retell çš„å¯¹è¯ä½“éªŒä¸åŒï¼Œä»–ä»¬å®Œå…¨æ­£ç¡®ã€‚é™¤äº†å»¶è¿Ÿä¹‹å¤–ï¼Œæˆ‘çš„ç³»ç»Ÿå®¹æ˜“å‡ºç°æ•…éšœï¼Œä¼šæ•æ‰åˆ°ç³»ç»Ÿå£°éŸ³ã€éšæœºçš„é”®ç›˜å’Œé¼ æ ‡ç‚¹å‡»å£°ï¼Œå¹¶ä¸”å¯¹ç¯å¢ƒå™ªéŸ³ä¸æ•æ„Ÿã€‚ä»–ä»¬è¿˜æ”¯æŒå›å£°ã€å¡«å……è¯­éŸ³å’Œä¸­æ–­ï¼Œè¿™åœ¨ä¸ä»–ä»¬çš„ä»£ç†è¿›è¡Œäº¤äº’æ—¶å¢åŠ äº†ä¸€äº›â€œçœŸå®æ„Ÿâ€å…ƒç´ ã€‚
- en: Now I didnâ€™t get around to adding backchannels or fillers, but I was able to
    make some slight improvements to the VAD algorithm I used, and I added support
    for interruptions.
  id: totrans-split-98
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨æˆ‘è¿˜æ²¡æ¥å¾—åŠæ·»åŠ å›å£°æˆ–å¡«å……è¯­éŸ³ï¼Œä½†æˆ‘èƒ½å¤Ÿå¯¹æˆ‘ä½¿ç”¨çš„ VAD ç®—æ³•è¿›è¡Œä¸€äº›å¾®å°çš„æ”¹è¿›ï¼Œå¹¶æ·»åŠ äº†å¯¹ä¸­æ–­çš„æ”¯æŒã€‚
- en: Fixing Some Failure Modes with VAD
  id: totrans-split-99
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: ä½¿ç”¨ VAD ä¿®å¤ä¸€äº›æ•…éšœæ¨¡å¼
- en: 'The first failure mode that seems to happen is echo from the system sounds.
    Nero always records and will start transcribing after audio spikes over a certain
    threshold. After some digging into the `getUserMedia` API, I found options for
    `echoCancellation`, `noiseSuppression`, and `autoGainControl`. This is the same
    point I realized that I could specify the microphone sample rate for the optimization
    I could added from the last section. Most of these options are on by default depending
    on your browser, but I added them explicitly anyway:'
  id: totrans-split-100
  prefs: []
  type: TYPE_NORMAL
  zh: çœ‹èµ·æ¥ç¬¬ä¸€ä¸ªå¤±è´¥æ¨¡å¼æ˜¯ç³»ç»Ÿå£°éŸ³çš„å›å£°ã€‚Nero æ€»æ˜¯å½•åˆ¶å¹¶åœ¨éŸ³é¢‘è¶…è¿‡æŸä¸ªé˜ˆå€¼åå¼€å§‹è½¬å½•ã€‚åœ¨æ·±å…¥ç ”ç©¶ `getUserMedia` API åï¼Œæˆ‘å‘ç°äº†
    `echoCancellation`ã€`noiseSuppression` å’Œ `autoGainControl` çš„é€‰é¡¹ã€‚æˆ‘æ„è¯†åˆ°ï¼Œä¸ºäº†ä¼˜åŒ–ï¼Œæˆ‘å¯ä»¥æŒ‡å®šéº¦å…‹é£çš„é‡‡æ ·ç‡ã€‚è¿™æ˜¯æˆ‘ä»ä¸Šä¸€èŠ‚æ·»åŠ çš„ä¼˜åŒ–ç‚¹ã€‚è¿™äº›é€‰é¡¹å¤§å¤šæ•°æƒ…å†µä¸‹æ˜¯æµè§ˆå™¨é»˜è®¤å¼€å¯çš„ï¼Œä½†æˆ‘è¿˜æ˜¯æ˜¾å¼åœ°æ·»åŠ äº†å®ƒä»¬ï¼š
- en: '[PRE15]'
  id: totrans-split-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Now that somewhat helped, but Nero still picks up itâ€™s own audio. This probably
    requires a more sophisticated solution, but I moved on to the next problem.
  id: totrans-split-102
  prefs: []
  type: TYPE_NORMAL
  zh: å°½ç®¡æœ‰äº›å¸®åŠ©ï¼Œä½† Nero ä»ç„¶ä¼šæ•æ‰åˆ°è‡ªå·±çš„éŸ³é¢‘ã€‚è¿™å¯èƒ½éœ€è¦æ›´å¤æ‚çš„è§£å†³æ–¹æ¡ˆï¼Œä½†æˆ‘ç»§ç»­å¤„ç†ä¸‹ä¸€ä¸ªé—®é¢˜ã€‚
- en: 'The second obvious failure mode is the fact that it picks up keyboard clicks,
    and the silence timeout is hard to tune. My first attempt to fix this was to â€œignoreâ€
    large spikes in audio by â€œsmoothingâ€ the volume at each frame:'
  id: totrans-split-103
  prefs: []
  type: TYPE_NORMAL
  zh: ç¬¬äºŒä¸ªæ˜æ˜¾çš„å¤±è´¥æ¨¡å¼æ˜¯å®ƒä¼šæ•æ‰é”®ç›˜ç‚¹å‡»å£°ï¼Œå¹¶ä¸”æ²‰é»˜è¶…æ—¶å¾ˆéš¾è°ƒèŠ‚ã€‚æˆ‘ç¬¬ä¸€æ¬¡å°è¯•ä¿®å¤è¿™ä¸ªé—®é¢˜æ˜¯é€šè¿‡åœ¨æ¯ä¸€å¸§ä¸­â€œå¹³æ»‘â€éŸ³é‡æ¥â€œå¿½ç•¥â€éŸ³é¢‘ä¸­çš„å¤§æ³¢åŠ¨ï¼š
- en: '[PRE16]'
  id: totrans-split-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Then, with some advice from [Paulo Valente](https://twitter.com/polvalente),
    I implemented a biquad filter to with a low and high-pass in order to filter audio
    to the range of human speech:'
  id: totrans-split-105
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åï¼Œåœ¨ [Paulo Valente](https://twitter.com/polvalente) çš„å»ºè®®ä¸‹ï¼Œæˆ‘å®æ–½äº†ä¸€ä¸ªåŒäºŒé˜¶æ»¤æ³¢å™¨ï¼Œä½¿ç”¨ä½é€šå’Œé«˜é€šæ»¤æ³¢å°†éŸ³é¢‘è¿‡æ»¤åˆ°äººç±»è¯­éŸ³çš„èŒƒå›´å†…ï¼š
- en: '[PRE17]'
  id: totrans-split-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: In practice, both of these solutions actually seemed to work decent, but they
    could absolutely be better. I know itâ€™s possible to improve the client-side filtering
    using a rolling-window that looks energy of the speaking frequences relative to
    energy of an entire sample. But, there are also machine learning models that perform
    VAD, and have `1ms` inference times. I realized that itâ€™s probably quicker to
    just send all of the data over the websocket in chunks, and perform VAD on the
    server. Iâ€™ll discuss that implementation a little later.
  id: totrans-split-107
  prefs: []
  type: TYPE_NORMAL
  zh: å®é™…ä¸Šï¼Œè¿™ä¸¤ç§è§£å†³æ–¹æ¡ˆä¼¼ä¹éƒ½æ•ˆæœä¸é”™ï¼Œä½†è‚¯å®šè¿˜æœ‰æ”¹è¿›çš„ç©ºé—´ã€‚æˆ‘çŸ¥é“å¯ä»¥é€šè¿‡ä½¿ç”¨ä¸€ä¸ªæ»šåŠ¨çª—å£æ¥æ”¹è¿›å®¢æˆ·ç«¯è¿‡æ»¤ï¼Œè¯¥çª—å£ä¼šæ ¹æ®å‘è¨€é¢‘ç‡çš„èƒ½é‡ç›¸å¯¹äºæ•´ä¸ªæ ·æœ¬çš„èƒ½é‡è¿›è¡Œåˆ¤æ–­ã€‚æ­¤å¤–ï¼Œè¿˜æœ‰ä¸€äº›æœºå™¨å­¦ä¹ æ¨¡å‹å¯ä»¥æ‰§è¡Œ
    VADï¼Œå¹¶ä¸”æ¨æ–­æ—¶é—´ä¸º `1ms`ã€‚æˆ‘æ„è¯†åˆ°ï¼Œé€šè¿‡åˆ†å—æ–¹å¼å°†æ‰€æœ‰æ•°æ®å‘é€åˆ° WebSocketï¼Œå¹¶åœ¨æœåŠ¡å™¨ç«¯æ‰§è¡Œ VAD å¯èƒ½æ›´å¿«ã€‚ç¨åæˆ‘ä¼šè¯¦ç»†è®¨è®ºè¿™ä¸ªå®ç°ã€‚
- en: Supporting Interruptions
  id: totrans-split-108
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: æ”¯æŒä¸­æ–­
- en: 'Next I wanted to add support for interruptions. In the Retell example, the
    speaker will cut off mid-speech if it detects that you are speaking. To implement
    this feature in Nero, I added a `pushEvent` to the Microphone hook which would
    push an `interrupt` event to the server anytime speech is detectected:'
  id: totrans-split-109
  prefs: []
  type: TYPE_NORMAL
  zh: æ¥ä¸‹æ¥ï¼Œæˆ‘æƒ³è¦æ·»åŠ ä¸­æ–­æ”¯æŒã€‚åœ¨ Retell çš„ä¾‹å­ä¸­ï¼Œå¦‚æœæ£€æµ‹åˆ°ä½ åœ¨è®²è¯ï¼Œè¯´è¯äººä¼šåœ¨ä¸­é€”ä¸­æ–­ã€‚ä¸ºäº†åœ¨ Nero ä¸­å®ç°è¿™ä¸€åŠŸèƒ½ï¼Œæˆ‘åœ¨éº¦å…‹é£é’©å­ä¸­æ·»åŠ äº†ä¸€ä¸ª
    `pushEvent`ï¼Œæ¯æ¬¡æ£€æµ‹åˆ°è¯­éŸ³æ—¶ä¼šå‘æœåŠ¡å™¨æ¨é€ä¸€ä¸ª `interrupt` äº‹ä»¶ï¼š
- en: '[PRE18]'
  id: totrans-split-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'The server handles this event and broadcasts an event to the TTS channel to
    stop speaking:'
  id: totrans-split-111
  prefs: []
  type: TYPE_NORMAL
  zh: æœåŠ¡å™¨å¤„ç†æ­¤äº‹ä»¶ï¼Œå¹¶å‘ TTS é€šé“å¹¿æ’­äº‹ä»¶ä»¥åœæ­¢è¯´è¯ï¼š
- en: '[PRE19]'
  id: totrans-split-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'And the channel handles the event by clearing out the output audio stream and
    queue:'
  id: totrans-split-113
  prefs: []
  type: TYPE_NORMAL
  zh: é€šé“é€šè¿‡æ¸…é™¤è¾“å‡ºéŸ³é¢‘æµå’Œé˜Ÿåˆ—æ¥å¤„ç†äº‹ä»¶ï¼š
- en: '[PRE20]'
  id: totrans-split-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: Unfortunately, this does create a race condition. Thereâ€™s a potential situation
    where a speaker interrupts and the speaking queue gets cleared on the client,
    but ElevenLabs is still streaming audio back to the server. The server is always
    going to just broadcast this info to the client, and as is the client will process
    it. This potentially creates a situation with weird continutations in the audio.
    To get around this, I refactored the TTS implementation so that each audio broadcast
    appends a 6 digit token to the payload. Then, all we need to do is keep the token
    in sync with the client and server. On the client, when processing the audio queue,
    it simply checks whether or not the token at the beginning of the payload matches,
    and if it doesnâ€™t it ignores that sample.
  id: totrans-split-115
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸å¹¸çš„æ˜¯ï¼Œè¿™ç¡®å®ä¼šé€ æˆç«æ€æ¡ä»¶ã€‚å¯èƒ½å‡ºç°çš„æƒ…å†µæ˜¯ï¼Œå‘è¨€è€…æ‰“æ–­å¹¶ä¸”å®¢æˆ·ç«¯ä¸Šçš„å‘è¨€é˜Ÿåˆ—è¢«æ¸…é™¤ï¼Œä½† ElevenLabs ä»åœ¨å‘æœåŠ¡å™¨æµå¼ä¼ è¾“éŸ³é¢‘ã€‚æœåŠ¡å™¨æ€»æ˜¯ä¼šå°†è¿™äº›ä¿¡æ¯å¹¿æ’­åˆ°å®¢æˆ·ç«¯ï¼Œå¹¶ä¸”å®¢æˆ·ç«¯ä¼šå¤„ç†å®ƒã€‚è¿™å¯èƒ½ä¼šå¯¼è‡´éŸ³é¢‘ä¸­å‡ºç°å¥‡æ€ªçš„è¿ç»­æ€§é—®é¢˜ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘é‡æ„äº†
    TTS å®ç°ï¼Œä½¿å¾—æ¯ä¸ªéŸ³é¢‘å¹¿æ’­éƒ½é™„åŠ ä¸€ä¸ª6ä½æ•°çš„ä»¤ç‰Œåˆ°è´Ÿè½½ä¸­ã€‚ç„¶åï¼Œæˆ‘ä»¬åªéœ€è¦ä¿æŒå®¢æˆ·ç«¯å’ŒæœåŠ¡å™¨çš„ä»¤ç‰ŒåŒæ­¥å³å¯ã€‚åœ¨å®¢æˆ·ç«¯å¤„ç†éŸ³é¢‘é˜Ÿåˆ—æ—¶ï¼Œåªéœ€æ£€æŸ¥è´Ÿè½½å¼€å¤´çš„ä»¤ç‰Œæ˜¯å¦åŒ¹é…ï¼Œå¦‚æœä¸åŒ¹é…åˆ™å¿½ç•¥è¯¥æ ·æœ¬ã€‚
- en: The limitation with this implementation is it does not update the chat transcript.
    Itâ€™s entirely possible because we have access to the alignment information from
    ElevenLabs, but I just didnâ€™t implement it at this time.
  id: totrans-split-116
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ç§å®ç°çš„é™åˆ¶æ˜¯å®ƒä¸ä¼šæ›´æ–°èŠå¤©è®°å½•ã€‚è¿™å®Œå…¨å¯èƒ½æ˜¯å› ä¸ºæˆ‘ä»¬å¯ä»¥ä» ElevenLabs è·å–å¯¹é½ä¿¡æ¯ï¼Œä½†æˆ‘ç›®å‰æ²¡æœ‰å®ç°å®ƒã€‚
- en: Time-based Hang Ups
  id: totrans-split-117
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: æ—¶é—´åŸºç¡€çš„æŒ‚èµ·
- en: 'Another thing the Retell demo has support for is cues and hang ups after a
    duration of silence. If you are silent for too long, youâ€™ll get a cue from the
    AI speaker asking you if youâ€™re still there. After another duration of silence,
    it will hang up. This is something thatâ€™s pretty easy to do with LiveView and
    `Process.send_after/4`:'
  id: totrans-split-118
  prefs: []
  type: TYPE_NORMAL
  zh: Retell æ¼”ç¤ºè¿˜æ”¯æŒåœ¨é™é»˜ä¸€æ®µæ—¶é—´åçš„æç¤ºå’ŒæŒ‚æ–­ã€‚å¦‚æœä½ ä¿æŒæ²‰é»˜å¤ªä¹…ï¼Œä½ ä¼šä» AI æ‰¬å£°å™¨é‚£é‡Œå¾—åˆ°ä¸€ä¸ªæç¤ºï¼Œé—®ä½ æ˜¯å¦è¿˜åœ¨é‚£é‡Œã€‚åœ¨å¦ä¸€ä¸ªé™é»˜æœŸåï¼Œå®ƒä¼šæŒ‚æ–­ã€‚è¿™åœ¨
    LiveView å’Œ `Process.send_after/4` ä¸­éå¸¸å®¹æ˜“å®ç°ï¼š
- en: '[PRE21]'
  id: totrans-split-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'And then we can cancel the timer anytime we receive a transcription, and restart
    it after every turn speaking. Note that we canâ€™t depend on the Phoenix `speak`
    async task ending as the trigger to send nudges. Instead, we need to push an event
    from the speaker hook that the audio has ended. This avoids a case where the speaker
    initiates a really long speech, which overlaps with the `nudge_ms` duration. Now,
    we can control the number of nudges with an assign. In my case, I just used a
    boolean:'
  id: totrans-split-120
  prefs: []
  type: TYPE_NORMAL
  zh: æ¯å½“æˆ‘ä»¬æ”¶åˆ°è½¬å½•æ—¶ï¼Œæˆ‘ä»¬å¯ä»¥éšæ—¶å–æ¶ˆè®¡æ—¶å™¨ï¼Œå¹¶åœ¨æ¯æ¬¡è¯´è¯åé‡æ–°å¯åŠ¨å®ƒã€‚è¯·æ³¨æ„ï¼Œæˆ‘ä»¬ä¸èƒ½ä¾èµ– Phoenix `speak` å¼‚æ­¥ä»»åŠ¡ç»“æŸæ¥è§¦å‘å‘é€æç¤ºã€‚ç›¸åï¼Œæˆ‘ä»¬éœ€è¦ä»æ‰¬å£°å™¨é’©å­æ¨é€ä¸€ä¸ªäº‹ä»¶ï¼Œè¡¨æ˜éŸ³é¢‘å·²ç»“æŸã€‚è¿™é¿å…äº†å‘è¨€è€…å¯åŠ¨ä¸€ä¸ªéå¸¸é•¿çš„è®²è¯ä¸
    `nudge_ms` æŒç»­æ—¶é—´é‡å çš„æƒ…å†µã€‚ç°åœ¨ï¼Œæˆ‘ä»¬å¯ä»¥é€šè¿‡åˆ†é…æ¥æ§åˆ¶æç¤ºçš„æ•°é‡ã€‚åœ¨æˆ‘çš„æƒ…å†µä¸‹ï¼Œæˆ‘åªæ˜¯ä½¿ç”¨äº†ä¸€ä¸ªå¸ƒå°”å€¼ï¼š
- en: '[PRE22]'
  id: totrans-split-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: Re-doing the Entire Thing
  id: totrans-split-122
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: é‡æ–°åšæ•´ä¸ªäº‹æƒ…
- en: Somewhere along the line I realized that my attempts at engineering solid VAD
    client-side were never going to deliver the experience that I wanted. I discussed
    with [Andres Alejos](https://twitter.com/ac_alejos) a bit, and he found a [Silero
    VAD](https://github.com/snakers4/silero-vad) model which is capable of performing
    VAD in `1ms` on a single CPU thread. They also had an ONNX modelâ€”and we have a
    library in the Elixir ecosystem called [Ortex](https://github.com/elixir-nx/ortex)
    which allows us to execute ONNX models.
  id: totrans-split-123
  prefs: []
  type: TYPE_NORMAL
  zh: æŸä¸ªæ—¶å€™ï¼Œæˆ‘æ„è¯†åˆ°æˆ‘åœ¨å®¢æˆ·ç«¯è¿›è¡Œå¯é çš„ VAD å·¥ç¨‹å°è¯•æ°¸è¿œæ— æ³•æä¾›æˆ‘æƒ³è¦çš„ä½“éªŒã€‚æˆ‘ä¸ [Andres Alejos](https://twitter.com/ac_alejos)
    è®¨è®ºäº†ä¸€ä¸‹ï¼Œä»–æ‰¾åˆ°äº†ä¸€ä¸ª [Silero VAD](https://github.com/snakers4/silero-vad) æ¨¡å‹ï¼Œèƒ½å¤Ÿåœ¨å•ä¸ª CPU
    çº¿ç¨‹ä¸Šä»¥ `1ms` æ‰§è¡Œ VADã€‚ä»–ä»¬è¿˜æœ‰ä¸€ä¸ª ONNX æ¨¡å‹â€”æˆ‘ä»¬åœ¨ Elixir ç”Ÿæ€ç³»ç»Ÿä¸­æœ‰ä¸€ä¸ªå«åš [Ortex](https://github.com/elixir-nx/ortex)
    çš„åº“ï¼Œå¯ä»¥æ‰§è¡Œ ONNX æ¨¡å‹ã€‚
- en: To accomodate for the new VAD model, I ended up re-implementing the original
    LiveView I had as a WebSocket. This actually works out well because the WebSocket
    server is generic, and can be consumed by any language with a WebSocket client.
    The implementation is also relatively simple, and easily expanded to accomodate
    for other LLMs, TTS, and STT models. The WebSocket implementation has low latency
    (when running on a GPU), and supports interrupts.
  id: totrans-split-124
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†é€‚åº”æ–°çš„ VAD æ¨¡å‹ï¼Œæˆ‘æœ€ç»ˆé‡æ–°å®ç°äº†æœ€åˆçš„ LiveView ä½œä¸º WebSocketã€‚è¿™å®é™…ä¸Šæ•ˆæœå¾ˆå¥½ï¼Œå› ä¸º WebSocket æœåŠ¡å™¨æ˜¯é€šç”¨çš„ï¼Œå¯ä»¥è¢«ä»»ä½•å…·æœ‰
    WebSocket å®¢æˆ·ç«¯çš„è¯­è¨€æ¶ˆè´¹ã€‚å®ç°ä¹Ÿç›¸å¯¹ç®€å•ï¼Œå¯ä»¥è½»æ¾æ‰©å±•ä»¥é€‚åº”å…¶ä»– LLMsã€TTS å’Œ STT æ¨¡å‹ã€‚WebSocket å®ç°å…·æœ‰ä½å»¶è¿Ÿï¼ˆåœ¨ GPU
    ä¸Šè¿è¡Œæ—¶ï¼‰ï¼Œå¹¶æ”¯æŒä¸­æ–­ã€‚
- en: You can find the [project on my GitHub](https://github.com/seanmor5/echo) as
    well as an [example using the server](https://github.com/seanmor5/echo_example).
  id: totrans-split-125
  prefs: []
  type: TYPE_NORMAL
  zh: æ‚¨å¯ä»¥åœ¨[æˆ‘çš„GitHubé¡¹ç›®](https://github.com/seanmor5/echo)æ‰¾åˆ°ï¼Œä»¥åŠä¸€ä¸ª[ä½¿ç”¨æœåŠ¡å™¨çš„ç¤ºä¾‹](https://github.com/seanmor5/echo_example)ã€‚
- en: Musings
  id: totrans-split-126
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æ€ç´¢
- en: The final implementation I ended up with still does not match the quality of
    the Retell demo. That said, I think itâ€™s a solid start for future work. I believe
    I acted with some hubris when first posting about this project, and I would like
    to say that Retellâ€™s work should not be understated. I can appreciate the attention
    to detail that goes into making an effective conversational agent, and Retellâ€™s
    demo shows they paid a lot of attention to the details. Kudos to them and their
    team.
  id: totrans-split-127
  prefs: []
  type: TYPE_NORMAL
  zh: æœ€ç»ˆå®ç°çš„ç‰ˆæœ¬ä»ç„¶æ— æ³•ä¸Retellæ¼”ç¤ºçš„è´¨é‡åŒ¹æ•Œã€‚å°½ç®¡å¦‚æ­¤ï¼Œæˆ‘è®¤ä¸ºè¿™æ˜¯æœªæ¥å·¥ä½œçš„ä¸€ä¸ªè‰¯å¥½èµ·ç‚¹ã€‚æˆ‘è®¤ä¸ºåœ¨æœ€åˆå‘å¸ƒå…³äºè¿™ä¸ªé¡¹ç›®æ—¶æœ‰äº›å‚²æ…¢ï¼Œæˆ‘æƒ³è¯´Retellçš„å·¥ä½œä¸åº”è¢«ä½ä¼°ã€‚æˆ‘èƒ½ç†è§£åˆ¶ä½œæœ‰æ•ˆå¯¹è¯ä»£ç†æ‰€éœ€çš„ç»†èŠ‚å…³æ³¨ï¼ŒRetellçš„æ¼”ç¤ºæ˜¾ç¤ºä»–ä»¬å¯¹ç»†èŠ‚éå¸¸é‡è§†ã€‚å‘ä»–ä»¬åŠå…¶å›¢é˜Ÿè‡´æ•¬ã€‚
- en: I will also admit that my demo is playing to one benchmark. Iâ€™m optimizing the
    hell out of latency to support a single userâ€”me. I think this solution would change
    if it needed to accomodate for multiple concurrent users.
  id: totrans-split-128
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘è¿˜è¦æ‰¿è®¤ï¼Œæˆ‘çš„æ¼”ç¤ºæ­£åœ¨é’ˆå¯¹ä¸€ä¸ªåŸºå‡†è¿›è¡Œä¼˜åŒ–ã€‚æˆ‘åœ¨æåŠ›ä¼˜åŒ–å»¶è¿Ÿï¼Œä»¥æ”¯æŒå•ä¸ªç”¨æˆ·â€”â€”æˆ‘è‡ªå·±ã€‚æˆ‘è®¤ä¸ºå¦‚æœéœ€è¦æ”¯æŒå¤šä¸ªå¹¶å‘ç”¨æˆ·ï¼Œè¿™ä¸ªè§£å†³æ–¹æ¡ˆå°†ä¼šæ”¹å˜ã€‚
- en: Retellâ€™s website claims they have a conversation orchestration model under the
    hood to manage the complexities of conversation. I had my doubts about that going
    into this, but I believe it now. Whether or not this model is actually a single
    model or a series of models for VAD, adding backchannels, etc. Iâ€™m not sure. I
    think eventually it *will* be a single model, but Iâ€™m not sure if it is now, which
    leads me to my next point.
  id: totrans-split-129
  prefs: []
  type: TYPE_NORMAL
  zh: Retellçš„ç½‘ç«™å£°ç§°ä»–ä»¬åœ¨åå°æœ‰ä¸€ä¸ªå¯¹è¯ç¼–æ’æ¨¡å‹æ¥ç®¡ç†å¯¹è¯çš„å¤æ‚æ€§ã€‚åœ¨å¼€å§‹æ—¶ï¼Œæˆ‘å¯¹æ­¤è¡¨ç¤ºæ€€ç–‘ï¼Œä½†ç°åœ¨æˆ‘ç›¸ä¿¡äº†ã€‚æ— è®ºè¿™ä¸ªæ¨¡å‹å®é™…ä¸Šæ˜¯å•ä¸€æ¨¡å‹è¿˜æ˜¯ä¸€ç³»åˆ—ç”¨äºVADã€æ·»åŠ åé¦ˆé€šé“ç­‰çš„æ¨¡å‹ï¼Œæˆ‘éƒ½ä¸ç¡®å®šã€‚æˆ‘è®¤ä¸ºæœ€ç»ˆå®ƒå°†ä¼šæˆä¸ºä¸€ä¸ªå•ä¸€æ¨¡å‹ï¼Œä½†æˆ‘ä¸ç¡®å®šç°åœ¨æ˜¯å¦å·²ç»æ˜¯è¿™æ ·ï¼Œè¿™ä¹Ÿå¸¦æˆ‘åˆ°æˆ‘çš„ä¸‹ä¸€ä¸ªè§‚ç‚¹ã€‚
- en: Another Bitter Lesson
  id: totrans-split-130
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: å¦ä¸€ä¸ªè‹¦æ¶©çš„æ•™è®­
- en: While doing all of these optimizations, I could not help but think that it will
    eventually be all for naught. Not because I donâ€™t think people will find it useful,
    but because large models trained on lots of data simply seem to always beat engineering
    effort. I believe the future of this area of work is in joint models. I think
    the only way to achieve real-time conversations is to merge parts of the stack.
    I predict in less than a year we will see an incredibly capable joint speech/text
    model. I recently saw a large audio model called [Qwen-Audio](https://github.com/QwenLM/Qwen-Audio)
    that I believe is similar to what I envision.
  id: totrans-split-131
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿›è¡Œæ‰€æœ‰è¿™äº›ä¼˜åŒ–çš„åŒæ—¶ï¼Œæˆ‘ä¸ç¦åœ¨æƒ³ï¼Œæœ€ç»ˆè¿™äº›åŠªåŠ›å¯èƒ½ä¼šä»˜è¯¸ä¸œæµã€‚ä¸æ˜¯å› ä¸ºæˆ‘è®¤ä¸ºäººä»¬ä¸ä¼šè§‰å¾—å®ƒæœ‰ç”¨ï¼Œè€Œæ˜¯å› ä¸ºå¤§é‡æ•°æ®è®­ç»ƒçš„å¤§æ¨¡å‹ä¼¼ä¹æ€»æ˜¯èƒ½å¤Ÿå‡»è´¥å·¥ç¨‹åŠªåŠ›ã€‚æˆ‘ç›¸ä¿¡è¿™ä¸ªå·¥ä½œé¢†åŸŸçš„æœªæ¥åœ¨äºè”åˆæ¨¡å‹ã€‚æˆ‘è®¤ä¸ºå®ç°å®æ—¶å¯¹è¯çš„å”¯ä¸€æ–¹æ³•æ˜¯åˆå¹¶å †æ ˆçš„éƒ¨åˆ†ã€‚æˆ‘é¢„æµ‹ä¸åˆ°ä¸€å¹´å†…æˆ‘ä»¬å°†çœ‹åˆ°ä¸€ä¸ªéå¸¸æœ‰èƒ½åŠ›çš„è”åˆè¯­éŸ³/æ–‡æœ¬æ¨¡å‹ã€‚æˆ‘æœ€è¿‘çœ‹åˆ°äº†ä¸€ä¸ªç§°ä¸º[Qwen-Audio](https://github.com/QwenLM/Qwen-Audio)çš„å¤§å‹éŸ³é¢‘æ¨¡å‹ï¼Œæˆ‘è®¤ä¸ºè¿™ä¸æˆ‘æ‰€è®¾æƒ³çš„ç±»ä¼¼ã€‚
- en: 'Specifically, if somebody were kind enough to give me some money to throw at
    this problem, here is exactly what I would do:'
  id: totrans-split-132
  prefs: []
  type: TYPE_NORMAL
  zh: å…·ä½“æ¥è¯´ï¼Œå¦‚æœæœ‰äººæ„¿æ„ä¸ºæˆ‘è§£å†³è¿™ä¸ªé—®é¢˜æä¾›èµ„é‡‘ï¼Œè¿™é‡Œæ˜¯æˆ‘å°†è¦åšçš„äº‹æƒ…ï¼š
- en: 'Generate an [Alpaca-style](https://github.com/tatsu-lab/stanford_alpaca/blob/main/alpaca_data.json)
    and/or LLaVA-style dataset of synthetic speech. Note that it would require a bit
    of pre-processing to change Alpaca inputs to mirror a style compatible with spoken-word.
    I would use ElevenLabs to generate the dataset in mulitple voices. Of course this
    dataset would be a bit too â€œclean,â€ so weâ€™d need to apply some augmentations which
    add ambient noise, change speaking pitch and speed, etc. Bonus points: adding
    samples of â€œnoiseâ€ which require no response to merge the VAD part of the pipeline
    in as well. You can even throw in text prompts that dictate when and when not
    to respond to support things like [wake word detection](https://picovoice.ai/platform/porcupine/)
    without needing to train a separate model.'
  id: totrans-split-133
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: åˆ›å»ºä¸€ä¸ª[Alpacaé£æ ¼](https://github.com/tatsu-lab/stanford_alpaca/blob/main/alpaca_data.json)å’Œ/æˆ–LLaVAé£æ ¼çš„åˆæˆè¯­éŸ³æ•°æ®é›†ã€‚è¯·æ³¨æ„ï¼Œéœ€è¦è¿›è¡Œä¸€äº›é¢„å¤„ç†æ¥æ”¹å˜Alpacaè¾“å…¥ï¼Œä»¥åæ˜ ä¸å£è¯­å…¼å®¹çš„é£æ ¼ã€‚æˆ‘å°†ä½¿ç”¨ElevenLabsåœ¨å¤šä¸ªå£°éŸ³ä¸­ç”Ÿæˆæ•°æ®é›†ã€‚å½“ç„¶ï¼Œè¿™ä¸ªæ•°æ®é›†å¯èƒ½è¿‡äºâ€œå¹²å‡€â€ï¼Œæ‰€ä»¥æˆ‘ä»¬éœ€è¦åº”ç”¨ä¸€äº›å¢å¼ºæŠ€æœ¯ï¼Œå¦‚å¢åŠ ç¯å¢ƒå™ªå£°ã€æ”¹å˜è¯´è¯éŸ³è°ƒå’Œé€Ÿåº¦ç­‰ã€‚é¢å¤–åŠ åˆ†ç‚¹ï¼šæ·»åŠ ä¸€äº›â€œå™ªå£°â€æ ·æœ¬ï¼Œæ— éœ€å“åº”ä»¥åˆå¹¶ç®¡é“ä¸­çš„VADéƒ¨åˆ†ã€‚ç”šè‡³å¯ä»¥æ·»åŠ æ–‡æœ¬æç¤ºï¼ŒæŒ‡å¯¼ä½•æ—¶ä»¥åŠä½•æ—¶ä¸éœ€è¦å“åº”ï¼Œæ”¯æŒè¯¸å¦‚[wake
    word detection](https://picovoice.ai/platform/porcupine/)ä¹‹ç±»çš„åŠŸèƒ½ï¼Œè€Œæ— éœ€è®­ç»ƒå•ç‹¬çš„æ¨¡å‹ã€‚
- en: Create a LLaVA-style model with a Whisper or equivalent base, an LLM, and a
    projection layer.
  id: totrans-split-134
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: åˆ›å»ºä¸€ä¸ªLLaVAé£æ ¼çš„æ¨¡å‹ï¼Œä½¿ç”¨Whisperæˆ–ç­‰æ•ˆåŸºç¡€ã€LLMå’ŒæŠ•å½±å±‚ã€‚
- en: Secure H100s, train model, and â€œturn H100s into $100sâ€ (thank you [@thmsmlr](https://twitter.com/thmsmlr?lang=en))
  id: totrans-split-135
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: å®‰å…¨çš„H100sï¼Œè®­ç»ƒæ¨¡å‹ï¼Œå¹¶å°†â€œH100så˜æˆ100ç¾å…ƒâ€ï¼ˆè°¢è°¢ [@thmsmlr](https://twitter.com/thmsmlr?lang=en)ï¼‰
- en: If you want to give me some $$$, my e-mail is smoriarity.5@gmail.com ğŸ™‚
  id: totrans-split-136
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœä½ æƒ³ç»™æˆ‘ä¸€äº› $$$ï¼Œæˆ‘çš„ç”µå­é‚®ä»¶æ˜¯ smoriarity.5@gmail.com ğŸ™‚
- en: I believe we are also close to just having full-on speech-to-speech models.
    A specific challenge I can see when creating these models is coming up with a
    high-quality dataset. I think if you make a deliberate attempt at â€œrecording conversationsâ€
    for the purposes of training, you will actually probably end up with a lower-quality
    dataset. People tend to change their behavior under observation. Additionally,
    conversations from movies and TV shows arenâ€™t actually very natural. Even some
    podcasts have an unnatural converastional rhythm.
  id: totrans-split-137
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘è®¤ä¸ºæˆ‘ä»¬ä¹Ÿæ¥è¿‘å®Œå…¨çš„è¯­éŸ³å¯¹è¯æ¨¡å‹ã€‚åœ¨åˆ›å»ºè¿™äº›æ¨¡å‹æ—¶ï¼Œæˆ‘èƒ½çœ‹åˆ°çš„ä¸€ä¸ªå…·ä½“æŒ‘æˆ˜æ˜¯å¦‚ä½•è·å¾—é«˜è´¨é‡çš„æ•°æ®é›†ã€‚æˆ‘è®¤ä¸ºï¼Œå¦‚æœæ‚¨è¯•å›¾â€œè®°å½•å¯¹è¯â€ä»¥è¿›è¡Œè®­ç»ƒï¼Œå®é™…ä¸Šå¯èƒ½ä¼šå¾—åˆ°ä¸€ä¸ªè´¨é‡è¾ƒä½çš„æ•°æ®é›†ã€‚äººä»¬å¾€å¾€åœ¨è§‚å¯Ÿä¸‹æ”¹å˜ä»–ä»¬çš„è¡Œä¸ºã€‚æ­¤å¤–ï¼Œç”µå½±å’Œç”µè§†èŠ‚ç›®ä¸­çš„å¯¹è¯å®é™…ä¸Šå¹¶ä¸éå¸¸è‡ªç„¶ã€‚ç”šè‡³ä¸€äº›æ’­å®¢ä¹Ÿæœ‰ä¸è‡ªç„¶çš„å¯¹è¯èŠ‚å¥ã€‚
- en: While watching [Love is Blind](https://en.wikipedia.org/wiki/Love_Is_Blind_(TV_series))
    with my fiancÃ©, I realized you could probably get a decent amount of quality data
    from reality tv shows. The conversations in reality TV are overly dramatic and
    chaotic, but are (I think) closer to realistic than anything else.
  id: totrans-split-138
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨å’Œæˆ‘çš„æœªå©šå¤«ä¸€èµ·è§‚çœ‹ [ã€Šçˆ±æƒ…æ˜¯ç›²çš„ã€‹](https://en.wikipedia.org/wiki/Love_Is_Blind_(TV_series))
    æ—¶ï¼Œæˆ‘æ„è¯†åˆ°ä½ å¯èƒ½å¯ä»¥ä»çœŸäººç§€ä¸­è·å¾—ç›¸å½“æ•°é‡çš„é«˜è´¨é‡æ•°æ®ã€‚çœŸäººç§€ä¸­çš„å¯¹è¯è¿‡äºå¤¸å¼ å’Œæ··ä¹±ï¼Œä½†ï¼ˆæˆ‘è®¤ä¸ºï¼‰æ¯”å…¶ä»–ä»»ä½•ä¸œè¥¿æ›´æ¥è¿‘ç°å®ã€‚
- en: Conversational Knowledge Base?
  id: totrans-split-139
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: å¯¹è¯çŸ¥è¯†åº“ï¼Ÿ
- en: I do wonder what a solid RAG implementation looks like on top of a conversational
    agent. RAG and complex CoT pipelines will introduce latency which could deteriorate
    the conversational experience. However, there are clever ways you can hide this.
    In conversations that require â€œsearchâ€ between humans, e.g. like scheduling an
    appointment, youâ€™ll often have one party saying â€œone moment pleaseâ€ before performing
    a system search. Building something like that in is entirely possible. Additionally,
    if your agent requires information up front about an individual, itâ€™s possible
    to include that in the initial prompt.
  id: totrans-split-140
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ç¡®å®å¾ˆæƒ³çŸ¥é“åœ¨å¯¹è¯ä»£ç†ä¹‹ä¸Šï¼Œä¸€ä¸ªç¨³å›ºçš„RAGå®ç°çœ‹èµ·æ¥æ˜¯ä»€ä¹ˆæ ·å­ã€‚RAGå’Œå¤æ‚çš„CoTç®¡é“å°†å¼•å…¥æ½œä¼æœŸï¼Œè¿™å¯èƒ½ä¼šæŸå®³å¯¹è¯ä½“éªŒã€‚ä½†æ˜¯ï¼Œä½ å¯ä»¥é‡‡ç”¨ä¸€äº›å·§å¦™çš„æ–¹æ³•æ¥éšè—è¿™ä¸€ç‚¹ã€‚åœ¨éœ€è¦äººä¸äººä¹‹é—´â€œæœç´¢â€çš„å¯¹è¯ä¸­ï¼Œä¾‹å¦‚å®‰æ’é¢„çº¦ï¼Œé€šå¸¸ä¼šæœ‰ä¸€æ–¹åœ¨æ‰§è¡Œç³»ç»Ÿæœç´¢ä¹‹å‰è¯´â€œè¯·ç¨ç­‰ç‰‡åˆ»â€ã€‚å®Œå…¨å¯ä»¥æ„å»ºç±»ä¼¼çš„åŠŸèƒ½ã€‚æ­¤å¤–ï¼Œå¦‚æœæ‚¨çš„ä»£ç†éœ€è¦å…³äºä¸ªäººçš„ä¿¡æ¯ï¼Œå¯ä»¥åœ¨åˆå§‹æç¤ºä¸­åŒ…å«è¿™äº›ä¿¡æ¯ã€‚
- en: You Should Use Elixir
  id: totrans-split-141
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: ä½ åº”è¯¥ä½¿ç”¨Elixir
- en: I was very excited for this problem in particular because itâ€™s literally the
    perfect application of Elixir and Phoenix. If you are building conversational
    agents, you should seriously consider giving Elixir a try. A large part of how
    quick this demo was to put together is because of how productive Elixir is.
  id: totrans-split-142
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘å¯¹è¿™ä¸ªé—®é¢˜ç‰¹åˆ«å…´å¥‹ï¼Œå› ä¸ºå®ƒå®é™…ä¸Šæ˜¯Elixirå’ŒPhoenixçš„å®Œç¾åº”ç”¨ã€‚å¦‚æœæ‚¨æ­£åœ¨æ„å»ºå¯¹è¯ä»£ç†ï¼Œåº”è¯¥è®¤çœŸè€ƒè™‘å°è¯•Elixirã€‚è¿™ä¸ªæ¼”ç¤ºèƒ½å¤Ÿå¦‚æ­¤å¿«é€Ÿåœ°å®Œæˆå¾ˆå¤§ç¨‹åº¦ä¸Šæ˜¯å› ä¸ºElixirçš„é«˜æ•ˆç‡ã€‚
- en: Conclusion
  id: totrans-split-143
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ç»“è®º
- en: This was a fun technical challenge. I am pleased with the performance of the
    final demonstration. Iâ€™m also happy I was able to OSS a small library for others
    to build off of. If you are interested in conversational agents, I encourage you
    to check it out, give feedback, and contribute! I know itâ€™s very rough right now,
    but it will get better with time.
  id: totrans-split-144
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ˜¯ä¸€ä¸ªæœ‰è¶£çš„æŠ€æœ¯æŒ‘æˆ˜ã€‚æˆ‘å¯¹æœ€ç»ˆæ¼”ç¤ºçš„è¡¨ç°æ„Ÿåˆ°æ»¡æ„ã€‚æˆ‘ä¹Ÿå¾ˆé«˜å…´èƒ½å¤Ÿä¸ºå…¶ä»–äººå¼€å‘ä¸€ä¸ªå°å‹åº“ã€‚å¦‚æœæ‚¨å¯¹å¯¹è¯ä»£ç†æ„Ÿå…´è¶£ï¼Œæˆ‘é¼“åŠ±æ‚¨æŸ¥çœ‹å®ƒï¼Œæä¾›åé¦ˆå¹¶è´¡çŒ®ï¼æˆ‘çŸ¥é“ç°åœ¨è¿˜å¾ˆç²—ç³™ï¼Œä½†éšç€æ—¶é—´çš„æ¨ç§»ï¼Œå®ƒä¼šå˜å¾—æ›´å¥½ã€‚
- en: Additionally, I plan to periodically build out the rest of the Nero project,
    so please follow me on [Twitter](https://twitter.com/sean_moriarity) if youâ€™d
    like to stay up to date.
  id: totrans-split-145
  prefs: []
  type: TYPE_NORMAL
  zh: æ­¤å¤–ï¼Œæˆ‘è®¡åˆ’å®šæœŸæ‰©å±•Neroé¡¹ç›®çš„å…¶ä½™éƒ¨åˆ†ï¼Œè¯·é€šè¿‡ [Twitter](https://twitter.com/sean_moriarity) å…³æ³¨æˆ‘ä»¥è·å–æœ€æ–°ä¿¡æ¯ã€‚
