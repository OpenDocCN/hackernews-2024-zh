- en: <!--yml
  id: totrans-split-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-split-1
  prefs: []
  type: TYPE_NORMAL
  zh: 'category: 未分类'
- en: 'date: 2024-05-27 13:19:22'
  id: totrans-split-2
  prefs: []
  type: TYPE_NORMAL
  zh: 'date: 2024-05-27 13:19:22'
- en: -->
  id: totrans-split-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: Implementing Natural Conversational Agents with Elixir – Sean Moriarity
  id: totrans-split-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 用Elixir实现自然对话代理 - Sean Moriarity
- en: 来源：[https://seanmoriarity.com/2024/02/25/implementing-natural-conversational-agents-with-elixir/](https://seanmoriarity.com/2024/02/25/implementing-natural-conversational-agents-with-elixir/)
  id: totrans-split-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://seanmoriarity.com/2024/02/25/implementing-natural-conversational-agents-with-elixir/](https://seanmoriarity.com/2024/02/25/implementing-natural-conversational-agents-with-elixir/)
- en: In [my last post](https://seanmoriarity.com/2024/02/25/nero-part-1-home-automations/),
    I discussed some work I had done building Nero, the assistant of the future that
    I’ve always wanted. I ended up creating an end-to-end example which used Nx, OpenAI
    APIs, and ElevenLabs to create an in-browser home automation assistant. For a
    first product, it’s decent. Nero is a neat little party trick that I can use to
    impress my non-tech friends. I am, however, not in this business to impress my
    friends. I want Nero to *actually help me* and *actually feel like an assistant*.
    My previous version is not that.
  id: totrans-split-6
  prefs: []
  type: TYPE_NORMAL
  zh: 在[我上一篇文章](https://seanmoriarity.com/2024/02/25/nero-part-1-home-automations/)中，我讨论了我构建的一些工作，构建了我一直想要的未来助手Nero。最终，我创建了一个端到端示例，使用了Nx、OpenAI
    APIs和ElevenLabs来创建一个浏览器内的家庭自动化助手。作为第一个产品，它还不错。Nero是一个小巧的聚会把戏，我可以用来给非技术朋友留下深刻印象。然而，我做这个并不是为了让朋友们印象深刻。我希望Nero能够*真正帮助我*，*真正感觉像一个助手*。我的上一个版本并不是那样。
- en: One missing piece is the ability to converse naturally without browser interaction.
    The first implementation of Nero’s “conversational” abilities relied on user interaction
    with the screen every time we wanted to initiate a response or action. Nero also
    did not retain any conversational history. In short, Nero was not a great conversational
    assistant. It was one of the things I wanted to fix; however, I was motivated
    to do it sooner rather than later after watching [an impressive demo from Retell](https://beta.retellai.com/home-agent).
  id: totrans-split-7
  prefs: []
  type: TYPE_NORMAL
  zh: 缺少的一环是在没有浏览器交互的情况下自然对话的能力。Nero的第一个“对话”能力实现依赖于用户每次想要启动响应或动作时与屏幕的交互。Nero也没有保留任何对话历史记录。简而言之，Nero不是一个很好的对话助手。看完[Retell的一个令人印象深刻的演示之后](https://beta.retellai.com/home-agent)，我立即想要尽快修复这些问题。
- en: 'The Retell demo implements a conversational agent backed by their WebSocket
    API in a browser. The demonstration has:'
  id: totrans-split-8
  prefs: []
  type: TYPE_NORMAL
  zh: Retell演示在浏览器中通过他们的WebSocket API实现了一个支持对话代理。演示包括：
- en: “Always on” recording
  id: totrans-split-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: “始终开启”录音
- en: Low latency
  id: totrans-split-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 低延迟
- en: Support for interrupts
  id: totrans-split-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 支持中断
- en: Impressive filtering (e.g. snapping and other non-voice activity doesn’t seem
    to throw off the agent)
  id: totrans-split-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 令人印象深刻的过滤（例如，捕捉和其他非语音活动似乎不会干扰代理）
- en: Their documentation suggests they also have support for [backchanneling](https://www.cs.utep.edu/nigel/bc/#:~:text=What%20is%20a%20Backchannel%3F,utterances%20such%20as%20uh%2Dhuh.)
    and intelligent end of turn detection—two things that are essential to natural
    conversational feel but which are very difficult to express programmatically.
  id: totrans-split-13
  prefs: []
  type: TYPE_NORMAL
  zh: 他们的文档表明他们还支持[后援反馈](https://www.cs.utep.edu/nigel/bc/#:~:text=What%20is%20a%20Backchannel%3F,utterances%20such%20as%20uh%2Dhuh.)和智能的对话结束检测，这两点对于实现自然对话感觉非常重要，但在程序上非常难以表达。
- en: I had previously convinced myself that I could implement a passable conversational
    agent experience in a short amount of time. So that is what I set out to do.
  id: totrans-split-14
  prefs: []
  type: TYPE_NORMAL
  zh: 我之前确信自己可以在很短的时间内实现一个过得去的对话体验助手。所以我就着手去做了。
- en: Always On Recording
  id: totrans-split-15
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 始终开启录音
- en: The first thing that needed to change about Nero’s design was the speech to
    text pipeline. My original demonstration relied on an [example from Bumblebee](https://github.com/elixir-nx/bumblebee/blob/main/examples/phoenix/speech_to_text.exs)
    which implemented a speech to text pipeline using Whisper. The pipeline uses mouse
    events in a [Phoenix LiveView Hook](https://hexdocs.pm/phoenix_live_view/js-interop.html#client-hooks-via-phx-hook)
    to start and stop recordings before sending them to the server to initiate transcription.
    If you’re not familiar, [Phoenix LiveView](https://hexdocs.pm/phoenix_live_view/Phoenix.LiveView.html)
    is a server-side rendering framework built on top of Elixir. LiveView has support
    for client-side JavaScript hooks which support bidirectional communication between
    client and server.
  id: totrans-split-16
  prefs: []
  type: TYPE_NORMAL
  zh: 关于Nero的设计需要改变的第一件事是语音到文本流水线。我的初始演示依赖于[Bumblebee的一个示例](https://github.com/elixir-nx/bumblebee/blob/main/examples/phoenix/speech_to_text.exs)，该示例使用Whisper实现了一个使用Phoenix
    LiveView Hook在发送到服务器进行转录之前开始和停止录音的语音到文本流水线。如果您不熟悉，[Phoenix LiveView](https://hexdocs.pm/phoenix_live_view/Phoenix.LiveView.html)是建立在Elixir之上的服务器端渲染框架。LiveView支持客户端JavaScript钩子，支持客户端和服务器之间的双向通信。
- en: The original speech to text implementation used a hook with an event listener
    attached to `mousedown` and `mouseup` on a button to start and stop recording.
    After recording stops, the hook decodes the recorded buffer into a PCM buffer,
    converts the endianness, and then pushes the buffer to the server with an upload.
    The original hook implements most of the functionality we want; however, we need
    to make some minor tweaks. Rather than trigger recordings to stop and start on
    mouse events, we want to trigger recordings to start and stop exactly when a person
    starts and stops speaking. Simple, right?
  id: totrans-split-17
  prefs: []
  type: TYPE_NORMAL
  zh: 初始的语音转文字实现使用了一个钩子，附加到按钮的`mousedown`和`mouseup`事件监听器上，用于开始和停止录音。在录音停止后，钩子将记录的缓冲区解码为PCM缓冲区，转换字节顺序，然后将缓冲区推送到服务器进行上传。原始钩子实现了大部分我们想要的功能；然而，我们需要进行一些小的调整。与其在鼠标事件上触发录音开始和停止，我们希望在人开始说话和停止说话时准确地触发录音的开始和停止。简单吧？
- en: My first idea in implementing what I called “always on recording” was to monitor
    the microphone’s volume, and trigger a recording when the volume reached a certain
    threshold. The recording would stop when the volume dipped below that threshold.
    At this point, I learned about [getUserMedia](https://developer.mozilla.org/en-US/docs/Web/API/MediaDevices/getUserMedia).
    `getUserMedia` prompts the user for permission to access media devices such as
    a microphone and/or webcam, and then produces a `MediaStream`. A `MediaStream`
    is a stream of media content containing information about audio and video tracks
    in the stream. We can use data from the `MediaStream` to determine speaker activity
    and thus trigger recordings.
  id: totrans-split-18
  prefs: []
  type: TYPE_NORMAL
  zh: 我在实现我称之为“始终录音”的想法时，第一个想到的是监视麦克风的音量，并且当音量达到一定的阈值时触发录音。当音量低于该阈值时停止录音。在这一点上，我了解了[getUserMedia](https://developer.mozilla.org/en-US/docs/Web/API/MediaDevices/getUserMedia)。`getUserMedia`提示用户允许访问媒体设备，比如麦克风和/或摄像头，然后生成一个`MediaStream`。`MediaStream`是一个包含有关流中音频和视频轨道信息的媒体内容流。我们可以使用`MediaStream`的数据来确定说话者的活动，从而触发录音。
- en: 'To determine the volume for a given sample, we can use an [AnalyserNode](https://developer.mozilla.org/en-US/docs/Web/API/AnalyserNode).
    Per the documentation `AnyalyserNode` is designed for processing generated audio
    data for visualization purposes, but we can use it to determine spikes in audio:'
  id: totrans-split-19
  prefs: []
  type: TYPE_NORMAL
  zh: 要确定特定样本的音量，我们可以使用[AnalyserNode](https://developer.mozilla.org/en-US/docs/Web/API/AnalyserNode)。根据文档，`AnyalyserNode`被设计用于处理生成的音频数据以进行可视化，但我们可以使用它来确定音频中的峰值：
- en: '[PRE0]'
  id: totrans-split-20
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: This uses an analyser and repeatedly checks if the volume of the microphone
    at a given frame exceeds the given `VOLUME_THRESHOLD`. If it does, it checks to
    see if we are recording and if not, starts the recording.
  id: totrans-split-21
  prefs: []
  type: TYPE_NORMAL
  zh: 这使用一个分析器并重复检查给定帧的麦克风音量是否超过给定的`VOLUME_THRESHOLD`。如果是，则检查我们是否正在录制，如果不是，则开始录制。
- en: 'After testing a bit, I realized this implementation sucked. Of the many issues
    with this approach, the biggest is that there are many natural dips in a person’s
    volume. Checking a single frame doesn’t account for these natural dips. To fix
    this, I thought it would be a good idea to introduce a timeout which only stopped
    recording after the volume was below a threshold for a certain amount of time:'
  id: totrans-split-22
  prefs: []
  type: TYPE_NORMAL
  zh: 测试了一下之后，我意识到这个实现并不好用。这种方法有很多问题，其中最大的问题是人的音量会自然地有很多波动。仅检查单个帧无法解决这些自然波动。为了解决这个问题，我想引入一个超时机制，只有在音量低于某个阈值一段时间后才停止录制：
- en: '[PRE1]'
  id: totrans-split-23
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: This actually ended up working decent, but required tuning hyperparameters for
    both `VOLUME_THRESHOLD` and `SILENCE_TIMEOUT`. The challenge here is that higher
    `SILENCE_TIMEOUT` introduces additionally latency in transition time between a
    speaker and Nero; however, lower timeouts might be too sensitive to speakers with
    slower and quieter speaking rhythms. Additionally, a static `VOLUME_THRESHOLD`
    does not account for ambient noise. Now, despite these shortcomings, I found I
    was able to passably detect a single speaker in a quiet room.
  id: totrans-split-24
  prefs: []
  type: TYPE_NORMAL
  zh: 这实际上效果还不错，但需要调整 `VOLUME_THRESHOLD` 和 `SILENCE_TIMEOUT` 的超参数。挑战在于，更高的 `SILENCE_TIMEOUT`
    会增加说话者与 Nero 之间转换时间的延迟，但较低的超时时间可能对说话节奏较慢和音量较低的说话者过于敏感。此外，静态的 `VOLUME_THRESHOLD`
    无法考虑环境噪声。尽管存在这些缺点，我发现我能够基本上在安静的房间里检测出单个说话者。
- en: After hooking this up to my existing LiveView and trying some end-to-end conversations,
    I realized something was significantly off. The transcriptions I was getting were
    off. I soon realized that they were always off at the beginning of a transcription.
    Shorter audio sequences were especially affected. It turns out that the detection
    algorithm always resulted in some amount of truncation at the beginning of an
    audio clip. When a speaker starts talking, their volume ramps up – it’s not an
    instantaneous spike. To account for this, I introduced a pre-recording buffer
    which always tracked the previous 150ms of audio. After recording started, I would
    stop the pre-recording buffer and start the actual recording, and then eventually
    splice these 2 together to send to the server for transcription.
  id: totrans-split-25
  prefs: []
  type: TYPE_NORMAL
  zh: 将这个连接到我的现有 LiveView 并尝试了一些端到端对话后，我意识到有些地方明显不对劲。我得到的转录结果不正确。很快我意识到问题总是出现在转录开始的时候。较短的音频序列尤其受影响。原来检测算法在音频剪辑的开头总是会有一定的截断。当说话者开始讲话时，他们的音量会逐渐增加，而不是瞬间的峰值。为了解决这个问题，我引入了一个预录音缓冲区，始终跟踪前
    150 毫秒的音频。录制开始后，我会停止预录音缓冲并开始实际录制，然后最终将这两者拼接在一起发送到服务器进行转录。
- en: Overall, this *actually* worked okay. While there are some obvious failure modes,
    it worked well enough to get a passable demonstration. If you can’t tell by now,
    I am not an audio engineer. I learned later that this is a very naive attempt
    at [voice activity detection](https://en.wikipedia.org/wiki/Voice_activity_detection).
    Later on in this post, I’ll run through some of the improvements I made based
    on my research into the field of VAD.
  id: totrans-split-26
  prefs: []
  type: TYPE_NORMAL
  zh: 总体来说，这 *实际上* 运行得还可以。虽然存在一些明显的失败模式，但足够演示了一下。如果到目前为止你还没发现，我不是一个音频工程师。后来我了解到，这是对
    [语音活动检测](https://en.wikipedia.org/wiki/Voice_activity_detection) 的一个非常幼稚的尝试。在本文的后面部分，我将介绍一些基于我对
    VAD 领域研究的改进。
- en: End-to-End Implementation
  id: totrans-split-27
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 端到端实现
- en: 'The demonstration I built for Nero in my [first post](https://seanmoriarity.com/2024/02/25/nero-part-1-home-automations/)
    already contained the scaffolding for an end-to-end transcription -> response
    -> speech pipeline. I only needed to make some slight modifications to get the
    phone call demo to work. The end-to-end the pipeline looks like this:'
  id: totrans-split-28
  prefs: []
  type: TYPE_NORMAL
  zh: 我在我的 [第一篇文章](https://seanmoriarity.com/2024/02/25/nero-part-1-home-automations/)
    中为 Nero 构建的演示已经包含了端到端转录 -> 响应 -> 语音管道的脚手架。我只需要进行一些微小的修改就能让电话演示工作起来。端到端的管道如下所示：
- en: 'When our algorithm detects that speech has stopped, it invokes the `stopRecording`
    method. `stopRecording` takes the recorded audio, does some client-side pre-processing,
    and uploads it to the server. The server consumes the uploaded entry as a part
    of [LiveView’s normal uploads lifecycle](https://hexdocs.pm/phoenix_live_view/uploads.html)
    and then invokes an [async task](https://hexdocs.pm/phoenix_live_view/Phoenix.LiveView.html#module-arbitrary-async-operations)
    to start transcription:'
  id: totrans-split-29
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们的算法检测到语音已停止时，它调用`stopRecording`方法。`stopRecording`获取录制的音频，执行一些客户端预处理，并将其上传到服务器。服务器作为[LiveView的正常上传生命周期](https://hexdocs.pm/phoenix_live_view/uploads.html)的一部分消耗上传的条目，并调用一个[异步任务](https://hexdocs.pm/phoenix_live_view/Phoenix.LiveView.html#module-arbitrary-async-operations)来启动转录：
- en: '[PRE2]'
  id: totrans-split-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Note that because we did most of the pre-processing client-side, we can just
    consume the audio binary as an `Nx.Tensor`, without any additional work. The `SpeechToText`
    module implements transcription using [Nx.Serving](https://hexdocs.pm/nx/Nx.Serving.html):'
  id: totrans-split-31
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，因为我们大部分预处理是在客户端完成的，所以我们可以直接将音频二进制作为`Nx.Tensor`消耗，无需额外工作。`SpeechToText`模块使用[Nx.Serving](https://hexdocs.pm/nx/Nx.Serving.html)实现转录：
- en: '[PRE3]'
  id: totrans-split-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '`Nx.Serving` is an abstraction in the Elixir Nx ecosystem for serving machine
    learning models directly in an Elixir application. It implements dynamic batching,
    encapsulates pre-processing, inference, and post-processing, supports distribution
    and load-balancing between multiple GPUs natively, and in general is an extremely
    easy way to serve machine learning models.'
  id: totrans-split-33
  prefs: []
  type: TYPE_NORMAL
  zh: '`Nx.Serving`是Elixir Nx生态系统中用于直接为Elixir应用程序提供机器学习模型的抽象。它实现动态批处理，封装预处理、推理和后处理，原生支持多个GPU之间的分布和负载平衡，并且通常是一种极其简单的方法来提供机器学习模型。'
- en: 'After transcription completes, we get an async result we can handle to initiate
    a response:'
  id: totrans-split-34
  prefs: []
  type: TYPE_NORMAL
  zh: 在转录完成后，我们获得了一个异步结果，可以处理以启动响应：
- en: '[PRE4]'
  id: totrans-split-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Here `Nero.Agent.respond/1` returns an Elixir `Stream` of text. For my original
    demonstration I just used the Elixir OpenAI library to produce a stream from a
    GPT-3.5 response:'
  id: totrans-split-36
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，`Nero.Agent.respond/1`返回一个Elixir文本流。对于我的原始演示，我只是使用Elixir OpenAI库从GPT-3.5响应中生成流：
- en: '[PRE5]'
  id: totrans-split-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'The response stream is consumed by `speak/2`. `speak/2` implements the text
    to speech pipeline:'
  id: totrans-split-38
  prefs: []
  type: TYPE_NORMAL
  zh: 响应流由`speak/2`消耗。`speak/2`实现了文本到语音的流水线：
- en: '[PRE6]'
  id: totrans-split-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Where `Nero.TextToSpeech.stream/1` uses the [ElevenLabs WebSocket API](https://elevenlabs.io/docs/api-reference/websockets)
    to stream text in and speech out. You can read a bit more about the implementation
    in my previous post.
  id: totrans-split-40
  prefs: []
  type: TYPE_NORMAL
  zh: '`Nero.TextToSpeech.stream/1`使用[ElevenLabs WebSocket API](https://elevenlabs.io/docs/api-reference/websockets)来流入文本并流出语音。您可以在我的上一篇文章中进一步了解其实现。'
- en: '`Nero.TextToSpeech.stream/1` returns the consumed response as text so we can
    append that to the chat history after the `:speak` task finishes:'
  id: totrans-split-41
  prefs: []
  type: TYPE_NORMAL
  zh: '`Nero.TextToSpeech.stream/1`将消耗的响应返回为文本，以便我们可以在`:speak`任务完成后将其附加到聊天历史记录中：'
- en: '[PRE7]'
  id: totrans-split-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'This is basically all of the scaffolding needed for an end-to-end demo, but
    I wanted to add a few more features. First, I wanted to support “intelligent”
    hang-ups. Basically, I wanted to be able to detect when a conversation was finished,
    and stop the recording. To do that, I used [Instructor](https://github.com/thmsmlr/instructor_ex):'
  id: totrans-split-43
  prefs: []
  type: TYPE_NORMAL
  zh: 这基本上是进行端到端演示所需的所有脚手架，但我想添加更多功能。首先，我希望支持“智能”挂断电话。基本上，我希望能够检测到对话何时结束，并停止录音。为此，我使用了[Instructor](https://github.com/thmsmlr/instructor_ex)：
- en: '[PRE8]'
  id: totrans-split-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Please ignore my wonderfully engineered prompt. This uses GPT-3.5 to determine
    whether or not a given conversation has ended. After every one of Nero’s turns,
    we check the transcript to possibly end the call:'
  id: totrans-split-45
  prefs: []
  type: TYPE_NORMAL
  zh: 请忽略我精心设计的提示。这使用GPT-3.5来确定给定对话是否已经结束。在Nero的每次对话之后，我们检查转录以可能结束通话：
- en: '[PRE9]'
  id: totrans-split-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'This pushes a `hang_up` event to the socket:'
  id: totrans-split-47
  prefs: []
  type: TYPE_NORMAL
  zh: 这将向套接字推送一个`hang_up`事件：
- en: '[PRE10]'
  id: totrans-split-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Which stops the recording, and then pushes an event to `toggle_conversation`
    back to the server. `toggle_conversation` implements the start/stop logic from
    the server:'
  id: totrans-split-49
  prefs: []
  type: TYPE_NORMAL
  zh: 这将停止录音，并将一个事件推送到`toggle_conversation`返回到服务器。`toggle_conversation`实现了服务器上的启动/停止逻辑：
- en: '[PRE11]'
  id: totrans-split-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Finally, I wanted to implement information extraction from the transcript.
    Again, I used instructor and defined an extraction schema:'
  id: totrans-split-51
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我想实现从转录中提取信息。同样，我使用了instructor并定义了一个提取模式：
- en: '[PRE12]'
  id: totrans-split-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'And used GPT-3.5 with a rough prompt to get the necessary information from
    the transcript:'
  id: totrans-split-53
  prefs: []
  type: TYPE_NORMAL
  zh: 并使用GPT-3.5与一个粗略的提示从转录中获取必要的信息：
- en: '[PRE13]'
  id: totrans-split-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'And then anytime a conversation ends, we attempt to retrieve appointment information:'
  id: totrans-split-55
  prefs: []
  type: TYPE_NORMAL
  zh: 每当对话结束时，我们尝试检索约会信息：
- en: '[PRE14]'
  id: totrans-split-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Now this is essentially the exact implementation that produced [this demonstration](https://twitter.com/sean_moriarity/status/1760435005119934862).
    End-to-end this amounted to a couple of hours of work; however, I already had
    most of the basic scaffold implemented from my previous work on Nero. In my biased
    opinion, I think my demo is pretty good, but as others have pointed out Retell’s
    demo kicks my ass in:'
  id: totrans-split-57
  prefs: []
  type: TYPE_NORMAL
  zh: 现在这基本上就是生成了[这个演示](https://twitter.com/sean_moriarity/status/1760435005119934862)的确切实现。从头到尾这大约花了几个小时的工作；然而，我已经在我之前在
    Nero 上的工作中实现了大部分基本框架。在我主观的看法中，我认为我的演示相当不错，但正如其他人指出的，Retell 的演示比我的好得多：
- en: Latency
  id: totrans-split-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 延迟
- en: Reliability
  id: totrans-split-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可靠性
- en: Natural sounding voice
  id: totrans-split-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 自然的声音
- en: And so, I set out to improve my implementation – starting with latency.
  id: totrans-split-61
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我开始改善我的实现——从延迟开始。
- en: Reducing Latency
  id: totrans-split-62
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 减少延迟
- en: Human conversations have extremely tight “time-to-turn.” In-person conversations
    are especially rapid because we rely on visual as well as audio signals to determine
    when it’s our time to participate in a conversation. The “average” time-to-turn
    in a conversation can be as quick as 200ms. That means for a conversational agent
    to feel realistic, it needs an extremely quick turn around time for “time to first
    spoken word.”
  id: totrans-split-63
  prefs: []
  type: TYPE_NORMAL
  zh: 人类对话有极其紧密的“响应时间”。面对面的对话尤其迅速，因为我们依赖视觉和听觉信号来确定我们何时参与对话。对话中的“平均”响应时间可以非常快，只有 200
    毫秒。这意味着为了使对话代理程序感觉真实，它需要极快的“第一个发言词的时间”。
- en: After posting my original demonstration, I already knew there were some very
    easy optimizations I could make, so I set out to improve the average latency of
    my implementation as much as possible in a short amount of time. First, I needed
    at least some method for determining whether an optimization worked. My rudimentary
    approach was to use [JavaScript Performance Timers](https://developer.mozilla.org/en-US/docs/Web/API/Performance/now)
    and logging. Basically, I computed a `startTime` from the exact moment an audio
    recording stopped and an `endTime` from the exact moment an audio output started,
    and then I logged that time to the console.
  id: totrans-split-64
  prefs: []
  type: TYPE_NORMAL
  zh: 在发布我的原始演示之后，我已经知道有一些非常简单的优化方法可以实现，因此我着手尽可能在短时间内改善实现的平均延迟。首先，我需要至少一种方法来确定优化是否奏效。我的初步方法是使用[JavaScript
    性能计时器](https://developer.mozilla.org/en-US/docs/Web/API/Performance/now)和日志记录。基本上，我从音频录制停止的确切时刻计算一个
    `startTime`，从音频输出开始的确切时刻计算一个 `endTime`，然后将该时间记录到控制台。
- en: This is a very unscientific way of doing business. In the future, I’d like to
    implement a much more involved profiling and benchmarking methodology. For this
    process though, it worked well enough.
  id: totrans-split-65
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一种非常不科学的做法。将来，我希望实现一个更加深入的分析和基准测试方法。尽管如此，在这个过程中，它表现得足够好。
- en: 'Next, I considered all of the areas that could introduce latency into the pipeline.
    From the moment a recording stops, these are all of the steps we take:'
  id: totrans-split-66
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我考虑了可能在流水线中引入延迟的所有领域。从录音停止的那一刻起，这些都是我们所采取的所有步骤：
- en: Pre-process recording by converting to PCM buffer, and then converting endianness
    to match server (if necessary)
  id: totrans-split-67
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过转换为 PCM 缓冲区预处理录音，然后转换字节序以匹配服务器（如果需要）
- en: Upload buffer to server
  id: totrans-split-68
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将缓冲区上传到服务器
- en: Perform speech to text on buffer to produce text
  id: totrans-split-69
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对缓冲区执行语音转文字以生成文本
- en: Send text to LLM
  id: totrans-split-70
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 发送文本到 LLM
- en: Send streamed text to ElevenLabs
  id: totrans-split-71
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 发送流式文本到 ElevenLabs
- en: Receive streamed audio from ElevenLabs
  id: totrans-split-72
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接收来自 ElevenLabs 的流式音频
- en: Broadcast audio to client
  id: totrans-split-73
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 向客户端广播音频
- en: Decode audio on client and play
  id: totrans-split-74
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在客户端解码音频并播放
- en: That’s a lot of steps that can introduce latency, including potentially 3 (in
    our case 2 because we own the STT pipeline) network calls.
  id: totrans-split-75
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个可能引入延迟的很多步骤，包括潜在的 3 个（在我们的情况下是 2 个，因为我们拥有 STT 流水线）网络调用。
- en: Next, I wanted to esablish a “baseline” of performance. To demonstrate this
    iterative process, I did a baseline example on my M3 Mac CPU. Note that this is
    going to be slow relative to my previous demo because the previous demo runs on
    a GPU. The baseline performance I got from the original demo running on my mac
    was `4537 ms`. 4.5 seconds turn around time. Yikes. Lots of work to do.
  id: totrans-split-76
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我想建立一个性能的“基线”。为了演示这个迭代过程，我在我的 M3 Mac CPU 上做了一个基准示例。请注意，与之前在 GPU 上运行的演示相比，这将会比较慢。在我的
    Mac 上运行原始演示的基准性能为 `4537 ms`。4.5 秒的反馈时间。天啊。还有很多工作要做。
- en: 'To start, I knew that the `SILENCE_TIMEOUT` used to wait for speech to end
    was rather long. For the original demo, I used 1000 ms, which basically means
    a speaker has to stop talking for a full second before we’ll even start the long
    response process. After some trial and error, I figured 500 ms was a “passable”
    hyperparameter. After adjusting this down, the latency change was almost exactly
    correlated to the dip: `4079 ms`.'
  id: totrans-split-77
  prefs: []
  type: TYPE_NORMAL
  zh: 起初，我知道等待语音结束的 `SILENCE_TIMEOUT` 时间相当长。在最初的演示中，我使用了 1000 毫秒，这基本上意味着说话者必须停止说话整整一秒钟，然后我们才会开始长时间的响应过程。经过一些试验和错误，我发现
    500 毫秒是一个“可以接受”的超参数。在将这个值调整下来后，延迟变化几乎完全与下降的相关性成正比：`4079 ms`。
- en: 'I had a hunch that my text to speech pipeline was not efficient. Fortunately,
    ElevenLabs gives us a nice [Latency Guide](https://elevenlabs.io/docs/api-reference/reducing-latency).
    The first suggestion is to use their turbo model by specifying `eleven_turbo_v2`.
    I set that and we got a slight performance boost: `4014 ms`.'
  id: totrans-split-78
  prefs: []
  type: TYPE_NORMAL
  zh: 我有一种预感我的文本到语音管道效率不高。幸运的是，ElevenLabs 给了我们一个很好的 [延迟指南](https://elevenlabs.io/docs/api-reference/reducing-latency)。第一个建议是通过指定
    `eleven_turbo_v2` 来使用他们的 turbo 模型。我设置了这个值，我们得到了轻微的性能提升：`4014 ms`。
- en: 'Next, they suggest adding `optimize_streaming_latency`. I set the value to
    `3` and we get: `3791 ms`. Their next suggestion is to use a pre-made voice. I
    actually didn’t realize until much later that I was not using a pre-made voice
    so I don’t have a comparison for how that change impacted latency.'
  id: totrans-split-79
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，他们建议添加 `optimize_streaming_latency`。我将值设置为 `3`，我们得到了：`3791 ms`。他们的下一个建议是使用预制的语音。直到很久以后我才意识到我没有使用预制的语音，所以我没有比较这种变化对延迟的影响。
- en: 'Now it says to limit closing WebSocket connections. my current implementation
    opens a connection everytime it speaks – which is not good. Basically every “turn”
    has to establish a new websocket connection. Additionally, ElevenLabs has a timeout
    of 20s from when you connect. So you need to send a message at least every 20s.
    I considered 2 options at this point:'
  id: totrans-split-80
  prefs: []
  type: TYPE_NORMAL
  zh: 现在建议限制关闭 WebSocket 连接。我的当前实现每次讲话都会打开一个连接，这不是一个好的做法。基本上每一个“回合”都需要建立一个新的 WebSocket
    连接。此外，ElevenLabs 从连接开始有 20 秒的超时时间。因此，你需要至少每 20 秒发送一条消息。在这一点上，我考虑了两个选项：
- en: Open a global WebSocket connection, or maybe even a pool of connections, and
    try to keep the connection alive. But that seems really wasteful, and I don’t
    think is the intended use of their API
  id: totrans-split-81
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打开一个全局 WebSocket 连接，或者甚至是一个连接池，并尝试保持连接活动。但这似乎非常浪费，并且我认为这不是他们 API 的预期使用方式。
- en: Open a WebSocket connection when convo starts. We don’t have to worry about
    20s pauses
  id: totrans-split-82
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 开始对话时打开一个 WebSocket 连接。我们不必担心 20 秒的暂停。
- en: I decided to go with option 2, but I still think there are some drawbacks and
    considerations for a production system. The implementation I used opens a websocket
    connection on first “speak” and stores the connection PID as an assign in the
    LiveView socket. If you have a system with potentially many concurrent users speaking,
    you run the risk of creating a potentially unbounded number of connections. A
    more robust solution would probably use connection pools; however, I’m not really
    worried about traffic or scaling here.
  id: totrans-split-83
  prefs: []
  type: TYPE_NORMAL
  zh: 我决定选择第二个选项，但我仍然认为在生产系统中有一些缺点和需要考虑的地方。我使用的实现在第一次“讲话”时打开 WebSocket 连接，并将连接 PID
    存储为 LiveView socket 中的一个分配。如果你的系统可能有许多并发用户在讲话，你就有可能创建一个潜在无限数量的连接的风险。一个更健壮的解决方案可能会使用连接池；然而，在这里，我并不真的担心流量或扩展问题。
- en: While adding this optimization, I struggled a bit because ElevenLabs would send
    the first frame back, then cut off. Then I realized that it was waiting to generate
    becuase it thought I was going to send more frames. So I needed to “flush” the
    generation after I finished sending my tokens. This also seemed to fix unnatural
    audio problems I was having. After applying this optimization, our time to first
    spoken word was slightly lower in the `3700 ms` range.
  id: totrans-split-84
  prefs: []
  type: TYPE_NORMAL
  zh: 在添加这个优化时，我遇到了一些困难，因为 ElevenLabs 会发送第一帧回来，然后就中断了。后来我意识到它是在等待生成，因为它以为我会发送更多的帧。所以我需要在发送完我的令牌后“刷新”生成。这似乎也解决了我遇到的不自然的音频问题。应用这一优化后，我们的首次说话时间在
    `3700 ms` 左右。
- en: After perusing their docs a bit more, I learned that ElevenLabs will send PCM
    buffers instead of MP3\. Web Browser’s have to decode MP3 to PCM, which potentially
    introduces some overhead. One drawback is that you need to be on the independent
    creator tier to receive PCM instead of MP3\. Now, if you’re wondering if I spent
    $99 to save some milliseconds for a meaningless demo, the answer is absolutely
    yes I did.
  id: totrans-split-85
  prefs: []
  type: TYPE_NORMAL
  zh: 在进一步研究他们的文档后，我了解到ElevenLabs将发送PCM缓冲而不是MP3。Web浏览器必须将MP3解码为PCM，这可能会引入一些额外开销。一个缺点是，你需要处于独立创作者层级才能接收PCM而不是MP3。现在，如果你想知道我是否花了99美元来节省一些毫秒的无意义演示，答案是绝对是的。
- en: At this point, I believe I’ve exhausted a lot of the “easy” optimizations for
    TTS latency. One thing that does bother me about the ElevenLabs Websocket API
    is that there’s no way to receive binary payloads instead of JSON payloads. This
    is probably because they send alignment data, but I’m not using the alignment
    data here. When handling an incoming frame from their API we have to first decode
    the JSON, and then decode the Base64 encoded audio buffer. I’m not sure what the
    latency impact is, but I’m sure we could shave *some* time by avoiding both of
    these conversions. I also think the Base64 representation results in slightly
    larger buffers which could impact network latency.
  id: totrans-split-86
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一点上，我相信我已经用完了很多关于TTS延迟的“简单”优化。有一件事让我不太高兴的是，ElevenLabs Websocket API没有办法接收二进制载荷而不是JSON载荷。这可能是因为他们发送了对齐数据，但我在这里没有使用对齐数据。当处理来自他们API的传入帧时，我们必须首先解码JSON，然后解码Base64编码的音频缓冲区。我不确定延迟的影响是什么，但我确信如果避免这两种转换，我们可以节省一些时间。我还认为Base64表示会导致稍大的缓冲区，这可能会影响网络延迟。
- en: 'The next area I looked to improve was the speech-to-text pipeline. I am using
    `Nx.Serving` specifically for Speech-to-Text. The benefit of this approach is
    that we can avoid an additional network call just for transcription. Of course,
    that assumes our transcription pipeline can run fast enough on our own hardware.
    XLA is notoriously slow on CPUs (it’s getting better). The first “optimization”
    I did was to switch to my GPU: `2050 ms`'
  id: totrans-split-87
  prefs: []
  type: TYPE_NORMAL
  zh: 我接下来想要改进的是语音转文本管道。我特别使用`Nx.Serving`来进行语音转文本。这种方法的好处是我们可以避免因为转录而额外进行网络调用。当然，这假设我们的转录管道在我们自己的硬件上运行足够快。XLA在CPU上是出了名的慢（正在变得更好）。我做的第一个“优化”是切换到我的GPU：`2050
    ms`
- en: And that right there is a bitter lesson, because it’s the largest performance
    boost we’re going to get.
  id: totrans-split-88
  prefs: []
  type: TYPE_NORMAL
  zh: 而这正是一个痛苦的教训，因为这是我们能得到的最大的性能提升。
- en: 'Next, I realized the model isn’t using F16, which can introduce some solid
    speed-ups: `1800 ms`. Now, there are probably some additional optimizations we
    could add to Nx and EXLA specifically. For example, we don’t have a flash attention
    implementation. Of course, XLA does a great job of applying similar optimizations
    as a baseline, so I’m not sure how much it would help. There’s also [fast JAX
    implementations of Whisper](https://github.com/sanchit-gandhi/whisper-jax) that
    claim up to 70x speed ups. One issue with a lof of these claimed speed-ups; however,
    is that they are almost **always** for long audio sequences. GPUs and TPUs do
    well with large batch sizes and sequence lengths, but not for batch size 1 and
    short sequence lengths like we care about in this implementation. One day I may
    go down the performance hole of fast batch size 1 transcription, but today is
    not that day.'
  id: totrans-split-89
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我意识到模型没有使用F16，这可能会引入一些明显的加速：`1800 ms`。现在，我们可能还可以为Nx和EXLA添加一些额外的优化。例如，我们没有快速的注意力实现。当然，XLA在应用类似优化作为基线时表现得很出色，所以我不确定它会帮助多少。还有[Whisper的快速JAX实现](https://github.com/sanchit-gandhi/whisper-jax)，声称可以加速达到70倍。然而，这些声称的加速几乎总是针对长音频序列。GPU和TPU在大批量和序列长度方面表现良好，但不适合我们在这个实现中关心的单批量和短序列长度。有一天我可能会深入研究批量大小为1的快速转录性能，但今天不是那一天。
- en: At this point, I had moved on to improving some of the failure modes of my demo.
    While doing so, I learned much more about audio than I had previously known, and
    realized that the configuration I used to record audio can significantly improve
    whisper performance as well. Turns out there’s a [nice guide](https://dev.to/mxro/optimise-openai-whisper-api-audio-format-sampling-rate-and-quality-29fj)
    of somebody discussing parameters that work. Specifically, you should use 16 kHz
    sample rate for transcriptions. Reducing the sample rate also should reduce network
    overhead because we have less data, but it could reduce quality of the transcription.
    Oh well. Additionally, I realized I wasn’t using a pre-made ElevenLabs voice.
    After introducing both of these optimizations, I was able to achieve `1520 ms`
    turn time.
  id: totrans-split-90
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一点上，我已经开始改进我的演示的一些失败模式。在这样做的过程中，我学到了比以前更多的关于音频的知识，并意识到我用来录制音频的配置可以显著提高Whisper的性能。结果发现有一个[不错的指南](https://dev.to/mxro/optimise-openai-whisper-api-audio-format-sampling-rate-and-quality-29fj)介绍了一些可以工作的参数。具体来说，应该使用16kHz的采样率进行转录。减少采样率还应该减少网络开销，因为数据量减少了，但可能会降低转录的质量。哦，好吧。此外，我意识到我没有使用预制的ElevenLabs语音。在引入了这两个优化措施后，我能够实现`1520
    ms`的转换时间。
- en: 'Finally, I realized I was doing all of my benchmarks on a development server.
    I switched my phoenix environment from `dev` to `prod` and got: `1375 ms`. So,
    with all of these optimizations we’re sitting at about 1.3s turn around time in
    a conversation. When conversing, it starts to feel somewhat close to natural.
    I should also point out that this is also running over Tailscale, so there is
    about 100 ms ping between my Mac and the server running on my GPU. When I run
    this locally on my GPU, I can consistently get about `1000 ms` and sometimes `900
    ms` turn around time. Still, unfortunately, this does not match Retell’s latency.
    According to them, they are able to achieve `800 ms` consistently. I have some
    musings at the end about how this is possible.'
  id: totrans-split-91
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我意识到我所有的基准测试都是在开发服务器上进行的。我将我的Phoenix环境从`dev`切换到`prod`后得到了：`1375 ms`。因此，通过所有这些优化，我们在对话中的响应时间约为1.3秒。在对话中，开始感觉接近自然了。我还应该指出，这也是在Tailscale上运行的，所以我的Mac和运行在GPU上的服务器之间大约有100ms的ping。当我在我的GPU上本地运行时，我可以稳定地得到约`1000
    ms`，有时`900 ms`的转换时间。不幸的是，这仍然无法与Retell的延迟匹配。据他们称，他们能够稳定地达到`800 ms`。关于这如何可能的问题，我最后有些感想。
- en: I believe the biggest area I could improve the implementation is to use a better
    VAD implementation that relies on small rolling windows of activity rather than
    frames. We could probably get away with using 20-30 ms windows, which could theoretically
    offer a 480 ms latency improvement. I would like to eventually explore this.
  id: totrans-split-92
  prefs: []
  type: TYPE_NORMAL
  zh: 我认为我可以改进实现的最大领域是使用一个更好的VAD实现，它依赖于小的活动滚动窗口而不是帧。我们可能可以使用20-30毫秒的窗口，这理论上可以提供480毫秒的延迟改进。我最终想探索这一点。
- en: In all honesty though, I think that is a significant improvement, and I could
    *probably* stop right here and be done with it.
  id: totrans-split-93
  prefs: []
  type: TYPE_NORMAL
  zh: 说实话，我认为这是一个显著的改进，我*可能*可以在这里停下来并完成它。
- en: If I were to keep going, I would explore using a local LLM with Nx and Bumblebee.
    Nx and Bumblebee support LLMs like Mistral and Llama out-of-the box. And our text
    generation servings support streaming. That means we can possibly eliminate any
    network latency to OpenAI, and instead run 2 of the 3 models locally. One issue
    with this is that Nx currently does not have any quantized inference support (it’s
    coming I promise), so my single 4090 is not sufficient to deploy both Whisper
    and Mistral. Fortunately, the folks at [Fly.io](https://fly.io/gpu) were kind
    enough to give me access to some 80GB A100s. I will post a demo when I get one
    deployed 🙂
  id: totrans-split-94
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我继续下去，我将探索使用本地LLM与Nx和Bumblebee。Nx和Bumblebee支持像Mistral和Llama这样的LLM开箱即用。我们的文本生成服务支持流式处理。这意味着我们可能可以消除到OpenAI的任何网络延迟，而是在本地运行其中的3个模型中的2个。其中一个问题是，Nx目前没有任何量化推理支持（我承诺这是在进行中），所以我的单个4090不足以部署Whisper和Mistral。幸运的是，[Fly.io](https://fly.io/gpu)的人们很友好地给了我一些80GB的A100。我将在部署后发布演示
    🙂
- en: Maybe one day I will implement StyleTTS2 and see how efficient we can get with
    an entirely local inference pipeline.
  id: totrans-split-95
  prefs: []
  type: TYPE_NORMAL
  zh: 也许有一天我会实现StyleTTS2，并看看我们能通过完全本地推理流水线有多高效。
- en: Improving the Conversational Experience
  id: totrans-split-96
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 改善对话体验
- en: Some people pointed out that my original demo did not have the same conversational
    experience as Retell’s, and they are absolutely right. Aside from latency, mine
    was prone to failure, picks up system sounds, picks up random noises like keyboard
    and mouse clicks, and doesn’t do well with ambient noise. They also have support
    for backchanneling, fillers and interruptions which introduces some element of
    “realness” when interacting with their agent.
  id: totrans-split-97
  prefs: []
  type: TYPE_NORMAL
  zh: 有些人指出，我的原始演示与 Retell 的对话体验不同，他们完全正确。除了延迟之外，我的系统容易出现故障，会捕捉到系统声音、随机的键盘和鼠标点击声，并且对环境噪音不敏感。他们还支持回声、填充语音和中断，这在与他们的代理进行交互时增加了一些“真实感”元素。
- en: Now I didn’t get around to adding backchannels or fillers, but I was able to
    make some slight improvements to the VAD algorithm I used, and I added support
    for interruptions.
  id: totrans-split-98
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我还没来得及添加回声或填充语音，但我能够对我使用的 VAD 算法进行一些微小的改进，并添加了对中断的支持。
- en: Fixing Some Failure Modes with VAD
  id: totrans-split-99
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用 VAD 修复一些故障模式
- en: 'The first failure mode that seems to happen is echo from the system sounds.
    Nero always records and will start transcribing after audio spikes over a certain
    threshold. After some digging into the `getUserMedia` API, I found options for
    `echoCancellation`, `noiseSuppression`, and `autoGainControl`. This is the same
    point I realized that I could specify the microphone sample rate for the optimization
    I could added from the last section. Most of these options are on by default depending
    on your browser, but I added them explicitly anyway:'
  id: totrans-split-100
  prefs: []
  type: TYPE_NORMAL
  zh: 看起来第一个失败模式是系统声音的回声。Nero 总是录制并在音频超过某个阈值后开始转录。在深入研究 `getUserMedia` API 后，我发现了
    `echoCancellation`、`noiseSuppression` 和 `autoGainControl` 的选项。我意识到，为了优化，我可以指定麦克风的采样率。这是我从上一节添加的优化点。这些选项大多数情况下是浏览器默认开启的，但我还是显式地添加了它们：
- en: '[PRE15]'
  id: totrans-split-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Now that somewhat helped, but Nero still picks up it’s own audio. This probably
    requires a more sophisticated solution, but I moved on to the next problem.
  id: totrans-split-102
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管有些帮助，但 Nero 仍然会捕捉到自己的音频。这可能需要更复杂的解决方案，但我继续处理下一个问题。
- en: 'The second obvious failure mode is the fact that it picks up keyboard clicks,
    and the silence timeout is hard to tune. My first attempt to fix this was to “ignore”
    large spikes in audio by “smoothing” the volume at each frame:'
  id: totrans-split-103
  prefs: []
  type: TYPE_NORMAL
  zh: 第二个明显的失败模式是它会捕捉键盘点击声，并且沉默超时很难调节。我第一次尝试修复这个问题是通过在每一帧中“平滑”音量来“忽略”音频中的大波动：
- en: '[PRE16]'
  id: totrans-split-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Then, with some advice from [Paulo Valente](https://twitter.com/polvalente),
    I implemented a biquad filter to with a low and high-pass in order to filter audio
    to the range of human speech:'
  id: totrans-split-105
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，在 [Paulo Valente](https://twitter.com/polvalente) 的建议下，我实施了一个双二阶滤波器，使用低通和高通滤波将音频过滤到人类语音的范围内：
- en: '[PRE17]'
  id: totrans-split-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: In practice, both of these solutions actually seemed to work decent, but they
    could absolutely be better. I know it’s possible to improve the client-side filtering
    using a rolling-window that looks energy of the speaking frequences relative to
    energy of an entire sample. But, there are also machine learning models that perform
    VAD, and have `1ms` inference times. I realized that it’s probably quicker to
    just send all of the data over the websocket in chunks, and perform VAD on the
    server. I’ll discuss that implementation a little later.
  id: totrans-split-107
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，这两种解决方案似乎都效果不错，但肯定还有改进的空间。我知道可以通过使用一个滚动窗口来改进客户端过滤，该窗口会根据发言频率的能量相对于整个样本的能量进行判断。此外，还有一些机器学习模型可以执行
    VAD，并且推断时间为 `1ms`。我意识到，通过分块方式将所有数据发送到 WebSocket，并在服务器端执行 VAD 可能更快。稍后我会详细讨论这个实现。
- en: Supporting Interruptions
  id: totrans-split-108
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 支持中断
- en: 'Next I wanted to add support for interruptions. In the Retell example, the
    speaker will cut off mid-speech if it detects that you are speaking. To implement
    this feature in Nero, I added a `pushEvent` to the Microphone hook which would
    push an `interrupt` event to the server anytime speech is detectected:'
  id: totrans-split-109
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我想要添加中断支持。在 Retell 的例子中，如果检测到你在讲话，说话人会在中途中断。为了在 Nero 中实现这一功能，我在麦克风钩子中添加了一个
    `pushEvent`，每次检测到语音时会向服务器推送一个 `interrupt` 事件：
- en: '[PRE18]'
  id: totrans-split-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'The server handles this event and broadcasts an event to the TTS channel to
    stop speaking:'
  id: totrans-split-111
  prefs: []
  type: TYPE_NORMAL
  zh: 服务器处理此事件，并向 TTS 通道广播事件以停止说话：
- en: '[PRE19]'
  id: totrans-split-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'And the channel handles the event by clearing out the output audio stream and
    queue:'
  id: totrans-split-113
  prefs: []
  type: TYPE_NORMAL
  zh: 通道通过清除输出音频流和队列来处理事件：
- en: '[PRE20]'
  id: totrans-split-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: Unfortunately, this does create a race condition. There’s a potential situation
    where a speaker interrupts and the speaking queue gets cleared on the client,
    but ElevenLabs is still streaming audio back to the server. The server is always
    going to just broadcast this info to the client, and as is the client will process
    it. This potentially creates a situation with weird continutations in the audio.
    To get around this, I refactored the TTS implementation so that each audio broadcast
    appends a 6 digit token to the payload. Then, all we need to do is keep the token
    in sync with the client and server. On the client, when processing the audio queue,
    it simply checks whether or not the token at the beginning of the payload matches,
    and if it doesn’t it ignores that sample.
  id: totrans-split-115
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，这确实会造成竞态条件。可能出现的情况是，发言者打断并且客户端上的发言队列被清除，但 ElevenLabs 仍在向服务器流式传输音频。服务器总是会将这些信息广播到客户端，并且客户端会处理它。这可能会导致音频中出现奇怪的连续性问题。为了解决这个问题，我重构了
    TTS 实现，使得每个音频广播都附加一个6位数的令牌到负载中。然后，我们只需要保持客户端和服务器的令牌同步即可。在客户端处理音频队列时，只需检查负载开头的令牌是否匹配，如果不匹配则忽略该样本。
- en: The limitation with this implementation is it does not update the chat transcript.
    It’s entirely possible because we have access to the alignment information from
    ElevenLabs, but I just didn’t implement it at this time.
  id: totrans-split-116
  prefs: []
  type: TYPE_NORMAL
  zh: 这种实现的限制是它不会更新聊天记录。这完全可能是因为我们可以从 ElevenLabs 获取对齐信息，但我目前没有实现它。
- en: Time-based Hang Ups
  id: totrans-split-117
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 时间基础的挂起
- en: 'Another thing the Retell demo has support for is cues and hang ups after a
    duration of silence. If you are silent for too long, you’ll get a cue from the
    AI speaker asking you if you’re still there. After another duration of silence,
    it will hang up. This is something that’s pretty easy to do with LiveView and
    `Process.send_after/4`:'
  id: totrans-split-118
  prefs: []
  type: TYPE_NORMAL
  zh: Retell 演示还支持在静默一段时间后的提示和挂断。如果你保持沉默太久，你会从 AI 扬声器那里得到一个提示，问你是否还在那里。在另一个静默期后，它会挂断。这在
    LiveView 和 `Process.send_after/4` 中非常容易实现：
- en: '[PRE21]'
  id: totrans-split-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'And then we can cancel the timer anytime we receive a transcription, and restart
    it after every turn speaking. Note that we can’t depend on the Phoenix `speak`
    async task ending as the trigger to send nudges. Instead, we need to push an event
    from the speaker hook that the audio has ended. This avoids a case where the speaker
    initiates a really long speech, which overlaps with the `nudge_ms` duration. Now,
    we can control the number of nudges with an assign. In my case, I just used a
    boolean:'
  id: totrans-split-120
  prefs: []
  type: TYPE_NORMAL
  zh: 每当我们收到转录时，我们可以随时取消计时器，并在每次说话后重新启动它。请注意，我们不能依赖 Phoenix `speak` 异步任务结束来触发发送提示。相反，我们需要从扬声器钩子推送一个事件，表明音频已结束。这避免了发言者启动一个非常长的讲话与
    `nudge_ms` 持续时间重叠的情况。现在，我们可以通过分配来控制提示的数量。在我的情况下，我只是使用了一个布尔值：
- en: '[PRE22]'
  id: totrans-split-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: Re-doing the Entire Thing
  id: totrans-split-122
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 重新做整个事情
- en: Somewhere along the line I realized that my attempts at engineering solid VAD
    client-side were never going to deliver the experience that I wanted. I discussed
    with [Andres Alejos](https://twitter.com/ac_alejos) a bit, and he found a [Silero
    VAD](https://github.com/snakers4/silero-vad) model which is capable of performing
    VAD in `1ms` on a single CPU thread. They also had an ONNX model—and we have a
    library in the Elixir ecosystem called [Ortex](https://github.com/elixir-nx/ortex)
    which allows us to execute ONNX models.
  id: totrans-split-123
  prefs: []
  type: TYPE_NORMAL
  zh: 某个时候，我意识到我在客户端进行可靠的 VAD 工程尝试永远无法提供我想要的体验。我与 [Andres Alejos](https://twitter.com/ac_alejos)
    讨论了一下，他找到了一个 [Silero VAD](https://github.com/snakers4/silero-vad) 模型，能够在单个 CPU
    线程上以 `1ms` 执行 VAD。他们还有一个 ONNX 模型—我们在 Elixir 生态系统中有一个叫做 [Ortex](https://github.com/elixir-nx/ortex)
    的库，可以执行 ONNX 模型。
- en: To accomodate for the new VAD model, I ended up re-implementing the original
    LiveView I had as a WebSocket. This actually works out well because the WebSocket
    server is generic, and can be consumed by any language with a WebSocket client.
    The implementation is also relatively simple, and easily expanded to accomodate
    for other LLMs, TTS, and STT models. The WebSocket implementation has low latency
    (when running on a GPU), and supports interrupts.
  id: totrans-split-124
  prefs: []
  type: TYPE_NORMAL
  zh: 为了适应新的 VAD 模型，我最终重新实现了最初的 LiveView 作为 WebSocket。这实际上效果很好，因为 WebSocket 服务器是通用的，可以被任何具有
    WebSocket 客户端的语言消费。实现也相对简单，可以轻松扩展以适应其他 LLMs、TTS 和 STT 模型。WebSocket 实现具有低延迟（在 GPU
    上运行时），并支持中断。
- en: You can find the [project on my GitHub](https://github.com/seanmor5/echo) as
    well as an [example using the server](https://github.com/seanmor5/echo_example).
  id: totrans-split-125
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以在[我的GitHub项目](https://github.com/seanmor5/echo)找到，以及一个[使用服务器的示例](https://github.com/seanmor5/echo_example)。
- en: Musings
  id: totrans-split-126
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 思索
- en: The final implementation I ended up with still does not match the quality of
    the Retell demo. That said, I think it’s a solid start for future work. I believe
    I acted with some hubris when first posting about this project, and I would like
    to say that Retell’s work should not be understated. I can appreciate the attention
    to detail that goes into making an effective conversational agent, and Retell’s
    demo shows they paid a lot of attention to the details. Kudos to them and their
    team.
  id: totrans-split-127
  prefs: []
  type: TYPE_NORMAL
  zh: 最终实现的版本仍然无法与Retell演示的质量匹敌。尽管如此，我认为这是未来工作的一个良好起点。我认为在最初发布关于这个项目时有些傲慢，我想说Retell的工作不应被低估。我能理解制作有效对话代理所需的细节关注，Retell的演示显示他们对细节非常重视。向他们及其团队致敬。
- en: I will also admit that my demo is playing to one benchmark. I’m optimizing the
    hell out of latency to support a single user—me. I think this solution would change
    if it needed to accomodate for multiple concurrent users.
  id: totrans-split-128
  prefs: []
  type: TYPE_NORMAL
  zh: 我还要承认，我的演示正在针对一个基准进行优化。我在极力优化延迟，以支持单个用户——我自己。我认为如果需要支持多个并发用户，这个解决方案将会改变。
- en: Retell’s website claims they have a conversation orchestration model under the
    hood to manage the complexities of conversation. I had my doubts about that going
    into this, but I believe it now. Whether or not this model is actually a single
    model or a series of models for VAD, adding backchannels, etc. I’m not sure. I
    think eventually it *will* be a single model, but I’m not sure if it is now, which
    leads me to my next point.
  id: totrans-split-129
  prefs: []
  type: TYPE_NORMAL
  zh: Retell的网站声称他们在后台有一个对话编排模型来管理对话的复杂性。在开始时，我对此表示怀疑，但现在我相信了。无论这个模型实际上是单一模型还是一系列用于VAD、添加反馈通道等的模型，我都不确定。我认为最终它将会成为一个单一模型，但我不确定现在是否已经是这样，这也带我到我的下一个观点。
- en: Another Bitter Lesson
  id: totrans-split-130
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 另一个苦涩的教训
- en: While doing all of these optimizations, I could not help but think that it will
    eventually be all for naught. Not because I don’t think people will find it useful,
    but because large models trained on lots of data simply seem to always beat engineering
    effort. I believe the future of this area of work is in joint models. I think
    the only way to achieve real-time conversations is to merge parts of the stack.
    I predict in less than a year we will see an incredibly capable joint speech/text
    model. I recently saw a large audio model called [Qwen-Audio](https://github.com/QwenLM/Qwen-Audio)
    that I believe is similar to what I envision.
  id: totrans-split-131
  prefs: []
  type: TYPE_NORMAL
  zh: 在进行所有这些优化的同时，我不禁在想，最终这些努力可能会付诸东流。不是因为我认为人们不会觉得它有用，而是因为大量数据训练的大模型似乎总是能够击败工程努力。我相信这个工作领域的未来在于联合模型。我认为实现实时对话的唯一方法是合并堆栈的部分。我预测不到一年内我们将看到一个非常有能力的联合语音/文本模型。我最近看到了一个称为[Qwen-Audio](https://github.com/QwenLM/Qwen-Audio)的大型音频模型，我认为这与我所设想的类似。
- en: 'Specifically, if somebody were kind enough to give me some money to throw at
    this problem, here is exactly what I would do:'
  id: totrans-split-132
  prefs: []
  type: TYPE_NORMAL
  zh: 具体来说，如果有人愿意为我解决这个问题提供资金，这里是我将要做的事情：
- en: 'Generate an [Alpaca-style](https://github.com/tatsu-lab/stanford_alpaca/blob/main/alpaca_data.json)
    and/or LLaVA-style dataset of synthetic speech. Note that it would require a bit
    of pre-processing to change Alpaca inputs to mirror a style compatible with spoken-word.
    I would use ElevenLabs to generate the dataset in mulitple voices. Of course this
    dataset would be a bit too “clean,” so we’d need to apply some augmentations which
    add ambient noise, change speaking pitch and speed, etc. Bonus points: adding
    samples of “noise” which require no response to merge the VAD part of the pipeline
    in as well. You can even throw in text prompts that dictate when and when not
    to respond to support things like [wake word detection](https://picovoice.ai/platform/porcupine/)
    without needing to train a separate model.'
  id: totrans-split-133
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个[Alpaca风格](https://github.com/tatsu-lab/stanford_alpaca/blob/main/alpaca_data.json)和/或LLaVA风格的合成语音数据集。请注意，需要进行一些预处理来改变Alpaca输入，以反映与口语兼容的风格。我将使用ElevenLabs在多个声音中生成数据集。当然，这个数据集可能过于“干净”，所以我们需要应用一些增强技术，如增加环境噪声、改变说话音调和速度等。额外加分点：添加一些“噪声”样本，无需响应以合并管道中的VAD部分。甚至可以添加文本提示，指导何时以及何时不需要响应，支持诸如[wake
    word detection](https://picovoice.ai/platform/porcupine/)之类的功能，而无需训练单独的模型。
- en: Create a LLaVA-style model with a Whisper or equivalent base, an LLM, and a
    projection layer.
  id: totrans-split-134
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个LLaVA风格的模型，使用Whisper或等效基础、LLM和投影层。
- en: Secure H100s, train model, and “turn H100s into $100s” (thank you [@thmsmlr](https://twitter.com/thmsmlr?lang=en))
  id: totrans-split-135
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 安全的H100s，训练模型，并将“H100s变成100美元”（谢谢 [@thmsmlr](https://twitter.com/thmsmlr?lang=en)）
- en: If you want to give me some $$$, my e-mail is smoriarity.5@gmail.com 🙂
  id: totrans-split-136
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想给我一些 $$$，我的电子邮件是 smoriarity.5@gmail.com 🙂
- en: I believe we are also close to just having full-on speech-to-speech models.
    A specific challenge I can see when creating these models is coming up with a
    high-quality dataset. I think if you make a deliberate attempt at “recording conversations”
    for the purposes of training, you will actually probably end up with a lower-quality
    dataset. People tend to change their behavior under observation. Additionally,
    conversations from movies and TV shows aren’t actually very natural. Even some
    podcasts have an unnatural converastional rhythm.
  id: totrans-split-137
  prefs: []
  type: TYPE_NORMAL
  zh: 我认为我们也接近完全的语音对话模型。在创建这些模型时，我能看到的一个具体挑战是如何获得高质量的数据集。我认为，如果您试图“记录对话”以进行训练，实际上可能会得到一个质量较低的数据集。人们往往在观察下改变他们的行为。此外，电影和电视节目中的对话实际上并不非常自然。甚至一些播客也有不自然的对话节奏。
- en: While watching [Love is Blind](https://en.wikipedia.org/wiki/Love_Is_Blind_(TV_series))
    with my fiancé, I realized you could probably get a decent amount of quality data
    from reality tv shows. The conversations in reality TV are overly dramatic and
    chaotic, but are (I think) closer to realistic than anything else.
  id: totrans-split-138
  prefs: []
  type: TYPE_NORMAL
  zh: 在和我的未婚夫一起观看 [《爱情是盲的》](https://en.wikipedia.org/wiki/Love_Is_Blind_(TV_series))
    时，我意识到你可能可以从真人秀中获得相当数量的高质量数据。真人秀中的对话过于夸张和混乱，但（我认为）比其他任何东西更接近现实。
- en: Conversational Knowledge Base?
  id: totrans-split-139
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 对话知识库？
- en: I do wonder what a solid RAG implementation looks like on top of a conversational
    agent. RAG and complex CoT pipelines will introduce latency which could deteriorate
    the conversational experience. However, there are clever ways you can hide this.
    In conversations that require “search” between humans, e.g. like scheduling an
    appointment, you’ll often have one party saying “one moment please” before performing
    a system search. Building something like that in is entirely possible. Additionally,
    if your agent requires information up front about an individual, it’s possible
    to include that in the initial prompt.
  id: totrans-split-140
  prefs: []
  type: TYPE_NORMAL
  zh: 我确实很想知道在对话代理之上，一个稳固的RAG实现看起来是什么样子。RAG和复杂的CoT管道将引入潜伏期，这可能会损害对话体验。但是，你可以采用一些巧妙的方法来隐藏这一点。在需要人与人之间“搜索”的对话中，例如安排预约，通常会有一方在执行系统搜索之前说“请稍等片刻”。完全可以构建类似的功能。此外，如果您的代理需要关于个人的信息，可以在初始提示中包含这些信息。
- en: You Should Use Elixir
  id: totrans-split-141
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 你应该使用Elixir
- en: I was very excited for this problem in particular because it’s literally the
    perfect application of Elixir and Phoenix. If you are building conversational
    agents, you should seriously consider giving Elixir a try. A large part of how
    quick this demo was to put together is because of how productive Elixir is.
  id: totrans-split-142
  prefs: []
  type: TYPE_NORMAL
  zh: 我对这个问题特别兴奋，因为它实际上是Elixir和Phoenix的完美应用。如果您正在构建对话代理，应该认真考虑尝试Elixir。这个演示能够如此快速地完成很大程度上是因为Elixir的高效率。
- en: Conclusion
  id: totrans-split-143
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 结论
- en: This was a fun technical challenge. I am pleased with the performance of the
    final demonstration. I’m also happy I was able to OSS a small library for others
    to build off of. If you are interested in conversational agents, I encourage you
    to check it out, give feedback, and contribute! I know it’s very rough right now,
    but it will get better with time.
  id: totrans-split-144
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个有趣的技术挑战。我对最终演示的表现感到满意。我也很高兴能够为其他人开发一个小型库。如果您对对话代理感兴趣，我鼓励您查看它，提供反馈并贡献！我知道现在还很粗糙，但随着时间的推移，它会变得更好。
- en: Additionally, I plan to periodically build out the rest of the Nero project,
    so please follow me on [Twitter](https://twitter.com/sean_moriarity) if you’d
    like to stay up to date.
  id: totrans-split-145
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我计划定期扩展Nero项目的其余部分，请通过 [Twitter](https://twitter.com/sean_moriarity) 关注我以获取最新信息。
