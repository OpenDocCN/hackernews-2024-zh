- en: <!--yml
  id: totrans-split-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-split-1
  prefs: []
  type: TYPE_NORMAL
  zh: 'category: 未分类'
- en: 'date: 2024-05-27 13:06:33'
  id: totrans-split-2
  prefs: []
  type: TYPE_NORMAL
  zh: 'date: 2024-05-27 13:06:33'
- en: -->
  id: totrans-split-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: Mixtral 8x22b latest-model benchmarks - Promptzone
  id: totrans-split-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Mixtral 8x22b最新模型基准测试 - Promptzone
- en: 来源：[https://www.promptzone.com/promptzone/mixtral-8x22b-latest-model-benchmarks-4mh9](https://www.promptzone.com/promptzone/mixtral-8x22b-latest-model-benchmarks-4mh9)
  id: totrans-split-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://www.promptzone.com/promptzone/mixtral-8x22b-latest-model-benchmarks-4mh9](https://www.promptzone.com/promptzone/mixtral-8x22b-latest-model-benchmarks-4mh9)
- en: 'Mistral AI, after OpenAI and Google, has quietly entered the race of Large
    Language Models by releasing one of its most potent models to date: the Mixtral
    8x22B.'
  id: totrans-split-6
  prefs: []
  type: TYPE_NORMAL
  zh: 在OpenAI和Google之后，Mistral AI悄然进入了大型语言模型竞赛，发布了迄今为止最强大的模型之一：Mixtral 8x22B。
- en: '**Highlights:**'
  id: totrans-split-7
  prefs: []
  type: TYPE_NORMAL
  zh: '**亮点：**'
- en: French startup Mistral AI has launched Mixtral 8x22B, its latest open-source
    LLM.
  id: totrans-split-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 法国初创公司Mistral AI推出了其最新的开源LLM，Mixtral 8x22B。
- en: This model employs a sophisticated Mixture of Experts (MoE) architecture and
    has shown promising initial benchmarks compared to previous models like the Mixtral
    8x7B.
  id: totrans-split-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 该模型采用复杂的专家混合（MoE）架构，并且相较于Mixtral 8x7B等先前模型，显示出了有前景的初始基准测试结果。
- en: The model weights are available for download at Hugging Face, complete with
    installation instructions.
  id: totrans-split-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型权重可在Hugging Face上下载，并附有安装说明。
- en: '**Why is Mixtral 8x22B So Powerful?**'
  id: totrans-split-11
  prefs: []
  type: TYPE_NORMAL
  zh: '**Mixtral 8x22B为何如此强大？**'
- en: The Mixtral 8x22B, utilizing the MoE architecture, boasts an impressive 176
    billion parameters and a context window of 65,000 tokens. This architecture allows
    for a sparse MoE strategy, providing access to various models each specialized
    in distinct areas, balancing performance and computational costs.
  id: totrans-split-12
  prefs: []
  type: TYPE_NORMAL
  zh: Mixtral 8x22B利用MoE架构，拥有1760亿参数和65,000个令牌的上下文窗口。该架构允许稀疏的MoE策略，提供访问各种专门领域模型，平衡了性能和计算成本。
- en: '**Accessibility and Open-Source Commitment:**'
  id: totrans-split-13
  prefs: []
  type: TYPE_NORMAL
  zh: '**可访问性和开源承诺：**'
- en: Mistral AI continues to challenge proprietary models by adhering to open-source
    principles, making Mixtral 8x22B available for torrent download via Hugging Face.
    Detailed instructions are provided for running the model at different precisions
    to accommodate varying system capabilities.
  id: totrans-split-14
  prefs: []
  type: TYPE_NORMAL
  zh: Mistral AI继续通过坚持开源原则挑战专有模型，使Mixtral 8x22B通过Hugging Face提供种子下载。提供了详细的运行模型的说明，以适应不同系统能力的需求。
- en: '**Market Position and Innovations:**'
  id: totrans-split-15
  prefs: []
  type: TYPE_NORMAL
  zh: '**市场定位与创新：**'
- en: As the latest LLM in the Generative AI market, Mixtral 8x22B stands alongside
    recent releases like Databricks’ DBRX, OpenAI’s GPT-4 Turbo Vision, and Anthropic’s
    Claude 3\. Although designed primarily as an autocomplete model, distinct from
    chat or instruct models, it offers effective computing and performance for a broad
    range of tasks.
  id: totrans-split-16
  prefs: []
  type: TYPE_NORMAL
  zh: 作为生成AI市场上最新的LLM，Mixtral 8x22B与最近发布的产品（如Databricks的DBRX、OpenAI的GPT-4 Turbo Vision和Anthropic的Claude
    3）并列。尽管其主要设计为自动补全模型，而非聊天或指导模型，但它在广泛任务中提供了有效的计算和性能。
- en: '**Benchmarks and Comparisons:**'
  id: totrans-split-17
  prefs: []
  type: TYPE_NORMAL
  zh: '**基准测试和比较：**'
- en: 'Despite the absence of official benchmarks, the Hugging Face community has
    conducted tests showing that Mixtral 8x22B closely competes with closed models
    from Google and OpenAI. It achieves notable scores in various benchmarks:'
  id: totrans-split-18
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管缺乏官方基准测试，但Hugging Face社区进行的测试显示，Mixtral 8x22B与Google和OpenAI的闭源模型密切竞争。它在各种基准测试中取得了显著分数：
- en: '**ARC-C Reasoning Abilities**: Scores 70.5, showcasing strong reasoning capabilities.'
  id: totrans-split-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**ARC-C推理能力**：得分70.5，展示出强大的推理能力。'
- en: '**Commonsense Reasoning**: Scores 88.9 on the HellaSwag benchmark, indicating
    robust commonsense reasoning skills.'
  id: totrans-split-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**常识推理**：在HellaSwag基准测试中得分为88.9，显示出强大的常识推理能力。'
- en: '**Natural Language Understanding**: Achieves a score of 77.3 in the MMLU benchmark,
    reflecting competitive NLP capabilities.'
  id: totrans-split-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**自然语言理解**：在MMLU基准测试中获得77.3分，反映出竞争力强的自然语言处理能力。'
- en: '**Truthfulness**: Exhibits improvement in truthfulness, crucial for countering
    model-generated hallucinations.'
  id: totrans-split-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**真实性**：在对抗模型生成的幻觉方面表现出改进。'
- en: '**Mathematical Reasoning**: With a score of 76.5 in the GSM8K, it''s well-suited
    for basic mathematical problem-solving.'
  id: totrans-split-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数学推理**：在GSM8K中获得76.5分，非常适合基本数学问题的解决。'
- en: '**Conclusion:**'
  id: totrans-split-24
  prefs: []
  type: TYPE_NORMAL
  zh: '**结论：**'
- en: Mistral AI's release of Mixtral 8x22B reflects a significant trend towards more
    transparent and cooperative AI development methods. The model’s potential for
    groundbreaking applications and research is generating considerable excitement
    within the AI community, promising to transform various technical fields globally.
  id: totrans-split-25
  prefs: []
  type: TYPE_NORMAL
  zh: Mistral AI发布的Mixtral 8x22B反映了向更加透明和合作的人工智能开发方法的重要趋势。该模型在人工智能社区内引起了极大的兴奋，因其具有突破性的应用和研究潜力，有望全球范围内改变各种技术领域。
