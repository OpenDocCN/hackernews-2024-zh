- en: <!--yml
  id: totrans-split-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-split-1
  prefs: []
  type: TYPE_NORMAL
  zh: 'category: 未分类'
- en: 'date: 2024-05-27 13:33:57'
  id: totrans-split-2
  prefs: []
  type: TYPE_NORMAL
  zh: 'date: 2024-05-27 13:33:57'
- en: -->
  id: totrans-split-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: 'Qwen1.5-110B: The First 100B+ Model of the Qwen1.5 Series | Qwen'
  id: totrans-split-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Qwen1.5-110B：Qwen1.5系列的第一个100B+模型 | Qwen
- en: 来源：[https://qwenlm.github.io/blog/qwen1.5-110b/](https://qwenlm.github.io/blog/qwen1.5-110b/)
  id: totrans-split-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://qwenlm.github.io/blog/qwen1.5-110b/](https://qwenlm.github.io/blog/qwen1.5-110b/)
- en: '[GITHUB](https://github.com/QwenLM/Qwen1.5) [HUGGING FACE](https://huggingface.co/Qwen)
    [MODELSCOPE](https://modelscope.cn/organization/qwen) [DEMO](https://huggingface.co/spaces/Qwen/Qwen1.5-110B-Chat-Demo)
    [DISCORD](https://discord.gg/yPEP2vHTu4)'
  id: totrans-split-6
  prefs: []
  type: TYPE_NORMAL
  zh: '[GITHUB](https://github.com/QwenLM/Qwen1.5) [HUGGING FACE](https://huggingface.co/Qwen)
    [MODELSCOPE](https://modelscope.cn/organization/qwen) [DEMO](https://huggingface.co/spaces/Qwen/Qwen1.5-110B-Chat-Demo)
    [DISCORD](https://discord.gg/yPEP2vHTu4)'
- en: Introduction[#](#introduction)
  id: totrans-split-7
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Introduction[#](#introduction)
- en: Recently we have witnessed a burst of large-scale models with over 100 billion
    parameters in the opensource community. These models have demonstrated remarkable
    performance in both benchmark evaluation and chatbot arena. Today, we release
    the first 100B+ model of the Qwen1.5 series, Qwen1.5-110B, which achieves comparable
    performance with Meta-Llama3-70B in the base model evaluation, and outstanding
    performance in the chat evaluation, including MT-Bench and AlpacaEval 2.0.
  id: totrans-split-8
  prefs: []
  type: TYPE_NORMAL
  zh: 最近我们在开源社区中见证了一波超过1000亿参数的大型模型的涌现。这些模型在基准评估和聊天机器人领域表现出色。今天，我们发布了Qwen1.5系列的第一个100B+模型，Qwen1.5-110B，在基础模型评估中与Meta-Llama3-70B有着可比较的表现，在包括MT-Bench和AlpacaEval
    2.0在内的聊天评估中表现出色。
- en: Model Features[#](#model-features)
  id: totrans-split-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Model Features[#](#model-features)
- en: Qwen1.5-110B is similar to other Qwen1.5 models and built with the same Transformer
    decoder architecture. It consists of grouped query attention (GQA) and it can
    be efficient in model serving. The model supports the context length 32K tokens,
    and the model is still multilingual, supporting a large number of languages including
    English, Chinese, French, Spanish, German, Russian, Korean, Japanese, Vietnamese,
    Arabic, etc.
  id: totrans-split-10
  prefs: []
  type: TYPE_NORMAL
  zh: Qwen1.5-110B与其他Qwen1.5模型类似，采用了相同的Transformer解码器架构。它包括分组查询注意力（GQA），在模型服务中可以高效运行。该模型支持32K令牌的上下文长度，仍然是多语言的，支持包括英语、中文、法语、西班牙语、德语、俄语、韩语、日语、越南语、阿拉伯语等多种语言。
- en: Model Quality[#](#model-quality)
  id: totrans-split-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Model Quality[#](#model-quality)
- en: We conduct a series of evaluations for the base language models, and we compare
    with Meta-Llama3-70B, the recent SOTA language model as well as Mixtral-8x22B.
  id: totrans-split-12
  prefs: []
  type: TYPE_NORMAL
  zh: 我们对基础语言模型进行了一系列评估，并与最近的SOTA语言模型Meta-Llama3-70B以及Mixtral-8x22B进行了比较。
- en: '|  | Qwen1.5-110B | Qwen1.5-72B | Llama-3-70B | Mixtral-8x22B |'
  id: totrans-split-13
  prefs: []
  type: TYPE_TB
  zh: '|  | Qwen1.5-110B | Qwen1.5-72B | Llama-3-70B | Mixtral-8x22B |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-split-14
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| MMLU | 80.4 | 77.5 | 79.5 | 77.8 |'
  id: totrans-split-15
  prefs: []
  type: TYPE_TB
  zh: '| MMLU | 80.4 | 77.5 | 79.5 | 77.8 |'
- en: '| TheoremQA | 34.9 | 29.3 | 32.0 | 35.9 |'
  id: totrans-split-16
  prefs: []
  type: TYPE_TB
  zh: '| TheoremQA | 34.9 | 29.3 | 32.0 | 35.9 |'
- en: '| GPQA | 35.9 | 36.3 | 36.4 | 34.3 |'
  id: totrans-split-17
  prefs: []
  type: TYPE_TB
  zh: '| GPQA | 35.9 | 36.3 | 36.4 | 34.3 |'
- en: '| Hellaswag | 87.5 | 86.0 | 88.0 | 88.7 |'
  id: totrans-split-18
  prefs: []
  type: TYPE_TB
  zh: '| Hellaswag | 87.5 | 86.0 | 88.0 | 88.7 |'
- en: '| BBH | 74.8 | 65.5 | 76.6 | 69.2 |'
  id: totrans-split-19
  prefs: []
  type: TYPE_TB
  zh: '| BBH | 74.8 | 65.5 | 76.6 | 69.2 |'
- en: '| ARC-C | 69.6 | 65.9 | 68.8 | 70.7 |'
  id: totrans-split-20
  prefs: []
  type: TYPE_TB
  zh: '| ARC-C | 69.6 | 65.9 | 68.8 | 70.7 |'
- en: '| GSM8K | 85.4 | 79.5 | 79.2 | 78.6 |'
  id: totrans-split-21
  prefs: []
  type: TYPE_TB
  zh: '| GSM8K | 85.4 | 79.5 | 79.2 | 78.6 |'
- en: '| MATH | 49.6 | 34.1 | 41.0 | 41.7 |'
  id: totrans-split-22
  prefs: []
  type: TYPE_TB
  zh: '| MATH | 49.6 | 34.1 | 41.0 | 41.7 |'
- en: '| HumanEval | 52.4 | 41.5 | 45.7 | 45.1 |'
  id: totrans-split-23
  prefs: []
  type: TYPE_TB
  zh: '| HumanEval | 52.4 | 41.5 | 45.7 | 45.1 |'
- en: '| MBPP | 58.1 | 53.4 | 55.1 | 71.2 |'
  id: totrans-split-24
  prefs: []
  type: TYPE_TB
  zh: '| MBPP | 58.1 | 53.4 | 55.1 | 71.2 |'
- en: The above results show that the new 110B model is at least competitive with
    the Llama-3-70B model in terms of base capabilities. In terms of this model, we
    did not change the pretraining and posttraining recipes drastically, and thus
    we believe that the performance improvement compared with 72B comes from increasing
    model size.
  id: totrans-split-25
  prefs: []
  type: TYPE_NORMAL
  zh: 上述结果显示，新的110B模型至少在基础能力方面与Llama-3-70B模型相媲美。在该模型中，我们没有大幅改变预训练和后训练配方，因此我们相信与72B相比的性能提升来自增加模型大小。
- en: We also test the chat models on MT-Bench and AlpacaEval 2.0.
  id: totrans-split-26
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还在MT-Bench和AlpacaEval 2.0上测试了聊天模型。
- en: '| Models | MT-Bench | AlpacaEval 2.0 |'
  id: totrans-split-27
  prefs: []
  type: TYPE_TB
  zh: '| Models | MT-Bench | AlpacaEval 2.0 |'
- en: '| Avg. Score | LC Win Rate |'
  id: totrans-split-28
  prefs: []
  type: TYPE_TB
  zh: '| 平均分数 | LC胜率 |'
- en: '| Llama-3-70B-Instruct | 8.85 | 34.40 |'
  id: totrans-split-29
  prefs: []
  type: TYPE_TB
  zh: '| Llama-3-70B-Instruct | 8.85 | 34.40 |'
- en: '| Qwen1.5-72B-Chat | 8.61 | 36.60 |'
  id: totrans-split-30
  prefs: []
  type: TYPE_TB
  zh: '| Qwen1.5-72B-Chat | 8.61 | 36.60 |'
- en: '| Qwen1.5-110B-Chat | 8.88 | 43.90 |'
  id: totrans-split-31
  prefs: []
  type: TYPE_TB
  zh: '| Qwen1.5-110B-Chat | 8.88 | 43.90 |'
- en: Compared with the previously released 72B model, on the two benchmark evaluation
    for chat models the 110B performs significantly better. The consistent improvement
    in the evaluation indicates that stronger and larger base language models can
    lead to better chat models even without changing the post-training recipes much.
  id: totrans-split-32
  prefs: []
  type: TYPE_NORMAL
  zh: 与之前发布的72B模型相比，在两个聊天模型的基准评估中，110B表现显著更好。评估中的持续改进表明，更强大更大的基础语言模型即使在不大幅改变训练后配方的情况下，也能导致更好的聊天模型。
- en: Develop with Qwen1.5-110B[#](#develop-with-qwen15-110b)
  id: totrans-split-33
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Qwen1.5-110B进行开发[#](#develop-with-qwen15-110b)
- en: We advise you to read our blog for [Qwen1.5](https://qwenlm.github.io/blog/qwen1.5/)
    to figure out the usages with Transformers, vLLM, llama.cpp, Ollama, LMStudio,
    SkyPilot, Axolotl, LLaMA-Factory, etc.
  id: totrans-split-34
  prefs: []
  type: TYPE_NORMAL
  zh: 我们建议您阅读我们的博客，了解有关使用Transformers、vLLM、llama.cpp、Ollama、LMStudio、SkyPilot、Axolotl、LLaMA-Factory等内容的[Qwen1.5](https://qwenlm.github.io/blog/qwen1.5/)用法。
- en: Conclusion[#](#conclusion)
  id: totrans-split-35
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论[#](#conclusion)
- en: The Qwen1.5-110B is the largest model in the Qwen1.5 series, and it is also
    the first one with over 100 billion parameters in the series. It demonstrates
    competitive performance against the very recently released SOTA model Llama-3-70B
    and it is significantly better than the 72B model. This tells us that there is
    still a lot of room in model size scaling for better performance. While the releease
    of Llama-3 indicates the significance of data scaling to an extremely large scale,
    we believe we can get the best of both worlds by scaling both data and model size
    in our future release. Stay tuned for Qwen2!
  id: totrans-split-36
  prefs: []
  type: TYPE_NORMAL
  zh: Qwen1.5-110B是Qwen1.5系列中最大的模型，也是该系列中首个具有超过1000亿参数的模型。它展示了与最近发布的SOTA模型Llama-3-70B的竞争性表现，并且明显优于72B模型。这告诉我们，在模型尺寸扩展方面还有很大的优化空间。虽然Llama-3的发布表明数据扩展至极大规模的重要性，但我们相信通过在未来的发布中同时扩展数据和模型尺寸，我们可以兼顾两者的优势。敬请关注Qwen2的发布！
- en: Citation[#](#citation)
  id: totrans-split-37
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 引用[#](#citation)
- en: '[PRE0]'
  id: totrans-split-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
