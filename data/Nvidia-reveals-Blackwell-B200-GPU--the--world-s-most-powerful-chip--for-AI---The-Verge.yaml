- en: <!--yml
  id: totrans-split-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-split-1
  prefs: []
  type: TYPE_NORMAL
  zh: 'category: 未分类'
- en: 'date: 2024-05-27 15:04:12'
  id: totrans-split-2
  prefs: []
  type: TYPE_NORMAL
  zh: 'date: 2024-05-27 15:04:12'
- en: -->
  id: totrans-split-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: Nvidia reveals Blackwell B200 GPU, the ‘world’s most powerful chip’ for AI -
    The Verge
  id: totrans-split-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Nvidia发布了Blackwell B200 GPU，这是用于AI的“世界上最强大的芯片” - The Verge
- en: 来源：[https://www.theverge.com/2024/3/18/24105157/nvidia-blackwell-gpu-b200-ai](https://www.theverge.com/2024/3/18/24105157/nvidia-blackwell-gpu-b200-ai)
  id: totrans-split-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://www.theverge.com/2024/3/18/24105157/nvidia-blackwell-gpu-b200-ai](https://www.theverge.com/2024/3/18/24105157/nvidia-blackwell-gpu-b200-ai)
- en: Nvidia’s must-have H100 AI chip made it [a multitrillion-dollar company](/2024/2/23/24080975/nvidia-ai-chips-h100-h200-market-capitalization),
    one that may be worth [more than Alphabet and Amazon](/2024/2/14/24073384/nvidia-market-cap-passes-amazon-alphabet),
    and competitors have been [fighting to catch up](/2024/2/1/24058186/ai-chips-meta-microsoft-google-nvidia).
    But perhaps Nvidia is about to extend its lead — with the new Blackwell B200 GPU
    and GB200 “superchip.”
  id: totrans-split-6
  prefs: []
  type: TYPE_NORMAL
  zh: Nvidia的必备H100 AI芯片使其成为[一家价值数万亿美元的公司](/2024/2/23/24080975/nvidia-ai-chips-h100-h200-market-capitalization)，可能超过了Alphabet和Amazon，并且竞争对手一直在[努力追赶](/2024/2/1/24058186/ai-chips-meta-microsoft-google-nvidia)。但也许Nvidia即将通过新的Blackwell
    B200 GPU和GB200“超级芯片”扩大其领先优势。
- en: '*Nvidia CEO Jensen Huang holds up his new GPU on the left, next to an H100
    on the right, from the GTC livestream.*'
  id: totrans-split-7
  prefs: []
  type: TYPE_NORMAL
  zh: '*Nvidia CEO黄仁勋在GTC直播中展示他的新GPU（左）和右侧的H100。*'
- en: 'Image: Nvidia'
  id: totrans-split-8
  prefs: []
  type: TYPE_NORMAL
  zh: 图片：Nvidia
- en: Nvidia says the new B200 GPU offers up to 20 *petaflops* of FP4 horsepower from
    its 208 billion transistors. Also, it says, a GB200 that combines two of those
    GPUs with a single Grace CPU can offer 30 times the performance for LLM inference
    workloads while also potentially being substantially more efficient. It “reduces
    cost and energy consumption by up to 25x” over an H100, says Nvidia, though there’s
    a questionmark around cost — [Nvidia’s CEO has suggested](/2024/3/19/24106141/nvidia-ceo-says-his-blackwell-b200-ai-gpu-will-cost-30k-40k-a-pop)
    each GPU might cost between $30,000 and $40,000\.
  id: totrans-split-9
  prefs: []
  type: TYPE_NORMAL
  zh: Nvidia表示，新的B200 GPU从其2080亿个晶体管中提供高达20 *petaflops*的FP4运算能力。此外，它表示，一个结合了两个GPU和单个Grace
    CPU的GB200，可以在LLM推理工作负载中提供30倍的性能，同时可能更加高效。Nvidia称其“可以将成本和能源消耗降低多达25倍”比H100，尽管成本尚存在疑问
    — [Nvidia的CEO暗示](/2024/3/19/24106141/nvidia-ceo-says-his-blackwell-b200-ai-gpu-will-cost-30k-40k-a-pop)
    每个GPU可能在30,000到40,000美元之间。
- en: Training a 1.8 trillion parameter model would have previously taken 8,000 Hopper
    GPUs and 15 megawatts of power, Nvidia claims. Today, Nvidia’s CEO says 2,000
    Blackwell GPUs can do it while consuming just four megawatts.
  id: totrans-split-10
  prefs: []
  type: TYPE_NORMAL
  zh: Nvidia声称，以前，训练一个1.8万亿参数模型需要8,000个Hopper GPU和15兆瓦的电力。如今，Nvidia的CEO表示，2,000个Blackwell
    GPU只需4兆瓦即可完成。
- en: On a GPT-3 LLM benchmark with 175 billion parameters, Nvidia says the GB200
    has a somewhat more modest seven times the performance of an H100, and Nvidia
    says it offers four times the training speed.
  id: totrans-split-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在一个具有1750亿个参数的GPT-3 LLM基准测试中，Nvidia表示GB200的性能比H100略高七倍，并且Nvidia表示它提供四倍的训练速度。
- en: '*Here’s what one GB200 looks like. Two GPUs, one CPU, one board.*'
  id: totrans-split-12
  prefs: []
  type: TYPE_NORMAL
  zh: '*这就是一台GB200的外观。两个GPU，一个CPU，一个主板。*'
- en: 'Image: Nvidia'
  id: totrans-split-13
  prefs: []
  type: TYPE_NORMAL
  zh: 图片：Nvidia
- en: 'Nvidia told journalists one of the key improvements is a second-gen transformer
    engine that doubles the compute, bandwidth, and model size by using four bits
    for each neuron instead of eight (thus, the 20 petaflops of FP4 I mentioned earlier).
    A second key difference only comes when you link up huge numbers of these GPUs:
    a next-gen NVLink switch that lets 576 GPUs talk to each other, with 1.8 terabytes
    per second of bidirectional bandwidth.'
  id: totrans-split-14
  prefs: []
  type: TYPE_NORMAL
  zh: Nvidia告诉记者，其中一个关键改进是第二代变压器引擎，通过每个神经元使用四位而不是八位（因此，我之前提到的20 petaflops的FP4）。第二个关键区别仅在连接大量这些GPU时出现：一款下一代NVLink开关，可让576个GPU相互通信，具有每秒1.8TB的双向带宽。
- en: 'That required Nvidia to build an entire new network switch chip, one with 50
    billion transistors and some of its own onboard compute: 3.6 teraflops of FP8,
    says Nvidia.'
  id: totrans-split-15
  prefs: []
  type: TYPE_NORMAL
  zh: 这要求Nvidia建造了一整套全新的网络交换芯片，拥有50亿晶体管和一些自身的板载计算能力：Nvidia称其为3.6 teraflops的FP8。
- en: '*Nvidia says it’s adding both FP4 and FP6 with Blackwell.*'
  id: totrans-split-16
  prefs: []
  type: TYPE_NORMAL
  zh: '*Nvidia表示，它正在Blackwell中添加FP4和FP6。*'
- en: 'Image: Nvidia'
  id: totrans-split-17
  prefs: []
  type: TYPE_NORMAL
  zh: 图片：Nvidia
- en: Previously, Nvidia says, a cluster of just 16 GPUs would spend 60 percent of
    their time communicating with one another and only 40 percent actually computing.
  id: totrans-split-18
  prefs: []
  type: TYPE_NORMAL
  zh: Nvidia表示，以前，仅16个GPU的集群会花费60％的时间进行通信，实际计算仅占40％。
- en: Nvidia is counting on companies to buy large quantities of these GPUs, of course,
    and is packaging them in larger designs, like the GB200 NVL72, which plugs 36
    CPUs and 72 GPUs into a single liquid-cooled rack for a total of 720 petaflops
    of AI training performance or 1,440 petaflops (aka 1.4 *exaflops*) of inference.
    It has nearly two miles of cables inside, with 5,000 individual cables.
  id: totrans-split-19
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，Nvidia 指望公司购买大量这些 GPU，并将它们打包成更大的设计，例如 GB200 NVL72，该设计将 36 个 CPU 和 72 个 GPU
    集成到单个液冷机架中，总共提供 720 petaflops 的 AI 训练性能或者 1,440 petaflops（即 1.4 exaflops）的推断性能。内部有将近两英里长的电缆，有
    5,000 条单独的电缆。
- en: '*The GB200 NVL72\.*'
  id: totrans-split-20
  prefs: []
  type: TYPE_NORMAL
  zh: GB200 NVL72。
- en: 'Image: Nvidia'
  id: totrans-split-21
  prefs: []
  type: TYPE_NORMAL
  zh: 图片：Nvidia
- en: Each tray in the rack contains either two GB200 chips or two NVLink switches,
    with 18 of the former and nine of the latter per rack. In total, Nvidia says one
    of these racks can support a 27-trillion parameter model. GPT-4 is rumored to
    be around a 1.7-trillion parameter model.
  id: totrans-split-22
  prefs: []
  type: TYPE_NORMAL
  zh: 机架中的每个托盘都含有两个 GB200 芯片或两个 NVLink 开关，每个机架分别有 18 个前者和 9 个后者。总体而言，Nvidia 表示这样的一个机架可以支持一个
    27 万亿参数模型。传闻中，GPT-4 的参数约为 1.7 万亿。
- en: The company says Amazon, Google, Microsoft, and Oracle are all already planning
    to offer the NVL72 racks in their cloud service offerings, though it’s not clear
    how many they’re buying.
  id: totrans-split-23
  prefs: []
  type: TYPE_NORMAL
  zh: 公司表示，亚马逊、谷歌、微软和甲骨文都已经计划在其云服务中提供 NVL72 机架，尽管目前尚不清楚他们打算购买多少。
- en: And of course, Nvidia is happy to offer companies the rest of the solution,
    too. Here’s the DGX Superpod for DGX GB200, which combines eight systems in one
    for a total of 288 CPUs, 576 GPUs, 240TB of memory, and 11.5 exaflops of FP4 computing.
  id: totrans-split-24
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，Nvidia 也乐意为公司提供其余的解决方案。这里是 DGX GB200 的 DGX Superpod，将八个系统合并为一个，总共包括 288 个
    CPU、576 个 GPU、240TB 内存和 11.5 exaflops 的 FP4 计算能力。
- en: Nvidia says its systems can scale to tens of thousands of the GB200 superchips,
    connected together with 800Gbps networking with its new Quantum-X800 InfiniBand
    (for up to 144 connections) or Spectrum-X800 ethernet (for up to 64 connections).
  id: totrans-split-25
  prefs: []
  type: TYPE_NORMAL
  zh: Nvidia 表示其系统可以扩展到数万个 GB200 超芯片，使用新的 Quantum-X800 InfiniBand（最多可达 144 个连接）或 Spectrum-X800
    以太网（最多可达 64 个连接）进行 800Gbps 网络连接。
- en: We don’t expect to hear anything about new gaming GPUs today, as this news is
    coming out of Nvidia’s GPU Technology Conference, which is usually almost entirely
    focused on GPU computing and AI, not gaming. But the Blackwell GPU architecture
    will [likely also power a future RTX 50-series lineup](https://videocardz.com/newz/nvidia-blackwell-gb203-gpu-to-feature-256-bit-bus-gb205-with-192-bit-claims-leaker)
    of desktop graphics cards.
  id: totrans-split-26
  prefs: []
  type: TYPE_NORMAL
  zh: 今天我们不指望能听到关于新游戏 GPU 的任何消息，因为这则消息来自 Nvidia 的 GPU 技术大会，通常几乎完全专注于 GPU 计算和人工智能，而不是游戏。但是
    Blackwell GPU 架构很可能也会驱动未来的 RTX 50 系列桌面图形卡。[（来源）](https://videocardz.com/newz/nvidia-blackwell-gb203-gpu-to-feature-256-bit-bus-gb205-with-192-bit-claims-leaker)
- en: '***Update, March 19th:** Added Nvidia CEO estimate that the new GPUs might
    cost up to $40K each.*'
  id: totrans-split-27
  prefs: []
  type: TYPE_NORMAL
  zh: '**更新，3 月 19 日：** Nvidia CEO 估计新 GPU 的成本可能高达每个 40K 美元。'
