- en: <!--yml
  id: totrans-split-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-split-1
  prefs: []
  type: TYPE_NORMAL
  zh: 类别：未分类
- en: 'date: 2024-05-27 14:37:18'
  id: totrans-split-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-05-27 14:37:18
- en: -->
  id: totrans-split-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: Stop Treating AI Models Like People
  id: totrans-split-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 不要将AI模型视为人类
- en: 来源：[https://garymarcus.substack.com/p/stop-treating-ai-models-like-people](https://garymarcus.substack.com/p/stop-treating-ai-models-like-people)
  id: totrans-split-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://garymarcus.substack.com/p/stop-treating-ai-models-like-people](https://garymarcus.substack.com/p/stop-treating-ai-models-like-people)
- en: '**By Sasha Luccioni and Gary Marcus**'
  id: totrans-split-6
  prefs: []
  type: TYPE_NORMAL
  zh: '**由Sasha Luccioni和Gary Marcus撰写**'
- en: The ELIZA effect is still with us, more than half a century later
  id: totrans-split-7
  prefs: []
  type: TYPE_NORMAL
  zh: 这种ELIZA效应至今仍然存在，已有半个多世纪了。
- en: 'For the last few months, people have had endless “conversations” with  chatbots
    like GPT-4 and Bard, asking these systems whether [climate change is real](https://www.foxnews.com/media/chatgtp-confession-global-warming-much-since-2016),
    [how to get people to fall in love with them,](https://www.techradar.com/opinion/i-asked-bing-about-love-the-results-broke-and-mended-my-heart)
    and even [their plans for AI-powered world domination](https://www.tomshardware.com/news/chatgpt-pi-furby-nightmare).
    This is apparently done by operating under the assumption that these system have
    genuine beliefs, and the capacity to teach themselves, as in this Tweet from the
    US Senator Chris Murphy:'
  id: totrans-split-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在过去几个月中，人们与像GPT-4和Bard这样的聊天机器人进行了无数次“对话”，询问这些系统关于[气候变化是否真实](https://www.foxnews.com/media/chatgtp-confession-global-warming-much-since-2016)，[如何让人们爱上他们](https://www.techradar.com/opinion/i-asked-bing-about-love-the-results-broke-and-mended-my-heart)，甚至是[他们对AI驱动的世界统治计划的看法](https://www.tomshardware.com/news/chatgpt-pi-furby-nightmare)。显然，这些行为基于这样一种假设，即这些系统有真实的信仰和自我教育的能力，就像美国参议员克里斯·墨菲在推特上所说的那样：
- en: In the language of cognitive psychology, all of this is “overattribution”, ascribing
    a kind of mental life to these machines that simply isn’t there, like when many
    years ago people thought that [Furbies were learning language](https://www.listenandlearn.org/blog/no-you-cant-teach-your-furby-to-swear-how-furbies-learn-language/),
    when in reality the unfolding of abilities  was pre-programmed. As [most experts
    realize](https://arxiv.org/pdf/2212.03551.pdf), the reality is that current AI
    doesn’t “decide to teach itself”, or even have consistent beliefs. One minute
    the string of words that it generates may tell you that it understands language.
  id: totrans-split-9
  prefs: []
  type: TYPE_NORMAL
  zh: 从认知心理学的角度来看，所有这些都是“过度归因”，将一种心理生活归因于这些机器，其实根本不存在，就像多年前人们以为[Furby在学习语言](https://www.listenandlearn.org/blog/no-you-cant-teach-your-furby-to-swear-how-furbies-learn-language/)时一样，事实上能力的展现是预先编程好的。正如[大多数专家所认识到的](https://arxiv.org/pdf/2212.03551.pdf)，现在的AI并不“决定自学”，甚至没有一致的信念。它生成的一串词语可能会告诉你它理解语言。
- en: And another it may say the opposite.
  id: totrans-split-10
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个可能会说反过来。
- en: There is [no there there](https://www.gradesaver.com/gertrude-stein-operas-and-plays/wikipedia/there-is-no-there-there),
    no homunculus inside the box, no inner agent with thoughts about the world, not
    even long-term memory. The AI systems that power these chatbots are simply systems
    (technically known as “language models” because they emulate (model) the statistical
    structure of language) that compute probabilities of word sequences, without any
    deep or human-like comprehension of what they say. Yet the urge to personify these
    systems is, for many people, irresistible, an extension of the same impulse that
    makes see a face on the Moon or [attributing agency and emotions](https://www.jstor.org/stable/1416950)
    to two triangles “chasing” each other around a screen. Everyone in the AI community
    is aware of this, and yet even experts are occasionally tempted to anthropomorphism,
    as deep learning pioneer Geoffrey Hinton’ recently [tweeted](https://twitter.com/geoffreyhinton/status/1636110447442112513)
    that “*Reinforcement Learning by Human Feedback is just parenting for a supernaturally
    precocious child.*”  Doing so can be cute, but also fundamentally misleading,
    and even dangerous.
  id: totrans-split-11
  prefs: []
  type: TYPE_NORMAL
  zh: '[没有那里](https://www.gradesaver.com/gertrude-stein-operas-and-plays/wikipedia/there-is-no-there-there)，盒子里没有小人，没有内在的代理人对世界有所思考，甚至没有长期记忆。这些驱动聊天机器人的AI系统仅仅是系统（在技术上被称为“语言模型”，因为它们模拟语言的统计结构），计算单词序列的概率，而没有深刻或类似人类的理解能力。然而，将这些系统拟人化的冲动对许多人来说是不可抗拒的，这是将面对月亮看见面孔或者在屏幕上看到两个三角形“追逐”归因给代理和情感的同一冲动的延伸。AI社区的每个人都意识到这一点，然而即使是专家有时也会受到拟人化的诱惑，如深度学习先驱杰弗里·辛顿最近在推特上发文称“*通过人类反馈进行强化学习就像是对一个超自然早熟孩子的育养*”。这样做可能很可爱，但也是根本性的误导，甚至是危险的。'
- en: The fact that people might over attribute intelligence to AI system has been
    known for a long time, at least back to [ELIZA](https://en.wikipedia.org/wiki/ELIZA),
    a computer program from the 1960s that was able to have faux-psychiatric conversations
    with humans by using a pattern matching approach, giving users the impression
    that the program truly understood them. What we are seeing now is simply an extension
    of the same “ELIZA effect”, 60 years later, where humans are continuing to project
    human qualities like emotions and understanding onto machines that lack them.
    With technology more and more able to emulate human responses based on larger
    and larger samples of text (and “reinforcement learning” from humans who instruct
    the machines), the problem has grown even more pernicious. In one instance, someone
    interacted with a bot as if it were somewhere between a lover and therapist and
    ultimately committed suicide; causality is hard to establish, but [the widow saw
    that interaction as having played an important role](https://garymarcus.substack.com/p/the-first-known-chatbot-associated);
    the risk of overattribution in a vulnerable patient is serious.
  id: totrans-split-12
  prefs: []
  type: TYPE_NORMAL
  zh: 人们可能会过度将智能归因于AI系统的事实，早在[ELIZA](https://en.wikipedia.org/wiki/ELIZA)（20世纪60年代的计算机程序，通过模式匹配方法与人类进行伪心理对话，让用户产生程序真正理解他们的印象）之时就已为人所知。现在我们所见到的只是同一“ELIZA效应”的延伸，60年后，人类继续将情感和理解等人类特质投射到这些缺乏这些特质的机器上。随着技术越来越能够根据更大更多样的文本样本模拟人类反应（以及从指导机器的人类中进行“强化学习”），这个问题变得更加棘手。在一个案例中，有人将一个机器人视为介于情人和治疗师之间，并最终自杀；因果关系难以建立，但[寡妇认为这种互动起了重要作用](https://garymarcus.substack.com/p/the-first-known-chatbot-associated)；对于一个易受伤害患者来说，过度归因的风险是严重的。
- en: As tempting as it is, we have to stop treating AI models like people. When we
    do so, we amplify the hype around AI, and lead people into thinking that these
    machines are trustworthy oracles capable of manipulation or decision-making, which
    they are not. As anyone who has used these systems to generate a biography is
    aware of, they are prone to simply making things up; treating them as intelligent
    agents means that people can develop unsound emotional relationships, treat unsound
    medical advice as more worthy than it is, and so forth. It’s also silly to ask
    these sorts of models for questions about themselves; as the mutually contradictory
    examples above make clear, they don’t actually “know”; they are just generating
    different word strings on different occasions, with no guarantee of anything.)
    The more false agency people ascribe to them, the more they can be exploited,
    suckered in by harmful applications like catfishing and fraud, as well as more
    subtly harmful applications like [chatbot-assisted therapy](https://www.fastcompany.com/90836906/ai-therapy-koko-chatgpt)
    or [flawed financial advice.](https://fortune.com/recommends/mortgages/i-used-chatgpt-as-my-financial-planner/)
    What we need is for the public to learn that human-sounding speech isn’t actually
    necessarily human anymore; caveat emptor. We also need new technical tools, like
    watermarks and generated content detectors, to help distinguish human- and machine-generated
    content, and policy measures to limit how and where AI models can be used.
  id: totrans-split-13
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管诱人，我们必须停止把AI模型当作人类对待。这样做会加大AI周围的炒作，并让人们误以为这些机器是可信赖的神谕，能够进行操纵或决策，但实际上并非如此。任何使用这些系统生成传记的人都会意识到，它们往往只是编造事实；把它们当作智能代理意味着人们可能会发展不稳健的情感关系，将不稳健的医疗建议看得比实际值得的多等等。向这些模型询问关于它们自身的问题也是愚蠢的；正如上述互相矛盾的例子清楚地表明的那样，它们实际上并不“知道”；它们只是在不同场合生成不同的词语串，没有任何保证。人们归因给它们的虚假代理越多，它们就越容易被利用，被有害应用如猫鱼网诈骗和欺诈，以及更微妙的有害应用如[聊天机器人辅助治疗](https://www.fastcompany.com/90836906/ai-therapy-koko-chatgpt)或[有缺陷的财务建议](https://fortune.com/recommends/mortgages/i-used-chatgpt-as-my-financial-planner/)。我们需要公众了解，听起来像人的语言并不一定是人类产生的；买方自负。我们还需要新的技术工具，如水印和生成内容检测器，来帮助区分人类生成和机器生成的内容，并采取政策措施限制AI模型的使用范围和方式。
- en: Educating people to overcome the overattribution bias will be a vital step;
    we can’t have senators and members of the AI community making the problem worse.
    It is crucial to retain a healthy skepticism towards these technologies, since
    they are very new, constantly evolving, and under-tested. Yes, they can generate
    cool haikus and well-written prose, but they also [constantly spew misinformation](https://www.theatlantic.com/technology/archive/2023/03/ai-chatbots-large-language-model-misinformation/673376/)
    (even about [themselves](https://twitter.com/katecrawford/status/1638524013432516610)),
    and cannot be trusted when it comes to answering questions about real-world events
    and phenomena, let alone to provide sound advice about mental health or marriage
    counseling.
  id: totrans-split-14
  prefs: []
  type: TYPE_NORMAL
  zh: 教育人们克服过度归因偏见将是一个关键步骤；我们不能让参议员和人工智能社区的成员加剧问题。对这些技术保持健康的怀疑态度至关重要，因为它们非常新颖，不断发展，并且经受了少量测试。是的，它们可以生成酷炫的俳句和写得很好的散文，但它们也会[持续地喷出错误信息](https://www.theatlantic.com/technology/archive/2023/03/ai-chatbots-large-language-model-misinformation/673376/)（甚至是[关于自己的](https://twitter.com/katecrawford/status/1638524013432516610)），在回答关于现实世界事件和现象的问题时，甚至提供心理健康或婚姻咨询的合理建议时都不可信。
- en: Treat them as fun toys, if you like, but don’t treat them as friends.
  id: totrans-split-15
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你愿意，把它们当作有趣的玩具，但不要把它们当作朋友。
- en: '**[Dr. Sasha Luccioni](https://www.sashaluccioni.com/)** is a Researcher and
    Climate Lead at Hugging Face, where she studies the ethical and societal impacts
    of AI models and datasets. She is also a Director of Women in Machine Learning
    (WiML), founding member of Climate Change AI (CCAI), and Chair of the NeurIPS
    Code of Ethics committee.'
  id: totrans-split-16
  prefs: []
  type: TYPE_NORMAL
  zh: '**[萨沙·卢奇奥尼博士](https://www.sashaluccioni.com/)** 是Hugging Face的研究员和气候负责人，她在这里研究人工智能模型和数据集的伦理和社会影响。她还是机器学习中的女性（WiML）的主任、气候变化人工智能（CCAI）的创始成员和NeurIPS伦理委员会的主席。'
- en: '***[Gary Marcus](http://garymarcus.com)** (@garymarcus),scientist, bestselling
    author, and entrepreneur, is deeply, deeply concerned about current AI but really
    hoping that we might do better.*'
  id: totrans-split-17
  prefs: []
  type: TYPE_NORMAL
  zh: '***[加里·马库斯](http://garymarcus.com)** (@garymarcus)，科学家，畅销书作家和企业家，对当前的人工智能非常关注，但真的希望我们可以做得更好。*'
- en: '*Watch for his new podcast, [Humans versus Machines](https://podcasts.apple.com/us/podcast/humans-vs-machines-with-gary-marcus/id1532110146?i=1000602693237),
    debuting April 25th, wherever you get your podcasts.*'
  id: totrans-split-18
  prefs: []
  type: TYPE_NORMAL
  zh: '*请关注他的新播客，[人类对抗机器](https://podcasts.apple.com/us/podcast/humans-vs-machines-with-gary-marcus/id1532110146?i=1000602693237)，将于4月25日首播，可在您获取播客的任何地方收听。*'
- en: '[Share](https://garymarcus.substack.com/p/stop-treating-ai-models-like-people?utm_source=substack&utm_medium=email&utm_content=share&action=share)'
  id: totrans-split-19
  prefs: []
  type: TYPE_NORMAL
  zh: '[分享](https://garymarcus.substack.com/p/stop-treating-ai-models-like-people?utm_source=substack&utm_medium=email&utm_content=share&action=share)'
