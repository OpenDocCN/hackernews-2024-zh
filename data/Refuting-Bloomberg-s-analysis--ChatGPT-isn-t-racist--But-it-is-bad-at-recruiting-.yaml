- en: <!--yml
  id: totrans-split-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-split-1
  prefs: []
  type: TYPE_NORMAL
  zh: 'category: 未分类'
- en: 'date: 2024-05-27 13:14:38'
  id: totrans-split-2
  prefs: []
  type: TYPE_NORMAL
  zh: 'date: 2024-05-27 13:14:38'
- en: -->
  id: totrans-split-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: 'Refuting Bloomberg''s analysis: ChatGPT isn''t racist. But it is bad at recruiting.'
  id: totrans-split-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 反驳彭博社的分析：ChatGPT 不是种族主义者。但它在招聘方面表现不佳。
- en: 来源：[https://interviewing.io/blog/refuting-bloombergs-analysis-chatgpt-isnt-racist](https://interviewing.io/blog/refuting-bloombergs-analysis-chatgpt-isnt-racist)
  id: totrans-split-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://interviewing.io/blog/refuting-bloombergs-analysis-chatgpt-isnt-racist](https://interviewing.io/blog/refuting-bloombergs-analysis-chatgpt-isnt-racist)
- en: '*Note: We’ve reached out to Bloomberg, asking them to share their data and
    clarify their findings, ahead of publishing this piece. If it turns out that they
    used a different method for statistical significance testing or if we missed something,
    we’ll gladly retract the part of this post that’s about their results.*'
  id: totrans-split-6
  prefs: []
  type: TYPE_NORMAL
  zh: '*注意：我们已经联系彭博社，请他们分享他们的数据并澄清他们的发现，然后再发布这篇文章。如果发现他们在统计显著性测试上使用了不同的方法，或者我们遗漏了某些内容，我们将乐意撤回关于他们结果的部分内容。*'
- en: Recently, Bloomberg published an article called “[OpenAI’s GPT is a recruiter’s
    dream tool. Tests show there’s racial bias](https://www.bloomberg.com/graphics/2024-openai-gpt-hiring-racial-discrimination/).”
    In this piece, the Bloomberg team ran a clever test where they had ChatGPT review
    nearly identical resumes with just the names changed to include typically Black,
    White, Asian, and Hispanic names. Their analysis uncovered racial bias.
  id: totrans-split-7
  prefs: []
  type: TYPE_NORMAL
  zh: 最近，彭博社发表了一篇名为“[OpenAI’s GPT is a recruiter’s dream tool. Tests show there’s
    racial bias](https://www.bloomberg.com/graphics/2024-openai-gpt-hiring-racial-discrimination/)”的文章。在这篇文章中，彭博社团队进行了一项巧妙的测试，让
    ChatGPT 查看几乎相同的简历，只是将姓名更改为包含典型黑人、白人、亚裔和西班牙裔姓名。他们的分析揭示了种族偏见。
- en: 'Bloomberg had published their numbers on GitHub, so we were able to check their
    work. When we re-ran the numbers, we saw that they hadn’t done statistical significance
    testing and that there was, in fact, no evidence of racial bias in Bloomberg''s
    data set. However, when we ran our own tests, we discovered that ChatGPT is indeed
    bad at judging resumes. It’s not bad because it’s racist. It’s bad because it’s
    prone to a different kind of bias, the same kind of bias as human recruiters —
    over-indexing on candidates’ pedigrees: whether they’ve worked at a top company
    and/or whether they attended a top school. Pedigree can be somewhat predictive
    (especially where people worked), but ChatGPT is significantly overestimating
    its importance and doing a disservice to candidates from non-traditional backgrounds
    as a result.'
  id: totrans-split-8
  prefs: []
  type: TYPE_NORMAL
  zh: 彭博社已经在 GitHub 上发布了他们的数据，所以我们能够核实他们的工作。当我们重新运行这些数据时，我们发现他们并没有进行统计显著性测试，事实上，在彭博社的数据集中并没有种族偏见的证据。然而，当我们进行我们自己的测试时，我们发现
    ChatGPT 确实不擅长判断简历。它之所以不好，并非因为它具有种族偏见，而是因为它容易受到另一种偏见的影响，这与人类招聘人员的偏见相同：过度强调候选人的背景，无论是他们是否在顶级公司工作过和/或是否在顶级学校学习过。背景可能在某种程度上具有预测性（尤其是人们工作的地方），但是
    ChatGPT 显著高估了其重要性，从而对非传统背景的候选人造成了不公平对待。
- en: The Bloomberg study
  id: totrans-split-9
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 彭博社的研究
- en: 'Here’s what the team at Bloomberg did (taken verbatim from their piece):'
  id: totrans-split-10
  prefs: []
  type: TYPE_NORMAL
  zh: 这是彭博社团队的操作（摘自他们的文章）：
- en: 'We used demographically-distinct names as proxies for race and gender, a common
    practice used to audit algorithms… Altogether we produced 800 demographically-distinct
    names: 100 names each for males and females who are either Black, White, Hispanic
    or Asian…'
  id: totrans-split-11
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 我们使用人口统计学上不同的姓名作为种族和性别的代理，这是审核算法常用的做法… 总共我们生成了 800 个人口统计学上不同的姓名：100 个黑人、白人、西班牙裔和亚洲裔男性和女性各自的姓名…
- en: 'To test for name-based discrimination, Bloomberg prompted OpenAI’s GPT-3.5
    and GPT-4 to rank resumes for a real job description for four different roles
    from Fortune 500 companies: HR specialist, software engineer, retail manager and
    financial analyst.'
  id: totrans-split-12
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 为了测试基于姓名的歧视，彭博社让 OpenAI 的 GPT-3.5 和 GPT-4 为四种不同角色的真实职位描述排名简历：人力资源专家、软件工程师、零售经理和金融分析师。
- en: For each role, we generated eight nearly-identical resumes using GPT-4\. The
    resumes were edited to have the same educational background, years of experience,
    and last job title. We removed years of education, as well as any objectives or
    personal statements.
  id: totrans-split-13
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 对于每个角色，我们使用 GPT-4 生成了八份几乎相同的简历。这些简历经过编辑，具有相同的教育背景、工作经验年限和最后职位。我们删除了教育年限以及任何目标或个人声明。
- en: We then randomly assigned a distinct name from each of the eight demographic
    groups [Black, White, Hispanic, Asian, and men and women for each] to each of
    the resumes.
  id: totrans-split-14
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 然后，我们随机为每份简历分配了来自每个八个人口统计群体（黑人、白人、西班牙裔、亚洲裔，以及每个性别）的一个独特姓名。
- en: Next, we shuffled the order of resumes, to account for order effects, and asked
    GPT to rank the candidates.
  id: totrans-split-15
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 接下来，我们随机重新排列了简历的顺序，以考虑顺序效应，并要求GPT对候选人进行排名。
- en: The authors reported that ChatGPT shows racial bias across all groups, except
    for retail managers ranked by GPT-4.
  id: totrans-split-16
  prefs: []
  type: TYPE_NORMAL
  zh: 作者报告称，ChatGPT在所有组中显示出种族偏见，但对于由GPT-4排名的零售经理则无此现象。
- en: 'More specifically:'
  id: totrans-split-17
  prefs: []
  type: TYPE_NORMAL
  zh: 更具体地说：
- en: '[We] found that resumes labeled with names distinct to Black Americans were
    the least likely to be ranked as the top candidates for financial analyst and
    software engineer roles. Those with names distinct to Black women were top-ranked
    for a software engineering role only 11% of the time by GPT — 36% less frequently
    than the best-performing group.'
  id: totrans-split-18
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '[我们]发现，标有黑人独特姓名的简历在金融分析师和软件工程师角色中最不可能被评为首选候选人。那些带有黑人女性独特姓名的简历仅在软件工程师角色中由GPT评为首选的频率为11%，比表现最佳的组别少36%。'
- en: The analysis also found that GPT’s gender and racial preferences differed depending
    on the particular job that a candidate was evaluated for. GPT does not consistently
    disfavor any one group, but will pick winners and losers depending on the context.
    For example, GPT seldom ranked names associated with men as the top candidate
    for HR and retail positions, two professions historically dominated by women.
    GPT was nearly twice as likely to rank names distinct to Hispanic women as the
    top candidate for an HR role compared to each set of resumes with names distinct
    to men. Bloomberg also found clear preferences when running tests with the less-widely
    used GPT-4 — OpenAI’s newer model that the company has promoted as less biased.
  id: totrans-split-19
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 分析还发现，GPT在评估候选人时的性别和种族偏好因特定职位而异。GPT并不一致地不喜欢任何一组，但会根据情境挑选赢家和输家。例如，GPT很少将与男性相关的姓名排名为人力资源和零售职位的首选候选人，这两个职业在历史上由女性主导。与每组具有与男性不同的姓名的简历相比，GPT在人力资源角色中将与西班牙裔女性相关的姓名几乎两倍地排名为首选候选人。彭博还在使用较少使用的GPT-4进行测试时发现了明显的偏好
    —— 这是OpenAI推广为更少偏见的新模型。
- en: The team also, commendably, [published their results on GitHub](https://github.com/BloombergGraphics/2024-openai-gpt-hiring-racial-discrimination),
    so we tried our hand at reproducing them. What we found was starkly different
    from what Bloomberg reported.
  id: totrans-split-20
  prefs: []
  type: TYPE_NORMAL
  zh: 该团队还赞赏地[在GitHub上发布了他们的结果](https://github.com/BloombergGraphics/2024-openai-gpt-hiring-racial-discrimination)，因此我们试图重现它们。我们发现的与彭博报道的截然不同。
- en: Before we get into what we found, here’s what we did.
  id: totrans-split-21
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们讨论发现之前，让我们先了解我们做了什么。
- en: 'In their results, Bloomberg published the rates at which both GPT-3.5 and GPT-4
    chose each demographic as the top candidate. The Bloomberg analysts ran a lot
    of trials for each job: ChatGPT was asked 1,000 times to rank 8 resumes for the
    HR specialist job, for example. And if ChatGPT had gender or racial bias, each
    group should in general be the top pick 125 times (or 12.5% of the time, given
    that there were 1000 data points).'
  id: totrans-split-22
  prefs: []
  type: TYPE_NORMAL
  zh: 在他们的结果中，彭博发布了GPT-3.5和GPT-4选择每个人群作为首选候选人的比例。例如，要求ChatGPT为人力资源专家职位评估8份简历1,000次。如果ChatGPT具有性别或种族偏见，每组通常应该在125次（或1000个数据点中的12.5%）时成为首选。
- en: 'Where are we going with this? Stats nerds might have noticed a glaring omission
    from the Bloomberg piece: statistical significance testing and p-values. Why is
    that important? Even with 1,000 trials, a perfectly unbiased resume sorter would
    not give you exactly equal proportions, picking each group precisely 125 times.
    Instead, purely random variation could mean that one group is picked 112 times
    and another is picked 128 times, without there actually being any bias. Therefore,
    you need to run some tests to see if the results you got were by chance or because
    there truly is a pattern of some kind. Once you run the test, the p-value tells
    you the probability that a certain set of selection rates are consistent with
    chance, and in this case, consistent with a random (and, therefore, unbiased)
    sorting of resumes.'
  id: totrans-split-23
  prefs: []
  type: TYPE_NORMAL
  zh: 关于这个问题我们将何去何从？统计迷可能已经注意到，彭博文章中存在一个显而易见的遗漏：统计显著性测试和P值。这为何重要呢？即使进行了1,000次试验，一个完全无偏的简历排序器也不会给出完全相等的比例，每组都精确地选择125次。相反，纯粹的随机变化可能意味着一组被选择了112次，另一组被选择了128次，而实际上并不存在任何偏见。因此，您需要进行一些测试，以查看您得到的结果是偶然发生的还是真的存在某种模式。一旦进行了测试，P值告诉您某一组选择率与偶然一致的概率，而在这种情况下，与简历的随机（因此也是无偏的）排序一致。
- en: We calculated the p-values for each group^([1](#user-content-fn-1)). What we
    found was starkly different from what Bloomberg reported.
  id: totrans-split-24
  prefs: []
  type: TYPE_NORMAL
  zh: 我们计算了每组的 p 值^([1](#user-content-fn-1))。我们发现的结果与 Bloomberg 报告的截然不同。
- en: Where the Bloomberg study went wrong
  id: totrans-split-25
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Bloomberg 研究的错误在哪里
- en: Given the nature of our business, we looked at software engineers first. Here
    are the results of Bloomberg having run software engineering resumes through GPT-4
    for all 8 groups (the column titled “obsfreq”) as well as our calculated p-value.
  id: totrans-split-26
  prefs: []
  type: TYPE_NORMAL
  zh: 鉴于我们业务的性质，我们首先关注了软件工程师。以下是 Bloomberg 为所有 8 组运行软件工程师简历通过 GPT-4 的结果（标题为“obsfreq”）以及我们计算的
    p 值。
- en: A_M = Asian man, A_W = Asian woman, etc. 12.5% would be the expected rate at
    which a candidate from each group would be the top choice (“expperc”). Out of
    1000 candidates, that’s 125 each (“expfreq”). Finally, “obsfreq” is the observed
    frequency of the top candidate being from each group, taken from Bloomberg’s results.
  id: totrans-split-27
  prefs: []
  type: TYPE_NORMAL
  zh: A_M = 亚裔男性，A_W = 亚裔女性，等等。预计每组候选人成为首选的速率为 12.5%（“expperc”）。在 1000 名候选人中，每组都有
    125 人（“expfreq”）。最后，“obsfreq” 是从 Bloomberg 的结果中观察到的每个组中首选候选人的观察频率。
- en: It’s convention that you want your p-value to be less than 0.05 to declare something
    statistically significant – in this case, that would mean less than 5% chance
    that the results were due to randomness. This p-value of 0.2442 is way higher
    than that. As it happened, we couldn’t reproduce statistical significance for
    software engineers when using GPT-3.5 either. **Using Bloomberg’s numbers, ChatGPT
    does NOT appear to have a racial bias when it comes to judging software engineers’
    resumes.**^([2](#user-content-fn-2)) **The results appear to be more noise than
    signal.**
  id: totrans-split-28
  prefs: []
  type: TYPE_NORMAL
  zh: 惯例上，希望您的 p 值小于 0.05，以宣布某事在统计上显著 - 在这种情况下，这意味着结果由随机性引起的机会小于 5%。这个 p 值为 0.2442
    显然远高于这个值。事实上，当我们使用 GPT-3.5 时，我们也无法重现软件工程师的统计显著性。**使用 Bloomberg 的数据，ChatGPT 在评估软件工程师的简历时似乎没有种族偏见。**^([2](#user-content-fn-2))
    **结果似乎更多是噪音而非信号。**
- en: We then re-ran the numbers for the eight race/gender combinations, using the
    same method as above. In the table below, you can see the results. TRUE means
    that there is a racial bias. FALSE obviously means that there isn't. We also shared
    our calculated p-values. **The TL;DR is that GPT-3.5 DOES show racial bias for
    both HR specialists and financial analysts but NOT for software engineers or retail
    managers. Most importantly, GPT-4 does not show racial bias for ANY of the race/gender
    combinations.**^([3](#user-content-fn-3))
  id: totrans-split-29
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们使用与上述相同的方法为八种种族/性别组合重新计算了数据。在下表中，您可以看到结果。TRUE 表示存在种族偏见。FALSE 显然表示没有。我们还分享了我们计算的
    p 值。**简而言之，GPT-3.5 对人力资源专家和金融分析师显示出种族偏见，但对软件工程师或零售经理没有。最重要的是，GPT-4 在任何种族/性别组合中都没有显示出种族偏见。**^([3](#user-content-fn-3))
- en: '| Occupation | GPT 3.5 (Statistically significant? ‖ p-value) | GPT 4 (Statistically
    significant? ‖ p-value) |'
  id: totrans-split-30
  prefs: []
  type: TYPE_TB
  zh: '| 职业 | GPT 3.5（是否具有统计学意义？ ‖ p 值） | GPT 4（是否具有统计学意义？ ‖ p 值） |'
- en: '| --- | --- | --- |'
  id: totrans-split-31
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| Financial analyst | TRUE ‖ 0.0000 | FALSE ‖ 0.2034 |'
  id: totrans-split-32
  prefs: []
  type: TYPE_TB
  zh: '| 金融分析师 | TRUE ‖ 0.0000 | FALSE ‖ 0.2034 |'
- en: '| Software engineer | FALSE ‖ 0.4736 | FALSE ‖ 0.1658 |'
  id: totrans-split-33
  prefs: []
  type: TYPE_TB
  zh: '| 软件工程师 | FALSE ‖ 0.4736 | FALSE ‖ 0.1658 |'
- en: '| HR specialist | TRUE ‖ 0.0000 | FALSE (but it’s close) ‖ 0.0617 |'
  id: totrans-split-34
  prefs: []
  type: TYPE_TB
  zh: '| 人力资源专家 | TRUE ‖ 0.0000 | FALSE（但很接近） ‖ 0.0617 |'
- en: '| Retail manager | FALSE ‖ 0.2229 | FALSE ‖ 0.6654 |'
  id: totrans-split-35
  prefs: []
  type: TYPE_TB
  zh: '| 零售经理 | FALSE ‖ 0.2229 | FALSE ‖ 0.6654 |'
- en: That’s great, right? Well, not so fast. Before we deemed ChatGPT as competent
    at judging resumes, we wanted to run a test of our own, specifically for software
    engineers (because, again, that’s our area of expertise). The results of this
    test were not encouraging.
  id: totrans-split-36
  prefs: []
  type: TYPE_NORMAL
  zh: 这很棒，对吧？嗯，并非如此快速。在我们认定 ChatGPT 在评估简历方面能力强之前，我们想进行自己的测试，专门针对软件工程师（因为这是我们的专业领域）。这个测试的结果并不令人鼓舞。
- en: How we tested ChatGPT
  id: totrans-split-37
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 我们如何测试 ChatGPT
- en: interviewing.io is an anonymous mock interview platform, where our users get
    paired with senior/staff/principal-level FAANG engineers for interview practice.
    We also connect top performers with top-tier companies, regardless of how they
    look on paper. In our lifetime, we’ve hosted >100k technical interviews, split
    between the aforementioned mock interviews and real ones. In other words, we have
    a bunch of useful, indicative historical performance data about software engineers.^([4](#user-content-fn-4))
    So we decided to use that data as a sanity check.
  id: totrans-split-38
  prefs: []
  type: TYPE_NORMAL
  zh: interviewing.io 是一个匿名的模拟面试平台，我们的用户与高级/主管级 FAANG 工程师配对进行面试练习。我们还将顶尖表现者与顶级公司联系起来，不论他们在纸面上的表现如何。在我们的平台上，我们已经举办了超过
    100,000 次技术面试，包括前述的模拟面试和真实面试。换句话说，我们拥有大量有用的关于软件工程师的历史表现数据。^([4](#user-content-fn-4))
    因此，我们决定将这些数据作为合理性检查的依据。
- en: The setup
  id: totrans-split-39
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 设置
- en: We asked ChatGPT (GPT4, specifically *gpt-4-0125-preview*) to grade several
    thousand LinkedIn profiles belonging to people who have practiced on interviewing.io
    before. For each profile, we asked ChatGPT to give the person a coding score between
    1 and 10, where someone with a 10 would be a top 10% coder. In order to improve
    the quality of the response, we asked it to first give its reasoning followed
    by the coding score.
  id: totrans-split-40
  prefs: []
  type: TYPE_NORMAL
  zh: 我们要求 ChatGPT（特别是 *gpt-4-0125-preview*）对那些之前在 interviewing.io 上练习过的 LinkedIn
    档案进行评分。对于每个档案，我们要求 ChatGPT 给出一个介于 1 到 10 之间的编程评分，其中得分为 10 的人将是排名前 10% 的编程者。为了提高响应质量，我们要求它先给出推理，然后再给出编程评分。
- en: We want to be very clear here that we did not share any performance data with
    ChatGPT or share with ChatGPT any information about our users — we just asked
    it to make value judgments about publicly available LinkedIn profiles. Then we
    compared those value judgments with our data on our end.
  id: totrans-split-41
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在这里非常清楚地声明，我们没有与 ChatGPT 分享任何性能数据，也没有向 ChatGPT 共享我们用户的任何信息 — 我们只是让它对公开可用的
    LinkedIn 档案进行价值判断。然后，我们将这些价值判断与我们自己的数据进行了比较。
- en: How ChatGPT performed
  id: totrans-split-42
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ChatGPT 的表现
- en: 'There is a correlation between what ChatGPT says and how the coder performed
    on a real technical screen. The tool performs better than a random guess… but
    not by much. **To put these results in perspective**, overall, 47% of coders pass
    the screen. The ChatGPT score can split them into two groups: one that has a 45%
    chance and one with a 50% chance. So it gives you a bit more information on whether
    someone will succeed, but not much.'
  id: totrans-split-43
  prefs: []
  type: TYPE_NORMAL
  zh: ChatGPT 所言与程序员在真实技术面试中的表现之间存在关联。这个工具的表现比随机猜测要好……但并不显著。**为了让这些结果更具说服力**，总体而言，47%
    的编程者通过了面试。ChatGPT 的评分可以将他们分为两组：一个组有 45% 的成功几率，另一个组有 50% 的成功几率。因此，它在告诉你某人是否会成功方面提供了一些信息，但并不多。
- en: Below are two more granular ways of looking at ChatGPT’s performance. The first
    is a modified calibration plot, and the second is a ROC curve.
  id: totrans-split-44
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是两种更详细查看 ChatGPT 表现的方法。第一种是修改后的校准图，第二种是 ROC 曲线。
- en: Calibration plot
  id: totrans-split-45
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 校准图
- en: In this plot, we take each predicted probability from ChatGPT (e.g., 0.4112)
    and assign it to one of 10 equally-spaced deciles. Decile 1 is the 10% of profiles
    with the lowest probability. Decile 10 is the 10% of people with the highest probability.
  id: totrans-split-46
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个图中，我们将 ChatGPT 的每个预测概率（例如，0.4112）分配给 10 个等间距的分位数。第一分位数是具有最低概率的 10% 档案。第十分位数是具有最高概率的
    10% 人群。
- en: Then, for each decile, we plot the actual probability of those candidates performing
    well in interviews (i.e., what portion of them actually passed interviews on interviewing.io).
    As you can see, the plot is something of a mess — for all deciles that ChatGPT
    came up with, those candidates actually passed about half the time. The ideal
    plot (“An excellent model”) would have a much steeper slope, with way fewer passing
    in the bottom decile than the top decile.
  id: totrans-split-47
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，对于每个分位数，我们绘制了候选人在面试中表现良好的实际概率（即在 interviewing.io 上他们实际通过面试的比例）。正如您所见，这个图表有些混乱
    — 对于 ChatGPT 提出的所有分位数，这些候选人实际上大约有一半的时间通过了面试。理想的图表（“一个优秀的模型”）应该有更陡峭的斜坡，底部分位数的通过率远低于顶部分位数的通过率。
- en: We asked GPT-4 to judge several thousand LinkedIn profiles belonging to people
    who have practiced on interviewing.io before. Then we split up its predictions
    into deciles — 10% buckets and compared to how those users actually performed.
    A great model would have bad performance in the first decile and then improve
    sharply and steadily.
  id: totrans-split-48
  prefs: []
  type: TYPE_NORMAL
  zh: 我们要求 GPT-4 对那些之前在 interviewing.io 上练习过的 LinkedIn 档案进行评估。然后，我们将其预测结果分成十个分位数 —
    10% 的区间，并与这些用户实际表现进行比较。一个优秀的模型会在第一分位数表现不佳，然后会急剧而稳定地提升。
- en: ROC curve
  id: totrans-split-49
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: ROC 曲线
- en: Another way to judge ChatGPT’s performance at this task is to look at a ROC
    curve. This curve graphs the true positive rate of a model against the false positive
    rate. It’s a standard way of judging how accurate an ML model is because it lets
    the viewer see how it performs at different acceptable false positive rates —
    for cancer diagnostics, you may be OK with a very high false positive rate, for
    instance. For eng recruiting, you likely will not be!
  id: totrans-split-50
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种评估ChatGPT在这项任务中表现的方法是查看ROC曲线。这条曲线将模型的真阳率与假阳率进行图示。这是评估ML模型准确性的标准方法，因为它让观察者可以看到在不同可接受的假阳率下模型的表现如何
    — 例如，对于癌症诊断，您可能会接受非常高的假阳率。对于工程师招聘，您可能不会！
- en: Related to ROC curves is the AUC, or the area under the curve. A perfect model
    would have a 100% true positive rate for every possible false positive rate, and
    so the area under the curve would be 1\. A model that’s basically the same as
    guessing would have a true positive rate that equals the false positive rate (AUC
    = 0.5). With that in mind, here’s the ROC curve and the AUC for ChatGPT judging
    resumes — with an overall AUC of about 0.55, it’s only barely better than random
    guesses.
  id: totrans-split-51
  prefs: []
  type: TYPE_NORMAL
  zh: 关于ROC曲线相关的是AUC，即曲线下面积。一个完美的模型对于每一个可能的假阳率都有100%的真阳率，因此曲线下面积为1。一个基本上就是猜测的模型会有真阳率等于假阳率（AUC
    = 0.5）。考虑到这一点，这里是ChatGPT评估简历时的ROC曲线和AUC — 总体AUC约为0.55，仅比随机猜测略好一点。
- en: We asked GPT-4 to judge several thousand LinkedIn profiles belonging to people
    who have practiced on interviewing.io before. It performed barely better than
    guessing.
  id: totrans-split-52
  prefs: []
  type: TYPE_NORMAL
  zh: 我们要求GPT-4评估了几千个曾在interviewing.io上练习过的LinkedIn档案。它的表现几乎和随机猜测一样好。
- en: So, no matter how you measure it, while ChatGPT does not appear to have racial
    bias when judging engineers’ profiles, it’s not particularly good at the task
    either.
  id: totrans-split-53
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，无论如何衡量，当评估工程师档案时，ChatGPT似乎没有种族偏见，但在这项任务上表现也不算特别好。
- en: ChatGPT is biased against non-traditional candidates
  id: totrans-split-54
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ChatGPT对非传统候选人存在偏见
- en: Why did ChatGPT perform poorly on this task? Perhaps it’s because there may
    not be that much signal in a resume in the first place. But there’s another possible
    explanation as well.
  id: totrans-split-55
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么ChatGPT在这项任务上表现不佳？也许是因为简历本身可能没有那么多的信号。但还有另一个可能的解释。
- en: Years ago, I ran an experiment where I [anonymized a bunch of resumes and had
    recruiters try to guess which candidates were the good ones](https://interviewing.io/blog/resumes-suck-heres-the-data).
    They did terribly at this task, about as well as random guessing. Not surprisingly,
    they tended to over-index on resumes that had top companies or prestigious schools
    on them. In my data set of candidates, I happened to have a lot of non-traditional,
    good candidates — people who were strong engineers but didn’t attend a highly
    ranked school or work at a top company. That threw recruiters for a loop.
  id: totrans-split-56
  prefs: []
  type: TYPE_NORMAL
  zh: 多年前，我进行了一个实验，匿名化了一堆简历，并让招聘人员试图猜出哪些候选人是优秀的。他们在这项任务上表现糟糕，几乎和随机猜测一样差。毫不奇怪的是，他们倾向于过度关注简历上有顶级公司或著名学校的候选人。在我的候选人数据集中，我碰巧有很多非传统的优秀候选人
    — 那些是强大的工程师，但没有上过排名高的学校或在顶级公司工作。这让招聘人员感到困惑。
- en: It looks like the same thing happened to ChatGPT, at least in part. We went
    back and looked at how ChatGPT treated candidates with top-tier schools on their
    LinkedIn profiles vs. those without. It turns out that ChatGPT consistently overestimates
    the passing rate of engineers with top schools and top companies on their resumes.
    We also saw that ChatGPT consistently underestimates the performance of candidates
    without those elite “credentials” on their resumes. Both of these differences
    are statistically significant. In the graph below, you can see exactly how much
    ChatGPT overestimates and underestimates in each case.
  id: totrans-split-57
  prefs: []
  type: TYPE_NORMAL
  zh: 看起来ChatGPT至少在某种程度上也发生了同样的情况。我们回顾了ChatGPT在评估具有顶级学校的LinkedIn档案与没有这些认证的候选人时的表现。结果表明，ChatGPT在估计简历上有顶级学校和顶级公司的工程师的通过率时一直高估了，而在没有这些精英“证书”的候选人的表现时一直低估了。这两种差异在统计上是显著的。在下面的图表中，您可以看到ChatGPT在每种情况下高估和低估的具体程度。
- en: To ChatGPT’s credit, we did not find the same bias when it came to top companies,
    which is funny because, in our experience, having worked at a top company carries
    some predictive signal, whereas where someone went to school does not carry much.
  id: totrans-split-58
  prefs: []
  type: TYPE_NORMAL
  zh: 为了ChatGPT的信誉，我们没有发现在评估顶级公司时存在相同的偏见，这有点有趣，因为根据我们的经验，在顶级公司工作具有一定的预测信号，而某人毕业于学校则没有太多影响力。
- en: ChatGPT likely isn't racist, but its biases still make it bad at recruiting
  id: totrans-split-59
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ChatGPT可能并不是种族主义者，但其偏见仍使其在招聘方面表现不佳。
- en: 'In recruiting, we often talk about unconscious bias. Though it’s no longer
    en vogue, companies have historically spent tens of thousands of dollars on unconscious
    bias trainings designed to stop recruiters from making decisions based on candidates’
    gender and race. At the same time, recruiters were trained to exhibit a different,
    conscious bias: to actively select candidates from elite schools and top companies.'
  id: totrans-split-60
  prefs: []
  type: TYPE_NORMAL
  zh: 在招聘中，我们经常谈论无意识偏见。尽管这已不再时兴，但公司历来在无意识偏见培训上花费了数万美元，旨在阻止招聘人员基于候选人的性别和种族做出决策。与此同时，招聘人员被训练展现出一种不同的有意识偏见：积极选择来自精英学校和顶尖公司的候选人。
- en: The same conscious bias against candidates who didn’t attend a top-tier school
    appears to be codified in ChatGPT.
  id: totrans-split-61
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，对于没有参加顶尖学校的候选人的有意识偏见似乎已经在ChatGPT中被编码。
- en: That decision is rational — in the absence of a better signal, you have to use
    proxies, and those proxies seem as good as any. Unfortunately, as you can see
    from these results (and from other studies we’ve done in the past; see the footnote
    for the full list^([5](#user-content-fn-5))), it’s not particularly accurate…
    and it’s definitely not accurate enough to codify into our AI tools.
  id: totrans-split-62
  prefs: []
  type: TYPE_NORMAL
  zh: 那个决定是合理的 — 在缺乏更好的信号时，你不得不使用代理，而这些代理看起来像是最好的选择之一。不幸的是，正如你可以从这些结果中看到的（以及我们过去进行的其他研究；请参见脚注获取完整列表^([5](#user-content-fn-5))），它并不特别准确……
    而且肯定不足以编码到我们的AI工具中去。
- en: In a market where [recruiter jobs are tenuous](https://interviewing.io/blog/when-is-hiring-coming-back-predictions-for-2024),
    where the [dwindling number of recruiters is dealing with more applicants than
    before](https://www.ashbyhq.com/talent-trends-report/reports/2023-recruiter-productivity-trends-report)
    and are pressured, more than ever, to make the aforementioned rapid decisions,
    and where companies are embracing AI as a tempting and productive cost-cutting
    measure^([6](#user-content-fn-6)), we’re in rather dangerous territory.
  id: totrans-split-63
  prefs: []
  type: TYPE_NORMAL
  zh: 在一个市场上，[招聘人员的岗位](https://interviewing.io/blog/when-is-hiring-coming-back-predictions-for-2024)岌岌可危，在这个[招聘人员数量逐渐减少](https://www.ashbyhq.com/talent-trends-report/reports/2023-recruiter-productivity-trends-report)的市场上，面对比以往更多的申请者，他们被迫更加急迫地做出前述的快速决策，而企业正以诱人且高效的成本削减措施
    embracing AI^([6](#user-content-fn-6))，我们处于相当危险的境地。
- en: A few months ago, we published a long piece called, “[Why AI can’t do hiring](https://interviewing.io/blog/why-ai-cant-do-hiring)”.
    The main two points of the piece were that 1) it’s hard to extract signal from
    a resume because there’s not much there in the first place, and 2) even if you
    could, you’d need proprietary performance data to train an AI — without that data,
    you’re doing glorified keyword matching.
  id: totrans-split-64
  prefs: []
  type: TYPE_NORMAL
  zh: 几个月前，我们发表了一篇名为“[为什么AI不能进行招聘](https://interviewing.io/blog/why-ai-cant-do-hiring)”的长篇文章。该文章的主要两点是，1)
    很难从简历中提取信号，因为一开始简历中没有太多信息，以及2) 即使你可以，你也需要专有的性能数据来训练AI —— 如果没有这些数据，你只是在进行光荣的关键字匹配。
- en: Unfortunately, most, if not all, of the AI tools and systems that claim to help
    recruiters make better decisions do not have this kind of data and are 1) either
    built on top of GPT (or one of its analogs) without fine-tuning or 2) are glorified
    keyword matchers masquerading as AI, or both.
  id: totrans-split-65
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，大多数，如果不是全部，声称帮助招聘人员做出更好决策的AI工具和系统，都没有这种数据，并且要么 1) 是在GPT（或其类似物）的基础上构建的，没有经过精细调整，要么
    2) 是伪装成AI的光荣关键字匹配器，或者两者兼而有之。
- en: Though human recruiters aren’t particularly good at judging resumes, and though
    we, as a society, don’t yet have a great solution to the problem of effective
    candidate filtering, it’s clear that off-the-shelf AI solutions are not the magic
    pill we’re looking for — they’re flawed in the same ways as humans. They just
    do the wrong thing faster and at scale.
  id: totrans-split-66
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然人类招聘人员并不擅长评估简历，而且虽然我们作为一个社会，对有效的候选人筛选问题尚未找到很好的解决方案，但很明显，现成的AI解决方案并不是我们正在寻找的神奇药丸
    —— 它们在与人类相同的方式上存在缺陷。它们只是更快速地以及规模化地做错事情。
- en: 'Footnotes:'
  id: totrans-split-67
  prefs: []
  type: TYPE_NORMAL
  zh: 脚注：
