- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 类别：未分类
- en: 'date: 2024-05-27 14:35:37'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-05-27 14:35:37
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: 'Turing Complete Transformers: Two Transformers Are More Powerful Than One |
    OpenReview'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 变形金刚图灵完备性：两个变形金刚比一个更强大 | OpenReview
- en: 来源：[https://openreview.net/forum?id=MGWsPGogLH](https://openreview.net/forum?id=MGWsPGogLH)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://openreview.net/forum?id=MGWsPGogLH](https://openreview.net/forum?id=MGWsPGogLH)
- en: '**Primary Area:** general machine learning (i.e., none of the above)'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: '**主要领域：** 通用机器学习（即以上所有领域之外）'
- en: '**Code Of Ethics:** I acknowledge that I and all co-authors of this work have
    read and commit to adhering to the ICLR Code of Ethics.'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '**伦理准则：** 我承认我和本作品的所有合作者已阅读并承诺遵守ICLR伦理准则。'
- en: '**Keywords:** transformers, computational complexity, computation, generalization,
    agents, multi-model'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '**关键词：** 变形金刚，计算复杂性，计算，泛化，代理，多模型'
- en: '**Submission Guidelines:** I certify that this submission complies with the
    submission instructions as described on https://iclr.cc/Conferences/2024/AuthorGuide.'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '**提交指南：** 我证明此提交符合https://iclr.cc/Conferences/2024/AuthorGuide上描述的提交说明。'
- en: '**TL;DR:** We prove transformers are not Turing complete, propose a new architecture
    that is Turing complete, and empirically demonstrate that the new architecture
    can generalize more effectively than transformers.'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '**TL;DR：** 我们证明变形金刚不是图灵完备的，提出一种新的架构，该架构是图灵完备的，并通过实验证明新的架构能够比变形金刚更有效地进行泛化。'
- en: '**Abstract:** This paper presents Find+Replace transformers, a family of multi-transformer
    architectures that can provably do things no single transformer can, and which
    outperforms GPT-4 on several challenging tasks. We first establish that traditional
    transformers and similar architectures are not Turing Complete, while Find+Replace
    transformers are. Using this fact, we show how arbitrary programs can be compiled
    into Find+Replace transformers, potentially aiding interpretability research.
    We also demonstrate the superior performance of Find+Replace transformers over
    GPT-4 on a set of composition challenge problems. This work aims to provide a
    theoretical basis for multi-transformer architectures, and to encourage their
    further exploration.'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '**摘要：** 本文提出了Find+Replace变形金刚，这是一系列多变形金刚架构，可以明确地完成单个变形金刚无法完成的任务，并且在几项具有挑战性的任务上胜过了GPT-4。我们首先确定了传统的变形金刚和类似的架构不是图灵完备的，而Find+Replace变形金刚是图灵完备的。利用这一事实，我们展示了如何将任意程序编译成Find+Replace变形金刚，可能有助于解释性研究。我们还展示了Find+Replace变形金刚在一组复合挑战问题上优于GPT-4的性能。本工作旨在为多变形金刚架构提供理论基础，并鼓励进一步探索。'
- en: '**Anonymous Url:** I certify that there is no URL (e.g., github page) that
    could be used to find authors'' identity.'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: '**匿名网址：** 我证明没有任何网址（例如 GitHub 页面）可用于查找作者的身份。'
- en: '**No Acknowledgement Section:** I certify that there is no acknowledgement
    section in this submission for double blind review.'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: '**无致谢部分：** 我证明此提交中没有致谢部分，以供双盲评审。'
- en: '**Submission Number:** 8781'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: '**提交编号：** 8781'
