- en: <!--yml
  id: totrans-split-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-split-1
  prefs: []
  type: TYPE_NORMAL
  zh: 'category: 未分类'
- en: 'date: 2024-05-27 13:00:49'
  id: totrans-split-2
  prefs: []
  type: TYPE_NORMAL
  zh: 'date: 2024-05-27 13:00:49'
- en: -->
  id: totrans-split-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: 'Hello OLMo: A truly open LLM. As the world races to deploy AI models… | by
    AI2 | AI2 Blog'
  id: totrans-split-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 你好，OLMo：一款真正开放的LLM。随着世界竞相部署AI模型... | 作者：AI2 | AI2 博客
- en: 来源：[https://blog.allenai.org/hello-olmo-a-truly-open-llm-43f7e7359222?gi=760105621962](https://blog.allenai.org/hello-olmo-a-truly-open-llm-43f7e7359222?gi=760105621962)
  id: totrans-split-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://blog.allenai.org/hello-olmo-a-truly-open-llm-43f7e7359222?gi=760105621962](https://blog.allenai.org/hello-olmo-a-truly-open-llm-43f7e7359222?gi=760105621962)
- en: '**Hello OLMo: A truly open LLM**'
  id: totrans-split-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**你好，OLMo：一款真正开放的LLM**'
- en: As the world races to deploy AI models that are effective and safe, the demand
    for Open Large Language Models (LLMs) has exploded. The massive adoption of both
    open and closed AI models means that AI capabilities have leapfrogged our ability
    to understand how they are created. Releasing the OLMo framework will provide
    the industry with an opportunity to understand what is going on inside AI models.
  id: totrans-split-7
  prefs: []
  type: TYPE_NORMAL
  zh: 随着世界竞相部署既有效又安全的AI模型，对开放大语言模型（LLM）的需求激增。开放和封闭AI模型的广泛采用意味着AI能力已经超越了我们理解它们如何被创造的能力。发布OLMo框架将为行业提供了解AI模型内部运作的机会。
- en: Today, [The Allen Institute for AI (AI2)](https://allenai.org/olmo) has released
    [OLMo 7B](https://huggingface.co/allenai/OLMo-7B), a truly open, state-of-the-art
    large language model released alongside the pre-training data and training code.
    This empowers researchers and developers to use the best *and* open models to
    advance the science of language models collectively.
  id: totrans-split-8
  prefs: []
  type: TYPE_NORMAL
  zh: 今天，[The Allen Institute for AI (AI2)](https://allenai.org/olmo)发布了[OLMo 7B](https://huggingface.co/allenai/OLMo-7B)，一款真正开放、先进的大语言模型，同时发布了预训练数据和训练代码。这使研究人员和开发人员能够共同使用最佳和开放的模型来推进语言模型科学。
- en: “Open foundation models have been critical in driving a burst of innovation
    and development around generative AI,” said Yann LeCun, Chief AI Scientist at
    Meta. “The vibrant community that comes from open source is the fastest and most
    effective way to build the future of AI.”
  id: totrans-split-9
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: “开放基础模型在推动生成AI周围的创新和发展方面起到了关键作用，”Meta的首席AI科学家Yann LeCun说道。“开源社区带来的充满活力的社区是构建AI未来的最快、最有效的方式。”
- en: OLMo and the framework is designed to aid researchers in training and experimenting
    with large language models. They are available for direct download on [Hugging
    Face](https://huggingface.co/allenai/OLMo-7B) and in [GitHub](https://github.com/allenai/OLMo).
    This work was made possible, in part, via a collaboration with the [Kempner Institute
    for the Study of Natural and Artificial Intelligence at Harvard University](https://www.harvard.edu/kempner-institute/)
    and partners including [AMD](https://www.amd.com/en.html), [CSC](https://www.csc.fi/en/home)
    ([Lumi Supercomputer](https://www.lumi-supercomputer.eu/)), the [Paul G. Allen
    School of Computer Science & Engineering at the University of Washington](https://www.cs.washington.edu/)
    and [Databricks](https://www.databricks.com/).
  id: totrans-split-10
  prefs: []
  type: TYPE_NORMAL
  zh: OLMo和其框架旨在帮助研究人员训练和实验大型语言模型。它们可直接在[Hugging Face](https://huggingface.co/allenai/OLMo-7B)和[GitHub](https://github.com/allenai/OLMo)上下载。这项工作在某种程度上通过与[哈佛大学肯普纳自然与人工智能研究所](https://www.harvard.edu/kempner-institute/)和包括[AMD](https://www.amd.com/en.html)、[CSC](https://www.csc.fi/en/home)（[Lumi超级计算机](https://www.lumi-supercomputer.eu/)）、[华盛顿大学保罗·艾伦计算机科学与工程学院](https://www.cs.washington.edu/)以及[Databricks](https://www.databricks.com/)的合作实现。
- en: 'The framework features a suite of completely open AI development tools, including:'
  id: totrans-split-11
  prefs: []
  type: TYPE_NORMAL
  zh: 该框架提供了一套完全开放的AI开发工具，包括：
- en: '**Full pretraining data**: The model is built on AI2’s [Dolma](/dolma-3-trillion-tokens-open-llm-corpus-9a0ff4b8da64)
    set which features three trillion token open corpus for language model pretraining,
    including code that produces the training data.'
  id: totrans-split-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**完整的预训练数据：** 该模型基于AI2的[Dolma](/dolma-3-trillion-tokens-open-llm-corpus-9a0ff4b8da64)数据集，包括用于语言模型预训练的三万亿标记的开放语料库，包括生成训练数据的代码。'
- en: '**Training code and model weights:** The OLMo framework includes full model
    weights for four model variants at the 7B scale, each trained to at least 2T tokens.
    Inference code, training metrics and training logs are all provided.'
  id: totrans-split-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**训练代码和模型权重：** OLMo 框架包括四种 7B 规模模型变体的完整模型权重，每个模型至少训练了 2T 个标记。推理代码、训练指标和训练日志均已提供。'
- en: '**Evaluation:** We’ve released the evaluation suite used in development, complete
    with 500+ checkpoints per model, from every 1000 steps during the training process
    and evaluation code under the umbrella of the Catwalk project.'
  id: totrans-split-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**评估：** 我们发布了开发中使用的评估套件，每个模型每1000步提供500多个检查点，以及Catwalk项目下的评估代码。'
- en: “I’m enthusiastic about getting OLMo into the hands of AI researchers,” said
    Eric Horvitz, Microsoft’s Chief Scientific Officer and a founding member of the
    AI2 Scientific Advisory Board. “The new offering continues Allen AI’s tradition
    of providing valuable open models, tools, and data, which have spurred numerous
    advancements in AI across the global community.”
  id: totrans-split-15
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: “我对将OLMo交到AI研究人员手中感到非常激动，”微软首席科学家、AI2科学顾问委员会的创始成员埃里克·霍维茨说道。“这一新提供延续了Allen AI提供有价值的开放模型、工具和数据的传统，这些已经在全球社区的AI领域推动了许多进步。”
- en: '**A truly open model**'
  id: totrans-split-16
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**真正开放的模型：**'
- en: By making OLMo and its training data fully available to the public, AI2 has
    taken a big step towards collaboratively building the best open language model
    in the world. In the coming months, AI2 will continue to iterate on OLMo and will
    bring different model sizes, modalities, datasets, and capabilities into the OLMo
    family.
  id: totrans-split-17
  prefs: []
  type: TYPE_NORMAL
  zh: 通过将OLMo及其训练数据完全提供给公众，AI2迈出了构建世界上最好的开放语言模型的重要一步。在未来几个月中，AI2将继续对OLMo进行迭代，并将不同的模型大小、模态、数据集和功能引入OLMo家族。
- en: “Many language models today are published with limited transparency. Without
    having access to training data, researchers cannot scientifically understand how
    a model is working. It’s the equivalent of drug discovery without clinical trials
    or studying the solar system without a telescope,” said [Hanna Hajishirzi](https://homes.cs.washington.edu/~hannaneh/),
    OLMo project lead, a senior director of NLP Research at AI2, and a professor in
    the UW’s Allen School. “With our new framework, researchers will finally be able
    to study the science of LLMs, which is critical to building the next generation
    of safe and trustworthy AI.”
  id: totrans-split-18
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: “如今许多语言模型发布时都缺乏透明度。没有访问训练数据，研究人员就无法科学地理解模型的工作原理。这相当于没有临床试验进行药物发现，或者在没有望远镜的情况下研究太阳系。”
    [汉娜·哈吉什尔齐](https://homes.cs.washington.edu/~hannaneh/)说道，她是OLMo项目负责人，AI2的NLP研究高级总监，也是华盛顿大学艾伦学院的教授。“通过我们的新框架，研究人员最终将能够研究LLM的科学，这对于构建下一代安全可信赖的AI至关重要。”
- en: 'With OLMo, AI researchers and developers will experience:'
  id: totrans-split-19
  prefs: []
  type: TYPE_NORMAL
  zh: 有了OLMo，AI研究人员和开发人员将体验到：
- en: '**More Precision:** With full insight into the training data behind the model,
    researchers will be able to work faster and no longer need to depend on qualitative
    assumptions of how we feel the model is performing but can test it scientifically.'
  id: totrans-split-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**更精确：** 有了对模型背后的训练数据的全面了解，研究人员将能够更快地工作，不再需要依赖对模型表现的定性假设，而可以进行科学测试。'
- en: '**Less Carbon:** Currently one training run is equivalent to the emissions
    of [nine US homes for one year.](https://www.epa.gov/energy/greenhouse-gas-equivalencies-calculator)
    By opening the full training and evaluation ecosystem, it radically reduces developmental
    redundancies, which is critical in the decarbonization of AI'
  id: totrans-split-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**更少的碳排放：** 目前，一次训练运行相当于[九个美国家庭一年的排放量](https://www.epa.gov/energy/greenhouse-gas-equivalencies-calculator)。通过开放完整的训练和评估生态系统，它大大减少了开发中的冗余，这对于AI的脱碳至关重要。'
- en: '**Lasting results:** Keeping models and their datasets in the open and not
    behind APIs enables researchers to learn and build from previous models and work.'
  id: totrans-split-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**持久的结果：** 将模型及其数据集保持开放，而不是隐藏在API后面，使研究人员能够从以前的模型和工作中学习和构建。'
- en: “With OLMo, open *actually* means ‘open’ and everyone in the AI research community
    will have access to all aspects of model creation, including training code, evaluation
    methods, data, and so on” said [Noah Smith](https://nasmith.github.io/), OLMo
    project lead, a senior director of NLP Research at AI2, and a professor in the
    UW’s Allen School. “AI was once an open field centered on an active research community,
    but as models grew, became more expensive, and started turning into commercial
    products, AI work started to happen behind closed doors. With OLMo we hope to
    work against this trend and empower the research community to come together to
    better understand and engage with language models in a scientific way, leading
    to more responsible AI technology that benefits everyone.”
  id: totrans-split-23
  prefs: []
  type: TYPE_NORMAL
  zh: “在OLMo的帮助下，开放*实际上*意味着‘开放’，AI研究界的每个人都将可以访问模型创建的所有方面，包括训练代码、评估方法、数据等等。” [诺亚·史密斯](https://nasmith.github.io/)
    说道，他是OLMo项目负责人，AI2的NLP研究高级总监，也是华盛顿大学艾伦学院的教授。“AI曾经是一个以积极的研究社区为中心的开放领域，但随着模型的增长，变得更加昂贵，并且开始变成商业产品，AI工作开始在幕后进行。通过OLMo，我们希望抵制这种趋势，并赋予研究社区以更好地以科学方式理解和参与语言模型，从而产生对每个人都有益的更负责任的AI技术。”
- en: “With AI2’s deep expertise in natural language processing combined with AMD
    high-performance computing engines, the OLMo models developed on the LUMI Supercomputer
    powered by AMD EPYC™ CPUs and AMD Instinct™ accelerators offer a unique opportunity
    to truly expand AI experimentation and innovation and advance the industry like
    never before. This new open framework will provide the AI research community across
    the world with trusted resources and a platform to contribute to and work directly
    on language models.” — Ian Ferreria, Senior Director, AI Solutions, AMD
  id: totrans-split-24
  prefs: []
  type: TYPE_NORMAL
  zh: “AI2在自然语言处理方面拥有深厚的专业知识，结合AMD高性能计算引擎，由AMD EPYC™ CPU和AMD Instinct™加速器驱动的LUMI超级计算机上开发的OLMo模型为真正扩展AI实验和创新，推动行业发展提供了独特的机会。这一新的开放框架将为全球AI研究社区提供可信赖的资源和平台，直接参与和开发语言模型。”
    — Ian Ferreria，AMD AI解决方案高级总监
- en: “We are happy that we can contribute to this important initiative by providing
    the computing capacity from the LUMI supercomputer along with our expertise. Public
    supercomputers like LUMI play a vital role in the infrastructure for open and
    transparent AI.” Dr. Pekka Manninen, Director of Science and Technology, CSC
  id: totrans-split-25
  prefs: []
  type: TYPE_NORMAL
  zh: “我们很高兴能通过提供来自LUMI超级计算机的计算能力以及我们的专业知识，为这一重要倡议做出贡献。像LUMI这样的公共超级计算机在开放和透明的AI基础设施中发挥着至关重要的作用。”
    Dr. Pekka Manninen，CSC科学技术总监
- en: LUMI supercomputer in Finland is hosted by CSC, and owned by EuroHPC Joint Undertaking
    and 10 European countries. LUMI is the fastest supercomputer in Europe, and is
    known for its entirely carbon-free operations and was critical in supporting the
    pre-training work necessary to develop OLMo.
  id: totrans-split-26
  prefs: []
  type: TYPE_NORMAL
  zh: LUMI超级计算机位于芬兰，由CSC托管，由EuroHPC联合企业和10个欧洲国家拥有。LUMI是欧洲最快的超级计算机，以完全无碳运营而闻名，并在支持开发OLMo所需的预训练工作中发挥了关键作用。
- en: “Databricks is excited to be collaborating with the Allen Institute for AI on
    the release of their OLMo open source model and framework. OLMo sets the standard
    for what it means to be open. Everyone in academia, industry, and the broader
    community will benefit enormously from access to not only the model but all of
    the training details, including the data, code, and intermediate checkpoints.
    I am especially proud that this model was developed on the Mosaic AI model training
    platform from Databricks. As with all great open source releases, the best is
    yet to come now that these artifacts and tools are in the hands of the community.”
    — Jonathan Frankle, Chief Scientist (Neural Networks), Databricks.
  id: totrans-split-27
  prefs: []
  type: TYPE_NORMAL
  zh: “Databricks很高兴与艾伦人工智能研究所合作发布他们的OLMo开源模型和框架。OLMo设定了开放标准的新标准。学术界、工业界和更广泛的社区的每个人都将从访问模型以及包括数据、代码和中间检查点在内的所有培训细节中受益良多。我特别自豪地宣布，这个模型是在Databricks的Mosaic
    AI模型训练平台上开发的。正如所有伟大的开源发布一样，现在这些工具和艺术品已经交到社区手中，最好的时代即将到来。” — Jonathan Frankle，Databricks首席科学家（神经网络）
- en: Learn more
  id: totrans-split-28
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 了解更多
- en: '[Getting started with OLMo technical blog](/olmo-open-language-model-87ccfc95f580)'
  id: totrans-split-29
  prefs: []
  type: TYPE_NORMAL
  zh: '[OLMo技术博客入门指南](/olmo-open-language-model-87ccfc95f580)'
- en: '[OLMo 7B technical report](https://allenai.org/olmo/olmo-paper.pdf)'
  id: totrans-split-30
  prefs: []
  type: TYPE_NORMAL
  zh: '[OLMo 7B技术报告](https://allenai.org/olmo/olmo-paper.pdf)'
- en: '[Get OLMo 7B](https://huggingface.co/allenai/OLMo-7B)'
  id: totrans-split-31
  prefs: []
  type: TYPE_NORMAL
  zh: '[获取OLMo 7B](https://huggingface.co/allenai/OLMo-7B)'
- en: For more information on the OLMo framework and The Allen Institute for AI visit
    [here](https://allenai.org/olmo).
  id: totrans-split-32
  prefs: []
  type: TYPE_NORMAL
  zh: 有关OLMo框架和艾伦人工智能研究所的更多信息，请访问[这里](https://allenai.org/olmo)。
