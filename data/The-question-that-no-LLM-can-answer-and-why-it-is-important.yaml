- en: <!--yml
  id: totrans-split-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-split-1
  prefs: []
  type: TYPE_NORMAL
  zh: 'category: 未分类'
- en: 'date: 2024-05-27 13:30:43'
  id: totrans-split-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-05-27 13:30:43
- en: -->
  id: totrans-split-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: The question that no LLM can answer and why it is important
  id: totrans-split-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 没有LLM能够回答的问题及其重要性
- en: 来源：[https://www.mindprison.cc/p/the-question-that-no-llm-can-answer](https://www.mindprison.cc/p/the-question-that-no-llm-can-answer)
  id: totrans-split-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://www.mindprison.cc/p/the-question-that-no-llm-can-answer](https://www.mindprison.cc/p/the-question-that-no-llm-can-answer)
- en: '***[Notes From the Desk](https://www.mindprison.cc/s/notes-from-the-desk)**
    are periodic posts that summarize recent topics of interest or other brief notable
    commentary that might otherwise be a tweet or note.*'
  id: totrans-split-6
  prefs: []
  type: TYPE_NORMAL
  zh: '***[来自办公桌的笔记](https://www.mindprison.cc/s/notes-from-the-desk)**是定期发布的文章，总结了最近的感兴趣的主题或其他简要的值得注意的评论，这些评论本来可能只是一条推特或注释。*'
- en: '* * *'
  id: totrans-split-7
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: “Which episode of Gilligan’s Island was about mind reading?”
  id: totrans-split-8
  prefs: []
  type: TYPE_NORMAL
  zh: “《格林那哥岛》的哪一集涉及读心术？”
- en: No LLM properly answers this question unless it uses a web search. But how can
    a LLM not know the answer if it was trained on essentially the entire internet
    of data and certainly most likely all the data in IMDB? The correct answer is
    “[Seer Gilligan](https://www.imdb.com/title/tt1230843/)”, one of the most popular
    episodes.
  id: totrans-split-9
  prefs: []
  type: TYPE_NORMAL
  zh: 除非进行网页搜索，否则没有LLM能够正确回答这个问题。但是，如果LLM是在基本上整个互联网的数据和几乎肯定IMDB中的所有数据上进行训练的话，它怎么可能不知道答案呢？正确的答案是“[Seer
    Gilligan](https://www.imdb.com/title/tt1230843/)”，这是最受欢迎的集数之一。
- en: 'The following are the responses from multiple top models:'
  id: totrans-split-10
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是多个顶级模型的响应：
- en: '**Opus**'
  id: totrans-split-11
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**作品**'
- en: I apologize, but there is no episode of Gilligan's Island that specifically
    focuses on mind reading.
  id: totrans-split-12
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 对不起，没有一集“Gilligan's Island”专门以读心术为主题。
- en: '* * *'
  id: totrans-split-13
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '* * *'
- en: '**GPT**  **4**'
  id: totrans-split-14
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**GPT**  **4**'
- en: The episode of "Gilligan's Island" that involves mind reading is titled "Ring
    Around Gilligan". It is the sixth episode of the third season.
  id: totrans-split-15
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: “Gilligan's Island”中涉及读心术的集数名为“Ring Around Gilligan”。这是第三季的第六集。
- en: '* * *'
  id: totrans-split-16
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '* * *'
- en: '**Gemini 1.5**'
  id: totrans-split-17
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**Gemini 1.5**'
- en: The episode of Gilligan's Island that features mind reading is Season 2, Episode
    12 - "The Mind Reader".
  id: totrans-split-18
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: “Gilligan's Island”中涉及读心术的集数是第2季第12集，名为“Mind Reader”。
- en: '* * *'
  id: totrans-split-19
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '* * *'
- en: '**Llama 3**'
  id: totrans-split-20
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**Llama 3**'
- en: After conducting a thorough search and reviewing episode guides, I found that
    there is no episode of Gilligan's Island that specifically focuses on mind reading.
  id: totrans-split-21
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 经过彻底搜索和审核集数指南后，我发现“Gilligan's Island”中没有专门以读心术为主题的集数。
- en: '* * *'
  id: totrans-split-22
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '* * *'
- en: '**Llama 3 -** ***asked to list and name episodes***'
  id: totrans-split-23
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**Llama 3 -** ***被要求列出并命名集数***'
- en: '"The Postman Cometh" (March 13, 1965)'
  id: totrans-split-24
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '"The Postman Cometh"（1965年3月13日）'
- en: '"**Seer Gilligan**" (Wait, no! I apologize, but there is no episode titled
    "Seer Gilligan")'
  id: totrans-split-25
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: “Seer Gilligan”（等等！对不起，没有名为“Seer Gilligan”的集数）
- en: '"Love Me, Love My Chicken" (March 20, 1965)'
  id: totrans-split-26
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '"爱我，爱我的鸡"（1965年3月20日）'
- en: Please note that there is no episode titled "**Seer Gilligan**". If you have
    any other questions or need help with a specific episode, feel free to ask!
  id: totrans-split-27
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 请注意，没有名为“**Seer Gilligan**”的集数。如果您有其他问题或需要帮助找到特定集数，请随时提问！
- en: All four models fail to give the correct answer. Two of the models hallucinate
    an answer and the remaining two deny such an episode exists. Interestingly, probing
    Llama 3 a bit more reveals it is aware of the episode; however, it still denies
    its existence. It seems to know there are 98 episodes, but the majority of titles
    it lists are made up.
  id: totrans-split-28
  prefs: []
  type: TYPE_NORMAL
  zh: 所有四个模型都未能给出正确答案。其中两个模型幻觉出一个答案，另外两个则否认存在这样的集数。有趣的是，再深入探索Llama 3时发现，它知道这一集的存在；然而，它仍然否认其存在。它似乎知道有98集，但它列出的大部分标题都是虚构的。
- en: In the case of Llama 3, we can probe some of the training dataset [using Infini-gram](https://huggingface.co/spaces/liujch1998/infini-gram)
    and verify that the episode does exist in the corpus along with text describing
    the episode.
  id: totrans-split-29
  prefs: []
  type: TYPE_NORMAL
  zh: 对于Llama 3，我们可以使用[Infini-gram](https://huggingface.co/spaces/liujch1998/infini-gram)来探索一些训练数据集，并验证该集中是否存在该集数以及描述该集数的文本。
- en: We also see another interesting data and training phenomenon revealed when LLMs
    are asked to provide a number between 1-100\. They all converge to 42!
  id: totrans-split-30
  prefs: []
  type: TYPE_NORMAL
  zh: 当LLM被要求提供1-100之间的数字时，我们还看到另一个有趣的数据和训练现象：它们都收敛到42！
- en: '* * *'
  id: totrans-split-31
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Pointed out by [Information is Beautiful](https://twitter.com/infobeautiful/status/1778059112250589561),
    a very interesting distribution forms when AI is asked to pick a number between
    1 and 100\. There is a heavy weighting toward picking the number ‘42’. Likely,
    this is the [Hitchhiker’s Guide to the Galaxy](https://en.wikipedia.org/wiki/Phrases_from_The_Hitchhiker%27s_Guide_to_the_Galaxy#The_Answer_to_the_Ultimate_Question_of_Life,_the_Universe,_and_Everything_is_42)
    effect. The number 42 is overrepresented or weighted in some way through training,
    resulting in a higher propensity for the LLM to choose 42.
  id: totrans-split-32
  prefs: []
  type: TYPE_NORMAL
  zh: 据 [信息美](https://twitter.com/infobeautiful/status/1778059112250589561) 表示，当人工智能被要求在1到100之间选择一个数字时，会形成一个非常有趣的分布。有一种强烈的偏好选择数字‘42’。很可能，这是《银河系漫游指南》效应。数字42在训练中被过度表示或加权，导致LLM更倾向于选择42。
- en: The implications are that LLMs do not perform reasoning over data in the way
    that most people conceive or desire.
  id: totrans-split-33
  prefs: []
  type: TYPE_NORMAL
  zh: 结果是LLM不能以大多数人理解或期望的方式进行数据推理。
- en: There is no self-reflection of its information; it does not know what it knows
    and what it does not. The line between hallucination and truth is simply a probability
    factored by the prevalence of training data and post-training processes like fine-tuning.
    Reliability will always be nothing more than a probability built on top of this
    architecture.
  id: totrans-split-34
  prefs: []
  type: TYPE_NORMAL
  zh: 它没有关于信息的自我反思；它不知道它知道什么和不知道什么。幻觉和真理之间的界线仅仅是由训练数据的普遍性和像微调之类的后处理过程因素化的概率事实。可靠性永远不会超过这种体系建立的概率。
- en: As such, it becomes unsuitable as a machine to find rare hidden truths or valuable
    neglected information. It will always simply converge toward popular narrative
    or data. At best, it can provide new permutations of views of existing well-known
    concepts, but it can not invent new concepts or reveal concepts rarely spoken
    about.
  id: totrans-split-35
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，它不适合作为发现罕见隐秘真相或有价值被忽视信息的工具。它总是简单地趋向于流行的叙事或数据。最多，它可以提供现有众所周知概念的新视角，但不能创造新概念或揭示很少被谈论的概念。
- en: “You can't cache reality in some compressed lookup table. If a particular outcome
    was never in the training data, the model will perform a random guess which is
    quite limiting.”
  id: totrans-split-36
  prefs: []
  type: TYPE_NORMAL
  zh: “你不能把现实存在压缩的查找表中。如果特定结果从未出现在训练数据中，模型将执行一个相当限制性的随机猜测。”
- en: — [Chomba Bupe](https://twitter.com/ChombaBupe/status/1781831176367312981)
  id: totrans-split-37
  prefs: []
  type: TYPE_NORMAL
  zh: — [乔姆巴·布佩](https://twitter.com/ChombaBupe/status/1781831176367312981)
- en: Furthermore, it can never be a system for absolute dependability. Mission-critical
    systems that require deterministic, provably correct behavior are not something
    applicable to LLM automation or control. The problem is that LLMs are impressively
    convincing when they are wrong, which may lead to ill-advised adoption. What business
    wants to balance the books with a hallucinating calculator?
  id: totrans-split-38
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，它永远不可能成为绝对可靠性的系统。需要确定性、可证明正确行为的任务关键系统并不适用于LLM的自动化或控制。问题在于，LLM在错误时令人印象深刻地令人信服，这可能导致不明智的采纳。哪家企业希望用幻觉计算器来平衡账目呢？
- en: '**Implications**:'
  id: totrans-split-39
  prefs: []
  type: TYPE_NORMAL
  zh: '**影响**：'
- en: Results are probabilities defined more by data prevalence than logic or reason.
  id: totrans-split-40
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 结果的概率更多地由数据的普遍性定义，而非逻辑或理性。
- en: It is indiscernible to what degree a LLM is reliable on a given question.
  id: totrans-split-41
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: LLM在特定问题上的可靠程度难以辨别。
- en: Not useful to find undiscovered truths or neglected but brilliant ideas.
  id: totrans-split-42
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 不适用于发现未知真理或被忽视但卓越的想法。
- en: Inability to theorize new concepts or discoveries.
  id: totrans-split-43
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 无法理论化新概念或发现。
- en: It is substantially ironic that LLMs are failing at the primary use cases that
    are attracting [billions of investment](https://www.mindprison.cc/p/stargate-the-100-billion-hail-mary-agi),
    but are rather proficient at the use cases we do not desire, such as [destruction
    of privacy and liberty](https://www.mindprison.cc/p/ai-end-of-privacy-end-of-sanity),
    a [post-truth society](https://www.mindprison.cc/p/ai-accelerates-post-truth-civilization),
    [social manipulation](https://www.mindprison.cc/p/ai-instructed-brainwashing-effectively),
    the [severance of human connection](https://www.mindprison.cc/p/ai-is-transforming-us-into-npcs),
    [fountains of noise](https://www.mindprison.cc/p/ai-automation-to-bury-civilization),
    the [devaluation of meaning](https://www.mindprison.cc/p/ai-art-challenges-meaning-in-a-world),
    and a plethora of other [societal issues](https://www.mindprison.cc/p/ai-and-the-end-to-all-things).
  id: totrans-split-44
  prefs: []
  type: TYPE_NORMAL
  zh: 是有些讽刺的是，LLM（大语言模型）正失败于吸引[数十亿美元的投资](https://www.mindprison.cc/p/stargate-the-100-billion-hail-mary-agi)的主要用例，而相反，在我们不希望的用例上表现出色，比如[隐私和自由的破坏](https://www.mindprison.cc/p/ai-end-of-privacy-end-of-sanity)，[后真相社会](https://www.mindprison.cc/p/ai-accelerates-post-truth-civilization)，[社会操控](https://www.mindprison.cc/p/ai-instructed-brainwashing-effectively)，[人类联系的断裂](https://www.mindprison.cc/p/ai-is-transforming-us-into-npcs)，[噪音的喷泉](https://www.mindprison.cc/p/ai-automation-to-bury-civilization)，[意义贬值](https://www.mindprison.cc/p/ai-art-challenges-meaning-in-a-world)，以及众多[社会问题](https://www.mindprison.cc/p/ai-and-the-end-to-all-things)。
- en: '* * *'
  id: totrans-split-45
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Unlike much of the internet now, there is a human mind behind all the content
    created here at Mind Prison. I typically spend hours to days on articles including
    creating the illustrations for each. I hope if you find them valuable and you
    still appreciate the creations from the organic hardware within someone’s head
    that you will consider subscribing. Thank you!
  id: totrans-split-46
  prefs: []
  type: TYPE_NORMAL
  zh: 与现在的互联网不同，Mind Prison 的所有内容都是由人类心智创造的。我通常花数小时到数天撰写文章，并为每篇文章制作插图。如果您觉得这些内容有价值，并且仍然欣赏来自某人有机硬件内的创作，希望您考虑订阅。谢谢！
- en: '* * *'
  id: totrans-split-47
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: '*No compass through the dark exists without hope of reaching the other side
    and the belief that it matters …*'
  id: totrans-split-48
  prefs: []
  type: TYPE_NORMAL
  zh: '*没有指南针能在没有希望到达另一边，并且相信这很重要的黑暗中存在……*'
- en: Mind Prison is a reader-supported publication. You can also assist by sharing.
  id: totrans-split-49
  prefs: []
  type: TYPE_NORMAL
  zh: Mind Prison 是一本由读者支持的出版物。您也可以通过分享来提供帮助。
- en: '[Share](https://www.mindprison.cc/p/the-question-that-no-llm-can-answer?utm_source=substack&utm_medium=email&utm_content=share&action=share)'
  id: totrans-split-50
  prefs: []
  type: TYPE_NORMAL
  zh: '[分享](https://www.mindprison.cc/p/the-question-that-no-llm-can-answer?utm_source=substack&utm_medium=email&utm_content=share&action=share)'
