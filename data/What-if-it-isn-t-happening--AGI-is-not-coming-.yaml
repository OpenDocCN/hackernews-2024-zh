- en: <!--yml
  id: totrans-split-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-split-1
  prefs: []
  type: TYPE_NORMAL
  zh: 'category: 未分类'
- en: 'date: 2024-05-27 14:42:54'
  id: totrans-split-2
  prefs: []
  type: TYPE_NORMAL
  zh: 'date: 2024-05-27 14:42:54'
- en: -->
  id: totrans-split-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: What if it isn't happening, AGI is not coming?
  id: totrans-split-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: What if it isn't happening, AGI is not coming?
- en: 来源：[https://www.mindprison.cc/p/what-if-agi-is-not-coming](https://www.mindprison.cc/p/what-if-agi-is-not-coming)
  id: totrans-split-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://www.mindprison.cc/p/what-if-agi-is-not-coming](https://www.mindprison.cc/p/what-if-agi-is-not-coming)
- en: '***[Notes From the Desk](https://www.mindprison.cc/s/notes-from-the-desk)**
    are periodic posts that summarize recent topics of interest or other brief notable
    commentary that might otherwise be a tweet or note.*'
  id: totrans-split-6
  prefs: []
  type: TYPE_NORMAL
  zh: '***[Notes From the Desk](https://www.mindprison.cc/s/notes-from-the-desk)**
    是定期发布的帖子，总结了最近的感兴趣的主题或其他简短的值得注意的评论，这些评论原本可能是推文或便签。*'
- en: '* * *'
  id: totrans-split-7
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: No matter what appears to be happening, we always have to consider what if it
    isn't. What If LLMs fail to turn into AGIs? Has our quest for intelligence simply
    unveiled our demonstrable lack thereof? Will trillions of dollars turn unpredictable
    hallucination machines into reliable universal productivity tools that can do
    anything?
  id: totrans-split-8
  prefs: []
  type: TYPE_NORMAL
  zh: 不管看起来发生了什么，我们始终要考虑如果事情不是那样。如果大语言模型（LLMs）不能演变成AGI，那么我们对智能的追求是否揭示了我们明显的不足？将数万亿美元转化为能做任何事情的可靠通用生产工具的无法预测幻觉机器？
- en: When it comes to hard problems the herd is almost universally always wrong.
    There is an enormous amount of institutional money flowing into AI development
    on the premise that LLM architecture is going to manifest something we call AGI
    that will solve all the world’s problems.
  id: totrans-split-9
  prefs: []
  type: TYPE_NORMAL
  zh: 当涉及到困难问题时，群体几乎总是错误的。大量机构资金涌入AI开发，基于LLM架构将演变出我们称之为AGI的东西，可以解决世界上的所有问题。
- en: So what are some of the counter perspectives?
  id: totrans-split-10
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，有哪些相反的观点呢？
- en: 'These are the primary arguments that are expanded below:'
  id: totrans-split-11
  prefs: []
  type: TYPE_NORMAL
  zh: 这些是下文详细展开的主要论点：
- en: We will soon be **reaching the limits of hardware scaling** for larger AI models.
  id: totrans-split-12
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们很快将**达到硬件扩展的极限**，以支持更大的AI模型。
- en: There have been **no major AI technology breakthroughs in decades**. Everything
    we are seeing is larger compute scaling.
  id: totrans-split-13
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 几十年来**没有发生过重大的AI技术突破**。我们所看到的一切都是基于更大的计算资源的扩展。
- en: '**Scaling alone is not going to create AGI**, need other novel discoveries.'
  id: totrans-split-14
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**单靠扩展规模并不能创造通用人工智能（AGI）**，需要其他新颖的发现。'
- en: The release of ChatGPT to the public made people think that generative AI was
    just getting started on an exponentially improving curve.
  id: totrans-split-15
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ChatGPT向公众发布使人们认为生成式人工智能正处于指数改进的起步阶段。
- en: Yet, generative AI itself had been around for a while before that & **what we
    are seeing is probably the best it will ever be**.
  id: totrans-split-16
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 然而，生成式人工智能本身在此之前已经存在了一段时间，而**我们所看到的可能是它能达到的最好的水平**。
- en: Of course improving past what GPT-4 is now is going to require more & more resources
    to the point that no company, however big, will have the incentive to improve
    the technology any further.
  id: totrans-split-17
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 当然，要超越目前的GPT-4需要更多和更多的资源，到了这一步，无论多么庞大的公司都没有动力进一步改进这项技术。
- en: '**Until a new breakthrough comes along, generative AI is almost at its peak
    now**.'
  id: totrans-split-18
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**在新的突破出现之前，生成式人工智能几乎已经达到了其巅峰**。'
- en: If Google struggled hard to make a model based on performance in the class of
    GPT-4, do you honestly think there is more juice in generative pretrained transformer
    (GPT) models to keep pumping out more improvements?
  id: totrans-split-19
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 如果谷歌为了制作一个性能与GPT-4相当的模型而苦苦挣扎，你真的认为生成预训练变换器（GPT）模型还能继续产生更多改进吗？
- en: I don't think so, **these models are already too big**.
  id: totrans-split-20
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 我不这么认为，**这些模型已经太大了**。
- en: …
  id: totrans-split-21
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: …
- en: Therefore GPT-4 is right at the elbow of the S-curve given the sort of diminishing
    returns already being observed - language models, both closed & open source, **aren't
    improving as much as they are demanding more & more resources** such as compute
    & data.
  id: totrans-split-22
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 因此，GPT-4正处于S型曲线的拐点处，由于语言模型，无论是封闭还是开放源代码，**都不是在大幅改进，而是在需求更多和更多的资源**，如计算资源和数据。
- en: 'See also:'
  id: totrans-split-23
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 另见：
- en: …
  id: totrans-split-24
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: …
- en: — Chomba Bupe, [X post](https://twitter.com/ChombaBupe/status/1763617694023033226)
  id: totrans-split-25
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: — Chomba Bupe, [X post](https://twitter.com/ChombaBupe/status/1763617694023033226)
- en: A deep neural net (DNN) is just an arrangement of simple nodes - the perceptrons
    from the 40s, with various nonlinearities - in layers one atop the other.
  id: totrans-split-26
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 深度神经网络（DNN）只是一种简单节点的排列 - 从40年代的感知器开始，带有各种非线性，叠加在一起的层。
- en: AI companies are **hoping that if they can mega size these DNNs by using lots
    of compute & data they can solve intelligence**.
  id: totrans-split-27
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: AI公司**希望通过大量使用计算资源和数据来扩展这些深度神经网络（DNNs），以解决智能问题**。
- en: The foundation for DNNs was **layed out in the 80s**.
  id: totrans-split-28
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 深度神经网络（DNNs）的基础**在80年代已经奠定**。
- en: Gradient descent (GD) was around even much earlier than that, stochastic gradient
    descent (SGD) is a variant of GD. For GD, the gradients are computed by passing
    through the entire training set before taking a step. SGD uses one sample - drawn
    randomly from the training set hence the term stochastic. Batch GD uses a greater
    than 1 sample but not all samples to compute gradients.
  id: totrans-split-29
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 梯度下降（GD）甚至早于此，随机梯度下降（SGD）是GD的一种变体。对于GD，在采取步骤之前需要通过整个训练集计算梯度。SGD使用一个随机抽取的样本，因此称为随机。批量GD使用大于1的样本但不使用所有样本来计算梯度。
- en: There are variants to help convergence faster to a solution. **But nothing much
    has changed**.
  id: totrans-split-30
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 有些变体可以帮助更快地收敛到解决方案。**但是并没有太多改变**。
- en: …
  id: totrans-split-31
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: …
- en: Thus if you look at it, there hasn't been enough innovation other than **just
    mega sizing old 80s techniques using modern hardware**. The success of today's
    systems are more attributed to breakthroughs in hardware computing like GPUs,
    TPUs etc than actual AI algorithms.
  id: totrans-split-32
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 因此，如果你仔细观察，除了**仅仅是利用现代硬件将80年代的技术规模化**之外，并没有足够的创新。今天系统的成功更多归因于像GPU、TPU等硬件计算的突破，而非实际的AI算法。
- en: That's why you expect AI to hit a plateau soon because **mega sizing these models
    has limits**. Physical limitations will come into effect to prevent further scaling
    up. **No matter how much compute or data they through into these, they won't solve
    intelligence**.
  id: totrans-split-33
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 这就是为什么你预期AI很快会达到一个平台，因为**这些模型的规模化有其极限**。物理限制将会生效，阻止进一步的规模化。**无论他们投入多少计算资源或数据，它们都无法解决智能**。
- en: …
  id: totrans-split-34
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: …
- en: Chomba Bupe, [X post](https://twitter.com/ChombaBupe/status/1763982592351629767)
  id: totrans-split-35
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: Chomba Bupe，[转帖](https://twitter.com/ChombaBupe/status/1763982592351629767)
- en: A short perspective from Gary Marcus on why we haven’t seen hints of GPT5 as
    of yet. This aligns with the view of reaching limits on scaling.
  id: totrans-split-36
  prefs: []
  type: TYPE_NORMAL
  zh: Gary Marcus对为什么我们还没有看到GPT5的迹象进行了简短的阐述。这与认为达到了规模化的极限的观点是一致的。
- en: 'Here’s a theory: [Why OpenAI has not already released GPT5 to compete with
    Gemini and Claude]'
  id: totrans-split-37
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 这里有一个理论：[为什么OpenAI还没有发布与Gemini和Claude竞争的GPT5]
- en: • OpenAI tried already[creating GPT5], but what they came up with wasn’t impressive
    enough, so they called it GPT 4.5 Turbo.
  id: totrans-split-38
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: • OpenAI已经尝试过[创造GPT5]，但他们所提出的方案并不令人印象深刻，因此他们将其称为GPT 4.5 Turbo。
- en: • They are still trying, but nothing they have come up with comes close to living
    up to expectations.
  id: totrans-split-39
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: • 他们仍在努力，但他们提出的方案都没有达到预期水平。
- en: • They aren’t really sure what to do next.
  id: totrans-split-40
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: • 他们并不确定接下来该做什么。
- en: • They aren’t sure they can do a lot better without a LOT of money, maybe even
    $7T worth of infrastructure.
  id: totrans-split-41
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: • 他们不确定是否没有大量资金，甚至是价值7万亿美元的基础设施就能取得更大的进展。
- en: • So we will have to wait a while longer, maybe a lot longer.
  id: totrans-split-42
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: • 因此，我们可能还需要等待更长一段时间，甚至可能更久。
- en: Gary Marcus, [X post](https://twitter.com/GaryMarcus/status/1765198638182256780)
  id: totrans-split-43
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: Gary Marcus，[转帖](https://twitter.com/GaryMarcus/status/1765198638182256780)
- en: '* * *'
  id: totrans-split-44
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: This suite of tests from [Maximum Truth](https://www.maximumtruth.org/p/ais-ranked-by-iq-ai-passes-100-iq)
    demonstrates the trouble of measuring the “IQ” of LLM intelligence.
  id: totrans-split-45
  prefs: []
  type: TYPE_NORMAL
  zh: 这套来自[Maximum Truth](https://www.maximumtruth.org/p/ais-ranked-by-iq-ai-passes-100-iq)的测试展示了衡量LLM智能“智商”所遇到的问题。
- en: The new Claude-3 scores 101 the highest of any LLM so far. However, Claude is
    still a strange mix of capabilities in that IQ measurements give us no hint as
    to how Claude performs. It still fails to answer basic questions and hallucinates,
    but at the same time has proven to solve far more complex problems than other
    models.
  id: totrans-split-46
  prefs: []
  type: TYPE_NORMAL
  zh: 新的Claude-3得分为101，迄今为止所有LLM中最高。然而，Claude仍然是一种能力混合体，智商测量并不能告诉我们Claude的表现如何。它依然无法回答基本问题，有幻觉，但同时已经证明能够解决比其他模型更复杂的问题。
- en: 'Here is Claude-3 failing to answer correctly the relative weight of feathers
    and bricks: How many more billions of dollars of compute will solve this one?'
  id: totrans-split-47
  prefs: []
  type: TYPE_NORMAL
  zh: Claude-3未能正确回答羽毛和砖头的相对重量：还需要多少十亿美元的计算资源来解决这个问题？
- en: 'And here is Claude-3 solving novel problems:'
  id: totrans-split-48
  prefs: []
  type: TYPE_NORMAL
  zh: 而这里是Claude-3解决新问题的例子：
- en: IQ tests don’t predict this type of divergence in expected capability and it
    is exactly this that continues to create such divergence in opinion as to whether
    we are getting closer to AGI or making very expensive and unreliable Dr. Jekylls
    and Mr. Hydes.
  id: totrans-split-49
  prefs: []
  type: TYPE_NORMAL
  zh: 智商测试并不预测这种预期能力上的分歧，而正是这种分歧导致人们对是否接近AGI或者只是在制造非常昂贵且不可靠的化身犹豫不决。
- en: '* * *'
  id: totrans-split-50
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Billions to trillions of dollars will be poured into research over the next
    decade. More humans than ever are looking for breakthroughs. We have exponentially
    increased the parallel efforts. LLM architecture might be unable to deliver in
    its current state, but it has ignited monumental investments into research that
    might find other paths.
  id: totrans-split-51
  prefs: []
  type: TYPE_NORMAL
  zh: 在未来十年内，将有数十亿到数万亿美元投入研究。比以往任何时候更多的人类都在寻找突破。我们已经指数级增加了并行的努力。尽管当前的LLM架构可能无法在其当前状态下交付，但它已经点燃了可能找到其他途径的研究的巨大投资。
- en: However, expectations have been set extremely high. AI is the perfect technology
    that is currently just good enough to excite the imagination of what it could
    do making it seemingly within grasp but potentially still far away. It is the
    problem of being almost there seemingly indefinitely. Despite the real utility
    of current LLMs, the promises are far greater and if not delivered will result
    in significant market collapse in due time.
  id: totrans-split-52
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，期望值已经被设置得非常高。人工智能是一种完美技术，目前只是足够好以激发想象力，看似可以触手可及，但实际上可能仍然遥远。这是几乎无限期地接近的问题。尽管当前的LLM在现实中具有实际效用，但承诺远远超过现实，如果不能兑现，将在适当的时机导致市场的显著崩溃。
- en: Without AGI we are still exponentially improving, just not vertically with intelligence,
    but horizontally as new applications and methods are discovered for its use. Refinement
    in training, data, inferencing and application are all still advancing.
  id: totrans-split-53
  prefs: []
  type: TYPE_NORMAL
  zh: 没有通用人工智能（AGI），我们仍在指数级改进，只是不是在智能垂直方面，而是在发现其使用新应用和方法的水平方面。在训练、数据、推理和应用的精炼方面，一切仍在进步。
- en: If we are not intelligent enough to know what is intelligence, then just how
    intelligent are we? And are we intelligent enough to build more of something that
    we don't know what it is?
  id: totrans-split-54
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们不够聪明以知道什么是智能，那我们到底有多聪明？我们是否足够聪明来建造更多我们不知道是什么的东西？
- en: '* * *'
  id: totrans-split-55
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: '*No compass through the dark exists without hope of reaching the other side
    and the belief that it matters …*'
  id: totrans-split-56
  prefs: []
  type: TYPE_NORMAL
  zh: '*没有希望到达另一边的罗盘也没有意义......*'
- en: Mind Prison is a reader-supported publication. You can also assist by sharing.
  id: totrans-split-57
  prefs: []
  type: TYPE_NORMAL
  zh: Mind Prison 是一份由读者支持的出版物。你也可以通过分享来提供帮助。
- en: '[Share](https://www.mindprison.cc/p/what-if-agi-is-not-coming?utm_source=substack&utm_medium=email&utm_content=share&action=share)'
  id: totrans-split-58
  prefs: []
  type: TYPE_NORMAL
  zh: '[分享](https://www.mindprison.cc/p/what-if-agi-is-not-coming?utm_source=substack&utm_medium=email&utm_content=share&action=share)'
