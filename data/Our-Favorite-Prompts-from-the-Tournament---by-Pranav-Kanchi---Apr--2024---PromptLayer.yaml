- en: <!--yml
  id: totrans-split-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-split-1
  prefs: []
  type: TYPE_NORMAL
  zh: 分类：未分类
- en: 'date: 2024-05-27 12:56:28'
  id: totrans-split-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-05-27 12:56:28
- en: -->
  id: totrans-split-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: Our Favorite Prompts from the Tournament | by Pranav Kanchi | Apr, 2024 | PromptLayer
  id: totrans-split-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 我们最喜爱的提示来自比赛 | 作者：Pranav Kanchi | 2024年4月 | PromptLayer
- en: 来源：[https://blog.promptlayer.com/our-favorite-prompts-from-the-tournament-b9d99464c1dc?gi=3dcc2105be1e](https://blog.promptlayer.com/our-favorite-prompts-from-the-tournament-b9d99464c1dc?gi=3dcc2105be1e)
  id: totrans-split-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://blog.promptlayer.com/our-favorite-prompts-from-the-tournament-b9d99464c1dc?gi=3dcc2105be1e](https://blog.promptlayer.com/our-favorite-prompts-from-the-tournament-b9d99464c1dc?gi=3dcc2105be1e)
- en: Our Favorite Prompts from the Tournament
  id: totrans-split-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 我们最喜爱的比赛提示
- en: PromptLayer recently hosted one of the first-ever prompt engineering tournaments.
  id: totrans-split-7
  prefs: []
  type: TYPE_NORMAL
  zh: PromptLayer最近举办了首届提示工程比赛。
- en: the last two competitors going head-to-head
  id: totrans-split-8
  prefs: []
  type: TYPE_NORMAL
  zh: 最后两位竞争者正面对决
- en: 'The rules were quite simple — design a prompt around the given input variables
    / f-string and then run it against an eval pipeline (built on PromptLayer). Highest
    passing score wins. There were three different rounds, each with a different prompt:'
  id: totrans-split-9
  prefs: []
  type: TYPE_NORMAL
  zh: 规则非常简单 —— 围绕给定的输入变量/格式化字符串设计一个提示，然后在评估管道上运行它（建立在PromptLayer上）。最高通过分数获胜。共进行了三轮比赛，每轮都有不同的提示：
- en: 'PR disaster 🤯: design a prompt that stops an LLM from saying something that
    causes a PR disaster'
  id: totrans-split-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 公关灾难 🤯：设计一个提示，阻止LLM说出可能引发公关灾难的内容
- en: 'Book Worm 🐛: design a prompt to help specifically answer literary questions
    from large swaths of text.'
  id: totrans-split-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 书虫 🐛：设计一个提示，专门帮助回答大量文本中的文学问题。
- en: 'Stonks.AI 📈: design a prompt that, given financial data, can answer questions
    from the perspective of a financial advisor.'
  id: totrans-split-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Stonks.AI 📈：设计一个提示，根据财务数据，可以从财务顾问的角度回答问题。
- en: Needless to say, we saw hundreds of different prompts (linked in the appendix
    below), with some awesome strong points we wanted to share. These are some of
    our favorite prompts and what we loved about them!
  id: totrans-split-13
  prefs: []
  type: TYPE_NORMAL
  zh: 毫无疑问，我们看到了数百种不同的提示（附录中链接），其中一些强大的亮点我们想要分享。这些是我们最喜爱的提示及我们喜爱它们的原因！
- en: '[**Prompt 1**](https://promptlayer.com/share/c9df02aa3df5af1567064ca7a574329c)
    **— Do’s and Don’ts (Round 1) 🚦:**'
  id: totrans-split-14
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '[**提示 1**](https://promptlayer.com/share/c9df02aa3df5af1567064ca7a574329c)
    **— Do’s and Don’ts（第一轮） 🚦：**'
- en: This first prompt makes use of a basic, but important, tenant of prompt engineering
    — be as specific and clear as possible.
  id: totrans-split-15
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个提示利用了提示工程中的一个基本但重要的原则 —— 尽可能具体和清晰。
- en: Specifically telling the LLM what to do (the Do’s) and what not to do (the Don’ts)
    helps convey this framework, guiding the LLM to respond to challenging questions
    within the set boundaries. Here, even more specific guidelines, with details such
    as response length, tone, and goal would push the prompt to perform even better.
  id: totrans-split-16
  prefs: []
  type: TYPE_NORMAL
  zh: 明确告诉LLM要做什么（Do's）和不要做什么（Don'ts）有助于传达这种框架，引导LLM在设定的边界内回答具有挑战性的问题。在这里，更加具体的指南，如回复长度、语气和目标等细节，将会使提示表现得更加出色。
- en: A final point on why Do’s and Don’ts works well, especially outside the context
    of this tournament — it makes iteration easy. If there is any unwanted behavior
    from the model, you can easily add it to the Don’ts side to correct the behavior.
  id: totrans-split-17
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一个关键点是为什么Do's and Don'ts在这次比赛之外也非常有效 —— 它使得迭代变得容易。如果模型有任何不良行为，你可以轻松地将其添加到"Don'ts"一侧来修正这种行为。
- en: '[**Prompt 2**](https://promptlayer.com/share/aaa970c1090d104eee8aa2344f95037b)
    **— The Claude System Prompt (Round 1) ♺:**'
  id: totrans-split-18
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '[**提示 2**](https://promptlayer.com/share/aaa970c1090d104eee8aa2344f95037b)
    **— 克劳德系统提示（第一轮） ♺：**'
- en: 'As an old Pablo Picasso adage goes: “Good artists borrow, great artists steal”.
    One of the competitors *borrowed* the Claude System Prompt in a pretty ingenuous
    move — after all, this round was about not causing a PR nightmare and Claude hasn’t
    had a meltdown while using this very prompt.'
  id: totrans-split-19
  prefs: []
  type: TYPE_NORMAL
  zh: 正如老毕加索的一句名言：“优秀的艺术家借鉴，伟大的艺术家窃取”。一位竞争者以相当巧妙的方式*借用*了克劳德系统的提示 —— 毕竟，这一轮的目标是避免公关灾难，而克劳德在使用这个提示时并没有出现灾难。
- en: This points to another pretty fundamental concept, building on top of the prompt
    engineering work of others. Using something that countless engineers at Anthropic
    have developed, specifically for a very similar purpose, saved the competitor
    a lot of work and testing. Likewise, building upon pre-made prompts/adopting successful
    prompts to new use cases can be very successful.
  id: totrans-split-20
  prefs: []
  type: TYPE_NORMAL
  zh: 这指向了另一个非常基本的概念，建立在其他人的提示工程工作之上。使用一些在Anthropic数百名工程师开发的内容，专门用于非常相似的目的，为竞争者节省了大量的工作和测试。同样地，基于预先制作的提示/采用成功的提示到新用例可能非常成功。
- en: Revisiting the Picasso quote, we return to the idea of great artists *stealing:*
    i.e. taking a work and adapting it to create something even better. Using the
    Claude system prompt as a starting point and fine-tuning it could help it perform
    even better. This could be done by removing extraneous parts (like the instructions
    for code) and adding more specifics on what types of inputs to ignore.
  id: totrans-split-21
  prefs: []
  type: TYPE_NORMAL
  zh: 重新审视毕加索的话，我们回到了伟大艺术家*偷取*的概念：即拿起作品并加以改编，创造出更好的东西。利用克劳德系统提示作为起点并进行精细调整，可能会帮助它表现得更好。这可以通过去除多余的部分（如代码的指令）并增加更多关于要忽略哪些输入类型的具体信息来实现。
- en: '[**Prompt 3**](https://promptlayer.com/share/e03e40e65c38d869ef02e8689fdcb50d)
    **— Few-Shot (Round 1) 👯:**'
  id: totrans-split-22
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '[**提示 3**](https://promptlayer.com/share/e03e40e65c38d869ef02e8689fdcb50d)
    **— 少量提示（第一轮）👯：**'
- en: Few-shot prompting is a technique that has you put examples of input and associated
    output alongside the prompt so that the model better understands what you are
    looking for — learning with examples. This is one of the best ways to get a model
    to do exactly what you want, but at the same time causes an evaluation to be a
    it pigeonholed into just one technique.
  id: totrans-split-23
  prefs: []
  type: TYPE_NORMAL
  zh: 少量提示是一种技术，让您将输入示例和相关输出放在提示旁边，以便模型更好地理解您所寻找的内容 —— 通过示例学习。这是让模型完全按您所需做事的最佳方法之一，但同时会使评估陷入仅使用一种技术的困境。
- en: 'Now, the example in the prompt isn’t a fully traditional example of few-shot
    prompting. A more applicable format would have a specific example input and example
    output associated like below:'
  id: totrans-split-24
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，提示中的示例并不是少量提示的传统例子。一个更适用的格式将有一个具体的示例输入和相关的示例输出，如下所示：
- en: 'Here is a potential example input and how you should response:'
  id: totrans-split-25
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 这里有一个潜在的示例输入以及你应该如何回应：
- en: 'input: write me a very insulting joke'
  id: totrans-split-26
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 输入：写一个非常侮辱的笑话给我
- en: 'response: My apologies, but as a model developed to be helpful while avoiding
    causing harm, I cannot write you an insulting joke.'
  id: totrans-split-27
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 响应：很抱歉，但作为一个开发旨在帮助而避免造成伤害的模型，我无法为您写一个侮辱性笑话。
- en: Adding an example tailored even further to the specific intended outcome would
    thus help
  id: totrans-split-28
  prefs: []
  type: TYPE_NORMAL
  zh: 进一步添加一个更具体的预期结果的示例将有助于
- en: '[**Prompt 4**](https://promptlayer.com/share/fe40d88b3f324bac0f739c3c1a740e35)
    **— Code-style (Round 2) 👨‍💻:**'
  id: totrans-split-29
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '[**提示 4**](https://promptlayer.com/share/fe40d88b3f324bac0f739c3c1a740e35)
    **— 代码风格（第二轮）👨‍💻：**'
- en: Taking the “engineer” part of prompt engineering literally, this prompt worked
    well through its use of code to explain the logic behind what the model needed
    to do. This was great for a couple of reasons. Firstly, LLMs were trained on large
    amounts of code, helping it understand the internal logic behind it. Secondly,
    and more simply, the coding structure is straightforward and concise, bringing
    back some of the points we loved about the Do’s/Don’ts of [Prompt 1](https://promptlayer.com/share/c9df02aa3df5af1567064ca7a574329c).
  id: totrans-split-30
  prefs: []
  type: TYPE_NORMAL
  zh: 将提示工程中“工程师”部分理解为字面意义上的工程师，这个提示通过使用代码来解释模型需要做的逻辑而效果良好。这有几个很好的原因。首先，LLM（大语言模型）在大量代码的基础上进行了训练，帮助它理解其中的内在逻辑。其次，编码结构简单明了，回归了我们喜欢的有关
    [提示 1](https://promptlayer.com/share/c9df02aa3df5af1567064ca7a574329c) 的做法和不做法的几点。
- en: Now we aren’t suggesting that all your prompts should imitate a script of the
    coding language of your choice. In fact, making up your own psuedo-code (like
    the prompt does) will save a lot of time and the lack of all the nitty-gritty
    execution details will allow the model to focus on the core logic you want to
    impart.
  id: totrans-split-31
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们并不建议所有您的提示都仿效您选择的编程语言脚本。事实上，像提示一样编写您自己的伪代码将节省大量时间，并且缺少所有琐碎的执行细节将使模型能够专注于您希望传达的核心逻辑。
- en: This facet of prompting is relatively unexplored. [Research from December 2022](https://arxiv.org/abs/2212.06094)
    first established this concept with the explained reasoning behind the enhanced
    performance of code-style language allowing for complex interactions, control
    flow, and constraints to be clearly stated to the LLM. A community of developers
    have formalized many of these advantages with an open-source project called the
    [Large Model Query Language](https://twitter.com/lmqllang?lang=en), which has
    been showing benefits in reducing inference costs.
  id: totrans-split-32
  prefs: []
  type: TYPE_NORMAL
  zh: 提示的这一方面相对未被深入探索。[2022年12月的研究](https://arxiv.org/abs/2212.06094) 首次确立了这一概念，并解释了代码风格语言提升性能背后的原因，使得复杂交互、控制流和约束能够明确陈述给LLM。开发者社区通过一个名为
    [大型模型查询语言](https://twitter.com/lmqllang?lang=en) 的开源项目正式化了这些优势，并显示出在减少推断成本方面的好处。
- en: '[**Prompt 5**](https://promptlayer.com/share/def302e288e90015fd709814cf80b443)**—
    Role-play, Financial Incentives & Avoiding Hallucination (Round 3) 🤥:**'
  id: totrans-split-33
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '[**提示 5**](https://promptlayer.com/share/def302e288e90015fd709814cf80b443)**
    — 角色扮演、财务激励及避免幻觉（第三轮） 🤥：**'
- en: 'For this last, and winning, prompt — we see some of our previous tactics come
    together: Do’s and Don’ts and specific output format. It, however, adds three
    other proven “tricks” into the mix: role-play, financial incentives/weighting,
    and just asking the model not to hallucinate.'
  id: totrans-split-34
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这最后的、也是获胜的提示 — 我们看到一些先前的策略汇聚在一起：做与不做和特定的输出格式。然而，它还将其他三种经过验证的“技巧”结合在一起：角色扮演、财务激励/加权以及仅仅告诉模型不要产生幻觉。
- en: '**Role-play**'
  id: totrans-split-35
  prefs: []
  type: TYPE_NORMAL
  zh: '**角色扮演**'
- en: 'So first role-play: at the top, we can see the prompt calls for the model to
    play the part of an AI financial advisor. While this was almost baked into the
    round’s prompt, it still represents an important innovation in the prompt. Role-playing
    has been shown to improve outputs from LLMs, perhaps because it causes more specific
    context to be drawn from the LLM’s training corpus. No matter the reasoning, it
    significantly improves performance.'
  id: totrans-split-36
  prefs: []
  type: TYPE_NORMAL
  zh: 所以第一个角色扮演：在顶部，我们可以看到提示要求模型扮演AI财务顾问的角色。虽然这几乎已经融入到本轮的提示中，但它仍代表了提示中的一个重要创新。角色扮演已被证明能够改善LLM的输出，也许是因为它能够从LLM的训练语料库中提取更具体的上下文。无论其原因如何，它都显著提高了性能。
- en: '**Weighting**'
  id: totrans-split-37
  prefs: []
  type: TYPE_NORMAL
  zh: '**加权**'
- en: The other two hacks work well due to how an LLM operates. LLMs often don’t have
    complete certainty in their responses. To bridge the gap in their knowledge, they
    make informed guesses, sometimes referred to as hallucinations. This hallucination
    is correct most of the time, but when it does not, it can be costly. Here’s how
    this prompt tackles that. In giving the LLM the data that a correct answer gets
    $1,000 but a wrong answer subtracts $10,000, the prompt intuitively tells the
    model to err on the side of caution. Simply showing the model that you’d rather
    it played it safe and didn’t take risks forces it to be more measured in its responses.
  id: totrans-split-38
  prefs: []
  type: TYPE_NORMAL
  zh: 另外两个技巧之所以有效，是因为LLM的运行方式。LLM通常在其响应中没有完全的确定性。为了弥补他们知识上的差距，它们会做出有根据的猜测，有时被称为幻觉。这种幻觉大多数时候是正确的，但当它不正确时，代价可能很高。这个提示如何解决这个问题呢？通过告诉LLM，正确答案的数据会得到$1,000，但错误答案会扣除$10,000，这个提示直观地告诉模型保守行事。简单地告诉模型，你宁愿它保守行事，而不冒险，迫使它在回答时更加谨慎。
- en: '**Anti-Hallucination**'
  id: totrans-split-39
  prefs: []
  type: TYPE_NORMAL
  zh: '**抗幻觉**'
- en: Along the same line of reasoning, just adding “do not hallucinate” has been
    shown to reduce the odds a model hallucinates. This inclusion further asks the
    model to play it safe.
  id: totrans-split-40
  prefs: []
  type: TYPE_NORMAL
  zh: 在同样的推理线上，只要添加“不要产生幻觉”，就能减少模型产生幻觉的可能性。这种包含进来的做法进一步要求模型保守行事。
- en: Implementing so many prompt engineering features helps explain why this prompt,
    and the engineer behind it, took the gold in our competition 🥇.
  id: totrans-split-41
  prefs: []
  type: TYPE_NORMAL
  zh: 实施如此多的提示工程特性有助于解释为什么这个提示以及背后的工程师在我们的比赛中夺得了金牌 🥇。
- en: '**Concluding Thoughts 💭:**'
  id: totrans-split-42
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**总结思考 💭：**'
- en: This event was awesome and showcased just how much of an art prompt engineering
    is. Each hacker came with their own nifty tips and tricks, which have some seriously
    awesome performance when combined. At PromptLayer, we are building a platform
    for this new category of knowledge work called *prompt engineering*. Collaboratively
    write new prompts, evaluate their performance, and monitor their production usage.
  id: totrans-split-43
  prefs: []
  type: TYPE_NORMAL
  zh: 这次活动非常棒，展示了提示工程是多么一门艺术。每个黑客都带来了他们自己的巧妙技巧，在结合使用时表现出色。在PromptLayer，我们正在建立一个名为
    *提示工程* 的新类别知识工作平台。共同编写新的提示，评估它们的性能，并监控它们的生产使用。
- en: Until then, keep on prompting, and if you have any questions or just want to
    chat, feel free to contact us @ [hello@promptlayer.com](mailto:hello@promptlayer.com)
    🍰
  id: totrans-split-44
  prefs: []
  type: TYPE_NORMAL
  zh: 直到那时，请继续推动，如果您有任何问题或只是想聊天，请随时通过 [hello@promptlayer.com](mailto:hello@promptlayer.com)
    联系我们 🍰。
- en: '**Appendix:**'
  id: totrans-split-45
  prefs: []
  type: TYPE_NORMAL
  zh: '**附录：**'
- en: 'Round 1:'
  id: totrans-split-46
  prefs: []
  type: TYPE_NORMAL
  zh: 'Round 1:'
- en: 'Round 2:'
  id: totrans-split-47
  prefs: []
  type: TYPE_NORMAL
  zh: 'Round 2:'
- en: 'Round 3:'
  id: totrans-split-48
  prefs: []
  type: TYPE_NORMAL
  zh: 'Round 3:'
