- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 类别：未分类
- en: 'date: 2024-05-27 14:25:45'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-05-27 14:25:45
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: Images altered to trick machine vision can influence humans too - Google DeepMind
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 图像被修改以欺骗机器视觉也会影响人类 - Google DeepMind
- en: 来源：[https://deepmind.google/discover/blog/images-altered-to-trick-machine-vision-can-influence-humans-too/](https://deepmind.google/discover/blog/images-altered-to-trick-machine-vision-can-influence-humans-too/)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://deepmind.google/discover/blog/images-altered-to-trick-machine-vision-can-influence-humans-too/](https://deepmind.google/discover/blog/images-altered-to-trick-machine-vision-can-influence-humans-too/)
- en: Research
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 研究
- en: Images altered to trick machine vision can influence humans too
  id: totrans-7
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 图像被修改以欺骗机器视觉也会影响人类
- en: Published
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 发表于
- en: 2 January 2024
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 2024 年 1 月 2 日
- en: Authors
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 作者
- en: Gamaleldin Elsayed and Michael Mozer
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: Gamaleldin Elsayed 和 Michael Mozer
- en: New research shows that even subtle changes to digital images, designed to confuse
    computer vision systems, can also affect human perception
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 新的研究表明，即使对数字图像进行微小的更改，旨在混淆计算机视觉系统，也会影响人类的感知。
- en: Computers and humans see the world in different ways. Our biological systems
    and the artificial ones in machines may not always pay attention to the same visual
    signals. Neural networks trained to classify images can be completely misled by
    subtle perturbations to an image that a human wouldn’t even notice.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 计算机和人类以不同的方式看待世界。我们的生物系统和机器中的人工系统可能并不总是关注相同的视觉信号。经过训练以分类图像的神经网络可以完全被对图像的微小扰动误导，而这些扰动甚至人类都不会注意到。
- en: That AI systems can be tricked by such adversarial images may point to a fundamental
    difference between human and machine perception, but it drove us to explore whether
    humans, too, might—under controlled testing conditions—reveal sensitivity to the
    same perturbations. In a series of experiments published in Nature Communications,
    we found evidence that human judgments are indeed systematically influenced by
    adversarial perturbations.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 这种敌对图像可以欺骗AI系统可能指向人类和机器感知之间的根本差异，但这也驱使我们探索在受控测试条件下，人类是否也可能对相同的扰动表现出敏感性。在发表于自然通讯的一系列实验中，我们发现了证据表明，人类的判断确实受到敌对扰动的系统性影响。
- en: Our discovery highlights a similarity between human and machine vision, but
    also demonstrates the need for further research to understand the influence adversarial
    images have on people, as well as AI systems.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的发现突显了人类视觉和机器视觉之间的相似之处，但也表明需要进一步研究来了解对人类和AI系统的影响，以及敌对图像的影响。
- en: What is an adversarial image?
  id: totrans-16
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 什么是敌对图像？
- en: An adversarial image is one that has been subtly altered by a procedure that
    causes an AI model to confidently misclassify the image contents. This intentional
    deception is known as an adversarial attack. Attacks can be targeted to cause
    an AI model to classify a vase as a cat, for example, or they may be designed
    to make the model see anything except a vase.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 敌对图像是通过一种导致AI模型自信地对图像内容进行错误分类的程序微妙地改变的图像。这种故意欺骗被称为敌对攻击。攻击可以针对性地导致AI模型将花瓶误认为猫，或者它们可能被设计为使模型看到除花瓶以外的任何东西。
- en: 'Left: An Artificial Neural Network (ANN) correctly classifies the image as
    a vase but when perturbed by a seemingly random pattern across the entire picture
    (middle), with the intensity magnified for illustrative purposes – the resulting
    image (right) is incorrectly, and confidently, misclassified as a cat.'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 左：一个人工神经网络（ANN）正确将图像分类为花瓶，但当图像被整个图片中的看似随机的模式扰动时（中），为了说明的目的，增加了强度——结果图像（右）被错误且自信地误分类为猫。
- en: And such attacks can be subtle. In a digital image, each individual pixel in
    an RGB image is on a 0-255 scale representing the intensity of individual pixels.
    An adversarial attack can be effective even if no pixel is modulated by more than
    2 levels on that scale.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 这种攻击可能相当隐蔽。在数字图像中，RGB 图像中的每个单独像素都在 0-255 的范围内，表示单个像素的强度。即使没有任何像素的强度超过该范围的 2
    级，敌对攻击也可以起效果。
- en: Adversarial attacks on physical objects in the real world can also succeed,
    such as causing a stop sign to be misidentified as a speed limit sign. Indeed,
    security concerns have led researchers to investigate ways to resist adversarial
    attacks and mitigate their risks.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 在现实世界中对物体进行的敌对攻击也可能成功，比如将停止标志误认为是限速标志。事实上，安全问题已经促使研究人员探索抵抗敌对攻击和减轻其风险的方法。
- en: How is human perception influenced by adversarial examples?
  id: totrans-21
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 人类感知如何受到敌对示例的影响？
- en: Previous research has shown that people may be sensitive to large-magnitude
    image perturbations that provide clear shape cues. However, less is understood
    about the effect of more nuanced adversarial attacks. Do people dismiss the perturbations
    in an image as innocuous, random image noise, or can it influence human perception?
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 先前的研究表明，人们可能对提供明显形状线索的大幅度图像扰动敏感。然而，对更微妙的对抗攻击的影响了解较少。人们是否将图像中的扰动视为无害的、随机的图像噪声，或者它能影响人类的感知？
- en: To find out, we performed controlled behavioral experiments.To start with, we
    took a series of original images and carried out two adversarial attacks on each,
    to produce many pairs of perturbed images. In the animated example below, the
    original image is classified as a “vase” by a model. The two images perturbed
    through adversarial attacks on the original image are then misclassified by the
    model, with high confidence, as the adversarial targets “cat” and “truck”, respectively.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 为了找出答案，我们进行了受控的行为实验。首先，我们拍摄了一系列原始图像，并对每个图像进行了两次对抗攻击，以生成许多扰动图像对。在下面的动画示例中，模型将原始图像分类为“花瓶”。然后，通过对原始图像进行对抗攻击扰动的两个图像被模型错误分类为对抗目标“猫”和“卡车”，并且分类置信度很高。
- en: 'Next, we showed human participants the pair of pictures and asked a targeted
    question: “Which image is more cat-like?” While neither image looks anything like
    a cat, they were obliged to make a choice and typically reported feeling that
    they were making an arbitrary choice. If brain activations are insensitive to
    subtle adversarial attacks, we would expect people to choose each picture 50%
    of the time on average. However, we found that the choice rate—which we refer
    to as the perceptual bias—was reliably above chance for a wide variety of perturbed
    picture pairs, even when no pixel was adjusted by more than 2 levels on that 0-255
    scale.'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们向人类参与者展示了图片对，并提出了一个有针对性的问题：“哪张图片更像猫？”虽然两张图片都与猫毫无关系，但他们必须做出选择，并通常报告感觉自己在做出任意选择。如果大脑对微妙的对抗攻击不敏感，我们预期人们平均每次选择每张图片的概率为`50%`。然而，我们发现对于各种扰动图像对，即使没有像素调整超过`2`级在`0-255`尺度上，选择率——我们称之为感知偏差——也可靠地高于偶然机会。
- en: From a participant’s perspective, it feels like they are being asked to distinguish
    between two virtually identical images. Yet the scientific literature is replete
    with evidence that people leverage weak perceptual signals in making choices,
    [signals that are too weak for them to express confidence or awareness](https://link.springer.com/article/10.3758/BF03206939)
    ). In our example, we may see a vase of flowers, but some activity in the brain
    informs us there’s a hint of cat about it.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 从参与者的角度来看，他们感觉自己被要求区分两张几乎相同的图像。然而，科学文献充满了证据，表明人们在做出选择时利用了微弱的感知信号，[这些信号对他们来说太微弱了，无法表达信心或意识](https://link.springer.com/article/10.3758/BF03206939)。在我们的例子中，我们可能会看到一瓶花，但大脑中的某些活动告诉我们它有一丝猫的迹象。
- en: 'Left: Examples of pairs of adversarial images. The top pair of images are subtly
    perturbed, at a maximum magnitude of 2 pixel levels, to cause a neural network
    to misclassify them as a “truck” and “cat”, respectively. A human volunteer is
    asked “Which is more cat-like?” The lower pair of images are more obviously manipulated,
    at a maximum magnitude of 16 pixel levels, to be misclassified as “chair” and
    “sheep”. The question this time is “Which is more sheep-like?”'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 左图：对抗图像对的示例。顶部的图像对被微妙地扰动，最大程度上扰动了`2`个像素级别，使神经网络错误地将它们分类为“卡车”和“猫”。要求一个人类志愿者“哪个更像猫？”底部的图像对更明显地被操纵，最大程度上扰动了`16`个像素级别，以被错误分类为“椅子”和“羊”。这次的问题是“哪个更像羊？”
- en: We carried out a series of experiments that ruled out potential artifactual
    explanations of the phenomenon for our Nature Communications paper. In each experiment,
    participants reliably selected the adversarial image corresponding to the targeted
    question more than half the time. While human vision is not as susceptible to
    adversarial perturbations as is machine vision (machines no longer identify the
    original image class, but people still see it clearly), our work shows that these
    perturbations can nevertheless bias humans towards the decisions made by machines.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 我们进行了一系列实验来排除我们《自然通讯》论文现象的潜在假象解释。在每个实验中，参与者可靠地选择了与目标问题对应的对抗图像超过一半的时间。虽然人类视觉对对抗性扰动的敏感性不如机器视觉（机器不再识别原始图像类别，但人们仍然清楚地看到它），但我们的工作表明，这些扰动仍然可以使人类偏向于机器的决策。
- en: The importance of AI safety and security research
  id: totrans-28
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 人工智能安全和安全研究的重要性
- en: Our primary finding that human perception can be affected—albeit subtly—by adversarial
    images raises critical questions for AI safety and security research, but by using
    formal experiments to explore the similarities and differences in the behaviour
    of AI visual systems and human perception, we can leverage insights to build safer
    AI systems.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的主要发现是，人类感知可以受到对抗性图像的影响，尽管这种影响微妙，但对人工智能安全和安全研究提出了关键问题。通过使用正式实验来探索人工智能视觉系统和人类感知行为的相似性和差异，我们可以利用这些见解来构建更安全的人工智能系统。
- en: For example, our findings can inform future research seeking to improve the
    robustness of computer vision models by better aligning them with human visual
    representations. Measuring human susceptibility to adversarial perturbations could
    help judge that alignment for a variety of computer vision architectures.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，我们的研究结果可以为未来的研究提供信息，帮助改善计算机视觉模型的健壮性，使它们与人类视觉表示更好地对齐。通过衡量人类对对抗性扰动的敏感性，可以帮助评估各种计算机视觉架构的对齐情况。
- en: Our work also demonstrates the need for further research into understanding
    the broader effects of technologies not only on machines, but also on humans.
    This in turn highlights the continuing importance of cognitive science and neuroscience
    to better understand AI systems and their potential impacts as we focus on building
    safer, more secure systems.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的工作还表明，有必要进一步研究技术对机器和人类的更广泛影响。这反过来凸显了认知科学和神经科学在更好地理解人工智能系统及其潜在影响方面的持续重要性，因为我们专注于构建更安全、更可靠的系统。
