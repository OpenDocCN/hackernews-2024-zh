- en: <!--yml
  id: totrans-split-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-split-1
  prefs: []
  type: TYPE_NORMAL
  zh: 'category: 未分类'
- en: 'date: 2024-05-27 14:52:10'
  id: totrans-split-2
  prefs: []
  type: TYPE_NORMAL
  zh: 'date: 2024-05-27 14:52:10'
- en: -->
  id: totrans-split-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: Sam Altman Wants $7 Trillion - by Scott Alexander
  id: totrans-split-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 《Sam Altman Wants $7 Trillion - by Scott Alexander》
- en: 来源：[https://www.astralcodexten.com/p/sam-altman-wants-7-trillion](https://www.astralcodexten.com/p/sam-altman-wants-7-trillion)
  id: totrans-split-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://www.astralcodexten.com/p/sam-altman-wants-7-trillion](https://www.astralcodexten.com/p/sam-altman-wants-7-trillion)
- en: '*[All numbers here are very rough and presented in a sloppy way. For the more
    rigorous versions of this, read [Tom Davidson](https://www.astralcodexten.com/p/davidson-on-takeoff-speeds),
    [Yafah Edelman](https://www.lesswrong.com/posts/nXcHe7t4rqHMjhzau/report-on-frontier-model-training),
    and [EpochAI](https://epochai.org/blog/trends-in-the-dollar-training-cost-of-machine-learning-systems))*'
  id: totrans-split-6
  prefs: []
  type: TYPE_NORMAL
  zh: '*[这里的所有数字都非常粗略，并且以松散的方式呈现。想要更严谨版本的，请阅读[Tom Davidson](https://www.astralcodexten.com/p/davidson-on-takeoff-speeds)，[Yafah
    Edelman](https://www.lesswrong.com/posts/nXcHe7t4rqHMjhzau/report-on-frontier-model-training)，和[EpochAI](https://epochai.org/blog/trends-in-the-dollar-training-cost-of-machine-learning-systems))*]'
- en: '**I.**'
  id: totrans-split-7
  prefs: []
  type: TYPE_NORMAL
  zh: '**I.**'
- en: Sam Altman [wants $7 trillion](https://www.cnbc.com/2024/02/09/openai-ceo-sam-altman-reportedly-seeking-trillions-of-dollars-for-ai-chip-project.html).
  id: totrans-split-8
  prefs: []
  type: TYPE_NORMAL
  zh: Sam Altman [想要7万亿美元](https://www.cnbc.com/2024/02/09/openai-ceo-sam-altman-reportedly-seeking-trillions-of-dollars-for-ai-chip-project.html)。
- en: In one sense, this isn’t news. Everyone wants $7 trillion. I want $7 trillion.
    I’m not going to get it, and Sam Altman probably won’t either.
  id: totrans-split-9
  prefs: []
  type: TYPE_NORMAL
  zh: 从某种意义上说，这不是新闻。每个人都想要7万亿美元。我也想要7万亿美元。我得不到，Sam Altman可能也不会得到。
- en: Still, the media treats this as worthy of comment, and I agree. It’s a useful
    reminder of what it will take for AI to scale in the coming years.
  id: totrans-split-10
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管如此，媒体把这件事当作值得评论的事情，我也同意。这是一个有用的提醒，AI在未来几年要扩展需要什么。
- en: 'The basic logic: GPT-1 cost approximately nothing to train. [GPT-2](https://blog.dataiku.com/pre-trained-models-ais-object-oriented-programming)
    cost $40,000\. [GPT-3](https://www.pcguide.com/apps/gpt-3-cost/) cost $4 million.
    [GPT-4](https://en.wikipedia.org/wiki/GPT-4#Training) cost $100 million. Details
    about GPT-5 are still secret, but [one extremely unreliable estimate](https://mpost.io/gpt-5-training-will-cost-2-5-billion-and-start-next-year/)
    says $2.5 billion, and this seems the right order of magnitude given the $8 billion
    that Microsoft gave OpenAI.'
  id: totrans-split-11
  prefs: []
  type: TYPE_NORMAL
  zh: 基本逻辑：GPT-1训练成本几乎为零。[GPT-2](https://blog.dataiku.com/pre-trained-models-ais-object-oriented-programming)训练成本为4万美元。[GPT-3](https://www.pcguide.com/apps/gpt-3-cost/)训练成本为400万美元。[GPT-4](https://en.wikipedia.org/wiki/GPT-4#Training)训练成本为1亿美元。关于GPT-5的细节仍然是秘密，但[一个极不可靠的估计](https://mpost.io/gpt-5-training-will-cost-2-5-billion-and-start-next-year/)说是25亿美元，这似乎是正确数量级，考虑到微软给OpenAI的8亿美元。
- en: So each GPT costs between 25x and 100x the last one. Let’s say 30x on average.
    That means we can expect GPT-6 to cost $75 billion, and GPT-7 to cost $2 trillion.
  id: totrans-split-12
  prefs: []
  type: TYPE_NORMAL
  zh: 所以每个GPT的成本是上一个的25倍到100倍。我们平均说是30倍。这意味着我们可以预期GPT-6的成本为750亿美元，GPT-7的成本为2万亿美元。
- en: (Unless they slap the name “GPT-6” on a model that isn’t a full generation ahead
    of GPT-5\. Consider these numbers to represent models that are eg as far ahead
    of GPT-4 as GPT-4 was to GPT-3, regardless of how they brand them.)
  id: totrans-split-13
  prefs: []
  type: TYPE_NORMAL
  zh: （除非他们在一个并非完全领先于GPT-5的模型上贴上“GPT-6”的名字。考虑这些数字代表的模型，它们可能与GPT-4相比如同GPT-4与GPT-3的距离一样远，无论它们如何标榜。）
- en: 'Let’s try to break that cost down. In a very abstract sense, training an AI
    takes three things:'
  id: totrans-split-14
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们试着分解这个成本。在一个非常抽象的意义上，训练一个AI需要三个要素：
- en: Compute (ie computing power, hardware, chips)
  id: totrans-split-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 计算（即计算能力、硬件、芯片）
- en: Electricity (to power the compute)
  id: totrans-split-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 电力（用于计算）
- en: Training data
  id: totrans-split-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练数据
- en: '*Compute*'
  id: totrans-split-18
  prefs: []
  type: TYPE_NORMAL
  zh: '*计算*'
- en: Compute is measured in floating point operations (FLOPs). GPT-3 took [10^23](https://airtable.com/appDFXXgaG1xLtXGL/shrBucz1oynb4AUab/tblhmFk3gP7psWh3C?backgroundColor=cyanDusty&viewControls=on)
    FLOPs to train, and GPT-4 plausibly [10^25](https://www.kdnuggets.com/2023/07/gpt4-details-leaked.html).
  id: totrans-split-19
  prefs: []
  type: TYPE_NORMAL
  zh: 计算使用浮点运算（FLOPs）来衡量。GPT-3训练使用了[10^23](https://airtable.com/appDFXXgaG1xLtXGL/shrBucz1oynb4AUab/tblhmFk3gP7psWh3C?backgroundColor=cyanDusty&viewControls=on)
    FLOPs，而GPT-4可能使用了[10^25](https://www.kdnuggets.com/2023/07/gpt4-details-leaked.html)。
- en: The capacity of all the computers in the world is about [10^21](https://wiki.aiimpacts.org/ai_timelines/hardware_and_ai_timelines/computing_capacity_of_all_gpus_and_tpus)
    FLOP/second, so they could train GPT-4 in 10^4 seconds (ie two hours). Since OpenAI
    has fewer than all the computers in the world, it took them six months. This suggests
    OpenAI was using about 1/2000th of all the computers in the world during that
    time.
  id: totrans-split-20
  prefs: []
  type: TYPE_NORMAL
  zh: 全球所有计算机的容量约为[10^21](https://wiki.aiimpacts.org/ai_timelines/hardware_and_ai_timelines/computing_capacity_of_all_gpus_and_tpus)
    FLOP/秒，因此它们可以在10^4秒内（即两小时内）训练GPT-4。由于OpenAI的计算机并不多，他们花了六个月的时间。这表明在那段时间内，OpenAI使用了全球计算机的约1/2000。
- en: If we keep our 30x scaling factor, GPT-5 will take 1/70th of all the computers
    in the world, GPT-6 will take 1/2, and GPT-7 will take 15x as many computers as
    exist. The computing capacity of the world grows quickly - [this source](https://www.metaculus.com/notebooks/10688/how-much-of-ai-progress-is-from-scaling-compute-and-how-far-will-it-scale/)
    says it doubles every 1.5 years, which means it grows by an order of magnitude
    every five years, which means these numbers are probably overestimates. If we
    imagine five years between GPTs, then GPT-6 will actually only need 1/10th of
    the world’s computers, and GPT-7 will only need 1/3\. Still, 1/3 of the world’s
    computers is a lot.
  id: totrans-split-21
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们保持30倍的缩放系数，GPT-5将占据全球计算机的1/70，GPT-6将占据1/2，而GPT-7将占据现有计算机的15倍。世界的计算能力增长很快
    - [此来源](https://www.metaculus.com/notebooks/10688/how-much-of-ai-progress-is-from-scaling-compute-and-how-far-will-it-scale/)称其每1.5年翻一番，这意味着每五年增长一个数量级，这意味着这些数字可能是过高估计。如果我们假设每个GPT之间相隔五年，那么GPT-6实际上只需全球计算机的1/10，而GPT-7只需全球计算机的1/3。但是，全球计算机的1/3仍然是相当多的。
- en: Probably you can’t get 1/3 of the world’s computers, especially when all the
    other AI companies want them too. You would need to vastly scale up chip manufacturing.
  id: totrans-split-22
  prefs: []
  type: TYPE_NORMAL
  zh: 可能你无法得到全球计算机的1/3，特别是当所有其他AI公司也想要它们时。你需要大幅扩展芯片制造。
- en: '*Energy*'
  id: totrans-split-23
  prefs: []
  type: TYPE_NORMAL
  zh: '*能源*'
- en: GPT-4 took about [50 gigawatt-hours](https://www.ri.se/en/news/blog/generative-ai-does-not-run-on-thin-air)
    of energy to train. Using our scaling factor of 30x, we expect GPT-5 to need 1,500,
    GPT-6 to need 45,000, and GPT-7 to need 1.3 million.
  id: totrans-split-24
  prefs: []
  type: TYPE_NORMAL
  zh: GPT-4的训练消耗了[50 gigawatt-hours](https://www.ri.se/en/news/blog/generative-ai-does-not-run-on-thin-air)的能源。按我们的30倍缩放系数，我们预计GPT-5将需要1,500，GPT-6将需要45,000，而GPT-7将需要1.3百万。
- en: Let’s say the training run lasts six months, ie 4,320 hours. That means GPT-6
    will need 10 GW - about half the output of the Three Gorges Dam, the biggest power
    plant in the world. GPT-7 will need fifteen Three Gorges Dams. This isn’t just
    “the world will need to produce this much power total and you can buy it”. You
    need the power pretty close to your data center. Your best bet here is either
    to get an entire pipeline like Nord Stream hooked up to your data center, or else
    a fusion reactor.
  id: totrans-split-25
  prefs: []
  type: TYPE_NORMAL
  zh: 假设训练过程持续六个月，即4,320小时。这意味着GPT-6将需要10 GW的能量 - 大约是世界上最大的三峡大坝发电量的一半。GPT-7将需要十五座三峡大坝。这不仅仅是“世界需要总产生这么多能量，你可以购买它”。你需要的能量必须非常接近你的数据中心。你最好的选择是要么像北欧石油公司那样有一整条输气管道直接连接到你的数据中心，要么选择核聚变反应堆。
- en: ([Sam Altman is working on fusion power](https://www.cnbc.com/2023/05/10/microsoft-agrees-to-buy-power-from-sam-altman-backed-helion-in-2028.html),
    but this seems to be a coincidence. At least, he’s been interested in fusion since
    at least 2016, which is way too early for him to have known about any of this.)
  id: totrans-split-26
  prefs: []
  type: TYPE_NORMAL
  zh: ([萨姆·阿尔特曼正在致力于核聚变能源](https://www.cnbc.com/2023/05/10/microsoft-agrees-to-buy-power-from-sam-altman-backed-helion-in-2028.html)，但这似乎是巧合。至少，自2016年以来，他一直对核聚变感兴趣，这对于他早于任何事情都知晓是为时过早的。)
- en: '*Training Data*'
  id: totrans-split-27
  prefs: []
  type: TYPE_NORMAL
  zh: '*训练数据*'
- en: This is the text or images or whatever that the AI reads to understand how its
    domain works. [GPT-3](https://lambdalabs.com/blog/demystifying-gpt-3) used 300
    billion tokens. [GPT-4](https://www.springboard.com/blog/data-science/machine-learning-gpt-3-open-ai/)
    used 13 trillion tokens (another source says 6 trillion). This sort of looks like
    our scaling factor of 30x still kind of holds, but in theory training data is
    supposed to scale as the square root of compute - so you should expect a scaling
    factor of 5.5x. That means GPT-5 will need somewhere in the vicinity of 50 trillion
    tokens, GPT-6 somewhere in the three-digit trillions, and GPT-7 somewhere in the
    quadrillions.
  id: totrans-split-28
  prefs: []
  type: TYPE_NORMAL
  zh: 这是AI用来理解其领域工作方式的文本或图像。[GPT-3](https://lambdalabs.com/blog/demystifying-gpt-3)使用了3000亿令牌。[GPT-4](https://www.springboard.com/blog/data-science/machine-learning-gpt-3-open-ai/)使用了13万亿令牌（另一来源称为6万亿）。这看起来我们的30倍缩放系数似乎仍然有些适用，但理论上训练数据应该与计算的平方根成比例
    - 所以你应该预期一个5.5倍的缩放系数。这意味着GPT-5将需要大约500万亿令牌，GPT-6将需要几百万亿令牌，而GPT-7将需要几千万亿令牌。
- en: There isn’t that much text in the whole world. You might be able to get a few
    trillion more by combining all published books, Facebook messages, tweets, text
    messages, and emails. You could get some more by adding in all images, videos,
    and movies, once the AIs learn to understand those. I still don’t think you’re
    getting to a hundred trillion, let alone a quadrillion.
  id: totrans-split-29
  prefs: []
  type: TYPE_NORMAL
  zh: 整个世界上没有那么多的文本。你也许可以通过结合所有已发布的书籍、Facebook消息、推文、短信和电子邮件来得到几万亿更多。一旦AI学会理解这些，你可以通过添加所有图像、视频和电影来获取更多。我仍然认为你不会达到一百万亿，更不用说一千万亿了。
- en: You could try to make an AI that can learn things with less training data. This
    ought to be possible, because the human brain learns things without reading all
    the text in the world. But this is hard and nobody has a great idea how to do
    it yet.
  id: totrans-split-30
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以尝试制造一个可以用较少训练数据学习东西的人工智能。这可能是可能的，因为人脑也可以在不阅读世界上所有的文字的情况下学习东西。但这很难，目前还没有人有很好的想法如何去做。
- en: More promising is synthetic data, where the AI generates data for itself. This
    sounds like a perpetual motion machine that won’t work, but there are tricks to
    get around this. For example, you can train a chess AI on synthetic data by making
    it play against itself a million times. You can train a math AI by having it randomly
    generate steps to a proof, eventually stumbling across a correct one by chance,
    automatically detecting the correct proof, and then training on that one. You
    can train a video game playing AI by having it make random motions, then see which
    one gets the highest score. In general you can use synthetic data when you don’t
    know how to create good data, but you do know how to recognize it once it exists
    (eg the chess AI won the game against itself, the math AI got a correct proof,
    the video game AI gets a good score). But nobody knows how to do this well for
    written text yet.
  id: totrans-split-31
  prefs: []
  type: TYPE_NORMAL
  zh: 更有希望的是合成数据，即人工智能为自己生成数据。这听起来像一个行不通的永动机，但有一些技巧可以规避这个问题。例如，你可以让一个象棋人工智能在合成数据上自己对弈一百万次。你可以让一个数学人工智能随机生成证明的步骤，最终偶然间发现了一个正确的证明，自动检测到了这个正确的证明，然后在其上进行训练。你可以训练一个玩游戏的人工智能，让它进行随机动作，然后看哪一个得分最高。总的来说，当你不知道如何创建好的数据时，你可以使用合成数据，但你确实知道一旦它存在你如何识别它（比如象棋人工智能赢得了对弈，数学人工智能得到了正确的证明，游戏人工智能获得了一个好的得分）。但目前还没有人知道如何为书面文本做到这一点。
- en: Maybe you can create a smart AI through some combination of text, chess, math,
    and video games - some humans pursue this curriculum, and it works fine for them,
    sort of.
  id: totrans-split-32
  prefs: []
  type: TYPE_NORMAL
  zh: 也许你可以通过一些文本、象棋、数学和电子游戏的组合创造一个聪明的人工智能 - 一些人类会追求这样的课程，并且对他们来说也是有效的。
- en: This is kind of the odd one out - compute and electricity can be solved with
    lots of money, but this one might take more of a breakthrough.
  id: totrans-split-33
  prefs: []
  type: TYPE_NORMAL
  zh: 这有点奇怪 - 计算和电力可以通过大量的资金解决，但这可能需要更多的突破。
- en: '*Algorithmic Progress*'
  id: totrans-split-34
  prefs: []
  type: TYPE_NORMAL
  zh: '*算法进展*'
- en: This means “people make breakthroughs and become better at building AI”. It
    seems to be another one of those things that gives an order of magnitude of progress
    per five years or so, so I’m revising the estimates above down by a little.
  id: totrans-split-35
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着“人们会突破并变得更擅长构建人工智能”。似乎这又是另一件每五年就会实现数量级进展的事情，所以我稍微降低了上面的估计。
- en: '*Putting It All Together*'
  id: totrans-split-36
  prefs: []
  type: TYPE_NORMAL
  zh: '*将其全部放在一起*'
- en: GPT-5 might need about 1% the world’s computers, a small power plant’s worth
    of energy, and a lot of training data.
  id: totrans-split-37
  prefs: []
  type: TYPE_NORMAL
  zh: GPT-5可能需要全球约1%的计算机，小型发电厂的能量，以及大量的训练数据。
- en: GPT-6 might need about 10% of the world’s computers, a large power plant’s worth
    of energy, and more training data than exists. Probably this looks like a town-sized
    data center attached to a lot of solar panels or a nuclear reactor.
  id: totrans-split-38
  prefs: []
  type: TYPE_NORMAL
  zh: GPT-6可能需要全球约10%的计算机，相当于一个大型发电厂的能量，以及更多的训练数据，甚至超过了现有的数据总和。可能看起来像是连接着许多太阳能面板或核反应堆的城镇大小的数据中心。
- en: GPT-7 might need all of the world’s computers, a gargantuan power plant beyond
    any that currently exist, and *way* more training data than exists. Probably this
    looks like a city-sized data center attached to a fusion plant.
  id: totrans-split-39
  prefs: []
  type: TYPE_NORMAL
  zh: GPT-7可能需要全球所有的计算机，比任何当前存在的巨大的发电厂更大，并且超过了现有的训练数据 *远远*。可能看起来像是连接着一个核聚变发电厂的城市大小的数据中心。
- en: Building GPT-8 is currently impossible. Even if you solve synthetic data and
    fusion power, and you take over the whole semiconductor industry, you wouldn’t
    come close. Your only hope is that GPT-7 is superintelligent and helps you with
    this, either by telling you how to build AIs for cheap, or by growing the global
    economy so much that it can fund currently-impossible things.
  id: totrans-split-40
  prefs: []
  type: TYPE_NORMAL
  zh: 目前建造GPT-8是不可能的。即使你解决了合成数据和核聚变能源，并控制了整个半导体行业，你也不会接近。你唯一的希望是GPT-7是超智能的，并帮助你完成这一点，要么告诉你如何为廉价建造人工智能，要么通过促进全球经济增长来资助目前不可能的事情。
- en: Everything about GPTs >5 is a naive projection of existing trends and probably
    false. Order of magnitude estimates only.
  id: totrans-split-41
  prefs: []
  type: TYPE_NORMAL
  zh: 所有有关GPTs >5的东西都只是对现有趋势的天真预测，可能是错误的。只是估算的数量级。
- en: You might call this “speculative” and “insane”. But if Sam Altman didn’t believe
    something at least this speculative and insane, he wouldn’t be asking for $7 trillion.
  id: totrans-split-42
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能会称这是“投机的”、“疯狂的”。但如果Sam Altman不相信至少有这种投机和疯狂的东西，他也不会要求7万亿美元。
- en: '**II.**'
  id: totrans-split-43
  prefs: []
  type: TYPE_NORMAL
  zh: '**II.**'
- en: Let’s back up.
  id: totrans-split-44
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们重新审视一下。
- en: GPT-6 will probably cost $75 billion or more. OpenAI can’t afford this. Microsoft
    or Google could afford it, but it would take a significant fraction (maybe half?)
    of company resources.
  id: totrans-split-45
  prefs: []
  type: TYPE_NORMAL
  zh: GPT-6 可能会花费 750 亿美元或更多。OpenAI 无法承担这个费用。微软或谷歌可能能负担得起，但这将占用公司资源的重要部分（也许一半？）。
- en: If GPT-5 fails, or is only an incremental improvement, nobody will want to spend
    $75 billion making GPT-6, and all of this will be moot.
  id: totrans-split-46
  prefs: []
  type: TYPE_NORMAL
  zh: 如果 GPT-5 失败，或者只是渐进改进，没有人愿意花 750 亿美元制造 GPT-6，所有这些都将无所谓。
- en: On the other hand, if GPT-5 is close to human-level, and revolutionizes entire
    industries, and seems poised to start an Industrial-Revolution-level change in
    human affairs, then $75 billion for the next one will seem like a bargain.
  id: totrans-split-47
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，如果 GPT-5 接近人类水平，并且彻底改变了整个行业，并且似乎即将引发人类事务的工业革命级变化，那么 750 亿美元用于下一代将会显得很划算。
- en: Also, if you’re starting an Industrial Revolution level change in human affairs,
    maybe things get cheaper. I don’t expect GPT-5 to be good enough that it can make
    a big contribution to planning for GPT-6\. But you’ve got to think of this stepwise.
    Can it do enough stuff that large projects (like GPT-6, or its associated chip
    fabs, or its associated power plants) get 10% cheaper? Maybe.
  id: totrans-split-48
  prefs: []
  type: TYPE_NORMAL
  zh: 另外，如果你正在引发人类事务的工业革命级变化，也许事情会变得更便宜。我不指望 GPT-5 足够优秀以至于能够为 GPT-6 的规划做出重大贡献。但你必须逐步考虑这一点。它能做足够的事情使得大型项目（如
    GPT-6、其相关的芯片工厂或相关的发电厂）便宜 10% 吗？也许可以。
- en: The upshot of this is that we’re looking at an exponential process, like R for
    a pandemic. If the exponent is > 1, it gets very big very quickly. If the exponent
    is < 1, it fizzles out.
  id: totrans-split-49
  prefs: []
  type: TYPE_NORMAL
  zh: 这的要点是我们正在看一个指数过程，就像流行病的 R 值一样。如果指数大于1，它会迅速增长。如果指数小于1，它就会逐渐消退。
- en: In this case, if each new generation of AI is exciting enough to inspire more
    investment, *and/or* smart enough to decrease the cost of the next generation,
    then these two factors combined allow the creation of another generation of AIs
    in a positive feedback loop (R > 1).
  id: totrans-split-50
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，如果每一代新的人工智能足够令人兴奋以激发更多的投资，并且/或者足够聪明以降低下一代的成本，那么这两个因素的结合允许在正反馈环路中创建另一代人工智能。
- en: But if each new generation of AI isn’t exciting enough to inspire the massive
    investment required to create the next one, and isn’t smart enough to help bring
    down the price of the next generation on its own, then at some point nobody is
    willing to fund more advanced AIs, and the current AI boom fizzles out (R < 1).
    This doesn’t mean you never hear about AI - people will probably generate amazing
    AI art and videos and androids and girlfriends and murderbots. It just means that
    raw intelligence of the biggest models won’t increase as quickly.
  id: totrans-split-51
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，如果每一代新的人工智能都不足以激发投入下一代所需的巨额投资，并且也不足够聪明以帮助降低下一代的价格，那么在某个时刻，没有人愿意资助更先进的人工智能，当前的人工智能繁荣就会消退（R
    < 1）。这并不意味着你再也听不到有关人工智能的消息 - 人们可能会生成令人惊叹的人工智能艺术作品、视频、机器人和机器人女友。这只是意味着最大模型的原始智能增长速度不会那么快。
- en: Even when R < 1, we still get the bigger models eventually. Chip factories can
    gradually churn out more chips. Researchers can gradually churn out more algorithmic
    breakthroughs. If nothing else, you can spend ten years training GPT-7 very slowly.
    It just means we get human or above-human level AI in the mid-21st century, instead
    of the early part.
  id: totrans-split-52
  prefs: []
  type: TYPE_NORMAL
  zh: 即使 R < 1，我们最终仍然会得到更大的模型。芯片工厂可以逐步生产更多的芯片。研究人员可以逐步推出更多的算法突破。如果没有其他选择，你可以花十年慢慢地训练
    GPT-7。这意味着我们在21世纪中叶会得到人类或超越人类水平的人工智能，而不是早期。
- en: '**III.**'
  id: totrans-split-53
  prefs: []
  type: TYPE_NORMAL
  zh: '**III.**'
- en: When Sam Altman asks for $7 trillion, I interpret him as wanting to do this
    process in a centralized, quick, efficient way. One guy builds the chip factories
    and power plants and has them all nice and ready by the time he needs to train
    the next big model.
  id: totrans-split-54
  prefs: []
  type: TYPE_NORMAL
  zh: 当 Sam Altman 要求 7 万亿美元时，我理解他希望通过集中、快速、高效的方式进行此过程。一个人建造芯片工厂和发电厂，并在他需要训练下一个大模型时都准备就绪。
- en: Probably he won’t get his $7 trillion. Then this same process will happen, but
    slower, more piecemeal, and more decentralized. They’ll come out with GPT-5\.
    If it’s good, someone will want to build GPT-6\. Normal capitalism will cause
    people to gradually increase chip capacity. People will make a lot of GPT-5.1s
    and GPT-5.2s until finally someone takes the plunge and builds the giant power
    plant somewhere. All of this will take decades, happen pretty naturally, and no
    one person or corporation will have a monopoly.
  id: totrans-split-55
  prefs: []
  type: TYPE_NORMAL
  zh: 或许他不会得到他的7万亿美元。那么同样的过程会发生，但更慢、更零散。他们会推出GPT-5。如果它好，有人会想要建造GPT-6。正常的资本主义会导致人们逐渐增加芯片容量。人们会制造很多GPT-5.1和GPT-5.2，直到最终有人决定在某处建造巨型发电厂。所有这些将需要几十年时间，自然发生，没有一个人或一个公司会垄断。
- en: 'I would be happier with the second situation: [the safety perspective](https://www.astralcodexten.com/p/pause-for-thought-the-ai-pause-debate)
    here is that we want as much time as we can get to prepare for disruptive AI.'
  id: totrans-split-56
  prefs: []
  type: TYPE_NORMAL
  zh: 我更愿意接受第二种情况：[安全视角](https://www.astralcodexten.com/p/pause-for-thought-the-ai-pause-debate)是，我们希望尽可能多地准备应对颠覆性AI所需的时间。
- en: Sam Altman previously endorsed this position! He said that OpenAI’s efforts
    were good for safety, because you want to avoid *compute overhang*. That is, you
    want AI progress to be as gradual as possible, not to progress in sudden jerks.
    And one way you can keep things gradual is to max out the level of AI you can
    build with your current chips, and then AI can grow (at worst) as fast as the
    chip supply, which naturally grows pretty slowly.
  id: totrans-split-57
  prefs: []
  type: TYPE_NORMAL
  zh: 之前Sam Altman支持过这个立场！他说OpenAI的努力对安全有益，因为你想避免*计算过剩*。也就是说，你希望AI的进展尽可能缓慢，而不是突然跳跃。而你可以保持事物渐进的方式之一是，用当前的芯片达到你能建造的AI的最高水平，然后AI可以（在最糟糕的情况下）以芯片供应的速度增长，这自然增长相当缓慢。
- en: …*unless* you ask for $7 trillion dollars to increase the chip supply in a giant
    leap as quickly as possible! People who trusted OpenAI’s good nature based on
    the compute overhang argument [are feeling betrayed right now](https://forum.effectivealtruism.org/posts/vBjSyNNnmNtJvmdAg/sam-altman-s-chip-ambitions-undercut-openai-s-safety).
  id: totrans-split-58
  prefs: []
  type: TYPE_NORMAL
  zh: …*除非*你要求提供7万亿美元，以尽快大幅增加芯片供应！基于计算过剩论点信任OpenAI的善意的人们[现在感到被背叛](https://forum.effectivealtruism.org/posts/vBjSyNNnmNtJvmdAg/sam-altman-s-chip-ambitions-undercut-openai-s-safety)。
- en: My current impression of OpenAI’s [multiple contradictory perspectives here](https://www.astralcodexten.com/p/openais-planning-for-agi-and-beyond)
    is that they are genuinely interested in safety - but only insofar as that’s compatible
    with scaling up AI as fast as possible. This is far from the worst way that an
    AI company could be. But it’s not reassuring either.
  id: totrans-split-59
  prefs: []
  type: TYPE_NORMAL
  zh: 我目前对OpenAI的印象[多个相互矛盾的观点在此](https://www.astralcodexten.com/p/openais-planning-for-agi-and-beyond)是，他们确实对安全感兴趣
    - 但仅限于这与尽快扩展AI的可行性相容。这远非AI公司可能采取的最糟糕的方式。但也并不令人放心。
