- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: æœªåˆ†ç±»'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: ç±»åˆ«ï¼šæœªåˆ†ç±»
- en: 'date: 2024-05-27 15:19:48'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: æ—¥æœŸï¼š2024å¹´05æœˆ27æ—¥15:19:48
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: 'ğŸ¦… Eagle 7B : Soaring past Transformers with 1 Trillion Tokens Across 100+ Languages
    (RWKV-v5)'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ğŸ¦… Eagle 7Bï¼šè·¨è¶Š100å¤šç§è¯­è¨€ä½¿ç”¨1ä¸‡äº¿æ ‡è®°çš„å˜å‹å™¨ï¼ˆRWKV-v5ï¼‰ã€‚
- en: æ¥æºï¼š[https://blog.rwkv.com/p/eagle-7b-soaring-past-transformers](https://blog.rwkv.com/p/eagle-7b-soaring-past-transformers)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: æ¥æºï¼š[https://blog.rwkv.com/p/eagle-7b-soaring-past-transformers](https://blog.rwkv.com/p/eagle-7b-soaring-past-transformers)
- en: An eagle, flying past a transformer-looking robot
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€åªé¹°ï¼Œé£è¿‡ä¸€ä¸ªçœ‹èµ·æ¥åƒå˜å‹å™¨çš„æœºå™¨äººã€‚
- en: 'Eagle 7B is a 7.52B parameter model that:'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: Eagle 7Bæ˜¯ä¸€ä¸ª7.52Bå‚æ•°æ¨¡å‹ï¼Œå®ƒï¼š
- en: We are releasing RWKV-v5 Eagle 7B, [licensed as Apache 2.0 license, under the
    Linux Foundation](https://blog.rwkv.com/p/rwkv-joins-the-linux-foundation-as),
    and can be used personally or commercially without restrictions
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å‘å¸ƒäº†RWKV-v5 Eagle 7Bï¼Œ[åœ¨LinuxåŸºé‡‘ä¼šä¸‹ä»¥Apache 2.0è®¸å¯è¯æˆæƒ](https://blog.rwkv.com/p/rwkv-joins-the-linux-foundation-as)ï¼Œå¯ä»¥ä¸ªäººæˆ–å•†ä¸šç”¨é€”æ— é™åˆ¶åœ°ä½¿ç”¨
- en: 'We performed multi-lingual performance across the following benchmarks: [xLAMBDA](https://github.com/EleutherAI/lm-evaluation-harness?tab=readme-ov-file#advanced-usage-tips),
    [xStoryCloze](https://huggingface.co/datasets/Muennighoff/xstory_cloze), [xWinograd](https://huggingface.co/datasets/Muennighoff/xwinograd),
    [xCopa](https://huggingface.co/datasets/xcopa)'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å¯¹ä»¥ä¸‹åŸºå‡†æµ‹è¯•è¿›è¡Œäº†å¤šè¯­è¨€æ€§èƒ½æµ‹è¯•ï¼š[xLAMBDA](https://github.com/EleutherAI/lm-evaluation-harness?tab=readme-ov-file#advanced-usage-tips)ï¼Œ[xStoryCloze](https://huggingface.co/datasets/Muennighoff/xstory_cloze)ï¼Œ[xWinograd](https://huggingface.co/datasets/Muennighoff/xwinograd)ï¼Œ[xCopa](https://huggingface.co/datasets/xcopa)
- en: Across a total of 23 languages
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æ€»å…±23ç§è¯­è¨€ä¸­
- en: Most of these benchmarks cover common sense reasoning, in their respective languages.
    And show a huge overall jump in multi-lingual performance for RWKV v4-to-v5 architecture.
    And the v2 world dataset.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™äº›åŸºå‡†å¤§éƒ¨åˆ†æ¶µç›–äº†å¸¸è¯†æ¨ç†ï¼Œä½¿ç”¨å„è‡ªçš„è¯­è¨€ã€‚å¹¶ä¸”æ˜¾ç¤ºäº†RWKV v4åˆ°v5æ¶æ„çš„å¤šè¯­è¨€æ€§èƒ½çš„å·¨å¤§æ•´ä½“è·ƒå‡ã€‚ä»¥åŠv2ä¸–ç•Œæ•°æ®é›†ã€‚
- en: It should also be noted, that there is a lack of multi-lingual benchmarks, as
    the above covers approximately the top 23 languages.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: è¿˜åº”è¯¥æ³¨æ„åˆ°ï¼Œç”±äºä¸Šè¿°æ¶µç›–äº†å¤§çº¦å‰23ç§è¯­è¨€ï¼Œæ‰€ä»¥ç¼ºä¹å¤šè¯­è¨€åŸºå‡†æµ‹è¯•ã€‚
- en: This makes it hard to evaluate model language performance directly over the
    remaining 75+ languages, over the total 100+ trained languages. A shortcoming
    we hope to improve in future models.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä½¿å¾—ç›´æ¥è¯„ä¼°å‰©ä½™çš„75å¤šç§è¯­è¨€çš„æ¨¡å‹è¯­è¨€æ€§èƒ½å˜å¾—å›°éš¾ï¼Œè¶…è¿‡äº†æ€»è®¡100å¤šç§å—è¿‡åŸ¹è®­çš„è¯­è¨€ã€‚è¿™æ˜¯æˆ‘ä»¬å¸Œæœ›åœ¨æœªæ¥çš„æ¨¡å‹ä¸­æ”¹è¿›çš„ä¸€ä¸ªç¼ºç‚¹ã€‚
- en: English performance was measured across 12 separate benchmarks, across commonsense
    reasoning, and world knowledge
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: è‹±è¯­è¡¨ç°æ˜¯åœ¨12ä¸ªç‹¬ç«‹çš„åŸºå‡†æµ‹è¯•ä¸­è¿›è¡Œçš„ï¼Œæ¶µç›–å¸¸è¯†æ¨ç†å’Œä¸–ç•ŒçŸ¥è¯†ã€‚
- en: Once again we see a huge overall jump from RWKV v4-to-v5 architecture. And the
    v2 world dataset.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å†æ¬¡çœ‹åˆ°ä»RWKV v4åˆ°v5æ¶æ„çš„å·¨å¤§æ•´ä½“è·ƒå‡ã€‚ä»¥åŠv2ä¸–ç•Œæ•°æ®é›†ã€‚
- en: Where v4 previously lost out to MPT-7b, the top model in the 1T token tier.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨v4ä¹‹å‰è¾“ç»™äº†MPT-7bï¼Œå³1T tokenå±‚ä¸­çš„é¡¶çº§æ¨¡å‹ã€‚
- en: v5 begins trading blows in benchmarks, in some cases even coming on top in certain
    benchmarks ( LAMBADA, StoryCloze16, WinoGrande, HeadQA_en, Sciq ) over Falcon,
    or even llama2.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: v5å¼€å§‹åœ¨åŸºå‡†æµ‹è¯•ä¸­äº‰å¤ºï¼ŒæŸäº›æƒ…å†µä¸‹ç”šè‡³åœ¨æŸäº›åŸºå‡†æµ‹è¯•ä¸­ï¼ˆLAMBADAï¼ŒStoryCloze16ï¼ŒWinoGrandeï¼ŒHeadQA_enï¼ŒSciqï¼‰è¶…è¿‡Falconï¼Œç”šè‡³ç¾Šé©¼2ã€‚
- en: In addition, v5 performance starts to fall in line with the expected transformer
    performance level, with its given approximate token training count.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: å¦å¤–ï¼Œv5çš„æ€§èƒ½å¼€å§‹ä¸é¢„æœŸçš„å˜å‹å™¨æ€§èƒ½æ°´å¹³ä¿æŒä¸€è‡´ï¼Œè€ƒè™‘åˆ°å…¶ç»™å®šçš„è¿‘ä¼¼æ ‡è®°è®­ç»ƒè®¡æ•°ã€‚
- en: With Mistral-7B maintaining its lead with its rumored 2~7 Trillion token training.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: éšç€ç‹‚é£-7Bä¿æŒå…¶é¢†å…ˆåœ°ä½ï¼Œæ®è¯´å…¶è®­ç»ƒäº†2åˆ°7ä¸‡äº¿ä¸ªæ ‡è®°ã€‚
- en: We expect to narrow the gap, as we train an additional 1T token, to cross the
    llama2 line and hopefully reach the mistral line.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬æœŸæœ›éšç€æˆ‘ä»¬é¢å¤–è®­ç»ƒ1T tokenï¼Œç¼©å°å·®è·ï¼Œè¶Šè¿‡ç¾Šé©¼2çº¿ï¼Œå¹¶å¸Œæœ›è¾¾åˆ°ç‹‚é£çº¿ã€‚
- en: Alternatively, as a base model, which is lightly tuned (really small instruct
    set mixed in), we are eager to see how the various community and instruct-tuned
    variants
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ–è€…ï¼Œä½œä¸ºä¸€ä¸ªåŸºç¡€æ¨¡å‹ï¼Œå®ƒè½»å¾®è°ƒæ•´äº†ï¼ˆçœŸçš„å¾ˆå°çš„æŒ‡ä»¤é›†æ··åˆï¼‰ï¼Œæˆ‘ä»¬æ¸´æœ›çœ‹åˆ°å„ç§ç¤¾åŒºå’ŒæŒ‡ä»¤è°ƒæ•´çš„å˜ä½“å¦‚ä½•ã€‚
- en: '* * *'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: A notable observation was that our checkpoints near the 300 Billion token point,
    show similar performance to pythia-6.9b
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€ä¸ªæ˜¾ç€çš„è§‚å¯Ÿæ˜¯ï¼Œæˆ‘ä»¬çš„æ£€æŸ¥ç‚¹åœ¨æ¥è¿‘3000äº¿æ ‡è®°ç‚¹é™„è¿‘ï¼Œæ˜¾ç¤ºå‡ºä¸pythia-6.9bç›¸ä¼¼çš„æ€§èƒ½ã€‚
- en: This is consistent with previous pile-based experiments on our RWKV-v4 architecture,
    that linear transformers like RWKV scale similarly in performance levels to transformers,
    with the same token count training.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¸æˆ‘ä»¬ä»¥å‰åœ¨RWKV-v4æ¶æ„ä¸Šè¿›è¡Œçš„å †å®éªŒä¸€è‡´ï¼Œå³çº¿æ€§å˜å‹å™¨åƒRWKVä¸€æ ·æŒ‰ç…§ç›¸åŒçš„æ ‡è®°è®¡æ•°è¿›è¡Œæ€§èƒ½çº§åˆ«çš„ç±»ä¼¼è§„æ¨¡ã€‚
- en: If so, it does repeat the question. If the exact architecture, matter less than
    the data for the model eval performance?
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœæ˜¯è¿™æ ·ï¼Œå®ƒæ˜¯å¦é‡å¤äº†é—®é¢˜ã€‚å¦‚æœç¡®åˆ‡çš„æ¶æ„ï¼Œå¯¹äºæ¨¡å‹è¯„ä¼°æ€§èƒ½ï¼Œæ•°æ®æ˜¯å¦æ¯”è¾ƒé‡è¦ï¼Ÿ
- en: CUDA computational cost, for RWKV-based architecture vs transformer models -
    that quadratic-vs-linear really scales!
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: RWKV åŸºäºæ¶æ„ä¸å˜å‹å™¨æ¨¡å‹çš„ CUDA è®¡ç®—æˆæœ¬â€”â€”è¿™çœŸæ˜¯ä¸€ä¸ªäºŒæ¬¡ vs çº¿æ€§çš„æ¯”è¾ƒï¼
- en: If true, perhaps we should seek more efficient and scalable architecture, to
    increase accessibility, drive the cost of AI downwards for everyone, and [lessen
    the impact on our environment.](https://blog.rwkv.com/p/the-worlds-greenest-ai-model-rwkvs)
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœæ˜¯çœŸçš„ï¼Œä¹Ÿè®¸æˆ‘ä»¬åº”è¯¥å¯»æ±‚æ›´é«˜æ•ˆã€å¯æ‰©å±•çš„æ¶æ„ï¼Œä»¥æé«˜å¯è®¿é—®æ€§ï¼Œé™ä½äººäººéƒ½èƒ½æ‰¿å—å¾—èµ·çš„ AI æˆæœ¬ï¼Œå¹¶[å‡å°‘å¯¹ç¯å¢ƒçš„å½±å“ã€‚](https://blog.rwkv.com/p/the-worlds-greenest-ai-model-rwkvs)
- en: '* * *'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: A common feedback we receive for the RWKV multi-lingual approach is
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬æ¥æ”¶åˆ°çš„ RWKV å¤šè¯­è¨€æ–¹æ³•çš„ä¸€æ¡å¸¸è§åé¦ˆæ˜¯
- en: And for most parts, we agree on both points.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹äºå¤§å¤šæ•°éƒ¨åˆ†ï¼Œæˆ‘ä»¬éƒ½åŒæ„è¿™ä¸¤ç‚¹ã€‚
- en: But we have no plans on changing this, as we are building AI for the world -
    which is not just an English world.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: ä½†æˆ‘ä»¬æ²¡æœ‰è®¡åˆ’æ”¹å˜è¿™ä¸€ç‚¹ï¼Œå› ä¸ºæˆ‘ä»¬æ­£åœ¨ä¸ºå…¨ä¸–ç•Œæ„å»º AIâ€”â€”è¿™ä¸ä»…ä»…æ˜¯ä¸€ä¸ªè‹±è¯­ä¸–ç•Œã€‚
- en: '[In 2023, only 17% of the world''s population speaks English](https://preply.com/en/blog/english-language-statistics/#:~:text=Current%20research%20suggests%20that%20the,widely%20spoken%20language%20in%202022%3F)'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: '[2023 å¹´ï¼Œå…¨çƒåªæœ‰ 17% çš„äººå£è®²è‹±è¯­](https://preply.com/en/blog/english-language-statistics/#:~:text=Current%20research%20suggests%20that%20the,widely%20spoken%20language%20in%202022%3F)'
- en: '[( 1.3 billion people )](https://preply.com/en/blog/english-language-statistics/#:~:text=Current%20research%20suggests%20that%20the,widely%20spoken%20language%20in%202022%3F)'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: ï¼ˆ13 äº¿äººå£ï¼‰
- en: 'World Map showing the distribution of regions and people who are fluent in
    English (source: [Wikipedia](https://en.wikipedia.org/wiki/List_of_countries_by_English-speaking_population))'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸–ç•Œåœ°å›¾æ˜¾ç¤ºæµåˆ©è®²è‹±è¯­çš„åœ°åŒºå’Œäººå£åˆ†å¸ƒï¼ˆæ¥æºï¼š[ç»´åŸºç™¾ç§‘](https://en.wikipedia.org/wiki/List_of_countries_by_English-speaking_population)ï¼‰
- en: However, by ensuring support for the top 25 languages in the world and beyond,
    we can cover approximately [4 billion people, or 50% of the world](https://en.wikipedia.org/wiki/List_of_languages_by_number_of_native_speakers#Top_languages_by_population)
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: ä½†æ˜¯ï¼Œé€šè¿‡ç¡®ä¿æ”¯æŒä¸–ç•Œå‰ 25 å¤§è¯­è¨€åŠä»¥ä¸Šï¼Œæˆ‘ä»¬å¯ä»¥è¦†ç›–å¤§çº¦[40 äº¿äººå£ï¼Œçº¦å å…¨çƒäººå£çš„ 50%](https://en.wikipedia.org/wiki/List_of_languages_by_number_of_native_speakers#Top_languages_by_population)
- en: Flawed map, highlighting where the eagle language model will support entirely
    or partially - the goal is to be able paint the whole map green with confidence
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: æœ‰ç¼ºé™·çš„åœ°å›¾ï¼Œçªå‡ºæ˜¾ç¤ºé¹°è¯­è¨€æ¨¡å‹å°†å®Œå…¨æˆ–éƒ¨åˆ†æ”¯æŒçš„åŒºåŸŸâ€”â€”ç›®æ ‡æ˜¯èƒ½å¤Ÿå……æ»¡ä¿¡å¿ƒåœ°å°†æ•´ä¸ªåœ°å›¾æ¶‚æˆç»¿è‰²
- en: This aligns well with the teamâ€™s common goal, of getting AI to support everyone,
    not just by allowing it to run cheaply and affordably even on lower-end hardware.
    But by supporting their language.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¸å›¢é˜Ÿçš„å…±åŒç›®æ ‡éå¸¸ä¸€è‡´ï¼Œå³è®© AI æ”¯æŒæ¯ä¸ªäººï¼Œä¸ä»…ä»…æ˜¯é€šè¿‡è®©å…¶åœ¨ä½ç«¯ç¡¬ä»¶ä¸Šä¾¿å®œä¸”å¯è´Ÿæ‹…ï¼Œè€Œæ˜¯é€šè¿‡æ”¯æŒä»–ä»¬çš„è¯­è¨€ã€‚
- en: Over time, we intend to grow the multi-lingual dataset, to support a wider variety
    of languages, and to slowly grow that coverage to 100% of the world - to ensure
    no language gets left behind.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: éšç€æ—¶é—´çš„æ¨ç§»ï¼Œæˆ‘ä»¬æ‰“ç®—æ‰©å¤§å¤šè¯­è¨€æ•°æ®é›†ï¼Œæ”¯æŒæ›´å¹¿æ³›çš„è¯­è¨€ç§ç±»ï¼Œå¹¶é€æ­¥å°†è¦†ç›–èŒƒå›´æ‰©å¤§åˆ°å…¨çƒ 100%â€”â€”ä»¥ç¡®ä¿æ²¡æœ‰ä»»ä½•ä¸€ç§è¯­è¨€è¢«è½ä¸‹ã€‚
- en: A major example of this in our community is the [Indonesian-NLP discord group](https://discord.gg/dy9YWXjV),
    which finetunes an Indonesian language model from the RWKV line of base models.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬ç¤¾åŒºä¸­çš„ä¸€ä¸ªä¸»è¦ä¾‹å­æ˜¯[å°å°¼ NLP Discord ç¾¤](https://discord.gg/dy9YWXjV)ï¼Œè¯¥ç¾¤ä¼šä» RWKV ç³»åˆ—åŸºç¡€æ¨¡å‹ä¸­å¾®è°ƒå°å°¼è¯­è¨€æ¨¡å‹ã€‚
- en: Allowing them to build strong language-specific models - on a cheap affordable
    basis (ie. single node), without needing to do half a million dollars of pre-training.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: ä½¿ä»–ä»¬èƒ½å¤Ÿåœ¨å»‰ä»·å¯è´Ÿæ‹…çš„åŸºç¡€ä¸Šï¼ˆå³å•èŠ‚ç‚¹ï¼‰æ„å»ºå¼ºå¤§çš„è¯­è¨€ç‰¹å®šæ¨¡å‹ï¼Œè€Œä¸éœ€è¦è¿›è¡Œäº”åä¸‡ç¾å…ƒçš„é¢„è®­ç»ƒã€‚
- en: '* * *'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: This release marks the release of the strongest linear transformer (in terms
    of eval benchmarks) to date.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: æ­¤ç‰ˆæœ¬æ ‡å¿—ç€è¿„ä»Šä¸ºæ­¢æœ€å¼ºçš„çº¿æ€§å˜å‹å™¨ï¼ˆåœ¨è¯„ä¼°åŸºå‡†æ–¹é¢ï¼‰çš„å‘å¸ƒã€‚
- en: While it may not have succeeded in passing LLaMA2 and Mistral. It provides strong
    evidence of the following
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: è™½ç„¶å®ƒå¯èƒ½æ²¡æœ‰æˆåŠŸé€šè¿‡ LLaMA2 å’Œ Mistralã€‚å®ƒæä¾›äº†ä»¥ä¸‹å¼ºæœ‰åŠ›çš„è¯æ®ã€‚
- en: The RWKV-v5 model architecture scales similarly to transformer performance with
    a similar token count
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: RWKV-v5 æ¨¡å‹æ¶æ„çš„è§„æ¨¡ä¸å˜å‹å™¨æ€§èƒ½ç±»ä¼¼ï¼Œå…·æœ‰ç›¸ä¼¼çš„ä»¤ç‰Œæ•°
- en: You can achieve a near LLaMA2-like level of performance, with a substantially
    lower inference cost
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æ‚¨å¯ä»¥ä»¥æä½çš„æ¨ç†æˆæœ¬å®ç°æ¥è¿‘ LLaMA2 çš„æ€§èƒ½æ°´å¹³
- en: While supporting multi-lingual levels of performance
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: åŒæ—¶æ”¯æŒå¤šè¯­è¨€æ°´å¹³çš„æ€§èƒ½
- en: We plan to follow by pushing further ahead with
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬è®¡åˆ’ç»§ç»­æ¨è¿›
- en: '[Feb 2024] An updated RWKV v5: Eagle paper, where we will go deeper in-depth
    on the architecture changes since v4, and the model benchmarks and evals'
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[2024 å¹´ 2 æœˆ] æ›´æ–°çš„ RWKV v5ï¼šé¹°è®ºæ–‡ï¼Œæˆ‘ä»¬å°†æ·±å…¥æ¢è®¨è‡ª v4 ä»¥æ¥çš„æ¶æ„å˜åŒ–ï¼Œä»¥åŠæ¨¡å‹åŸºå‡†å’Œè¯„ä¼°'
- en: '[Feb 2024] A further 1T token in training (2T total), for direct comparisons
    with the LLaMA2 7B model'
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Feb 2024] å¦å¤– 1T token åœ¨è®­ç»ƒä¸­ï¼ˆæ€»å…± 2Tï¼‰ï¼Œä»¥ä¸ LLaMA2 7B æ¨¡å‹è¿›è¡Œç›´æ¥æ¯”è¾ƒ'
- en: '[Mar 2024] An MoE model based on the v5 Eagle 2T model'
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Mar 2024] åŸºäº v5 é¹° 2T æ¨¡å‹çš„ An MoE æ¨¡å‹'
- en: '[Mar 2024] RWKV-v6: â€œFinchâ€ 1.5B, 3B world models'
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Mar 2024] RWKV-v6ï¼šâ€œé›€é¸Ÿâ€ 1.5Bï¼Œ3B ä¸–ç•Œæ¨¡å‹'
- en: 'Disclaimer: All dates are approximate, and is heavily subjected to compute
    avaliability from our sponsors/provider'
  id: totrans-52
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: å…è´£å£°æ˜ï¼šæ‰€æœ‰æ—¥æœŸå‡ä¸ºè¿‘ä¼¼å€¼ï¼Œå¹¶ä¸”å—åˆ°æˆ‘ä»¬èµåŠ©å•†/ä¾›åº”å•†è®¡ç®—èµ„æºçš„ä¸¥é‡å½±å“
- en: Find more about the RWKV Project at
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: äº†è§£æ›´å¤šæœ‰å…³ RWKV é¡¹ç›®çš„ä¿¡æ¯ï¼Œè¯·è®¿é—®
- en: '* * *'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: 'We are grateful and would like to thank the following key groups:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬éå¸¸æ„Ÿæ¿€å¹¶è¦æ„Ÿè°¢ä»¥ä¸‹å…³é”®å›¢ä½“ï¼š
- en: Along with the various developers, working on the growing collection of [RWKV-related
    projects](https://wiki.rwkv.com).
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸å„ç§å¼€å‘äººå‘˜ä¸€èµ·ï¼Œè‡´åŠ›äºå‘å±• [RWKV ç›¸å…³é¡¹ç›®](https://wiki.rwkv.com) çš„äººå‘˜ã€‚
