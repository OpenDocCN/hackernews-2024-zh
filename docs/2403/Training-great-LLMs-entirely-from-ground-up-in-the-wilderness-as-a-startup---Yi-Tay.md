<!--yml

category: 未分类

date: 2024-05-27 14:39:19

-->

# 作为初创公司在荒野中完全从零开始训练出伟大的LLMs — Yi Tay

> 来源：[https://www.yitay.net/blog/training-great-llms-entirely-from-ground-zero-in-the-wilderness](https://www.yitay.net/blog/training-great-llms-entirely-from-ground-zero-in-the-wilderness)

#### LLMs时代的硬件彩票

训练模型的首要条件是获取计算资源。这似乎很直接且足够容易。然而，最大的意外是计算资源提供者的不稳定性，以及依赖来源的集群、加速器及其连接质量的巨大差异。

人们总是假设这仅仅是加速器选择的问题/辩论（TPUs vs GPUs等），并且所有GPU集群都是一样的。对我们来说，这很快被证明是错误的。当我们在不同的服务提供商之间取样时，我们发现*甚至对于同样的硬件，硬件质量的差异也是巨大的*，即，GPU（H100s）。请注意，这里的硬件指的是整体集群质量，而不一定是芯片或加速器本身。就像买彩票一样。基本上：

> 并非所有硬件都是相同的。*不同硬件提供商之间的集群质量差异如此之大，以至于在训练优秀模型时，会面临多少痛苦实际上是个彩票。简而言之，在LLMs时代是硬件的彩票。*

具体来说，我们从几家计算资源提供商那里租用了几个集群，每个集群拥有数百到数千个芯片。我们看到的集群从还算过得去的（只有一些可以通过少量软件工程师小时解决的恼人问题）到完全无法使用的集群，由于各种原因每隔几个小时就会失败。具体来说，一些集群的节点每隔N小时就会失败，问题包括布线问题（其中N非常小），GPU硬件错误等。更令人惊讶的是，同一提供商的每个集群在稳健性方面也可能大不相同。

与此同时，即使某些其他集群的节点可能更加稳定，它们可能遭受I/O和文件系统不良的困扰，即使是保存检查点也可能导致超时或者长时间耗费集群利用率。其他一些计算资源甚至需要完全不同的软件层才能运行，并且对于带自己代码库的团队来说并不友好 — 需要额外的迁移成本来运行实验或大规模作业。

事物无完美！但有些比其他的确糟糕得多。

最令人沮丧的部分？几乎不可能提前知道，特别是在一切都非常繁忙的时候，人们将会得到什么样的硬件，以及体验将有多么强大/容错。

除此之外，您也无法确定供应商是否仅仅是不能按时交付并延迟了几个月的发货，让人一度无法从其他来源采购数周或数月。有些供应商也会意外地删除您的检查点 ¯\_(ツ)_/¯。

我是否提到过，您还将在不同的集群中获得不同的模型浮点操作利用率（MFU）！？如果不幸地找到有糟糕布线的节点或其他问题的供应商，这将导致相当大量的计算资源浪费。拥有非常次优文件系统的系统会在队友开始在集群之间传输大量数据时，训练运行的 MFU 瞬间下降。

每个服务提供商的支持水平也各不相同。从彬彬有礼到漫不经心，“ChatGPT 风格”的机械回复，再到把每一个出错的事情都归咎于用户。

总的来说，我们试过的每一个集群都感觉它们有自己的氛围、挣扎和失败模式。几乎每一个集群似乎都需要针对其自身问题的即时修复 - 有些问题比其他问题更容忍。话虽如此，我们已经学到了故障保护的重要性，以及为任何集群找到快速修复的关键性。

在过去的几个月中，我们做了很多工作，只是为了确保事情是可用的，例如，围绕监控、高效的检查点管理以及各种其他优化措施，甚至包括安装我们自定义的文件系统以实现可扩展的数据存储 - 这只是实际所需的冰山一角。

这些工具的组合导致了模型浮点操作利用率的显著提高，同时在面对糟糕的硬件时，最小化了停机时间。

#### **关于 GPU vs TPU**

我们在 Reka 大部分时间都在使用 GPU 训练我们的模型。就个人而言，在 Reka 之前的生活中，当涉及到 Google 大型语言模型的训练时，我一直使用 TPUs。CUDA 和 *nccl* 对我来说是最陌生的东西。（我只是从一个曾在 Nvidia 工作过的同事那里学到它的发音是“Nickel”，哈哈）

对于 GPU 的故障率，与我在 Google 的 TPU 上的经验形成了鲜明对比，让我彻底吃惊了。事实上，我实际上几乎不记得 TPUs 在大规模运行中出现过故障，虽然*我并不确定这是否仅仅是由于极为强大的基础设施和专门的硬件团队的绝佳韧性所保护而已*。实际上，在 Google，[UL2 20B](https://blog.research.google/2022/10/ul2-20b-open-source-unified-language.html) 模型是因为意外地让工作运行了一个月而被训练的。它从未失败过。如果这是在 GPU 领域，它肯定会在最初几天内失败。

说实话，我认为这更多地与管理加速器的硬件团队的能力有关，而不是底层芯片的问题。有一个良好的硬件支持（来自您的计算提供商）是很重要的。因此，很大程度上取决于他们是否真正 kompetent，这加强了“硬件抽奖”的概念。

GPU领域感觉很奇怪。在GPU领域，它似乎更像是多节点训练作为TPU pod上的分布式训练的附带思想。在GPU领域，似乎不同的提供商以不同的方式对它们进行布线，以实现多节点训练，这导致了在不同地方完成任务的高变异性。虽然我不是硬件专家，但这是我得到的印象。

#### **多集群设置的痛苦**

我的大部分职业生涯都花在了谷歌基础设施上，主要运行在 [Borg](https://research.google/pubs/large-scale-cluster-management-at-google-with-borg/)、[Xmanager](https://github.com/google-deepmind/xmanager) 和 [Colossus](https://cloud.google.com/blog/products/storage-data-transfer/a-peek-behind-colossus-googles-file-system) 上。这些东西几乎可以从任何地方进行评估。因此，我对于实际在不同集群中设置新环境的概念感到陌生。

在当前世界中，除非专门为单一位置中的大量数量构建，否则拥有多个加速器池集群似乎是不可避免的。更具体地说，GPU供应（或其缺乏）也自然地导致了这种集群采购模式，自然地事物被碎片化。训练大型模型也需要大量的数据，即使仅仅是移动它们也会带来很多不便。同时，复制数据通常也不直接并且在极大规模上是禁锢的。

显然，这里的理想情况是一种特别设计的编排层，用于向不同的服务器发送作业。我认为许多大型以人工智能为先的公司通常都有某种基础设施来提高AI研究人员的生活质量。然而，对于一个刚刚起步的精益和新兴的初创公司来说，构建这种复杂和花哨的ML训练基础设施并不是真的可能。

目前，我们开发了许多内部工作流来减轻许多这些问题，并且正在继续向世界一流实验基础设施的黄金标准迈进。

（我被告知这种令人不快的设置对非一流/大公司来说更像是常态）。

#### **野外代码**

毫无秘密，我最喜欢的所有时间代码库是 [T5X](https://github.com/google-research/t5x) 和 [Mesh Tensorflow](https://github.com/tensorflow/mesh)（命名张量 ftw），但这些选项很快就变得不可行了，因为 1）它们在谷歌之外的支持不足，2）它们有点过时，3）它们对于我们团队中的非 xoogler 并不友好。

最终，我们选择了一些普通的、看起来稳定且更受欢迎的东西（即pytorch），这对大多数团队成员来说更容易接触（除了我哈哈）。在我最初的几个月里，我在pip、git、docker以及所有这些狂野的东西上都摔了几个跟头。不过，我并不完全确定使用Google外部的代码库会有多稳定或用户友好（我想那可能会相当糟糕）。

坦率地说，我必须说外部代码库的质量明显落后于我在Google使用习惯的那些代码库。主要是因为Google内部的代码库通常由机器学习的明星们自己编写（例如，Noam Shazeer，Barret Zoph，Adam Roberts，Hyung Won Chung等），感觉比我尝试过的外部代码库要好得多（例如，更出色的氛围）。特别是当我涉足其他公司构建的东西时，我发现自己对代码质量感到非常恼火（有些比其他公司差得多🤗）。

而且，我从未意识到改变模型并行性的能力不是自动的（免费），直到某些代码库要求我编写转换器来改变模型的并行性。对我来说确实是一个令人震惊的时刻。

另一个显著的事实是，这些代码库对大规模编码器-解码器训练甚至前缀LM训练的支持非常有限。为此，即使是快闪注意力模型也一直拒绝为前缀LM训练（即自定义掩码）提供支持，尽管在他们的github问题中有合理的需求。

我知道我应该使用Jax。一个朋友刚才羞辱我使用pytorch，但这是一家初创公司，我们决定快速前进。对不起，我们下次会更酷一点。我对这个事实并不感到自豪。

#### **不那么原则，更多Yolo**

系统地扩展模型通常要求我们以原则性的方式从小规模到大规模运行实验，即在多个阶段运行实验（1B->8B->64B->300B等），选择优胜者并持续扩展它们。在初创企业中，我们没有足够的计算能力来执行这些大规模的参数扫描。最终，我们不得不依靠许多[Yolo runs](https://twitter.com/_jasonwei/status/1757486124082303073)（幸运的是结果还不错）。

最后，我们只需要进行了少量较小规模和较短的剥蚀运行，就达到了强大的21B Reka Flash和7B边缘模型（以及即将推出的最大核心模型）。在极其有限的运行次数内找到一个可靠的配方是具有挑战性的，需要同时改变许多变量，鉴于搜索空间极其庞大。为了做到这一点，一个人必须放弃Bigtech的系统性，并且非常依赖“Yolo”、直觉和本能。

幸运的是，我（以及团队中的许多人）在我们的机器学习职业生涯中积累了相当多的直觉，使我们能够在相当短的尝试次数内就做对。虽然我们以前在我们的前职中训练过非常好的模型，但是训练基础设施、数据、新想法的整合以及其他环境问题的差异仍然可能导致结果上的非常规差异。尽管如此，强大的先验知识有助于显著减少搜索空间，可能是我们能够用如此少的尝试、资源和实验来训练非常强大的模型的最简单解释之一。

#### **简而言之**

在荒野中摸索是一次有趣的经历。不幸的是，这并不是毫无痛苦的。计算资源稀缺以及不可靠的计算服务提供商使事情变得比预期的更加困难，但我们很高兴我们凭借强大的技术实力度过了难关。

总的来说，这只是我们开始公司、筹集资金、购买芯片以及匹配 Gemini pro/GPT 3.5 并在不到一年的时间里胜过许多其他公司的故事的一小部分，不得不从零开始构建一切。

还有更多内容需要写，数据管道、人工评估等等，但这篇文章已经非常长了。下次再说。给 @YitayML 发送 X 上的私信以获得反馈！

**致谢**

我要感谢 Dani Yogatama、Piotr Padlewski、Matthew Henderson、Donovan Ong、Che Zheng、Aitor Ormazabal、傅德宇，在 Reka 团队对这篇文章的反馈。

我还要感谢外部的朋友们，Jason Wei、Hyung Won Chung、Vinh Tran 和 Mostafa Dehghani，对这篇文章的反馈。
