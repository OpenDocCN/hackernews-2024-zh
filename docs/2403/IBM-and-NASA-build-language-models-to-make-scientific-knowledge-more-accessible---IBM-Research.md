<!--yml

category: 未分类

date: 2024-05-27 14:53:52

-->

# IBM 和 NASA 开发语言模型，旨在使科学知识更易获取 - IBM 研究

> 来源：[https://research.ibm.com/blog/science-expert-LLM](https://research.ibm.com/blog/science-expert-LLM)

在一项新的合作中，IBM 和 NASA 通过在科学文献上进行训练，创建了一套高效的语言模型。基于 Transformer 结构，这些模型可用于各种应用，从分类和实体提取到问答和信息检索。这些模型在各种领域中表现出色，并能够迅速响应。我们已在 Hugging Face 上开源这些模型，以造福科学和学术界社区。

基于 Transformer 的语言模型，包括 [BERT](https://aclanthology.org/N19-1423/)、[RoBERTa](https://arxiv.org/abs/1907.11692)，以及 IBM 的 Slate 和 Granite 系列模型，在各种自然语言理解任务中无可替代。这些模型之所以强大，在于对语言运作方式的统计理解。它们通过掩码语言建模任务进行训练，通过重建被遮蔽的单词来学习。分词器在为模型分解单词成单位方面发挥了至关重要的作用，从而有助于学习庞大的词汇量。虽然通用文本训练在像维基百科或 BooksCorpus 这样的数据集上使用受欢迎的分词器是有效的，但科学领域需要专门用于术语如 "phosphatidylcholine" 的分词器。

我们的模型在宇宙物理学、行星科学、地球科学、太阳物理学以及生物和物理科学数据的语料库上训练了 600 亿个标记。与通用分词器不同，我们开发的分词器能够识别诸如 "axes" 和 "polycrystalline" 等科学术语。我们的模型处理的 5 万个标记中，有超过一半是与 Hugging Face 上的开源 RoBERTa 模型不同的独特标记。

IBM-NASA 模型，经过域特定词汇的训练，在流行的 [BLURB](https://microsoft.github.io/BLURB/) 基准测试上，比开源 RoBERTa 模型提升了 5% 的性能，还在内部科学问答基准测试中显示出 2.4% 的 F1 得分提升，以及 5.5% 在内部地球科学实体识别测试中的提升。

我们训练的编码器模型可以针对许多非生成性语言任务进行微调，并能通过 [检索增强生成](https://arxiv.org/abs/2005.11401)（RAG）生成信息丰富的嵌入，以便进行文档检索。RAG 通常遵循一个两步框架：检索模型首先编码问题并从向量数据库中检索相关文档。然后将这些文档传递给生成模型以回答问题，同时确保与检索到的文档的一致性。

我们在我们的编码器模型之上构建了一个检索模型，以生成信息丰富的嵌入，映射文本对之间的相似度。具体来说，我们优化了对比损失函数，将锚定文本的嵌入推向相关的（“正面”）文档，远离随机的（“负面”）文档。

这些模型使用了大约2.68亿个文本对，包括标题和摘要，以及问题和答案。因此，它们在NASA策划的约400个问题的测试集中表现出色。这体现在与类似微调的RoBERTa模型相比提高了6.5%，以及与另一种流行的开源模型[BGE-base](https://huggingface.co/BAAI/bge-base-en-v1.5)相比提高了5%。

我们模型取得的显著增强归因于专门的训练数据、定制的分词器和训练方法。与IBM和NASA致力于开放和透明的AI一致，这两个模型都可以在Hugging Face上找到：[编码器模型](https://huggingface.co/nasa-impact/nasa-smd-ibm-v0.1)可以进一步用于太空领域的应用微调，而[检索模型](https://huggingface.co/nasa-impact/nasa-smd-ibm-st)可以用于RAG信息检索应用。我们还与NASA合作，利用这些模型增强科学搜索引擎。
