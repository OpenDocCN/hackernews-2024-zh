- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-05-29 12:31:34'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: Nvidia Unveils Blackwell, Its Next GPU - IEEE Spectrum
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://spectrum.ieee.org/nvidia-blackwell](https://spectrum.ieee.org/nvidia-blackwell)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Today at Nvidia’s developer conference, [GTC 2024](https://www.nvidia.com/gtc/),
    the company revealed its next GPU, [the B200](https://nvidianews.nvidia.com/news/nvidia-blackwell-platform-arrives-to-power-a-new-era-of-computing).
    The B200 is capable of delivering four times the training performance, up to 30
    times the inference performance, and up to 25 times better energy efficiency,
    compared to its predecessor, the [Hopper H100 GPU](https://spectrum.ieee.org/mlperf-inferencing).
    Based on the new Blackwell architecture, the GPU can be combined with the company’s
    Grace CPUs to form a new generation of DGX SuperPOD computers capable of up to
    11.5 billion billion floating point operations (exaflops) of AI computing using
    a new, low-precision number format.
  prefs: []
  type: TYPE_NORMAL
- en: “Blackwell is a new class of AI superchip,” says [Ian Buck](https://www.linkedin.com/in/ian-buck-19201315/),
    Nvidia’s vice president of high-performance computing and hyperscale. Nvidia named
    the GPU architecture for mathematician [David Harold Blackwell](https://en.wikipedia.org/wiki/David_Blackwell),
    the first Black inductee into the U.S. National Academy of Sciences.
  prefs: []
  type: TYPE_NORMAL
- en: The B200 is composed of about 1600 square millimeters of processor on two silicon
    dies that are linked in the same package by a 10 terabyte per second connection,
    so they perform as if they were a single 208-billion-transistor chip. Those slices
    of silicon are made using [TSMC’s N4P chip technology](https://pr.tsmc.com/english/news/2874),
    which provides a 6 percent performance boost over the N4 technology used to make
    Hopper architecture GPUs, like the H100.
  prefs: []
  type: TYPE_NORMAL
- en: Like Hopper chips, the B200 is surrounded by high-bandwidth memory, increasingly
    important to reducing the latency and energy consumption of large AI models. B200’s
    memory is the latest variety, HBM3e, and it totals 192 GB (up from 141 GB for
    the second generation Hopper chip, H200). Additionally, the memory bandwidth is
    boosted to 8 terabytes per second from the [H200](https://www.nvidia.com/en-us/data-center/h200/)’s
    4.8 TB/s.
  prefs: []
  type: TYPE_NORMAL
- en: Smaller Numbers, Faster Chips
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Chipmaking technology did some of the job in making Blackwell, but its what
    the GPU does with the transistors that really makes the difference. In explaining
    [Nvidia’s AI success](https://spectrum.ieee.org/nvidia-gpu) to computer scientists
    last year at [IEEE Hot Chips,](https://ieeexplore.ieee.org/xpl/conhome/10254689/proceeding)
    Nvidia chief scientist [Bill Dally](https://research.nvidia.com/person/william-dally)
    said that the majority came from using fewer and fewer bits to represent numbers
    in AI calculations. Blackwell continues that trend.
  prefs: []
  type: TYPE_NORMAL
- en: It’s predecessor architecture, Hopper, was the first instance of what Nvidia
    calls the [transformer engine](https://spectrum.ieee.org/nvidias-next-gpu-shows-that-transformers-are-transforming-ai).
    It’s a system that examines each layer of a neural network and determines whether
    it could be computed using lower-precision numbers. Specifically, Hopper can use
    floating point number formats as small as 8 bits. Smaller numbers are faster and
    more energy efficient to compute, require less memory and memory bandwidth, and
    the logic required to do the math takes up less silicon.
  prefs: []
  type: TYPE_NORMAL
- en: “With Blackwell, we have taken a step further,” says Buck. The new architecture
    has units that do matrix math with floating point numbers just 4 bits wide. What’s
    more, it can decide to deploy them on parts of each neural network layer, not
    just entire layers like Hopper. “Getting down to that level of fine granularity
    is a miracle in itself,” says Buck.
  prefs: []
  type: TYPE_NORMAL
- en: NVLink and Other Features
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Among the other architectural insights Nvidia revealed about Blackwell are that
    it incorporates a dedicated “engine” devoted to the GPU’s reliability, availability,
    and serviceability. According to Nvidia, it uses an AI-based system to run diagnostics
    and forecast reliability issues, with the aim of increasing up time and helping
    massive AI systems run uninterrupted for weeks at a time, a period often needed
    to train large language models.
  prefs: []
  type: TYPE_NORMAL
- en: Nvidia also included systems to help keep AI models secure and to decompress
    data to speed database queries and data analytics.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, Blackwell incorporates Nvidia’s fifth generation computer interconnect
    technology NVLink, which now delivers 1.8 terabytes per second bidirectionally
    between GPUs and allows for high-speed communication among up to 576 GPUs. Hopper’s
    version of NVLink could only reach half that bandwidth.
  prefs: []
  type: TYPE_NORMAL
- en: SuperPOD and Other Computers
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: NVLink’s bandwidth is key to [building large-scale computers from Blackwell](https://nvidianews.nvidia.com/news/nvidia-blackwell-dgx-generative-ai-supercomputing),
    capable of crunching through trillion-parameter neural network models.
  prefs: []
  type: TYPE_NORMAL
- en: The base computing unit is called the DGX GB200\. Each of those include 36 GB200
    superchips. These are modules that include a Grace CPU and two Blackwell GPUs,
    all connected together with NVLink.
  prefs: []
  type: TYPE_NORMAL
- en: The Grace Blackwell superchip is two Blackwell GPUs and a Grace CPU in the same
    module.Nvidia
  prefs: []
  type: TYPE_NORMAL
- en: Eight DGX GB200s can be connected further via NVLINK to form a 576-GPU supercomputer
    called a DGX SuperPOD. Nvidia says such a computer can blast through 11.5 exaflops
    using 4-bit precision calculations. Systems of tens of thousands of GPUs are possible
    using the company’s [Quantum Infiniband](https://www.nvidia.com/en-us/networking/quantum2/)
    networking technology.
  prefs: []
  type: TYPE_NORMAL
- en: The company says to expect SuperPODs and other Nvidia computers to become available
    later this year. Meanwhile, chip foundry TSMC and electronic design automation
    company Synopsys each announced that they would be moving Nvidia’s inverse lithography
    tool, cuLitho, [into production](https://spectrum.ieee.org/inverse-lithography).
    Lastly, the Nvidia announced a [new foundation model for humanoid robots](https://spectrum.ieee.org/nvidia-gr00t-ros)
    called GR00T.
  prefs: []
  type: TYPE_NORMAL
