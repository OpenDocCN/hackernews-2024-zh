<!--yml

类别：未分类

日期：2024-05-27 14:37:18

-->

# 不要将AI模型视为人类

> 来源：[https://garymarcus.substack.com/p/stop-treating-ai-models-like-people](https://garymarcus.substack.com/p/stop-treating-ai-models-like-people)

**由Sasha Luccioni和Gary Marcus撰写**

这种ELIZA效应至今仍然存在，已有半个多世纪了。

在过去几个月中，人们与像GPT-4和Bard这样的聊天机器人进行了无数次“对话”，询问这些系统关于[气候变化是否真实](https://www.foxnews.com/media/chatgtp-confession-global-warming-much-since-2016)，[如何让人们爱上他们](https://www.techradar.com/opinion/i-asked-bing-about-love-the-results-broke-and-mended-my-heart)，甚至是[他们对AI驱动的世界统治计划的看法](https://www.tomshardware.com/news/chatgpt-pi-furby-nightmare)。显然，这些行为基于这样一种假设，即这些系统有真实的信仰和自我教育的能力，就像美国参议员克里斯·墨菲在推特上所说的那样：

从认知心理学的角度来看，所有这些都是“过度归因”，将一种心理生活归因于这些机器，其实根本不存在，就像多年前人们以为[Furby在学习语言](https://www.listenandlearn.org/blog/no-you-cant-teach-your-furby-to-swear-how-furbies-learn-language/)时一样，事实上能力的展现是预先编程好的。正如[大多数专家所认识到的](https://arxiv.org/pdf/2212.03551.pdf)，现在的AI并不“决定自学”，甚至没有一致的信念。它生成的一串词语可能会告诉你它理解语言。

另一个可能会说反过来。

[没有那里](https://www.gradesaver.com/gertrude-stein-operas-and-plays/wikipedia/there-is-no-there-there)，盒子里没有小人，没有内在的代理人对世界有所思考，甚至没有长期记忆。这些驱动聊天机器人的AI系统仅仅是系统（在技术上被称为“语言模型”，因为它们模拟语言的统计结构），计算单词序列的概率，而没有深刻或类似人类的理解能力。然而，将这些系统拟人化的冲动对许多人来说是不可抗拒的，这是将面对月亮看见面孔或者在屏幕上看到两个三角形“追逐”归因给代理和情感的同一冲动的延伸。AI社区的每个人都意识到这一点，然而即使是专家有时也会受到拟人化的诱惑，如深度学习先驱杰弗里·辛顿最近在推特上发文称“*通过人类反馈进行强化学习就像是对一个超自然早熟孩子的育养*”。这样做可能很可爱，但也是根本性的误导，甚至是危险的。

人们可能会过度将智能归因于AI系统的事实，早在[ELIZA](https://en.wikipedia.org/wiki/ELIZA)（20世纪60年代的计算机程序，通过模式匹配方法与人类进行伪心理对话，让用户产生程序真正理解他们的印象）之时就已为人所知。现在我们所见到的只是同一“ELIZA效应”的延伸，60年后，人类继续将情感和理解等人类特质投射到这些缺乏这些特质的机器上。随着技术越来越能够根据更大更多样的文本样本模拟人类反应（以及从指导机器的人类中进行“强化学习”），这个问题变得更加棘手。在一个案例中，有人将一个机器人视为介于情人和治疗师之间，并最终自杀；因果关系难以建立，但[寡妇认为这种互动起了重要作用](https://garymarcus.substack.com/p/the-first-known-chatbot-associated)；对于一个易受伤害患者来说，过度归因的风险是严重的。

尽管诱人，我们必须停止把AI模型当作人类对待。这样做会加大AI周围的炒作，并让人们误以为这些机器是可信赖的神谕，能够进行操纵或决策，但实际上并非如此。任何使用这些系统生成传记的人都会意识到，它们往往只是编造事实；把它们当作智能代理意味着人们可能会发展不稳健的情感关系，将不稳健的医疗建议看得比实际值得的多等等。向这些模型询问关于它们自身的问题也是愚蠢的；正如上述互相矛盾的例子清楚地表明的那样，它们实际上并不“知道”；它们只是在不同场合生成不同的词语串，没有任何保证。人们归因给它们的虚假代理越多，它们就越容易被利用，被有害应用如猫鱼网诈骗和欺诈，以及更微妙的有害应用如[聊天机器人辅助治疗](https://www.fastcompany.com/90836906/ai-therapy-koko-chatgpt)或[有缺陷的财务建议](https://fortune.com/recommends/mortgages/i-used-chatgpt-as-my-financial-planner/)。我们需要公众了解，听起来像人的语言并不一定是人类产生的；买方自负。我们还需要新的技术工具，如水印和生成内容检测器，来帮助区分人类生成和机器生成的内容，并采取政策措施限制AI模型的使用范围和方式。

教育人们克服过度归因偏见将是一个关键步骤；我们不能让参议员和人工智能社区的成员加剧问题。对这些技术保持健康的怀疑态度至关重要，因为它们非常新颖，不断发展，并且经受了少量测试。是的，它们可以生成酷炫的俳句和写得很好的散文，但它们也会[持续地喷出错误信息](https://www.theatlantic.com/technology/archive/2023/03/ai-chatbots-large-language-model-misinformation/673376/)（甚至是[关于自己的](https://twitter.com/katecrawford/status/1638524013432516610)），在回答关于现实世界事件和现象的问题时，甚至提供心理健康或婚姻咨询的合理建议时都不可信。

如果你愿意，把它们当作有趣的玩具，但不要把它们当作朋友。

**[萨沙·卢奇奥尼博士](https://www.sashaluccioni.com/)** 是Hugging Face的研究员和气候负责人，她在这里研究人工智能模型和数据集的伦理和社会影响。她还是机器学习中的女性（WiML）的主任、气候变化人工智能（CCAI）的创始成员和NeurIPS伦理委员会的主席。

***[加里·马库斯](http://garymarcus.com)** (@garymarcus)，科学家，畅销书作家和企业家，对当前的人工智能非常关注，但真的希望我们可以做得更好。*

*请关注他的新播客，[人类对抗机器](https://podcasts.apple.com/us/podcast/humans-vs-machines-with-gary-marcus/id1532110146?i=1000602693237)，将于4月25日首播，可在您获取播客的任何地方收听。*

[分享](https://garymarcus.substack.com/p/stop-treating-ai-models-like-people?utm_source=substack&utm_medium=email&utm_content=share&action=share)
