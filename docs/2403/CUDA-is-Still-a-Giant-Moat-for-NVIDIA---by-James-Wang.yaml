- en: <!--yml
  id: totrans-split-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-split-1
  prefs: []
  type: TYPE_NORMAL
  zh: 类别：未分类
- en: 'date: 2024-05-29 12:38:43'
  id: totrans-split-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024年5月29日 12:38:43
- en: -->
  id: totrans-split-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: CUDA is Still a Giant Moat for NVIDIA - by James Wang
  id: totrans-split-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: CUDA依然是NVIDIA的重要护城河 - 作者：詹姆斯·王
- en: 来源：[https://weightythoughts.com/p/cuda-is-still-a-giant-moat-for-nvidia](https://weightythoughts.com/p/cuda-is-still-a-giant-moat-for-nvidia)
  id: totrans-split-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://weightythoughts.com/p/cuda-is-still-a-giant-moat-for-nvidia](https://weightythoughts.com/p/cuda-is-still-a-giant-moat-for-nvidia)
- en: In a way, this is a post-**[GTC](https://www.nvidia.com/gtc/)** update. However,
    **[there are already great overviews of the event](https://www.fabricatedknowledge.com/p/jensens-world-compressing-reality-be4?utm_campaign=email-post&r=4de3d&utm_source=substack&utm_medium=email)**.
    Here, I’ll explain more about why I’ve been a somewhat lonely voice (until recently)
    about why other players like **[SMIC/Huawei/etc](https://weightythoughts.com/p/fear-and-loathing-in-7nm)**
    improving on their hardware still doesn’t displace NVIDIA at all.
  id: totrans-split-6
  prefs: []
  type: TYPE_NORMAL
  zh: 从某种意义上说，这是一篇**[GTC](https://www.nvidia.com/gtc/)**会议的更新。然而，**[已经有了关于该事件的很好的概述](https://www.fabricatedknowledge.com/p/jensens-world-compressing-reality-be4?utm_campaign=email-post&r=4de3d&utm_source=substack&utm_medium=email)**。在这里，我将更详细地解释为什么我一直是一个有些孤独的声音（直到最近）关于其他像**[SMIC/华为等](https://weightythoughts.com/p/fear-and-loathing-in-7nm)**改进他们的硬件依然无法取代NVIDIA。
- en: That view mainly relates to CUDA’s ecosystem and NVIDIA’s proprietary interconnects.
  id: totrans-split-7
  prefs: []
  type: TYPE_NORMAL
  zh: 这个观点主要与CUDA的生态系统和NVIDIA的专有互连有关。
- en: CUDA remains as dominant as ever, and while the announcements about **[Blackwell](https://nvidianews.nvidia.com/news/nvidia-blackwell-platform-arrives-to-power-a-new-era-of-computing)**
    are interesting, it’s a big incremental step forward rather than something truly
    groundbreaking. The much more compelling announcement related to how much more
    NVIDIA is creating a lead in interconnects for large-scale computing (NVLink especially).
  id: totrans-split-8
  prefs: []
  type: TYPE_NORMAL
  zh: CUDA依然占据着主导地位，虽然有关**[Blackwell](https://nvidianews.nvidia.com/news/nvidia-blackwell-platform-arrives-to-power-a-new-era-of-computing)**的公告很有意思，但这只是进步的一大步，而不是真正开创性的东西。更具吸引力的公告与NVIDIA在大规模计算中的互连方面的领先地位（尤其是NVLink）有关。
- en: I’ll get into a deeper post about interconnects, which is quite involved, but
    I think I should finally more fully explain my views on CUDA. Instead of a pure
    technical explanation, let me tell a story to make it more visceral.
  id: totrans-split-9
  prefs: []
  type: TYPE_NORMAL
  zh: 我将在一篇更深入的文章中介绍有关互连的内容，这相当复杂，但我觉得我应该更充分地解释一下我对CUDA的看法。与其纯粹的技术解释，不如让我讲一个故事，以使它更具感染力。
- en: I once took a graduate class at UC Berkeley on high-performance and parallel
    computing. The students were mainly PhDs in computer science, computer engineering,
    and statistics (machine learning). I was a weirdo taking it during my MBA—nothing
    in the rules *technically* said I couldn’t fulfill my electives with this and
    PhD seminars in computer science and statistics. This was before I got my own
    Master’s degree in computer science.
  id: totrans-split-10
  prefs: []
  type: TYPE_NORMAL
  zh: 我曾经在加州大学伯克利分校学习高性能和并行计算的研究生课程。学生主要是计算机科学、计算机工程和统计（机器学习）的博士生。而我在攻读工商管理硕士期间选择修读这门课程可能有些奇怪——虽然规定上**技术上**并没有说我不能用这门课和计算机科学和统计学的博士研讨会来满足我的选修课要求。这是在我自己获得计算机科学硕士学位之前。
- en: Anyway, we went through the concepts and techniques involved in parallel computing,
    in terms of memory architecture, deep dives in matrix multiplication (which is
    fundamental to most modern computing, including current AI), locality, SIMD/CPU
    vectorization, lock/etc primitives. These were quite theoretical, but we also
    discussed and implemented specific technologies including OpenMP, OpenCL/GL, and
    the like.
  id: totrans-split-11
  prefs: []
  type: TYPE_NORMAL
  zh: 无论如何，我们探讨了并行计算涉及的概念和技术，包括存储器体系结构、矩阵乘法（这对于现代计算，包括当前的人工智能，至关重要）、局部性、SIMD/CPU向量化、锁定/等待原语。这些内容相当理论化，但我们还讨论并实施了包括OpenMP、OpenCL/GL等特定技术。
- en: All of them were a pain in the ass.
  id: totrans-split-12
  prefs: []
  type: TYPE_NORMAL
  zh: 所有这些都让我很头疼。
- en: Then, still fairly early in the term, a former PhD student of the professor
    who now worked at NVIDIA came in to guest lecture on GPUs and CUDA.
  id: totrans-split-13
  prefs: []
  type: TYPE_NORMAL
  zh: 接着，在学期还相当早的时候，一位曾在教授门下攻读博士学位并现在在NVIDIA工作的学生讲解了有关GPU和CUDA的内容。
- en: It’s really difficult to explain to non-programmers—or even programmers who
    don’t deal with massive compute problems—just how night-and-day the difference
    is between CUDA and its alternatives.
  id: totrans-split-14
  prefs: []
  type: TYPE_NORMAL
  zh: 非程序员或者不涉足大规模计算问题的程序员很难解释CUDA和其替代方案之间的显著差异。
- en: Some of it has extremely nice primitives for standard math and atomic memory
    operations, while also offering easy explicit memory allocation… and a ton of
    other stuff that for most of my audience is completely meaningless. I’ll just
    say it gives you super pleasant tools.
  id: totrans-split-15
  prefs: []
  type: TYPE_NORMAL
- en: But beyond that, because NVIDIA controls CUDA *and* their GPUs, they control
    the entire stack. Software, firmware, and hardware. This is the same thing that
    gives Apple a killer advantage on their platforms. NVIDIA has the same thing,
    and even aside from programmer productivity/ease (which, as we’ve learned over
    the years, is itself huge), stuff in CUDA just magically works much better. Behind
    the scenes, CUDA performs significant optimizations.
  id: totrans-split-16
  prefs: []
  type: TYPE_NORMAL
- en: Additionally, as new GPUs come out, as they inevitably do, CUDA continues to
    be forward and backward-compatible (depending on the features you use), and you
    continue to reap the benefits of even better hardware and firmware that NVIDIA
    continues to pump out.
  id: totrans-split-17
  prefs: []
  type: TYPE_NORMAL
- en: One of the primary arguments about why the hardware is what matters in the AI
    compute war is that it’s easy to see why that is a “hard” barrier. If you can’t
    make the hardware, well, you’re just out of luck.
  id: totrans-split-18
  prefs: []
  type: TYPE_NORMAL
- en: However, if that were the case, we shouldn’t be seeing such tepid adoption (if
    it exists at all) of AMD in AI workloads. AMD is worse, sure, but it’s not *that*
    far behind, and if you’re stuck with massive waitlists and impossible-to-get NVIDIA
    GPUs, why wouldn’t you do AMD?
  id: totrans-split-19
  prefs: []
  type: TYPE_NORMAL
- en: The answer is CUDA.
  id: totrans-split-20
  prefs: []
  type: TYPE_NORMAL
- en: AMD has its own “drop-in” replacement for CUDA called **[ROCm](https://www.amd.com/en/products/software/rocm.html)**,
    which for the longest time didn’t even support floating-point operations (meaning,
    for non-technical folks, decimals). It has a lot of the same primitives, it just
    sucks.
  id: totrans-split-21
  prefs: []
  type: TYPE_NORMAL
- en: Even if you were an ambitious PhD in computer science who is also an outstanding
    programmer who somehow wants to write a new AI-related library in OpenCL or Vulcan
    or whatever, you still wouldn’t get the performance you could with CUDA (at least
    not without a massive amount of extra low-level programming effort).
  id: totrans-split-22
  prefs: []
  type: TYPE_NORMAL
- en: In reality, although I know this is strange to non-PhD/technical/academic readers,
    most PhDs in computer science, engineering, and statistics actually kind of suck
    at programming, unless their specialty is programming languages. They aren’t motivated
    by trying to do fancy stuff with writing code. Additionally, they have enough
    headaches and time pressure, why would they deliberately choose to make life harder
    on themselves?
  id: totrans-split-23
  prefs: []
  type: TYPE_NORMAL
- en: This is why a lot of bleeding-edge, prototype machine learning algorithms are
    written in and only available in R (which can make one internally scream, but
    I’ve been forced to get extremely familiar with it), which offers no performance
    benefits at all. But for stuff that is meant to run on “real” workloads to try
    to demonstrate cutting-edge performance on massive datasets, that’s in the CUDA
    ecosystem.
  id: totrans-split-24
  prefs: []
  type: TYPE_NORMAL
- en: One can liken it to Adobe/Microsoft/AutoCAD software being used extensively
    in educational institutions, and that makes them also very sticky in industry
    (which in turn reinforces why schools use them). Of course, it’s even more painful
    to switch from CUDA in this case.
  id: totrans-split-25
  prefs: []
  type: TYPE_NORMAL
  zh: 类似于Adobe/Microsoft/AutoCAD这些软件在学校中的广泛应用使它们在行业中也变得难以动摇，这反过来也强化了学校为什么使用它们的原因（学校亦受到这些行业的推动）。显然，从CUDA切换到其他平台所付出的努力会更甚于此。
- en: Tensorflow and PyTorch nowadays support platforms apart from CUDA and (in turn)
    NVIDIA GPUs. However, for the longest time, they didn’t—or, at least, they didn’t
    if you wanted to finish your workloads anytime this century and not run them on
    CPUs.
  id: totrans-split-26
  prefs: []
  type: TYPE_NORMAL
  zh: 当前，Tensorflow 和 PyTorch 等工具支持 CUDA 和 NVIDIA GPU 之外的平台。然而，很长一段时间里，这些工具并未提供这种支持—除非你计划在本世纪内使用非CPU平台完成工作。
- en: Now that these fundamental tools support other platforms may sound like it’s
    countering my argument, but it’s more to illustrate that years of tools, low-level
    libraries, and the like assumed that everyone was using NVIDIA GPUs.
  id: totrans-split-27
  prefs: []
  type: TYPE_NORMAL
  zh: 这些基础工具现在支持其他平台似乎在驳斥我的论点，但其实是为了说明多年的工具、底层库等都假设所有人都在使用 NVIDIA GPU。
- en: There were PhD blog posts/guides during the great GPU shortage in 2020-2022
    about buying used RTX 2090s for poor graduate students. **[Today, more updated
    guides still exclusively talk about NVIDIA GPUs](https://timdettmers.com/2023/01/30/which-gpu-for-deep-learning/comment-page-1/)**.
  id: totrans-split-28
  prefs: []
  type: TYPE_NORMAL
  zh: 2020-2022年大缺显卡时期，有人撰写了博士生(post-graduate students)的购买二手RTX 2090的指南。**[如今，更新的指南仍然只侧重介绍NVIDIA
    GPU**](https://timdettmers.com/2023/01/30/which-gpu-for-deep-learning/comment-page-1/)。
- en: I’ve heard arguments about how China has infinite money and resources to throw
    at conquering AI (… which is an assertion that is also debatable, especially now),
    they can just port everything to a new platform like **[Huawei’s CANN](https://github.com/opencv/opencv/wiki/Huawei-CANN-Backend)**.
  id: totrans-split-29
  prefs: []
  type: TYPE_NORMAL
  zh: 我曾经听到有人提出中国拥有无穷无尽的资金和资源来征服AI（这是一项争议性观点，特别考虑到现在的局势），他们可以将所有工具移植到一个像**[华为CANN](https://github.com/opencv/opencv/wiki/Huawei-CANN-Backend)**这样的新平台。
- en: Aside from the hand-wavy nature of the rather insane proposal (which I think
    is mainly “simple” only to hardware people) to just “port everything,” that still
    doesn’t solve the problem. The ***[leading edge](https://weightythoughts.com/p/compute-is-overrated-as-ais-bottleneck)***
    **[of AI software and models, which is](https://weightythoughts.com/p/compute-is-overrated-as-ais-bottleneck)**
    ***[really](https://weightythoughts.com/p/compute-is-overrated-as-ais-bottleneck)***
    **[what’s driving the AI revolution, not hardware/compute](https://weightythoughts.com/p/compute-is-overrated-as-ais-bottleneck)**,
    is still within the CUDA ecosystem.
  id: totrans-split-30
  prefs: []
  type: TYPE_NORMAL
  zh: '除了这个对“迁移一切”这样的疯狂提案的浮泛（其主要观点对于硬件工程师来说似乎是“简单”）指控外，这并没有解决问题。**[推动AI革命的真正动力在于AI软件和模型的前沿，而非硬件/计算**](https://weightythoughts.com/p/compute-is-overrated-as-ais-bottleneck)**，而且仍旧位于CUDA生态系统中**。   '
- en: If you’re ok with always lagging, sure, that’s ok, but if AI’s frontier moves
    as fast as everyone seems to think it will (which I do too), it’ll be similar
    to Moore’s Law pre-2010\. Being generations behind constantly is a *bad thing*
    in that case and not the key to any kind of technological innovation, let alone
    leadership.
  id: totrans-split-31
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你愿意一直在技术上滞后，那么这样做是可接受的，但如果AI前沿科技的发展速度像人们普遍认为的那样快（我也是这么认为的），这将类似于摩尔定律在2010年前的状态。一直落后好几代将是一个**糟糕的事情**，不是任何形式的科技创新的关键，更不用说是领导力的来源。
- en: '**[Chip War](https://en.wikipedia.org/wiki/Chip_War:_The_Fight_for_the_World%27s_Most_Critical_Technology)**
    has a great section on how the Soviet Union tried a “just copy/steal” strategy
    in semiconductors and fell hopelessly behind because of it. It’s a great theoretical
    idea to just copy/steal and fast-follow, but semiconductors, AI, and other “harder
    technologies” require building human and intellectual capital that will get better
    with time. From there, you need to have the prior generation to keep up with ever-increasing
    complexity and difficulty as these things get more advanced.'
  id: totrans-split-32
  prefs: []
  type: TYPE_NORMAL
  zh: '**[半导体战争](https://en.wikipedia.org/wiki/Chip_War:_The_Fight_for_the_World%27s_Most_Critical_Technology)**一文精彩地阐述了前苏联在半导体领域尝试的“复制或盗取”策略并因此彻底落后。简单复制或盗取并快速跟随的理论想法很好，但对于半导体、AI以及其他的“硬技术”，需要培养持续提升的人力与智力资本。在这一点上，需要上一代保持与年复一年增加的复杂度和难度同步。'
- en: The idea that NVIDIA is dominant isn’t exactly a controversial take **[immediately
    after a GTC that has been likened to a Taylor Swift concert](https://www.businessinsider.com/nvidia-ceo-taylor-swift-nod-shows-aware-of-tech-reputation-2024-3)**.
  id: totrans-split-33
  prefs: []
  type: TYPE_NORMAL
  zh: NVIDIA占主导地位的想法并不是一个有争议的看法 **[在被比作泰勒·斯威夫特演唱会的GTC之后](https://www.businessinsider.com/nvidia-ceo-taylor-swift-nod-shows-aware-of-tech-reputation-2024-3)**。
- en: However, the *reasons* continue to be the same, and I think are worth emphasizing.
    Will CUDA one day lose its advantage? Will Huawei’s CANN or various open-source
    projects like Vulcan be able to take away its sheer dominance in the AI field,
    and thus degrade NVIDIA’s absolute stranglehold on AI compute?
  id: totrans-split-34
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，*原因*依然如故，我认为值得强调。CUDA是否有一天会失去优势？华为的CANN或各种开源项目如Vulcan能否夺走其在人工智能领域的绝对主导地位，从而削弱NVIDIA在AI计算上的绝对控制力？
- en: Maybe. But I still haven’t seen a distinctive shift. The reasons aren’t in numbers
    or simplistic top-down thought experiments about how NVIDIA can lose its leadership.
  id: totrans-split-35
  prefs: []
  type: TYPE_NORMAL
  zh: 或许吧。但我还没有看到明显的转变。原因并不在数字或者关于NVIDIA如何失去领导地位的简单自上而下的思想实验。
- en: The reasons for NVIDIA’s dominance are years and billions of dollars in investment
    in the CUDA ecosystem, evangelism, and education of the community that builds
    AI. Even if it were fading, it would take quite a while for the impacts of such
    a heavy, long-term strategy to go away.
  id: totrans-split-36
  prefs: []
  type: TYPE_NORMAL
  zh: NVIDIA占主导地位的原因在于多年以及数十亿美元投资于CUDA生态系统，以及对建设人工智能的社区进行的传道、教育工作。即使它正在衰退，这样重的、长期的战略影响要消失还需要相当长的时间。
- en: As it stands, NVIDIA is only retrenching its strength within the space. And
    the reason (apart from interconnects—which, as said, I promise a piece at some
    point) is CUDA and its ecosystem.
  id: totrans-split-37
  prefs: []
  type: TYPE_NORMAL
  zh: 就目前而言，NVIDIA只是在自己的领域内巩固其实力。而原因（除了互联互通——正如我之前说过的，我承诺在某个时候会详述）是CUDA及其生态系统。
