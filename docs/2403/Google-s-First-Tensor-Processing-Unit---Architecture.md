<!--yml

类别：未分类

日期：2024-05-29 12:38:54

-->

# 谷歌的第一个张量处理单元 - 架构

> 来源：[https://thechipletter.substack.com/p/googles-first-tpu-architecture](https://thechipletter.substack.com/p/googles-first-tpu-architecture)

… 我们半开玩笑地说，TPU v1 “启动了一千片芯片”。

在[谷歌的第一个张量处理单元 - 起源](https://thechipletter.substack.com/p/googles-first-tensor-processing-unit)中，我们看到为什么以及如何谷歌在仅 15 个月内从2013年底开始开发了第一个张量处理单元（或 TPU v1）。

今天的文章将更详细地查看从该工作中产生的架构及其性能。

[分享](https://thechipletter.substack.com/p/googles-first-tpu-architecture?utm_source=substack&utm_medium=email&utm_content=share&action=share)

对于 TPU v1 项目的目标，这里有一个快速的提醒。当谷歌看到不仅能通过新的深度学习服务范围提供的机会，还有所需硬件的大规模和成本时，项目的目标将是……

> … 开发一种特定应用集成电路（ASIC），将在推理时与 GPU 相比提供 10 倍的成本性能优势。

并且……

> 快速建立
> 
> 实现高性能
> 
> ......在规模上
> 
> ... 用于全新的工作负载...
> 
> 同时还要保持成本效益

在我们更详细地查看从项目中产生的 TPU v1 之前，简要提醒一下给 TPU 其名称的张量运算。

> 为什么称之为张量处理单元？因为它被设计用于加速涉及张量的操作。确切地说，是哪些操作呢？这些操作被称为……“将不同对象（如向量、标量，甚至其他张量）之间的映射（多线性关系）”。
> 
> 让我们举一个简单的例子。一个二维数组可以描述两个一维数组之间的多线性关系。数学上倾向的人会认识到从一个向量到另一个向量的过程，即通过将向量乘以矩阵来获得另一个向量。
> 
> 这可以推广到表示高维数组之间关系的张量。然而，尽管张量描述了任意高维数组之间的关系，但实际上，我们将考虑的 TPU 硬件旨在执行与一维和二维数组相关的计算。或者更具体地说，向量和矩阵运算。

让我们看看其中一个操作，矩阵乘法。如果我们取两个 2x2 矩阵（2x2 数组），然后将它们相乘，以如下方式相乘矩阵元素来得到另一个 2x2 矩阵。

为什么矩阵乘法对神经网络的运算至关重要？我们可以看一个简单的具有四层的神经网络如下（仅显示每个后续层中第一个节点的连接，以简化）：

这里的 'f' 是 [激活函数](https://en.wikipedia.org/wiki/Activation_function)。

因此，隐藏层和输出层是对应于将激活函数应用于输入值向量与权重矩阵相乘的结果向量中的每个元素的结果。在数据输入的情况下，这相当于对结果矩阵中的每个条目应用激活函数。

* * *

正如我们所见，TPU v1 团队采用的方法是由 H.T Kung 和 Charles E. Leiserson 在他们 1978 年的论文《[Systolic Arrays (for VLSI)](https://www.eecs.harvard.edu/htk/static/files/1978-cmu-cs-report-kung-leiserson.pdf)》中首次提出的架构。

> 一个系统阵列是一个网络处理器，它们通过系统节奏地计算和传递数据。在系统阵列计算机系统中，处理器的功能类似于心脏的功能。每个处理器定期地将数据输入和输出，每次执行一些简短的计算，以便在网络中保持数据的常规流动。

那么，TPU v1 如何有效地使用系统阵列方法来执行矩阵乘法？让我们回到我们的 2x2 矩阵乘法的例子。

如果我们有一个由乘法单元组成的 2x2 网格连接的数组，并且以正确的顺序将要相乘的矩阵元素输入到网格中，则矩阵乘法的结果将自然地从数组中得出。

计算可以用下面的图表表示。每个角落的正方形代表一个乘积/累加单元（MAC），可以执行乘法和加法操作。

在这个图表中，黄色的值是从顶部和左侧输入到矩阵中的值。浅蓝色的值是存储的部分和值。深蓝色的值是最终的结果。

让我们一步一步来。

> *步骤 1:*
> 
> 值 a11 和 b11 被加载到左上角的乘积/累加单元（MAC）中。它们相乘的结果被存储。
> 
> *步骤 2:*
> 
> 值 a12 和 b21 被加载到左上角的 MAC。它们相乘，然后加到先前计算的结果中。这给出了结果矩阵的左上角的值。
> 
> 同时，b11 被传输到右上角的 MAC，与新加载的值 a21 相乘，并存储结果。此外，a11 被传输到左下角的 MAC，与新加载的值 b12 相乘，并存储结果。
> 
> *步骤 3:*
> 
> b21 被传输到右上角的 MAC，与新加载的值 a22 相乘，并将结果添加到先前存储的结果中。同时，a12 被传输到左下角的 MAC，与新加载的值 b22 相乘，并将结果添加到先前存储的结果中。在这一步中，我们计算了结果矩阵的右上角和左下角的值。
> 
> 与此同时，a12和b21被传输到右下角MAC，它们相乘，并存储结果。
> 
> *第四步：*
> 
> 最后，a22和b22被传输到右下角MAC，它们相乘，结果添加到先前存储的值中，得到结果矩阵的右下角值。
> 
> 因此，矩阵乘法的结果以MAC矩阵中的移动‘对角线’形式出现。

在我们的示例中，执行 2 x 2 矩阵乘法需要 4 个步骤，但仅因为一些 MAC 在计算的开始和结束时未被利用。实际上，一旦 MAC 空闲，新的矩阵乘法将从左上角开始。因此，该单元能够每两个周期进行一次新的矩阵乘法。

这是收缩阵列如何工作的简化表示，并且我们略过了在TPU v1中实现收缩阵列的一些细节。但我希望这个架构如何工作的原理已经清楚了。

这是最简单的矩阵乘法，但可以扩展到具有更大乘法单元阵列的更大矩阵。

关键点在于，如果按正确顺序将数据馈送到收缩阵列中，则值和结果通过系统的流动将确保所需结果随时间从阵列中出现。

关键的是，无需将中间结果存储和获取到‘主存储’区域。由于矩阵乘法单元的结构以及输入被馈送到单元的顺序，中间结果在需要时会自动可用。

当然，矩阵乘法单元并不孤立存在，完整系统的最简单展示如下：

首先要注意的是，TPUv1 依赖于与主机计算机通过[PCIe](https://en.wikipedia.org/wiki/PCI_Express)（高速串行总线）接口的通信。它还直接访问自己的DDR3动态RAM存储。

我们可以将这扩展到更详细的设计展示中：

让我们从这个设计展示中挑选一些关键元素，从顶部开始，（大体上）顺时针移动：

+   **DDR3 DRAM / 权重 FIFO：** 权重存储在连接到 TPU v1 的 DDR3 RAM 芯片中，通过 DDR3-2133 接口传输。权重从主机计算机的内存通过 PCIe “预加载”到这些芯片上，然后可以转移到‘权重 FIFO’存储器，以供矩阵乘法单元使用。

+   **矩阵乘法单元：** 这是一个带有 256 x 256 矩阵乘积/累加单元的“收缩”阵列，从顶部接收 256 个‘权重’值和从左侧接收 256 个数据输入。

+   **累加器：** 结果从收缩矩阵单元底部出现，并存储在‘累加器’内存存储器中。

+   **激活：** 上述神经网络中描述的激活函数在此应用。

+   **统一缓冲区 / systolic 数据设置：** 应用激活函数的结果存储在 '统一缓冲区' 内存中，准备好作为输入馈送回矩阵乘法单元，以计算下一层所需的值。

到目前为止，我们还没有明确指定矩阵乘法单元执行的乘法性质。TPU v1 执行 8 位 x 8 位整数乘法，利用量化来避免更多面积占用的浮点计算需求。

TPU v1 使用 CISC（复杂指令集计算机）设计，约仅有约 20 条指令。重要的是指出，这些指令是通过 PCIe 接口由主机计算机发送给它，而不是从内存中提取的。

五个关键指令如下：

***读取主机内存***

从主机计算机内存读取输入值到统一缓冲区上通过 PCIe。

***读取权重***

从计算机主内存通过 PCIe 读取权重，然后将这些权重加载到权重内存中。

***矩阵乘法 / 卷积***

根据论文的这一指令

> … 导致矩阵单元从统一缓冲区执行矩阵乘法或卷积到累加器。矩阵操作采用可变大小的 B*256 输入，将其与 256x256 的常数权重输入相乘，并产生 B*256 输出，完成 B 管道化周期。

这是执行 systolic 数组矩阵乘法的指令。它还可以执行卷积神经网络所需的卷积计算。

***激活***

根据论文的这一指令

> 执行人工神经元的非线性函数，包括 ReLU、Sigmoid 等选项。其输入是累加器，输出是统一缓冲区。

如果回顾我们简单的神经网络模型，隐藏层中的值是通过将权重乘以输入并应用 '激活函数' 而得出的。[ReLU](https://en.wikipedia.org/wiki/Rectifier_(neural_networks)) 和 [Sigmoid](https://en.wikipedia.org/wiki/Sigmoid_function) 是两种最流行的激活函数之一。在硬件中实现这些函数将大大加快激活函数的应用速度。

***写入主机内存***

通过 PCIe 从统一缓冲区向主机计算机内存写入结果。

* * *

值得暂停一刻来反思这五条指令在 TPU v1 推理中提供了几乎完整的实现的优雅性。在伪代码中，我们可以大致描述 TPU v1 的操作如下：

```
Read_Host_Memory
Read_Weights
Loop_Start
    Matrix_Multiply
    Activate
Loop_End
Write_Host_Memory
```

强调 systolic 单元在实现这一过程中的重要性和效率是非常有用的。正如 TPU v1 团队描述的那样（我们已经看到的）：

> .. 矩阵单元使用脉动执行来通过减少统一缓冲区的读写来节省能量... 它依赖于来自不同方向的数据定期到达数组中的单元，这些数据在那里被组合。 ... 数据从左侧流入，权重从顶部加载。 给定的256元素乘积累加操作通过矩阵以对角波前的形式移动。

TPU v1的硬件如果没有软件堆栈来支持它将毫无用处。 Google开发并使用Tensorflow，因此创建‘驱动程序’以便Tensorflow可以与TPU v1一起工作是主要步骤。

> TPU软件堆栈必须与为CPU和GPU开发的堆栈兼容，以便应用程序可以快速移植到TPU上。 在TPU上运行的应用程序部分通常使用TensorFlow编写，并编译为可以在GPU或TPU上运行的API。
> 
> 类似于GPU，TPU堆栈分为用户空间驱动程序和内核驱动程序。 内核驱动程序轻量级，仅处理内存管理和中断。 它设计用于长期稳定性。 用户空间驱动程序经常更改。 它设置并控制TPU执行，将数据重新格式化为TPU顺序，将API调用转换为TPU指令，并将其转换为应用程序二进制。

正如我们在早期文章中所看到的，TPU v1是由TSMC使用相对‘成熟’的28纳米TSMC工艺制造的。 Google表示，与此时期其数据中心中使用的更先进的Intel Haswell CPU和Nvidia的K80 GPU芯片相比，芯片面积小于其一半。

我们已经看到了TPU v1指令集的简单性，仅有20条CISC指令。 ISA的简单性导致TPU v1在解码和相关活动方面的‘开销’非常低，仅有2％的芯片面积专用于被标记为‘控制’的部分。

相比之下，24％的芯片面积专用于矩阵乘法单元，29％专用于存储输入和中间结果的‘统一缓冲区’内存。

在这一点上，值得提醒我们的是，TPU v1的设计目标是使推断 - 即在Google规模提供的现实世界服务中使用已经训练过的模型 - 更加高效。 它并不是为了改善训练的速度或效率。 尽管推断和训练在开发专用硬件时具有一些共同的特征，但提供了完全不同的挑战。

那么TPU v1表现如何呢？

在2013年，TPU v1的关键比较对象是Intel的Haswell CPU和Nvidia的K80 GPU。

至关重要的是，TPU v1比GPU更加节能：

在TPU v1的第一篇文章中，我们集中讨论了像Google这样的组织能够迅速集结资源来构建TPU v1的事实。

在本文中，我们看到了 TPU v1 的定制架构如何至关重要，使其在比当时的 CPU 和 GPU 能效更高的情况下，表现出更好的性能。

TPU v1 只是故事的开始。 TPU v1 设计迅速，唯一目标是使推理速度更快、更节能。 它有一些明显的局限性，并且不适用于训练。 Google 内外的公司很快开始研究如何改进 TPU v1。 我们将在后续文章中看一些它的后继产品。

支付墙之后，有关 Google TPU v1 的进一步阅读和观看的小部分选择。
