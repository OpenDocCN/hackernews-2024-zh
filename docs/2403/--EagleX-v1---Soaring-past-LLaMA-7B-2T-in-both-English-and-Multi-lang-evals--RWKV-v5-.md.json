["```\n## 15% Code \n\nContains code/programming related topics\n- the-stack\n- codeparrot\n- devopedia\n- mdn\n\n## 15% Multi lang\n\nGenerally multi-lang webtext\n- sea-lion (Singapore)\n- madlad\n- culturax\n- multi lang wiki\n\n## The giant soup\n\nCreative content\n- fandom (only sites with permissive licenses, and low spam)\n- scp-foundation\n\nWikipedia\n- Various Permissively licensed wikis.\n- wikipedia\n\nPapers:\n- Mainly arxiv (Permissive Licenses) and pes2o\n\nBooks:\nAll the books contained in out train sets are public domains books.\n- gutenberg, \n- standardebooks\n\nWebtext\n- webtext\n- refinedweb (Note: This chunk made the model worse, we recommend against refinedweb in future trains)\n- slimpajama\n- europarl\n- eurlex.\n- stackexchange\n\nVarious\n- aya (multilang convo)\n- some system prompt, instruct\n- long list of sub 100B training datasets on HF\n- rewritten text !!! (splicing in, to replicate the rewritten web paper)\n```"]