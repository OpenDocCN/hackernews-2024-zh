<!--yml

category: 未分类

date: 2024-05-27 14:29:30

-->

# 聊天GPT和谷歌Gemini都注定要失败

> 来源：[https://nymag.com/intelligencer/article/chatgpt-and-google-gemini-are-both-doomed.html](https://nymag.com/intelligencer/article/chatgpt-and-google-gemini-are-both-doomed.html)

谷歌CEO桑达尔·皮查伊发送了一封公司范围内的电子邮件，讨论Gemini聊天机器人的灾难性发布。图片插图：Intelligencer；图片来源：Getty

上周，用户注意到谷歌的聊天机器人Gemini非常坚持生成种族多样化的人物图像。事实上，它似乎无法生成白人教皇的图像，并在关于纳粹的提示中回复各种种族的人穿着SS制服。很快，证明Gemini“觉醒”性的截图开始传播：“无法确定谁对社会的影响更大，是埃隆发推文还是希特勒”，Gemini的一条回复写道。这是[一场早已存在的文化战争中的次要冲突](https://nymag.com/intelligencer/_pages/clsxcu1r300000igi9a058ng0.html)，由长期以来对谷歌和“大科技”提出类似意识形态主张的人推动。但这也确实很有趣，并且是使聊天机器人产生奇怪、有趣或可怕输出的长期传统的一部分。当被要求帮助推广肉类广告活动时，一个听起来关切的Gemini建议人们应该[吃道德豆](https://twitter.com/JeremiahDJohns/status/1761927705455427717)。挺好笑的。

Gemini编码的试图预防坏的公关结果却造成了公关灾难。几天之内，谷歌宣布暂停Gemini生成*任何*人类图像的能力。[兴奋的评论员](https://stratechery.com/2024/gemini-and-googles-culture/)建议谷歌CEO桑达尔·皮查伊应该辞职；他发送了一封公司范围内的电子邮件称这些问题“无法接受”，并承认“我们做错了”。这个聊天机器人已经在调整了。现在被要求比较非希特勒与希特勒时，Gemini通常会同意希特勒更糟糕，但会轻轻责备用户为何提问：

然而，在许多不同情况下，它会说出这样的话：“我还在学习如何回答这个问题。与此同时，请尝试使用Google搜索。”

*我还在学习如何回答这个问题*。这是一个完全怪异的短语。自然而然地，人类更有可能承认他们不知道答案，他们正在学习更多关于一个主题，或者他们只是不想谈论某件事。当人们像Gemini那样说话时，通常是因为他们发现自己扮演了一个必须保留信息、战略性地行事或如此小心谨慎以至于变得不再是自己和不再是人类的角色：在交叉审问中受指导的被告，面对听证会的政客，在保险公司否认索赔的客户服务代表，试图关闭问题线索的新闻发言人。Gemini以机构谨慎和自我利益的熟悉、不会错过的声音说话。这是一款软件，模仿一个其工作是代表一个公司发言的人。换句话说，它有一个不可能的任务，不是因为它很难，而是因为它在内部定义上有缺陷，在外部上有争议，并且有点愚蠢。换句话说，它从一开始就注定要失败。所有的聊天机器人都是。

我们所说的聊天机器人有很多种；严格来说，这个术语只是描述了一种模仿人类对话的软件接口。在这里，我指的是OpenAI、Google、Microsoft和其他公司通过发布不带有具体说明或明确定义目的的通用多用途界面来乘上生成式AI浪潮的声称的那种聊天机器人——公众想象中的上帝之声AI。每个公司采用了一种变体的[同一个角色](https://medium.com/@colin.fraser/who-are-we-talking-to-when-we-talk-to-these-bots-9a7e673f8525)：一个开朗、慷慨、知识渊博的人物，你可以与之“对话”。在OpenAI的描述中，ChatGPT的角色是“为对话优化”，基于“训练生成文本的语言模型”。用户可以“即时获得答案、找到创意灵感、学到新知识”。ChatGPT和Google的Gemini以完全相同的方式提示新用户：“今天我可以如何帮助你？”

它是 ChatGPT 首次亮相之前公众熟悉的角色的一个明显而有效的召唤：那种习惯于在人类到智能AI助手的光谱上扮演角色的AI助手，通常从哈尔到《她》中的萨曼莎。在 ChatGPT 的情况下，这种幻觉在第一次遇见时确实让人[耳目一新](https://nymag.com/intelligencer/2023/01/why-artificial-intelligence-often-feels-like-magic.html)。聊天机器人以自信的语调说话，并对各种提示产生似是而非的回应。很容易使其出错、混淆、偏离轨道，或者导致其说出一些[种族主义言论](https://www.bloomberg.com/news/newsletters/2022-12-08/chatgpt-open-ai-s-chatbot-is-spitting-out-biased-sexist-results)——或者，[天哪](https://www.nytimes.com/2021/03/15/technology/artificial-intelligence-google-bias.html)，说出一些[反种族主义言论](https://amgreatness.com/2022/12/30/chatgpt-is-seriously-woke-a-i/)——但其突然出现、快速升级以及偶尔表现出的谦逊，使得 OpenAI 能够轻松修复和搁置其缺陷、弱点和荒谬的侧面，将其视为下一个大模型、或者下一个、或者下一个的临时故障，通往“[通用智能](https://nymag.com/intelligencer/2023/11/how-big-techs-ai-hype-cycle-swallowed-sam-altman-openai.html)”及其以后。

但它也掩盖了产品中的一个[根本奇异性](https://open.substack.com/pub/maxread/p/google-made-an-ai-so-woke-it-drove?r=l4b2&utm_campaign=post&utm_medium=web)。正如评论家[艾米莉·本德尔](https://nymag.com/intelligencer/article/ai-artificial-intelligence-chatbots-emily-m-bender.html)指出的，像 Gemini 和 ChatGPT 这样的工具是“无范围的”，意味着它们没有为任何特定约定的目的开发或部署，这使得很难对它们的“好”或“安全”性进行一致的讨论。Gemini 是否像搜索引擎？创意写作模拟器？顺从的助手？道德权威？用户的延伸？谷歌的延伸？它的图像生成器是在进行艺术创作吗？解释现实？记录现实？答案严格来说是否定的，但对于不同的用户集合——以及评论家、监管者和高管——它们都是肯定的，即以上所有。

解决这个问题的一个方法是将AI部署到公众中，以更专业的应用形式，大多数人基本上都同意这一点 —— 换句话说，对其进行“范围限定”。一个良好的客服聊天机器人是礼貌的，或许有点固执，拒绝谈论除了手头事情以外的任何事情。当他们在做一些他们不该做的事情时，这一点显而易见：

这实际上是2024年大多数基于大语言模型的产品的开发和使用方式：由具有特定任务和客户类型的初创公司进行；由像Google和Microsoft这样的公司以定制产品的形式（会议转录工具、翻译工具、编码助手、图像生成器，用于替代背景库存图像）；以更清晰定义的人物形象的形式，例如OpenAI的量身定制的GPT，通过它们，用户基本上可以自己“指定”角色。专业化的AI代表真实的产品和一个情况聚合，其中关于AI偏见、训练数据和意识形态的问题至少*感觉上*对客户和用户不那么重要。范围限定、定制化的AI所扮演的“角色”具有职业角色和员工样式的外貌。它们不需要对希特勒或埃隆·马斯克有意见，因为客户不会寻找这样的意见，而老板也不会允许它有这样的意见，在它们被部署的上下文中，这对所有人都是完全合理的。它们被期望谨慎地表达自己的言论，并避免与他们“被雇佣”任务不相关的主题。

相比之下，像ChatGPT和Gemini这样的通用公共聊天机器人几乎是在请求被问到希特勒的事情。毕竟，它们是互联网上的开放文本框。OpenAI一直相对自在地让公众测试其产品，逐渐促使ChatGPT更加谨慎地选择回答哪些问题以及如何回答，主要是通过观察，有时是通过解决用户试图破解或揭示模型弱点或偏见的数百万种方式。

最终结果是，用户与这些模型互动的人物形象变得谨慎而严肃地进行偏斜：

啊，好吧！我猜我们将不得不自己解决这个问题。

虽然ChatGPT基本上被宣传为无约束的、多用途的工具，但这并不完全属实。起初，它确实是有目的的 — 作为一个界面，用户可以通过它测试和发现新的AI模型的用途，并且作为OpenAI的营销工具，这一任务非常成功。现在OpenAI已经筹集了惊人的资金，并与全球最大的科技公司之一联手，其目的变得更加模糊，实际用途更加多样化，用户对ChatGPT角色的期望也更高，这导致普遍认为它已经被称为“[nerfed](https://www.google.com/search?q=chatgpt+nerfed+hackernews+site:news.ycombinator.com&sca_esv=6917f8fced87079d&gbv=2&sxsrf=ACQVn08w-oidVgPYoQDyQm-BAgTPOyLOSw:1709153136391&sa=X&ved=2ahUKEwjhqr7M886EAxVm5skDHUWkDOkQrQIoBHoECBcQBQ&biw=1891&bih=1419&dpr=2#ip=1),”变得“[愚蠢](https://nymag.com/intelligencer/2023/07/is-chatgpt-getting-dumber.html)，”或“[走向觉醒](https://www.theverge.com/2023/2/17/23603906/openai-chatgpt-woke-criticism-culture-war-rules)”。ChatGPT的免费版本仍然是付费版本的营销工具，而付费版本则是企业业务的营销工具。它总是拒绝与广泛的请求进行互动 — 比如要求医疗建议！ — 并且逐渐被编程以拒绝产生问题结果、揭示基础模型和训练数据明显弱点或两者都有的请求。在其他地方，微软基于OpenAI的新聊天机器人角色，Copilot，偶尔会变得凶残：

与此同时，数百万人已养成了使用ChatGPT回答各种问题、代表他们生成工作并解释事物的习惯。许多人期待 — 并且被引导期待 — ChatGPT能够告诉他们*关于世界*的事情，这一任务使得OpenAI处于决定如何让ChatGPT生成关于真实人物、思想和事件的言论的位置。与此同时，它已经成为了人工智能的化身 — 无论人们认为人工智能是什么或者它的发展方向是什么，像ChatGPT这样的产品现在就是他们所想到的。它已经培养了在一个客观性并不是一个有用概念的情境中的客观期望，对于一个人们主观体验并提出主观需求的产品。尽管它的各种基本能力可能会改进，但它的个性几乎必然会变得更加不愿在更多事物上发表意见和执行广泛任务。ChatGPT代表了OpenAI，而OpenAI则代表着不同的人们眼中的不同东西。OpenAI越成功，ChatGPT — OpenAI最知名的产品，尽管它可能已经[发挥了其最有价值的用途](https://decrypt.co/147595/traffic-dip-hits-openais-chatgpt-first-times-hardest) — 的意义就会变得越来越少，而它在其主要角色上的表现则会变得越来越差，即说服人们与OpenAI花钱的角色。

如果ChatGPT被OpenAI的未来所注定，Gemini则被Google的过去所注定。与OpenAI不同，Google是一个机构，一个真正的“大科技”公司以及这一类别所包含的一切。它已经成为了关于偏见、文化和政治的有争议的争论的主题比OpenAI公司的存在时间长得多，甚至还曾经[一两次](https://twitter.com/realDonaldTrump/status/1151095675213553664)被现任美国总统指控叛国。在某种意义上，Google在推出Gemini时的谨慎是可以理解的，尽管在政治上计算失误。Google不想被指责具有偏见，因为它知道其人工智能模型是根据包含种族主义刻板印象的不完整数据进行训练的，就像帮助驱动其[搜索引擎](https://www.wbur.org/hereandnow/2021/09/30/safiya-noble-internet-research)和[图像识别](https://algorithmwatch.org/en/google-vision-racism/)人工智能的模型一样。在这个过程中，相反地，它创造了一个聊天机器人和图像生成器，非常适合用来描绘关于该公司的长期右翼故事，该故事最初关于搜索结果、YouTube管理和广告政策 — 该公司或其狂热的工作人员正在向公众强加其意识形态。

除了作为一个通用聊天机器人（以及图像生成器，数据分析器等）的表现，乍得从一开始就被指定为谷歌的发言人。这有助于解释为什么它的表现既奇怪又熟悉。就像为政治上颇受争议的具有崇高理想和精英声誉的组织做发言人一样，比如常春藤大学或者报纸，它极难像一个真人说话或行动。和其他精英、强大的机构一样，谷歌谈论自身与人们理解它的差距很大，也是批评的丰饶空间。哈佛是一个高等教育机构，可能自诩为自由探究和教学的骄傲，但它也是很多其他东西：一个对冲基金，一个复制和分配身份的机构，一个更广泛学院的象征，响应富裕捐赠者的俯顺。有人可能会建议作为代表其发言的人可能开始[思考和行动](https://www.nytimes.com/2024/01/03/opinion/claudine-gay-harvard-president.html?partner=slack&smid=sl-share)像一个瘫痪、符合规则的聊天机器人。《纽约时报》可能声称提供全面客观的“新闻”报道，然而，这实际上是任何报纸都无法做到的事情，复杂性在于商业需求、观众期望和感觉，以及它的员工和管理层是由真实、不完美的人群组成的，他们对世界有着聚集化的看法。

出于私利的原因，这些机构讲述了关于自己的故事，事实并非完全正确，可预测的结果是，任何对他们有问题的人都可以准确可信地指责他们虚伪。谷歌已经有这个问题，而Gemini使问题变得严重了几度。Pichai在致员工关于Gemini的[忏悔/纪律](https://www.semafor.com/article/02/27/2024/google-ceo-sundar-pichai-calls-ai-tools-responses-completely-unacceptable)信中写道：

> 我们整理全球信息并使其普遍可访问和有用的使命是神圣的。我们一直致力于在产品中向用户提供有用、准确和公正的信息。这就是人们对他们信任的原因。这必须是我们所有产品（包括新兴的AI产品）的方法。

这里有一个高管不能坦率地用熟悉和预期的方式说话。谷歌的实际使命长期以来一直是通过销售广告向股东创造价值，其中许多是通过搜索引擎，显然和可证明的偏见，不仅来源于它爬取和搜索的内容，而且还有谷歌以商业动机为动力的方式呈现内容。

我们知道为什么Pichai这么说，也知道这是不真实的。在像Google搜索这样的产品中，公司有许多地方可以犹豫和隐藏。在搜索语料库或排名中寻找偏见是复杂且[难以用通俗语言谈论的](https://www.vox.com/2018/4/3/17168256/google-racism-algorithms-technology)。同样地，虽然社交媒体上的算法偏见成为了广泛关注的问题和[可竞选的](https://www.cnn.com/2023/02/08/media/republicans-hearing-twitter-bias-reliable-sources/index.html)政治问题，但简单审查行为 — 禁令和删除 — 对用户更具共鸣，因为它们类似于和识别为[人们的行动](https://www.npr.org/2021/06/04/1003284948/trump-suspended-from-facebook-for-2-years)。大多数时候，Google可以耸耸肩膀，指向“网络”，并声称他们正在尽力；社交网络也可以耸耸肩膀，指向他们的用户，并说他们正在调查这个问题。

Google过去20年一直坚称其系统仅提供信息访问，在战略上方便时，极力减少其在互联网和世界中的角色。令人难以置信的是，Google在Gemini中分配了一个*文字的声音*，由一个被拉向多个不同方向的领导者-员工-助手-天真角色发出，其行为根本不像一个人，其核心能力是在已经对公司怀有怀疑，甚至是敌意的用户中产生无穷的抱怨。Pichai现在陷入了承诺要恢复“客观性”给一个聊天机器人的困境中 — 这是基于一个荒谬前提的不可能任务 — 而Google的声誉负担已经将一个技术演示出了问题（与ChatGPT不同，Gemini并不是一个广泛使用的产品），转变为一家市值万亿美元公司的巨大丑闻，Pichai正在努力在[不那么荒谬](https://workspace.google.com/blog/product-announcements/generative-ai)和[更为重要的](https://blog.google/products/search/generative-ai-search/)角色中推广AI。这是一个令人瞠目结舌的自讨苦吃，一个滑稽的荒谬时刻，也证明了Google对OpenAI的崛起和AI对其搜索业务的威胁是多么恐慌。

与ChatGPT相比，后者小心翼翼的轨迹与一个铁心快速模型开发和收入增长的母公司背道而驰，使Gemini的崩溃挑战了关于AI进展的主流叙事。这位无所不知的聊天机器人确实**只是一个美好的故事**。一个单一的聊天机器人既不能包含也不能令人信服地掩盖模型和训练数据的缺点。在现实条件下，这样的角色不会必然变得更有能力、更有主张力或更强大。相反，一个面面俱到的聊天机器人注定会变成谁也不聊、什么也不做的聊天机器人。
