<!--yml

category: 未分类

date: 2024-05-29 12:37:54

-->

# 十年的网络爬取：关于销售网络数据的视角

> 来源：[https://substack.thewebscraping.club/p/selling-web-scraped-data](https://substack.thewebscraping.club/p/selling-web-scraped-data)

在过去的几天里，我多次思考我在网络爬取行业的旅程，这一旅程始于十年前，至今仍在继续。这段旅程很不平凡，所以请允许我与你分享我在网络爬取这十年的个人视角，同时也展望未来。

我和

, [Databoutique.com](https://www.databoutique.com/) 的其他联合创始人，曾是意大利一家四大咨询公司的同事，从事所谓的商业智能工作。

我们大约在2008年左右认识彼此：那时，对于一家银行来说拥有数据仓库（尽管不是最关键的，但对市场部门来说是必需品）意味着花费六位数购买硬件和软件许可证，从Oracle到ETL再到数据可视化工具。此外，还需要添加设计和构建所有内容的顾问费用。

经过几年的不懈努力，我们开始意识到除了公司内部数据外，网络上还有大量数据流可供利用（至少是我们意识到的）。

随后，我们开始了房地产网站的爬取项目，与今天的挑战相比，当时真是简单得不能再简单了。

只是提醒一下：AWS EC2在2008年底才刚刚推出，我们花了一些时间才明白这是什么云计算。

在此之前，我们的爬取基础设施是一个Hetzner服务器，所有爬虫都在上面运行，并有一个数据库用于存储解析后的数据。就是这样。

没有代理，也没有轮换IP，但它确实有效。

软件也同样简陋。Scrapy的最初版本可以追溯到2008年，但我们花了一些时间才发现它，因此我们编写的每个爬虫都是一个C++程序，使用cURL进行请求，并包含一些其他指令来解析数据。

尽管架构简单，但在转换为更现代化的架构之前，这种方式持续了2-3年，足以覆盖西方世界的房地产市场。

这些年来，技术栈和目标网站都发生了变化，跟随着网络技术的演进以及网络爬取技术的反思。

* * *

我将作为嘉宾在[NetNut本周四的网络研讨会](https://netnut.io/lp/website-unblocker-webinar/?utm_source=pier)上谈论网络爬取的挑战，届时还将推出他们的新网站解锁工具，该工具应能够绕过大多数常见的反机器人系统。

**[点击此处免费注册，我们在那里见！](https://netnut.io/lp/website-unblocker-webinar/?utm_source=pier)**

* * *

尽管解决方案有些天真，我认为我们可以同时抓取50多个网站，创建一个来自世界各地的列表庞大数据库。

这个数据库的销售数量？0

虽然技术栈随时间改变，但在这些年里有一件事是没有改变的：**出售网页数据真TM难。**

在我们与 Re Analytics 的冒险中，我们的数据工厂公司，Andrea 和我探索了不同的方法来赚取网页数据，我们可以将它们分为两大类：向金融界出售替代数据和向零售行业出售网页数据。

虽然在这两种情况下我们都取得了某种程度的成功，但我们也可以尝试其中背后的痛苦。

替代数据是从非传统来源收集的数据。对投资者来说，这通常意味着超越公司申报和经纪人说法，以在市场上获取优势。它们可以是信用卡交易、收据收集，当然也包括网页抓取。

我们两个都没有为对冲基金工作过，所以多年来我们必须学习如何成为一个优秀的数据供应商，我可以用这个公式总结：

\(准确性 + 数据历史 + 独家性\)

这意味着您需要建立一个获取高质量数据的过程（并最终学会以最佳方式填补其中的空缺），并且需要足够长的时间来能够使用您的数据对任何模型进行回测。一旦您拥有了它，只需将其出售给几个客户，否则，他们获得的*Alpha*将大大降低。

让我进一步发展这个想法，简化一些概念：金融市场是前瞻性的。投资者希望了解现在在未来几天、几周或几个月会发生什么，因此他们正在寻找一些数据来帮助他们解释某个上市公司或市场的走势。

例如：根据信用卡交易的某种趋势和与前几年的比较，他们可以了解消费品行业在下一个圣诞季节是否会遭受影响。

另一个例子，这次来自网页数据，是这样的：通过监控某个网站多年的库存水平，您可以了解单个品牌或行业在官方提交公司申报之前在销售方面的表现如何。

* * *

* * *

但与信用卡或其他专有数据提供不同，除了网站之外，没有人对数据负责。这意味着网站可能会改变，并且当然，他们不会提前通知你。因此，您需要尽快修复损坏的抽取器，提供尽可能高的***准确性***，因为任何间隙或错误的数据可能会降低建立在时间序列之上的金融模型的效果。

而且所有这些工作至少需要进行一两年，因为分析师需要使用您的数据进行回溯测试其模型，以了解它是否与股票价格有某种相关性。是的，甚至可能发生最糟糕的情况，您收集了多年的数据，只是发现它与股市没有任何有意义的相关性。只有在获取足够的***数据历史***开始实验之后，您才会知道这一点。

最后但并非最不重要的是，假设您为投资者找到了有意义的内容，并且足够好（也足够幸运）以能够连续几年抓取数据。现在是时候将其货币化了：在任何其他业务中，一旦您创建了产品，您就希望将其销售给尽可能多的客户。不幸的是，在替代数据领域，除非您的数据集变成必备数据集，否则您无法这样做。原因很简单：这就像与朋友分享一个秘密。只有少数人知道这个秘密时，它才更有价值；而如果每个人都知道它，它就会变成公开的，不再有趣。少数人购买您的数据集，您的客户就会享有更大的优势，因为他们比竞争对手掌握更多信息。尽管***独家性***对于您的客户至关重要，但对于您的业务来说，却不那么好。

实际上，让我们完整地列出原始方程式：

\(准确性 + 数据历史 + 独家性 =（少数客户 * 良好合同）/ 高资本支出 = 非可扩展业务\)

虽然对我们来说这提供了一个不错的收入来源，但我们不能说替代数据应该成为我们活动的唯一核心。

您有少数客户支付了大量资金，但创建这类数据集的成本非常高，以至于很难仅依靠它们来建立可扩展且盈利的业务。

作为这一理论的确认，该领域成功的企业是那些掌握大量专有数据的公司，例如支付处理器、在线旅行社等，他们找到了一种几乎不费力地获得这些数据并可靠地货币化的方法。

相反，我心中没有任何一家公司是通过网页抓取创建替代数据，并在多年内实现了超过十/二十亿美元的收入。但这是我的个人看法，我很乐意被证明错误。

参与网页数据提取的公司的另一个主要收入来源是将其出售给零售公司：品牌、在线平台等等。

但是，仅销售原始数据通常是不够的：有些公司未准备好从第一天开始接收数据源，并且在许多情况下，从数据中得出的洞见距离原始水平太远，需要算法、模型以及专注于特定领域的公司的所有专业知识。

在这种情况下，销售网页数据的方程式如下：

\(数据收集 + 专有专业知识 + 定制化\)

对于我们Re Analytics来说，这也是真实的：我们正在努力提取网络数据，并将这些年来积累的专业知识添加进去，以创建一种Saas，供我们在时尚行业的客户购买。

问题在于，每个客户都希望访问不同的网站，并且在放置焦点的位置有不同的需求：有人希望获得有关他们孩子系列的更多见解，有人则关注折扣，几乎每个新客户都希望访问我们目前不涵盖的新网站。

而不是销售产品，我们在销售定制服务，但是让我问你一个问题：你们中有多少人去过Zara或H&M，有多少人相反去过专业的裁缝那里定制过套装？

我想我知道答案了，我们都知道如今Zara或H&M有多大，而工匠们可以靠他们的工作谋生，但无疑无法在收入方面与他们竞争。

原因相当简单：定制的西装成本很高，因为其中蕴含了大量的工作和专业知识，它可能也是世界上最好的西装，但与Zara的5美元T恤相比，能购买它的人很少。

即使在这种情况下，我们也可以按照以下方程式完成：

\(数据收集 + 专有专业知识 + 定制 = (少数客户 * 良好合同) / 高运营成本 = 不可扩展的业务 \)

相反地，我依然相信网络爬虫有着巨大的潜力，主要因为今天我们离大规模采用还有很大距离，正如之前所述的原因。

没有一家公司不至少尝试过内部进行一些网络爬取，但考虑到实际挑战和数据源维护的需求，几乎每个内部项目都失败了，而市场上当前的解决方案对大多数公司来说都太昂贵。

凭借这些因素的结合，存在着大量未开发的潜在客户群体，这使得理解整体网络爬取市场潜力变得更加困难。

确实，有一件事是肯定的：如今不再像10年前那样有空间留给天真，今天的网络爬虫需要专业人士，他们知道如何应对当今不同的挑战。

公司不能认为可以将这些任务交给内部的普通IT人员来完成：他们需要创建爬取团队，或者如果没有足够的空间，寻求专业人士的帮助。

尽管这对最终客户来说是真实的，但我们学到的这个教训也同样适用于像Re-Analytics这样依赖实际网络数据的公司。

如果你正在构建一个AI代理、一个SAAS或者其他什么，你应该专注于它，而不是数据收集。这种现实检验和对我们业务的增强认识，去年导致我们将Re Analytics转型为Databoutique.com，一个用于销售网络爬取数据的市场。

在 Re Analytics，我们在过去的十年里被迫成为网页抓取的专家，尽管如此，我们意识到我们能够依靠我们的力量取得成功。我们不得不拒绝涉及新网站收购的较小项目，因为相比之下，付出的努力太多而回报太少。我们从潜在客户那里无数次收到拒绝，因为我们需要获取新的网站和数据无法及时准备好。我们花费了大量的精力和时间来维持前端和数据采集管道的运行，以至于在两个方面的任何改进都很难实现。

通过将数据生产与客户的价值主张分离，每家公司都可以更专注于其核心业务。假设你的优势在于为客户从数据中提取价值，例如AI代理、仪表板、SAAs或其他形式，那么你就不再需要担心数据采集了。例如，在Databoutique上，你可以浏览目录，看看是否有你需要的网站：如果有，你可以直接花几块钱购买数据集，并根据你希望的频率要求刷新。如果没有，只需提出请求，有人会为你完成。但是，如果你希望将你的爬取操作保留在内部，你可以简单地将你的报价与其他网站集成，同时作为市场上的卖家，为你的公司增加一条收入流。

或者，如果你现在刚刚开始你的公司，需要网页数据，你可以直接购买数据来验证你的想法，而不是制作爬虫，在成本的一小部分上进行。这比学习如何爬取或者简单地在Upwork上询问要便宜得多。

因为在卖方这边，网页抓取不再是手工艺性的工作，而是产品化的，以服务更多客户。这种工业化使他们能够将提取成本分摊给更多的买家，使价格低廉到比内部操作更为便捷。

作为数据卖家，销售方程式看起来像这样：

\(\begin{cases} LowSellingPrice * Many Customers > HigherPrice * Single Buyer \\ LowSellingPrice * Many Customers = MoreMoneyForYou+More Money For Tools = More Reliable Data Feeds \end{cases} \)

理论上，对于卖家来说，以公平市场价格销售更多数据集比单独从事自由职业更为便捷，而这种额外收入使他们能够投资更多工具并创建更可靠的数据源。

对于买家来说，方程式将以以下方式改变：

\(Affordable Datasets = (MoreDatasetBought * More Customers) / Fewer Data Acquisition Costs \)

Databoutique 的角色是吸引更多的数据买家进入平台并使他们满意，从而保持他们的购买行为。这样一来，越来越多的卖家赚钱，越来越多的数据集可用，使得Databoutique对买家更具吸引力，形成了一种良性循环，使网络抓取行业更加高效和有利可图。

我们几个月前刚刚开始，收到的反馈非常好。如果你同意一个更高效的网络抓取行业是可能的这一观点，我请求你一个忙：请与需要网页数据的朋友或同事分享这篇文章，并与他谈论一下Databoutique。

[分享](https://substack.thewebscraping.club/p/selling-web-scraped-data?utm_source=substack&utm_medium=email&utm_content=share&action=share)

这将帮助我们和已经参与项目的所有网络抓取专业人员成长，并吸引新的参与者加入。

感谢！

* * *
