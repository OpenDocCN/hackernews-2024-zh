<!--yml

category: 未分类

date: 2024-05-27 14:46:13

-->

# AI生成的数据可能会对未来的AI模型产生负面影响 | Scientific American

> 来源：[https://www.scientificamerican.com/article/ai-generated-data-can-poison-future-ai-models/](https://www.scientificamerican.com/article/ai-generated-data-can-poison-future-ai-models/)

多亏了[生成人工智能](https://www.scientificamerican.com/podcast/episode/why-were-worried-about-generative-ai/)的蓬勃发展，能够生成文本、计算机代码、图像和音乐的程序已经轻易地供普通人使用。而我们已经在使用它们：AI内容正在[占领互联网](https://www.wsj.com/articles/chatgpt-already-floods-some-corners-of-the-internet-with-spam-its-just-the-beginning-9c86ea25?mod=tech_lead_pos6&mc_cid=987d4025e9&mc_eid=74dd22853c)，并且由“[大语言模型](https://www.scientificamerican.com/article/what-the-new-gpt-4-ai-can-do/)”生成的文本填满了包括CNET和Gizmodo在内的数百个网站。但是随着AI开发人员从互联网上抓取信息，AI生成的内容很快可能会进入用于[训练新模型](https://www.scientificamerican.com/article/why-we-need-to-see-inside-ais-black-box/)以模仿人类反应的数据集中。一些专家称，这将无意中引入随着每一代模型逐渐积累的错误。

支持这一观点的证据越来越多。它表明，即使是少量的AI生成文本训练，最终也会对正在训练的模型产生“毒害”。目前很少有明显的解毒剂。“虽然现在或者说在未来的几个月内可能不是问题，但我相信在几年内将成为一个考虑因素，”苏格兰爱丁堡大学信息学院的计算机科学家Rik Sarkar说道。

[https://www.youtube.com/embed/ZWvTr5wKGCA](https://www.youtube.com/embed/ZWvTr5wKGCA)

视频

* * *

## 支持科学新闻报道

如果您喜欢本文，请考虑支持我们屡获殊荣的新闻报道，通过[订阅](/getsciam/)来确保我们今天世界上形成的发现和思想的未来。

* * *

AI 模型可能自我污染的可能性有点类似于某个二十世纪的困境。在第二次世界大战结束后第一批原子弹爆炸之后，数十年的核试验使地球大气中充满了放射性尘埃。当这些空气进入新制造的钢材时，就会带来升高的辐射。对于特别对辐射敏感的钢材应用，如盖革计数器控制台，这种尘埃带来了一个明显的问题：盖革计数器不能自我检测。因此，人们开始争相获取日益减少的低辐射金属供应。搜寻者从旧沉船中回收前战前钢材的碎片。现在，一些内行人士认为，在生成式 AI 领域，一个类似的循环即将重演，只不过这次是在训练数据而不是钢材上。

研究人员可以看到 AI 自我毒化的过程。例如，从一个以人类生成数据为训练基础的语言模型开始。使用该模型生成一些 AI 输出。然后使用该输出来训练一个新实例的模型，再使用结果输出来训练第三个版本，依此类推。每个迭代中，错误会堆积在一起。第十个模型，被要求写关于历史英国建筑的内容，竟然输出了关于丘兔的胡言乱语。

“到了某个程度，你的模型几乎是无意义的，”牛津大学的机器学习研究员 Ilia Shumailov 表示。

Shumailov 和他的同事称这种现象为“模型崩溃”。他们观察到这种现象发生在一个名为 OPT-125m 的语言模型上，以及另一个生成看起来像手写数字的 AI 模型，甚至是一个试图分离两个概率分布的简单模型上。Shumailov 表示：“即使在最简单的模型中，这种现象已经发生。”“我向你保证，在更复杂的模型中，这种现象已经百分之百发生了。”

在最近的预印本研究中，Sarkar 和他在马德里和爱丁堡的同事们进行了一项类似的实验，使用了一种名为扩散模型的 AI 图像生成器。他们的系列中的第一个模型可以生成可识别的花朵或鸟类图片。到了第三个模型，这些图片已经变得模糊不清。

其他测试显示，即使是部分由 AI 生成的训练数据集也是有毒的，Sarkar 表示。“只要有一定比例的 AI 生成内容，就会成为问题，”他解释道。“现在，究竟需要多少 AI 生成的内容才会在什么样的模型中引起问题，这是需要进一步研究的。”

两个研究小组都进行了相对谨慎的实验，使用的是相对较小、使用的训练数据较少的程序，而不是像语言模型 GPT-4 或图像生成器 Stable Diffusion 那样的大型模型。可能更大的模型会更抗拒模型崩溃，但研究人员表示没有太多理由相信这一点。

到目前为止的研究表明，模型在其数据的“尾部”可能会受到最大的影响——这些数据元素在模型的训练集中出现频率较低。由于这些尾部数据包括远离“正常”数据的数据，模型的崩溃可能导致AI的输出失去研究人员认为是人类数据的独特性。尤其是Shumailov担心这会加剧模型对边缘群体现有偏见的情况。“很明显，未来模型会变得更加偏见”，他说。“必须付出明确的努力以限制它。”

或许这一切都是猜测，但AI生成的内容已经开始进入机器学习工程师依赖的领域。以语言模型为例：甚至主流新闻媒体[已经开始发布AI生成的文章](https://www.wired.com/story/cnet-published-ai-generated-stories-then-its-staff-pushed-back/)，还有一些维基百科编辑[希望使用语言模型](https://www.vice.com/en/article/v7bdba/ai-is-tearing-wikipedia-apart)来为网站生成内容。

“我觉得我们正处于一个转折点，很多我们用来训练这些模型的现有工具很快就会被合成文本所饱和”，瑞士洛桑联邦理工学院（EPFL）的研究生Veniamin Veselovskyy说道。

还有一些警示信号表明，AI生成的数据可能会从其他地方进入模型训练中。机器学习工程师长期以来一直依赖于众包平台，如亚马逊的Mechanical Turk，来注释他们模型的训练数据或审查输出。Veselovskyy及其EPFL的同事们要求Mechanical Turk的工作者总结医学研究摘要。他们发现大约有[三分之一的总结带有ChatGPT的痕迹](https://arxiv.org/abs/2306.07899)。

EPFL团队的工作上个月在预印本服务器arXiv.org上发布，仅研究了来自机械土耳其工作者的46个回应，总结是一个经典的语言模型任务。但是这一结果引起了机器学习工程师们的一种担忧。“使用ChatGPT标注文本数据要容易得多，而且结果非常好”，EPFL的研究生Manoel Horta Ribeiro说道。像Veselovskyy和Ribeiro这样的研究人员已经开始考虑保护众包数据的人性化方法，包括调整Mechanical Turk等网站的方式，以防止用户转向语言模型，并重新设计实验以鼓励更多的人类数据。

针对模型崩溃的威胁，一名倒霉的机器学习工程师该如何应对？答案可能相当于盖革计数器中的战前钢铁：已知不受生成人工智能影响（或者尽可能不受影响）的数据。例如，Sarkar建议采用由了解其内容仅由人类创作组成并且开放供开发人员使用的“标准化”图像数据集的想法。

一些工程师可能会试图打开互联网档案库，并查找AI繁荣之前的内容，但Shumailov认为回溯历史数据并非解决方案。首先，他认为可能没有足够的历史信息来满足增长模型的需求。另外，这些数据只是历史性的，并不一定反映变化中的世界。

“如果你想收集过去100年的新闻并试图预测今天的新闻，显然行不通，因为技术已经改变了，” Shumailov说道。“行话已经改变了。对问题的理解也已经改变了。”

那么，挑战可能更为直接：辨别人类生成的数据与合成内容，并滤除后者。但即使存在这种技术，这也远非一项简单的任务。正如Sarkar所指出的，当Adobe Photoshop允许用户使用生成人工智能编辑图像时，结果是AI生成的图像吗？还是不是？
