<!--yml

类别：未分类

日期：2024年5月29日 12:38:43

-->

# CUDA依然是NVIDIA的重要护城河 - 作者：詹姆斯·王

> 来源：[https://weightythoughts.com/p/cuda-is-still-a-giant-moat-for-nvidia](https://weightythoughts.com/p/cuda-is-still-a-giant-moat-for-nvidia)

从某种意义上说，这是一篇**[GTC](https://www.nvidia.com/gtc/)**会议的更新。然而，**[已经有了关于该事件的很好的概述](https://www.fabricatedknowledge.com/p/jensens-world-compressing-reality-be4?utm_campaign=email-post&r=4de3d&utm_source=substack&utm_medium=email)**。在这里，我将更详细地解释为什么我一直是一个有些孤独的声音（直到最近）关于其他像**[SMIC/华为等](https://weightythoughts.com/p/fear-and-loathing-in-7nm)**改进他们的硬件依然无法取代NVIDIA。

这个观点主要与CUDA的生态系统和NVIDIA的专有互连有关。

CUDA依然占据着主导地位，虽然有关**[Blackwell](https://nvidianews.nvidia.com/news/nvidia-blackwell-platform-arrives-to-power-a-new-era-of-computing)**的公告很有意思，但这只是进步的一大步，而不是真正开创性的东西。更具吸引力的公告与NVIDIA在大规模计算中的互连方面的领先地位（尤其是NVLink）有关。

我将在一篇更深入的文章中介绍有关互连的内容，这相当复杂，但我觉得我应该更充分地解释一下我对CUDA的看法。与其纯粹的技术解释，不如让我讲一个故事，以使它更具感染力。

我曾经在加州大学伯克利分校学习高性能和并行计算的研究生课程。学生主要是计算机科学、计算机工程和统计（机器学习）的博士生。而我在攻读工商管理硕士期间选择修读这门课程可能有些奇怪——虽然规定上**技术上**并没有说我不能用这门课和计算机科学和统计学的博士研讨会来满足我的选修课要求。这是在我自己获得计算机科学硕士学位之前。

无论如何，我们探讨了并行计算涉及的概念和技术，包括存储器体系结构、矩阵乘法（这对于现代计算，包括当前的人工智能，至关重要）、局部性、SIMD/CPU向量化、锁定/等待原语。这些内容相当理论化，但我们还讨论并实施了包括OpenMP、OpenCL/GL等特定技术。

所有这些都让我很头疼。

接着，在学期还相当早的时候，一位曾在教授门下攻读博士学位并现在在NVIDIA工作的学生讲解了有关GPU和CUDA的内容。

非程序员或者不涉足大规模计算问题的程序员很难解释CUDA和其替代方案之间的显著差异。

一些CUDA具有非常好的基本数学和原子内存操作，同时还提供了简单的显式内存分配... 还有许多其他大部分听众完全无关紧要的东西。我只会说它为您提供了非常愉快的工具。

但更重要的是，因为NVIDIA控制着CUDA *和*他们的GPU，他们控制整个技术堆栈。软件、固件和硬件。这与Apple在其平台上的优势是一样的。NVIDIA也有同样的优势，即使是在程序员生产力/易用性方面（多年来我们已经学到，这本身就是一个巨大的优势），CUDA中的东西只是神奇地运行得更好。在幕后，CUDA进行了重要的优化。

此外，随着新的GPU推出（它们不可避免地会推出），CUDA继续保持向前和向后兼容（取决于您使用的功能），您将继续受益于NVIDIA不断推出的更好的硬件和固件。

关于为什么硬件是AI计算战争中的关键问题的一个主要论点是，很容易看出这是一个“硬”障碍。如果您不能制造硬件，那么您就真的没有运气了。

但是，如果情况是这样的话，我们在AI工作负载中应该不会看到如此冷淡的AMD采用（如果真有的话）。AMD确实更差，但它并不是*那么*落后，如果您被困在大量等待名单和无法获得的NVIDIA GPU中，为什么不选择AMD呢？

答案就是CUDA。

AMD有自己的“替代CUDA”的解决方案**[ROCm](https://www.amd.com/en/products/software/rocm.html)**，长期以来甚至没有支持浮点运算（即非技术人员指小数）。它具有许多相同的基本操作，但表现不佳。

即使您是一个雄心勃勃的计算机科学博士，同时也是一个出色的程序员，希望在OpenCL或Vulcan或其他任何地方编写一个新的与AI相关的库，您仍然不会获得与CUDA相同的性能（至少不需要大量额外的低级编程工作）。

实际上，尽管我知道这对非博士/非技术/非学术读者来说很奇怪，但计算机科学、工程和统计学的大多数博士在编程方面其实都很差，除非他们的专业是编程语言。他们并不是通过尝试用编写代码来做花哨的事情而受到激励。此外，他们有足够的头痛和时间压力，为什么他们会故意选择让生活变得更加艰难呢？

这就是为什么很多尖端、原型机器学习算法都是用R编写的，而且只能在R中使用（这可能会让人内心尖叫，但我被迫与之极为熟悉），这在性能上根本没有任何好处。但对于旨在在“真实”工作负载上运行、试图展示在大规模数据集上切割性能的东西，那就属于CUDA生态系统。

类似于Adobe/Microsoft/AutoCAD这些软件在学校中的广泛应用使它们在行业中也变得难以动摇，这反过来也强化了学校为什么使用它们的原因（学校亦受到这些行业的推动）。显然，从CUDA切换到其他平台所付出的努力会更甚于此。

当前，Tensorflow 和 PyTorch 等工具支持 CUDA 和 NVIDIA GPU 之外的平台。然而，很长一段时间里，这些工具并未提供这种支持—除非你计划在本世纪内使用非CPU平台完成工作。

这些基础工具现在支持其他平台似乎在驳斥我的论点，但其实是为了说明多年的工具、底层库等都假设所有人都在使用 NVIDIA GPU。

2020-2022年大缺显卡时期，有人撰写了博士生(post-graduate students)的购买二手RTX 2090的指南。**[如今，更新的指南仍然只侧重介绍NVIDIA GPU**](https://timdettmers.com/2023/01/30/which-gpu-for-deep-learning/comment-page-1/)。

我曾经听到有人提出中国拥有无穷无尽的资金和资源来征服AI（这是一项争议性观点，特别考虑到现在的局势），他们可以将所有工具移植到一个像**[华为CANN](https://github.com/opencv/opencv/wiki/Huawei-CANN-Backend)**这样的新平台。

除了这个对“迁移一切”这样的疯狂提案的浮泛（其主要观点对于硬件工程师来说似乎是“简单”）指控外，这并没有解决问题。**[推动AI革命的真正动力在于AI软件和模型的前沿，而非硬件/计算**](https://weightythoughts.com/p/compute-is-overrated-as-ais-bottleneck)**，而且仍旧位于CUDA生态系统中**。   

如果你愿意一直在技术上滞后，那么这样做是可接受的，但如果AI前沿科技的发展速度像人们普遍认为的那样快（我也是这么认为的），这将类似于摩尔定律在2010年前的状态。一直落后好几代将是一个**糟糕的事情**，不是任何形式的科技创新的关键，更不用说是领导力的来源。

**[半导体战争](https://en.wikipedia.org/wiki/Chip_War:_The_Fight_for_the_World%27s_Most_Critical_Technology)**一文精彩地阐述了前苏联在半导体领域尝试的“复制或盗取”策略并因此彻底落后。简单复制或盗取并快速跟随的理论想法很好，但对于半导体、AI以及其他的“硬技术”，需要培养持续提升的人力与智力资本。在这一点上，需要上一代保持与年复一年增加的复杂度和难度同步。

NVIDIA占主导地位的想法并不是一个有争议的看法 **[在被比作泰勒·斯威夫特演唱会的GTC之后](https://www.businessinsider.com/nvidia-ceo-taylor-swift-nod-shows-aware-of-tech-reputation-2024-3)**。

然而，*原因*依然如故，我认为值得强调。CUDA是否有一天会失去优势？华为的CANN或各种开源项目如Vulcan能否夺走其在人工智能领域的绝对主导地位，从而削弱NVIDIA在AI计算上的绝对控制力？

或许吧。但我还没有看到明显的转变。原因并不在数字或者关于NVIDIA如何失去领导地位的简单自上而下的思想实验。

NVIDIA占主导地位的原因在于多年以及数十亿美元投资于CUDA生态系统，以及对建设人工智能的社区进行的传道、教育工作。即使它正在衰退，这样重的、长期的战略影响要消失还需要相当长的时间。

就目前而言，NVIDIA只是在自己的领域内巩固其实力。而原因（除了互联互通——正如我之前说过的，我承诺在某个时候会详述）是CUDA及其生态系统。
