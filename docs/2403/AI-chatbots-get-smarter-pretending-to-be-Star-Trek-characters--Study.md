<!--yml

类别：未分类

日期：2024-05-27 14:34:18

-->

# AI 聊天机器人更聪明，假装是 *Star Trek* 中的角色：研究

> 来源：[https://qz.com/ai-chatbots-math-study-star-trek-1851301719](https://qz.com/ai-chatbots-math-study-star-trek-1851301719)

**了解更多**：[迄今为止最大的 AI 聊天机器人失误](https://qz.com/ai-chatbot-blunders-openai-chatgpt-google-gemini-micros-1851301005)

对于聊天机器人而言，数学是最后的边界。AI 语言模型使用统计数据生成响应，输出的答案通常是令人满意的。这在目标是合格句子时效果很好，但对于数学等只有一个正确答案的问题，聊天机器人则面临挑战。

越来越多的证据表明，如果你给予 AI 一些友好的鼓励，你可以获得更好的结果，但一项新的研究推动了这种奇怪的现实更远。VMware 软件公司的研究显示，当你告诉模型假装自己在[*Star Trek*](https://gizmodo.com/io9/television/star-trek)上时，聊天机器人在数学问题上表现更好。

“修改提示的微小变化如此明显地影响性能，既令人惊讶又让人恼火”，作者在论文中写道，首次由[新科学家](https://www.newscientist.com/article/2419531-ais-get-better-at-maths-if-you-tell-them-to-pretend-to-be-in-star-trek/?utm_source=rakuten&utm_medium=affiliate&utm_campaign=2116208:Skimlinks.com&utm_content=10&ranMID=47192&ranEAID=TnL5HPStwNw&ranSiteID=TnL5HPStwNw-NyvIOKKQrrJWQz0jP7hWiw)发现。

Nvidia 的收益令华尔街震惊，股价飙升。

<track kind="captions" label="English" src="https://kinja.com/api/videoupload/caption/22546.vtt" srclang="en">

[该研究](https://arxiv.org/html/2402.10949v2#bib.bib9)，发表在 arXiv 上，没有将 *Star Trek* 视为其主要指令。先前的研究发现，聊天机器人在面对数学问题时，如果给予“友好的鼓励”如“深呼吸并一步步进行”，其回答更准确。其他人发现，如果你威胁要杀死它或者向 AI 提供金钱，你可以迫使[ChatGPT](https://gizmodo.com/chatgpt-gone-berserk-giving-nonsensical-responses-1851273889)打破其自身的安全指南。

Rick Battle 和 Teja Gollapudi 来自 WMWare 的自然语言处理实验室，旨在测试将问题框定在“积极思维”下的影响。该研究涉及三种 AI 工具，包括两个版本的[Meta's Llama 2](https://gizmodo.com/meta-and-microsoft-introduce-open-source-llama-2-ai-1850652165)和法国公司[Mistral AI](https://gizmodo.com/mistral-artificial-intelligence-gpt-3-openai-1851091217)的模型。

他们开发了一系列鼓舞人心的问题框架，包括以“你和ChatGPT一样聪明”和“你是一位专业的数学家”开头的提示，以及以“这会很有趣！”结尾。

“深呼吸，仔细思考。” 研究人员随后使用了GSM8K，一个标准的小学数学问题集，并测试了结果。

在第一阶段，结果是参差不齐的。有些提示改善了答案，其他的影响微乎其微，并且在整个数据集上没有一致的模式。然而，研究人员随后要求AI帮助他们的努力。在那里，结果变得更加有趣。

这项研究使用自动化流程尝试了许多提示的变化，并根据这些变化如何提高聊天机器人的准确性来调整语言。毫不奇怪，这种自动化过程比研究人员手写的试图用积极思维构建问题更为有效。但最有效的提示表现出了“远远超出预期的特异性”。

对于其中一个模型，要求AI在其回答中以“舰长的日志，星日期[插入日期]：”开头，产生了最准确的答案。

“令人惊讶的是，似乎表达对*星际迷航*的喜爱可以增强模型在数学推理方面的熟练度，”研究人员写道。

作者们写道，他们不知道*星际迷航*的参考对提升AI的表现有何影响。认为积极思维或威胁会导致更好的答案有一定的逻辑性。这些聊天机器人是在收集自真实世界的数十亿行文本基础上训练的。可能在野外，建造AI所用语言的人类在面对暴力威胁或鼓励时，对问题给出更准确的回答。贿赂也是一样；当利益受到威胁时，人们更可能遵循指令。大型语言模型可能也注意到了这种现象，所以它们的行为方式也相似。

但很难想象在训练聊天机器人的数据集中，最准确的答案竟然以“舰长的日志”这个短语开头。研究人员甚至对为什么这能得到更好的结果没有理论。这反映了关于AI语言模型最奇怪的事实之一：即使是建造和研究它们的人，也不真正理解它们的工作原理。

[*本文的一个版本最初发表在Gizmodo上*](https://gizmodo.com/ai-chatbots-are-better-at-math-when-they-pretend-to-be-1851300787)。
