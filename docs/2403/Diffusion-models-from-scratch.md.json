["```\ndef training_loop(loader  : DataLoader,\n                  model   : nn.Module,\n                  schedule: Schedule,\n                  epochs  : int = 10000):\n    optimizer = torch.optim.Adam(model.parameters())\n    for _ in range(epochs):\n        for x0 in loader:\n            optimizer.zero_grad()\n            sigma, eps = generate_train_sample(x0, schedule)\n            eps_hat = model(x0 + sigma * eps, sigma)\n            loss = nn.MSELoss()(eps_hat, eps)\n            optimizer.backward(loss)\n            optimizer.step() \n```", "```\ndef generate_train_sample(x0: torch.FloatTensor, schedule: Schedule):\n    sigma = schedule.sample_batch(x0)\n    eps = torch.randn_like(x0)\n    return sigma, eps \n```", "```\nclass Schedule:\n    def __init__(self, sigmas: torch.FloatTensor):\n        self.sigmas = sigmas\n    def __getitem__(self, i) -> torch.FloatTensor:\n        return self.sigmas[i]\n    def __len__(self) -> int:\n        return len(self.sigmas)\n    def sample_batch(self, x0:torch.FloatTensor) -> torch.FloatTensor:\n        return self[torch.randint(len(self), (x0.shape[0],))].to(x0) \n```", "```\nclass ScheduleLogLinear(Schedule):\n    def __init__(self, N: int, sigma_min: float=0.02, sigma_max: float=10):\n        super().__init__(torch.logspace(math.log10(sigma_min), math.log10(sigma_max), N)) \n```", "```\ndataset = Swissroll(np.pi/2, 5*np.pi, 100)\nloader  = DataLoader(dataset, batch_size=2048) \n```", "```\ndef get_sigma_embeds(sigma):\n    sigma = sigma.unsqueeze(1)\n    return torch.cat([torch.sin(torch.log(sigma)/2),\n                      torch.cos(torch.log(sigma)/2)], dim=1)\n\nclass TimeInputMLP(nn.Module):\n    def __init__(self, dim, hidden_dims):\n        super().__init__()\n        layers = []\n        for in_dim, out_dim in pairwise((dim + 2,) + hidden_dims):\n            layers.extend([nn.Linear(in_dim, out_dim), nn.GELU()])\n        layers.append(nn.Linear(hidden_dims[-1], dim))\n        self.net = nn.Sequential(*layers)\n        self.input_dims = (dim,)\n\n    def rand_input(self, batchsize):\n        return torch.randn((batchsize,) + self.input_dims)\n\n    def forward(self, x, sigma):\n        sigma_embeds = get_sigma_embeds(sigma)         # shape: b x 2\n        nn_input = torch.cat([x, sigma_embeds], dim=1) # shape: b x (dim + 2)\n        return self.net(nn_input)\n\nmodel = TimeInputMLP(dim=2, hidden_dims=(16,128,128,128,128,16)) \n```", "```\nschedule = ScheduleLogLinear(N=200, sigma_min=0.005, sigma_max=10)\ntrainer  = training_loop(loader, model, schedule, epochs=15000)\nlosses   = [ns.loss.item() for ns in trainer] \n```", "```\ndef sq_norm(M, k):\n    # M: b x n --(norm)--> b --(repeat)--> b x k\n    return (torch.norm(M, dim=1)**2).unsqueeze(1).repeat(1,k)\n\nclass IdealDenoiser:\n    def __init__(self, dataset: torch.utils.data.Dataset):\n        self.data = torch.stack(list(dataset))\n\n    def __call__(self, x, sigma):\n        x = x.flatten(start_dim=1)\n        d = self.data.flatten(start_dim=1)\n        xb, db = x.shape[0], d.shape[0]\n        sq_diffs = sq_norm(x, db) + sq_norm(d, xb).T - 2 * x @ d.T\n        weights = torch.nn.functional.softmax(-sq_diffs/2/sigma**2, dim=1)\n        return (x - torch.einsum('ij,j...->i...', weights, self.data))/sigma \n```", "```\nclass Schedule:\n    ...\n    def sample_sigmas(self, steps: int) -> torch.FloatTensor:\n        indices = list((len(self) * (1 - np.arange(0, steps)/steps))\n                       .round().astype(np.int64) - 1)\n        return self[indices + [0]]\n\nbatchsize = 2000\nsigmas = schedule.sample_sigmas(20)\nxt = model.rand_input(batchsize) * sigmas[0]\nfor sig, sig_prev in pairwise(sigmas):\n    eps = model(xt, sig.to(xt))\n    xt -= (sig - sig_prev) * eps \n```", "```\n@torch.no_grad()\ndef samples(model      : nn.Module,\n            sigmas     : torch.FloatTensor, # Iterable with N+1 values for N sampling steps\n            gam        : float = 1.,        # Suggested to use gam >= 1\n            mu         : float = 0.,        # Requires mu in [0, 1)\n            xt         : Optional[torch.FloatTensor] = None,\n            batchsize  : int = 1):\n    xt = model.rand_input(batchsize) * sigmas[0]\n    eps = None\n    for i, (sig, sig_prev) in enumerate(pairwise(sigmas)):\n        eps, eps_prev = model(xt, sig.to(xt)), eps\n        eps_av = eps * gam + eps_prev * (1-gam)  if i > 0 else eps\n        sig_p = (sig_prev/sig**mu)**(1/(1-mu)) # sig_prev == sig**mu sig_p**(1-mu)\n        eta = (sig_prev**2 - sig_p**2).sqrt()\n        xt = xt - (sig - sig_p) * eps_av + eta * model.rand_input(batchsize).to(xt)\n        yield xt \n```", "```\nschedule = ScheduleLDM(1000)\nmodel    = ModelLatentDiffusion('stabilityai/stable-diffusion-2-1-base')\nmodel.set_text_condition('An astronaut riding a horse')\n*xts, x0 = samples(model, schedule.sample_sigmas(50))\ndecoded  = model.decode_latents(x0) \n```", "```\n@article{permenter2023interpreting,\n  title={Interpreting and improving diffusion models using the euclidean distance function},\n  author={Permenter, Frank and Yuan, Chenyang},\n  journal={arXiv preprint arXiv:2306.04848},\n  year={2023}\n} \n```"]