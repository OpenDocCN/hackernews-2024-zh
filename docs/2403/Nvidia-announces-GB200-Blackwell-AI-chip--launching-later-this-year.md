<!--yml

category: 未分类

date: 2024-05-27 15:04:19

-->

# Nvidia 宣布 GB200 Blackwell AI 芯片，将于今年晚些时候推出。

> 来源：[https://www.cnbc.com/2024/03/18/nvidia-announces-gb200-blackwell-ai-chip-launching-later-this-year.html](https://www.cnbc.com/2024/03/18/nvidia-announces-gb200-blackwell-ai-chip-launching-later-this-year.html)

Nvidia CEO Jensen Huang 在 2024 年 3 月 18 日于加利福尼亚州圣何塞的 Nvidia GTC 人工智能大会上发表主题演讲。

Justin Sullivan | Getty Images

[Nvidia](/quotes/NVDA/) 在周一宣布了一款新一代人工智能芯片和运行人工智能模型的软件。此项宣布是在 Nvidia 于圣何塞举行的开发者大会上进行的，旨在巩固其作为 AI 公司首选供应商的地位。

自 2022 年底 OpenAI 的 ChatGPT 掀起 AI 热潮以来，Nvidia 的股价猛涨五倍，总销售额也增长了三倍以上。Nvidia 的高端服务器 GPU 对于训练和部署大型 AI 模型至关重要。像[Microsoft](/quotes/MSFT/)和[Meta](/quotes/META/)这样的公司已经花费数十亿美元购买这些芯片。

新一代 AI 图形处理器命名为 Blackwell。第一款 Blackwell 芯片名为 GB200，将于今年晚些时候发货。Nvidia 通过更强大的芯片来吸引客户，以刺激新订单。例如，公司和软件制造商仍在争相获取当前一代的“Hopper” H100 和类似的芯片。

“霍珀很棒，但我们需要更大的 GPU，”Nvidia CEO Jensen Huang 在周一于公司加利福尼亚州开发者大会上说道。

Nvidia 的股价在周一延长交易中下跌超过 1%。

公司还推出了名为 NIM 的收入生成软件，将更易于部署 AI，这让客户有更多理由选择 Nvidia 芯片而不是[日益增长的竞争对手](https://www.cnbc.com/2023/12/07/amd-stock-spikes-after-company-launches-ai-chip-to-rival-nvidia.html)。

Nvidia 的高管们表示，公司正在逐渐从一个雇佣军芯片提供商转变为像 Microsoft 或 Apple 这样的平台提供商，其他公司可以在其上构建软件。

"Blackwell 不仅是一款芯片，更是一个平台的名称，" Huang 表示。

"可销售的商业产品是 GPU，而软件则是帮助人们以不同方式使用 GPU 的一切，" Nvidia 企业副总裁 Manuvir Das 在一次采访中说道。"当然，我们仍然在这样做。但真正改变的是，我们现在真正拥有一个商业软件业务。"

Das 表示，Nvidia 的新软件将使得在任何 Nvidia GPU 上运行程序变得更加容易，即使是那些可能更适合部署而不是构建 AI 的旧型号。

"如果你是开发者，你有一个有趣的模型想让人们采用，如果你把它放在 NIM 上，我们会确保它可以在我们所有的 GPU 上运行，这样你就能触及到很多人群，" Das 说道。

## 会见Blackwell，霍珀的继任者

Nvidia的GB200 Grace Blackwell超级芯片，配备两个B200图形处理器和一个基于Arm架构的中央处理器。

每两年，Nvidia更新其GPU架构，实现了性能的大幅提升。过去一年发布的许多AI模型是在该公司的霍珀架构上训练的，这些架构被诸如H100之类的芯片所使用，该架构于2022年宣布。

Nvidia表示，像GB200这样基于Blackwell的处理器为AI公司提供了巨大的性能升级，AI性能达到20 petaflops，而H100仅为4 petaflops。Nvidia表示，额外的处理能力将使AI公司能够训练更大更复杂的模型。

该芯片包括Nvidia称为“变压器引擎”，专门用于运行基于transformers的AI，这是ChatGPT的核心技术之一。

Blackwell GPU非常大，将两个分别制造的芯片整合到一个由[TSMC](/quotes/TSM/)制造的芯片中。它还将作为一个名为GB200 NVLink 2的整个服务器出售，结合了72个Blackwell GPU和其他Nvidia部件，旨在训练AI模型。

Nvidia首席执行官黄仁勋在公司的开发者大会上，比较了新的“Blackwell”芯片与当前的“Hopper” H100芯片的大小，该大会在加州圣何塞举行。

Nvidia

亚马逊，谷歌，微软和甲骨文将通过云服务销售GB200的访问权限。GB200将两个B200 Blackwell GPU与一个基于Arm架构的Grace CPU配对。Nvidia表示，亚马逊Web服务将建立一个包含20000个GB200芯片的服务器集群。

Nvidia表示，该系统可以部署一个27000亿参数的模型。这比如GPT-4这样的最大模型要大得多，据报道GPT-4有1.7万亿个参数。许多人工智能研究人员认为，具有更多参数和数据的更大模型可能会开启新的能力[链接](https://openai.com/research/ai-and-compute)。

Nvidia没有为新的GB200或其所用系统提供成本。根据分析师的估计，Nvidia基于Hopper的H100芯片的成本在每个芯片25,000至40,000美元之间，整个系统的成本高达200,000美元。

Nvidia还将B200图形处理器作为整个服务器机架的一部分进行销售。

## Nvidia推理微服务

Nvidia还宣布将其Nvidia企业软件订阅中增加一个名为NIM的新产品，全称为Nvidia推理微服务。

NIM使得使用旧版Nvidia GPU进行推理（或者运行AI软件的过程）变得更加容易，并将允许公司继续使用他们已经拥有的数亿个Nvidia GPU。推理所需的计算能力比新AI模型的初始训练少。NIM使得那些希望运行自己AI模型的公司能够实现这一目标，而不是通过像OpenAI这样的公司购买AI结果的访问权限。

策略是让购买基于 Nvidia 的服务器的客户注册 Nvidia 企业服务，每 GPU 每年需支付 4,500 美元的许可费用。

Nvidia 将与微软或 Hugging Face 等 AI 公司合作，确保它们的 AI 模型经过调整，可以在所有兼容的 Nvidia 芯片上运行。然后，开发者可以使用 NIM，在他们自己的服务器或基于云的 Nvidia 服务器上高效地运行模型，而不需要漫长的配置过程。

"在我的代码中，我原本调用 OpenAI 的地方，我只需要替换一行代码，让它指向我从 Nvidia 获得的这个 NIM，" Das 说道。

Nvidia 表示，该软件还将帮助 AI 在配备 GPU 的笔记本电脑上运行，而不是在云服务器上运行。
