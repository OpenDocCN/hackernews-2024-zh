- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-05-29 12:35:59'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: Why the hell is your Kubernetes API public? | lbr.
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://leebriggs.co.uk/blog/2024/03/23/why-public-k8s-controlplane](https://leebriggs.co.uk/blog/2024/03/23/why-public-k8s-controlplane)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Do you ever really think about how you get access to your Kubernetes control
    plane? Whatever mechanism you use to provision your cluster, you get a `KUBECONFIG`
    and usually just go on your merry way to overcomplicating your infrastructure.
  prefs: []
  type: TYPE_NORMAL
- en: However, if you’ve ever looked at your `KUBECONFIG` you’ll see you have a server
    address.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can check the health of your cluster by doing the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Assuming everything is working as expected (and if it isn’t, you should probably
    stop reading and go figure out what), it should return `ok`.
  prefs: []
  type: TYPE_NORMAL
- en: Has it ever come to your attention that this *just works* from a networking
    perspective? More than likely you didn’t have to a connect to a VPN, or SSH into
    a bastion/jump host host?
  prefs: []
  type: TYPE_NORMAL
- en: How did I know that? Well, because the vast majority of Kubernetes clusters
    are just hanging out on the public internet, without a care in the world.
  prefs: []
  type: TYPE_NORMAL
- en: If you search [shodan.io for Kubernetes clusters](https://www.shodan.io/search?query=kubernetes)
    you’ll see there’s almost 1.4 million clusters, readily accessible and open to
    the scary, scary world.
  prefs: []
  type: TYPE_NORMAL
- en: For reasons I can’t quite understand, we’ve sort of collectively decided that
    it’s okay to put our Kubernetes control planes on the public internet. At the
    very least, we’ve sort of decided it’s okay to give them a public IP address -
    sure you might add some security groups of firewall rules from specific IP addresses,
    but the control plane is still accessible from the internet.
  prefs: []
  type: TYPE_NORMAL
- en: To put this into some sort of perspective, how many of the people reading this
    get a cold shudder when they think about putting their database on the public
    internet? Or a windows server with RDP? Or a Linux server with SSH?
  prefs: []
  type: TYPE_NORMAL
- en: Established practices say this is generally not a good idea, and yet in order
    to make our lives easier, we’ve decided that it’s *okay* to let every person and
    their dog free access to try and make our clusters theirs.
  prefs: []
  type: TYPE_NORMAL
- en: Private Networks
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As soon as you put something in a private subnet in the cloud, you add a layer
    of complexity to the act of actually using it. You have to explain to everyone
    who needs access to it, including that developer who’s just joined the team, where
    it is and how to use it. You might use an SSH tunnel, or a bastion instance, or
    god forbid that VPN server someone set up years ago that nobody dares touch.
  prefs: []
  type: TYPE_NORMAL
- en: We sort of accept these for things like databases because we very rarely need
    to get into them except in case of emergency, and we think it’s okay to have to
    route through something else because the data in them is important enough to protect.
  prefs: []
  type: TYPE_NORMAL
- en: In addition to this, when cloud providers started offering managed Kubernetes
    servers, most of them didn’t even *have* private control planes. It was only in
    the last few years that they started offering this as a feature, and even then,
    it’s not the default.
  prefs: []
  type: TYPE_NORMAL
- en: So the practice of putting a very important API on the internet has proliferated
    because it’s just *easier* to do it that way. The real concern here is that we’re
    one severe vulnerability from having a bitcoin miner on every Kubernetes cluster
    on the internet.
  prefs: []
  type: TYPE_NORMAL
- en: An alternative
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'I [previously](/blog/2024/02/26/cheap-kubernetes-loadbalancers.html) wrote
    about the [Tailscale Kubernetes Operator](https://tailscale.com/kb/1236/kubernetes-operator)
    and its ability to expose services running inside your Kubernetes cluster to your
    Tailnet, but it has another amazing feature:'
  prefs: []
  type: TYPE_NORMAL
- en: It can act as a Kubernetes proxy for your cluster.
  prefs: []
  type: TYPE_NORMAL
- en: Say what now?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Well, let’s say you provision a Kubernetes cluster in AWS. You decide that you
    that in order to give yourself another layer of protection, you’re going to make
    sure the control plane is only accessible within the VPC.
  prefs: []
  type: TYPE_NORMAL
- en: If you install the Tailscale Kubernetes operator and set the `apiServerProxyConfig`
    flag, it’ll create a device in your Tailnet that makes it accessible to anyone
    on the tailnet. This means before you’re able to use the cluster, you need to
    be connected to the Tailnet. All of that pain I mentioned previously with Bastion
    hosts and networking just vanishes into thin air.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s take it for a spin!
  prefs: []
  type: TYPE_NORMAL
- en: Installing and using the Tailscale Operator
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Prerequisites
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: You’ll need to have your own Tailnet, and be connected to it. You can [sign
    up for Tailscale](https://login.tailscale.com/start?utm=leebriggs.co.uk) and it’s
    free for personal use.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once that’s done, you’ll need to make a slight change to your ACL. The Kubernetes
    operator uses Tailscale’s tagging mechanism so let’s create a tag for the operator
    to use, and then a tag for it to give to client devices it registers:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Then, register an oauth client with `Devices` write scope and the `tag:k8s-operator`
    tag.
  prefs: []
  type: TYPE_NORMAL
- en: 'Step 1: Get a Kubernetes Cluster'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: There’s a lot of ways to do this, choose the way you prefer. If you’re a fan
    of eksctl, [this page](https://eksctl.io/usage/eks-private-cluster/) shows you
    how to create a fully private cluster.
  prefs: []
  type: TYPE_NORMAL
- en: You can of course use the defaults and try this out with a public cluster if
    you like, but I’m going to assume you’re doing this because you want to make your
    cluster private.
  prefs: []
  type: TYPE_NORMAL
- en: You may have to do this from inside your actual VPC, because remember, any post
    install steps that interact with the Kubenrnetes API server won’t work. I leverage
    a [Tailscale Subnet Router](https://tailscale.com/kb/1019/subnets) to make this
    easier, more on this later.
  prefs: []
  type: TYPE_NORMAL
- en: 'Step 2: Install the Tailscale Kubernetes Operator'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The easiest way to do this is with [Helm](https://tailscale.com/kb/1236/kubernetes-operator#installation).
    You’ll need to add the Tailscale Helm repository:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Then install the operator with your oauth keys from the prerequites step, and
    enable the proxy:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Now, if you look at your Tailscale dashboard, or use `tailscale status` you
    should see a couple of new devices - the operator and a service just for the API
    server proxy.
  prefs: []
  type: TYPE_NORMAL
- en: You can now access your Kubernetes cluster from anywhere that has a Tailscale
    client installed, no faffing required.
  prefs: []
  type: TYPE_NORMAL
- en: 'Step 3: Use it'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'You’ll need to configure your `KUBECONFIG` using the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'This will set up your `KUBECONFIG` to use the Tailscale API server proxy as
    the server for your cluster. If you examine your `KUBECONFIG` you should see something
    like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Hang on a minute..
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You probably have some questions. Firstly, what’s that `unused` token all about?
    What does `noauauthmodeth` mean in our operator installation? How does this work?
  prefs: []
  type: TYPE_NORMAL
- en: 'Well, if you run a basic `kubectl` command now (and assuming you’re connected
    to your Tailnet) you’ll get something back, but it won’t help much:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'What’s happened here? Well, the good news is, we’ve been able to route to our
    private Kubernetes control plane, but we’re not sending any information back about
    who we are. So let’s make a small change to our Tailscale ACL in the Tailscale
    console. Add the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: By adding this grant, I’m assuming you’re an admin of your Tailnet.
  prefs: []
  type: TYPE_NORMAL
- en: Now, give that `kubectl` command another try.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: That’s more like it.
  prefs: []
  type: TYPE_NORMAL
- en: 'As you can see here, I’ve solved two distinct problems:'
  prefs: []
  type: TYPE_NORMAL
- en: I’ve made my public Kubernetes control plane accessible over a VPN, without
    needing to worry about routing and networking - Tailscale has handled it for me.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: I’ve also been able to leveraging Tailscale’s ACL mechanism to provide authentication
    to Kubernetes groups in a clusterrolebinding.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: noauth Mode
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Now, if you’re already happy with your current authorization mode, you can still
    use Tailscale’s access mechanism to solve the routing problem. In this particular
    case, you’d install the Operator in `noauth` mode and then use your cloud providers
    existing mechanism to retrieve a token.
  prefs: []
  type: TYPE_NORMAL
- en: 'Modify your Tailscale Operator installation like so:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Once that’s installed, if you run your `kubectl` command again, you’ll see
    a different error message:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: The reason for this is a little obvious, the EKS cluster I’m using has absolutely
    no idea who `jaxxstorm@github` is - because it uses IAM to authenticate me to
    the cluster.
  prefs: []
  type: TYPE_NORMAL
- en: 'So let’s modify our `KUBECONFIG` to retrieve a token as EKS expects. We’ll
    modify the `user` section to leverage an `exec` directive - it should look a little
    bit like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: Now we’re able to route to the Kubernetes control plane, and authenticate with
    it using the cloud providers authorization mechanism.
  prefs: []
  type: TYPE_NORMAL
- en: Some FAQs
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Why wouldn’t I just use a Subnet Router?
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: A common question I get asked is why wouldn’t I just use a subnet router in
    the VPC to route anything to the private address of the control plane? I leveraged
    this mechanism when I installed the operator, because my control plane was initially
    unrouteable from the internet anyway.
  prefs: []
  type: TYPE_NORMAL
- en: This is a legimate solution to the problem, and if you don’t want to use the
    operator in `auth` mode, keep living your life. However, one benefit you get by
    installing the operator and talking directly to it via your `KUBECONFIG` is being
    able to use Tailscale’s ACLs to dictate who can actually communicate with the
    operator.
  prefs: []
  type: TYPE_NORMAL
- en: If you recall our `grant` from earlier we were able to dictate who was able
    to impersonate users inside the cluster in `auth` mode, but with Tailscale’s ACL
    system we can also be prescriptive about connectivity. Consider if you removed
    the default, permissive ACL
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Then defining a group of users who can access the cluster, and adding a more
    explicit ACL:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: I can be much more granular about my access to my cluster, and have a Zero Trust
    model for my Kubernetes control plane at the *network* level as well as the authorization
    level. Your information security team will *love* you for this.
  prefs: []
  type: TYPE_NORMAL
- en: When you provision the operator in the cluster, you can modify the tags it uses
    to even further allow you to segment your Tailnet, see the [operator config in
    the Helm chart’s values.yaml](https://github.com/tailscale/tailscale/blob/main/cmd/k8s-operator/deploy/chart/values.yaml#L23)
  prefs: []
  type: TYPE_NORMAL
- en: Just remember to ensure your oauth clients has the correct permissions to manage
    those tags!
  prefs: []
  type: TYPE_NORMAL
- en: Can I scope the access on the cluster side?
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: If you have the operator installed in `auth` mode, you can scope the access
    both at the network level (using the aforementioned tags) *and* the Kubernetes
    RBAC system.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s say we want to give our aforementioned engineers group access to only
    a single namespace called `demo`.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we’d create the ClusterRole (or Role) and then cluster role binding:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Then update our Tailscale ACL to modify the grants:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, if I try to access the `demo` namespace, I can do the stuff I need to
    do:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'But not in the `kube-system` namespace:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: Wrap Up
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As always with the posts I write on here, I’m writing in my personal capacity,
    but obviously I’m a Tailscale employee with a vested interest in the success of
    the company. Do I want you to sign up for Tailscale and pay us money? You bet
    I do. Do I want you to get your Kubernetes clusters off the public internet even
    if you *don’t* want to sign up for Tailscale and pay us money? **Yes**.
  prefs: []
  type: TYPE_NORMAL
- en: I’ve always had an uneasy feeling about these public clusters, and I can’t help
    but feel we’re one RCE away from a disaster.
  prefs: []
  type: TYPE_NORMAL
- en: So now you know how easy it is to get your Kubernetes control plane off the
    internet, what are you waiting for?
  prefs: []
  type: TYPE_NORMAL
