- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-05-27 14:46:29'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: Waymo’s Double-Crash With Pickup Truck, Cyclist & Arson Examined
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://www.forbes.com/sites/bradtempleton/2024/03/05/waymos-double-crash-with-pickup-trucks-and-more-examined/](https://www.forbes.com/sites/bradtempleton/2024/03/05/waymos-double-crash-with-pickup-trucks-and-more-examined/)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: <fbs-accordion class="expandable" current="-1">After the arson, the Waymo was
    badly burnt, but drives under the rear trunk might survive, ... [+] particularly
    if protected.</fbs-accordion> Séraphine Hossenlopp via SFFD
  prefs: []
  type: TYPE_NORMAL
- en: Waymo has recently seen good news, with the [quick approval of their application](https://www.forbes.com/sites/bradtempleton/2024/03/03/waymo-wins-permission-for-major-expansion-to-la-sf-peninsula/
    "https://www.forbes.com/sites/bradtempleton/2024/03/03/waymo-wins-permission-for-major-expansion-to-la-sf-peninsula/")
    to serve large service areas in the San Francisco and Los Angeles. They also recently
    had a small set of more negative headlines worthy of attention, in particular
    a day in Phoenix where two of their vehicles had modest impacts with the very
    same truck being towed within minutes of each other.
  prefs: []
  type: TYPE_NORMAL
- en: There was also an arson of a Waymo and a minor but significant impact with a
    bicyclist. Though it’s nothing on the scale of the chaotic month that led to [the
    Fall of GM’s Cruise](https://www.forbes.com/sites/bradtempleton/2023/12/28/robocar-2023-in-review-the-fall-of-cruise/
    "https://www.forbes.com/sites/bradtempleton/2023/12/28/robocar-2023-in-review-the-fall-of-cruise/")
    last year, a few have wondered if more trouble is next for the company. The answer
    is of course yes, but that turns out to be a good thing.
  prefs: []
  type: TYPE_NORMAL
- en: The development of self-driving cars, one of the most ambitious projects in
    computers, software and AI ever attempted, is fraught with difficulty and peril
    and mistakes, and it can’t be any other way—something that must be understood
    if this technology is to bring about the benefits it promises.
  prefs: []
  type: TYPE_NORMAL
- en: None of these incidents (except the arson) involved significant injuries or
    property damage, but they stand out because of Waymo’s generally exemplary safety
    record, which has been superior to the record of Cruise that resulted in them
    being pulled from U.S. streets
  prefs: []
  type: TYPE_NORMAL
- en: <fbs-ad position="inread" progressive="" ad-id="article-0-inread" aria-hidden="true"
    role="presentation"></fbs-ad>
  prefs: []
  type: TYPE_NORMAL
- en: Towed Pickup Impacts
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Most interesting was the pair of crashes between Waymos and the corner of the
    same pickup truck being towed in the center lane of a Phoenix street. As [reported
    to NHTSA](https://static.nhtsa.gov/odi/rcl/2024/RCLRPT-24E013-4528.PDF "https://static.nhtsa.gov/odi/rcl/2024/RCLRPT-24E013-4528.PDF"),
    the pickup was behind a tow truck, which had craned up the front, but its front
    wheels were on the ground. However, in an unusual (and Waymo says improper) fashion,
    the steering wheel on the truck was apparently locked to the right. The tow truck
    was moving in the shared center “left turn” lane, which is generally only for
    vehicles turning immediately left from either direction. Because of the turned
    front wheels, the dragged truck was twisting itself to the right, constantly putting
    its nose into the lane to the (tow truck’s) right of it, in which the Waymos were
    driving. The Waymo car touched the corner of the truck, but there was not significant
    damage and the tow truck continued on. “Several minutes” later another Waymo came
    along and made the same mistake.
  prefs: []
  type: TYPE_NORMAL
- en: In Waymo’s favor, a tow truck should not have been towing like that, nor should
    it be driving for long in that center lane. Counter to that, however, is the fact
    that a human driver would be very unlikely to make the mistake the Waymo made.
    The robotic nature of the system is also evident in the fact a second Waymo made
    the same mistake.
  prefs: []
  type: TYPE_NORMAL
- en: The most important part of any self driving car is the prediction engine, which
    tries to calculate where everything on the road is likely to go as we move into
    the future. Speculation suggests the Waymo prediction engine saw the orientation
    of the pickup truck, with wheels turned away, and incorrectly predicted it would
    head towards the lane it was being towed in. Waymo has declined to comment on
    the details of this, but there are two potential sources of a bad prediction.
    In one case, they might have not understood the pickup was being towed, and just
    viewed it as a truck incorrectly making an incursion into the Waymo’s lane, but
    clearly steering out of it into the left turn lane. Another, more likely error,
    would be that they identified the pickup as being towed, and predicted that it
    would trail behind the tow truck in the way this normally works. Towed vehicles
    can sway out, but they will return to following the tow vehicle quickly.
  prefs: []
  type: TYPE_NORMAL
- en: As such, I speculate that the Waymo predicted the pickup would quickly move
    to the center lane, and as such the Waymo could continue it its adjacent lane
    as the truck would be gone by the time it got there. It probably kept predicting
    that up until the time it was too close and it winged the pickup truck. Based
    on the crash description, it’s probable the tow truck driver didn’t even know
    this had happened, and so kept going, and then the 2nd Waymo to come along.
  prefs: []
  type: TYPE_NORMAL
- en: Disturbingly for Waymo, there is an echo here to one of Cruise’s most famous
    crashes, where they drove right into the back of an articulated city bus in San
    Francisco. An articulated bus is effectively one vehicle being towed by another,
    and Cruise mistakenly programmed their car to ignore sensor data from the back
    of the bus, and instead predict where the back of the bus would be based on what
    the front was doing. Foolishly, Cruise did this even when the front of the bus
    was hidden, and so it had to guess where it was. It guessed wrong, as the bus
    was braking but the Cruise couldn’t see that. Well, it could see the back, but
    was ignoring it, and couldn’t see the front. That was a stupid bug, but more concerning
    was that no system was able to notice that the back of the bus was coming up fast
    and would be hit, which should have caused an emergency override of plan that
    was based on the wrong predictions.
  prefs: []
  type: TYPE_NORMAL
- en: Cruise’s crash should have been an immediate red flag for Waymo to look more
    carefully at their code to handle vehicles being towed. If they did that, they
    missed a complete analysis of the ways that towed vehicles can act strangely.
    Since this is a rare situation, and involves improper towing, that might get missed.
    They also may have missed that bigger picture solution, which is to notice when
    sensor data says a prediction is more and more clearly wrong. That should have
    been drop-dead easy for Cruise, as their target was “the size of a bus.” For Waymo
    to get smarter about disagreements between sensor data and predictions is a more
    challenging task.
  prefs: []
  type: TYPE_NORMAL
- en: The double crash is another thing that can happen to robots but not to humans.
    Normally, it’s a plus. When one robot makes a mistake, the team will immediately
    work to fix it, and within a few days, no robot will make that mistake again.
    But it won’t get fixed in a few minutes. That’s not generally trouble, because
    rare situations don’t repeat quickly, but they can when it’s literally the same
    vehicle or location. It could make sense for any crash to not just tag a location
    as risky, but a specific other vehicle or class of vehicle.
  prefs: []
  type: TYPE_NORMAL
- en: Waymo Makes Mistakes
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The lesson for the public and regulators however, is that the best self-driving
    cars will still make mistakes. Generally, after any incident, it is common for
    critics to come forward and say, “this mistake shows these vehicles are not ready”
    to be on the roads. Unfortunately, they rarely are able to describe what the bar
    is to decide what is ready and what is not, and the implication is that any mistake
    is too much.
  prefs: []
  type: TYPE_NORMAL
- en: If that’s the case, though, the vehicles will never be ready, for nobody expects
    them to be perfect. At best they hope to have risk levels much better than human
    drivers, and teams started going on the roads when they calculated risk levels
    modestly better than the human drivers. At the same time, we allow teen-age student
    drivers on the road when their risk levels are a fair bit worse than typical human
    drivers, in the hope that they will improve. By many standards, these student
    and newly licensed drivers are “not ready” but we embrace them.
  prefs: []
  type: TYPE_NORMAL
- en: For this technology to be developed, it must learn, and it can only learn many
    of the important lessons driving on real roads. Regulators pulled Cruise off the
    roads for both not being safe enough and for hiding information, but they have
    yet to define what safe enough means, or what will make them ready. While there
    is an instinct to be wary of any incident (particularly those with injuries) policy
    decisions must be based on statistical analysis of large datasets, not on single
    incidents.
  prefs: []
  type: TYPE_NORMAL
- en: For every incident, I have recommended there be analysis of fault, severity
    and probability of repeat. This situation was unusual, in that it repeated within
    just a few minutes, but that’s because it was with the same vehicle. At the level
    of maturity Waymo has, their data shows it’s pretty rare (but not impossible)
    for them to encounter entirely novel situations they can’t handle. After you drive
    for tens of millions of miles, anything never seen before is guaranteed to be
    something rare and unlikely, which is good. As such, even when an incident happens,
    it is not necessary to “shut down the fleet” out of fear it could happen again
    immediately—except in this way, with the very same truck.
  prefs: []
  type: TYPE_NORMAL
- en: In this crash, fault belongs mostly with the poorly towed truck, but some fault
    lies with the Waymo driver. The severity of the event was minor. The probability
    of a repeat with a different towed vehicle is low, but the probability with the
    exact same vehicle is high. As such, the recommendation is that any unusual location
    or vehicle which triggers an incident be immediately avoided by all other vehicles
    until analysis can determine more about the cause. That’s not easy, but it’s better
    than a fleet shutdown.
  prefs: []
  type: TYPE_NORMAL
- en: Cyclist Hit
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Waymo has been fairly mum about an incident where they collided with a cyclist
    at a 4-way stop. According to them, the cyclist rolled through the 4-way stop,
    as cyclists very often do. He was following a truck which had stopped at the stop
    sign. The Waymo decided it was its turn, which seems correct, and the cyclist
    effectively went out of turn, and was hidden from view, according to Waymo. When
    they finally were able to see the cyclist, it was too late to avoid a collision.
    Fortunately any injuries were minor, the cyclist reportedly declined medical attention
    and wanted to be on his way.
  prefs: []
  type: TYPE_NORMAL
- en: Most cyclists frequently roll through stop signs. It’s not legal (rolling is
    legal in some states but not if another road user has priority) but it’s probably
    more common than seeing one make a full stop. I’ve certainly done this. When you
    do it, you take on responsibility for the fact other road users may not expect
    you or may not even see you, and so you must be alert to make sure nobody else
    is going to intersect you—you will pay a far greater price if that happens. This
    cyclist appears to have failed at that. At the same time, it is interesting to
    ask if the Waymo driver could have done something to prevent the incident. Waymo
    has not commented, but they might have perceived the cyclist riding up behind
    the truck, and possibly noticed his “disappearance.” While it’s a reasonable assumption
    that a cyclist might roll through the stop, the unwritten rules of the road don’t
    readily allow this. If you assume there could be a cyclist behind every other
    large vehicle at a 4 way stop, you will be much too timid and impede traffic.
    If we presume the Waymo had no chance to see the cyclist before he approached
    the stop sign, it’s not clear a human driver could have avoided this situation.
    We don’t have information on the reaction times of the Waymo once the cyclist
    became visible to see if they could have done that better.
  prefs: []
  type: TYPE_NORMAL
- en: In this case, fault appears to be with the cyclist, but the probability of this
    event is moderately high, and the severity, while low in this case, could be extreme.
    It seems like a problem worthy of more analysis. One interesting option might
    be more attention to radar returns. Sometimes radar will give returns from hidden
    vehicles, even bicycles, though if the bike is very close to the truck that may
    not happen. One can’t drive assuming every truck has a hidden cyclist behind it,
    but you might do that on occasions where radar blips suggest it is more likely.
  prefs: []
  type: TYPE_NORMAL
- en: Arson
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[ForbesCrowd Attacks Waymo, Sets It On FireBy Brad Templeton](https://www.forbes.com/sites/bradtempleton/2024/02/11/crowd-attacks-waymo-sets-it-on-fire/)'
  prefs: []
  type: TYPE_NORMAL
- en: Previous reporting of an arson against a Waymo on the eve of the SuperBowl and
    Chinese New Year indicated there might be a serious effort to catch the perpetrators,
    who appear fairly clearly on publicly posted videos. In addition, the Waymo’s
    cameras should have gotten good images unless the recordings on the hard drives
    were destroyed in the fire. The mayor of San Francisco, worried about the reputation
    the city has been getting for lawlessness and poor law enforcement, seemed to
    want the crime to be solved. However, at present there have been no arrests.
  prefs: []
  type: TYPE_NORMAL
- en: Based on the video, if anybody turned in any of the perpetrators there would
    be strong evidence against them. Waymo also will have video from other cars in
    the area any of the perpetrators may have walked past. Police and Waymo would
    be wary of the creepy nature of using face recognition in this case, but other
    clues should have been present. With the perpetrators possibly unable to pay for
    the damage, Waymo may have little motive to pursue arrest in this case, unless
    it happens again, in which case they will want a deterrent against that.
  prefs: []
  type: TYPE_NORMAL
