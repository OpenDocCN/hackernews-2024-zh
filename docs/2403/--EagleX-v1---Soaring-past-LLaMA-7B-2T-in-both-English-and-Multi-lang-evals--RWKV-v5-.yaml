- en: <!--yml
  id: totrans-split-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: æœªåˆ†ç±»'
  id: totrans-split-1
  prefs: []
  type: TYPE_NORMAL
  zh: ç±»åˆ«ï¼šæœªåˆ†ç±»
- en: 'date: 2024-05-27 15:00:14'
  id: totrans-split-2
  prefs: []
  type: TYPE_NORMAL
  zh: æ—¥æœŸï¼š2024å¹´05æœˆ27æ—¥ 15:00:14
- en: -->
  id: totrans-split-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: 'ğŸ¦… EagleX v1 : Soaring past LLaMA 7B 2T in both English and Multi-lang evals
    (RWKV-v5)'
  id: totrans-split-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ğŸ¦… EagleX v1ï¼šåœ¨è‹±è¯­å’Œå¤šè¯­è¨€è¯„ä¼°ä¸­é£è¶Š LLaMA 7B 2Tï¼ˆRWKV-v5ï¼‰
- en: æ¥æºï¼š[https://substack.recursal.ai/p/eaglex-17t-soaring-past-llama-7b](https://substack.recursal.ai/p/eaglex-17t-soaring-past-llama-7b)
  id: totrans-split-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: æ¥æºï¼š[https://substack.recursal.ai/p/eaglex-17t-soaring-past-llama-7b](https://substack.recursal.ai/p/eaglex-17t-soaring-past-llama-7b)
- en: An eagle, flying past llama
  id: totrans-split-6
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€åªé¹°ï¼Œé£è¶Šäº†ä¸€åªç¾Šé©¼
- en: If you are fine-tuning, we recommend waiting for the full EagleX 2T model coming
    out later this month instead, unless you are doing so for research purpose.
  id: totrans-split-7
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: å¦‚æœä½ åœ¨è¿›è¡Œå¾®è°ƒï¼Œæˆ‘ä»¬å»ºè®®ç­‰å¾…æœ¬æœˆæ™šäº›æ—¶å€™å‘å¸ƒçš„å…¨æ–° EagleX 2T æ¨¡å‹ï¼Œé™¤éä½ æ˜¯å‡ºäºç ”ç©¶ç›®çš„è¿™ä¹ˆåšã€‚
- en: This model is released for research purposes, as it represents the major checkpoint
    that surpasses LLaMA2 7B, as part of our current training to 2T tokens and beyond.
  id: totrans-split-8
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: è¿™ä¸ªæ¨¡å‹æ˜¯å‡ºäºç ”ç©¶ç›®çš„å‘å¸ƒçš„ï¼Œå› ä¸ºå®ƒä»£è¡¨äº†è¶…è¶Š LLaMA2 7B çš„ä¸»è¦æ£€æŸ¥ç‚¹ï¼Œä½œä¸ºæˆ‘ä»¬ç›®å‰è®­ç»ƒ 2T ä»¤ç‰ŒåŠæ›´å¤šçš„ä¸€éƒ¨åˆ†ã€‚
- en: 'EagleX 1.7T is a early research release of our 7.52B parameter model training
    that:'
  id: totrans-split-9
  prefs: []
  type: TYPE_NORMAL
  zh: 'EagleX 1.7T æ˜¯æˆ‘ä»¬æ­£åœ¨è®­ç»ƒçš„7.52Bå‚æ•°æ¨¡å‹çš„æ—©æœŸç ”ç©¶å‘å¸ƒ:'
- en: We are releasing RWKV-v5 EagleX 1.7T, [licensed under Apache 2.0](https://blog.rwkv.com/p/rwkv-joins-the-linux-foundation-as),
    which can be used personally or commercially without restrictions.
  id: totrans-split-10
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å‘å¸ƒäº† RWKV-v5 EagleX 1.7Tï¼Œ[åœ¨Apache 2.0ä¸‹è®¸å¯](https://blog.rwkv.com/p/rwkv-joins-the-linux-foundation-as)ï¼Œå¯ä»¥åœ¨ä¸ªäººæˆ–å•†ä¸šç”¨é€”ä¸­æ— é™åˆ¶åœ°ä½¿ç”¨ã€‚
- en: It is a definitely a very big claim to say you have caught up and pass the â€œGold
    Standardâ€ of the 7B weight class from scratch, which nearly every other major
    open access model is built on (allegedly even Mistral). Even more so given that
    this is done with a comparatively lower dataset token count of 1.7 trillion token
    (vs. 2 trillion tokens).
  id: totrans-split-11
  prefs: []
  type: TYPE_NORMAL
  zh: å®ƒç»å¯¹æ˜¯ä¸€ä¸ªéå¸¸å¤§èƒ†çš„è¯´æ³•ï¼Œè¯´ä½ å·²ç»ä»å¤´å¼€å§‹èµ¶ä¸Šå¹¶è¶…è¿‡äº†â€œ7Bâ€é‡é‡çº§çš„â€œé»„é‡‘æ ‡å‡†â€ï¼Œå‡ ä¹æ¯ä¸€ä¸ªå…¶ä»–ä¸»è¦çš„å¼€æ”¾è®¿é—®æ¨¡å‹éƒ½å»ºç«‹åœ¨å…¶ä¸Šï¼ˆæ®è¯´ç”šè‡³åŒ…æ‹¬ Mistralï¼‰ã€‚å°¤å…¶æ˜¯è€ƒè™‘åˆ°è¿™æ˜¯ç”¨ç›¸å¯¹è¾ƒä½çš„æ•°æ®é›†ä»¤ç‰Œæ•°1.7ä¸‡äº¿ï¼ˆè€Œä¸æ˜¯2ä¸‡äº¿ä»¤ç‰Œï¼‰å®Œæˆçš„ã€‚
- en: As this is a entirely different model, trained from scratch, there will be evals
    that we win and we lose, which we are fully transparent about, in showing how
    we are ahead of LLaMA 7B on average.
  id: totrans-split-12
  prefs: []
  type: TYPE_NORMAL
  zh: ç”±äºè¿™æ˜¯ä¸€ä¸ªå®Œå…¨ä¸åŒçš„æ¨¡å‹ï¼Œä»å¤´è®­ç»ƒï¼Œæ‰€ä»¥æˆ‘ä»¬ä¼šæœ‰èµ¢å’Œè¾“çš„è¯„ä¼°ç»“æœï¼Œæˆ‘ä»¬å¯¹æ­¤æ˜¯å®Œå…¨é€æ˜çš„ï¼Œå±•ç¤ºæˆ‘ä»¬åœ¨å¹³å‡å€¼ä¸Šå¦‚ä½•é¢†å…ˆäº LLaMA 7Bã€‚
- en: 'Instead of simply cherry picking 14 different evals which we won and calling
    it a day with a victory, we ran ALL the benchmarks in EleutherAI `[lm-eval-harness](https://github.com/EleutherAI/lm-evaluation-harness)`,
    at commit `f78e2da` that we could do, with the following limitations:'
  id: totrans-split-13
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸å…¶ç®€å•åœ°ç²¾é€‰14ä¸ªä¸åŒçš„æˆ‘ä»¬èµ¢å¾—çš„è¯„ä¼°ï¼Œå¹¶æ‰“ä¸Šä¸€ä¸ªèƒœåˆ©çš„æ ‡ç­¾ï¼Œæˆ‘ä»¬åœ¨ EleutherAI `[lm-eval-harness](https://github.com/EleutherAI/lm-evaluation-harness)`
    ä¸­è¿è¡Œäº†æˆ‘ä»¬èƒ½å¤Ÿåšçš„æ‰€æœ‰åŸºå‡†æµ‹è¯•ï¼Œå…·æœ‰ä»¥ä¸‹é™åˆ¶ï¼š
- en: It has to complete in under 30 minutes on 8x4090 (we were running lots of evals)
  id: totrans-split-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å®ƒå¿…é¡»åœ¨8x4090ä¸Šåœ¨30åˆ†é’Ÿå†…å®Œæˆï¼ˆæˆ‘ä»¬è¿è¡Œäº†å¤§é‡çš„è¯„ä¼°ï¼‰
- en: We excluded all the personality / alignment evals
  id: totrans-split-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬æ’é™¤äº†æ‰€æœ‰äººæ ¼/å–å‘è¯„ä¼°
- en: Eval has to be executable across a wide variety of models, via lm-eval-harness
  id: totrans-split-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: è¯„ä¼°å¿…é¡»èƒ½åœ¨å„ç§ä¸åŒçš„æ¨¡å‹ä¸Šè¿è¡Œï¼Œé€šè¿‡ lm-eval-harness
- en: All evals are 0 shot (no 5 shot-ing an MCQ question)
  id: totrans-split-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æ‰€æœ‰çš„è¯„ä¼°éƒ½æ˜¯é›¶å°„å‡»ï¼ˆä¸ä¼š5æ¬¡å°„å‡»ä¸€é“é€‰æ‹©é¢˜ï¼‰
- en: We limited comparison to other models within the 7B weight class
  id: totrans-split-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬é™åˆ¶äº†ä¸7Bé‡é‡çº§å†…çš„å…¶ä»–æ¨¡å‹çš„æ¯”è¾ƒ
- en: These resulted into running 60+ major eval groups, which generated over 1,000+
    data points per model. A data point count so high, that we had to drop standard
    error deviations, just to ensure the raw CSV file can be loaded in MacOS numbers.
  id: totrans-split-19
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™äº›å¯¼è‡´è¿è¡Œ60å¤šä¸ªä¸»è¦è¯„ä¼°ç»„ï¼Œæ¯ä¸ªæ¨¡å‹äº§ç”Ÿ1000å¤šä¸ªæ•°æ®ç‚¹ã€‚æ•°æ®ç‚¹æ•°é‡å¦‚æ­¤ä¹‹é«˜ï¼Œä»¥è‡³äºæˆ‘ä»¬ä¸å¾—ä¸æ”¾å¼ƒæ ‡å‡†è¯¯å·®åå·®ï¼Œåªæ˜¯ä¸ºäº†ç¡®ä¿åŸå§‹ CSV æ–‡ä»¶å¯ä»¥åœ¨
    MacOS numbers ä¸­åŠ è½½ã€‚
- en: What it takes to fit 184 english eval data point onto the screen.
  id: totrans-split-20
  prefs: []
  type: TYPE_NORMAL
  zh: æŠŠ184ä¸ªè‹±è¯­è¯„ä¼°æ•°æ®ç‚¹é€‚åˆæ”¾åœ¨å±å¹•ä¸Šéœ€è¦åšäº›ä»€ä¹ˆã€‚
- en: 'Whew, thatâ€™s a crazy number of data points to digest. Let me break it down
    to more digestible parts:'
  id: totrans-split-21
  prefs: []
  type: TYPE_NORMAL
  zh: å“‡ï¼Œè¿™æ˜¯ä¸€ä¸ªç–¯ç‹‚çš„æ•°æ®ç‚¹æ•°é‡è¦æ¶ˆåŒ–ã€‚è®©æˆ‘æŠŠå®ƒåˆ†è§£æˆæ›´æ˜“æ¶ˆåŒ–çš„éƒ¨åˆ†ï¼š
- en: 'All data shown here is made available in the Google Sheet over here:'
  id: totrans-split-22
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€æœ‰è¿™é‡Œå±•ç¤ºçš„æ•°æ®å¯é€šè¿‡æ­¤ Google è¡¨æ ¼è·å¾—ï¼š
- en: We included explanations of what several of the evals mean, which you can keep
    in mind in future eval results you see (demystify what those numbers mean!)
  id: totrans-split-23
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬è§£é‡Šäº†ä¸€äº›è¯„ä¼°çš„å«ä¹‰ï¼Œè¿™äº›ä½ å°†æ¥çœ‹åˆ°çš„è¯„ä¼°ç»“æœå¯ä»¥ç‰¢è®°åœ¨å¿ƒä¸­ï¼ˆè§£å¼€è¿™äº›æ•°å­—çš„å«ä¹‰ï¼ï¼‰
- en: '* * *'
  id: totrans-split-24
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: 'We start with basics: Perplexity. This is the loss value against the test dataset
    (lower score = better), i.e. how good the model is with next token prediction.'
  id: totrans-split-25
  prefs: []
  type: TYPE_NORMAL
  zh: 'æˆ‘ä»¬ä»åŸºç¡€å¼€å§‹: å›°æƒ‘åº¦ã€‚è¿™æ˜¯å¯¹æµ‹è¯•æ•°æ®é›†çš„æŸå¤±å€¼ï¼ˆåˆ†æ•°è¶Šä½=è¶Šå¥½ï¼‰ï¼Œå³æ¨¡å‹å¯¹ä¸‹ä¸€ä¸ªæ ‡è®°é¢„æµ‹çš„å¥½åã€‚'
- en: In general, with the perplexity improvements, the EagleX model outperforms LLaMA2-7b,
    ranking between Falcom/LLaMA2-7b and Mistral.
  id: totrans-split-26
  prefs: []
  type: TYPE_NORMAL
  zh: æ€»çš„æ¥è¯´ï¼Œéšç€å›°æƒ‘åº¦æ”¹è¿›ï¼ŒEagleX æ¨¡å‹åœ¨è‹±æ–‡å’Œå¤šè¯­è¨€è¯„ä¼°ä¸­è¡¨ç°ä¼˜äº LLaMA2-7bï¼Œåœ¨ Falcom/LLaMA2-7b å’Œ Mistral
    ä¹‹é—´æ’åã€‚
- en: '**Why do experts care about perplexity?**'
  id: totrans-split-27
  prefs: []
  type: TYPE_NORMAL
  zh: '**ä¸ºä»€ä¹ˆä¸“å®¶å…³å¿ƒå›°æƒ‘åº¦ï¼Ÿ**'
- en: Eval in general can be very subjective, and opinion driven, and commonly gives
    mixed results. Perplexity in a way gives the TLDR summary for most experts to
    start with
  id: totrans-split-28
  prefs: []
  type: TYPE_NORMAL
  zh: è¯„ä¼°æ€»ä½“ä¸Šå¯èƒ½éå¸¸ä¸»è§‚å’Œå—æ„è§é©±åŠ¨ï¼Œå¹¶ä¸”é€šå¸¸ä¼šäº§ç”Ÿä¸ä¸€è‡´çš„ç»“æœã€‚å›°æƒ‘åº¦åœ¨æŸç§ç¨‹åº¦ä¸Šä¸ºå¤§å¤šæ•°ä¸“å®¶æä¾›äº†æœ€ç®€çŸ­çš„æ‘˜è¦å¼€å§‹é˜…è¯»ã€‚
- en: EagleX maintains the lead for best in class multi-lingual performance, with
    the incremental improvements weâ€™re making to the Eagle line of models.
  id: totrans-split-29
  prefs: []
  type: TYPE_NORMAL
  zh: EagleXä¿æŒäº†æœ€ä½³å¤šè¯­è¨€æ€§èƒ½çš„é¢†å…ˆåœ°ä½ï¼Œéšç€æˆ‘ä»¬å¯¹Eagleç³»åˆ—æ¨¡å‹è¿›è¡Œçš„å¢é‡æ”¹è¿›ã€‚
- en: Most of the tasks here are common sense reasoning tests of wide variety of formats,
    across languages including [23 of the worldâ€™s most widely used languages.](https://blog.rwkv.com/i/141130059/multi-lingual-performance-details)
  id: totrans-split-30
  prefs: []
  type: TYPE_NORMAL
  zh: å¤§å¤šæ•°ä»»åŠ¡éƒ½æ˜¯å¸¸è¯†æ¨ç†æµ‹è¯•ï¼Œæ ¼å¼å¤šæ ·ï¼Œæ¶µç›–äº†[åŒ…æ‹¬å…¨çƒ23ç§æœ€å¸¸ç”¨è¯­è¨€åœ¨å†…çš„å¤šç§è¯­è¨€ã€‚](https://blog.rwkv.com/i/141130059/multi-lingual-performance-details)
- en: For the remaining languages, we urge the community to test and judge it themselves,
    over a 100+ languages was trained. Over time, we would want more languages to
    be added into evals.
  id: totrans-split-31
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹äºå‰©ä½™çš„è¯­è¨€ï¼Œæˆ‘ä»¬æ•¦ä¿ƒç¤¾åŒºè‡ªè¡Œæµ‹è¯•å’Œè¯„åˆ¤ï¼Œå·²ç»è®­ç»ƒäº†è¶…è¿‡100ç§è¯­è¨€ã€‚éšç€æ—¶é—´çš„æ¨ç§»ï¼Œæˆ‘ä»¬å¸Œæœ›èƒ½å¤Ÿå¢åŠ æ›´å¤šè¯­è¨€è¿›è¡Œè¯„ä¼°ã€‚
- en: '**Why is multi-lingual perf important?**'
  id: totrans-split-32
  prefs: []
  type: TYPE_NORMAL
  zh: '**å¤šè¯­è¨€æ€§èƒ½ä¸ºä½•é‡è¦ï¼Ÿ**'
- en: The goal of the RWKV project & Eagle line of models, is to build **inclusive**
    AI for everyone regardless of their language. Our mission is to build AI models
    not just made for English, but also for the 83% of the worldâ€™s population using
    a non-English language everyday.
  id: totrans-split-33
  prefs: []
  type: TYPE_NORMAL
  zh: RWKVé¡¹ç›®åŠEagleç³»åˆ—æ¨¡å‹çš„ç›®æ ‡æ˜¯ä¸ºæ¯ä¸ªäººå»ºç«‹**åŒ…å®¹æ€§**AIï¼Œä¸è®ºå…¶è¯­è¨€å¦‚ä½•ã€‚æˆ‘ä»¬çš„ä½¿å‘½æ˜¯ä¸ä»…ä¸ºè‹±è¯­è€Œå»ºç«‹AIæ¨¡å‹ï¼Œè¿˜ä¸ºä½¿ç”¨éè‹±è¯­è¯­è¨€çš„å…¨çƒ83%äººå£å»ºç«‹AIæ¨¡å‹ã€‚
- en: 'Nevertheless, English is still important. We reduced the evals down to 21 of
    the argubly most popular English evals, such as Lambada, Glue, Swag, Winogrande,
    TruthfulQA, MMLU:'
  id: totrans-split-34
  prefs: []
  type: TYPE_NORMAL
  zh: å°½ç®¡å¦‚æ­¤ï¼Œè‹±è¯­ä»ç„¶å¾ˆé‡è¦ã€‚æˆ‘ä»¬å°†è¯„ä¼°å‡å°‘åˆ°äº†21ä¸ªå¯ä»¥è¯´æ˜¯æœ€æµè¡Œçš„è‹±è¯­è¯„ä¼°ä¹‹ä¸€ï¼Œå¦‚Lambadaã€Glueã€Swagã€Winograndeã€TruthfulQAã€MMLUï¼š
- en: Narrowing it down to the 4 models that most of us actually care about - LLaMA,
    Mistral, EagleX and Eagle-7b - the new EagleX model outperforms LLaMA-2-7b on
    average across the 21 evals, and lags not far behind Mistral.
  id: totrans-split-35
  prefs: []
  type: TYPE_NORMAL
  zh: å°†å…¶ç¼©å°åˆ°æˆ‘ä»¬å¤§å¤šæ•°äººå®é™…å…³å¿ƒçš„4ä¸ªæ¨¡å‹ - LLaMAã€Mistralã€EagleXå’ŒEagle-7b - æ–°çš„EagleXæ¨¡å‹åœ¨æ‰€æœ‰21æ¬¡è¯„ä¼°ä¸­çš„å¹³å‡è¡¨ç°è¶…è¿‡äº†LLaMA-2-7bï¼Œå¹¶ä¸”ä¸Mistralç›¸å·®æ— å‡ ã€‚
- en: Keep in mind that this average shown, is across all 21 evals
  id: totrans-split-36
  prefs: []
  type: TYPE_NORMAL
  zh: è¯·è®°ä½ï¼Œæ‰€æ˜¾ç¤ºçš„å¹³å‡å€¼æ˜¯é’ˆå¯¹æ‰€æœ‰21æ¬¡è¯„ä¼°çš„ã€‚
- en: '* * *'
  id: totrans-split-37
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Now, letâ€™s look at where our model is blowing the rest of the models out of
    the water.
  id: totrans-split-38
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨ï¼Œè®©æˆ‘ä»¬çœ‹çœ‹æˆ‘ä»¬çš„æ¨¡å‹åœ¨å“ªäº›æ–¹é¢è¶…è¶Šäº†å…¶ä»–æ¨¡å‹ã€‚
- en: First, the big stand out is the first 6 evals, which even our small 1.7T trained
    model beats out even Mistral 2T++ trained model (sciq, glue, anli, mmnli, swag),
    across multiple tasks focused around either contextual based simple Q&A with common
    sense reasoning, or deductive logic. EagleX also performs better than LLaMA-2-7b
    in wingrade and wnli evals, which also involves contextual common sense reasoning
    as well. This implies that the EagleX model would be applicable in RAG use cases,
    which are mainly contextual Q&A, with the right prompt engineering.
  id: totrans-split-39
  prefs: []
  type: TYPE_NORMAL
  zh: é¦–å…ˆï¼Œæœ€å¼•äººæ³¨ç›®çš„æ˜¯å‰6æ¬¡è¯„ä¼°ï¼Œå³ä½¿æ˜¯æˆ‘ä»¬å°å‹çš„1.7Tè®­ç»ƒæ¨¡å‹ä¹Ÿèƒ½å‡»è´¥ç”šè‡³æ˜¯Mistral 2T++è®­ç»ƒæ¨¡å‹ï¼ˆsciqã€glueã€anliã€mmnliã€swagï¼‰ï¼Œæ¶µç›–äº†å›´ç»•åŸºäºä¸Šä¸‹æ–‡çš„ç®€å•å¸¸è¯†æ¨ç†æˆ–æ¼”ç»é€»è¾‘çš„å¤šä¸ªä»»åŠ¡ã€‚EagleXåœ¨Wingradeå’ŒWNLIè¯„ä¼°ä¸­ä¹Ÿæ¯”LLaMA-2-7bè¡¨ç°æ›´å¥½ï¼Œè¿™ä¹Ÿæ¶‰åŠåˆ°åŸºäºä¸Šä¸‹æ–‡çš„å¸¸è¯†æ¨ç†ã€‚è¿™è¡¨æ˜EagleXæ¨¡å‹åœ¨ä¸»è¦æ˜¯ä¸Šä¸‹æ–‡é—®ç­”ï¼ˆRAGï¼‰ç”¨ä¾‹ä¸­æ˜¯é€‚ç”¨çš„ï¼Œåªè¦æœ‰æ­£ç¡®çš„æç¤ºå·¥ç¨‹ã€‚
- en: Finally, for truthfulqa, while it outperforms LLaMA, but in my opinion, this
    is still indicative of how vulnerable all models are from learning common human
    misconceptions from the web, seeing how bad the scores are across all models.
  id: totrans-split-40
  prefs: []
  type: TYPE_NORMAL
  zh: æœ€åï¼Œåœ¨TruthfulQAæ–¹é¢ï¼Œè™½ç„¶å®ƒçš„è¡¨ç°ä¼˜äºLLaMAï¼Œä½†åœ¨æˆ‘çœ‹æ¥ï¼Œè¿™ä»ç„¶è¡¨æ˜æ‰€æœ‰æ¨¡å‹éƒ½å®¹æ˜“ä»ç½‘ç»œä¸­å­¦ä¹ åˆ°å¸¸è§çš„äººç±»è¯¯è§£ï¼Œçœ‹åˆ°æ‰€æœ‰æ¨¡å‹å¾—åˆ†å¦‚æ­¤ç³Ÿç³•ã€‚
- en: (to be fair, this is hard for most humans as well)
  id: totrans-split-41
  prefs: []
  type: TYPE_NORMAL
  zh: ï¼ˆå…¬å¹³åœ°è¯´ï¼Œè¿™å¯¹å¤§å¤šæ•°äººç±»æ¥è¯´ä¹Ÿå¾ˆå›°éš¾ï¼‰
- en: 'PS: The jump for glue/mnli was high enough, that we needed to check the dataset
    specifically for contamination. Which we were not be able to find any. This jump
    is currently being attributed to multiple training datasets, along with data augmented
    / machine rewritten instruct dataset following a similar structure.'
  id: totrans-split-42
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: PSï¼šGlue/mnliçš„é£è·ƒè¶³å¤Ÿå¤§ï¼Œæˆ‘ä»¬éœ€è¦æ£€æŸ¥æ•°æ®é›†æ˜¯å¦å­˜åœ¨æ±¡æŸ“ã€‚ç„¶è€Œï¼Œæˆ‘ä»¬æœªèƒ½æ‰¾åˆ°ä»»ä½•æ±¡æŸ“ã€‚è¿™ä¸€è·³è·ƒç›®å‰è¢«å½’å› äºå¤šä¸ªè®­ç»ƒæ•°æ®é›†ï¼Œä»¥åŠæ•°æ®å¢å¼º/æœºå™¨é‡å†™çš„æŒ‡å¯¼æ•°æ®é›†ï¼Œéµå¾ªäº†ç±»ä¼¼çš„ç»“æ„ã€‚
- en: Strong common sense reasoning over context,
  id: totrans-split-43
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ä¸Šä¸‹æ–‡ä¸­ï¼Œå¼ºå¤§çš„å¸¸è¯†æ¨ç†ã€‚
- en: has very strong applicable use cases for multiple RAG use cases
  id: totrans-split-44
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹å¤šä¸ªRAGç”¨ä¾‹å…·æœ‰éå¸¸å¼ºçš„é€‚ç”¨æ€§
- en: 'Next: the eval sets with mixed results. Here, we have very similar evals with
    2 major variants. The results between EagleX and LLaMA are close enough, that
    itâ€™s hard to say which model is clearly better between the two for these evals.'
  id: totrans-split-45
  prefs: []
  type: TYPE_NORMAL
- en: Whatâ€™s interesting, is that even though logiqa can be seen as form of â€œcommon
    senseâ€ reasoning test, the EagleX model scored much lower compared to the 6 evals
    (sciq, glue, anli, mmnli, swag). This could mean that while the model is better
    at reasoning given a context, but it lacks the depth of knowledge compared to
    other models with more token training.
  id: totrans-split-46
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  id: totrans-split-47
  prefs: []
  type: TYPE_NORMAL
- en: These are the evals the EagleX performs worse on compared to both Mistral and
    LLaMA. However, for the evals that weâ€™ve lost to LLaMA, itâ€™s by a narrow margin.
    But weâ€™ll be keeping track of these as we train past 2T tokens.
  id: totrans-split-48
  prefs: []
  type: TYPE_NORMAL
- en: 'Letâ€™s look what went really badly: Math.'
  id: totrans-split-49
  prefs: []
  type: TYPE_NORMAL
- en: The results for arithmetic eval sank drastically, like a rock, even compared
    to our original Eagle model.
  id: totrans-split-50
  prefs: []
  type: TYPE_NORMAL
- en: What went wrong?
  id: totrans-split-51
  prefs: []
  type: TYPE_NORMAL
- en: We dug through the dataset we used for training, and realized we missed out
    the entire math dataset (along with a few others) due to an error. Oops.
  id: totrans-split-52
  prefs: []
  type: TYPE_NORMAL
- en: This emphasize the importance of maintaining the dataset composition over the
    training run. Weâ€™re adding math back for future runs.
  id: totrans-split-53
  prefs: []
  type: TYPE_NORMAL
- en: We expect overall math score to rise back up as the training continue, however
    realistically IMO - no one should be depending on a 7B model for math (just saying)
  id: totrans-split-54
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '* * *'
  id: totrans-split-55
  prefs: []
  type: TYPE_NORMAL
- en: We do not simply want to cherry pick 9 or 21 evals and claim victory over LLaMA,
    or even Mistral. So, letâ€™s zoom out, and look at it holistically across 183 English
    evals.
  id: totrans-split-56
  prefs: []
  type: TYPE_NORMAL
- en: '[You can view the full results here](https://docs.google.com/spreadsheets/d/1PFELH3u8yQlr-bGs9D5lBYXCXqSFZw2O0vfW084jbgI/edit?usp=sharing)'
  id: totrans-split-57
  prefs: []
  type: TYPE_NORMAL
- en: Although using the overall averages across all the evals does have a bias the
    results towards larger eval sets (due to double counting, e.g. mmlu overall and
    many indivudall mmlu test), it does not change the ranking among the EagleX, Mistral,
    LLaMA and the original Eagle models.
  id: totrans-split-58
  prefs: []
  type: TYPE_NORMAL
- en: However these results is extremely useful for smaller insights, for example
  id: totrans-split-59
  prefs: []
  type: TYPE_NORMAL
- en: The EagleX model lost to LLaMA2 when it comes to US history, but won in world
    history. This makes sense, given the broader approach we took to making the dataset
    from a more inclusive, more global view, instead of a US centric one.
  id: totrans-split-60
  prefs: []
  type: TYPE_NORMAL
- en: The detailed insights will be used by our dataset team to iterate and improve
    on our future datasets.
  id: totrans-split-61
  prefs: []
  type: TYPE_NORMAL
- en: How the model answer, is a reflection of the dataset experiences it has learnt
  id: totrans-split-62
  prefs: []
  type: TYPE_NORMAL
- en: How much resources the model consumes, is a reflection of its architecture
  id: totrans-split-63
  prefs: []
  type: TYPE_NORMAL
- en: One of the biggest change we did was to change the dataset for the current 1T
    tokens, which now uses a cleaner filtered set of data with **careful considerations
    to ensure permissible licensed content sources used**.
  id: totrans-split-64
  prefs: []
  type: TYPE_NORMAL
- en: There are also huge implications on the fact, the model crossed the llama2 line
    earlier then the plan schedule. That either the architecture is more efficient
    in training, or that the improvements in dataset quality has a large impact in
    model performance.
  id: totrans-split-65
  prefs: []
  type: TYPE_NORMAL
  zh: äº‹å®ä¸Šï¼Œæ¨¡å‹æ¯”è®¡åˆ’æ—¶é—´æ›´æ—©åœ°è·¨è¶Šäº†llama2çº¿ï¼Œè¿™è¡¨æ˜è¯¥æ¶æ„åœ¨è®­ç»ƒä¸­æ›´åŠ é«˜æ•ˆï¼Œæˆ–è€…æ•°æ®é›†è´¨é‡çš„æ”¹è¿›å¯¹æ¨¡å‹æ€§èƒ½æœ‰é‡å¤§å½±å“ã€‚
- en: The following is a summary of the dataset used, its public release will be made
    available next month after the current 2T training is completed.
  id: totrans-split-66
  prefs: []
  type: TYPE_NORMAL
  zh: ä»¥ä¸‹æ˜¯ä½¿ç”¨çš„æ•°æ®é›†çš„æ‘˜è¦ï¼Œå…¶å…¬å¼€å‘å¸ƒå°†åœ¨å½“å‰2Tè®­ç»ƒå®Œæˆåçš„ä¸‹ä¸ªæœˆè¿›è¡Œã€‚
- en: '[PRE0]'
  id: totrans-split-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '* * *'
  id: totrans-split-68
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: We are over a 100x more scalable then the transformer architecture.
  id: totrans-split-69
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬çš„å¯æ‰©å±•æ€§æ¯”å˜å‹å™¨æ¶æ„é«˜å‡º100å€ä»¥ä¸Šã€‚
- en: Transformers became the most prominent architecture in AI, not because it was
    the best, but it was the first to successfully scale to billion of parameters
    in training.
  id: totrans-split-70
  prefs: []
  type: TYPE_NORMAL
  zh: å˜å‹å™¨ä¹‹æ‰€ä»¥æˆä¸ºAIä¸­æœ€çªå‡ºçš„æ¶æ„ï¼Œä¸æ˜¯å› ä¸ºå®ƒæ˜¯æœ€å¥½çš„ï¼Œè€Œæ˜¯å› ä¸ºå®ƒæ˜¯é¦–ä¸ªæˆåŠŸæ‰©å±•åˆ°æ•°åäº¿å‚æ•°è®­ç»ƒçš„æ¶æ„ã€‚
- en: Till today
  id: totrans-split-71
  prefs: []
  type: TYPE_NORMAL
  zh: è‡³ä»Š
- en: CUDA computational cost, for RWKV-based architecture vs transformer models -
    that quadratic-vs-linear really scales!
  id: totrans-split-72
  prefs: []
  type: TYPE_NORMAL
  zh: RWKVæ¶æ„ä¸å˜å‹å™¨æ¨¡å‹çš„CUDAè®¡ç®—æˆæœ¬-äºŒæ¬¡ä¸çº¿æ€§çš„ç¡®å®å­˜åœ¨å·¨å¤§çš„å·®è·ï¼
- en: '* * *'
  id: totrans-split-73
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: In overall, the release of this model marks an important milestone and transition
    for many of us, within both the commercial team within Recursal AI, and the open
    source team in the RWKV group.
  id: totrans-split-74
  prefs: []
  type: TYPE_NORMAL
  zh: æ€»ä½“è€Œè¨€ï¼Œè¿™æ¬¾æ¨¡å‹çš„å‘å¸ƒæ ‡å¿—ç€æˆ‘ä»¬è®¸å¤šäººåœ¨Recursal AIçš„å•†ä¸šå›¢é˜Ÿä»¥åŠRWKVå›¢é˜Ÿå†…éƒ¨å¼€æºå›¢é˜Ÿä¸­çš„é‡è¦é‡Œç¨‹ç¢‘å’Œè½¬å˜ã€‚
- en: Its the first major training done by the Recursal AI team, in partnership with
    AWS as our main compute provider
  id: totrans-split-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä½œä¸ºæˆ‘ä»¬çš„ä¸»è¦è®¡ç®—æä¾›è€…ï¼ŒRecursal AIå›¢é˜Ÿä¸AWSåˆä½œè¿›è¡Œäº†ç¬¬ä¸€ä¸ªé‡å¤§è®­ç»ƒ
- en: This model is being released under Apache 2 licensing
  id: totrans-split-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: è¯¥æ¨¡å‹é‡‡ç”¨Apache 2è®¸å¯è¯å‘å¸ƒ
- en: The fully trained 2T model will be released under the RWKV group, under the
    Linux Foundation
  id: totrans-split-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å®Œå…¨è®­ç»ƒçš„2Tæ¨¡å‹å°†åœ¨Linux Foundationä¸‹çš„RWKVé›†å›¢å‘å¸ƒ
- en: The first Non-Transformer Architecture to pass LLaMA2 in evals
  id: totrans-split-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: é¦–ä¸ªéå˜å‹å™¨æ¶æ„é€šè¿‡LLaMA2è¯„ä¼°
- en: The strongest Linear Transformer to date
  id: totrans-split-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: è‡³ä»Šæœ€å¼ºå¤§çš„çº¿æ€§å˜å‹å™¨
- en: Proof you can have both strong multi-lingual and english performance
  id: totrans-split-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: è¯æ˜ä½ å¯ä»¥åœ¨å¤šè¯­è¨€å’Œè‹±è¯­æ€§èƒ½ä¸Šéƒ½éå¸¸å¼ºå¤§
- en: '* * *'
  id: totrans-split-81
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Similar to the original Eagle 7B announcements, the following is the revised
    goals for the model training
  id: totrans-split-82
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸åŸå§‹Eagle 7Bçš„å…¬å‘Šç±»ä¼¼ï¼Œä»¥ä¸‹æ˜¯æ¨¡å‹è®­ç»ƒçš„ä¿®è®¢ç›®æ ‡
- en: '[April 2024] Completion of the 2T Eagle 7B models'
  id: totrans-split-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[2024å¹´4æœˆ] å®Œæˆäº†2T Eagle 7Bæ¨¡å‹'
- en: '[March-May 2024] Training of our v6 â€œFinchâ€line of models'
  id: totrans-split-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[2024å¹´3æœˆè‡³5æœˆ] æˆ‘ä»¬çš„v6â€œFinchâ€ç³»åˆ—æ¨¡å‹è®­ç»ƒ'
- en: '[June 2024] v6 MoE model, for GPT 3.5 class performance'
  id: totrans-split-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[2024å¹´6æœˆ] v6 MoEæ¨¡å‹ï¼Œä¸ºGPT 3.5çº§æ€§èƒ½'
- en: 'Disclaimer: All dates are approximate, and is heavily subjected to compute
    availability from our sponsors/compute-provider/investors'
  id: totrans-split-86
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: å…è´£å£°æ˜ï¼šæ‰€æœ‰æ—¥æœŸå‡ä¸ºè¿‘ä¼¼å€¼ï¼Œå¹¶ä¸”ä¸¥é‡ä¾èµ–äºæˆ‘ä»¬èµåŠ©å•†/è®¡ç®—æä¾›è€…/æŠ•èµ„è€…çš„è®¡ç®—å¯ç”¨æ€§
- en: If you want find more about the RWKV opensource Project at
  id: totrans-split-87
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœä½ æƒ³äº†è§£æ›´å¤šå…³äºRWKVå¼€æºé¡¹ç›®çš„ä¿¡æ¯ï¼Œè¯·è®¿é—®
- en: If you like to try the model today, you can do so on our platform at [recursal.ai](https://recursal.ai)
    - the best place host, run, and create finetunes of the Eagle line of RWKV models.
  id: totrans-split-88
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœä½ ä»Šå¤©æƒ³å°è¯•è¿™æ¬¾æ¨¡å‹ï¼Œå¯ä»¥åœ¨æˆ‘ä»¬çš„å¹³å°ä¸Š[recursal.ai](https://recursal.ai)è¿›è¡Œï¼Œè¿™æ˜¯æ‰˜ç®¡ã€è¿è¡Œå’Œåˆ›å»ºEagleç³»åˆ—RWKVæ¨¡å‹çš„æœ€ä½³åœ°ç‚¹ã€‚
