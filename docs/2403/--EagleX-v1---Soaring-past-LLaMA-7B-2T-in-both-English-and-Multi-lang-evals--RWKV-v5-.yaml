- en: <!--yml
  id: totrans-split-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-split-1
  prefs: []
  type: TYPE_NORMAL
  zh: 类别：未分类
- en: 'date: 2024-05-27 15:00:14'
  id: totrans-split-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024年05月27日 15:00:14
- en: -->
  id: totrans-split-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: '🦅 EagleX v1 : Soaring past LLaMA 7B 2T in both English and Multi-lang evals
    (RWKV-v5)'
  id: totrans-split-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 🦅 EagleX v1：在英语和多语言评估中飞越 LLaMA 7B 2T（RWKV-v5）
- en: 来源：[https://substack.recursal.ai/p/eaglex-17t-soaring-past-llama-7b](https://substack.recursal.ai/p/eaglex-17t-soaring-past-llama-7b)
  id: totrans-split-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://substack.recursal.ai/p/eaglex-17t-soaring-past-llama-7b](https://substack.recursal.ai/p/eaglex-17t-soaring-past-llama-7b)
- en: An eagle, flying past llama
  id: totrans-split-6
  prefs: []
  type: TYPE_NORMAL
  zh: 一只鹰，飞越了一只羊驼
- en: If you are fine-tuning, we recommend waiting for the full EagleX 2T model coming
    out later this month instead, unless you are doing so for research purpose.
  id: totrans-split-7
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 如果你在进行微调，我们建议等待本月晚些时候发布的全新 EagleX 2T 模型，除非你是出于研究目的这么做。
- en: This model is released for research purposes, as it represents the major checkpoint
    that surpasses LLaMA2 7B, as part of our current training to 2T tokens and beyond.
  id: totrans-split-8
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 这个模型是出于研究目的发布的，因为它代表了超越 LLaMA2 7B 的主要检查点，作为我们目前训练 2T 令牌及更多的一部分。
- en: 'EagleX 1.7T is a early research release of our 7.52B parameter model training
    that:'
  id: totrans-split-9
  prefs: []
  type: TYPE_NORMAL
  zh: 'EagleX 1.7T 是我们正在训练的7.52B参数模型的早期研究发布:'
- en: We are releasing RWKV-v5 EagleX 1.7T, [licensed under Apache 2.0](https://blog.rwkv.com/p/rwkv-joins-the-linux-foundation-as),
    which can be used personally or commercially without restrictions.
  id: totrans-split-10
  prefs: []
  type: TYPE_NORMAL
  zh: 我们发布了 RWKV-v5 EagleX 1.7T，[在Apache 2.0下许可](https://blog.rwkv.com/p/rwkv-joins-the-linux-foundation-as)，可以在个人或商业用途中无限制地使用。
- en: It is a definitely a very big claim to say you have caught up and pass the “Gold
    Standard” of the 7B weight class from scratch, which nearly every other major
    open access model is built on (allegedly even Mistral). Even more so given that
    this is done with a comparatively lower dataset token count of 1.7 trillion token
    (vs. 2 trillion tokens).
  id: totrans-split-11
  prefs: []
  type: TYPE_NORMAL
  zh: 它绝对是一个非常大胆的说法，说你已经从头开始赶上并超过了“7B”重量级的“黄金标准”，几乎每一个其他主要的开放访问模型都建立在其上（据说甚至包括 Mistral）。尤其是考虑到这是用相对较低的数据集令牌数1.7万亿（而不是2万亿令牌）完成的。
- en: As this is a entirely different model, trained from scratch, there will be evals
    that we win and we lose, which we are fully transparent about, in showing how
    we are ahead of LLaMA 7B on average.
  id: totrans-split-12
  prefs: []
  type: TYPE_NORMAL
  zh: 由于这是一个完全不同的模型，从头训练，所以我们会有赢和输的评估结果，我们对此是完全透明的，展示我们在平均值上如何领先于 LLaMA 7B。
- en: 'Instead of simply cherry picking 14 different evals which we won and calling
    it a day with a victory, we ran ALL the benchmarks in EleutherAI `[lm-eval-harness](https://github.com/EleutherAI/lm-evaluation-harness)`,
    at commit `f78e2da` that we could do, with the following limitations:'
  id: totrans-split-13
  prefs: []
  type: TYPE_NORMAL
  zh: 与其简单地精选14个不同的我们赢得的评估，并打上一个胜利的标签，我们在 EleutherAI `[lm-eval-harness](https://github.com/EleutherAI/lm-evaluation-harness)`
    中运行了我们能够做的所有基准测试，具有以下限制：
- en: It has to complete in under 30 minutes on 8x4090 (we were running lots of evals)
  id: totrans-split-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它必须在8x4090上在30分钟内完成（我们运行了大量的评估）
- en: We excluded all the personality / alignment evals
  id: totrans-split-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们排除了所有人格/取向评估
- en: Eval has to be executable across a wide variety of models, via lm-eval-harness
  id: totrans-split-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 评估必须能在各种不同的模型上运行，通过 lm-eval-harness
- en: All evals are 0 shot (no 5 shot-ing an MCQ question)
  id: totrans-split-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 所有的评估都是零射击（不会5次射击一道选择题）
- en: We limited comparison to other models within the 7B weight class
  id: totrans-split-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们限制了与7B重量级内的其他模型的比较
- en: These resulted into running 60+ major eval groups, which generated over 1,000+
    data points per model. A data point count so high, that we had to drop standard
    error deviations, just to ensure the raw CSV file can be loaded in MacOS numbers.
  id: totrans-split-19
  prefs: []
  type: TYPE_NORMAL
  zh: 这些导致运行60多个主要评估组，每个模型产生1000多个数据点。数据点数量如此之高，以至于我们不得不放弃标准误差偏差，只是为了确保原始 CSV 文件可以在
    MacOS numbers 中加载。
- en: What it takes to fit 184 english eval data point onto the screen.
  id: totrans-split-20
  prefs: []
  type: TYPE_NORMAL
  zh: 把184个英语评估数据点适合放在屏幕上需要做些什么。
- en: 'Whew, that’s a crazy number of data points to digest. Let me break it down
    to more digestible parts:'
  id: totrans-split-21
  prefs: []
  type: TYPE_NORMAL
  zh: 哇，这是一个疯狂的数据点数量要消化。让我把它分解成更易消化的部分：
- en: 'All data shown here is made available in the Google Sheet over here:'
  id: totrans-split-22
  prefs: []
  type: TYPE_NORMAL
  zh: 所有这里展示的数据可通过此 Google 表格获得：
- en: We included explanations of what several of the evals mean, which you can keep
    in mind in future eval results you see (demystify what those numbers mean!)
  id: totrans-split-23
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 我们解释了一些评估的含义，这些你将来看到的评估结果可以牢记在心中（解开这些数字的含义！）
- en: '* * *'
  id: totrans-split-24
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: 'We start with basics: Perplexity. This is the loss value against the test dataset
    (lower score = better), i.e. how good the model is with next token prediction.'
  id: totrans-split-25
  prefs: []
  type: TYPE_NORMAL
  zh: '我们从基础开始: 困惑度。这是对测试数据集的损失值（分数越低=越好），即模型对下一个标记预测的好坏。'
- en: In general, with the perplexity improvements, the EagleX model outperforms LLaMA2-7b,
    ranking between Falcom/LLaMA2-7b and Mistral.
  id: totrans-split-26
  prefs: []
  type: TYPE_NORMAL
  zh: 总的来说，随着困惑度改进，EagleX 模型在英文和多语言评估中表现优于 LLaMA2-7b，在 Falcom/LLaMA2-7b 和 Mistral
    之间排名。
- en: '**Why do experts care about perplexity?**'
  id: totrans-split-27
  prefs: []
  type: TYPE_NORMAL
  zh: '**为什么专家关心困惑度？**'
- en: Eval in general can be very subjective, and opinion driven, and commonly gives
    mixed results. Perplexity in a way gives the TLDR summary for most experts to
    start with
  id: totrans-split-28
  prefs: []
  type: TYPE_NORMAL
  zh: 评估总体上可能非常主观和受意见驱动，并且通常会产生不一致的结果。困惑度在某种程度上为大多数专家提供了最简短的摘要开始阅读。
- en: EagleX maintains the lead for best in class multi-lingual performance, with
    the incremental improvements we’re making to the Eagle line of models.
  id: totrans-split-29
  prefs: []
  type: TYPE_NORMAL
  zh: EagleX保持了最佳多语言性能的领先地位，随着我们对Eagle系列模型进行的增量改进。
- en: Most of the tasks here are common sense reasoning tests of wide variety of formats,
    across languages including [23 of the world’s most widely used languages.](https://blog.rwkv.com/i/141130059/multi-lingual-performance-details)
  id: totrans-split-30
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数任务都是常识推理测试，格式多样，涵盖了[包括全球23种最常用语言在内的多种语言。](https://blog.rwkv.com/i/141130059/multi-lingual-performance-details)
- en: For the remaining languages, we urge the community to test and judge it themselves,
    over a 100+ languages was trained. Over time, we would want more languages to
    be added into evals.
  id: totrans-split-31
  prefs: []
  type: TYPE_NORMAL
  zh: 对于剩余的语言，我们敦促社区自行测试和评判，已经训练了超过100种语言。随着时间的推移，我们希望能够增加更多语言进行评估。
- en: '**Why is multi-lingual perf important?**'
  id: totrans-split-32
  prefs: []
  type: TYPE_NORMAL
  zh: '**多语言性能为何重要？**'
- en: The goal of the RWKV project & Eagle line of models, is to build **inclusive**
    AI for everyone regardless of their language. Our mission is to build AI models
    not just made for English, but also for the 83% of the world’s population using
    a non-English language everyday.
  id: totrans-split-33
  prefs: []
  type: TYPE_NORMAL
  zh: RWKV项目及Eagle系列模型的目标是为每个人建立**包容性**AI，不论其语言如何。我们的使命是不仅为英语而建立AI模型，还为使用非英语语言的全球83%人口建立AI模型。
- en: 'Nevertheless, English is still important. We reduced the evals down to 21 of
    the argubly most popular English evals, such as Lambada, Glue, Swag, Winogrande,
    TruthfulQA, MMLU:'
  id: totrans-split-34
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管如此，英语仍然很重要。我们将评估减少到了21个可以说是最流行的英语评估之一，如Lambada、Glue、Swag、Winogrande、TruthfulQA、MMLU：
- en: Narrowing it down to the 4 models that most of us actually care about - LLaMA,
    Mistral, EagleX and Eagle-7b - the new EagleX model outperforms LLaMA-2-7b on
    average across the 21 evals, and lags not far behind Mistral.
  id: totrans-split-35
  prefs: []
  type: TYPE_NORMAL
  zh: 将其缩小到我们大多数人实际关心的4个模型 - LLaMA、Mistral、EagleX和Eagle-7b - 新的EagleX模型在所有21次评估中的平均表现超过了LLaMA-2-7b，并且与Mistral相差无几。
- en: Keep in mind that this average shown, is across all 21 evals
  id: totrans-split-36
  prefs: []
  type: TYPE_NORMAL
  zh: 请记住，所显示的平均值是针对所有21次评估的。
- en: '* * *'
  id: totrans-split-37
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Now, let’s look at where our model is blowing the rest of the models out of
    the water.
  id: totrans-split-38
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们看看我们的模型在哪些方面超越了其他模型。
- en: First, the big stand out is the first 6 evals, which even our small 1.7T trained
    model beats out even Mistral 2T++ trained model (sciq, glue, anli, mmnli, swag),
    across multiple tasks focused around either contextual based simple Q&A with common
    sense reasoning, or deductive logic. EagleX also performs better than LLaMA-2-7b
    in wingrade and wnli evals, which also involves contextual common sense reasoning
    as well. This implies that the EagleX model would be applicable in RAG use cases,
    which are mainly contextual Q&A, with the right prompt engineering.
  id: totrans-split-39
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，最引人注目的是前6次评估，即使是我们小型的1.7T训练模型也能击败甚至是Mistral 2T++训练模型（sciq、glue、anli、mmnli、swag），涵盖了围绕基于上下文的简单常识推理或演绎逻辑的多个任务。EagleX在Wingrade和WNLI评估中也比LLaMA-2-7b表现更好，这也涉及到基于上下文的常识推理。这表明EagleX模型在主要是上下文问答（RAG）用例中是适用的，只要有正确的提示工程。
- en: Finally, for truthfulqa, while it outperforms LLaMA, but in my opinion, this
    is still indicative of how vulnerable all models are from learning common human
    misconceptions from the web, seeing how bad the scores are across all models.
  id: totrans-split-40
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，在TruthfulQA方面，虽然它的表现优于LLaMA，但在我看来，这仍然表明所有模型都容易从网络中学习到常见的人类误解，看到所有模型得分如此糟糕。
- en: (to be fair, this is hard for most humans as well)
  id: totrans-split-41
  prefs: []
  type: TYPE_NORMAL
  zh: （公平地说，这对大多数人类来说也很困难）
- en: 'PS: The jump for glue/mnli was high enough, that we needed to check the dataset
    specifically for contamination. Which we were not be able to find any. This jump
    is currently being attributed to multiple training datasets, along with data augmented
    / machine rewritten instruct dataset following a similar structure.'
  id: totrans-split-42
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: PS：Glue/mnli的飞跃足够大，我们需要检查数据集是否存在污染。然而，我们未能找到任何污染。这一跳跃目前被归因于多个训练数据集，以及数据增强/机器重写的指导数据集，遵循了类似的结构。
- en: Strong common sense reasoning over context,
  id: totrans-split-43
  prefs: []
  type: TYPE_NORMAL
  zh: 在上下文中，强大的常识推理。
- en: has very strong applicable use cases for multiple RAG use cases
  id: totrans-split-44
  prefs: []
  type: TYPE_NORMAL
  zh: 对多个RAG用例具有非常强的适用性
- en: 'Next: the eval sets with mixed results. Here, we have very similar evals with
    2 major variants. The results between EagleX and LLaMA are close enough, that
    it’s hard to say which model is clearly better between the two for these evals.'
  id: totrans-split-45
  prefs: []
  type: TYPE_NORMAL
- en: What’s interesting, is that even though logiqa can be seen as form of “common
    sense” reasoning test, the EagleX model scored much lower compared to the 6 evals
    (sciq, glue, anli, mmnli, swag). This could mean that while the model is better
    at reasoning given a context, but it lacks the depth of knowledge compared to
    other models with more token training.
  id: totrans-split-46
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  id: totrans-split-47
  prefs: []
  type: TYPE_NORMAL
- en: These are the evals the EagleX performs worse on compared to both Mistral and
    LLaMA. However, for the evals that we’ve lost to LLaMA, it’s by a narrow margin.
    But we’ll be keeping track of these as we train past 2T tokens.
  id: totrans-split-48
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s look what went really badly: Math.'
  id: totrans-split-49
  prefs: []
  type: TYPE_NORMAL
- en: The results for arithmetic eval sank drastically, like a rock, even compared
    to our original Eagle model.
  id: totrans-split-50
  prefs: []
  type: TYPE_NORMAL
- en: What went wrong?
  id: totrans-split-51
  prefs: []
  type: TYPE_NORMAL
- en: We dug through the dataset we used for training, and realized we missed out
    the entire math dataset (along with a few others) due to an error. Oops.
  id: totrans-split-52
  prefs: []
  type: TYPE_NORMAL
- en: This emphasize the importance of maintaining the dataset composition over the
    training run. We’re adding math back for future runs.
  id: totrans-split-53
  prefs: []
  type: TYPE_NORMAL
- en: We expect overall math score to rise back up as the training continue, however
    realistically IMO - no one should be depending on a 7B model for math (just saying)
  id: totrans-split-54
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '* * *'
  id: totrans-split-55
  prefs: []
  type: TYPE_NORMAL
- en: We do not simply want to cherry pick 9 or 21 evals and claim victory over LLaMA,
    or even Mistral. So, let’s zoom out, and look at it holistically across 183 English
    evals.
  id: totrans-split-56
  prefs: []
  type: TYPE_NORMAL
- en: '[You can view the full results here](https://docs.google.com/spreadsheets/d/1PFELH3u8yQlr-bGs9D5lBYXCXqSFZw2O0vfW084jbgI/edit?usp=sharing)'
  id: totrans-split-57
  prefs: []
  type: TYPE_NORMAL
- en: Although using the overall averages across all the evals does have a bias the
    results towards larger eval sets (due to double counting, e.g. mmlu overall and
    many indivudall mmlu test), it does not change the ranking among the EagleX, Mistral,
    LLaMA and the original Eagle models.
  id: totrans-split-58
  prefs: []
  type: TYPE_NORMAL
- en: However these results is extremely useful for smaller insights, for example
  id: totrans-split-59
  prefs: []
  type: TYPE_NORMAL
- en: The EagleX model lost to LLaMA2 when it comes to US history, but won in world
    history. This makes sense, given the broader approach we took to making the dataset
    from a more inclusive, more global view, instead of a US centric one.
  id: totrans-split-60
  prefs: []
  type: TYPE_NORMAL
- en: The detailed insights will be used by our dataset team to iterate and improve
    on our future datasets.
  id: totrans-split-61
  prefs: []
  type: TYPE_NORMAL
- en: How the model answer, is a reflection of the dataset experiences it has learnt
  id: totrans-split-62
  prefs: []
  type: TYPE_NORMAL
- en: How much resources the model consumes, is a reflection of its architecture
  id: totrans-split-63
  prefs: []
  type: TYPE_NORMAL
- en: One of the biggest change we did was to change the dataset for the current 1T
    tokens, which now uses a cleaner filtered set of data with **careful considerations
    to ensure permissible licensed content sources used**.
  id: totrans-split-64
  prefs: []
  type: TYPE_NORMAL
- en: There are also huge implications on the fact, the model crossed the llama2 line
    earlier then the plan schedule. That either the architecture is more efficient
    in training, or that the improvements in dataset quality has a large impact in
    model performance.
  id: totrans-split-65
  prefs: []
  type: TYPE_NORMAL
  zh: 事实上，模型比计划时间更早地跨越了llama2线，这表明该架构在训练中更加高效，或者数据集质量的改进对模型性能有重大影响。
- en: The following is a summary of the dataset used, its public release will be made
    available next month after the current 2T training is completed.
  id: totrans-split-66
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是使用的数据集的摘要，其公开发布将在当前2T训练完成后的下个月进行。
- en: '[PRE0]'
  id: totrans-split-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '* * *'
  id: totrans-split-68
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: We are over a 100x more scalable then the transformer architecture.
  id: totrans-split-69
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的可扩展性比变压器架构高出100倍以上。
- en: Transformers became the most prominent architecture in AI, not because it was
    the best, but it was the first to successfully scale to billion of parameters
    in training.
  id: totrans-split-70
  prefs: []
  type: TYPE_NORMAL
  zh: 变压器之所以成为AI中最突出的架构，不是因为它是最好的，而是因为它是首个成功扩展到数十亿参数训练的架构。
- en: Till today
  id: totrans-split-71
  prefs: []
  type: TYPE_NORMAL
  zh: 至今
- en: CUDA computational cost, for RWKV-based architecture vs transformer models -
    that quadratic-vs-linear really scales!
  id: totrans-split-72
  prefs: []
  type: TYPE_NORMAL
  zh: RWKV架构与变压器模型的CUDA计算成本-二次与线性的确实存在巨大的差距！
- en: '* * *'
  id: totrans-split-73
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: In overall, the release of this model marks an important milestone and transition
    for many of us, within both the commercial team within Recursal AI, and the open
    source team in the RWKV group.
  id: totrans-split-74
  prefs: []
  type: TYPE_NORMAL
  zh: 总体而言，这款模型的发布标志着我们许多人在Recursal AI的商业团队以及RWKV团队内部开源团队中的重要里程碑和转变。
- en: Its the first major training done by the Recursal AI team, in partnership with
    AWS as our main compute provider
  id: totrans-split-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 作为我们的主要计算提供者，Recursal AI团队与AWS合作进行了第一个重大训练
- en: This model is being released under Apache 2 licensing
  id: totrans-split-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 该模型采用Apache 2许可证发布
- en: The fully trained 2T model will be released under the RWKV group, under the
    Linux Foundation
  id: totrans-split-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 完全训练的2T模型将在Linux Foundation下的RWKV集团发布
- en: The first Non-Transformer Architecture to pass LLaMA2 in evals
  id: totrans-split-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 首个非变压器架构通过LLaMA2评估
- en: The strongest Linear Transformer to date
  id: totrans-split-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 至今最强大的线性变压器
- en: Proof you can have both strong multi-lingual and english performance
  id: totrans-split-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 证明你可以在多语言和英语性能上都非常强大
- en: '* * *'
  id: totrans-split-81
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Similar to the original Eagle 7B announcements, the following is the revised
    goals for the model training
  id: totrans-split-82
  prefs: []
  type: TYPE_NORMAL
  zh: 与原始Eagle 7B的公告类似，以下是模型训练的修订目标
- en: '[April 2024] Completion of the 2T Eagle 7B models'
  id: totrans-split-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[2024年4月] 完成了2T Eagle 7B模型'
- en: '[March-May 2024] Training of our v6 “Finch”line of models'
  id: totrans-split-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[2024年3月至5月] 我们的v6“Finch”系列模型训练'
- en: '[June 2024] v6 MoE model, for GPT 3.5 class performance'
  id: totrans-split-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[2024年6月] v6 MoE模型，为GPT 3.5级性能'
- en: 'Disclaimer: All dates are approximate, and is heavily subjected to compute
    availability from our sponsors/compute-provider/investors'
  id: totrans-split-86
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 免责声明：所有日期均为近似值，并且严重依赖于我们赞助商/计算提供者/投资者的计算可用性
- en: If you want find more about the RWKV opensource Project at
  id: totrans-split-87
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想了解更多关于RWKV开源项目的信息，请访问
- en: If you like to try the model today, you can do so on our platform at [recursal.ai](https://recursal.ai)
    - the best place host, run, and create finetunes of the Eagle line of RWKV models.
  id: totrans-split-88
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你今天想尝试这款模型，可以在我们的平台上[recursal.ai](https://recursal.ai)进行，这是托管、运行和创建Eagle系列RWKV模型的最佳地点。
