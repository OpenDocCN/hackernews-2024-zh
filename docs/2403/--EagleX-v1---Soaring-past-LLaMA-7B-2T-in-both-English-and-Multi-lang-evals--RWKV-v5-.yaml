- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: Êú™ÂàÜÁ±ª'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-05-27 15:00:14'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: 'ü¶Ö EagleX v1 : Soaring past LLaMA 7B 2T in both English and Multi-lang evals
    (RWKV-v5)'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Êù•Ê∫êÔºö[https://substack.recursal.ai/p/eaglex-17t-soaring-past-llama-7b](https://substack.recursal.ai/p/eaglex-17t-soaring-past-llama-7b)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: An eagle, flying past llama
  prefs: []
  type: TYPE_NORMAL
- en: If you are fine-tuning, we recommend waiting for the full EagleX 2T model coming
    out later this month instead, unless you are doing so for research purpose.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: This model is released for research purposes, as it represents the major checkpoint
    that surpasses LLaMA2 7B, as part of our current training to 2T tokens and beyond.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'EagleX 1.7T is a early research release of our 7.52B parameter model training
    that:'
  prefs: []
  type: TYPE_NORMAL
- en: We are releasing RWKV-v5 EagleX 1.7T, [licensed under Apache 2.0](https://blog.rwkv.com/p/rwkv-joins-the-linux-foundation-as),
    which can be used personally or commercially without restrictions.
  prefs: []
  type: TYPE_NORMAL
- en: It is a definitely a very big claim to say you have caught up and pass the ‚ÄúGold
    Standard‚Äù of the 7B weight class from scratch, which nearly every other major
    open access model is built on (allegedly even Mistral). Even more so given that
    this is done with a comparatively lower dataset token count of 1.7 trillion token
    (vs. 2 trillion tokens).
  prefs: []
  type: TYPE_NORMAL
- en: As this is a entirely different model, trained from scratch, there will be evals
    that we win and we lose, which we are fully transparent about, in showing how
    we are ahead of LLaMA 7B on average.
  prefs: []
  type: TYPE_NORMAL
- en: 'Instead of simply cherry picking 14 different evals which we won and calling
    it a day with a victory, we ran ALL the benchmarks in EleutherAI `[lm-eval-harness](https://github.com/EleutherAI/lm-evaluation-harness)`,
    at commit `f78e2da` that we could do, with the following limitations:'
  prefs: []
  type: TYPE_NORMAL
- en: It has to complete in under 30 minutes on 8x4090 (we were running lots of evals)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We excluded all the personality / alignment evals
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Eval has to be executable across a wide variety of models, via lm-eval-harness
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: All evals are 0 shot (no 5 shot-ing an MCQ question)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We limited comparison to other models within the 7B weight class
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These resulted into running 60+ major eval groups, which generated over 1,000+
    data points per model. A data point count so high, that we had to drop standard
    error deviations, just to ensure the raw CSV file can be loaded in MacOS numbers.
  prefs: []
  type: TYPE_NORMAL
- en: What it takes to fit 184 english eval data point onto the screen.
  prefs: []
  type: TYPE_NORMAL
- en: 'Whew, that‚Äôs a crazy number of data points to digest. Let me break it down
    to more digestible parts:'
  prefs: []
  type: TYPE_NORMAL
- en: 'All data shown here is made available in the Google Sheet over here:'
  prefs: []
  type: TYPE_NORMAL
- en: We included explanations of what several of the evals mean, which you can keep
    in mind in future eval results you see (demystify what those numbers mean!)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: 'We start with basics: Perplexity. This is the loss value against the test dataset
    (lower score = better), i.e. how good the model is with next token prediction.'
  prefs: []
  type: TYPE_NORMAL
- en: In general, with the perplexity improvements, the EagleX model outperforms LLaMA2-7b,
    ranking between Falcom/LLaMA2-7b and Mistral.
  prefs: []
  type: TYPE_NORMAL
- en: '**Why do experts care about perplexity?**'
  prefs: []
  type: TYPE_NORMAL
- en: Eval in general can be very subjective, and opinion driven, and commonly gives
    mixed results. Perplexity in a way gives the TLDR summary for most experts to
    start with
  prefs: []
  type: TYPE_NORMAL
- en: EagleX maintains the lead for best in class multi-lingual performance, with
    the incremental improvements we‚Äôre making to the Eagle line of models.
  prefs: []
  type: TYPE_NORMAL
- en: Most of the tasks here are common sense reasoning tests of wide variety of formats,
    across languages including [23 of the world‚Äôs most widely used languages.](https://blog.rwkv.com/i/141130059/multi-lingual-performance-details)
  prefs: []
  type: TYPE_NORMAL
- en: For the remaining languages, we urge the community to test and judge it themselves,
    over a 100+ languages was trained. Over time, we would want more languages to
    be added into evals.
  prefs: []
  type: TYPE_NORMAL
- en: '**Why is multi-lingual perf important?**'
  prefs: []
  type: TYPE_NORMAL
- en: The goal of the RWKV project & Eagle line of models, is to build **inclusive**
    AI for everyone regardless of their language. Our mission is to build AI models
    not just made for English, but also for the 83% of the world‚Äôs population using
    a non-English language everyday.
  prefs: []
  type: TYPE_NORMAL
- en: 'Nevertheless, English is still important. We reduced the evals down to 21 of
    the argubly most popular English evals, such as Lambada, Glue, Swag, Winogrande,
    TruthfulQA, MMLU:'
  prefs: []
  type: TYPE_NORMAL
- en: Narrowing it down to the 4 models that most of us actually care about - LLaMA,
    Mistral, EagleX and Eagle-7b - the new EagleX model outperforms LLaMA-2-7b on
    average across the 21 evals, and lags not far behind Mistral.
  prefs: []
  type: TYPE_NORMAL
- en: Keep in mind that this average shown, is across all 21 evals
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Now, let‚Äôs look at where our model is blowing the rest of the models out of
    the water.
  prefs: []
  type: TYPE_NORMAL
- en: First, the big stand out is the first 6 evals, which even our small 1.7T trained
    model beats out even Mistral 2T++ trained model (sciq, glue, anli, mmnli, swag),
    across multiple tasks focused around either contextual based simple Q&A with common
    sense reasoning, or deductive logic. EagleX also performs better than LLaMA-2-7b
    in wingrade and wnli evals, which also involves contextual common sense reasoning
    as well. This implies that the EagleX model would be applicable in RAG use cases,
    which are mainly contextual Q&A, with the right prompt engineering.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, for truthfulqa, while it outperforms LLaMA, but in my opinion, this
    is still indicative of how vulnerable all models are from learning common human
    misconceptions from the web, seeing how bad the scores are across all models.
  prefs: []
  type: TYPE_NORMAL
- en: (to be fair, this is hard for most humans as well)
  prefs: []
  type: TYPE_NORMAL
- en: 'PS: The jump for glue/mnli was high enough, that we needed to check the dataset
    specifically for contamination. Which we were not be able to find any. This jump
    is currently being attributed to multiple training datasets, along with data augmented
    / machine rewritten instruct dataset following a similar structure.'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Strong common sense reasoning over context,
  prefs: []
  type: TYPE_NORMAL
- en: has very strong applicable use cases for multiple RAG use cases
  prefs: []
  type: TYPE_NORMAL
- en: 'Next: the eval sets with mixed results. Here, we have very similar evals with
    2 major variants. The results between EagleX and LLaMA are close enough, that
    it‚Äôs hard to say which model is clearly better between the two for these evals.'
  prefs: []
  type: TYPE_NORMAL
- en: What‚Äôs interesting, is that even though logiqa can be seen as form of ‚Äúcommon
    sense‚Äù reasoning test, the EagleX model scored much lower compared to the 6 evals
    (sciq, glue, anli, mmnli, swag). This could mean that while the model is better
    at reasoning given a context, but it lacks the depth of knowledge compared to
    other models with more token training.
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: These are the evals the EagleX performs worse on compared to both Mistral and
    LLaMA. However, for the evals that we‚Äôve lost to LLaMA, it‚Äôs by a narrow margin.
    But we‚Äôll be keeping track of these as we train past 2T tokens.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let‚Äôs look what went really badly: Math.'
  prefs: []
  type: TYPE_NORMAL
- en: The results for arithmetic eval sank drastically, like a rock, even compared
    to our original Eagle model.
  prefs: []
  type: TYPE_NORMAL
- en: What went wrong?
  prefs: []
  type: TYPE_NORMAL
- en: We dug through the dataset we used for training, and realized we missed out
    the entire math dataset (along with a few others) due to an error. Oops.
  prefs: []
  type: TYPE_NORMAL
- en: This emphasize the importance of maintaining the dataset composition over the
    training run. We‚Äôre adding math back for future runs.
  prefs: []
  type: TYPE_NORMAL
- en: We expect overall math score to rise back up as the training continue, however
    realistically IMO - no one should be depending on a 7B model for math (just saying)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: We do not simply want to cherry pick 9 or 21 evals and claim victory over LLaMA,
    or even Mistral. So, let‚Äôs zoom out, and look at it holistically across 183 English
    evals.
  prefs: []
  type: TYPE_NORMAL
- en: '[You can view the full results here](https://docs.google.com/spreadsheets/d/1PFELH3u8yQlr-bGs9D5lBYXCXqSFZw2O0vfW084jbgI/edit?usp=sharing)'
  prefs: []
  type: TYPE_NORMAL
- en: Although using the overall averages across all the evals does have a bias the
    results towards larger eval sets (due to double counting, e.g. mmlu overall and
    many indivudall mmlu test), it does not change the ranking among the EagleX, Mistral,
    LLaMA and the original Eagle models.
  prefs: []
  type: TYPE_NORMAL
- en: However these results is extremely useful for smaller insights, for example
  prefs: []
  type: TYPE_NORMAL
- en: The EagleX model lost to LLaMA2 when it comes to US history, but won in world
    history. This makes sense, given the broader approach we took to making the dataset
    from a more inclusive, more global view, instead of a US centric one.
  prefs: []
  type: TYPE_NORMAL
- en: The detailed insights will be used by our dataset team to iterate and improve
    on our future datasets.
  prefs: []
  type: TYPE_NORMAL
- en: How the model answer, is a reflection of the dataset experiences it has learnt
  prefs: []
  type: TYPE_NORMAL
- en: How much resources the model consumes, is a reflection of its architecture
  prefs: []
  type: TYPE_NORMAL
- en: One of the biggest change we did was to change the dataset for the current 1T
    tokens, which now uses a cleaner filtered set of data with **careful considerations
    to ensure permissible licensed content sources used**.
  prefs: []
  type: TYPE_NORMAL
- en: There are also huge implications on the fact, the model crossed the llama2 line
    earlier then the plan schedule. That either the architecture is more efficient
    in training, or that the improvements in dataset quality has a large impact in
    model performance.
  prefs: []
  type: TYPE_NORMAL
- en: The following is a summary of the dataset used, its public release will be made
    available next month after the current 2T training is completed.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: We are over a 100x more scalable then the transformer architecture.
  prefs: []
  type: TYPE_NORMAL
- en: Transformers became the most prominent architecture in AI, not because it was
    the best, but it was the first to successfully scale to billion of parameters
    in training.
  prefs: []
  type: TYPE_NORMAL
- en: Till today
  prefs: []
  type: TYPE_NORMAL
- en: CUDA computational cost, for RWKV-based architecture vs transformer models -
    that quadratic-vs-linear really scales!
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: In overall, the release of this model marks an important milestone and transition
    for many of us, within both the commercial team within Recursal AI, and the open
    source team in the RWKV group.
  prefs: []
  type: TYPE_NORMAL
- en: Its the first major training done by the Recursal AI team, in partnership with
    AWS as our main compute provider
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This model is being released under Apache 2 licensing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The fully trained 2T model will be released under the RWKV group, under the
    Linux Foundation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The first Non-Transformer Architecture to pass LLaMA2 in evals
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The strongest Linear Transformer to date
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Proof you can have both strong multi-lingual and english performance
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Similar to the original Eagle 7B announcements, the following is the revised
    goals for the model training
  prefs: []
  type: TYPE_NORMAL
- en: '[April 2024] Completion of the 2T Eagle 7B models'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[March-May 2024] Training of our v6 ‚ÄúFinch‚Äùline of models'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[June 2024] v6 MoE model, for GPT 3.5 class performance'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Disclaimer: All dates are approximate, and is heavily subjected to compute
    availability from our sponsors/compute-provider/investors'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: If you want find more about the RWKV opensource Project at
  prefs: []
  type: TYPE_NORMAL
- en: If you like to try the model today, you can do so on our platform at [recursal.ai](https://recursal.ai)
    - the best place host, run, and create finetunes of the Eagle line of RWKV models.
  prefs: []
  type: TYPE_NORMAL
