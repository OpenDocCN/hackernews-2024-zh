- en: <!--yml
  id: totrans-split-0
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  id: totrans-split-1
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-05-27 14:41:44'
  id: totrans-split-2
  prefs: []
  type: TYPE_NORMAL
- en: -->
  id: totrans-split-3
  prefs: []
  type: TYPE_NORMAL
- en: The Pile
  id: totrans-split-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://pile.eleuther.ai/](https://pile.eleuther.ai/)
  id: totrans-split-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: What is the Pile?
  id: totrans-split-6
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The Pile is a 825 GiB diverse, open source language modelling data set that
    consists of 22 smaller, high-quality datasets combined together.
  id: totrans-split-7
  prefs: []
  type: TYPE_NORMAL
- en: Download
  id: totrans-split-8
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The Pile is hosted by [the Eye](https://the-eye.eu/).
  id: totrans-split-9
  prefs: []
  type: TYPE_NORMAL
- en: Have a model that uses or evaluates on the Pile? [Let us know](mailto:contact@eleuther.ai)!
  id: totrans-split-10
  prefs: []
  type: TYPE_NORMAL
- en: Why is the Pile a good training set?
  id: totrans-split-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Recent work has shown that especially for large models, diversity in data sources
    improves general cross-domain knowledge of the model, as well as downstream generalization
    capability. In our evaluations, not only do models trained on the Pile show moderate
    improvements in traditional language modeling benchmarks, they also show significant
    improvements on Pile BPB.
  id: totrans-split-12
  prefs: []
  type: TYPE_NORMAL
- en: Why is the Pile a good benchmark?
  id: totrans-split-13
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To score well on Pile BPB (bits per byte), a model must be able to understand
    many disparate domains including books, github repositories, webpages, chat logs,
    and medical, physics, math, computer science, and philosophy papers. Pile BPB
    is a measure of world knowledge and reasoning ability in these domains, making
    it a robust benchmark of general, cross-domain text modeling ability for large
    language models.
  id: totrans-split-14
  prefs: []
  type: TYPE_NORMAL
- en: Citing
  id: totrans-split-15
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: If you use the Pile or any of the components, please cite us!
  id: totrans-split-16
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  id: totrans-split-17
  prefs: []
  type: TYPE_PRE
