- en: <!--yml
  id: totrans-split-0
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  id: totrans-split-1
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-05-27 14:52:49'
  id: totrans-split-2
  prefs: []
  type: TYPE_NORMAL
- en: -->
  id: totrans-split-3
  prefs: []
  type: TYPE_NORMAL
- en: Robots Dream of Root Shells
  id: totrans-split-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://blog.isosceles.com/robots-dream-of-root-shells/](https://blog.isosceles.com/robots-dream-of-root-shells/)
  id: totrans-split-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: It's been an incredible year for AI. Back in the early 2000s, there were AI
    posters up all over my local computer science department, and it was all genetic
    algorithms, genetic programming, and particle swarm optimization as far as you
    could see. They could figure out if a circle was centered on an image, but it
    didn't work very well. It was the tail-end of a long [AI winter](https://en.wikipedia.org/wiki/AI_winter?ref=blog.isosceles.com).
    Fast forward to today and we're seeing all sorts of emergent *stuff*, the rate
    of progress is off the charts, and there's not a genetic algorithm in sight.
  id: totrans-split-6
  prefs: []
  type: TYPE_NORMAL
- en: Recently I've been following the new DARPA competition, the [Artificial Intelligence
    Cyber Challenge (AIxCC)](https://aicyberchallenge.com/?ref=blog.isosceles.com).
    The basic idea is to discover if we can use AI to find security vulnerabilities,
    and then use AI to fix them as well. The net result would be an autonomous system
    that can make software more secure with no humans in the loop, and that could
    have big implications for... well, *all the software in the world*.
  id: totrans-split-7
  prefs: []
  type: TYPE_NORMAL
- en: If that sounds familiar, the goal of AIxCC is somewhat similar to a previous
    DARPA competition, the Cyber Grand Challenge (CGC) in 2016\. The original hope
    for CGC was that we could take the thought processes used by capture the flag
    (CTF) players and turn them into automated systems, and overall it was quite successful.
    I suspect the organizers were expecting more in the way of novel program analysis
    techniques, but in practice most competitors converged on using [fuzzing](https://en.wikipedia.org/wiki/Fuzzing?ref=blog.isosceles.com).
    Perhaps the inadvertent success of CGC was in highlighting how much more effective
    fuzzing is than other automated bug discovery methodologies, because there's been
    a huge amount of energy and attention around fuzzing in the past 8 years, and
    not so much on all the other stuff.
  id: totrans-split-8
  prefs: []
  type: TYPE_NORMAL
- en: For better or worse, the CGC was designed around a highly contrived execution
    environment and relatively simple challenge binaries. This made the problem tractable,
    but it did raise some questions about real-world applicability, particularly around
    the exploit generation and binary patching parts. The idea of AIxCC is to set
    up a similar competition structure, but to drop the contrivances. Challenges will
    be based on real software like the Linux kernel and Jenkins, and all of the source
    code will be available. Find the bugs, fix the bugs, but you don't need to solve
    the exploit generation problem.
  id: totrans-split-9
  prefs: []
  type: TYPE_NORMAL
- en: Overall, AIxCC is probably a harder challenge than CGC, if only because performing
    automated reasoning on such vastly different types of software is so hard, and
    you can't get much more vastly different than a modern operating system kernel
    and a Java-based CI/CD server. If you think about it, the Venn diagram intersection
    of people who even know how to get both of these things set up and running is
    going to be fairly tiny.
  id: totrans-split-10
  prefs: []
  type: TYPE_NORMAL
- en: But maybe that's the point? We have AI now, and AI is in that intersection.
    AI is in the intersection of ***every*** Venn diagram.
  id: totrans-split-11
  prefs: []
  type: TYPE_NORMAL
- en: The problem is that AI doesn't seem to be very good at finding security bugs
    yet, even when the model is [specifically designed](https://arxiv.org/pdf/2306.17193.pdf?ref=blog.isosceles.com)
    to analyze code. There's some [promising results](https://arxiv.org/pdf/2402.11814.pdf?ref=blog.isosceles.com)
    around using AI to solve CTF challenges, but so far that hasn't translated to
    real world software very well. To get a sense for how quickly this space is moving
    though, a few days ago it looked like we had a [potential leap forward](https://twitter.com/JasonDClinton/status/1766233772805288006?ref=blog.isosceles.com)
    on AI-automated bug-discovery for the Linux kernel, only for it it to quickly
    fizzle out once the [details were checked](https://github.com/SeanHeelan/claude_opus_cve_2023_0266?ref=blog.isosceles.com).
  id: totrans-split-12
  prefs: []
  type: TYPE_NORMAL
- en: 'It turns out that finding security bugs in real software is hard – impossibly,
    stupidly hard – at least from the perspective of computational complexity. The
    basic problem is state explosion, where each system interaction leads to an exponential
    number of new possibilities, which in turn leads to an exponential number of new
    possibilities, and so on. If you see "find a bug in this source code" as a search
    optimization problem, then the search space is mind boggling. One way to make
    it tractable is to simplify the problem: CTF problems, CGC challenge binaries,
    looking at a single file/function at a time. But real world security bugs don''t
    work like that, they involve huge codebases with all sorts of cross-function,
    cross-library, and cross-process interactions that blow up the search space immediately.'
  id: totrans-split-13
  prefs: []
  type: TYPE_NORMAL
- en: This is why fuzzing has been winning the methodology wars. When the search space
    is this big, all of the fancy program analysis stuff breaks down, and you're left
    with some fairly primitive tools – random mutations with a code-coverage/compare-value
    feedback loop, and a bunch of clever trimming of the search space (like enabling
    compiler sanitizers to make bugs easier to trigger). But maybe AI can change that?
  id: totrans-split-14
  prefs: []
  type: TYPE_NORMAL
- en: 'At the moment there''s a minor practical problem: the LLM tech of 2024 has
    a relatively small context window, and so it''s usually not possible (or cost-effective)
    to reason on the entire system at once, unless the system is [very basic](https://twitter.com/moyix/status/1765967602982027550?ref=blog.isosceles.com).
    That means splitting the work up into smaller chunks, which is tricky to do in
    a way that doesn''t introduce ambiguity or incorrectness into the analysis, and
    that will also fundamentally limit the ability to find bugs that are "spread out"
    across a larger codebase (like use-after-free bugs, which tend not to be localized
    to a single function). I don''t think this is going to be a big problem in the
    future, because there are already models coming down the pipeline with context
    windows large enough to handle 100k lines-of-code (LOC), and that number keeps
    growing. To put this in context though (excuse the pun), the entire Linux kernel
    is about 27 million LOC – so there''s still a long way to go, and there''s definitely
    some uncertainty about how well the transformer architecture will continue to
    scale.'
  id: totrans-split-15
  prefs: []
  type: TYPE_NORMAL
- en: The bigger problem is that AI can't just magically erase the state explosion
    involved in this type of analysis, and it still has to find a way to navigate
    the same search space that a fuzzer or a human code reviewer does. We know that
    humans can navigate this search space with some degree of success, and that sometimes
    humans can even find bugs that fuzzers can't. With that said, the reason we want
    to automate this task is that human code auditors are excruciatingly slow and
    generally pretty unreliable.
  id: totrans-split-16
  prefs: []
  type: TYPE_NORMAL
- en: So how will the AI navigate this search space? I've spent years talking to security
    researchers about their techniques and approaches for finding security bugs, and
    when it comes to code review, we're really quite bad at explaining how we do it.
    The notion of "show your working" never caught on in our scene, and so you'll
    find hundreds of blog posts that intricately explain how to trigger a bug and
    what happens next, but very few that succinctly explain the steps that led to
    that discovery, and even fewer that explain why those exact steps were chosen
    in the first place.
  id: totrans-split-17
  prefs: []
  type: TYPE_NORMAL
- en: All of this is to say that the training corpus for AI-driven bug hunting seems
    to have some problematic gaps. We can still expect some amount of emergent behavior,
    but it's too much to expect these systems to match or surpass a human reviewer
    if we can't even begin to describe what the "bug hunter's mind palace" really
    looks like. The promising news here is that AI is getting really good at in-context
    learning, so if you can find a way to describe the thought process of a human
    code review, then good prompt engineering should be able to transfer some of those
    insights, even if the training corpus is bad.
  id: totrans-split-18
  prefs: []
  type: TYPE_NORMAL
- en: As an aside, I suspect the public training corpus is only going to get worse
    – I've talked about this in the past, but recently there's been a steady divergence
    between the public and private state-of-the-art in security research. Attackers
    are highly incentivized to keep their knowledge hidden and to share it with as
    few people as possible, and they're also investing in exploit development at a
    much higher rate than defenders are, so the net effect is that the security research
    you see in public presentations and blog posts is often on a completely different
    wavelength to the stuff that's happening in private. That's a big opportunity
    if you're one of the handful of groups that have access to enough relevant R&D
    to build a training dataset that has modern exploit development included, but
    not such good news for defenders.
  id: totrans-split-19
  prefs: []
  type: TYPE_NORMAL
  zh: 顺便说一句，我怀疑公共训练语料库只会变得更糟——我过去曾谈论过这个问题，但最近公共和私人安全研究的最新状态有稳定的分歧。攻击者有很强的动机保持他们的知识不公开，并尽可能少地分享给他人，他们还在比较高的速度上投入到利用开发中，而防御者的投入远远不及。因此，你在公开演示和博客文章中看到的安全研究，往往与私人领域中的研究完全不在一个频率上。如果你是少数几个能够获取足够相关研发数据集的团队之一，这就是一个巨大的机会，但对于防御者来说这并不是什么好消息。
- en: So what does this all mean for AIxCC? There's some chatter about some of the
    early work being competitive with traditional fuzzers, but nobody seems to be
    willing to show their hand before the big event. It looks like there might be
    two potential strategies emerging though. The first, AI-focused, will try to use
    the analytical ability of LLMs to directly point out where vulnerabilities in
    the source code lie. The second, fuzzing-focused, will use AI to assist in setting
    up a more traditional fuzzing environment.
  id: totrans-split-20
  prefs: []
  type: TYPE_NORMAL
  zh: 那么这一切对AIXCC意味着什么呢？关于早期工作在与传统模糊测试相竞争的传言不绝于耳，但在大事件之前没有人愿意透露他们的底牌。看起来可能会出现两种潜在的策略。第一种是以人工智能为重点，将尝试利用LLM的分析能力直接指出源代码中的漏洞位置。第二种是以模糊测试为重点，将利用人工智能来协助建立更传统的模糊测试环境。
- en: Interestingly the winners of the previous CGC competition, [Mayhem](https://www.mayhem.security/blog/mayhem-wins-darpa-cgc?ref=blog.isosceles.com),
    weren't included in the funded track for AIxCC. Perhaps the organizers thought
    that Mayhem's proposal was too fuzzing-heavy? I don't know for sure, but I hope
    that Mayhem competes in the open track so that we can get a comparison of AI-focused
    and fuzzing-focused approaches to this problem, and it would be nice to have that
    historic link back to CGC.
  id: totrans-split-21
  prefs: []
  type: TYPE_NORMAL
  zh: 有趣的是，前一届CGC竞赛的获胜者[Mayhem](https://www.mayhem.security/blog/mayhem-wins-darpa-cgc?ref=blog.isosceles.com)，并未被包括在AIXCC的资助轨道中。也许组织者认为Mayhem的提案过于侧重于模糊测试？我不能确定，但我希望Mayhem能参加公开赛轨道，这样我们可以比较以人工智能为重点和以模糊测试为重点方法在解决这个问题上的差异，而且能够回溯到CGC的历史联系也是件好事。
- en: If I were designing an entry, I would be leaning toward a fuzzing-focused approach,
    particularly given the short timelines and the current code analysis limitations
    of LLMs in 2024\. The counter-argument to this is that the organizers would probably
    prefer an AI-focused winner, so there's a decent chance that challenges will be
    designed in a way that's highly amenable to that approach.
  id: totrans-split-22
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我设计一个参赛项目，我会倾向于采用以模糊测试为重点的方法，特别是考虑到2024年LLM在当前代码分析方面的限制和时间紧迫的情况。这种方法的反对意见是，组织者可能更倾向于选择一个以人工智能为重点的获胜者，因此挑战很可能会以这种方法为基础进行设计。
- en: My intuition would still be to use LLMs to 1) help with building the target
    projects with coverage instrumentation and sanitizers enabled, 2) finding a good
    seed corpus (or a good way to generate one), and 3) generating the actual fuzzing
    harness. Creating fuzzing harnesses is a lot of manual work, and it looks like
    Google has had some good success with [LLM-generated harnesses](https://security.googleblog.com/2023/08/ai-powered-fuzzing-breaking-bug-hunting.html?ref=blog.isosceles.com).
    Then I'd let a target-appropriate fuzzer (syzkaller, afl++, ffuf, domato, etc)
    do the heavy lifting on the actual bug discovery part. Once you have an interesting
    test-case in hand, I think you could bring the LLM back into the picture for the
    source code patching process. LLMs seem to be able to handle well-scoped debugging
    tasks, and you can narrow the problem space down significantly once you have a
    test case that triggers something interesting (like a crash). Based on that I
    think you should be able to get some quite good results on the automated patching
    side, and hopefully this is the area where we will see a lasting impact from AIxCC.
  id: totrans-split-23
  prefs: []
  type: TYPE_NORMAL
  zh: 我的直觉仍然是使用LLMs来：1）帮助构建具有覆盖仪器和启用消毒剂的目标项目，2）找到一个好的种子语料库（或生成的好方法），和3）生成实际的模糊测试韧性。创建模糊测试韧性需要大量的手工工作，看起来谷歌在[LLM生成的韧性](https://security.googleblog.com/2023/08/ai-powered-fuzzing-breaking-bug-hunting.html?ref=blog.isosceles.com)方面取得了一些良好的成功。然后我会让一个适合目标的模糊器（如syzkaller、afl++、ffuf、domato等）在实际的漏洞发现部分进行大量的工作。一旦你手头有一个有趣的测试用例，我认为你可以将LLM重新引入图像进行源代码修补过程。LLMs似乎能够很好地处理范围明确的调试任务，一旦有一个触发有趣情况（如崩溃）的测试用例，你可以显著地缩小问题空间。基于此，我认为你应该能够在自动修补方面取得一些相当不错的结果，希望这是我们能从AIxCC中看到持久影响的领域。
- en: The AIxCC semi-finals are due to take place in Las Vegas later this year, with
    the final being held a year later in August 2025\. It looks like we're going to
    find out soon if AI can find and fix bugs in real world software. If it can, then
    things are going to get exciting very quickly.
  id: totrans-split-24
  prefs: []
  type: TYPE_NORMAL
  zh: AIxCC半决赛定于今年晚些时候在拉斯维加斯举行，最终将于2025年8月举行。看起来我们很快就会知道AI是否能在真实世界的软件中找到并修复错误。如果可以的话，事情将会变得非常令人兴奋。
- en: Good luck to all of the competitors, I'll be watching from afar!
  id: totrans-split-25
  prefs: []
  type: TYPE_NORMAL
  zh: 祝所有参赛者好运，我会远远地关注着！
- en: '- Ben Hawkes'
  id: totrans-split-26
  prefs: []
  type: TYPE_NORMAL
  zh: '- 本·霍克斯'
