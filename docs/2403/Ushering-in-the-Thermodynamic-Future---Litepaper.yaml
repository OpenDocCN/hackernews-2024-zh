- en: <!--yml
  id: totrans-split-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-split-1
  prefs: []
  type: TYPE_NORMAL
  zh: 'category: 未分类'
- en: 'date: 2024-05-27 14:50:06'
  id: totrans-split-2
  prefs: []
  type: TYPE_NORMAL
  zh: 'date: 2024-05-27 14:50:06'
- en: -->
  id: totrans-split-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: Ushering in the Thermodynamic Future - Litepaper
  id: totrans-split-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 开启热力学未来 - 轻论文
- en: 来源：[https://www.extropic.ai/future](https://www.extropic.ai/future)
  id: totrans-split-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://www.extropic.ai/future](https://www.extropic.ai/future)
- en: Mar 11th, 2024
  id: totrans-split-6
  prefs: []
  type: TYPE_NORMAL
  zh: 2024年3月11日
- en: /
  id: totrans-split-7
  prefs: []
  type: TYPE_NORMAL
  zh: /
- en: Litepaper
  id: totrans-split-8
  prefs: []
  type: TYPE_NORMAL
  zh: 轻论文
- en: Ushering in the Thermodynamic Future
  id: totrans-split-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 开启热力学未来
- en: Message from the team
  id: totrans-split-10
  prefs: []
  type: TYPE_NORMAL
  zh: 团队留言
- en: '*We are very excited to finally share more about what Extropic is building:
    a full-stack hardware platform to harness matter''s natural fluctuations as a
    computational resource for Generative AI.*'
  id: totrans-split-11
  prefs: []
  type: TYPE_NORMAL
  zh: '*我们非常激动地分享更多关于Extropic正在构建的内容：一个全栈硬件平台，利用物质的自然波动作为生成式人工智能的计算资源。*'
- en: What does this novel paradigm of computing practically mean for the world?
  id: totrans-split-12
  prefs: []
  type: TYPE_NORMAL
  zh: 这种新型的计算范式对世界实际意味着什么？
- en: '*Extends hardware scaling well beyond the constraints of digital computing*'
  id: totrans-split-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*将硬件扩展超越数字计算的限制*'
- en: '*Enables AI accelerators that are many orders of magnitude faster and more
    energy efficient than digital processors (CPUs/GPUs/TPUs/FPGAs)*'
  id: totrans-split-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*使得AI加速器比数字处理器（CPU/GPU/TPU/FPGA）快几个数量级，并且更加节能高效*'
- en: '*Unlocks powerful probabilistic AI algorithms that are not feasible on digital
    processors*'
  id: totrans-split-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*解锁强大的概率AI算法，在数字处理器上不可行*'
- en: '*Our brief Litepaper (below) provides an early glimpse at our technologies.
    We hope the following excites you for the journey ahead. Join us as we accelerate
    towards the thermodynamically intelligent future.*'
  id: totrans-split-16
  prefs: []
  type: TYPE_NORMAL
  zh: '*我们简短的轻论文（如下所示）提供了我们技术的早期展示。我们希望以下内容能激发您对未来旅程的期待。加入我们，共同加速走向热力学智能的未来。*'
- en: '**- Gill and Trev**'
  id: totrans-split-17
  prefs: []
  type: TYPE_NORMAL
  zh: '**- 吉尔和特雷夫**'
- en: The demand for computing power in the AI era is increasing at an unprecedented
    exponential rate. Luckily, for the past several decades, the miniaturization of
    CMOS transistor technology following Moore’s law [[1]](#references) has allowed
    much of this exponential growth to be accounted for by increasing computer efficiency.
  id: totrans-split-18
  prefs: []
  type: TYPE_NORMAL
  zh: AI时代对计算能力的需求以前所未有的指数速率增长。幸运的是，过去几十年来，随着摩尔定律[[1]](#references)推动的CMOS晶体管技术的微型化使得计算效率的提升可以解释这种指数增长的大部分。
- en: 'Unfortunately, Moore’s law is starting to slow down [[2]](#references). The
    reason for this is rooted in fundamental physics: transistors are approaching
    the atomic scale where effects like thermal noise start to forbid rigid digital
    operation [[3, 4, 5]](#references).'
  id: totrans-split-19
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，摩尔定律开始放缓[[2]](#references)。其原因根源于基础物理学：晶体管接近原子尺度，热噪声等效应开始阻止刚性数字操作[[3, 4,
    5]](#references)。
- en: As a result, the energy requirements of modern AI are beginning to take off.
    Major players are proposing measures as extreme as building nuclear reactor-powered
    data centers dedicated to large model training and inference. Continuing this
    scaling for a few more decades will require infrastructure engineering efforts
    of unprecedented scale and represents an arduous path forward for scaling humanity’s
    aggregate intelligence.
  id: totrans-split-20
  prefs: []
  type: TYPE_NORMAL
  zh: 现代AI的能源需求开始腾飞。一些主要参与者提出了极端措施，如建设核反应堆驱动的数据中心，专门用于大型模型的训练和推理。继续这种规模化工作几十年将需要前所未有的基础设施工程努力，并代表了人类智能总体扩展的艰难道路。
- en: On the other hand, biology is neither rigid nor digital and hosts computing
    circuitry that is much more efficient than anything humanity has built to date.
    Inter-cellular chemical reaction networks drive computation in biological systems.
    Cells are small, and as a result, the number of reactants in these networks is
    countable [[6, 7]](#references). Therefore, reactions between reactants are genuinely
    discrete and intrinsically random. The relative effect of this intrinsic randomness
    scales inversely with the number of reactant molecules, and as such, fluctuations
    tend to dominate the dynamics of these systems.
  id: totrans-split-21
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，生物学既非刚性也非数字化，其托管的计算电路比迄今人类建造的任何东西都要高效得多。细胞间化学反应网络驱动生物系统中的计算。细胞很小，因此这些网络中的反应物数目是可数的[[6,
    7]](#references)。因此，反应物之间的反应是真正离散且固有随机的。这种固有随机性的相对影响随着反应物分子数的增加而逆向缩小，因此波动往往主导这些系统的动态。
- en: 'From this, we can say with certainty that there is no fundamental reason for
    the constraints of digital logic to bind the efficiency of computing devices.
    The engineering challenge is clear: how can we design a complete AI hardware and
    software system from the ground up that thrives in an intrinsically noisy environment?'
  id: totrans-split-22
  prefs: []
  type: TYPE_NORMAL
  zh: 由此可以确定，数字逻辑的约束并没有束缚计算设备的效率。工程挑战显而易见：我们如何从根本上设计一个完整的AI硬件和软件系统，使其在固有噪声环境中得以发展？
- en: Energy-Based Models (EBMs) offer hints at a potential solution, as they are
    a concept that appears both in thermodynamic physics and in fundamental probabilistic
    machine learning. In physics, they are known as parameterized thermal states,
    arising from steady-states of systems with tunable parameters. In machine learning,
    they are known as exponential families.
  id: totrans-split-23
  prefs: []
  type: TYPE_NORMAL
  zh: 能量基模型（EBMs）提供了潜在解决方案的线索，因为它们既出现在热力学物理学中，又存在于基础概率机器学习中的概念。在物理学中，它们被称为参数化热态，源于具有可调参数系统的稳态。在机器学习中，它们被称为指数族。
- en: Exponential families are known to be the optimal way to parameterize probability
    distributions, requiring the minimal amount of data to uniquely determine their
    parameters [[8]](#references). They are thus excellent in the low-data regime,
    which encompasses scenarios where one needs to model tail events in mission-critical
    applications, as depicted in figure 1\. The way they achieve this is by filling
    the blanks in data with noise; they seek to maximize their entropy while matching
    the statistics of the target distribution. This process of hallucinating every
    possibility that is not included in a dataset and penalizing such occurrences
    energetically requires the usage of a lot of randomness, both at training and
    inference time.
  id: totrans-split-24
  prefs: []
  type: TYPE_NORMAL
  zh: 已知指数族是参数化概率分布的最优方式，需要最少量的数据来唯一确定它们的参数[[8]](#references)。因此，在低数据情境中它们表现出色，这包括需要模拟关键任务应用中的尾事件，如图1所示。它们的实现方式是通过用噪声填补数据中的空白；它们在最大化熵的同时匹配目标分布的统计特性。在训练和推断时，这种使所有未包含在数据集中的可能性的过程，并对此类事件进行严格的能量惩罚，需要大量随机性。
- en: 'Figure 1: The principles of Extropic probabilistic AI accelerators A simple
    example of low-complexity distributional learning failing to capture the effect
    of a tail event. Low air pressure almost always means rain and high crop yields.
    However, every so often, very low air pressure corresponds to a hurricane.'
  id: totrans-split-25
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：Extropic概率AI加速器的原理 低复杂度分布学习的一个简单例子未能捕捉尾事件的影响。低气压几乎总是意味着降雨和高产量。然而，偶尔，非常低的气压对应于飓风。
- en: 'This requirement of sampling has been the main limiter in production uses of
    EBMs. The fundamental reason for this is that sampling from generic energy landscapes
    is very difficult on digital hardware, which must expend substantial electrical
    energy to generate and shape the entropy required for the diffusion processes.
    From a hardware perspective, digital sampling seems quite contrived: why put so
    much effort into building increasingly complex pristine digital computers when
    the most common and compute-intensive algorithms turn around and pump them full
    of noise?'
  id: totrans-split-26
  prefs: []
  type: TYPE_NORMAL
  zh: 这种需要采样的需求一直是EBMs在生产中使用的主要限制因素。其根本原因是，从通用能量景观中进行采样在数字硬件上非常困难，必须消耗大量电能来生成和塑造扩散过程所需的熵。从硬件角度看，数字采样似乎相当牵强：为什么要在构建越来越复杂的完美数字计算机上花费如此大的精力，而最常见且计算密集的算法却要将其充斥着噪声？
- en: Extropic is shortcutting this inefficiency and unlocking the full potential
    of generative AI by implementing EBMs directly as parameterized stochastic analog
    circuits. Extropic accelerators will achieve many orders of magnitude of improvement
    over digital computers in terms of both runtime and energy efficiency for algorithms
    based on sampling from complex landscapes.
  id: totrans-split-27
  prefs: []
  type: TYPE_NORMAL
  zh: Extropic通过将EBMs直接实现为参数化的随机模拟电路，跳过了这种低效率，并释放了生成AI的全部潜力。在从复杂景观中采样的算法方面，Extropic加速器将在运行时和能量效率方面实现数量级的改进。
- en: The operational principle of Extropic accelerators is analogous to Brownian
    motion. In Brownian motion, macroscopic but lightweight particles suspended in
    a fluid experience random forces due to many collisions with microscopic liquid
    molecules. These collisions lead to the random diffusion of the particles around
    the vessel. One could imagine anchoring the Brownian particles to the vessel walls
    and each other with springs, as depicted in Fig. 2 (a). In this case, the springs
    will resist the random forces, and the particles will prefer to reside in particular
    parts of the vessel more than others. If one repeatedly sampled the positions
    of the particles, waiting sufficiently long in between samples (as illustrated
    in Fig. 2 (b)), one would find that they follow a predictable *steady-state* probability
    distribution. If we changed the stiffness of the springs, this distribution would
    change. This simple mechanical system is a source of programmable randomness.
  id: totrans-split-28
  prefs: []
  type: TYPE_NORMAL
  zh: 超熵加速器的运行原理类似于布朗运动。在布朗运动中，悬浮在流体中的宏观但轻质粒子受到由与微观液体分子的多次碰撞引起的随机力的影响。这些碰撞导致粒子在容器周围的随机扩散。可以想象用弹簧将布朗粒子锚定在容器壁和彼此上，如图2（a）所示。在这种情况下，弹簧将抵抗随机力，粒子更倾向于驻留在容器的特定部分而不是其他部分。如果重复采样粒子的位置，并在样本之间等待足够长的时间（如图2（b）所示），则会发现它们遵循可预测的*稳态*概率分布。如果改变弹簧的刚度，这个分布将会改变。这种简单的机械系统是可编程随机性的来源。
- en: 'Figure 2: The operational principle of Extropic accelerators  **(a)** A simple
    mechanical analogy to Extropic accelerators. Since there are three masses in two
    dimensions, the steady state of this device would be a probability distribution
    over a 6-dimensional space. (b) Samples may be drawn from an Extropic accelerator
    by repeatedly observing the system, waiting at least the equilibration time teq
    between observations. teq is how long it takes for the noise in the system to
    destroy all correlations with the previous sample.'
  id: totrans-split-29
  prefs: []
  type: TYPE_NORMAL
  zh: 图2：超熵加速器的运行原理  **(a)** 超熵加速器的简单机械类比。由于在二维空间中有三个质量，该设备的稳态将是对6维空间的概率分布。**(b)**
    可以通过重复观察系统并在观察之间等待至少达到平衡时间teq来从超熵加速器中提取样本。teq是系统中的噪声消除与先前样本的所有相关性所需的时间。
- en: There is a direct connection between this mechanical picture and the parameterized
    stochastic analog circuits that make up Extropic accelerators. The lightweight
    particles represent electrons, and the liquid molecules represent the atoms of
    the conducting medium, which can transfer energy to the electrons upon collision.
    The springs represent circuit components that confine the motion of the electrons,
    such as inductors or transistors. Control voltages/currents can be applied to
    tune the values of these components, changing the distribution from which the
    circuit samples.
  id: totrans-split-30
  prefs: []
  type: TYPE_NORMAL
  zh: 这种机械模型与组成超熵加速器的参数化随机模拟电路之间存在直接联系。轻质粒子代表电子，液体分子代表导电介质的原子，这些分子在碰撞时可以向电子传递能量。弹簧代表限制电子运动的电路元件，如电感或晶体管。可以施加控制电压/电流来调节这些元件的值，从而改变电路采样的分布。
- en: Although every circuit is noisy, not every circuit is useful as an Extropic
    accelerator. Making a noise-dominated yet well-behaved device is challenging from
    an engineering perspective. Thermal fluctuations are small, so devices must be
    physically small and low power to be strongly affected by them. For this reason,
    if one wanted to make an Extropic accelerator out of macroscopic components (on
    a PCB, for example), one would have to introduce synthetic noise. Doing so erodes
    the fundamental time and energy savings and ends up performing similarly to running
    the algorithm digitally.
  id: totrans-split-31
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管每个电路都存在噪声，但并非每个电路都能作为超熵加速器有用。制造一个噪声主导但行为良好的设备在工程上具有挑战性。热波动很小，因此设备必须物理上小且低功耗，才能受其强烈影响。因此，如果想要利用宏观组件（例如PCB）制造超熵加速器，就必须引入合成噪声。这样做会侵蚀基本的时间和能量节省，并最终表现类似于数字算法运行。
- en: Extropic’s first processors are nano fabricated from aluminum and run at low
    temperatures where they are superconducting. Fig. 3 shows an early device that
    tested several possible superconducting neuron designs. Some of these neurons
    are similar to existing super conducting flux qubits [[9]](#references). These
    neurons exploit the Josephson effect as a source of nonlinearity, which occurs
    when two superconductors are near one another. This nonlinearity is required for
    the device to access non-Gaussian probability distributions, which are necessary
    to model real-world applications with fat tails. Additionally, digital Gaussian
    sampling routines are ubiquitous and highly optimized. So, if an analog device
    is to provide a considerable speedup compared to a traditional processor, non-Gaussianity
    is required.
  id: totrans-split-32
  prefs: []
  type: TYPE_NORMAL
  zh: '**超越**的首批处理器由铝纳米制造，并在低温下运行，处于超导状态。图3展示了一个早期设备，测试了几种可能的超导神经元设计。其中一些神经元类似于现有的超导通量量子比特[[9]](#references)。这些神经元利用Josephson效应作为非线性源，当两个超导体靠近时会发生这种非线性效应。这种非线性效应对设备访问非高斯概率分布至关重要，这些分布对于模拟具有长尾分布的真实应用是必要的。此外，数字高斯抽样例程是无处不在且高度优化的。因此，如果模拟设备要比传统处理器提供显著加速，就需要非高斯性。'
- en: Figure 3: Microscope image of an Extropic chip. The inset shows two Josephson
    junctions, which are the devices that provide the processor with its critical
    nonlinearity.
  id: totrans-split-33
  prefs: []
  type: TYPE_NORMAL
  zh: 图3：**超越芯片**的显微镜图像。插图显示两个Josephson结，这些是为处理器提供关键非线性的设备。
- en: These neurons provide the basic building blocks that are combined to form a
    larger superconducting system. In such a larger system, many linear and non-linear
    neurons are combined together to create a circuit that samples from a rich and
    high-dimensional distribution. The neuron biases and interaction strengths are
    all tunable parameters of the distribution, allowing a single device to embody
    a wide family of probability distributions.
  id: totrans-split-34
  prefs: []
  type: TYPE_NORMAL
  zh: 这些神经元提供了基本的构建模块，用于构成更大的超导系统。在这样一个更大的系统中，许多线性和非线性神经元被组合在一起，创建一个从丰富和高维分布中抽样的电路。神经元的偏置和相互作用强度是分布的可调参数，使得单个设备可以包含广泛的概率分布家族。
- en: 'Extropic’s superconducting chips are entirely passive, meaning we only expend
    energy when measuring or manipulating its state. This likely makes these neurons
    the most energy-efficient in the universe. These systems will be highly energy
    efficient at scale: Extropic targets low-volume, high-value customers like governments,
    banks, and private clouds with these systems.'
  id: totrans-split-35
  prefs: []
  type: TYPE_NORMAL
  zh: '**超越**的超导芯片完全被动，这意味着只有在测量或操作其状态时才会消耗能量。这可能使得这些神经元在宇宙中是最节能的系统。在规模上，这些系统将高度能效：**超越**以低体积、高价值的客户群体如政府、银行和私有云为目标。'
- en: Extropic is also building semiconductor devices that operate at room temperature
    to extend our reach to a larger market. These devices trade the Josephson junction
    for the transistor. Doing so sacrifices some energy efficiency compared to superconducting
    devices. In exchange, it allows one to build them using standard manufacturing
    processes and supply chains, unlocking massive scale. Since they operate at room
    temperature, it will be possible to pack them into a GPU-like expansion card form
    factor. This will allow us to put an Extropic accelerator in every home, enabling
    everyone to partake in the thermodynamic AI acceleration.
  id: totrans-split-36
  prefs: []
  type: TYPE_NORMAL
  zh: '**超越**还在制造在室温下运行的半导体设备，以扩展我们的市场覆盖范围。这些设备使用晶体管代替Josephson结。这样做虽然在能效上牺牲了一些，与超导设备相比，但可以使用标准制造工艺和供应链进行大规模生产。由于它们在室温下运行，可以将它们打包成类似GPU扩展卡的形式因子。这将使我们能够在每个家庭中放置一个**超越**加速器，使每个人都能参与热力学AI加速。'
- en: To support a wide range of hardware substrates, Extropic is also building a
    software layer that compiles from abstract specifications of EBMs to the relevant
    hardware control language. This compilation layer is built upon the theoretical
    framework of factor graphs [[8]](#references). Factor graphs specify how large
    distributions factorize into local chunks. This allows Extropic accelerators to
    breakdown and run programs that are too big to fit on any given analog core.
  id: totrans-split-37
  prefs: []
  type: TYPE_NORMAL
  zh: 为了支持各种硬件基板，**超越**还在构建一个软件层，从EBM的抽象规范编译到相关的硬件控制语言。这个编译层建立在因子图的理论框架之上[[8]](#references)。因子图指定了如何将大分布分解为局部块。这使得**超越**加速器能够分解和运行超过任何给定模拟核心的程序。
- en: Many previous AI accelerator companies have struggled to find an advantage because
    of the memory-boundedness of deep learning; today’s algorithms spend around 25%
    of their time moving numbers around in memory. As a result of this, via Amdahl’s
    law, any chip that accelerates a particular operation (such as matrix multiplication)
    will struggle to achieve more than a 4x speedup. As Extropic chips natively accelerate
    a broad class of probabilistic algorithms by running them physically as a rapid
    and energy-efficient process in their entirety, we are bound to unlock a whole
    new regime of artificial intelligence acceleration well beyond what was previously
    thought achievable.
  id: totrans-split-38
  prefs: []
  type: TYPE_NORMAL
  zh: 许多以前的AI加速器公司因为深度学习的内存限制而难以找到优势；如今的算法大约花费25%的时间在内存中移动数字。因此，根据阿姆达尔定律，任何加速特定操作（如矩阵乘法）的芯片都很难实现超过4倍的加速。由于Extropic芯片通过将广泛类别的概率算法作为整体物理运行，从而快速且高效地运行，我们将开启一整个全新的人工智能加速领域，远远超出以往所能想象的范畴。
