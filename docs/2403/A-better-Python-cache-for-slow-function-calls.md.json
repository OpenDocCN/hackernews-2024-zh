["```\nimport time\nfrom file_cache import file_cache\n\n@file_cache()\ndef  slow_function(x,  y):\n time.sleep(30)\n return x + y\n\nprint(slow_function(1, 2))  # -> 3, takes 30 seconds\nprint(slow_function(1, 2))  # -> 3, takes 0 seconds\n```", "```\n(Pdb) print(todays_date)\n'2024-03-14'\n(Pdb) re.match(r\"^\\d{4}-\\d{2}-\\d{2}$\",  todays_date)\n<re.Match object; span=(0,  10),  match='2024-03-14'>\n```", "```\nimport hashlib\nfrom cache import recursive_hash\n\nclass  Obj:\n def  __init__(self,  name):\n self.name = name\n\nobj =  Obj(\"test\")\nprint(recursive_hash(obj))  # -> this works fine\ntry:\n hashlib.md5(obj).hexdigest()\nexcept  Exception  as e:\n print(e)  # -> this doesn't work\n```", "```\ndef  recursive_hash(value,  depth=0,  ignore_params=[]):\n \"\"\"Hash primitives recursively with maximum depth.\"\"\"\n if depth > MAX_DEPTH:\n return hashlib.md5(\"max_depth_reached\".encode()).hexdigest()\n\n if  isinstance(value, (int, float, str, bool, bytes)):\n return hashlib.md5(str(value).encode()).hexdigest()\n elif  isinstance(value, (list, tuple)):\n return hashlib.md5(\n \"\".join(\n [recursive_hash(item, depth +  1, ignore_params)  for item in value]\n ).encode()\n ).hexdigest()\n elif  isinstance(value, dict):\n return hashlib.md5(\n \"\".join(\n [\n recursive_hash(key, depth +  1, ignore_params)\n +  recursive_hash(val, depth +  1, ignore_params)\n for key, val in value.items()\n if key not  in ignore_params\n ]\n ).encode()\n ).hexdigest()\n elif  hasattr(value, \"__dict__\")  and value.__class__.__name__  not  in ignore_params:\n return  recursive_hash(value.__dict__, depth +  1, ignore_params)\n else:\n return hashlib.md5(\"unknown\".encode()).hexdigest()\n```", "```\n@file_cache()\ndef  search_codebase(\n cloned_github_repo,\n query,\n):\n # ... take a long time ...\n # ... llm agent logic to search through the codebase ...\n return top_results\n```", "```\ndef  wrapper(*args,  **kwargs):\n cache_dir =  \"/tmp/file_cache\"\n os.makedirs(cache_dir, exist_ok=True)\n```", "```\n func_source_code_hash =  hash_code(inspect.getsource(func))\n```", "```\n args_names = func.__code__.co_varnames[: func.__code__.co_argcount]\n args_dict =  dict(zip(args_names, args))\n\n # Remove ignored params\n kwargs_clone = kwargs.copy()\n for param in ignore_params:\n args_dict.pop(param, None)\n kwargs_clone.pop(param, None)\n\n # Create hash based on argument names, argument values, and function source code\n arg_hash = (\n recursive_hash(args_dict, ignore_params=ignore_params)\n +  recursive_hash(kwargs_clone, ignore_params=ignore_params)\n +  func_source_code_hash\n )\n cache_file = os.path.join(\n cache_dir, f\"{func.__module__}_{func.__name__}_{arg_hash}.pickle\"\n )\n```", "```\n try:\n # If cache exists, load and return it\n if os.path.exists(cache_file):\n if verbose:\n print(\"Used cache for function: \"  + func.__name__)\n with  open(cache_file, \"rb\")  as f:\n return pickle.load(f)\n except  Exception:\n logger.info(\"Unpickling failed\")\n\n # Otherwise, call the function and save its result to the cache\n result =  func(*args, **kwargs)\n try:\n with  open(cache_file, \"wb\")  as f:\n pickle.dump(result, f)\n except  Exception  as e:\n logger.info(f\"Pickling failed: {e}\")\n return result\n```"]