- en: <!--yml
  id: totrans-split-0
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  id: totrans-split-1
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-05-27 14:30:31'
  id: totrans-split-2
  prefs: []
  type: TYPE_NORMAL
- en: -->
  id: totrans-split-3
  prefs: []
  type: TYPE_NORMAL
- en: The Case for Distributed SQL Streaming Databases
  id: totrans-split-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://risingwave.com/blog/the-case-for-distributed-sql-streaming-databases/](https://risingwave.com/blog/the-case-for-distributed-sql-streaming-databases/)
  id: totrans-split-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Introduction
  id: totrans-split-6
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The statement above may seem provocative, but there is an element of truth in
    it. As businesses strives to leverage data for implementing new business models,
    it becomes clear that the existing data stack falls short. The traditional data
    stack was not designed for today’s ultra-low latency requirements. Before delving
    into the emerging requirements of new applications, it is important to step back
    and understand the expectations of data-driven organizations. Equally important
    is identifying common trends in data characteristics that are driving these new
    requirements.
  id: totrans-split-7
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s take a trip down memory lane to about a decade ago. During that time,
    the key trend in the data and analytics field was undoubtedly the ''Big Data''
    movement. Thought leaders had defined ‘Big Data’ by three V’s: Volume, Velocity
    and Variety.'
  id: totrans-split-8
  prefs: []
  type: TYPE_NORMAL
- en: In simple terms, big data refers to large and complex data sets, often from
    new sources. These data sets are too big for traditional software to handle, but
    they can be used to solve business problems that were previously impossible to
    address.
  id: totrans-split-9
  prefs: []
  type: TYPE_NORMAL
- en: There was immense promise for businesses to extract meaningful information from
    massive amounts of data. However, this potential remained untapped due to a lack
    of tools for handling such large datasets. The introduction of technologies like
    Hadoop was expected to unlock this potential. However, it became clear that these
    Big Data technologies primarily focused on addressing the ***volume*** aspect.
    As a result, widespread adoption by enterprises was not possible, as the majority
    of users didn't see the need or value in these tools.
  id: totrans-split-10
  prefs: []
  type: TYPE_NORMAL
- en: There are multiple reasons, but the primary one is the limited shelf life of
    data. Data practitioners faced challenges in accessing data in real-time, precisely
    when its intrinsic value is high. Simply storing raw data in a data lake resembled
    a data dump rather than leveraging a valuable resource.
  id: totrans-split-11
  prefs: []
  type: TYPE_NORMAL
- en: Another significant reason was the state of data engineering practices. Even
    when data was accessible, it often proved inadequate for analysis in its original
    form. As a result, complex Extract-Transform-Load (ETL) processes became necessary
    to extract valuable information from the data. Data remained isolated in separate
    systems, known as 'siloes', and closely tied to specific applications. The integration
    of data sources only recently improved through mechanisms such as message queues
    and CDC connectors.
  id: totrans-split-12
  prefs: []
  type: TYPE_NORMAL
- en: Evolution in the Characteristics of Data
  id: totrans-split-13
  prefs: []
  type: TYPE_NORMAL
- en: 'Traditionally, all data practitioners have cared about the following characteristics:'
  id: totrans-split-14
  prefs: []
  type: TYPE_NORMAL
- en: These characteristics have been addressed in relational database systems. Database
    systems' transaction management features support ACID properties to handle these
    characteristics.
  id: totrans-split-15
  prefs: []
  type: TYPE_NORMAL
- en: 'Atomicity: Ensures **completeness** with all-or-nothing semantics.'
  id: totrans-split-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Consistency: Enforces **data** **accuracy** through constraints.'
  id: totrans-split-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Isolation: Provides guarantees for **data integrity** and **correctness**.'
  id: totrans-split-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Durability: Ensures **reliability** of data based on immutable writes.'
  id: totrans-split-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These characteristics have been effective in addressing various business requirements.
    Current data processing systems have ensured robust support for these characteristics
    in any data stack. As a result, businesses have been able to execute workloads
    that rely on static snapshots of data. While there has been significant effort
    to enhance the speed and real-time capabilities of these workloads through various
    optimizations, these improvements are no longer adequate.
  id: totrans-split-20
  prefs: []
  type: TYPE_NORMAL
- en: Among industry observers, there is a growing consensus on the value of viewing
    data as a continuous unbounded stream, rather than a snapshot. Businesses are
    no longer content with knowing what happened in the past; instead, they are focused
    on predicting future outcomes, which necessitates analyzing data in "real-time".
    In this context, "real-time" is defined by data latency, rather than query latency.
    To gain a better understanding, we need to establish a new set of characteristics
    for defining data.
  id: totrans-split-21
  prefs: []
  type: TYPE_NORMAL
- en: 'To address these characteristics, a new data processing paradigm is required.
    This paradigm will:'
  id: totrans-split-22
  prefs: []
  type: TYPE_NORMAL
- en: Handle discrete granular event data.
  id: totrans-split-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Process live data continuously.
  id: totrans-split-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Integrate multiple data streams for stateful processing.
  id: totrans-split-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Early Stream Processing Solutions
  id: totrans-split-26
  prefs: []
  type: TYPE_NORMAL
- en: 'A new data processing stack is necessary to support the new paradigm discussed
    in the previous section. This data stack should have the following features:'
  id: totrans-split-27
  prefs: []
  type: TYPE_NORMAL
- en: Event data semantics to maintain consistency of event data.
  id: totrans-split-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An incremental computation model for continuous updates on live data.
  id: totrans-split-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A familiar relational data model to treat streams as tables, enabling seamless
    integration of various data sources.
  id: totrans-split-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**First-generation stream processing systems**: Stream processing systems have
    been trying to meet these requirements for quite some time. The first-generation
    stream processing systems, such as Spark Streaming, Apache Heron, and early versions
    of Flink, have proven their value in certain aspects. For instance, they excelled
    in micro-batch processing and were well-suited for specific use cases. Spark Streaming,
    for instance, was a valuable addition for Spark users looking to incorporate stream
    processing into their existing workloads. In general, these systems inherited
    many benefits from the mature batch processing model.'
  id: totrans-split-31
  prefs: []
  type: TYPE_NORMAL
- en: However, they also inherited scheduling and coordination issues from the legacy
    batch model. They did not support true event time semantics, which is crucial
    for building applications in event-driven architectures. Additionally, these technologies
    only focused on the data processing aspect. The lack of a data store meant that
    a separate data store was needed for persistence, resulting in slower application
    performance and increased operational overhead. Furthermore, these systems were
    primarily designed for early adopters who were comfortable working with low-level
    APIs and interfaces. As a result, these technologies did not significantly advance
    the ability to quickly and easily build real-time applications.
  id: totrans-split-32
  prefs: []
  type: TYPE_NORMAL
- en: New Breed of Stream Processing Solutions
  id: totrans-split-33
  prefs: []
  type: TYPE_NORMAL
- en: Early stream processing systems have traditionally been designed with a code-first
    approach. In order for stream processing to become more widely adopted, it is
    essential to incorporate SQL as the standard API. Additionally, the new system
    should include a built-in storage layer to efficiently handle data retrieval.
  id: totrans-split-34
  prefs: []
  type: TYPE_NORMAL
- en: '**Enter streaming databases**: They are designed to combine the incremental
    processing capabilities of stream-processing engines with the SQL-based analysis
    and persistence capabilities of traditional databases. The emergence of a new
    breed of streaming databases could improve operational inefficiencies compared
    to approaches that rely on separate platforms for stream and batch processing.
    Streaming databases, such as [RisingWave](https://www.risingwave.com/) and [Materialize](https://materialize.com/),
    are designed to continuously process streams of event data using SQL queries and
    real-time materialized views. They also persist historical event data for further
    analysis.'
  id: totrans-split-35
  prefs: []
  type: TYPE_NORMAL
- en: Unlike streaming compute engines that store data in an external database, streaming
    databases are specifically designed to offer built-in processing and persistence
    capabilities. This means that a single streaming database can serve as a viable
    alternative to using a combination of tools such as Apache Flink and Apache Cassandra.
    By doing so, it simplifies deployment, configuration, integration, and management
    tasks. With a streaming database, there is a paradigm shift towards moving the
    database functionality upstream, enabling real-time processing of data as it arrives
    and facilitating prompt data serving.
  id: totrans-split-36
  prefs: []
  type: TYPE_NORMAL
- en: Looking Ahead
  id: totrans-split-37
  prefs: []
  type: TYPE_NORMAL
- en: We are making stream processing accessible to a wider range of users by combining
    the strengths of early stream processing systems with traditional database systems.
    This innovative approach revolutionizes real-time data processing and empowers
    more people to utilize it. By bridging the gap between stream processing agility
    and traditional database reliability, we create a more inclusive and user-friendly
    data processing landscape.
  id: totrans-split-38
  prefs: []
  type: TYPE_NORMAL
- en: The implications of this convergence are profound. Businesses can leverage real-time
    data analysis to make informed decisions, predict outcomes, and gain a competitive
    edge. Continuous live data processing and integration of multiple data streams
    enable various use cases, including fraud detection, real-time personalization,
    supply chain optimization, and IoT analytics. Moreover, the democratization of
    stream processing allows data engineers, scientists, and analysts to tap into
    real-time data processing without requiring extensive technical expertise.
  id: totrans-split-39
  prefs: []
  type: TYPE_NORMAL
- en: In conclusion, this integration addresses limitations and empowers businesses
    with real-time data analysis. Stream processing is becoming more accessible, efficient,
    and transformative for organizations across industries. The democratization of
    stream processing is now within reach, opening up possibilities for innovation
    and growth.
  id: totrans-split-40
  prefs: []
  type: TYPE_NORMAL
