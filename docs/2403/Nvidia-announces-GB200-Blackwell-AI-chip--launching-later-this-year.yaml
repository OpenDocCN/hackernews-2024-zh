- en: <!--yml
  id: totrans-split-0
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  id: totrans-split-1
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-05-27 15:04:19'
  id: totrans-split-2
  prefs: []
  type: TYPE_NORMAL
- en: -->
  id: totrans-split-3
  prefs: []
  type: TYPE_NORMAL
- en: Nvidia announces GB200 Blackwell AI chip, launching later this year
  id: totrans-split-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://www.cnbc.com/2024/03/18/nvidia-announces-gb200-blackwell-ai-chip-launching-later-this-year.html](https://www.cnbc.com/2024/03/18/nvidia-announces-gb200-blackwell-ai-chip-launching-later-this-year.html)
  id: totrans-split-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Nvidia CEO Jensen Huang delivers a keynote address during the Nvidia GTC Artificial
    Intelligence Conference at SAP Center on March 18, 2024 in San Jose, California.
  id: totrans-split-6
  prefs: []
  type: TYPE_NORMAL
- en: Justin Sullivan | Getty Images
  id: totrans-split-7
  prefs: []
  type: TYPE_NORMAL
- en: '[Nvidia](/quotes/NVDA/) on Monday announced a new generation of artificial
    intelligence chips and software for running artificial intelligence models. The
    announcement, made during Nvidia''s developer''s conference in San Jose, comes
    as the chipmaker seeks to solidify its position as the go-to supplier for AI companies.'
  id: totrans-split-8
  prefs: []
  type: TYPE_NORMAL
- en: Nvidia's share price is up five-fold and total sales have more than tripled
    since OpenAI's ChatGPT kicked off the AI boom in late 2022\. Nvidia's high-end
    server GPUs are essential for training and deploying large AI models. Companies
    like [Microsoft](/quotes/MSFT/) and [Meta](/quotes/META/) have spent billions
    of dollars buying the chips.
  id: totrans-split-9
  prefs: []
  type: TYPE_NORMAL
- en: The new generation of AI graphics processors is named Blackwell. The first Blackwell
    chip is called the GB200 and will ship later this year. Nvidia is enticing its
    customers with more powerful chips to spur new orders. Companies and software
    makers, for example, are still scrambling to get their hands on the current generation
    of "Hopper" H100s and similar chips.
  id: totrans-split-10
  prefs: []
  type: TYPE_NORMAL
- en: “Hopper is fantastic, but we need bigger GPUs,” Nvidia CEO Jensen Huang said
    on Monday at the company's developer conference in California.
  id: totrans-split-11
  prefs: []
  type: TYPE_NORMAL
- en: Nvidia shares fell more than 1% in extended trading on Monday.
  id: totrans-split-12
  prefs: []
  type: TYPE_NORMAL
- en: The company also introduced revenue-generating software called NIM that will
    make it easier to deploy AI, giving customers another reason to stick with Nvidia
    chips over [a rising field of competitors](https://www.cnbc.com/2023/12/07/amd-stock-spikes-after-company-launches-ai-chip-to-rival-nvidia.html).
  id: totrans-split-13
  prefs: []
  type: TYPE_NORMAL
- en: Nvidia executives say that the company is becoming less of a mercenary chip
    provider and more of a platform provider, like Microsoft or Apple, on which other
    companies can build software.
  id: totrans-split-14
  prefs: []
  type: TYPE_NORMAL
- en: '"Blackwell''s not a chip, it''s the name of a platform," Huang said.'
  id: totrans-split-15
  prefs: []
  type: TYPE_NORMAL
- en: '"The sellable commercial product was the GPU and the software was all to help
    people use the GPU in different ways," said Nvidia enterprise VP Manuvir Das in
    an interview. "Of course, we still do that. But what''s really changed is, we
    really have a commercial software business now."'
  id: totrans-split-16
  prefs: []
  type: TYPE_NORMAL
- en: Das said Nvidia's new software will make it easier to run programs on any of
    Nvidia's GPUs, even older ones that might be better suited for deploying but not
    building AI.
  id: totrans-split-17
  prefs: []
  type: TYPE_NORMAL
- en: '"If you''re a developer, you''ve got an interesting model you want people to
    adopt, if you put it in a NIM, we''ll make sure that it''s runnable on all our
    GPUs, so you reach a lot of people," Das said.'
  id: totrans-split-18
  prefs: []
  type: TYPE_NORMAL
- en: Meet Blackwell, the successor to Hopper
  id: totrans-split-19
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Nvidia's GB200 Grace Blackwell Superchip, with two B200 graphics processors
    and one Arm-based central processor.
  id: totrans-split-20
  prefs: []
  type: TYPE_NORMAL
- en: Every two years Nvidia updates its GPU architecture, unlocking a big jump in
    performance. Many of the AI models released over the past year were trained on
    the company's Hopper architecture — used by chips such as the H100 — which was
    announced in 2022.
  id: totrans-split-21
  prefs: []
  type: TYPE_NORMAL
- en: Nvidia says Blackwell-based processors, like the GB200, offer a huge performance
    upgrade for AI companies, with 20 petaflops in AI performance versus 4 petaflops
    for the H100\. The additional processing power will enable AI companies to train
    bigger and more intricate models, Nvidia said.
  id: totrans-split-22
  prefs: []
  type: TYPE_NORMAL
- en: The chip includes what Nvidia calls a "transformer engine specifically built
    to run transformers-based AI, one of the core technologies underpinning ChatGPT.
  id: totrans-split-23
  prefs: []
  type: TYPE_NORMAL
- en: The Blackwell GPU is large and combines two separately manufactured dies into
    one chip manufactured by [TSMC](/quotes/TSM/). It will also be available as an
    entire server called the GB200 NVLink 2, combining 72 Blackwell GPUs and other
    Nvidia parts designed to train AI models.
  id: totrans-split-24
  prefs: []
  type: TYPE_NORMAL
- en: Nvidia CEO Jensen Huang compares the size of the new "Blackwell" chip versus
    the current "Hopper" H100 chip at the company's developer conference, in San Jose,
    California.
  id: totrans-split-25
  prefs: []
  type: TYPE_NORMAL
- en: Nvidia
  id: totrans-split-26
  prefs: []
  type: TYPE_NORMAL
- en: '[Amazon](/quotes/AMZN/), [Google](/quotes/GOOGL/), [Microsoft](/quotes/MSFT/),
    and [Oracle](/quotes/ORCL/) will sell access to the GB200 through cloud services.
    The GB200 pairs two B200 Blackwell GPUs with one Arm-based Grace CPU. Nvidia said
    Amazon Web Services would build a server cluster with 20,000 GB200 chips.'
  id: totrans-split-27
  prefs: []
  type: TYPE_NORMAL
- en: Nvidia said that the system can deploy a 27-trillion-parameter model. That's
    much larger than even the biggest models, such as GPT-4, which reportedly has
    1.7 trillion parameters. Many artificial intelligence researchers believe bigger
    models with more parameters and data [could unlock new capabilities](https://openai.com/research/ai-and-compute).
  id: totrans-split-28
  prefs: []
  type: TYPE_NORMAL
- en: Nvidia didn't provide a cost for the new GB200 or the systems it's used in.
    Nvidia's Hopper-based H100 costs between $25,000 and $40,000 per chip, with whole
    systems that cost as much as $200,000, according to analyst estimates.
  id: totrans-split-29
  prefs: []
  type: TYPE_NORMAL
- en: Nvidia will also sell B200 graphics processors as part of a complete system
    that takes up an entire server rack.
  id: totrans-split-30
  prefs: []
  type: TYPE_NORMAL
- en: Nvidia inference microservice
  id: totrans-split-31
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Nvidia also announced it's adding a new product named NIM, which stands for
    Nvidia Inference Microservice, to its Nvidia enterprise software subscription.
  id: totrans-split-32
  prefs: []
  type: TYPE_NORMAL
- en: NIM makes it easier to use older Nvidia GPUs for inference, or the process of
    running AI software, and will allow companies to continue to use the hundreds
    of millions of Nvidia GPUs they already own. Inference requires less computational
    power than the initial training of a new AI model. NIM enables companies that
    want to run their own AI models, instead of buying access to AI results as a service
    from companies like OpenAI.
  id: totrans-split-33
  prefs: []
  type: TYPE_NORMAL
- en: The strategy is to get customers who buy Nvidia-based servers to sign up for
    Nvidia enterprise, which costs $4,500 per GPU per year for a license.
  id: totrans-split-34
  prefs: []
  type: TYPE_NORMAL
- en: Nvidia will work with AI companies like Microsoft or Hugging Face to ensure
    their AI models are tuned to run on all compatible Nvidia chips. Then, using a
    NIM, developers can efficiently run the model on their own servers or cloud-based
    Nvidia servers without a lengthy configuration process.
  id: totrans-split-35
  prefs: []
  type: TYPE_NORMAL
- en: '"In my code, where I was calling into OpenAI, I will replace one line of code
    to point it to this NIM that I got from Nvidia instead," Das said.'
  id: totrans-split-36
  prefs: []
  type: TYPE_NORMAL
- en: Nvidia says the software will also help AI run on GPU-equipped laptops, instead
    of on servers in the cloud.
  id: totrans-split-37
  prefs: []
  type: TYPE_NORMAL
