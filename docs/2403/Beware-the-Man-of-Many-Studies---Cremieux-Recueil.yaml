- en: <!--yml
  id: totrans-split-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-split-1
  prefs: []
  type: TYPE_NORMAL
  zh: 'category: 未分类'
- en: 'date: 2024-05-27 14:46:48'
  id: totrans-split-2
  prefs: []
  type: TYPE_NORMAL
  zh: 'date: 2024-05-27 14:46:48'
- en: -->
  id: totrans-split-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: Beware the Man of Many Studies - Cremieux Recueil
  id: totrans-split-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Beware the Man of Many Studies - Cremieux Recueil
- en: 来源：[https://www.cremieux.xyz/p/beware-the-man-of-many-studies](https://www.cremieux.xyz/p/beware-the-man-of-many-studies)
  id: totrans-split-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://www.cremieux.xyz/p/beware-the-man-of-many-studies](https://www.cremieux.xyz/p/beware-the-man-of-many-studies)
- en: 'In 2014, Scott Alexander penned [a post](https://web.archive.org/web/20230604231635/https://slatestarcodex.com/2014/12/12/beware-the-man-of-one-study/)
    that has been widely-cited ever since. He described something everyone can agree
    is a problem: placing too much trust in singular studies instead of the results
    of literatures.'
  id: totrans-split-6
  prefs: []
  type: TYPE_NORMAL
  zh: 在2014年，斯科特·亚历山大撰写了[一篇文章](https://web.archive.org/web/20230604231635/https://slatestarcodex.com/2014/12/12/beware-the-man-of-one-study/)，此后广为引用。他描述了一个大家都认为是问题的事情：过于信任单一研究而不是文献结果。
- en: In turn, he emphasized the need to do a few things. For one, seek out comprehensive
    summaries of *all* of the evidence on a particular topic, like a high-quality
    funnel plot. For two, think about how results may need to be qualified. In his
    minimum wage example, the effects estimated in different studies could have differed
    for reasons besides the ideology of the economist behind the study. They might
    have depended on the size of the change in the minimum wage, the region or industry
    the change affected, or the time the change happened. Finally, be less confident
    about results, beware the conclusions of people who are clearly biased, and beware
    the conclusions of people who appear to be presenting a very strong case until
    you’ve done some research of your own.
  id: totrans-split-7
  prefs: []
  type: TYPE_NORMAL
  zh: 他强调需要做一些事情。首先，寻找关于特定主题的所有证据的综合总结，如高质量的漏斗图。其次，思考结果可能需要进行限定的方式。以他关于最低工资的例子来说，不同研究中估计的影响可能由于研究背后经济学家的意识形态之外的原因而有所不同。它们可能取决于最低工资变化的大小、受影响的地区或行业，或者变化发生的时间。最后，对结果要保持谨慎，警惕那些明显偏见的人的结论，并警惕那些看似提出非常强有力案例的人，直到你自己做了一些研究。
- en: But this isn’t enough. Sometimes literatures are corrupted. Scott wasn’t unaware
    of this; he did provide a passing mention of fraud and publication bias. But the
    problem is more extreme. This post briefly discusses why it is often better to
    trust a single study than a whole literature.
  id: totrans-split-8
  prefs: []
  type: TYPE_NORMAL
  zh: 但这还不够。有时文献是被扭曲的。斯科特并不对此无知；他确实提到了欺诈和出版偏倚。但问题更为严重。本文简要讨论了为什么往往更好地信任单一研究而不是整个文献的结果。
- en: '* * *'
  id: totrans-split-9
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Publication bias occurs when some results that belong in a literature are systematically
    more likely to be published than others. This can happen for many reasons, like
    editors preferring flashy results and flashy results having something in common
    like very large effects or using diverse samples.
  id: totrans-split-10
  prefs: []
  type: TYPE_NORMAL
  zh: 发表偏倚发生在某些结果更可能被出版的文献中。这可能由于编辑偏好闪光的结果，而闪光的结果有共同之处，如非常大的效应或使用多样化样本等原因。
- en: One of the more common ways it happens is through filtering for significance.
    Authors know the public and journal editors do not want studies that did not produce
    significant results, so there is an incentive to only submit a study when the
    *p-*value is below 0.05\. This also creates an incentive to take results that
    are not *p* < 0.05 and adjust them, whether through residualizing for enough covariates,
    filtering sample observations, or checking results in subsets. The point is to
    find some permutation of the data that produces *p* < 0.05 that can thus be considered
    worthy of publication.
  id: totrans-split-11
  prefs: []
  type: TYPE_NORMAL
  zh: 这种情况比较常见的一种发生方式是通过寻找显著性来实现的。作者知道公众和期刊编辑不希望没有产生显著结果的研究，因此有动机只在*p-*值低于0.05时提交研究。这也导致了对不满足*p*
    < 0.05的结果进行调整，无论是通过残差化足够的协变量、筛选样本观察，还是在子集中检查结果。关键是找到一些数据的排列组合，使得*p* < 0.05，从而被认为值得出版。
- en: There is a particular effect that is the smallest you can detect with a given
    sample size. Because of this, if your sample is small enough, your effect must
    be very large.
  id: totrans-split-12
  prefs: []
  type: TYPE_NORMAL
  zh: 有一个特定的效应，是在给定样本大小下可以检测到的最小效应。因此，如果你的样本足够小，你的效应必须非常大。
- en: 'The selection for significance and the dependencies between *p*-values, sample
    sizes, and effect sizes combine to give birth to the publication bias pattern,
    the consistent observation that effect sizes in many literatures are larger the
    less precisely they’re estimated. If you’ve read me before, you probably know
    what this looks like, but if you don’t, here’s [an example provided by Andreas
    Schneck](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5712469/):'
  id: totrans-split-13
  prefs: []
  type: TYPE_NORMAL
  zh: 显著性选择和*p*-值、样本大小和效应大小之间的依赖关系结合在一起，产生了出版偏见模式，即在许多文献中，效应大小越大，估计越不精确。如果你以前读过我的东西，你可能知道这是什么样子，但如果你不知道，这里是[安德烈亚斯·施内克提供的例子](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5712469/)。
- en: 'In funnel plots like these, the publication bias pattern is readily visible.
    The effect on the meta-analytic estimate is also quite clear: because of the large
    number of studies that have estimates that are too large because they weren’t
    precise enough, the effect seems to be much larger than it really is when there’s
    bias!'
  id: totrans-split-14
  prefs: []
  type: TYPE_NORMAL
  zh: 在这样的漏斗图中，出版偏见模式是很容易看到的。对元分析估计值的影响也非常明显：由于有大量的研究估计值过大，因为它们不够精确，所以在存在偏见时，效果似乎比实际上更大！
- en: 'Many literatures actually look like that plot on the right. Consider two examples:
    [air pollution effects on various outcomes](https://web.archive.org/web/20230526233206/https://www.econstor.eu/bitstream/10419/266386/1/I4R-DP011.pdf)
    and [mindfulness effects on students outcomes](https://www.frontiersin.org/articles/10.3389/fpsyg.2014.00603/full).'
  id: totrans-split-15
  prefs: []
  type: TYPE_NORMAL
  zh: 许多文献实际上看起来就像右边的图表。考虑两个例子：[空气污染对各种结果的影响](https://web.archive.org/web/20230526233206/https://www.econstor.eu/bitstream/10419/266386/1/I4R-DP011.pdf)
    和 [正念对学生结果的影响](https://www.frontiersin.org/articles/10.3389/fpsyg.2014.00603/full)。
- en: '[The trim-and-fill method](https://onlinelibrary.wiley.com/doi/abs/10.1111/j.0006-341X.2000.00455.x)
    is a ready-made way to correct this bias by adding datapoints that mirror the
    observed asymmetry in the plot on the other side of the meta-analytic estimate.
    The original examples makes it pretty clear how this works. In these figures,
    the “filled-in” estimates are the empty white dots and the black dots are the
    estimates that were observed in the meta-analyses. After adding the white dots
    and effectively limiting the power of the imprecise, outlying results to exaggerate
    the meta-analytic estimate, the new estimates are greatly reduced. There are many
    more severe cases where estimates become entirely null after correction.'
  id: totrans-split-16
  prefs: []
  type: TYPE_NORMAL
  zh: '[修剪和填充方法](https://onlinelibrary.wiley.com/doi/abs/10.1111/j.0006-341X.2000.00455.x)
    是一种现成的方法，通过添加反映元分析估计值另一侧绘图中观察到的不对称的数据点来纠正这种偏差。原始例子很清楚地说明了这是如何运作的。在这些图表中，“填充”估计值是空白的白点，黑点是在元分析中观察到的估计值。在添加白点并有效限制不准确的、异常的结果夸大元分析估计的能力之后，新的估计大大减少。在许多更严重的情况下，估计值在校正后变得完全为零。'
- en: There are other publication bias correction methods like PET, PEESE, PET-PEESE,
    *p*-curve, and three-part selection models. The only problem with these methods
    is that they only work when the number of studies is larger, the publication bias
    pattern is observed because of a filter for significance, and when the pattern
    is weak, but not so weak that the algorithms for bias correction don’t recognize
    points as requiring correction.
  id: totrans-split-17
  prefs: []
  type: TYPE_NORMAL
  zh: 还有其他出版偏见校正方法，如PET、PEESE、PET-PEESE、*p*-曲线和三部分选择模型。这些方法的唯一问题是，它们仅在研究数量较大、出版偏见模式由于对显著性的过滤而观察到，以及模式较弱但算法校正偏见不认为需要校正的情况下才有效。
- en: 'We know this is true because we can simulate it and because it has been empirically
    tested. The empirical tests show very poor results: every method tends to undercorrect
    the meta-analytic estimate. On the left-hand side, you have the meta-analytic
    effect size and right next to it, you have the effect size from large, preregistered
    replications. The yellow columns represent the effect sizes obtained when the
    meta-analytic estimates are adjusted with PET-PEESE — a correction many consider
    too conservative — 3PSM, and trim-and-fill.'
  id: totrans-split-18
  prefs: []
  type: TYPE_NORMAL
  zh: 我们知道这是真的，因为我们可以模拟它，而且已经进行了实证测试。实证测试结果非常糟糕：每种方法都倾向于未能正确校正元分析估计值。在左边，你有元分析效应大小，紧邻其旁，你有来自大型、预注册复制研究的效应大小。黄色的柱代表通过PET-PEESE（许多人认为过于保守的校正），3PSM和修剪填充调整元分析估计值时获得的效应大小。
- en: '[This point is not unique](https://www.frontiersin.org/articles/10.3389/fpsyg.2015.01365/full).
    My favorite replication showed an incredibly visible publication bias effect in
    the literature on money priming. Note the results for published and unpublished
    studies. One was clearly larger and more biased than the other. Interaction effects,
    which are harder to detect than main effects, were also subject to very notable
    publication bias. Because of the low power to detect them, preregistered studies
    probably didn’t even attempt to look for them either. And finally, pre-registered
    effects were centered precisely around 0\. Money priming is not real. Like many
    effects that build extensive literatures, it was an invention that was built by
    publication bias.'
  id: totrans-split-19
  prefs: []
  type: TYPE_NORMAL
  zh: '[这一点并不独特](https://www.frontiersin.org/articles/10.3389/fpsyg.2015.01365/full)。我最喜欢的复制研究显示，货币启动文献中存在着非常显著的出版偏见效应。注意已发表和未发表研究的结果。一个明显比另一个更大且更有偏见。交互作用效应比主效应更难检测，也受到非常显著的出版偏见的影响。由于检测它们的能力较低，预注册研究可能根本没有尝试去寻找它们。最后，预注册的效应正好集中在0周围。货币启动并不真实。像许多建立广泛文献的效应一样，它是出版偏见所造成的一种发明。'
- en: Some of the publication bias pattern happens due to minimal filtering for significance,
    simply shooting for a *p-*value less than 0.05, and *just* under 0.05\. [As I’ve
    written elsewhere](https://cremieux.substack.com/p/ranking-fields-by-p-value-suspiciousness),
    this leads to a bump in the distribution of *p*-values so that there’s an excess
    right below the threshold for significance. But it doesn’t have to be this way,
    and that weakens publication bias corrections. This occurs when people perform
    their *p*-hacking — knowingly or otherwise — in a way that creates an effect that
    is *extremely* significant rather than *just* significant.
  id: totrans-split-20
  prefs: []
  type: TYPE_NORMAL
  zh: 一些出版偏见模式的发生是由于对显著性的最小过滤，简单地追求小于0.05的 *p-* 值，并且 *刚好* 在0.05以下。[正如我在其他地方写过的](https://cremieux.substack.com/p/ranking-fields-by-p-value-suspiciousness)，这导致
    *p-* 值分布中的一个峰值，在显著性阈值下方有过多的存在。但这并非必然如此，这削弱了出版偏见的纠正作用。这发生在人们进行他们的 *p-* 操纵时，无论是否故意，都会创造一个
    *极其* 显著而不是 *仅仅* 显著的效果。
- en: Sometimes something similar to the publication bias pattern crops up for reasons
    that are not publication bias. This is what happens [when an intervention fails
    to scale](https://web.archive.org/web/20230118032728/https://www.nber.org/papers/w30850).
    Because scaling failures could lead to a lower expectation for future effects,
    they may actually be a good thing if they lead to power analyses that are more
    conservative and either suggest the need for larger samples or more funding per
    person. But if larger samples result in smaller effects because of scaling issues,
    this can be a self-reinforcing problem. Because of this, differentiating publication
    bias from a scaling failure can be difficult and requires large, preregistered
    replications to provide a credible adjudication between or qualification of different
    explanations.
  id: totrans-split-21
  prefs: []
  type: TYPE_NORMAL
  zh: 有时，类似于出版偏见模式的东西出现的原因并不是出版偏见。这就是 [干预措施未能扩展时](https://web.archive.org/web/20230118032728/https://www.nber.org/papers/w30850)
    的情况。因为扩展失败可能导致对未来效果的期望降低，如果这导致更为保守的功效分析并建议需要更大的样本或更多的资金每人。但如果由于扩展问题导致更大样本的结果更小，这可能会成为一个自我强化的问题。因此，区分出版偏见和扩展失败可能会很困难，并需要大规模、预注册的复制实验来提供可靠的裁定或不同解释的限定。
- en: One thing is certain though, and it’s that scaling failures *should not* produce
    results that are *just* significant or which appear to produce a bump below significance
    thresholds. Scaling failures without publication bias should lead to results that
    are not significant rather than *just* significant. They could augment the risk
    that *p*-hacking happens, but this means scaling failures should be thought of
    as a promoter of publication bias rather than a complete alternative to it.
  id: totrans-split-22
  prefs: []
  type: TYPE_NORMAL
  zh: 然而有一点是肯定的，那就是扩展失败 *不应该* 产生 *仅仅* 显著或者看起来产生显著性阈值下方的峰值的结果。没有出版偏见的扩展失败应该导致结果不显著而不是
    *仅仅* 显著。它们可能增加 *p-* 操纵发生的风险，但这意味着扩展失败应被视为出版偏见的促进者，而不是完全的替代品。
- en: Because of how often preregistered replications vindicate the explanation being
    largely or entirely publication bias, I tend to expect it to explain results far
    more often than scaling failures do.
  id: totrans-split-23
  prefs: []
  type: TYPE_NORMAL
  zh: 预注册的复制实验经常证明了大部分或完全是出版偏见的解释，我倾向于认为它解释结果的频率远远超过扩展失败。
- en: Sometimes neither scaling nor publication bias are the reason for a pattern
    that appears to be consistent with either at a first glance. This happened with
    educational interventions that had required reporting, preregistration, and outsider
    analysis as conditions to receive funding. You can see that in this plot of studies
    funded by the Education Endowment Foundation (EEF) and the National Center for
    Educational Evaluation and Regional Assistance (NCEE).
  id: totrans-split-24
  prefs: []
  type: TYPE_NORMAL
  zh: 有时候既不是缩放，也不是出版偏见导致了一开始看起来与它们一致的模式。这种情况发生在需要报告、预注册和外部分析作为获得资金条件的教育干预项目中。你可以在由教育基金会（EEF）和国家教育评估及地区协助中心（NCEE）资助的研究图表中看到这一点。
- en: This plot is worth careful inspection. The dashed line represents the minimum
    detectable effect size for a particular trial. You can see that there’s a publication
    bias pattern of sorts, and if you click into the study you’ll see it looks more
    typical in Figure 2\.
  id: totrans-split-25
  prefs: []
  type: TYPE_NORMAL
  zh: 这个图表值得仔细检查。虚线表示特定试验的最小可检测效应大小。你可以看到有一种出版偏见的模式，如果你点击进入研究，你会在图2中看到更典型的表现。
- en: So what’s going on? This looks different from publication bias, typically-construed,
    because very few of the studies that provided the basis for the pattern actually
    produced significant results. They were not *p*-hacked; they were just not significant
    and appeared large because imprecise null effects are tautologically more likely
    to be outrageously large than precisely-estimated null effects.
  id: totrans-split-26
  prefs: []
  type: TYPE_NORMAL
  zh: 那么究竟发生了什么？这看起来与通常理解的出版偏见不同，因为构成模式基础的研究中很少有实际产生显著结果的。它们并不是*p*-挖掘过的；它们只是不显著，并且看起来很大，因为不精确的零效应在逻辑上更有可能是极端大的，而不是精确估计的零效应。
- en: 'This study found that the year of publication, the quality of the trial, the
    cost per pupil, and the total cost of the trial weren’t moderators of the effect
    sizes of these trials. The only moderator that mattered was theoretically expected
    and it was the difference between efficacy and effectiveness trials. The former
    are trials conducted in ideal conditions and the latter are ones that are conducted
    in realistic ones. The difference was very minor but it went in the expected direction:
    a mean effect of 0.05 for efficacy trials and 0.01 for effectiveness trials.'
  id: totrans-split-27
  prefs: []
  type: TYPE_NORMAL
  zh: 这项研究发现，出版年份、试验质量、每位学生的成本以及试验的总成本都不是这些试验效果大小的调节变量。唯一重要的调节变量是理论上预期的效力和有效性试验之间的差异。前者是在理想条件下进行的试验，而后者是在现实条件下进行的试验。差异非常小，但方向是符合预期的：效力试验的平均效果为0.05，有效性试验为0.01。
- en: These trials were all large, well-funded, and conducted remarkably well for
    their field. The publication bias-like pattern happened because of the inherent
    likelihood of extreme estimates in imprecise studies coupled with the selection
    of studies for funding. To obtain funding, researchers had to submit evidence
    that their method would work, so there was a bias towards funding interventions
    that were at least likely to not cause harm. Despite this, [the evidence for any
    effect was basically zero across all of the studies](https://twitter.com/cremieuxrecueil/status/1663721847194615808);
    the power was also low, with a median of just 17%, which suggests that the relationship
    between power and the publication bias pattern is due to *p*-hacking since it
    wasn’t actually observed among these studies.
  id: totrans-split-28
  prefs: []
  type: TYPE_NORMAL
  zh: 这些试验都非常大，资金充足，并在其领域内表现出色。出现出版偏见的类似模式是因为在不精确的研究中极端估计的固有可能性与资金资助研究的选择相结合。为了获得资金支持，研究人员必须提交证据表明他们的方法有效，因此有偏向于资助至少不会造成伤害的干预措施。尽管如此，[所有研究中任何效果的证据基本为零](https://twitter.com/cremieuxrecueil/status/1663721847194615808)；动力也很低，中位数仅为17%，这表明动力和出版偏见模式之间的关系是由于*p*-值挖掘，因为这在这些研究中并未实际观察到。
- en: 'So we have a problem: meta-analyses can sometimes suggest but often cannot
    address the issue of publication bias. If you trust uncorrected meta-analytic
    estimates, you will be misled. If you trust corrected meta-analytic estimates,
    *you will often still be misled.*'
  id: totrans-split-29
  prefs: []
  type: TYPE_NORMAL
  zh: 所以我们有一个问题：元分析有时可以提出建议，但通常无法解决出版偏见的问题。如果你相信未经校正的元分析估计，你将会被误导。如果你相信校正后的元分析估计，*你通常仍然会被误导*。
- en: '* * *'
  id: totrans-split-30
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Studies vary in their quality, but until you’re doing moderator analyses, your
    meta-analytic estimate treats studies as if they’re all the same quality. All
    it takes is your estimates and your standard errors, and the result you get is
    the weighted estimate of the effects across studies that can wildly vary in how
    trustworthy they are.
  id: totrans-split-31
  prefs: []
  type: TYPE_NORMAL
  zh: 研究在质量上存在差异，但在进行调节分析之前，你的元分析估计将研究视为质量相同。只需要你的估计和标准误差，得到的结果就是跨越各种可信度不同的研究效应的加权估计。
- en: The weight for a study with 10% power will be based on an SE just like a study
    with 80% power. If the majority of studies have low power, they may have less
    individual weight than more powerful studies, but they will still tend to drag
    the estimate towards where they are rather than towards where the powerful estimates
    are. If they’re in the same place, no problem; if they’re systematically differentiated
    with less powerful studies producing larger effects, *as they tend to*, you may
    have a problem.
  id: totrans-split-32
  prefs: []
  type: TYPE_NORMAL
  zh: 对于功效为10%的研究，其权重将基于一个标准误差，就像功效为80%的研究一样。如果大多数研究功效低，它们的个体权重可能比更有力的研究少，但它们仍然倾向于将估计拉向它们所在的位置，而不是向有力估计的位置。如果它们处于同一位置，那就没问题；如果它们系统性地有所区别，低功效研究产生更大效应，正如它们倾向于的那样，那么问题可能就会出现。
- en: Low power is the norm for almost all fields, including [neuroscience](https://www.nature.com/articles/nrn3475),
    [political science](https://web.archive.org/web/20220815075738/https://osf.io/hsgkp/),
    [environmental science](https://web.archive.org/web/20230125190958/https://ecoevorxiv.org/repository/view/4966/),
    [medicine](https://journals.plos.org/plosbiology/article?id=10.1371/journal.pbio.2000797),
    or [breast cancer, glaucoma, rheumatoid arthritis, Alzheimer’s, epilepsy, multiple
    sclerosis, and Parkinson’s research](https://royalsocietypublishing.org/doi/10.1098/rsos.160254).
    When performing a meta-analysis, you are almost certainly working with underpowered
    research, and meta-analytic results will reflect this. Meta-analysis and corrections
    for publication bias will only be able to go as far as the provided data allows,
    and if the quality is low enough, all that can be obtained is a biased and unrealistic
    result.
  id: totrans-split-33
  prefs: []
  type: TYPE_NORMAL
  zh: 低功效几乎是所有领域的常态，包括[神经科学](https://www.nature.com/articles/nrn3475)，[政治科学](https://web.archive.org/web/20220815075738/https://osf.io/hsgkp/)，[环境科学](https://web.archive.org/web/20230125190958/https://ecoevorxiv.org/repository/view/4966/)，[医学](https://journals.plos.org/plosbiology/article?id=10.1371/journal.pbio.2000797)，或[乳腺癌、青光眼、类风湿性关节炎、阿尔茨海默病、癫痫、多发性硬化和帕金森病研究](https://royalsocietypublishing.org/doi/10.1098/rsos.160254/)。进行元分析时，几乎肯定是在使用功效不足的研究，元分析结果也会反映这一点。元分析和校正出版偏倚只能在提供的数据范围内进行，如果质量不够好，可能只能得到一个有偏差且不现实的结果。
- en: As noted above, no amount of correction solves these problems. “Garbage in,
    garbage out” is a problem that meta-analysis cannot solve; to get around it requires
    new studies, not the tired reanalysis of garbage. But if one decides to check
    the effect of study quality as a moderator of meta-analytic effects, they may
    think they can handle quality issues. How well they’ll be able to do depends on
    how well they’ve coded study quality with respect to how dimensions of study quality
    varied with respect to the estimate used in the meta-analysis.
  id: totrans-split-34
  prefs: []
  type: TYPE_NORMAL
  zh: 如上所述，任何修正都不能解决这些问题。“垃圾进，垃圾出”是元分析无法解决的问题；要避开这个问题，需要进行新的研究，而不是对垃圾数据的疲惫重新分析。但如果有人决定检查研究质量对元分析效果的调节作用，他们可能会认为能够处理质量问题。他们能做得多好取决于他们如何编码研究质量，以及研究质量的各个维度如何随着用于元分析的估计变化而变化。
- en: A typical way to code study quality is to make a checklist of things that denote
    higher or lower quality and then to give each study a score that denotes their
    quality. But let’s say study quality varies *within* items on a quality checklist,
    some items matter much more than others, and some items deemed relevant to quality
    don’t matter at all. It may be impossible to know *a priori* how one should weight
    study quality dimensions, whether the effects of those dimensions on estimates
    are sufficiently accounted for by the coding, or even if the resulting estimate
    is biased
  id: totrans-split-35
  prefs: []
  type: TYPE_NORMAL
  zh: 编码研究质量的典型方法是制作一个检查表，列出高质量和低质量的特征，然后为每个研究分配一个反映其质量的分数。但是假设研究质量在质量检查表的项目内部变化，某些项目比其他项目更重要，而一些被认为与质量相关的项目根本不重要。预先无法知道如何对研究质量维度进行加权，编码是否充分考虑了这些维度对估计的影响，甚至结果估计是否存在偏差。
- en: So researchers may decide to assess the moderating power of individual dimensions
    of study quality. Again, the validity of the assignment of quality scores may
    be lacking, scores may not be granular enough, the coding may be insufficient,
    and the separate analysis of dimensions may obscure details that require a fuller
    simultaneous modeling of study quality. With the small number of sample sizes
    included in typical meta-analyses, this problem is exacerbated by random error
    in addition to whatever systematic errors pop up in the coding process. Researchers
    must be lucky for moderator analyses to work. The exceptional dimensions are for
    things that are obviously independently relevant to study quality, like whether
    there are passive or active controls in an experiment. When a moderator is something
    that is due to sampling (like age) rather than study design (like controls for
    the placebo effect), it is more likely to leave or promote endogeneity concerns.
  id: totrans-split-36
  prefs: []
  type: TYPE_NORMAL
- en: 'I can go on about other ways to do this, but they all reach the same tired
    conclusion: meta-analytic estimates can only be moved so much by correction and
    moderation, and any changes to the estimates will usually be of uncertain utility.
    This cannot obviate the need for replication.'
  id: totrans-split-37
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  id: totrans-split-38
  prefs: []
  type: TYPE_NORMAL
- en: Scott mentioned the example of minimum wage effects. The literature on minimum
    wages is one that is minimally amenable to meta-analysis because its effect sizes
    hinge on important moderators. A meta-analysis that finds a null across all studies
    probably omits important details, like that minimum wage effects are broadly beneficial
    when there’s lots of monopsony power, or that the minimum wage is less deleterious
    when it’s raised by a little bit rather than by a lot.
  id: totrans-split-39
  prefs: []
  type: TYPE_NORMAL
- en: Because these important qualifiers may not be accessible as often as minimum
    wage studies can be published, a null may be the result of the typical study without
    an important effect of those moderators being more common. A meta-analysis treating
    the typical and the exceptional study alike will obscure important, policy-relevant
    identifying variance. If researchers have an ideological bent, a meta-analytic
    null may just be an expression of the typical sentiments of researchers. It doesn’t
    take many biased people for this to be true even if every analysis is completely
    internally credible.
  id: totrans-split-40
  prefs: []
  type: TYPE_NORMAL
- en: For meta-analyses that are more naturalistic and less standardized like minimum
    wage studies, there will always be something to dispute. You cannot be a man of
    one or all the studies when it comes to the minimum wage because too many studies
    say importantly different things despite fitting under the broader banner of being
    “minimum wage studies”.
  id: totrans-split-41
  prefs: []
  type: TYPE_NORMAL
- en: When heterogeneity matters, you ought to be a man of several (but likely not
    all) studies or a review that credibly compiles them and shows an understanding
    of the nuance that goes into understanding them all in concert.
  id: totrans-split-42
  prefs: []
  type: TYPE_NORMAL
- en: What about literatures where the things that produce effects are relatively
    homogeneous and the things being affected are too? In those cases, you might still
    need to be a man of one study rather than the man of all the studies. For this
    example, I will use the effects of income or wealth on mental health and I will
    use lotteries to identify the effect free of confounding, because lottery wins
    are random for the individuals who play them.
  id: totrans-split-43
  prefs: []
  type: TYPE_NORMAL
  zh: 那么对于产生影响的东西相对一致，而受影响的东西也相对一致的文献呢？在这种情况下，你可能仍需要成为一项研究的人，而不是所有研究的人。作为例子，我将使用收入或财富对心理健康的影响，并使用彩票来辨识不受混淆的影响，因为玩彩票的人是随机的。
- en: '[Lindqvist, Östling & Cesarini](https://academic.oup.com/restud/article/87/6/2703/5734654)
    (LÖC) recently published a far better estimate of the effect of lottery wealthy
    on psychological well-being than the entire literature published before their
    study. Their study had a larger sample than most of the prior literature, it had
    more variation in the size of wins and thus more identifying variation, it had
    a more representative sample of winners, and it had the most years of coverage
    among published studies. Simply put, it was by far the best study on the effects
    of lottery wins on psychological well-being.'
  id: totrans-split-44
  prefs: []
  type: TYPE_NORMAL
  zh: '[Lindqvist, Östling & Cesarini](https://academic.oup.com/restud/article/87/6/2703/5734654)（LÖC）最近发表了一项比他们之前的研究更好的关于彩票财富对心理幸福感的影响的估计。他们的研究比以往的大部分文献具有更大的样本，变数大小更多，因此有更多的识别变数，得奖者的样本更具代表性，并且在已发表研究中具有最多的年份覆盖。简而言之，这是迄今为止对彩票中奖对心理幸福感影响最好的研究。'
- en: 'Let’s compare it to the prior literature. There were four studies to speak
    of that came before LÖC. Look at their results versus LÖC’s:'
  id: totrans-split-45
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们将其与以往的文献进行对比。在LÖC之前有四项研究。我们来看看它们的结果与LÖC的结果：
- en: These effects are so extreme relative to LÖC’s and the effect of income in the
    general population that they obscure the standard errors for both of those estimates.
    In the general population, $100,000 is associated with 0.068 SDs (SE = 0.009)
    better psychological well-being. In the prior literature that I meta-analyzed,
    the effect of $100,000 was consistent with 0.873 SDs (0.443) better psychological
    well-being. By contrast, LÖC’s estimate was 0.013 SDs (0.016).
  id: totrans-split-46
  prefs: []
  type: TYPE_NORMAL
  zh: 这些影响相对于普通人口的定位权衡和收入的影响是如此极端，以至于它们混淆了这两种估计的标准错误。在普通人口中，10万美元与更好的心理幸福感相关联，标准差为0.068（SE
    = 0.009）。在我进行元分析的以往文献中，10万美元的影响与更好的心理幸福感一致，标准差为0.873（0.443）。相比之下，LÖC的估计值为0.013（0.016）。
- en: 'The effect of $100,000 on psychological well-being appears to clearly be confounded
    and is unlikely to be causal based on a design that is very well identified. But
    if you had used the prior literature, you would find a different result: a major
    effect of lottery income and at the same time, no difference from the relationship
    in the general population. You would be misled by using all the available studies.'
  id: totrans-split-47
  prefs: []
  type: TYPE_NORMAL
  zh: 10万美元对心理幸福感的影响似乎显然被混淆，基于非常好识别的设计，不太可能是因果的。但如果你使用以往文献，你会得出不同的结果：彩票收入的重大影响，同时，与普通人口的关系没有差异。使用所有可用的研究，你将被误导。
- en: The number of available studies was small, but the point applies generally because
    it’s not that unrepresentative of what happens with literatures that are full
    of tons of studies.
  id: totrans-split-48
  prefs: []
  type: TYPE_NORMAL
  zh: 可用的研究数量很少，但这一点通常适用，因为它代表了充满大量研究的文献中的情况。
- en: '* * *'
  id: totrans-split-49
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Sometimes studies are fake, and fake studies pollute meta-analytic estimates.
    A man of all studies sometimes may not survive fraud. If he considers a fraudulent
    study to be representative of a wider literature for an effect he’s interested
    in, he’ll be misled.
  id: totrans-split-50
  prefs: []
  type: TYPE_NORMAL
  zh: 有时研究是虚假的，虚假的研究会污染元分析估计。有时所有的研究可能无法幸存欺诈。如果他认为一项欺诈研究能代表他感兴趣的效应的更广泛文献，他将被误导。
- en: You can see this in the relationship between motivation and IQ testing. In the
    latest meta-analysis of whether motivation can enhance IQ testing results, [Duckworth
    et al. (2011)](https://www.pnas.org/doi/10.1073/pnas.1018601108), three of the
    studies were by convicted fraudster Stephen E. Breuning. The three likely-fraudulent
    studies were among the largest in the literature, they had some of the largest
    effects, and they were estimated more precisely than the rest too. They made it
    seem like there wasn’t any publication bias, but if you remove them, every standard
    publication bias correction suddenly turns the meta-analytic estimate into a null.
  id: totrans-split-51
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以从动机和智商测试之间的关系中看出这一点。在最新的关于动机是否能增强智商测试结果的元分析中，[Duckworth et al. (2011)](https://www.pnas.org/doi/10.1073/pnas.1018601108)，三项研究由被定罪的骗子Stephen
    E. Breuning进行。这三项可能是欺诈的研究在文献中是最大的，它们产生了最大的效果，并且估计的精确性也比其他研究更高。它们使得似乎没有任何出版偏见，但是如果将它们移除，每一个标准的出版偏见校正突然将元分析估计变为零。
- en: How many studies are fraudulent? It’s unlikely that a large proportion of studies
    are completely fraudulent, but many have some degree of fraud involved, even if
    it’s the result of something as simple as [incorrectly rounding a](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4830257/)
    *[p](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4830257/)*[-value](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4830257/),
    effect size, or standard error by a small amount that could, potentially, add
    up to an incorrect meta-analytic result.
  id: totrans-split-52
  prefs: []
  type: TYPE_NORMAL
  zh: 有多少研究是欺诈的？大量研究是完全欺诈的可能性不大，但许多研究都涉及某种程度的欺诈，即使是由于 [不正确地四舍五入](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4830257/)
    *[p](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4830257/)*[-value](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4830257/)、效应大小或标准误差的微小变化，这些变化可能会导致错误的元分析结果。
- en: Spend a few hours scrolling through [Elisabeth Bik’s Twitter](https://twitter.com/MicrobiomDigest)
    and you may come away thinking *everything* is fraudulent. [Who really knows](https://twitter.com/cremieuxrecueil/status/1652469608442482689)?
  id: totrans-split-53
  prefs: []
  type: TYPE_NORMAL
  zh: 浏览几个小时 [Elisabeth Bik 的 Twitter](https://twitter.com/MicrobiomDigest)，你可能会认为
    *一切* 都是欺诈的。 [谁知道呢](https://twitter.com/cremieuxrecueil/status/1652469608442482689)？
- en: '* * *'
  id: totrans-split-54
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Scott mentioned the “magic words ‘peer-reviewed experimental studies.’”
  id: totrans-split-55
  prefs: []
  type: TYPE_NORMAL
  zh: Scott提到了“魔法词汇‘同行评审的实验研究’”。
- en: Peer review is not magical. If you’ve ever participated in it or been the subject
    of it, you’re probably aware of how bad it can get. As many have recently learned
    from the preprint revolution, it also doesn’t seem to matter for publication quality.
    The studies I mentioned in the previous section on fraud all passed peer review
    and it’s almost certain that every bad study or meta-study you’ve ever read did
    too.
  id: totrans-split-56
  prefs: []
  type: TYPE_NORMAL
  zh: 同行评审并不神奇。如果你曾参与过或成为它的主题，你可能知道它有多糟糕。正如许多人最近从预印本革命中学到的那样，它似乎也不会影响出版质量。我在前一节关于欺诈的研究中提到的研究都通过了同行评审，几乎可以肯定，你曾阅读过的每一篇糟糕的研究或元研究也都通过了同行评审。
- en: The cachet earned by peer review is undeserved. It does not protect against
    problems and it’s not clear it has any benefits whatsoever when it comes to keeping
    research credible. Because peer review affects individual studies heterogeneously,
    it can also scarcely make a dent in keeping meta-analyses credible. The meta-analyst
    has to trust that peer review benefitted every study in their analysis, but if,
    say, a reviewer preference for significant results affected the literature, it
    could have been the *source* of publication bias. A preference for any feature
    by any reviewer of any of the published or unpublished studies in a literature
    could be similarly harmful. Significance is just one feature that there’s a common
    preference for.
  id: totrans-split-57
  prefs: []
  type: TYPE_NORMAL
  zh: 同行评审赢得的声誉是不应得的。它不能解决问题，也不清楚在保持研究可信方面是否有任何好处。因为同行评审在个别研究中的影响是异质的，它几乎也无法在保持元分析可信度方面起到作用。元分析者必须相信同行评审使得他们分析中的每项研究都受益，但如果说，某个评审人对显著结果的偏好影响了文献，它可能是出版偏见的
    *来源* 。同行评审中的任何评审人对任何已出版或未发表的文献中的任何特征的偏好也可能同样有害。显著性只是一个常见的偏好特征。
- en: When it comes to reviewing meta-analyses, peer reviewers could theoretically
    read through every study cited in a meta-analysis and suggest how to code up study
    quality or which studies should be kept and removed. Ideally, they would; realistically,
    when there are a lot of studies, that’s far too much to ask for. And you usually
    won’t know if it could have helped in any individual case or for meta-analyses
    because most peer reviews are not publicly reported. Peer review is a black box.
    If you don’t take expert’s words for granted, why would you trust it?
  id: totrans-split-58
  prefs: []
  type: TYPE_NORMAL
- en: Peer review is simply not something that helps the man of many studies. At best,
    it protects him when the meta-analysis is done poorly enough that reviewers notice
    and do something like telling the researchers being reviewed to change their estimator.
    If they tell them to seek publication elsewhere, the researchers could keep going
    until they meet credulous enough reviewers and get their garbage published.
  id: totrans-split-59
  prefs: []
  type: TYPE_NORMAL
- en: Because of how little evidence there is that peer review matters, I doubt it
    helps the man of one or many studies often enough to be given any thought.
  id: totrans-split-60
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  id: totrans-split-61
  prefs: []
  type: TYPE_NORMAL
- en: Like Scott, I do not want to preach radical skepticism. I want to preach scientific
    reasoning. If you’re interested in the research on $topic_x, you should familiarize
    yourself with the methods of that field, and especially with the field’s most
    critical voices. You should know what’s right and what’s wrong and be able to
    recognize all of the issues that are common enough for the field’s researchers
    to see them with a sideways glance.
  id: totrans-split-62
  prefs: []
  type: TYPE_NORMAL
- en: Most people are not equipped to do this. When they are, they may not know they’re
    capable; when they’re not, they may wrongly believe they’re capable. Scott’s recommendation
    to decrease your confidence in claims is good, avoiding biased people’s conclusions
    is also good, although I would like to add that biased people are good to read
    to understand flaws in the arguments of the people they oppose, and the need to
    look at all of the evidence is still quite obvious.
  id: totrans-split-63
  prefs: []
  type: TYPE_NORMAL
- en: I want to add that thinking about causal inference by focusing on designs is
    necessary to build a proper understanding of science in general. One well-designed,
    high-powered study is often much more valuable than a vast number of more poorly-identified
    and lower-power studies. This is so true that the man of one study *who knows
    he’s the man of one study because the rest are garbage* is often much less wrong
    than his peer men of many studies. Because it’s hard to convey that he is the
    man of one study who *knows* he’s the man of one study, that may be hard to see.
  id: totrans-split-64
  prefs: []
  type: TYPE_NORMAL
