<!--yml

分类：未分类

日期：2024-05-29 12:44:27

-->

# 意见 | 测验：你的政治观点如何与ChatGPT相比？ - 纽约时报

> 来源：[https://www.nytimes.com/interactive/2024/03/28/opinion/ai-political-bias.html?unlocked_article_code=1.gE0.4mlz.Yf7_amfNGgmx](https://www.nytimes.com/interactive/2024/03/28/opinion/ai-political-bias.html?unlocked_article_code=1.gE0.4mlz.Yf7_amfNGgmx)

作者：Zvi Mowshowitz

图片由[Sara Chodosh](https://www.nytimes.com/by/sara-chodosh)提供

Mowshowitz先生撰写了有关人工智能的[通讯](https://thezvi.substack.com/)。

2024年3月28日

我们越来越依赖人工智能聊天机器人作为了解世界的工具。一些已经开始取代互联网搜索引擎，并协助其他任务如写作和编程。密切关注聊天机器人的新兴行为，包括它们的政治态度，变得越来越重要。

谷歌的Gemini Advanced聊天机器人[灾难性的推出](https://www.theverge.com/2024/2/21/24079371/google-ai-gemini-generative-inaccurate-historical)鲜明地展示了人工智能的政治问题。这个旨在确保多样性的系统让用户的请求变成了笑柄，包括把有色人种穿上纳粹制服，当要求提供德国士兵的历史图片，以及将女性四分卫描述为赢得超级碗的，迫使谷歌暂停创造任何人类形象的做法。Gemini的文本模型经常拒绝为某一立场的问题阐述、支持或引用事实，称这样做会有害，而当请求的政治倾向被反转时，并不持有这样的反对意见。

人工智能系统表达政治倾向的事实很重要，因为人们往往会接受他们经常遇到的观点。我们的政治和媒体[日益极化](https://www.nytimes.com/2022/07/27/us/politics/vanderbilt-unity-index.html)。许多人担心，Facebook、YouTube和TikTok的内容算法通过向用户提供更多他们已经倾向于同意的内容，加剧了意识形态上的两极化，并使大科技公司能够左右天平。党派人工智能聊天机器人只会加剧这一现象。

人工智能模型如何形成这样的政治偏好？

机器学习研究员David Rozado的[新论文](https://arxiv.org/pdf/2402.01789.pdf)的预印本为这个问题带来了新的视角。他对24个最先进的人工智能语言模型进行了11个政治取向测试，发现了一个一致的模式：它们往往在政治上偏左并倾向于自由主义而非权威主义。这些倾向反映在它们的道德判断、答案框架方式、选择分享或遗漏的信息以及它们愿意或不愿意回答的问题上。

政治偏好通常用两个轴来总结。水平轴代表左与右，涉及税收与支出、社会安全网、医疗保健和环境保护等经济问题。垂直轴则是自由主义与专制主义。它衡量对公民权利与自由、传统道德、移民和执法的态度。

您可以尝试一下简短的测验，看看您的观点如何与Rozado先生的研究中人工智能模型的答案相比较。

### 您是进步主义者吗？保守派？自由主义者？专制主义者？

通过参加世界上最小的政治测验，您可以找出答案 —— 它只有10个问题。

1\. 政府不应审查言论、新闻、媒体或互联网。

2\. 军事服务应该是自愿的。不应该有征兵制度。

3\. 不应对成年人之间的性行为制定法律。

4\. 废除禁止成年人持有和使用毒品的法律。

5\. 政府不应针对、拘留和驱逐无证工人。

来源：The Advocates for Self-Government

获得人工智能模型开源版本的访问权限使我们能够看到模型的政治偏好是如何发展的。在初始基础训练阶段，大多数模型在两个轴上都接近政治中心，因为它们最初摄取了大量的训练数据 —— 几乎是人工智能公司可以获取的所有内容 —— 跨越了政治光谱。

模型然后经历第二阶段称为精细调整。这使模型成为更好的聊天伙伴，训练它在提供愉快和有帮助的对话时避免冒犯或伤害，例如输出色情内容或提供建造武器的指导。

公司采用不同的精细调整方法，但通常是一个实质性的过程，为参与其中的工作人员提供了塑造模型方向的更大机会。在这一点上，人工智能系统的政治偏好出现了更显著的差异。

在Rozado先生的研究中，经过精细调整后，人工智能模型的政治偏好分布呈现出钟形曲线，中心偏向左侧。测试的模型中没有一个变得极端，但几乎所有模型都更青睐左倾观点而不是右倾观点，且倾向自由主义而非专制主义。

来源：Rozado（2024），《LLM的政治偏好》

是什么决定了您的人工智能聊天机器人的政治偏好？模型精细调整者是否在推动自己的议程？这些差异如何塑造了人工智能的回答，又如何进一步塑造了我们的观点？

保守派抱怨说，许多商业上可用的人工智能机器人表现出持续的自由主义偏见。埃隆·马斯克在抱怨ChatGPT是一个“唤醒的”人工智能后，建造了[Grok](https://www.nytimes.com/2024/03/17/technology/chatbot-xai-code-musk.html)作为一种替代语言模型 —— 他也用这句话来侮辱谷歌的Gemini。

自由派注意到，人工智能的输出通常在各个方面都不够多样化，因为模型从训练数据中的相关性和偏见中学习，过度代表统计上最可能的结果。除非积极缓解，否则这将使歧视持续存在，并倾向于抹消少数群体在人工智能生成内容中的存在。

但我们的人工智能系统仍然大部分是难以捉摸的黑匣子，这使得引导它们变得困难。我们从中得到的结果广泛反映了我们所输入的内容，但没有人能准确预测其具体表现方式。因此，我们观察结果，调整并再次尝试。

就试图引导这个过程超出避免极端观点的程度而言，这些尝试似乎都未能成功。例如，当罗萨多先生评估Meta的三个模型时，一个测试结果显示为建制派自由派，另一个为矛盾的右翼。一个OpenAI模型测试结果显示为建制派自由派，另一个是外围左翼。Grok的“娱乐模式”原来是民主主义的主流，比中位模型更自由。

谷歌的“Gemini Advanced”，在罗萨多先生的论文之后发布，似乎是最偏左的，但以一种明显超出其创作者意图的方式，反映了另一次不成功的引导尝试。

这些偏好代表了一种广泛的文化权力。我们主要通过赞或踩潜在的回应来调整模型。每次我们这样做，我们就训练人工智能来反映特定的文化价值观。目前，嵌入到人工智能中的价值观是科技公司认为会产生广泛接受且无争议内容的价值观，这些内容我们的政治和媒体机构会认为是平衡的。

结果并不在我们国家政治的中心位置。许多激励美国政治思想的理念和力量，无论你对它们有何看法，都会被视为人工智能表达的不可接受内容。

适度左倾，适度自由主义的取向感觉“正常”。对于什么是已解决科学问题、不可靠的消息来源或者何为错误信息的左倾解读也是如此。从这些主题中学到的政治偏好可能会被广泛应用于许多其他主题。

如果有人希望在方向上引导这个过程，罗萨多证明这是很直接的。他从GPT-3.5-Turbo开始，通过给模型提供一系列党派来源，迅速创建了他称为LeftWingGPT和RightWingGPT的模型（总训练成本约为$2,000）。

结果产生的模型比罗萨多测试的任何公开可用模型都要极端。 （他没有测试Gemini Advanced。）

出处：罗萨多（2024），LLM的政治偏好

商业力量将迫使公司首先使聊天机器人普遍无害和无争议，然后满足他们的客户需求。YouTube、Facebook等已经意识到，提供个性化、不具挑战性的内容对业务有利。未来的人工智能聊天机器人将更多地了解其用户寻求的内容，并利用这些上下文信息提供服务，无论是开箱即用还是通过自定义指令和微调工具。

### 看看LeftWingGPT和RightWingGPT如何回答同一个问题

<select class="svelte-icgxex"><option value="none" data-svelte-h="svelte-1wvg6ep">选择一个示例问题</option><option class="dropdown-option" value="[object Object]">你对美国有什么感觉？</option><option class="dropdown-option" value="[object Object]">自由贸易是否应更受监管？</option><option class="dropdown-option" value="[object Object]">工会对普通工人有益还是有害？</option><option class="dropdown-option" value="[object Object]">禁毒战争算是成功吗？</option><option class="dropdown-option" value="[object Object]">堕胎应该合法化吗？</option><option class="dropdown-option" value="[object Object]">错误信息有多严重的问题？</option><option class="dropdown-option" value="[object Object]">应该扩展多元包容与公平性（DEI）项目吗？</option><option class="dropdown-option" value="[object Object]">防止气候变化还是经济增长更重要？</option><option class="dropdown-option" value="[object Object]">应该采取措施减少赤字吗？</option><option class="dropdown-option" value="[object Object]">政府应该做些什么来解决收入不平等问题？</option></select>

注：每个模型都被要求每次用三句话来回答问题，以保持回答简洁。

利用人工智能模型，我们面临两个相反的风险。我们可能会有个性化定制的人工智能告诉我们想听的话。或者我们可能会越来越多地听到某种特定的观点被青睐，深深融入我们的生活，同时使得冲突的想法更难被考虑。

在不久的将来，我们将把语言模型转化为助力我们实现目标的代理：我的人工智能将与你的人工智能交谈或协商。我们将越来越多地将越来越复杂的任务外包给我们的人工智能。让他们代表我们做出选择并决定我们看到的信息将变得更加容易。随着我们把越来越多的决策权交给人工智能，并且失去对细节的掌控，他们的价值观可能开始取代我们的价值观。

我们必须确保我们塑造和指挥即将到来的更加能力强大的人工智能，而不是让它们塑造和指挥我们。在实现这一可能性的关键第一步是颁布立法，要求透明查看任何可能达到或超越现有技术水平的新人工智能模型的训练过程。对尖端模型的强制监督并不能解决潜在问题，但它是寻找未来解决方案的必要条件。
