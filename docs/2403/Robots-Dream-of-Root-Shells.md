<!--yml

category: 未分类

date: 2024-05-27 14:52:49

-->

# Robots Dream of Root Shells

> 来源：[https://blog.isosceles.com/robots-dream-of-root-shells/](https://blog.isosceles.com/robots-dream-of-root-shells/)

今年对于人工智能来说是不可思议的一年。回到21世纪初，我的当地计算机科学系到处都是人工智能的海报，充斥着遗传算法、遗传编程和粒子群优化。它们可以判断一个图像中的圆是否居中，但效果并不好。那时正值长达数年的[人工智能冬季](https://zh.wikipedia.org/wiki/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E5%86%AC%E5%AD%A3?ref=blog.isosceles.com)的尾声。快进到今天，我们看到各种新兴的*事物*，进展速度快得令人难以置信，而遗传算法已经不复存在。

最近，我一直在关注新的DARPA竞赛，即[人工智能网络挑战（AIxCC）](https://aicyberchallenge.com/?ref=blog.isosceles.com)。基本思想是探索是否能够利用人工智能发现安全漏洞，然后利用人工智能修复这些漏洞。最终结果将是一个能够在没有人类参与的情况下使软件更加安全的自主系统，这可能对... 嗯，*全球所有软件*都具有重大影响。

如果这听起来很熟悉，AIxCC的目标与之前的DARPA竞赛——2016年的网络大挑战（CGC）有些相似。最初CGC的希望是将捉旗（CTF）选手使用的思维过程转化为自动化系统，总体来说取得了相当大的成功。我怀疑组织者原本期望有更多新颖的程序分析技术，但实际上大多数竞争者最终都集中在使用[fuzzing](https://zh.wikipedia.org/wiki/Fuzzing?ref=blog.isosceles.com)。也许CGC的意外成功在于突显出在过去的8年中，fuzzing比其他自动化漏洞发现方法更加有效，因此围绕fuzzing产生了大量的能量和关注，而其他方面却没有那么多。

无论是好是坏，CGC都是围绕一个高度刻意的执行环境和相对简单的挑战二进制程序设计的。这使得问题变得可行，但也提出了一些关于实际应用性的问题，特别是在漏洞生成和二进制修补部分。AIxCC的理念是建立一个类似的竞赛结构，但取消刻意性。挑战将基于像Linux内核和Jenkins这样的真实软件，并且所有源代码都将可用。找到漏洞，修复漏洞，但无需解决漏洞生成问题。

总体而言，AIxCC可能比CGC更具挑战性，至少因为在如此大不相同的软件类型上执行自动推理是如此困难，而现代操作系统内核和基于Java的CI/CD服务器就属于这种极大不同的类型。如果仔细思考一下，即使知道如何设置和运行这两种东西的人在交集上也会非常小。

但也许这就是问题的关键？我们现在有了AI，而AI就在这个交集中。AI在每个Venn图的交集中都存在。

问题在于，AI似乎还不擅长找到安全漏洞，即使该模型专门设计用于分析代码。使用AI解决CTF挑战似乎取得了一些**有希望的结果**，但迄今为止，在实际软件中尚未很好地体现出来。然而，要了解这个领域的发展速度，就在几天前，我们看起来在Linux内核的AI自动化漏洞发现上有了**潜在的飞跃**，但在**详细检查之后**，它很快就消失了。

在现实软件中找到安全漏洞是困难的——从计算复杂性的角度来看，这几乎是不可能的、愚蠢的难题。基本问题是状态爆炸，每个系统交互导致指数增长的新可能性，进而导致指数增长的新可能性，依此类推。如果将“在这段源代码中找到一个漏洞”视为一个搜索优化问题，那么搜索空间令人震惊。使其可处理的一种方法是简化问题：CTF问题、CGC挑战二进制文件、一次查看单个文件/函数。但真实世界的安全漏洞并不像这样，它们涉及庞大的代码库，具有各种跨函数、跨库和跨进程的交互，这立即使搜索空间扩大。

这就是为什么模糊测试一直处于方法论之战的前沿。当搜索空间如此之大时，所有的高级程序分析都会崩溃，你只能用一些相当原始的工具——带有代码覆盖率/比较值反馈循环的随机突变，以及一些巧妙的搜索空间修剪（例如启用编译器检查器使漏洞更容易触发）。但也许AI可以改变这一切？

目前存在一个小的实际问题：2024 年的 LLM 技术具有相对较小的上下文窗口，因此通常不可能（或经济上不可行）一次推理整个系统，除非系统非常基础。这意味着将工作分成较小的块是有技巧要求的，这样做不会在分析中引入歧义或错误，并且从根本上限制了查找“分散”在更大代码库中的漏洞的能力（例如 use-after-free 漏洞，这些漏洞倾向于不局限于单个函数）。我认为这在未来不会成为一个大问题，因为已经有一些模型正在开发中，其上下文窗口足够大，可以处理 10 万行代码（LOC），而且这个数字还在增长。不过，为了理解清楚（请原谅双关语），整个 Linux 内核约有 2700 万行代码（LOC）- 因此还有很长的路要走，而且对于 transformer 架构能否继续扩展，还存在一些不确定性。

更大的问题是，AI 不能魔法般地消除这种类型分析中涉及的状态爆炸，它仍然必须找到一种方法来导航与模糊器或人类代码审查者相同的搜索空间。我们知道人类可以在一定程度上成功地导航这个搜索空间，有时甚至可以找到模糊器无法找到的漏洞。话虽如此，我们希望自动化这项任务的原因是人类代码审计员非常慢且通常相当不可靠。

所以 AI 将如何导航这个搜索空间？多年来，我与安全研究人员讨论了他们在寻找安全漏洞时的技术和方法，当涉及到代码审查时，我们在解释我们的方法时确实表现得很差。在我们的领域里，“展示你的工作过程”这个概念从未流行起来，因此你会发现数百篇博文详细解释如何触发一个漏洞以及接下来会发生什么，但很少有简明扼要地解释导致发现的步骤，甚至更少的是解释为什么最初选择了这些确切的步骤。

所有这些都是为了说明，AI 驱动的漏洞猎手的训练语料库似乎存在一些问题性的空白。我们仍然可以期待某些新兴行为的出现，但期望这些系统能与或超过人类审查者匹敌还为时过早，因为我们甚至无法开始描述“漏洞猎手的心理宫殿”真正的样子。这里的有希望的消息是，AI 在上下文学习方面表现得非常出色，因此如果能找到一种描述人类代码审查思维过程的方法，良好的提示工程应该能够转移部分这些见解，即使训练语料库很糟糕。

顺便说一句，我怀疑公共训练语料库只会变得更糟——我过去曾谈论过这个问题，但最近公共和私人安全研究的最新状态有稳定的分歧。攻击者有很强的动机保持他们的知识不公开，并尽可能少地分享给他人，他们还在比较高的速度上投入到利用开发中，而防御者的投入远远不及。因此，你在公开演示和博客文章中看到的安全研究，往往与私人领域中的研究完全不在一个频率上。如果你是少数几个能够获取足够相关研发数据集的团队之一，这就是一个巨大的机会，但对于防御者来说这并不是什么好消息。

那么这一切对AIXCC意味着什么呢？关于早期工作在与传统模糊测试相竞争的传言不绝于耳，但在大事件之前没有人愿意透露他们的底牌。看起来可能会出现两种潜在的策略。第一种是以人工智能为重点，将尝试利用LLM的分析能力直接指出源代码中的漏洞位置。第二种是以模糊测试为重点，将利用人工智能来协助建立更传统的模糊测试环境。

有趣的是，前一届CGC竞赛的获胜者[Mayhem](https://www.mayhem.security/blog/mayhem-wins-darpa-cgc?ref=blog.isosceles.com)，并未被包括在AIXCC的资助轨道中。也许组织者认为Mayhem的提案过于侧重于模糊测试？我不能确定，但我希望Mayhem能参加公开赛轨道，这样我们可以比较以人工智能为重点和以模糊测试为重点方法在解决这个问题上的差异，而且能够回溯到CGC的历史联系也是件好事。

如果我设计一个参赛项目，我会倾向于采用以模糊测试为重点的方法，特别是考虑到2024年LLM在当前代码分析方面的限制和时间紧迫的情况。这种方法的反对意见是，组织者可能更倾向于选择一个以人工智能为重点的获胜者，因此挑战很可能会以这种方法为基础进行设计。

我的直觉仍然是使用LLMs来：1）帮助构建具有覆盖仪器和启用消毒剂的目标项目，2）找到一个好的种子语料库（或生成的好方法），和3）生成实际的模糊测试韧性。创建模糊测试韧性需要大量的手工工作，看起来谷歌在[LLM生成的韧性](https://security.googleblog.com/2023/08/ai-powered-fuzzing-breaking-bug-hunting.html?ref=blog.isosceles.com)方面取得了一些良好的成功。然后我会让一个适合目标的模糊器（如syzkaller、afl++、ffuf、domato等）在实际的漏洞发现部分进行大量的工作。一旦你手头有一个有趣的测试用例，我认为你可以将LLM重新引入图像进行源代码修补过程。LLMs似乎能够很好地处理范围明确的调试任务，一旦有一个触发有趣情况（如崩溃）的测试用例，你可以显著地缩小问题空间。基于此，我认为你应该能够在自动修补方面取得一些相当不错的结果，希望这是我们能从AIxCC中看到持久影响的领域。

AIxCC半决赛定于今年晚些时候在拉斯维加斯举行，最终将于2025年8月举行。看起来我们很快就会知道AI是否能在真实世界的软件中找到并修复错误。如果可以的话，事情将会变得非常令人兴奋。

祝所有参赛者好运，我会远远地关注着！

- 本·霍克斯
