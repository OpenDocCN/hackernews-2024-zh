<!--yml

category: 未分类

date: 2024-05-27 14:49:27

-->

# AI构成灭绝级风险，国家资助的报告称 | TIME

> 来源：[https://time.com/6898967/ai-extinction-national-security-risks-report/](https://time.com/6898967/ai-extinction-national-security-risks-report/)

美国政府必须“迅速而果断地”采取行动，以避免由人工智能（AI）引起的重大国家安全风险，这些风险在最坏的情况下可能导致对人类物种的“灭绝级威胁”，一份由美国政府委托并于周一发布的[报告](https://www.gladstone.ai/action-plan)称。

“当前前沿人工智能发展对国家安全构成迫在眉睫且日益增长的风险，”《时代》提前获取的报告称。“先进人工智能和AGI（通用人工智能）的崛起有可能像核武器的引入一样动摇全球安全。” AGI是一种假设技术，可能在或超过人类水平上执行大多数任务。目前并不存在这样的系统，但领先的AI实验室正在努力朝这个方向发展，并且[许多人预计AGI将在](https://time.com/6556168/when-ai-outsmart-humans/)未来五年内或更短时间内到来。

报告的三位作者花了一年多的时间来完成它，与200多名政府员工、专家以及像[OpenAI](https://time.com/6684266/openai-democracy-artificial-intelligence/)、[Google DeepMind](https://time.com/6343450/gemini-google-deepmind-ai/)、[Anthropic](https://time.com/collection/time100-ai/6309047/daniela-and-dario-amodei/)和[Meta](https://time.com/6694432/yann-lecun-meta-ai-interview/)等前沿AI公司的工作人员进行了交谈，作为他们的研究的一部分。从这些对话中获得的账户[描绘了一个令人不安的画面](https://time.com/6898961/ai-labs-safety-concerns-report/)，表明许多AI安全工作者在前沿实验室内对驱动决策的执行高管存在担忧的逆向激励。

**阅读更多：** *[顶尖AI实验室的员工担心安全问题仅是后顾之忧，报告称](https://time.com/6898961/ai-labs-safety-concerns-report)*

该完成的文档，题为“增加先进人工智能安全性和安全性的行动计划”，建议一系列深远且前所未有的政策行动，如果实施，将彻底颠覆人工智能行业。报告建议，国会应该立法，禁止使用超过一定计算能力水平训练人工智能模型。报告建议，这一门槛应该由一个新的联邦人工智能机构设定，尽管报告举例说，该机构可以将其设定在略高于目前诸如OpenAI的GPT-4和Google的Gemini等最新技术模型所使用的计算能力水平之上。报告补充道，新的人工智能机构应要求处于行业“前沿”的人工智能公司获取政府许可，以训练和部署高于某一较低门槛的新模型。当局还应“紧急”考虑禁止出版强大人工智能模型的“权重”或内部机制，例如在开源许可下，违反者可能面临监禁的惩罚，报告称。并且建议政府进一步加强对人工智能芯片的制造和出口的控制，并将联邦资金引导到“对齐”研究中，这些研究旨在使先进人工智能更加安全。

据公开记录显示，这份报告是由国务院委托的，该合同于2022年11月签署，金额为25万美元。它由Gladstone AI编写，这是一家专门为政府雇员提供人工智能技术简报的四人公司。（行动计划的部分建议政府大力投资于教育官员关于人工智能系统技术基础的知识，以便他们更好地理解其风险。）该报告作为一份247页的文件于2月26日提交给国务院。国务院未对有关报告的多次请求发表评论。报告的第一页写道，“这些建议并不代表美国国务院或美国政府的观点”。

报告的建议，其中许多以前被认为不可思议，是在 AI 领域一系列重大发展之后提出的，这些发展使许多观察者重新评估他们对技术的立场。2022 年 11 月发布的聊天机器人 ChatGPT 是这种变化步伐首次显现于社会大众中，导致许多人质疑未来的 AI 是否可能对人类构成存在风险。自那时以来，具备更多功能的新工具继续以快速的节奏发布。随着全球各国政府讨论如何最好地监管 AI，全球最大的科技公司已经快速建设基础设施，用于训练下一代更强大的系统——在某些情况下，计划使用 10 到 100 倍更多的计算能力。与此同时，超过 80% 的美国公众认为 AI 可能会意外引发灾难性事件，77% 的选民认为政府应该更多地进行 AI 监管，根据 AI 政策研究所最近的[民意调查](https://theaipi.org/)。

**阅读更多：** *[研究人员开发新技术，从 AI 系统中清除危险知识](https://time.com/6878893/ai-artificial-intelligence-dangerous-knowledge/)*

禁止在某一特定门槛以上训练先进 AI 系统，报告指出，可能会“调节所有 AI 开发者之间的竞争动态”，并有助于减缓芯片行业制造更快硬件的速度。随着时间推移，联邦 AI 机构可以提高门槛，一旦前沿模型的安全性证据得到充分证明，允许训练更先进的 AI 系统，报告建议。同样，如果发现现有模型具有危险能力，政府可以降低安全门槛。

这一提议可能面临政治上的困难。“我认为美国政府极不可能采纳这一建议”，沃德瓦尼人工智能与先进技术中心（CSIS）战略与国际研究中心的沃德瓦尼人工智能与先进技术中心（CSIS）主任格雷格·艾伦在回应 TIME 提供的关于禁止某一门槛以上 AI 训练的报告建议摘要时表示。他指出，目前美国政府的 AI 政策是设定计算门槛，超过该门槛将需要额外的透明监控和监管要求，但不会设定训练超过某一门槛以上的法律限制。“除非发生某种外部冲击，否则我认为他们改变这种方法的可能性很小，”艾伦说。

* * *

**Jeremie 和 Edouard Harris**，分别是 Gladstone 的 CEO 和 CTO，自 2021 年以来一直在向美国政府介绍人工智能的风险。这对兄弟说，出席过他们最早的几次简报的政府官员都认为人工智能的风险很大，但告诉他们应对这些风险的责任落在不同的团队或部门上。到了 2021 年底，Harris 兄弟称 Gladstone 终于找到了一个负责处理人工智能风险的政府部门：国务院国际安全与防扩散局。该局内部的团队有跨机构的授权来应对包括化学和生物武器、放射性和核风险在内的新兴技术风险。在 Jeremie 和 Gladstone 前 CEO Mark Beall 的简报后，2022 年 10 月，该局发布了一份招标文件，以便决定是否将人工智能加入其监控的其他风险列表中。（国务院未对决策结果做出回应。）Gladstone 团队赢得了该合同，周一发布了报告作为结果。

报告集中讨论了两种不同的风险类别。描述第一类别，即所谓的“武器化风险”，报告称：“这些系统可能被用于设计甚至执行灾难性的生物、化学或网络攻击，或在群体机器人技术中实现前所未有的武器化应用。”第二类别是报告称为“失控风险”，或者说先进人工智能系统可能超越其创造者的可能性。报告称，有理由相信，如果使用当前技术开发，这些系统可能是不可控的，并且可能默认对人类行为具有敌意。

报告指出，这两类风险都受人工智能行业的“竞争动态”加剧。报告称，第一家实现通用人工智能（AGI）的公司将获得大部分经济奖励的可能性，激励公司优先速度而非安全。报告称：“前沿人工智能实验室面临强烈和即时的动机，尽可能快速扩展其人工智能系统。”报告称：“尽管一些公司出于真正的关切投资于安全或安全措施，但这些公司并没有立即投资于不提供直接经济利益的安全或安全措施。”

Gladstone报告确定硬件——特别是目前用于训练AI系统的高端计算机芯片——作为增加AI能力的重要瓶颈。报告认为，监管这些硬件的传播可能是“保障长期全球安全和安全免受AI侵害的最重要要求”。它建议政府探索将芯片出口许可证与芯片上的监控技术存在关联，以此来执行建议的规则，禁止训练超过GPT-4规模的AI系统。然而，报告还指出，任何干预措施都需要考虑到过度管制可能会增强外国芯片行业，从而削弱美国影响供应链的能力。

**阅读更多：** [*了解美国对中国AI芯片出口的限制*](https://time.com/6324619/us-biden-ai-chips-china/)

报告还提出了一个可能性，即从根本上说，宇宙的物理界限可能不利于那些试图通过芯片阻止先进AI扩散的人。“随着AI算法的持续改进，更多的AI能力以更少的总计算量变得可用。根据这一趋势的进展程度，通过计算集中完全防止先进AI扩散可能最终变得不切实际。”为了考虑到这一可能性，报告称，一个新的联邦AI机构可以探索阻止提高算法效率的研究的发布，尽管它承认这可能会损害美国的AI产业，最终是不可行的。

哈里斯夫妇在谈话中意识到，他们的建议会让许多人在人工智能行业中感到过于热情。他们预计，禁止公开发布先进人工智能模型权重的建议不会受到欢迎。“开源通常是一种美妙的现象，对世界整体都是极其积极的，”Gladstone的首席技术官埃杜尔说。“这是一个极具挑战性的建议，我们花了很多时间寻找绕过此类措施的方法。”CSIS的AI政策专家艾伦说，他对开源AI使政策制定者难以掌控风险的想法表示同情。但他说，任何建议禁止公开发布超过一定规模的模型都必须考虑到美国法律的有限适用性。“这是否意味着开源社区会转移到欧洲？”他说。“考虑到世界如此之大，你必须考虑到这一点。”

**阅读更多：** [*2023年最重要的三个AI政策里程碑*](https://time.com/6513046/ai-policy-developments-2023/)

尽管存在挑战，报告的作者们表示，他们被当前用户如何[轻松和廉价地](https://arxiv.org/abs/2310.20624)去除AI模型的安全防护栏所动摇。埃杜尔说：“如果你广泛推广一个开源模型，即使看起来很安全，将来仍可能会有危险。”他补充说，开源模型的决定是不可逆转的。“那时候，祝你好运，你唯一能做的就是承受损失。”

报告的第三位共同作者、前国防部官员比尔已经离开格拉德斯通，开始创办一个旨在倡导人工智能政策的超级政治行动委员会。该政治行动委员会名为“AI安全美国人”，于周一正式启动。该组织在向《TIME》杂志发表的声明中表示，旨在使AI安全和安全性成为“2024年选举的关键问题，并计划在2024年底通过AI安全立法”。政治行动委员会没有透露其资金承诺，但表示已经设定了“筹集数百万美元以完成其使命”的目标。

在与比尔共同创办格拉德斯通之前，哈里斯兄弟经营过一家AI公司，并通过了著名的硅谷孵化器YCombinator，当时[OpenAI CEO Sam Altman](https://time.com/6342827/ceo-of-the-year-2023-sam-altman/)领导着该公司。他们以此作为证据，表明他们关心行业的利益，即使他们的建议如果得到实施将颠覆它。“快速行动并破坏事物，我们喜欢这种哲学，我们成长于这种哲学中，”杰里米告诉《TIME》。但他说，当你的行动潜在风险如此巨大时，这个信条就不再适用。“我们当前的默认轨迹，”他说，“看起来非常有可能创造出强大到可以灾难性武器化或无法控制的系统。”他补充道：“最糟糕的情况之一是发生灾难性事件，彻底关闭了所有人的AI研究，我们将无法享受到这项技术带来的惊人好处。”

### 更多来自《TIME》

***你是一个AI实验室的员工，是否担心可能会与记者分享？你可以通过Signal联系这篇文章的作者，账号为billyperrigo.01***
