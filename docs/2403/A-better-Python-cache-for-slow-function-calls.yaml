- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: Êú™ÂàÜÁ±ª'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-05-29 12:30:08'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: A better Python cache for slow function calls
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Êù•Ê∫êÔºö[https://docs.sweep.dev/blogs/file-cache](https://docs.sweep.dev/blogs/file-cache)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: <main class="nx-w-full nx-min-w-0 nx-max-w-6xl nx-px-6 nx-pt-4 md:nx-px-12">
  prefs: []
  type: TYPE_NORMAL
- en: üìö Blogs
  prefs: []
  type: TYPE_NORMAL
- en: üìÇ A better Python cache for slow function calls
  prefs: []
  type: TYPE_NORMAL
- en: A better Python cache for slow function calls
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**William Zeng** - March 18th, 2024'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: We wrote a file cache - it's like Python's `lru_cache`, but it stores the values
    in files instead of in memory. This has saved us hours of time running our LLM
    benchmarks, and we'd like to share it as its own Python module. Thanks to [Luke
    Jaggernauth (opens in a new tab)](https://github.com/lukejagg) (former Sweep engineer)
    for building the initial version of this!
  prefs: []
  type: TYPE_NORMAL
- en: 'Here''s the link: [https://github.com/sweepai/sweep/blob/main/docs/public/file_cache.py
    (opens in a new tab)](https://github.com/sweepai/sweep/blob/main/docs/public/file_cache.py).
    To use it, simply add the `file_cache` decorator to your function. Here''s an
    example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Background
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We spend a lot of time prompt engineering our agents at Sweep. Our agents take
    a set of input strings, formats them as a prompt, then sends the prompt and any
    other information off to the LLM. We chain multiple agents to turn a GitHub issue
    to a pull request. For example, to modify code we'll input the old code, any relevant
    context, and instructions then output the new code.
  prefs: []
  type: TYPE_NORMAL
- en: 'A typical improvement involves tweaking a small part of our pipeline (like
    improving our planning algorithm), then running the entire pipeline again. We
    use pdb (python''s native debugger) to set breakpoints and inspect the state of
    our prompts, input values, and parsing logic. For example, we can check whether
    a certain string matches a regex:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: This lets us debug at runtime with the actual data.
  prefs: []
  type: TYPE_NORMAL
- en: Cached pdb
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: pdb works great, but we have to wait for the entire pipeline to run again. Imagine
    a version of pdb that not only interrupted execution, but also cached the entire
    program state up to that point. Our time to hit that same bug could be cut from
    10 minutes to 15 seconds (a 40x improvement).
  prefs: []
  type: TYPE_NORMAL
- en: We didn't build this, but we think our file_cache works just as well.
  prefs: []
  type: TYPE_NORMAL
- en: LLM calls are slow but their inputs and outputs are easy to cache, saving a
    lot of time. We can use the input prompt/string as the cache key, and the output
    string as the cache value.
  prefs: []
  type: TYPE_NORMAL
- en: What's different from lru_cache?
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: lru_cache is great for memoizing repeated function calls, but it doesn't support
    two key features.
  prefs: []
  type: TYPE_NORMAL
- en: we need to persist the cache between runs.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: lru_cache stores the results in-memory, which means that the next time you run
    the program the cache will be empty. file_cache stores the results on disk. We
    also considered using Redis, but writing to disk is easier to set up/manage.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: lru_cache doesn't support ignoring arguments that invalidate the cache.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We'll use a custom `chat_logger` which stores the chats for visualization. It
    contains the current timestamp `chat_logger.expiration`, which will invalidate
    the cache if it's serialized.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'To counteract this we added ignored parameters, used like this: `file_cache(ignore_params=["chat_logger"])`.
    This removes `chat_logger` from the cache key construction and prevents bad invalidation
    due to the constantly changing `expiration`.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Our two main methods are `recursive_hash` and `file_cache`.
  prefs: []
  type: TYPE_NORMAL
- en: recursive_hash
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We want to stably hash objects, and this is [not natively supported in python
    (opens in a new tab)](https://death.andgravity.com/stable-hashing).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'hashlib.md5 alone doesn''t work for objects, giving us the error: `TypeError:
    object supporting the buffer API required`. We use recursive_hash, which works
    for arbitrary python objects.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: file_cache
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: file_cache is a decorator that handles the caching logic for us.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Without the cache, searching through the codebase using our LLM agent to get
    `top_results` takes 5 minutes - way too long if we're not actually testing it.
    Instead with file_cache, we just need to wait for deserialization of the pickled
    object - basically instantaneous for search results.
  prefs: []
  type: TYPE_NORMAL
- en: Wrapper
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: First we store our cache in `/tmp/file_cache`. This lets us remove the cache
    by simply deleting the directory (running `rm -rf /tmp/file_cache`). We can also
    selectively remove function calls using `rm -rf /tmp/file_cache/search_codebase*`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Then we can create a cache key.
  prefs: []
  type: TYPE_NORMAL
- en: Cache Key Creation / Miss Conditions
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'We have another problem - we want to miss our cache under two conditions:'
  prefs: []
  type: TYPE_NORMAL
- en: The arguments to the function change - handled by `recursive_hash`
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The code changes
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: To handle 2\. we used `inspect.getsource(func)` to add the function's source
    code to the hash, correctly missing the cache when the the code changes.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Cache hits and misses
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Finally we check cache key existence and write to the cache in the case of a
    cache miss.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Conclusion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We hope this code is useful to you. We've found it to be a massive time saver
    when debugging LLM calls. We'd love to hear your feedback and contributions at
    [https://github.com/sweepai/sweep (opens in a new tab)](https://github.com/sweepai/sweep)!
  prefs: []
  type: TYPE_NORMAL
- en: </main>
  prefs: []
  type: TYPE_NORMAL
