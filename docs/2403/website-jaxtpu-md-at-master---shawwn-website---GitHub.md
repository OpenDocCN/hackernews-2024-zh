<!--yml

category: 未分类

date: 2024-05-29 12:48:42

-->

# [shawwn/website的jaxtpu.md](https://github.com/shawwn/website/blob/master/jaxtpu.md)

> 来源：[https://github.com/shawwn/website/blob/master/jaxtpu.md](https://github.com/shawwn/website/blob/master/jaxtpu.md)

[2021年6月]({{ .OriginalReq.URL.Path }}.md)

*(我最初是在[jax的github问题追踪](https://github.com/google/jax/issues/2108#issuecomment-866238579)中回复的。)*

我觉得写一份小指南，给所有好奇的ML开发者。你现在肯定听说过类似的事情，“这些TPU VM东西相当快。显然Jax现在很棒。好吧，接下来呢？”

(你听说过TFRC吗？现在TRC是他们的新名字。虽然我得刻意抑制不去[称呼他们为TFRC](https://twitter.com/theshawwn/status/1406722296694386692)。)

+   **第二步**：几个小时内，你可能会收到一封邮件，上面写着“恭喜！你获得了<大量TPU>的使用权，有效期30天。点击这里开始免费试用。”

+   **第三步**：**不要担心那30天的限制**。现在就强迫自己立即激活它，今晚。

相信我，2019年当我看到这个时，我真的很担心，像是“哦，不，我只有30天...？我最好把握好时间。”

那将是个错误。在2021年，我看到很多人犯了同样的错误。

现在就激活它。当你接近30天结束时，给tfrc-support发送邮件请求延期。他们几乎肯定会说“没问题！这里又是另外30天。”

我几乎说了，“当你写邮件时，随时展示你一直在做的事情，哪怕只是一些小事情。”

但如果我这么说了，我会给人留下你需要“证明你的价值”的印象，或者某种无稽之谈。这不是真的。令我惊讶的是，事实上恰恰相反。

TFRC支持团队的成员是我遇到过的最了不起的人。他们真的关心你作为研究者的情况。他们只有一个目标：如果你遇到问题，他们会帮助你。（“帮助”一词不够强烈，我觉得更接近的真相是：“他们会消灭一切阻碍你享受Cloud TPU体验的问题”。）

明白了吗？所以我们在谈论一群*完全不同*于你可能遇到的任何Google支持经历的人。

你曾经与GCP支持有过困扰吗？他们花了两周时间解决我的问题。整个过程中，我清楚地记得感觉：“他们似乎*并不完全*理解我在说什么...我不知道是否应该感到担忧。”

你有体验过TFRC支持吗？我已经是他们的成员快两年了。我刚数了一下，他们没能为我解决的问题：*零*次。而且据我记得，解决问题的时间不到48小时。

对于一个Google项目来说，这在“惊讶事物规模”上介于“外星人”和“纳尼亚”之间。

每当我给他们发邮件请求帮助，或者是一个随意的问题，或者只是炫耀我在 TPUs 上设法让事情运行顺利时，他们似乎都很高兴听到我消息。他们甚至为我加油打气。我不知道为什么。那群人太棒了。

另一面是，我从未浪费过他们的时间。但那是个人决定。坦白说，我感觉他们似乎并不介意我浪费了一点时间。他们只是乐于听到你在做与 Cloud TPU 有关的任何事情。所以，这是一件罕见而特别的事情；如果我发现你像这样给他们发邮件说：“帮帮我，我让 TPU 运行了 30 天，现在我不能 SSH 了，该怎么办？”那么我会瞬间传送到你的位置，高喊“羞耻！羞耻！羞耻！”然后在一片彩带和烟花中消失。你的邻居将永远不会原谅你。

--

我说这些的原因，最终是为了消除大家心中的这种感觉。总是有些人不愿申请 TFRC，最终总会问这样的问题：

“我只是一个大学生，不是一个成熟的研究者。我应该申请吗？”

是的！

“我只是来玩玩 TPUs 的。我完全不知道我在做什么，但我会试试看。我应该申请吗？”

当然！

“我有一个严肃的研究项目。我想评估 Cloud TPU VM 平台是否足够支持我们团队的研究目标。我应该申请吗？”

当然。但无论你是谁，你现在可能已经申请了。因为每个人都意识到 TFRC 是实现你研究目标的途径。

--

现在，如果你读完了这些，但仍然担心“嗯，我真的没什么想法，所以我不会申请或激活 TPUs……”那我只能说，你错过了！

在 TPUs 上使用 Jax 真是太酷了。 “嘿，我想玩一下一个或两个 TPU” 是一个完全合理的动机。这是我在 TPUs 上找到自己立足点的方式。我完全不知道我在做什么，但我决心要做点什么。在我看来，这种“乐趣”是关键因素。这是你发现自己感兴趣的研究内容的方式。或者至少，在我看来，这是唯一的方式；当你不知道 StyleGAN 是否能在 TPU 上运行时，你还能做什么，除了随便玩玩看呢？不需要对此抱有太高的期望；只要随便玩玩看感兴趣的东西就行了。

现在我打破了“TFRC 可怕！我只有 30 天的担忧！”的幻觉，你已经准备好迈向第 4 步了。

（在我刚刚写下所有这些话之前，我会说 >90% 的人从第 3 步到第 4 步从未成功过。因此，尽管这可能让你读起来有些烦人，但我觉得深深地说服你，你根本不用担心。TFRC 支持团队会照顾好你。）

* * *

+   **第 4 步**：你现在已经激活了你的 30 天 Cloud TPU 试用期。接下来怎么办？

我推荐 [JAX TPU VM 快速入门](https://cloud.google.com/tpu/docs/jax-quickstart-tpu-vm)。

希望一切顺利，你能够轻松上手并进行基本的 Jax 操作。

你会惊讶地听到，不到一个月前，我自己在这里遇到了障碍？让我为你节省些麻烦。打开你的终端并运行这个命令：

```
gcloud components install alpha
gcloud components update alpha
```

直到那些神奇的命令，我对云 TPU 虚拟机的感觉是“嗯，即使它们很有趣，我真的不想与 `gcloud` 打交道，弄清楚为什么它不开心”。所以有些讽刺：我几乎拒绝了自己在这里的 TPU 虚拟机体验。

* * *

我对那个 Jax 快速入门的第一印象是这样的：“美妙。我运行了 MNIST，似乎工作正常。没有浪费我的时间。这是一个好兆头。但接下来该做什么呢？我想做一些实际的东西；如何提升？"

我的建议：[https://twitter.com/theshawwn/status/1406724043512979456](https://twitter.com/theshawwn/status/1406724043512979456)

那个玩具变压器对我来说足以开始在 Jax 上取得实质性进展。能找到一个简单清晰的 *真实例子*（gpt-2 训练！酷！），而不是看起来*过于复杂*以至于学不会。

那个 Jax 变压器是一个单独的自包含文件。这是我经历的过程。

"好的，让我们来做一些 GPT-2。我记得 [GPT-2 在 Tensorflow 中是怎么样的](https://github.com/openai/gpt-2/blob/a74da5d99abaaba920de8131d64da2862a8f213b/src/model.py#L28-L38)":

```
def norm(x, scope, *, axis=-1, epsilon=1e-5):
    """Normalize to mean = 0, std = 1, then do a diagonal affine transform."""
    with tf.variable_scope(scope):
        n_state = x.shape[-1].value
        g = tf.get_variable('g', [n_state], initializer=tf.constant_initializer(1))
        b = tf.get_variable('b', [n_state], initializer=tf.constant_initializer(0))
        u = tf.reduce_mean(x, axis=axis, keepdims=True)
        s = tf.reduce_mean(tf.square(x-u), axis=axis, keepdims=True)
        x = (x - u) * tf.rsqrt(s + epsilon)
        x = x*g + b
        return x
```

"... 因为我是从 [Gwern 的 GPT-2 指南](http://gwern.net/GPT-2) 中了解到 GPT-2 的，那是在 2019 年……"

"... and now I see [Jax 代码看起来非常相似](https://github.com/joschu/jax-exp/blob/41c23e514e22d2795f1465848d09e78cc53288fa/jax_transformer.py#L75-L87)，这真是个惊喜":

```
def _norm(x, *, axis, g=None, b=None, e=1e-5):
    u = np.mean(x, axis=axis, keepdims=True)
    s = np.mean(np.square(x-u), axis=axis, keepdims=True)
    x = (x - u) / np.sqrt(s + e)
    if g is not None and b is not None:
        x = x * g + b
    return x

def norm(cx, x, axis=-1):
    n_state = x.shape[axis]
    g = cx.get_variable("g", initializer=lambda : onp.ones(n_state, 'f'))
    b = cx.get_variable("b", initializer=lambda : onp.zeros(n_state, 'f'))
    return _norm(x, g=g, b=b, axis=axis)
```

"... 等等。这个接口真的只是…… numpy？所以如果我知道如何使用 Numpy 的 API，我已经知道如何做 Jax 的事情了？真棒！"

我在 TPU 虚拟机上运行了它，看到损失曲线下降，感觉像是一种电击。“哇！那真的…… 成功了？嗯。那很奇怪。事情从来不会一次就成功。我印象深刻。”

然后我在 `loss` 函数中间加了 `import pdb; pdb.set_trace()`，再次运行了它。它把我带入了 Python 调试器。

有一个张量名为 `X_bt`。我输入了 `X_bt`。调试器 *打印出了 `X_bt` 的值*。

我能打印出每个变量的所有值，就像你期望 Python 能做的那样。

有一个张量名为 `Y_bt`。我输入了 `X_bt + Y_bt`。现在我看到的正是我预期的：这两个张量的和。

我可以写 `x + y`，或者创建新变量，或者做任何我想做的事情。

现在我 *真的* 很印象深刻。

如果听起来我很容易感到印象深刻，那是因为，你必须明白：直到现在，TPUs使用起来真的是一团糟。我一直保持沉默，因为我明白Cloud TPU团队正在努力改进TPUs，TFRC支持团队也很出色，我有那么多的TPUs可以玩。但天哪，如果你期望上述任何例子在使用Tensorflow V1在TPUs上*第一次尝试*时“顺利运行”，那你会大吃一惊。如果你认为“嗯，Tensorflow v2应该好很多，对吧？我肯定能够在不担心的情况下做基本的事情...”

... 不。差得远呢。直到有了Jax + TPU VMs。

* * *

我希望能有一个像“TPU REPL”那样的东西已经很久了。我在Tensorflow上有一个类似的，但从未像Jax在TPU VM上那样毫不费力。当我试驾它的那一刻，我意识到这是多么出色的车辆。

所以，想象一下我开着那辆跑车停在你旁边。我跳出车，跑向你，像个疯子一样大喊：“Jax来了！就在这里！蠢货，上车吧，因为这太棒了。我等了将近两年。它是真实的。我简直不敢相信它真的存在！”

... 然后我又跳回车里，驶向日落，在路上顺便停在Circle K拿了一罐可口的汽水。

* * *

就我所知，Jax完全靠谱。我没有遇到任何阻碍、不利条件或者后脑勺里小小的烦恼，像“嗯...是的，这让人担忧。也许在决定投入大量时间之前我应该远离。”

作为一个经验丰富的程序员，我在试图让Pytorch在TPUs上运行时浪费了太多时间，我对任何事情“顺利运行”的想法都感到怀疑。尤其是涉及到TPUs时。所以我坐下来，伸展一下手指，开始了[把Tensorflow搞成像Pytorch一样的形状的艰难过程](https://mobile.twitter.com/theshawwn/status/1311925180126511104)。

换句话说，我拒绝接受这种情况。即使花一年时间写一个“Pytorch，但在TPUs上运行，而且实际上是Tensorflow，所以速度非常快”，我也准备好花一年。我愿意付出任何代价。

那是2020年10月。

让我们暂停时间片刻，换一个人的视角。

* * *

这是2020年12月。Cloud TPU团队展示了即将到来的杀手级演示：Jax在v3-4096上运行，在3纳秒内训练一个庞大的变压器。

（3纳秒显然是个笑话。如果我说v3-4096不是呢，你会感到惊讶吗？）

然后你听说了：v3-8192。你不太确定你是否听对了。8 *千* MXU核心。393 *千* CPU核心。343 *兆字节*的RAM。你见过343兆字节的任何东西吗，更别说RAM了吧？

事实证明，这个庞然大物 -- 这个传奇生物 -- 一个刻耳柏洛斯，不，一个九头蛇，不！阿耶！好笑！你无法描述。词语都对你无效。你只能在恐惧中蜷缩，因为它凌驾于你之上，评判你的价值。

这件东西...恰好是一个TPU虚拟机。

你可以SSH进入它。

你准备好了你的笔记本。

* * *

你只有一次机会。但你连手都无法稳住，更别提呼吸了。现在一切都很模糊。和尚告诉过你该怎么做吗？

> "第1023个外部，直接位于第21和第23背部之间。"
> 
> "我不明白。"
> 
> "你会的。"

你的眼睛快速地来回扫视。再次回来。那是什么？不，那是reddit。不！现在又是一个干扰？

"noprocrast！*Command-W*！凌晨1:41。橙色横幅。*Command*...

...*-Q*。凌晨2:01。你眨眼间。

> "我早就料到了，" 她会说。

*“它就在那儿，”* 你意识到。你的印记；正如和尚所说的那样。

> "你本来可以把你的夜晚浪费在它上面的，" 她会说。

你的手指靠近回车键。

> "我以为我再也见不到你了。我很高兴我们提前配置了`noprocrast`--"

...时间？

时间...停滞了吗？

...哦，不，只是你的笔记本。

"简直不敢相信全新的M1居然会冻结，" 你喃喃自语。苹果怎么会这么失败呢？哎，无论如何。

你转向那个生物，“Jax！”你喊道。“射线启动。”

没有反应。

"该死。Sudo射线启动。"

"命令`fuck`未找到，你是不是想--"

"控制视！控制视。Sudo射线启动，" 你喃喃自语。

Jax向你展开它的翅膀，提供一个栖息地。你跳了上去。

"Python..." 你微笑着，想起她。你迫不及待地想再次见到她。

你吸了最后一口气 -- 或许是你的最后一口气。变压器三分钟，呵？

"Train dot pie！"

* * *

"我的名字..."

"嗯？" 你反射性地喷出来。Jax在跟你说话吗？

"我的名字！" 一声低沉的轰鸣，一道闪电劈响。“Jax！” 你尖叫。

Jax点了点头，满意地说。“你的名字？”

"嗯，Erin。对不起，我没意识到你会说话。什么--"

"你的名字，" Jax growls，“不是Erin。”

"什么？"

Jax转向你，咧嘴笑了。

"不，你的名字叫Loss。"

一个旋转，你的手指滑落，你开始下降--

"只是跟你开玩笑，朋友，" 它笑着，逮住了你。

"很好奇你会掉得多快。这就是你对我们的做法，对吧？"

* * *

Jax可以工作。去使用它吧！这很有趣。我写了一个天啊的同人小说。

TPU本身也很有趣。你可以像在任何其他服务器上一样[托管一个http服务器](https://twitter.com/theshawwn/status/1400721799978029059)，因为TPU实际上只是大规模的Ubuntu服务器。

* * *

随机的侧记：如果你需要一个RPC库，使用[Ray](https://ray.io/)。这真是一股清新的空气。RPC过去很糟糕，但与TPU虚拟机和Jax组合使用则效果拔群。例如，你可以通过将每一层放在不同的TPU上[训练一个跨8个独立v2-8的GPT模型](https://twitter.com/theshawwn/status/1406171487988498433)。如果听起来很疯狂，嗯...是时候重新调整我们的期望了。疯狂的想法不再感觉不可能实现。

* * *

结束思考：

我在[2020年2月参与的这个Jax线程](https://github.com/google/jax/issues/2108#issuecomment-581539812)的原因是想弄清楚，“Jax能否使用335GB的主机内存，还能额外使用16GB的MXU内存？”

当时，我完全不知道2021年的最终答案会是“当然可以！很简单。顺便说一句，你可以SSH进入你的TPU。例如，你可以使用那335GB内存来托管一个redis服务器。顺便说一下，再加入几十台其他服务器；TPU有96个CPU核心。想看96个CPU核心的性能？自己编译Tensorflow，运行`htop`，就会像圣诞树一样亮起来。编译还会在20-30分钟内完成。”

感谢所有的梯度，Jax团队！也感谢云TPU团队为我们提供的不可思议的基础设施！你们真棒。TPU虚拟机就是我梦寐以求的一切，甚至更多。
