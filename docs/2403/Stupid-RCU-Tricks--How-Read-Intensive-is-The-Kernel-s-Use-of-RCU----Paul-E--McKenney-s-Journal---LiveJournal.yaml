- en: <!--yml
  id: totrans-split-0
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  id: totrans-split-1
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-05-29 12:50:15'
  id: totrans-split-2
  prefs: []
  type: TYPE_NORMAL
- en: -->
  id: totrans-split-3
  prefs: []
  type: TYPE_NORMAL
- en: 'Stupid RCU Tricks: How Read-Intensive is The Kernel''s Use of RCU? - Paul E.
    McKenney''s Journal — LiveJournal'
  id: totrans-split-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://paulmck.livejournal.com/67547.html](https://paulmck.livejournal.com/67547.html)
  id: totrans-split-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: RCU is a specialized synchronization mechanism, and is typically used where
    there are far more readers (
  id: totrans-split-6
  prefs: []
  type: TYPE_NORMAL
- en: '`rcu_read_lock()`'
  id: totrans-split-7
  prefs: []
  type: TYPE_NORMAL
- en: ','
  id: totrans-split-8
  prefs: []
  type: TYPE_NORMAL
- en: '`rcu_read_unlock()`'
  id: totrans-split-9
  prefs: []
  type: TYPE_NORMAL
- en: ','
  id: totrans-split-10
  prefs: []
  type: TYPE_NORMAL
- en: '`rcu_dereference()`'
  id: totrans-split-11
  prefs: []
  type: TYPE_NORMAL
- en: ', and so on) than there are updaters ('
  id: totrans-split-12
  prefs: []
  type: TYPE_NORMAL
- en: '`synchronize_rcu()`'
  id: totrans-split-13
  prefs: []
  type: TYPE_NORMAL
- en: ','
  id: totrans-split-14
  prefs: []
  type: TYPE_NORMAL
- en: '`call_rcu()`'
  id: totrans-split-15
  prefs: []
  type: TYPE_NORMAL
- en: ','
  id: totrans-split-16
  prefs: []
  type: TYPE_NORMAL
- en: '`rcu_assign_pointer()`'
  id: totrans-split-17
  prefs: []
  type: TYPE_NORMAL
- en: ', and so on). But does the Linux kernel really make heavier use of RCU''s read-side
    primitives than of its update-side primitives?'
  id: totrans-split-18
  prefs: []
  type: TYPE_NORMAL
- en: One way to determine this would be to use something like ftrace to record all
    the calls to these functions. This works, but trace messages can be lost, especially
    when applied to frequently invoked functions. Also, dumping out the trace buffer
    can perturb the syatem. Another approach is to modify the kernel source code to
    count these function invocations in a cache-friendly manner, then come up with
    some way to dump this to userspace. This works, but I am lazy. Yet another approach
    is to ask the tracing folks for advice.
  id: totrans-split-19
  prefs: []
  type: TYPE_NORMAL
- en: This last is what I actually did, and because the tracing person I happened
    to ask happened to be Andrii Nakryiko, I learned quite a bit about BPF in general
    and the
  id: totrans-split-20
  prefs: []
  type: TYPE_NORMAL
- en: '`bpftrace`'
  id: totrans-split-21
  prefs: []
  type: TYPE_NORMAL
- en: command in particular. If you don't happen to have Andrii on hand, you can do
    quite well with Appendix A and Appendix B of Brendan Gregg's “BPF Performance
    Tools”. You will of course need to install
  id: totrans-split-22
  prefs: []
  type: TYPE_NORMAL
- en: '`bpftrace`'
  id: totrans-split-23
  prefs: []
  type: TYPE_NORMAL
- en: itself, which is reasonably straightforward on many Linux distributions.
  id: totrans-split-24
  prefs: []
  type: TYPE_NORMAL
- en: Linux-Kernel RCU Read Intensity
  id: totrans-split-25
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Those of you who have used
  id: totrans-split-26
  prefs: []
  type: TYPE_NORMAL
- en: '`sed`'
  id: totrans-split-27
  prefs: []
  type: TYPE_NORMAL
- en: and
  id: totrans-split-28
  prefs: []
  type: TYPE_NORMAL
- en: '`awk`'
  id: totrans-split-29
  prefs: []
  type: TYPE_NORMAL
- en: have a bit of a running start because you can invoke
  id: totrans-split-30
  prefs: []
  type: TYPE_NORMAL
- en: '`bpftrace`'
  id: totrans-split-31
  prefs: []
  type: TYPE_NORMAL
- en: with a
  id: totrans-split-32
  prefs: []
  type: TYPE_NORMAL
- en: '`-e`'
  id: totrans-split-33
  prefs: []
  type: TYPE_NORMAL
- en: argument and a series of tracepoint/program pairs, where a program is
  id: totrans-split-34
  prefs: []
  type: TYPE_NORMAL
- en: '`bpftrace`'
  id: totrans-split-35
  prefs: []
  type: TYPE_NORMAL
- en: code enclosed in curly braces. This code is compiled, verified, and loaded into
    the running kernel as a kernel module. When the code finishes executing, the results
    are printed right there for you on
  id: totrans-split-36
  prefs: []
  type: TYPE_NORMAL
- en: '`stdout`'
  id: totrans-split-37
  prefs: []
  type: TYPE_NORMAL
- en: '. For example:'
  id: totrans-split-38
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  id: totrans-split-39
  prefs:
  - PREF_BQ
  type: TYPE_PRE
- en: This command uses the
  id: totrans-split-40
  prefs: []
  type: TYPE_NORMAL
- en: '`kprobe`'
  id: totrans-split-41
  prefs: []
  type: TYPE_NORMAL
- en: facility to attach a program to the
  id: totrans-split-42
  prefs: []
  type: TYPE_NORMAL
- en: '`__rcu_read_lock()`'
  id: totrans-split-43
  prefs: []
  type: TYPE_NORMAL
- en: function and to attach a very similar program to the
  id: totrans-split-44
  prefs: []
  type: TYPE_NORMAL
- en: '`rcu_gp_fqs_loop()`'
  id: totrans-split-45
  prefs: []
  type: TYPE_NORMAL
- en: function, which happens to be invoked exactly once per RCU grace period. Both
    programs count the number of calls, with
  id: totrans-split-46
  prefs: []
  type: TYPE_NORMAL
- en: '`@gp`'
  id: totrans-split-47
  prefs: []
  type: TYPE_NORMAL
- en: being the
  id: totrans-split-48
  prefs: []
  type: TYPE_NORMAL
- en: '`bpftrace`'
  id: totrans-split-49
  prefs: []
  type: TYPE_NORMAL
- en: “variable” accumulating the count, and the
  id: totrans-split-50
  prefs: []
  type: TYPE_NORMAL
- en: '`count()`'
  id: totrans-split-51
  prefs: []
  type: TYPE_NORMAL
- en: function doing the counting in a cache-friendly manner. The final
  id: totrans-split-52
  prefs: []
  type: TYPE_NORMAL
- en: '`interval:s:10`'
  id: totrans-split-53
  prefs: []
  type: TYPE_NORMAL
- en: in effect attaches a program to a timer, so that this last program will execute
    every 10 seconds (“
  id: totrans-split-54
  prefs: []
  type: TYPE_NORMAL
- en: '`s:10`'
  id: totrans-split-55
  prefs: []
  type: TYPE_NORMAL
- en: ”). Except that the program invokes the
  id: totrans-split-56
  prefs: []
  type: TYPE_NORMAL
- en: '`exit()`'
  id: totrans-split-57
  prefs: []
  type: TYPE_NORMAL
- en: function that terminates this
  id: totrans-split-58
  prefs: []
  type: TYPE_NORMAL
- en: '`bpftrace`'
  id: totrans-split-59
  prefs: []
  type: TYPE_NORMAL
- en: program at the end of the very first 10-second time interval. Upon termination,
  id: totrans-split-60
  prefs: []
  type: TYPE_NORMAL
- en: '`bpftrace`'
  id: totrans-split-61
  prefs: []
  type: TYPE_NORMAL
- en: 'outputs the following on an idle system:'
  id: totrans-split-62
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  id: totrans-split-63
  prefs:
  - PREF_BQ
  type: TYPE_PRE
- en: In other words, there were about a thousand grace periods and more than six
    million RCU readers during that 10-second time period, for a read-to-grace-period
    ratio of more than six thousand. This certainly qualifies as read-intensive.
  id: totrans-split-64
  prefs: []
  type: TYPE_NORMAL
- en: But what if the system is busy? Much depends on exactly how busy the system
    is, as well as exactly how it is busy, but let's use that old standby, the kernel
    build (but using the
  id: totrans-split-65
  prefs: []
  type: TYPE_NORMAL
- en: '`nice`'
  id: totrans-split-66
  prefs: []
  type: TYPE_NORMAL
- en: command to avoid delaying
  id: totrans-split-67
  prefs: []
  type: TYPE_NORMAL
- en: '`bpftrace`'
  id: totrans-split-68
  prefs: []
  type: TYPE_NORMAL
- en: ). Let's also put the
  id: totrans-split-69
  prefs: []
  type: TYPE_NORMAL
- en: '`bpftrace`'
  id: totrans-split-70
  prefs: []
  type: TYPE_NORMAL
- en: script into a creatively named file
  id: totrans-split-71
  prefs: []
  type: TYPE_NORMAL
- en: '`rcu1.bpf`'
  id: totrans-split-72
  prefs: []
  type: TYPE_NORMAL
- en: 'like so:'
  id: totrans-split-73
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  id: totrans-split-74
  prefs:
  - PREF_BQ
  type: TYPE_PRE
- en: This allows the command
  id: totrans-split-75
  prefs: []
  type: TYPE_NORMAL
- en: '`bpftrace rcu1.bpf`'
  id: totrans-split-76
  prefs: []
  type: TYPE_NORMAL
- en: 'to produce the following output:'
  id: totrans-split-77
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  id: totrans-split-78
  prefs:
  - PREF_BQ
  type: TYPE_PRE
- en: Where the idle system had about one thousand grace periods over the course of
    ten seconds, the busy system had only 274\. On the other hand, the busy system
    had 78 million RCU read-side critical sections, more than ten times that of the
    idle system. The busy system had more than one quarter million RCU read-side critical
    sections per grace period, which is seriously read-intensive.
  id: totrans-split-79
  prefs: []
  type: TYPE_NORMAL
- en: RCU works hard to make the same grace-period computation cover multiple requests.
    Because
  id: totrans-split-80
  prefs: []
  type: TYPE_NORMAL
- en: '`synchronize_rcu()`'
  id: totrans-split-81
  prefs: []
  type: TYPE_NORMAL
- en: invokes
  id: totrans-split-82
  prefs: []
  type: TYPE_NORMAL
- en: '`call_rcu()`'
  id: totrans-split-83
  prefs: []
  type: TYPE_NORMAL
- en: ', we can use the number of'
  id: totrans-split-84
  prefs: []
  type: TYPE_NORMAL
- en: '`call_rcu()`'
  id: totrans-split-85
  prefs: []
  type: TYPE_NORMAL
- en: invocations as a rough proxy for the number of updates, that is, the number
    of requests for a grace period. (The more invocations of
  id: totrans-split-86
  prefs: []
  type: TYPE_NORMAL
- en: '`synchronize_rcu_expedited()`'
  id: totrans-split-87
  prefs: []
  type: TYPE_NORMAL
- en: and
  id: totrans-split-88
  prefs: []
  type: TYPE_NORMAL
- en: '`kfree_rcu()`'
  id: totrans-split-89
  prefs: []
  type: TYPE_NORMAL
- en: ', the rougher this proxy will be.)'
  id: totrans-split-90
  prefs: []
  type: TYPE_NORMAL
- en: We can make the
  id: totrans-split-91
  prefs: []
  type: TYPE_NORMAL
- en: '`bpftrace`'
  id: totrans-split-92
  prefs: []
  type: TYPE_NORMAL
- en: script more concise by assigning the same action to a group of tracepoints,
    as in the
  id: totrans-split-93
  prefs: []
  type: TYPE_NORMAL
- en: '`rcu2.bpf`'
  id: totrans-split-94
  prefs: []
  type: TYPE_NORMAL
- en: 'file shown here:'
  id: totrans-split-95
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  id: totrans-split-96
  prefs: []
  type: TYPE_PRE
- en: With this file in place,
  id: totrans-split-97
  prefs: []
  type: TYPE_NORMAL
- en: '`bpftrace rcu2.bpf`'
  id: totrans-split-98
  prefs: []
  type: TYPE_NORMAL
- en: 'produces the following output in the midst of a kernel build:'
  id: totrans-split-99
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  id: totrans-split-100
  prefs: []
  type: TYPE_PRE
- en: These results look quite different from the earlier kernel-build results, confirming
    any suspicions you might harbor about the suitability of kernel builds as a repeatable
    benchmark. Nevertheless, there are about 180K RCU read-side critical sections
    per grace period, which is still seriously read-intensive. Furthermore, there
    are also almost 2K
  id: totrans-split-101
  prefs: []
  type: TYPE_NORMAL
- en: '`call_rcu()`'
  id: totrans-split-102
  prefs: []
  type: TYPE_NORMAL
- en: invocations per RCU grace period, which means that RCU is able to amortize the
    overhead of a given grace period down to almost nothing per grace-period request.
  id: totrans-split-103
  prefs: []
  type: TYPE_NORMAL
- en: Linux-Kernel RCU Grace-Period Latency
  id: totrans-split-104
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The following
  id: totrans-split-105
  prefs: []
  type: TYPE_NORMAL
- en: '`bpftrace`'
  id: totrans-split-106
  prefs: []
  type: TYPE_NORMAL
- en: program makes a histogram of grace-period latencies, that is, the time from
    the call to
  id: totrans-split-107
  prefs: []
  type: TYPE_NORMAL
- en: '`rcu_gp_init()`'
  id: totrans-split-108
  prefs: []
  type: TYPE_NORMAL
- en: to the return from
  id: totrans-split-109
  prefs: []
  type: TYPE_NORMAL
- en: '`rcu_gp_cleanup()`'
  id: totrans-split-110
  prefs: []
  type: TYPE_NORMAL
- en: ':'
  id: totrans-split-111
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  id: totrans-split-112
  prefs: []
  type: TYPE_PRE
- en: The
  id: totrans-split-113
  prefs: []
  type: TYPE_NORMAL
- en: '`kretprobe`'
  id: totrans-split-114
  prefs: []
  type: TYPE_NORMAL
- en: attaches the program to the return from
  id: totrans-split-115
  prefs: []
  type: TYPE_NORMAL
- en: '`rcu_gp_cleanup()`'
  id: totrans-split-116
  prefs: []
  type: TYPE_NORMAL
- en: . The
  id: totrans-split-117
  prefs: []
  type: TYPE_NORMAL
- en: '`hist()`'
  id: totrans-split-118
  prefs: []
  type: TYPE_NORMAL
- en: function computes a log-scale histogram. The check of the
  id: totrans-split-119
  prefs: []
  type: TYPE_NORMAL
- en: '`@start`'
  id: totrans-split-120
  prefs: []
  type: TYPE_NORMAL
- en: variable avoids a beginning-of-time value for this variable in the common case
    where this script start in the middle of a grace period. (Try it without that
    check!)
  id: totrans-split-121
  prefs: []
  type: TYPE_NORMAL
- en: 'The output is as follows:'
  id: totrans-split-122
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  id: totrans-split-123
  prefs: []
  type: TYPE_PRE
- en: Most of the grace periods complete within between four and eight milliseconds,
    with most of the remainder completing within between two and four milliseconds
    and then between eight and sixteen milliseonds, but with a few stragglers taking
    up to 64 milliseconds. The final
  id: totrans-split-124
  prefs: []
  type: TYPE_NORMAL
- en: '`@start`'
  id: totrans-split-125
  prefs: []
  type: TYPE_NORMAL
- en: line shows that
  id: totrans-split-126
  prefs: []
  type: TYPE_NORMAL
- en: '`bpftrace`'
  id: totrans-split-127
  prefs: []
  type: TYPE_NORMAL
- en: simply dumps out all the variables. You can use the
  id: totrans-split-128
  prefs: []
  type: TYPE_NORMAL
- en: '`delete(@start)`'
  id: totrans-split-129
  prefs: []
  type: TYPE_NORMAL
- en: function to prevent printing of
  id: totrans-split-130
  prefs: []
  type: TYPE_NORMAL
- en: '`@start`'
  id: totrans-split-131
  prefs: []
  type: TYPE_NORMAL
- en: ', but please note that the next invocation of'
  id: totrans-split-132
  prefs: []
  type: TYPE_NORMAL
- en: '`rcu_gp_init()`'
  id: totrans-split-133
  prefs: []
  type: TYPE_NORMAL
- en: will re-create it.
  id: totrans-split-134
  prefs: []
  type: TYPE_NORMAL
- en: It is nice to know the internal latency of an RCU grace period, but most in-kernel
    users will be more concerned about the latency of the
  id: totrans-split-135
  prefs: []
  type: TYPE_NORMAL
- en: '`synchronize_rcu()`'
  id: totrans-split-136
  prefs: []
  type: TYPE_NORMAL
- en: function, which will need to wait for the current grace period to complete and
    also for callback invocation. We can measure this function's latency with the
    following
  id: totrans-split-137
  prefs: []
  type: TYPE_NORMAL
- en: '`bpftrace`'
  id: totrans-split-138
  prefs: []
  type: TYPE_NORMAL
- en: 'script:'
  id: totrans-split-139
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  id: totrans-split-140
  prefs: []
  type: TYPE_PRE
- en: The
  id: totrans-split-141
  prefs: []
  type: TYPE_NORMAL
- en: '`tid`'
  id: totrans-split-142
  prefs: []
  type: TYPE_NORMAL
- en: variable contains the ID of the currently running task, which allows this script
    to associate a given return from
  id: totrans-split-143
  prefs: []
  type: TYPE_NORMAL
- en: '`synchronize_rcu()`'
  id: totrans-split-144
  prefs: []
  type: TYPE_NORMAL
- en: with the corresponding call by using
  id: totrans-split-145
  prefs: []
  type: TYPE_NORMAL
- en: '`tid`'
  id: totrans-split-146
  prefs: []
  type: TYPE_NORMAL
- en: as an index to the
  id: totrans-split-147
  prefs: []
  type: TYPE_NORMAL
- en: '`@start`'
  id: totrans-split-148
  prefs: []
  type: TYPE_NORMAL
- en: variable.
  id: totrans-split-149
  prefs: []
  type: TYPE_NORMAL
- en: 'As you would expect, the resulting histogram is weighted towards somewhat longer
    latencies, though without the stragglers:'
  id: totrans-split-150
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  id: totrans-split-151
  prefs: []
  type: TYPE_PRE
- en: In addition, we see not one but two values for
  id: totrans-split-152
  prefs: []
  type: TYPE_NORMAL
- en: '`@start`'
  id: totrans-split-153
  prefs: []
  type: TYPE_NORMAL
- en: . The
  id: totrans-split-154
  prefs: []
  type: TYPE_NORMAL
- en: '`delete`'
  id: totrans-split-155
  prefs: []
  type: TYPE_NORMAL
- en: statement gets rid of old ones, but any new call to
  id: totrans-split-156
  prefs: []
  type: TYPE_NORMAL
- en: '`synchronize_rcu()`'
  id: totrans-split-157
  prefs: []
  type: TYPE_NORMAL
- en: will create more of them.
  id: totrans-split-158
  prefs: []
  type: TYPE_NORMAL
- en: Linux-Kernel Expedited RCU Grace-Period Latency
  id: totrans-split-159
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Linux kernels will sometimes executed
  id: totrans-split-160
  prefs: []
  type: TYPE_NORMAL
- en: '`synchronize_rcu_expedited()`'
  id: totrans-split-161
  prefs: []
  type: TYPE_NORMAL
- en: to obtain a faster grace period, and the following command will further cause
  id: totrans-split-162
  prefs: []
  type: TYPE_NORMAL
- en: '`synchronize_rcu()`'
  id: totrans-split-163
  prefs: []
  type: TYPE_NORMAL
- en: to act like
  id: totrans-split-164
  prefs: []
  type: TYPE_NORMAL
- en: '`synchronize_rcu_expedited()`'
  id: totrans-split-165
  prefs: []
  type: TYPE_NORMAL
- en: ':'
  id: totrans-split-166
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  id: totrans-split-167
  prefs:
  - PREF_BQ
  type: TYPE_PRE
- en: Doing this on a dual-socket system with 80 hardware threads might be ill-advised,
    but you only live once!
  id: totrans-split-168
  prefs: []
  type: TYPE_NORMAL
- en: Ill-advised or not, the following
  id: totrans-split-169
  prefs: []
  type: TYPE_NORMAL
- en: '`bpftrace`'
  id: totrans-split-170
  prefs: []
  type: TYPE_NORMAL
- en: script measures
  id: totrans-split-171
  prefs: []
  type: TYPE_NORMAL
- en: '`synchronize_rcu_expedited()`'
  id: totrans-split-172
  prefs: []
  type: TYPE_NORMAL
- en: 'latency, but in microseconds rather than milliseconds:'
  id: totrans-split-173
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  id: totrans-split-174
  prefs:
  - PREF_BQ
  type: TYPE_PRE
- en: 'The output of this script run concurrently with a kernel build is as follows:'
  id: totrans-split-175
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  id: totrans-split-176
  prefs:
  - PREF_BQ
  type: TYPE_PRE
- en: Most
  id: totrans-split-177
  prefs: []
  type: TYPE_NORMAL
- en: '`synchronize_rcu_expedited()`'
  id: totrans-split-178
  prefs: []
  type: TYPE_NORMAL
- en: invocations complete within a few hundred microseconds, but with a few stragglers
    around ten milliseconds.
  id: totrans-split-179
  prefs: []
  type: TYPE_NORMAL
- en: But what about linear histograms? This is what the
  id: totrans-split-180
  prefs: []
  type: TYPE_NORMAL
- en: '`lhist()`'
  id: totrans-split-181
  prefs: []
  type: TYPE_NORMAL
- en: 'function is for, with added minimum, maximum, and bucket-size arguments:'
  id: totrans-split-182
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  id: totrans-split-183
  prefs:
  - PREF_BQ
  type: TYPE_PRE
- en: 'Running this with the usual kernel build in the background:'
  id: totrans-split-184
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  id: totrans-split-185
  prefs:
  - PREF_BQ
  type: TYPE_PRE
- en: The final bucket is overflow, containing measurements that exceeded the one-millisecond
    limit.
  id: totrans-split-186
  prefs: []
  type: TYPE_NORMAL
- en: The above histogram had only a few empty buckets, but that is mostly because
    the 18
  id: totrans-split-187
  prefs: []
  type: TYPE_NORMAL
- en: '`synchronize_rcu_expedited()`'
  id: totrans-split-188
  prefs: []
  type: TYPE_NORMAL
- en: instances that overflowed the one-millisecond limit are consolidated into a
    single
  id: totrans-split-189
  prefs: []
  type: TYPE_NORMAL
- en: '`[1000, ...)`'
  id: totrans-split-190
  prefs: []
  type: TYPE_NORMAL
- en: overflow bucket. This is sometimes what is needed, but other times losing the
    maximum latency can be a problem. This can be dealt with given the following
  id: totrans-split-191
  prefs: []
  type: TYPE_NORMAL
- en: '`bpftrace`'
  id: totrans-split-192
  prefs: []
  type: TYPE_NORMAL
- en: 'program:'
  id: totrans-split-193
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  id: totrans-split-194
  prefs:
  - PREF_BQ
  type: TYPE_PRE
- en: 'Given the usual kernel-build background load, this produces the following output:'
  id: totrans-split-195
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  id: totrans-split-196
  prefs:
  - PREF_BQ
  type: TYPE_PRE
- en: 'This is a bit hard to read, but simple scripting can be applied to this output
    to produce something like this:'
  id: totrans-split-197
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  id: totrans-split-198
  prefs:
  - PREF_BQ
  type: TYPE_PRE
- en: This produces compact output despite outliers such as the last entry, corresponding
    to an invocation that took somewhere between 1.6 and 1.7 milliseconds.
  id: totrans-split-199
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-split-200
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The
  id: totrans-split-201
  prefs: []
  type: TYPE_NORMAL
- en: '`bpftrace`'
  id: totrans-split-202
  prefs: []
  type: TYPE_NORMAL
- en: command can be used to quickly and easily script compiled in-kernel programs
    that can measure and monitor a wide variety of things. This post focused on a
    few aspects of RCU, but quite a bit more material may be found in Brendan Gregg's
    “BPF Performance Tools” book.
  id: totrans-split-203
  prefs: []
  type: TYPE_NORMAL
