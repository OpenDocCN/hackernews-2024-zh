- en: <!--yml
  id: totrans-split-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-split-1
  prefs: []
  type: TYPE_NORMAL
  zh: 'category: 未分类'
- en: 'date: 2024-05-29 12:31:28'
  id: totrans-split-2
  prefs: []
  type: TYPE_NORMAL
  zh: 'date: 2024-05-29 12:31:28'
- en: -->
  id: totrans-split-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: Bug hunting in Btrfs - tavianator.com
  id: totrans-split-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Btrfs的Bug追踪 - tavianator.com
- en: 来源：[https://tavianator.com/2024/btrfs_bug.html](https://tavianator.com/2024/btrfs_bug.html)
  id: totrans-split-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://tavianator.com/2024/btrfs_bug.html](https://tavianator.com/2024/btrfs_bug.html)
- en: <main>
  id: totrans-split-6
  prefs: []
  type: TYPE_NORMAL
  zh: <main>
- en: 'The other day I was implementing [multi-threaded `stat()` calls](/cgit/bfs.git/commit/?id=89ecb2a08467cd8aa6ba70f8519df494652cac96)
    in [`bfs`](../projects/bfs.html). When I ran some benchmarks, I saw something
    that made my heart skip a beat:'
  id: totrans-split-7
  prefs: []
  type: TYPE_NORMAL
  zh: 前几天我在 [`bfs`](../projects/bfs.html) 中实现了多线程的 `stat()` 调用。在运行了一些基准测试后，我发现了一些令人激动的事情：
- en: '[PRE0]'
  id: totrans-split-8
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: If you're not familiar, "structure needs cleaning" is the human-readable description
    of `EUCLEAN`, an `errno` value that usually indicates filesystem corruption. Fearing
    the worst, I checked `dmesg` and saw
  id: totrans-split-9
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你不熟悉，"structure needs cleaning" 是 `EUCLEAN` 的人类可读描述，通常表示文件系统损坏。担心情况严重，我检查了
    `dmesg` 并看到
- en: '[PRE1]'
  id: totrans-split-10
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: I immediately rebooted into a live environment and ran `btrfs check`, but to
    my surprise, there were no errors found. `btrfs scrub` also found no evidence
    of corruption. And in fact, the file from the error message was completely fine.
    Whatever the issue was, it seemed to have gone away on its own.
  id: totrans-split-11
  prefs: []
  type: TYPE_NORMAL
  zh: 我立即重启进入live环境并运行了 `btrfs check`，但令我惊讶的是，并没有发现错误。`btrfs scrub` 也没有发现任何损坏的证据。事实上，错误消息中的文件完全没有问题。无论问题是什么，它似乎已经自行消失了。
- en: 'I searched Google for that "corrupted node" message and found that it had happened
    to someone else recently too: [Linus Torvalds](https://lore.kernel.org/linux-btrfs/CAHk-=whNdMaN9ntZ47XRKP6DBes2E5w7fi-0U3H2+PS18p+Pzw@mail.gmail.com/),
    just after merging a Btrfs pull request. (This was not the first time Linus and
    I had hit the same bug. We both have the same CPU in our desktops. Last time it
    led to one of my [favourite bugfixes ever](https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/commit/?id=a51ab63b297ce9e26e3ffb9be896018a42d5f32f).)'
  id: totrans-split-12
  prefs: []
  type: TYPE_NORMAL
  zh: 我在Google上搜索了那个 "corrupted node" 消息，发现最近也发生在了别人身上：[Linus Torvalds](https://lore.kernel.org/linux-btrfs/CAHk-=whNdMaN9ntZ47XRKP6DBes2E5w7fi-0U3H2+PS18p+Pzw@mail.gmail.com/)，就在合并Btrfs的一个pull请求后。这不是我和Linus第一次碰到相同的bug。我们的台式机都使用相同的CPU。上次这导致了我一次
    [最喜欢的bug修复](https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/commit/?id=a51ab63b297ce9e26e3ffb9be896018a42d5f32f)。
- en: But the difference between me and Linus was that I could (semi-)reliably reproduce
    the error. Not every time, but maybe 10% of the time I ran `bfs` with parallel
    `stat()` calls on a large directory tree, I would see that "structure needs cleaning"
    error. I reported my [findings](https://lore.kernel.org/linux-btrfs/20240206033807.15498-1-tavianator@tavianator.com/)
    to the Btrfs mailing list, and though it led to some discussion about potential
    causes, we didn't narrow down the root cause.
  id: totrans-split-13
  prefs: []
  type: TYPE_NORMAL
  zh: 但我和Linus之间的不同之处在于，我可以（半）可靠地重现这个错误。虽然不是每次，但可能有10%的时间我在大型目录树上并行运行 `bfs` 的 `stat()`
    调用时，我会看到 "structure needs cleaning" 错误。我将我的 [发现](https://lore.kernel.org/linux-btrfs/20240206033807.15498-1-tavianator@tavianator.com/)
    报告给了Btrfs邮件列表，尽管这引发了关于潜在原因的讨论，但我们并没有找到根本原因。
- en: Based on the logs, the Btrfs developers assumed the problem had to do allocation
    of [`struct extent_buffer`](https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/tree/fs/btrfs/extent_io.h?id=a51ab63b297ce9e26e3ffb9be896018a42d5f32f#n76)
    and their memory pages. I'm not a Btrfs internals expert by any means, but I understand
    that `extent_buffer`s are used for metadata I/O; that is, reading and writing
    [B-tree](https://en.wikipedia.org/wiki/B-tree) nodes. An `extent_buffer` is associated
    with an array of `struct folio`/`struct page` (depending on kernel version), and
    those pages hold the actual contents of the extents that are read or written.
    The abstraction helps with file system block sizes that are different from the
    system page size.
  id: totrans-split-14
  prefs: []
  type: TYPE_NORMAL
  zh: 基于日志记录，Btrfs 开发人员认为问题涉及到 [`struct extent_buffer`](https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/tree/fs/btrfs/extent_io.h?id=a51ab63b297ce9e26e3ffb9be896018a42d5f32f#n76)
    的分配及其内存页面。我并不是Btrfs内部专家，但我了解 `extent_buffer` 用于元数据I/O，即读取和写入 [B-tree](https://en.wikipedia.org/wiki/B-tree)
    节点。`extent_buffer` 关联着一个 `struct folio`/`struct page` 数组（取决于内核版本），这些页面持有读取或写入的extent的实际内容。这种抽象有助于处理与系统页面大小不同的文件系统块大小。
- en: Allocating an `extent_buffer` and its pages is [more complicated than you might
    expect](https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/tree/fs/btrfs/extent_io.c?id=a51ab63b297ce9e26e3ffb9be896018a42d5f32f#n3681).
    Each `struct extent_buffer` holds an array of `struct folio`, and those folios
    have a private reference that points back to the `extent_buffer`. There should
    only ever be one `extent_buffer` at a time for any particular extent.
  id: totrans-split-15
  prefs: []
  type: TYPE_NORMAL
  zh: 分配`extent_buffer`及其页面比你想象的[更复杂](https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/tree/fs/btrfs/extent_io.c?id=a51ab63b297ce9e26e3ffb9be896018a42d5f32f#n3681)。每个`struct
    extent_buffer`包含一个`struct folio`数组，这些folios有一个私有引用指向`extent_buffer`。对于任何特定extent，应该始终只有一个`extent_buffer`。
- en: To reduce locking on the allocation path, the `extent_buffer` and the corresponding
    folios are allocated separately, linked together, and then inserted into a radix
    tree keyed by the start offset of the extent. Only the radix tree insertion needs
    locking. At the point of insertion, if it turns out someone else beat us to allocating
    the same extent, we instead take an extra reference to that `extent_buffer` and
    free the one we just allocated (and its pages).
  id: totrans-split-16
  prefs: []
  type: TYPE_NORMAL
  zh: 为了减少分配路径上的锁定，`extent_buffer`和相应的folios是分开分配的，然后链接在一起，然后插入到以extent起始偏移量为键的基数树中。只有在插入时，如果发现其他人已经比我们提前分配了相同的extent，我们才会额外引用该`extent_buffer`并释放我们刚刚分配的（及其页面）。
- en: This code path had undergone some churn recently, both [changing the race detection/handling
    strategy](https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/commit/?id=09e6cef19c9fc0e10547135476865b5272aa0406)
    and [converting from `struct page` to `struct folio`](https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/commit/?id=082d5bb9b336d533b7b968f4f8712e7755a9876a),
    so there was suspicion that a race or reference counting bug had crept in. But
    the extra debug logging didn't show it. It appeared that the `extent_buffer` and
    the `folio`s had all the right reference counts, but the memory itself had garbage
    rather than the appropriate on-disk contents. Btrfs does some sanity checks after
    reading these blocks in, and those checks were failing, causing the errors. But
    the on-disk blocks themselves were fine, and the next time we read them in, everything
    worked.
  id: totrans-split-17
  prefs: []
  type: TYPE_NORMAL
  zh: 这段代码路径最近经历了一些变动，包括[更改了竞争检测/处理策略](https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/commit/?id=09e6cef19c9fc0e10547135476865b5272aa0406)和[从`struct
    page`转换为`struct folio`](https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/commit/?id=082d5bb9b336d533b7b968f4f8712e7755a9876a)，因此有人怀疑可能潜伏了竞争或引用计数错误。但是额外的调试日志没有显示出问题。看起来`extent_buffer`和`folio`的引用计数都是正确的，但是内存本身却不是正确的磁盘内容，而是垃圾数据。Btrfs在读取这些块后会进行一些健全性检查，而这些检查失败了，导致了错误。但磁盘上的块本身是正确的，在下次读取时一切正常。
- en: One of the best bug hunting tools for the Linux kernel is [`git bisect`](https://git-scm.com/docs/git-bisect).
    Unfortunately, bisecting bugs like this is challenging. For intermittent issues,
    it's easy enough to know when to mark a commit bad, but harder to know if a commit
    is good—maybe the bug just hasn't triggered *yet*. You also need a known-good
    commit to start from, but that was hard to find. I thought at first the bug was
    new in Linux 6.7, but I could still reproduce it on 6.6 and 6.5. The Btrfs devs
    suggested I start from 6.0.
  id: totrans-split-18
  prefs: []
  type: TYPE_NORMAL
  zh: Linux内核中最好的bug调试工具之一是[`git bisect`](https://git-scm.com/docs/git-bisect)。不幸的是，像这样的二分法调试bug非常具有挑战性。对于间歇性问题，很容易知道何时标记一个提交为有问题，但更难确定一个提交是否好——也许bug只是还没有触发*而已*。你还需要一个已知好的提交来开始，但这很难找到。起初我以为bug是在Linux
    6.7中新出现的，但我仍然能在6.6和6.5上重现它。Btrfs的开发人员建议我从6.0开始。
- en: Bisecting can also be dangerous because you are running arbitrary old mid-development-cycle
    kernel commits that may have since had bugs fixed in the official releases. It's
    safer to do it from a virtual machine, but so far I had been unable to reproduce
    the bug on a VM. So started bisecting on my actual desktop.
  id: totrans-split-19
  prefs: []
  type: TYPE_NORMAL
  zh: 二分法调试也可能很危险，因为你会运行任意旧的中间开发周期内核提交，这些提交可能在官方发布版本中已经修复了bug。从虚拟机中进行会更安全，但到目前为止我无法在虚拟机上重现这个bug。因此我开始在我的实际桌面上进行二分法调试。
- en: Normally I would be far too scared to bisect a filesystem corruption bug on
    my actual computer that I use daily. But this seemed like it wasn't "real" corruption,
    plus I have backups, so I risked it. Unfortunately, after a couple rounds, I ran
    into [actual, on-disk filesystem corruption](https://lore.kernel.org/linux-btrfs/CABg4E-=u7m_g3HCFUYHS-+RC==pefkUZXiTT2Aor86jruHSF9Q@mail.gmail.com/).
    2,073,625 uncorrectable read errors is a new record for me, so I was too scared
    to continue the bisect. I restored what I could from backups and carried on with
    my life.
  id: totrans-split-20
  prefs: []
  type: TYPE_NORMAL
  zh: 通常情况下，我会因为害怕在我日常使用的实际计算机上对文件系统损坏 bug 进行二分查找而望而却步。但这似乎不像是“真正”的损坏，再加上我有备份，所以我冒了险。不幸的是，经过几轮尝试后，我遇到了[实际的磁盘文件系统损坏](https://lore.kernel.org/linux-btrfs/CABg4E-=u7m_g3HCFUYHS-+RC==pefkUZXiTT2Aor86jruHSF9Q@mail.gmail.com/)。2073625
    个不可纠正的读取错误对我来说是一个新记录，所以我太害怕继续进行二分查找了。我从备份中尽可能地恢复了数据，然后继续了我的生活。
- en: A few weeks later, Btrfs developer Qu Wenruo got back to me with some [extra
    debugging patches](https://lore.kernel.org/linux-btrfs/c7241ea4-fcc6-48d2-98c8-b5ea790d6c89@gmx.com/)
    and I decided to give it another shot. I ran his patch on bare metal, but wasn't
    able to reproduce the bug with the debugging patch applied. Such is life when
    debugging race conditions!
  id: totrans-split-21
  prefs: []
  type: TYPE_NORMAL
  zh: 几周后，Btrfs 开发者曲文若通过一些[额外的调试补丁](https://lore.kernel.org/linux-btrfs/c7241ea4-fcc6-48d2-98c8-b5ea790d6c89@gmx.com/)回复了我，我决定再试一次。我在裸机上运行了他的补丁，但是在应用调试补丁后无法重现这个
    bug。调试竞争条件时就是这样的生活！
- en: 'I also tried again to reproduce the issue in a VM. I configured it to more
    closely match my actual system configuration: multiple disks with [dm-crypt](https://wiki.archlinux.org/title/dm-crypt/Device_encryption)
    full disk encryption, joined together in a Btrfs RAID 0 array. And I made them
    emulated NVME drives instead of VirtIO. That did the trick: I could now reproduce
    the issue in a VM, though it took around 30 runs of my reproducer instead of the
    ~10 it took in real life. But this time I could still reproduce it with the debugging
    patch applied.'
  id: totrans-split-22
  prefs: []
  type: TYPE_NORMAL
  zh: 我还尝试在虚拟机中再次复现这个问题。我将其配置得更接近我的实际系统配置：多个带有[dm-crypt](https://wiki.archlinux.org/title/dm-crypt/Device_encryption)全磁盘加密的磁盘，在
    Btrfs RAID 0 阵列中合并在一起。我将它们模拟成 NVME 驱动而不是 VirtIO。这样做起作用了：我现在可以在虚拟机中复现这个问题，尽管需要大约
    30 次运行我的重现器，而不是实际生活中的大约 10 次。但这次即使在应用了调试补丁后我仍然可以复现这个问题。
- en: 'After a couple runs and tweaking the trace messages to make cross-referencing
    easier, I noticed something odd. Looking at just the lines from the trace corresponding
    to the "corrupted" error message, I saw this:'
  id: totrans-split-23
  prefs: []
  type: TYPE_NORMAL
  zh: 经过几次运行和调整跟踪消息以便于交叉引用，我注意到了一些奇怪的地方。仅查看与“损坏”错误消息对应的跟踪行时，我看到了这个：
- en: '[PRE2]'
  id: totrans-split-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: The only relevant operations seem to be allocating this `extent_buffer`, and
    then reading it ... twice. Reading it once ought to be enough, I thought, so I
    looked more closely at the code that actually reads them in and managed to eyeball
    the bug.
  id: totrans-split-25
  prefs: []
  type: TYPE_NORMAL
  zh: 唯一相关的操作似乎是分配这个`extent_buffer`，然后读取它……两次。我认为读取一次应该足够了，所以我更仔细地查看了实际读取它们的代码，并设法发现了这个错误。
- en: 'Just like how multiple threads might try to allocate the same `extent_buffer`
    at the same time, they might also try to read it at the same time. To ensure the
    read only happens once, a custom locking protocol is used:'
  id: totrans-split-26
  prefs: []
  type: TYPE_NORMAL
  zh: 就像多个线程可能同时尝试分配同一个`extent_buffer`一样，它们也可能同时尝试读取它。为了确保读取仅发生一次，使用了自定义的锁定协议：
- en: '[PRE3]'
  id: totrans-split-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'The logic is intended to work like this:'
  id: totrans-split-28
  prefs: []
  type: TYPE_NORMAL
  zh: 逻辑设计的目的是这样的：
- en: '[PRE4]'
  id: totrans-split-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '[PRE5]'
  id: totrans-split-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Unfortunately, this locking protocol has a bug:'
  id: totrans-split-31
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，这个锁定协议存在一个错误：
- en: '[PRE6]'
  id: totrans-split-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '[PRE7]'
  id: totrans-split-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '[PRE8]'
  id: totrans-split-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: If the first thread starts *and finishes* the I/O between the second thread
    checking the `UPTODATE` bit and the `READING` bit, the second thread will start
    a totally unnecessary read even though we already read in the extent. Worse, because
    the `UPTODATE` bit is already set, the third will return without waiting for the
    read to complete. That thread will then think it's safe to access the `extent_buffer`'s
    pages, despite them currently being under I/O, leading to a data race!
  id: totrans-split-35
  prefs: []
  type: TYPE_NORMAL
  zh: 如果第一个线程在第二个线程检查`UPTODATE`位和`READING`位之间启动并完成 I/O，第二个线程将开始完全不必要的读取，即使我们已经读取了
    extent。更糟糕的是，由于`UPTODATE`位已经设置，第三个线程将在等待读取完成之前返回。然后该线程会认为可以安全地访问`extent_buffer`的页面，尽管它们目前正在进行
    I/O，从而导致数据竞争！
- en: 'The fix is to use [double-checked locking](https://en.wikipedia.org/wiki/Double-checked_locking):
    after setting the `READING` bit, check `UPTODATE` again so we don''t kick off
    more useless (actively harmful, even) I/O.'
  id: totrans-split-36
  prefs: []
  type: TYPE_NORMAL
  zh: 解决方法是使用[双重检查锁定](https://en.wikipedia.org/wiki/Double-checked_locking)：在设置了`READING`位后，再次检查`UPTODATE`，以防止我们启动更多无用（甚至有害）的
    I/O。
- en: '[PRE9]'
  id: totrans-split-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '[PRE10]'
  id: totrans-split-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '[PRE11]'
  id: totrans-split-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: I submitted [a patch](https://lore.kernel.org/linux-btrfs/1ca6e688950ee82b1526bb3098852af99b75e6ba.1710551459.git.tavianator@tavianator.com/)
    that adds this missing check, as well as a couple [cleanup patches](https://lore.kernel.org/linux-btrfs/cover.1710769876.git.tavianator@tavianator.com/T/).
    With that fix, I can run the reproducer overnight without triggering any more
    errors.
  id: totrans-split-40
  prefs: []
  type: TYPE_NORMAL
  zh: 我提交了[一个补丁](https://lore.kernel.org/linux-btrfs/1ca6e688950ee82b1526bb3098852af99b75e6ba.1710551459.git.tavianator@tavianator.com/)来添加这个缺失的检查，以及几个[清理补丁](https://lore.kernel.org/linux-btrfs/cover.1710769876.git.tavianator@tavianator.com/T/)。通过这个修复，我可以在过夜运行重现器时不再触发任何错误。
- en: You might be surprised that this race was not benign. After all, we're reading
    some disk blocks into memory that already holds the contents of those same disk
    blocks. It's still technically a data race to read and write memory at the same
    time, even if the writes don't modify the memory, but usually it would be unobservable
    in practice.
  id: totrans-split-41
  prefs: []
  type: TYPE_NORMAL
  zh: 也许你会感到惊讶，这种竞争并不无害。毕竟，我们正在将某些磁盘块读入内存，而这些内存已经包含了同一磁盘块的内容。从技术上讲，同时读取和写入内存仍然属于数据竞争，即使写入不修改内存，但通常在实践中是不可观察的。
- en: '[PRE12]'
  id: totrans-split-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '[PRE13]'
  id: totrans-split-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '[PRE14]'
  id: totrans-split-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'The reason I could observe it was already mentioned above: full disk encryption.
    When dm-crypt processes a disk read, it reads the *encrypted* contents into memory
    and then decrypts them in-place.'
  id: totrans-split-45
  prefs: []
  type: TYPE_NORMAL
  zh: 我能够观察到的原因已经在上面提到过：全盘加密。当 dm-crypt 处理磁盘读取时，它会将*加密*内容读入内存，然后就地解密。
- en: '[PRE15]'
  id: totrans-split-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: '[PRE16]'
  id: totrans-split-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: '[PRE17]'
  id: totrans-split-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: '[PRE18]'
  id: totrans-split-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: '[PRE19]'
  id: totrans-split-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'It''s obvious why this causes problems: sometimes the racing thread will see
    valid metadata, but sometimes it will see encrypted bytes that appear totally
    random. I even captured a trace where the error messages show the extent being
    gradually filled in:'
  id: totrans-split-51
  prefs: []
  type: TYPE_NORMAL
  zh: 很明显，这会导致问题：有时竞争线程会看到有效的元数据，但有时会看到看似完全随机的加密字节。我甚至捕获了一条跟踪，在该错误信息中显示范围逐渐填充：
- en: '[PRE20]'
  id: totrans-split-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: There you can see the *same block* switch from an internal node to a leaf, and
    the block number change from a random number to the start of the extent, over
    about 0.02 seconds.
  id: totrans-split-53
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里你可以看到*同一个块*从内部节点切换到叶子节点，并且块号在大约0.02秒内从一个随机数变为范围的开始。
- en: </main>
  id: totrans-split-54
  prefs: []
  type: TYPE_NORMAL
  zh: </main>
