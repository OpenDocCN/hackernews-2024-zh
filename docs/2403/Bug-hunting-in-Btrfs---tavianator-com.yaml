- en: <!--yml
  id: totrans-split-0
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  id: totrans-split-1
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-05-29 12:31:28'
  id: totrans-split-2
  prefs: []
  type: TYPE_NORMAL
- en: -->
  id: totrans-split-3
  prefs: []
  type: TYPE_NORMAL
- en: Bug hunting in Btrfs - tavianator.com
  id: totrans-split-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://tavianator.com/2024/btrfs_bug.html](https://tavianator.com/2024/btrfs_bug.html)
  id: totrans-split-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: <main>
  id: totrans-split-6
  prefs: []
  type: TYPE_NORMAL
- en: 'The other day I was implementing [multi-threaded `stat()` calls](/cgit/bfs.git/commit/?id=89ecb2a08467cd8aa6ba70f8519df494652cac96)
    in [`bfs`](../projects/bfs.html). When I ran some benchmarks, I saw something
    that made my heart skip a beat:'
  id: totrans-split-7
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  id: totrans-split-8
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: If you're not familiar, "structure needs cleaning" is the human-readable description
    of `EUCLEAN`, an `errno` value that usually indicates filesystem corruption. Fearing
    the worst, I checked `dmesg` and saw
  id: totrans-split-9
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  id: totrans-split-10
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: I immediately rebooted into a live environment and ran `btrfs check`, but to
    my surprise, there were no errors found. `btrfs scrub` also found no evidence
    of corruption. And in fact, the file from the error message was completely fine.
    Whatever the issue was, it seemed to have gone away on its own.
  id: totrans-split-11
  prefs: []
  type: TYPE_NORMAL
- en: 'I searched Google for that "corrupted node" message and found that it had happened
    to someone else recently too: [Linus Torvalds](https://lore.kernel.org/linux-btrfs/CAHk-=whNdMaN9ntZ47XRKP6DBes2E5w7fi-0U3H2+PS18p+Pzw@mail.gmail.com/),
    just after merging a Btrfs pull request. (This was not the first time Linus and
    I had hit the same bug. We both have the same CPU in our desktops. Last time it
    led to one of my [favourite bugfixes ever](https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/commit/?id=a51ab63b297ce9e26e3ffb9be896018a42d5f32f).)'
  id: totrans-split-12
  prefs: []
  type: TYPE_NORMAL
- en: But the difference between me and Linus was that I could (semi-)reliably reproduce
    the error. Not every time, but maybe 10% of the time I ran `bfs` with parallel
    `stat()` calls on a large directory tree, I would see that "structure needs cleaning"
    error. I reported my [findings](https://lore.kernel.org/linux-btrfs/20240206033807.15498-1-tavianator@tavianator.com/)
    to the Btrfs mailing list, and though it led to some discussion about potential
    causes, we didn't narrow down the root cause.
  id: totrans-split-13
  prefs: []
  type: TYPE_NORMAL
- en: Based on the logs, the Btrfs developers assumed the problem had to do allocation
    of [`struct extent_buffer`](https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/tree/fs/btrfs/extent_io.h?id=a51ab63b297ce9e26e3ffb9be896018a42d5f32f#n76)
    and their memory pages. I'm not a Btrfs internals expert by any means, but I understand
    that `extent_buffer`s are used for metadata I/O; that is, reading and writing
    [B-tree](https://en.wikipedia.org/wiki/B-tree) nodes. An `extent_buffer` is associated
    with an array of `struct folio`/`struct page` (depending on kernel version), and
    those pages hold the actual contents of the extents that are read or written.
    The abstraction helps with file system block sizes that are different from the
    system page size.
  id: totrans-split-14
  prefs: []
  type: TYPE_NORMAL
- en: Allocating an `extent_buffer` and its pages is [more complicated than you might
    expect](https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/tree/fs/btrfs/extent_io.c?id=a51ab63b297ce9e26e3ffb9be896018a42d5f32f#n3681).
    Each `struct extent_buffer` holds an array of `struct folio`, and those folios
    have a private reference that points back to the `extent_buffer`. There should
    only ever be one `extent_buffer` at a time for any particular extent.
  id: totrans-split-15
  prefs: []
  type: TYPE_NORMAL
- en: To reduce locking on the allocation path, the `extent_buffer` and the corresponding
    folios are allocated separately, linked together, and then inserted into a radix
    tree keyed by the start offset of the extent. Only the radix tree insertion needs
    locking. At the point of insertion, if it turns out someone else beat us to allocating
    the same extent, we instead take an extra reference to that `extent_buffer` and
    free the one we just allocated (and its pages).
  id: totrans-split-16
  prefs: []
  type: TYPE_NORMAL
- en: This code path had undergone some churn recently, both [changing the race detection/handling
    strategy](https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/commit/?id=09e6cef19c9fc0e10547135476865b5272aa0406)
    and [converting from `struct page` to `struct folio`](https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/commit/?id=082d5bb9b336d533b7b968f4f8712e7755a9876a),
    so there was suspicion that a race or reference counting bug had crept in. But
    the extra debug logging didn't show it. It appeared that the `extent_buffer` and
    the `folio`s had all the right reference counts, but the memory itself had garbage
    rather than the appropriate on-disk contents. Btrfs does some sanity checks after
    reading these blocks in, and those checks were failing, causing the errors. But
    the on-disk blocks themselves were fine, and the next time we read them in, everything
    worked.
  id: totrans-split-17
  prefs: []
  type: TYPE_NORMAL
- en: One of the best bug hunting tools for the Linux kernel is [`git bisect`](https://git-scm.com/docs/git-bisect).
    Unfortunately, bisecting bugs like this is challenging. For intermittent issues,
    it's easy enough to know when to mark a commit bad, but harder to know if a commit
    is good—maybe the bug just hasn't triggered *yet*. You also need a known-good
    commit to start from, but that was hard to find. I thought at first the bug was
    new in Linux 6.7, but I could still reproduce it on 6.6 and 6.5. The Btrfs devs
    suggested I start from 6.0.
  id: totrans-split-18
  prefs: []
  type: TYPE_NORMAL
- en: Bisecting can also be dangerous because you are running arbitrary old mid-development-cycle
    kernel commits that may have since had bugs fixed in the official releases. It's
    safer to do it from a virtual machine, but so far I had been unable to reproduce
    the bug on a VM. So started bisecting on my actual desktop.
  id: totrans-split-19
  prefs: []
  type: TYPE_NORMAL
- en: Normally I would be far too scared to bisect a filesystem corruption bug on
    my actual computer that I use daily. But this seemed like it wasn't "real" corruption,
    plus I have backups, so I risked it. Unfortunately, after a couple rounds, I ran
    into [actual, on-disk filesystem corruption](https://lore.kernel.org/linux-btrfs/CABg4E-=u7m_g3HCFUYHS-+RC==pefkUZXiTT2Aor86jruHSF9Q@mail.gmail.com/).
    2,073,625 uncorrectable read errors is a new record for me, so I was too scared
    to continue the bisect. I restored what I could from backups and carried on with
    my life.
  id: totrans-split-20
  prefs: []
  type: TYPE_NORMAL
  zh: 通常情况下，我会因为害怕在我日常使用的实际计算机上对文件系统损坏 bug 进行二分查找而望而却步。但这似乎不像是“真正”的损坏，再加上我有备份，所以我冒了险。不幸的是，经过几轮尝试后，我遇到了[实际的磁盘文件系统损坏](https://lore.kernel.org/linux-btrfs/CABg4E-=u7m_g3HCFUYHS-+RC==pefkUZXiTT2Aor86jruHSF9Q@mail.gmail.com/)。2073625
    个不可纠正的读取错误对我来说是一个新记录，所以我太害怕继续进行二分查找了。我从备份中尽可能地恢复了数据，然后继续了我的生活。
- en: A few weeks later, Btrfs developer Qu Wenruo got back to me with some [extra
    debugging patches](https://lore.kernel.org/linux-btrfs/c7241ea4-fcc6-48d2-98c8-b5ea790d6c89@gmx.com/)
    and I decided to give it another shot. I ran his patch on bare metal, but wasn't
    able to reproduce the bug with the debugging patch applied. Such is life when
    debugging race conditions!
  id: totrans-split-21
  prefs: []
  type: TYPE_NORMAL
  zh: 几周后，Btrfs 开发者曲文若通过一些[额外的调试补丁](https://lore.kernel.org/linux-btrfs/c7241ea4-fcc6-48d2-98c8-b5ea790d6c89@gmx.com/)回复了我，我决定再试一次。我在裸机上运行了他的补丁，但是在应用调试补丁后无法重现这个
    bug。调试竞争条件时就是这样的生活！
- en: 'I also tried again to reproduce the issue in a VM. I configured it to more
    closely match my actual system configuration: multiple disks with [dm-crypt](https://wiki.archlinux.org/title/dm-crypt/Device_encryption)
    full disk encryption, joined together in a Btrfs RAID 0 array. And I made them
    emulated NVME drives instead of VirtIO. That did the trick: I could now reproduce
    the issue in a VM, though it took around 30 runs of my reproducer instead of the
    ~10 it took in real life. But this time I could still reproduce it with the debugging
    patch applied.'
  id: totrans-split-22
  prefs: []
  type: TYPE_NORMAL
  zh: 我还尝试在虚拟机中再次复现这个问题。我将其配置得更接近我的实际系统配置：多个带有[dm-crypt](https://wiki.archlinux.org/title/dm-crypt/Device_encryption)全磁盘加密的磁盘，在
    Btrfs RAID 0 阵列中合并在一起。我将它们模拟成 NVME 驱动而不是 VirtIO。这样做起作用了：我现在可以在虚拟机中复现这个问题，尽管需要大约
    30 次运行我的重现器，而不是实际生活中的大约 10 次。但这次即使在应用了调试补丁后我仍然可以复现这个问题。
- en: 'After a couple runs and tweaking the trace messages to make cross-referencing
    easier, I noticed something odd. Looking at just the lines from the trace corresponding
    to the "corrupted" error message, I saw this:'
  id: totrans-split-23
  prefs: []
  type: TYPE_NORMAL
  zh: 经过几次运行和调整跟踪消息以便于交叉引用，我注意到了一些奇怪的地方。仅查看与“损坏”错误消息对应的跟踪行时，我看到了这个：
- en: '[PRE2]'
  id: totrans-split-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: The only relevant operations seem to be allocating this `extent_buffer`, and
    then reading it ... twice. Reading it once ought to be enough, I thought, so I
    looked more closely at the code that actually reads them in and managed to eyeball
    the bug.
  id: totrans-split-25
  prefs: []
  type: TYPE_NORMAL
  zh: 唯一相关的操作似乎是分配这个`extent_buffer`，然后读取它……两次。我认为读取一次应该足够了，所以我更仔细地查看了实际读取它们的代码，并设法发现了这个错误。
- en: 'Just like how multiple threads might try to allocate the same `extent_buffer`
    at the same time, they might also try to read it at the same time. To ensure the
    read only happens once, a custom locking protocol is used:'
  id: totrans-split-26
  prefs: []
  type: TYPE_NORMAL
  zh: 就像多个线程可能同时尝试分配同一个`extent_buffer`一样，它们也可能同时尝试读取它。为了确保读取仅发生一次，使用了自定义的锁定协议：
- en: '[PRE3]'
  id: totrans-split-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'The logic is intended to work like this:'
  id: totrans-split-28
  prefs: []
  type: TYPE_NORMAL
  zh: 逻辑设计的目的是这样的：
- en: '[PRE4]'
  id: totrans-split-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '[PRE5]'
  id: totrans-split-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Unfortunately, this locking protocol has a bug:'
  id: totrans-split-31
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，这个锁定协议存在一个错误：
- en: '[PRE6]'
  id: totrans-split-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '[PRE7]'
  id: totrans-split-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '[PRE8]'
  id: totrans-split-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: If the first thread starts *and finishes* the I/O between the second thread
    checking the `UPTODATE` bit and the `READING` bit, the second thread will start
    a totally unnecessary read even though we already read in the extent. Worse, because
    the `UPTODATE` bit is already set, the third will return without waiting for the
    read to complete. That thread will then think it's safe to access the `extent_buffer`'s
    pages, despite them currently being under I/O, leading to a data race!
  id: totrans-split-35
  prefs: []
  type: TYPE_NORMAL
  zh: 如果第一个线程在第二个线程检查`UPTODATE`位和`READING`位之间启动并完成 I/O，第二个线程将开始完全不必要的读取，即使我们已经读取了
    extent。更糟糕的是，由于`UPTODATE`位已经设置，第三个线程将在等待读取完成之前返回。然后该线程会认为可以安全地访问`extent_buffer`的页面，尽管它们目前正在进行
    I/O，从而导致数据竞争！
- en: 'The fix is to use [double-checked locking](https://en.wikipedia.org/wiki/Double-checked_locking):
    after setting the `READING` bit, check `UPTODATE` again so we don''t kick off
    more useless (actively harmful, even) I/O.'
  id: totrans-split-36
  prefs: []
  type: TYPE_NORMAL
  zh: 解决方法是使用[双重检查锁定](https://en.wikipedia.org/wiki/Double-checked_locking)：在设置了`READING`位后，再次检查`UPTODATE`，以防止我们启动更多无用（甚至有害）的
    I/O。
- en: '[PRE9]'
  id: totrans-split-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '[PRE10]'
  id: totrans-split-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '[PRE11]'
  id: totrans-split-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: I submitted [a patch](https://lore.kernel.org/linux-btrfs/1ca6e688950ee82b1526bb3098852af99b75e6ba.1710551459.git.tavianator@tavianator.com/)
    that adds this missing check, as well as a couple [cleanup patches](https://lore.kernel.org/linux-btrfs/cover.1710769876.git.tavianator@tavianator.com/T/).
    With that fix, I can run the reproducer overnight without triggering any more
    errors.
  id: totrans-split-40
  prefs: []
  type: TYPE_NORMAL
  zh: 我提交了[一个补丁](https://lore.kernel.org/linux-btrfs/1ca6e688950ee82b1526bb3098852af99b75e6ba.1710551459.git.tavianator@tavianator.com/)来添加这个缺失的检查，以及几个[清理补丁](https://lore.kernel.org/linux-btrfs/cover.1710769876.git.tavianator@tavianator.com/T/)。通过这个修复，我可以在过夜运行重现器时不再触发任何错误。
- en: You might be surprised that this race was not benign. After all, we're reading
    some disk blocks into memory that already holds the contents of those same disk
    blocks. It's still technically a data race to read and write memory at the same
    time, even if the writes don't modify the memory, but usually it would be unobservable
    in practice.
  id: totrans-split-41
  prefs: []
  type: TYPE_NORMAL
  zh: 也许你会感到惊讶，这种竞争并不无害。毕竟，我们正在将某些磁盘块读入内存，而这些内存已经包含了同一磁盘块的内容。从技术上讲，同时读取和写入内存仍然属于数据竞争，即使写入不修改内存，但通常在实践中是不可观察的。
- en: '[PRE12]'
  id: totrans-split-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '[PRE13]'
  id: totrans-split-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '[PRE14]'
  id: totrans-split-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'The reason I could observe it was already mentioned above: full disk encryption.
    When dm-crypt processes a disk read, it reads the *encrypted* contents into memory
    and then decrypts them in-place.'
  id: totrans-split-45
  prefs: []
  type: TYPE_NORMAL
  zh: 我能够观察到的原因已经在上面提到过：全盘加密。当 dm-crypt 处理磁盘读取时，它会将*加密*内容读入内存，然后就地解密。
- en: '[PRE15]'
  id: totrans-split-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: '[PRE16]'
  id: totrans-split-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: '[PRE17]'
  id: totrans-split-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: '[PRE18]'
  id: totrans-split-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: '[PRE19]'
  id: totrans-split-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'It''s obvious why this causes problems: sometimes the racing thread will see
    valid metadata, but sometimes it will see encrypted bytes that appear totally
    random. I even captured a trace where the error messages show the extent being
    gradually filled in:'
  id: totrans-split-51
  prefs: []
  type: TYPE_NORMAL
  zh: 很明显，这会导致问题：有时竞争线程会看到有效的元数据，但有时会看到看似完全随机的加密字节。我甚至捕获了一条跟踪，在该错误信息中显示范围逐渐填充：
- en: '[PRE20]'
  id: totrans-split-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: There you can see the *same block* switch from an internal node to a leaf, and
    the block number change from a random number to the start of the extent, over
    about 0.02 seconds.
  id: totrans-split-53
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里你可以看到*同一个块*从内部节点切换到叶子节点，并且块号在大约0.02秒内从一个随机数变为范围的开始。
- en: </main>
  id: totrans-split-54
  prefs: []
  type: TYPE_NORMAL
  zh: </main>
