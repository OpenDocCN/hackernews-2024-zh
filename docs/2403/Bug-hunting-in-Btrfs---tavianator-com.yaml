- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-05-29 12:31:28'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: Bug hunting in Btrfs - tavianator.com
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://tavianator.com/2024/btrfs_bug.html](https://tavianator.com/2024/btrfs_bug.html)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: <main>
  prefs: []
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The other day I was implementing [multi-threaded `stat()` calls](/cgit/bfs.git/commit/?id=89ecb2a08467cd8aa6ba70f8519df494652cac96)
    in [`bfs`](../projects/bfs.html). When I ran some benchmarks, I saw something
    that made my heart skip a beat:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: If you're not familiar, "structure needs cleaning" is the human-readable description
    of `EUCLEAN`, an `errno` value that usually indicates filesystem corruption. Fearing
    the worst, I checked `dmesg` and saw
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: I immediately rebooted into a live environment and ran `btrfs check`, but to
    my surprise, there were no errors found. `btrfs scrub` also found no evidence
    of corruption. And in fact, the file from the error message was completely fine.
    Whatever the issue was, it seemed to have gone away on its own.
  prefs: []
  type: TYPE_NORMAL
- en: 'I searched Google for that "corrupted node" message and found that it had happened
    to someone else recently too: [Linus Torvalds](https://lore.kernel.org/linux-btrfs/CAHk-=whNdMaN9ntZ47XRKP6DBes2E5w7fi-0U3H2+PS18p+Pzw@mail.gmail.com/),
    just after merging a Btrfs pull request. (This was not the first time Linus and
    I had hit the same bug. We both have the same CPU in our desktops. Last time it
    led to one of my [favourite bugfixes ever](https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/commit/?id=a51ab63b297ce9e26e3ffb9be896018a42d5f32f).)'
  prefs: []
  type: TYPE_NORMAL
- en: But the difference between me and Linus was that I could (semi-)reliably reproduce
    the error. Not every time, but maybe 10% of the time I ran `bfs` with parallel
    `stat()` calls on a large directory tree, I would see that "structure needs cleaning"
    error. I reported my [findings](https://lore.kernel.org/linux-btrfs/20240206033807.15498-1-tavianator@tavianator.com/)
    to the Btrfs mailing list, and though it led to some discussion about potential
    causes, we didn't narrow down the root cause.
  prefs: []
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Based on the logs, the Btrfs developers assumed the problem had to do allocation
    of [`struct extent_buffer`](https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/tree/fs/btrfs/extent_io.h?id=a51ab63b297ce9e26e3ffb9be896018a42d5f32f#n76)
    and their memory pages. I'm not a Btrfs internals expert by any means, but I understand
    that `extent_buffer`s are used for metadata I/O; that is, reading and writing
    [B-tree](https://en.wikipedia.org/wiki/B-tree) nodes. An `extent_buffer` is associated
    with an array of `struct folio`/`struct page` (depending on kernel version), and
    those pages hold the actual contents of the extents that are read or written.
    The abstraction helps with file system block sizes that are different from the
    system page size.
  prefs: []
  type: TYPE_NORMAL
- en: Allocating an `extent_buffer` and its pages is [more complicated than you might
    expect](https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/tree/fs/btrfs/extent_io.c?id=a51ab63b297ce9e26e3ffb9be896018a42d5f32f#n3681).
    Each `struct extent_buffer` holds an array of `struct folio`, and those folios
    have a private reference that points back to the `extent_buffer`. There should
    only ever be one `extent_buffer` at a time for any particular extent.
  prefs: []
  type: TYPE_NORMAL
- en: To reduce locking on the allocation path, the `extent_buffer` and the corresponding
    folios are allocated separately, linked together, and then inserted into a radix
    tree keyed by the start offset of the extent. Only the radix tree insertion needs
    locking. At the point of insertion, if it turns out someone else beat us to allocating
    the same extent, we instead take an extra reference to that `extent_buffer` and
    free the one we just allocated (and its pages).
  prefs: []
  type: TYPE_NORMAL
- en: This code path had undergone some churn recently, both [changing the race detection/handling
    strategy](https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/commit/?id=09e6cef19c9fc0e10547135476865b5272aa0406)
    and [converting from `struct page` to `struct folio`](https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/commit/?id=082d5bb9b336d533b7b968f4f8712e7755a9876a),
    so there was suspicion that a race or reference counting bug had crept in. But
    the extra debug logging didn't show it. It appeared that the `extent_buffer` and
    the `folio`s had all the right reference counts, but the memory itself had garbage
    rather than the appropriate on-disk contents. Btrfs does some sanity checks after
    reading these blocks in, and those checks were failing, causing the errors. But
    the on-disk blocks themselves were fine, and the next time we read them in, everything
    worked.
  prefs: []
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: One of the best bug hunting tools for the Linux kernel is [`git bisect`](https://git-scm.com/docs/git-bisect).
    Unfortunately, bisecting bugs like this is challenging. For intermittent issues,
    it's easy enough to know when to mark a commit bad, but harder to know if a commit
    is good—maybe the bug just hasn't triggered *yet*. You also need a known-good
    commit to start from, but that was hard to find. I thought at first the bug was
    new in Linux 6.7, but I could still reproduce it on 6.6 and 6.5. The Btrfs devs
    suggested I start from 6.0.
  prefs: []
  type: TYPE_NORMAL
- en: Bisecting can also be dangerous because you are running arbitrary old mid-development-cycle
    kernel commits that may have since had bugs fixed in the official releases. It's
    safer to do it from a virtual machine, but so far I had been unable to reproduce
    the bug on a VM. So started bisecting on my actual desktop.
  prefs: []
  type: TYPE_NORMAL
- en: Normally I would be far too scared to bisect a filesystem corruption bug on
    my actual computer that I use daily. But this seemed like it wasn't "real" corruption,
    plus I have backups, so I risked it. Unfortunately, after a couple rounds, I ran
    into [actual, on-disk filesystem corruption](https://lore.kernel.org/linux-btrfs/CABg4E-=u7m_g3HCFUYHS-+RC==pefkUZXiTT2Aor86jruHSF9Q@mail.gmail.com/).
    2,073,625 uncorrectable read errors is a new record for me, so I was too scared
    to continue the bisect. I restored what I could from backups and carried on with
    my life.
  prefs: []
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A few weeks later, Btrfs developer Qu Wenruo got back to me with some [extra
    debugging patches](https://lore.kernel.org/linux-btrfs/c7241ea4-fcc6-48d2-98c8-b5ea790d6c89@gmx.com/)
    and I decided to give it another shot. I ran his patch on bare metal, but wasn't
    able to reproduce the bug with the debugging patch applied. Such is life when
    debugging race conditions!
  prefs: []
  type: TYPE_NORMAL
- en: 'I also tried again to reproduce the issue in a VM. I configured it to more
    closely match my actual system configuration: multiple disks with [dm-crypt](https://wiki.archlinux.org/title/dm-crypt/Device_encryption)
    full disk encryption, joined together in a Btrfs RAID 0 array. And I made them
    emulated NVME drives instead of VirtIO. That did the trick: I could now reproduce
    the issue in a VM, though it took around 30 runs of my reproducer instead of the
    ~10 it took in real life. But this time I could still reproduce it with the debugging
    patch applied.'
  prefs: []
  type: TYPE_NORMAL
- en: 'After a couple runs and tweaking the trace messages to make cross-referencing
    easier, I noticed something odd. Looking at just the lines from the trace corresponding
    to the "corrupted" error message, I saw this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: The only relevant operations seem to be allocating this `extent_buffer`, and
    then reading it ... twice. Reading it once ought to be enough, I thought, so I
    looked more closely at the code that actually reads them in and managed to eyeball
    the bug.
  prefs: []
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Just like how multiple threads might try to allocate the same `extent_buffer`
    at the same time, they might also try to read it at the same time. To ensure the
    read only happens once, a custom locking protocol is used:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'The logic is intended to work like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Unfortunately, this locking protocol has a bug:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: If the first thread starts *and finishes* the I/O between the second thread
    checking the `UPTODATE` bit and the `READING` bit, the second thread will start
    a totally unnecessary read even though we already read in the extent. Worse, because
    the `UPTODATE` bit is already set, the third will return without waiting for the
    read to complete. That thread will then think it's safe to access the `extent_buffer`'s
    pages, despite them currently being under I/O, leading to a data race!
  prefs: []
  type: TYPE_NORMAL
- en: 'The fix is to use [double-checked locking](https://en.wikipedia.org/wiki/Double-checked_locking):
    after setting the `READING` bit, check `UPTODATE` again so we don''t kick off
    more useless (actively harmful, even) I/O.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: I submitted [a patch](https://lore.kernel.org/linux-btrfs/1ca6e688950ee82b1526bb3098852af99b75e6ba.1710551459.git.tavianator@tavianator.com/)
    that adds this missing check, as well as a couple [cleanup patches](https://lore.kernel.org/linux-btrfs/cover.1710769876.git.tavianator@tavianator.com/T/).
    With that fix, I can run the reproducer overnight without triggering any more
    errors.
  prefs: []
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You might be surprised that this race was not benign. After all, we're reading
    some disk blocks into memory that already holds the contents of those same disk
    blocks. It's still technically a data race to read and write memory at the same
    time, even if the writes don't modify the memory, but usually it would be unobservable
    in practice.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'The reason I could observe it was already mentioned above: full disk encryption.
    When dm-crypt processes a disk read, it reads the *encrypted* contents into memory
    and then decrypts them in-place.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'It''s obvious why this causes problems: sometimes the racing thread will see
    valid metadata, but sometimes it will see encrypted bytes that appear totally
    random. I even captured a trace where the error messages show the extent being
    gradually filled in:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: There you can see the *same block* switch from an internal node to a leaf, and
    the block number change from a random number to the start of the extent, over
    about 0.02 seconds.
  prefs: []
  type: TYPE_NORMAL
- en: </main>
  prefs: []
  type: TYPE_NORMAL
