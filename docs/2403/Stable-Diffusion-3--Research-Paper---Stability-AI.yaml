- en: <!--yml
  id: totrans-split-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-split-1
  prefs: []
  type: TYPE_NORMAL
  zh: 'category: 未分类'
- en: 'date: 2024-05-27 14:38:37'
  id: totrans-split-2
  prefs: []
  type: TYPE_NORMAL
  zh: 'date: 2024-05-27 14:38:37'
- en: -->
  id: totrans-split-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: 'Stable Diffusion 3: Research Paper — Stability AI'
  id: totrans-split-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 稳定扩散3：研究论文 — Stability AI
- en: 来源：[https://stability.ai/news/stable-diffusion-3-research-paper](https://stability.ai/news/stable-diffusion-3-research-paper)
  id: totrans-split-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://stability.ai/news/stable-diffusion-3-research-paper](https://stability.ai/news/stable-diffusion-3-research-paper)
- en: We have compared output images from Stable Diffusion 3 with various other open
    models including [SDXL](https://stability.ai/news/stable-diffusion-sdxl-1-announcement),
    [SDXL Turbo](https://stability.ai/news/stability-ai-sdxl-turbo), [Stable Cascade](https://stability.ai/news/introducing-stable-cascade),
    Playground v2.5 and Pixart-α as well as closed-source systems such as DALL·E 3,
    Midjourney v6 and Ideogram v1 to evaluate performance based on human feedback.
    During these tests, human evaluators were provided with example outputs from each
    model and asked to select the best results based on how closely the model outputs
    follow the context of the prompt it was given (“prompt following”), how well text
    was rendered based on the prompt (“typography”) and, which image is of higher
    aesthetic quality (“visual aesthetics”).
  id: totrans-split-6
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经将稳定扩散3的输出图像与其他各种开放模型（包括[SDXL](https://stability.ai/news/stable-diffusion-sdxl-1-announcement)，[SDXL
    Turbo](https://stability.ai/news/stability-ai-sdxl-turbo)，[稳定级联](https://stability.ai/news/introducing-stable-cascade)，Playground
    v2.5和Pixart-α）以及封闭源系统（如DALL·E 3，Midjourney v6和Ideogram v1）进行比较，以人类反馈评估性能。在这些测试中，人类评估员得到了来自每个模型的示例输出，并被要求根据模型输出如何紧随所给的背景（“prompt
    following”）、根据提示如何呈现文本（“typography”）以及哪张图像具有更高美学质量（“visual aesthetics”）来选择最佳结果。
- en: From the results of our testing, we have found that Stable Diffusion 3 is equal
    to or outperforms current state-of-the-art text-to-image generation systems in
    all of the above areas.
  id: totrans-split-7
  prefs: []
  type: TYPE_NORMAL
  zh: 从我们的测试结果来看，我们发现稳定扩散3在上述所有领域中要么与当前最先进的文本到图像生成系统相当，要么表现更好。
- en: In early, unoptimized inference tests on consumer hardware our largest SD3 model
    with 8B parameters fits into the 24GB VRAM of a RTX 4090 and takes 34 seconds
    to generate an image of resolution 1024x1024 when using 50 sampling steps. Additionally,
    there will be multiple variations of Stable Diffusion 3 during the initial release,
    ranging from 800m to 8B parameter models to further eliminate hardware barriers.
  id: totrans-split-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在消费者硬件上早期的、未经优化的推理测试中，我们最大的SD3模型具有8B参数，在RTX 4090的24GB VRAM中运行，使用50个采样步骤时，生成分辨率为1024x1024的图像需时34秒。此外，初始发布期间将会有多个稳定扩散3的变体，从8亿到80亿参数的模型，以进一步消除硬件限制。
