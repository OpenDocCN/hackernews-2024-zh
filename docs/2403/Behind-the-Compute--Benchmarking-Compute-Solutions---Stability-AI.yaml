- en: <!--yml
  id: totrans-split-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-split-1
  prefs: []
  type: TYPE_NORMAL
  zh: 'category: 未分类'
- en: 'date: 2024-05-27 14:50:01'
  id: totrans-split-2
  prefs: []
  type: TYPE_NORMAL
  zh: 'date: 2024-05-27 14:50:01'
- en: -->
  id: totrans-split-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: 'Behind the Compute: Benchmarking Compute Solutions — Stability AI'
  id: totrans-split-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 'Behind the Compute: Benchmarking Compute Solutions — Stability AI'
- en: 来源：[https://stability.ai/news/putting-the-ai-supercomputer-to-work](https://stability.ai/news/putting-the-ai-supercomputer-to-work)
  id: totrans-split-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://stability.ai/news/putting-the-ai-supercomputer-to-work](https://stability.ai/news/putting-the-ai-supercomputer-to-work)
- en: In this configuration, the Gaudi 2 cluster processed over 3x more images per
    second, compared to A100-80GB GPUs. This is particularly impressive considering
    that the A100s have a very optimized software stack.
  id: totrans-split-6
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种配置下，与A100-80GB GPU相比，Gaudi 2集群每秒处理的图像数量提高了3倍以上。考虑到A100具有非常优化的软件堆栈，这一点尤为令人印象深刻。
- en: On inference tests with the Stable Diffusion 3 8B parameter model the Gaudi
    2 chips offer inference speed similar to Nvidia A100 chips using base PyTorch.
    However, with TensorRT optimization, the A100 chips produce images 40% faster
    than Gaudi 2\. We anticipate that with further optimization, Gaudi 2 will soon
    outperform A100s on this model. In earlier tests on our SDXL model with base PyTorch,
    Gaudi 2 generates a 1024x1024 image in 30 steps in 3.2 seconds, versus 3.6 seconds
    for PyTorch on A100s and 2.7 seconds for a generation with TensorRT on an A100.
  id: totrans-split-7
  prefs: []
  type: TYPE_NORMAL
  zh: 在稳定扩散3 8B参数模型的推断测试中，Gaudi 2芯片使用基础PyTorch提供了与Nvidia A100芯片类似的推断速度。然而，通过TensorRT优化，A100芯片的图像生成速度比Gaudi
    2快40%。我们预计随着进一步的优化，Gaudi 2将很快在这个模型上超越A100。在早期对我们的SDXL模型进行的基于基础PyTorch的测试中，Gaudi
    2在30步中以3.2秒生成1024x1024图像，而A100上的PyTorch为3.6秒，A100上的TensorRT为2.7秒。
- en: The higher memory and fast interconnect of Gaudi 2, plus other design considerations,
    make it competitive to run the Diffusion Transformer architecture that underpins
    this next generation of media models.
  id: totrans-split-8
  prefs: []
  type: TYPE_NORMAL
  zh: Gaudi 2的更高内存和快速互连，加上其他设计考虑因素，使其在支持下一代媒体模型的扩散变压器架构方面具有竞争力。
- en: '**Model 2:**'
  id: totrans-split-9
  prefs: []
  type: TYPE_NORMAL
  zh: '**模型2：**'
- en: '[**Stable Beluga 2.5 70B**](https://stability.ai/news/stable-beluga-large-instruction-fine-tuned-models)is
    our fine-tuned version of LLaMA 2 70B, building on the [Stable Beluga 2](https://stability.ai/news/stable-beluga-large-instruction-fine-tuned-models)
    model which was the first open model to best ChatGPT 3.5 in select benchmarks.
    We ran this training benchmark on 256 Gaudi 2 accelerators. Running our PyTorch
    code out of the box, with no extra optimizations, we measured an impressive total
    average throughput of 116,777 tokens/second. More specifically, this involves
    using a FP16 datatype, a global batch size of 1024, gradient accumulation steps
    of 2, and micro batch size of 2.'
  id: totrans-split-10
  prefs: []
  type: TYPE_NORMAL
  zh: '[**稳定Beluga 2.5 70B**](https://stability.ai/news/stable-beluga-large-instruction-fine-tuned-models)是我们对LLaMA
    2 70B的优化版本，建立在第一个在特定基准中超越ChatGPT 3.5的开放模型[**稳定Beluga 2**](https://stability.ai/news/stable-beluga-large-instruction-fine-tuned-models)基础上。我们在256个Gaudi
    2加速器上运行了这个训练基准。我们使用PyTorch代码直接运行，没有额外的优化，测得了令人印象深刻的平均总吞吐量为116,777 tokens/second。具体来说，这包括使用FP16数据类型，全局批大小为1024，梯度累积步骤为2，微批大小为2。'
- en: On inference tests with our 70B language model on Gaudi 2, it generates 673
    tokens/second per accelerator, using an input token size of 128 and output token
    size of 2048\. In comparison to [TensorRT-LLM](https://nvidia.github.io/TensorRT-LLM/performance.html),
    Gaudi 2 appears to be 28% faster than the 525 tokens/second for the A100\. We
    also anticipate further speed improvements with FP8.
  id: totrans-split-11
  prefs: []
  type: TYPE_NORMAL
  zh: On inference tests with our 70B language model on Gaudi 2, it generates 673
    tokens/second per accelerator, using an input token size of 128 and output token
    size of 2048\. In comparison to [TensorRT-LLM](https://nvidia.github.io/TensorRT-LLM/performance.html),
    Gaudi 2 appears to be 28% faster than the 525 tokens/second for the A100\. We
    also anticipate further speed improvements with FP8.
- en: Companies like ours face an increasing demand for more powerful and efficient
    computing solutions. Our findings underscore the need for alternatives like the
    Gaudi 2, which not only offers superior performance to other 7nm chips, but also
    addresses critical market needs such as affordability, reduced lead times, and
    superior price-to-performance ratios. Ultimately, the opportunity for choice in
    computing options broadens participation and innovation, thereby making advanced
    AI technologies more accessible to all.
  id: totrans-split-12
  prefs: []
  type: TYPE_NORMAL
  zh: 像我们这样的公司面临着对更强大和高效的计算解决方案日益增长的需求。我们的研究结果强调了像Gaudi 2这样的替代方案的必要性，它不仅提供比其他7nm芯片更优越的性能，还解决了关键市场需求，如可负担性、缩短交货时间以及卓越的性价比。最终，计算选项的多样性扩大了参与和创新的机会，从而使先进的AI技术更加普及。
- en: Stay tuned for more insights in our next installment of "Behind the Compute."
  id: totrans-split-13
  prefs: []
  type: TYPE_NORMAL
  zh: Stay tuned for more insights in our next installment of "Behind the Compute."
- en: To stay updated on our progress follow us on [Twitter](https://twitter.com/stabilityai),
    [Instagram](https://www.instagram.com/stability.ai/), [LinkedIn](https://www.linkedin.com/company/stability-ai),
    and join our [Discord Community](https://discord.gg/stablediffusion).
  id: totrans-split-14
  prefs: []
  type: TYPE_NORMAL
  zh: 要了解我们的进展，请关注我们的[Twitter](https://twitter.com/stabilityai)，[Instagram](https://www.instagram.com/stability.ai/)，[LinkedIn](https://www.linkedin.com/company/stability-ai)，并加入我们的[Discord社区](https://discord.gg/stablediffusion)。
