<!--yml
category: 未分类
date: 2024-05-27 15:00:48
-->

# GitHub - tspeterkim/flash-attention-minimal: Flash Attention in ~100 lines of CUDA (forward pass only)

> 来源：[https://github.com/tspeterkim/flash-attention-minimal](https://github.com/tspeterkim/flash-attention-minimal)

<include-fragment class="js-notification-shelf-include-fragment" data-base-src="https://github.com/notifications/beta/shelf"></include-fragment>

<main id="js-repo-pjax-container"> <turbo-frame id="repo-content-turbo-frame" target="_top" data-turbo-action="advance" class=""># tspeterkim/flash-attention-minimal

This commit does not belong to any branch on this repository, and may belong to a fork outside of the repository.

<include-fragment src="/tspeterkim/flash-attention-minimal/spoofed_commit_check/1570487ff64ff77b1416fa1b193e5bf2714caef1" data-test-selector="spoofed-commit-check"></include-fragment></turbo-frame></main>

 <ghcc-consent id="ghcc" class="position-fixed bottom-0 left-0" data-initial-cookie-consent-allowed="" data-cookie-consent-required="false">You can’t perform that action at this time.

<template id="site-details-dialog"></template><template id="snippet-clipboard-copy-button"></template><template id="snippet-clipboard-copy-button-unpositioned"></template></ghcc-consent>