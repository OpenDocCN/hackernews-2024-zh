<!--yml
category: 未分类
date: 2024-05-29 12:29:45
-->

# Apt archive mirrors in Git-LFS – Simon Josefsson's blog

> 来源：[https://blog.josefsson.org/2024/03/18/apt-archive-mirrors-in-git-lfs/](https://blog.josefsson.org/2024/03/18/apt-archive-mirrors-in-git-lfs/)

My effort to improve transparency and confidence of public apt archives continues. I started to work on this in “[Apt Archive Transparency](https://blog.josefsson.org/2023/02/01/apt-archive-transparency-debdistdiff-apt-canary/)” in which I mention the [debdistget](https://gitlab.com/debdistutils/debdistget/) project in passing. **Debdistget** is responsible for mirroring index files for some public apt archives. I’ve realized that having a publicly auditable and preserved mirror of the apt repositories is central to being able to do apt transparency work, so the debdistget project has become more central to my project than I thought. Currently I track [Trisquel](https://trisquel.info/), [PureOS](https://pureos.net/), [Gnuinos](https://www.gnuinos.org/) and their upstreams [Ubuntu](https://ubuntu.com/), [Debian](https://www.debian.org/) and [Devuan](https://www.devuan.org/).

Debdistget download **Release/Package/Sources** files and store them in a git repository published on [GitLab](https://about.gitlab.com/). Due to size constraints, it uses two repositories: one for the **Release/InRelease** files (which are small) and one that also include the **Package/Sources** files (which are large). See for example the repository for [Trisquel release files](https://gitlab.com/debdistutils/archives/trisquel/releases) and the [Trisquel package/sources files](https://gitlab.com/debdistutils/archives/trisquel/packages). Repositories for all distributions can be found in [debdistutils’ archives GitLab sub-group](https://gitlab.com/debdistutils/archives).

The reason for splitting into two repositories was that the git repository for the combined files become large, and that some of my use-cases only needed the release files. Currently the repositories with packages (which contain a couple of months worth of data now) are 9GB for [Ubuntu](https://gitlab.com/debdistutils/archives/ubuntu/packages), 2.5GB for [Trisquel](https://gitlab.com/debdistutils/archives/trisquel/packages)/[Debian](https://gitlab.com/debdistutils/archives/debian/packages)/[PureOS](https://gitlab.com/debdistutils/archives/pureos/packages), 970MB for [Devuan](https://gitlab.com/debdistutils/archives/devuan/packages) and 450MB for [Gnuinos](https://gitlab.com/debdistutils/archives/gnuinos/packages). The repository size is correlated to the size of the archive (for the initial import) plus the frequency and size of updates. Ubuntu’s use of [Apt Phased Updates](https://wiki.ubuntu.com/PhasedUpdates) (which triggers a higher churn of Packages file modifications) appears to be the primary reason for its larger size.

Working with large Git repositories is inefficient and the GitLab CI/CD jobs generate quite some network traffic downloading the git repository over and over again. The most heavy user is the [debdistdiff](https://gitlab.com/debdistutils/debdistdiff) project that download all distribution package repositories to do diff operations on the package lists between distributions. The daily job takes around **80 minutes** to run, with the majority of time is spent on downloading the archives. Yes I know I could look into runner-side caching but I dislike complexity caused by caching.

Fortunately not all use-cases requires the package files. The [debdistcanary](https://gitlab.com/debdistutils/debdistcanary) project only needs the **Release/InRelease** files, in order to commit signatures to the [Sigstore](https://docs.sigstore.dev/) and [Sigsum](https://www.sigsum.org/) transparency logs. These jobs still run fairly quickly, but watching the repository size growth worries me. Currently these repositories are at [Debian](https://gitlab.com/debdistutils/canary/debian) 440MB, [PureOS](https://gitlab.com/debdistutils/canary/pureos) 130MB, [Ubuntu](https://gitlab.com/debdistutils/canary/ubuntu)/[Devuan](https://gitlab.com/debdistutils/canary/devuan) 90MB, [Trisquel](https://gitlab.com/debdistutils/canary/trisquel) 12MB, [Gnuinos](https://gitlab.com/debdistutils/canary/gnuinos) 2MB. Here I believe the main size correlation is update frequency, and Debian is large because I track the volatile unstable.

So I hit a scalability end with my first approach. A couple of months ago I “solved” this by discarding and resetting these archival repositories. The GitLab CI/CD jobs were fast again and all was well. However this meant discarding precious historic information. A couple of days ago I was reaching the limits of practicality again, and started to explore ways to fix this. I like having data stored in git (it allows easy integration with software integrity tools such as [GnuPG](https://gnupg.org/) and Sigstore, and the git log provides a kind of temporal ordering of data), so it felt like giving up on nice properties to use a traditional database with on-disk approach. So I started to learn about [Git-LFS](https://git-lfs.com/) and understanding that it was able to [handle multi-GB worth of data](https://devblogs.microsoft.com/bharry/the-largest-git-repo-on-the-planet/) that looked promising.

Fairly quickly I scripted up a [GitLab CI/CD job](https://gitlab.com/debdistutils/debdistget/-/blob/main/ci-debdistget-dists.yml) that incrementally update the **Release/Package/Sources** files in a git repository that uses Git-LFS to store all the files. The repository size is now at [Ubuntu 650kb](https://gitlab.com/debdistutils/dists/ubuntu), [Debian 300kb](https://gitlab.com/debdistutils/dists/debian), [Trisquel 50kb](https://gitlab.com/debdistutils/dists/trisquel), [Devuan 250kb](https://gitlab.com/debdistutils/dists/devuan), [PureOS 172kb](https://gitlab.com/debdistutils/dists/pureos) and [Gnuinos 17kb](https://gitlab.com/debdistutils/dists/gnuinos). As can be expected, jobs are quick to clone the git archives: [debdistdiff pipelines](https://gitlab.com/debdistutils/debdistdiff/-/pipelines) went from a **run-time of 80 minutes down to 10 minutes** which more reasonable correlate with the archive size and CPU run-time.

The LFS storage size for those repositories are at [Ubuntu 15GB](https://gitlab.com/debdistutils/dists/ubuntu), [Debian 8GB](https://gitlab.com/debdistutils/dists/debian), [Trisquel 1.7GB](https://gitlab.com/debdistutils/dists/trisquel), [Devuan 1.1GB](https://gitlab.com/debdistutils/dists/devuan), [PureOS](https://gitlab.com/debdistutils/dists/pureos)/[Gnuinos](https://gitlab.com/debdistutils/dists/gnuinos) 420MB. This is for a couple of days worth of data. It seems native Git is better at compressing/deduplicating data than Git-LFS is: the combined size for Ubuntu is already 15GB for a couple of days data compared to 8GB for a couple of months worth of data with pure Git. This may be a sub-optimal implementation of Git-LFS in GitLab but it does worry me that this new approach will be difficult to scale too. At some level the difference is understandable, Git-LFS probably store two different **Packages** files — around 90MB each for Trisquel — as two 90MB files, but native Git would store it as one compressed version of the 90MB file and one relatively small patch to turn the old files into the next file. So the Git-LFS approach surprisingly scale less well for overall storage-size. Still, the original repository is much smaller, and you usually don’t have to pull all LFS files anyway. So it is net win.

Throughout this work, I kept thinking about how my approach relates to [Debian’s snapshot service](https://snapshot.debian.org/). Ultimately what I would want is a combination of these two services. To have a good foundation to do transparency work I would want to have a collection of all **Release/Packages/Sources** files ever published, and ultimately also the source code and binaries. While it makes sense to start on the latest stable releases of distributions, this effort should scale backwards in time as well. For reproducing binaries from source code, I need to be able to securely find earlier versions of binary packages used for rebuilds. So I need to import all the **Release/Packages/Sources** packages from snapshot into my repositories. The latency to retrieve files from that server is slow so I haven’t been able to find an efficient/parallelized way to download the files. If I’m able to finish this, I would have confidence that my new Git-LFS based approach to store these files will scale over many years to come. This remains to be seen. Perhaps the repository has to be split up per release or per architecture or similar.

Another factor is storage costs. While the git repository size for a Git-LFS based repository with files from several years may be possible to sustain, the Git-LFS storage size surely won’t be. It seems GitLab charges the same for files in repositories and in Git-LFS, and it is around **$500 per 100GB** per year. It may be possible to setup a separate Git-LFS backend not hosted at GitLab to serve the LFS files. Does anyone know of a suitable server implementation for this? I had a quick look at the [Git-LFS implementation list](https://github.com/git-lfs/git-lfs/wiki/Implementations) and it seems the closest reasonable approach would be to setup the Gitea-clone [Forgejo](https://forgejo.org/) as a self-hosted server. Perhaps a cloud storage approach a’la S3 is the way to go? The cost to host this on GitLab will be manageable for up to **~1TB ($5000/year)** but scaling it to storing say **500TB** of data would mean an yearly fee of **$2.5M** which seems like poor value for the money.

I realized that ultimately I would want a git repository locally with the entire content of all apt archives, including their binary and source packages, ever published. The storage requirements for a service like snapshot (~300TB of data?) is today not prohibitly expensive: 20TB disks are $500 a piece, so a storage enclosure with 36 disks would be around **$18.000 for 720TB** and using RAID1 means 360TB which is a good start. While I have heard about ~TB-sized Git-LFS repositories, would Git-LFS scale to 1PB? Perhaps the size of a git repository with multi-millions number of Git-LFS pointer files will become unmanageable? To get started on this approach, I decided to import a mirror of **Debian’s bookworm for amd64** into a Git-LFS repository. That is around **175GB** so reasonable cheap to host even on GitLab ($1000/year for 200GB). Having this repository publicly available will make it possible to write software that uses this approach (e.g., porting [debdistreproduce](https://gitlab.com/debdistutils/debdistreproduce)), to find out if this is useful and if it could scale. Distributing the apt repository via Git-LFS would also enable other interesting ideas to protecting the data. Consider configuring apt to use a local **file://** URL to this git repository, and verifying the git checkout using some method similar to [Guix’s approach to trusting git](https://archive.fosdem.org/2023/schedule/event/security_where_does_that_code_come_from/) content or [Sigstore’s gitsign](https://github.com/sigstore/gitsign).

A naive push of the **175GB** archive in a single git commit ran into pack size limitations:

`remote: fatal: pack exceeds maximum allowed size (4.88 GiB)`

however breaking up the commit into smaller commits for parts of the archive made it possible to push the entire archive. Here are the commands to create this repository:

`git init
git lfs install
git lfs track 'dists/**' 'pool/**'
git add .gitattributes
git commit -m"Add Git-LFS track attributes." .gitattributes
time debmirror --method=rsync --host ftp.se.debian.org --root :debian --arch=amd64 --source --dist=bookworm,bookworm-updates --section=main --verbose --diff=none --keyring /usr/share/keyrings/debian-archive-keyring.gpg --ignore .git .
git add dists project
git commit -m"Add." -a
git remote add origin git@gitlab.com:debdistutils/archives/debian/mirror.git
git push --set-upstream origin --all
for d in pool/*/*; do
echo $d;
time git add $d;
git commit -m"Add $d." -a
git push
done`

The [resulting repository](https://gitlab.com/debdistutils/archives/debian/mirror) size is around 27MB with Git LFS object storage around 174GB. I think this approach would scale to handle all architectures for one release, but working with a single git repository for all releases for all architectures may lead to a too large git repository (>1GB). So maybe one repository per release? These repositories could also be split up on a subset of **pool/** files, or there could be one repository per release per architecture or sources.

Finally, I have concerns about using SHA1 for identifying objects. It seems both Git and Debian’s snapshot service is currently using SHA1\. For Git there is [SHA-256 transition](https://git-scm.com/docs/hash-function-transition) and it seems GitLab is working on support for SHA256-based repositories. For serious long-term deployment of these concepts, it would be nice to go for SHA256 identifiers directly. Git-LFS already uses SHA256 but Git internally uses SHA1 as does the Debian snapshot service.

What do you think? Happy Hacking!