<!--yml
category: 未分类
date: 2024-05-27 14:30:03
-->

# Where Is Noether's Principle in Machine Learning?

> 来源：[https://cgad.ski/blog/where-is-noethers-principle-in-machine-learning.html](https://cgad.ski/blog/where-is-noethers-principle-in-machine-learning.html)

# Where Is Noether's Principle in Machine Learning?

*This post accompanies a [poster](/math/mlss_poster.pdf) presented at the [MLSS 2024](http://mlss.cc/).*

Physics likes optimization! Subject to its boundary conditions, the time evolution of a physical system is a critical point for a quantity called an **action.** This point of view sets the stage for **Noether's principle**, a remarkable correspondence between continuous invariances of the action and conservation laws of the system.

In machine learning, we often deal with discrete "processes" whose control parameters are chosen to minimize some quantity. For example, we can see a deep residual network as a process where the role of "time" is played by depth. We may ask:

1.  Does Noether's theorem apply to these processes?
2.  Can we find meaningful conserved quantities?

Our answers: "yes," and "not sure!"

## Noether's Principle in Physics

In 1630, Fermat observed that the trajectory a beam of light takes through a lens is a path of least time. Formally, where <math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>v</mi><mo stretchy="false">(</mo><mi>q</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">v(q)</annotation></semantics></math>v(q) gives the velocity of light at a point <math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>q</mi><mo separator="true">,</mo></mrow><annotation encoding="application/x-tex">q,</annotation></semantics></math>q, a trajectory <math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>q</mi><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">q(t)</annotation></semantics></math>q(t) taken by a beam of light between two points <math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>q</mi><mo stretchy="false">(</mo><msub><mi>t</mi><mn>0</mn></msub><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">q(t_0)</annotation></semantics></math>q(t0​) and <math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>q</mi><mo stretchy="false">(</mo><msub><mi>t</mi><mn>1</mn></msub><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">q(t_1)</annotation></semantics></math>q(t1​) will be a stationary point for <math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>S</mi><mo stretchy="false">[</mo><mi>q</mi><mo stretchy="false">]</mo><mo>=</mo><msubsup><mo>∫</mo><msub><mi>t</mi><mn>0</mn></msub><msub><mi>t</mi><mn>1</mn></msub></msubsup><mfrac><mn>1</mn><mrow><mi>v</mi><mo stretchy="false">(</mo><mi>q</mi><mo stretchy="false">)</mo></mrow></mfrac><mo stretchy="false">∥</mo><mover accent="true"><mi>q</mi><mo>˙</mo></mover><mo stretchy="false">∥</mo><mi>d</mi><mi>t</mi><mo separator="true">,</mo></mrow><annotation encoding="application/x-tex">S[q] = \int_{t_0}^{t_1} \frac{1}{v(q)} \lVert \dot q \rVert \, dt,</annotation></semantics></math>S[q]=∫t0​t1​​v(q)1​∥q˙​∥dt, in the sense that <math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mfrac><mi>d</mi><mrow><mi>d</mi><mi>ϵ</mi></mrow></mfrac><mi>S</mi><mo stretchy="false">[</mo><mi>q</mi><mo>+</mo><mi>ϵ</mi><mi>h</mi><mo stretchy="false">]</mo><mo>=</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">\frac{d}{d \epsilon} S[q + \epsilon h] = 0</annotation></semantics></math>dϵd​S[q+ϵh]=0 for any "perturbation" <math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>h</mi></mrow><annotation encoding="application/x-tex">h</annotation></semantics></math>h verifying <math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>h</mi><mo stretchy="false">(</mo><msub><mi>t</mi><mn>0</mn></msub><mo stretchy="false">)</mo><mo>=</mo><mi>h</mi><mo stretchy="false">(</mo><msub><mi>t</mi><mn>1</mn></msub><mo stretchy="false">)</mo><mo>=</mo><mn>0.</mn></mrow><annotation encoding="application/x-tex">h(t_0) = h(t_1) = 0.</annotation></semantics></math>h(t0​)=h(t1​)=0. Furthermore, this condition turns out to characterize the paths that light can take.

In fact, all fundamental physical theories can be written in the form <math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>δ</mi><mi>S</mi><mo>=</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">\delta S = 0</annotation></semantics></math>δS=0 for a suitable "action" <math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>S</mi><mo separator="true">,</mo></mrow><annotation encoding="application/x-tex">S,</annotation></semantics></math>S, where <math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>δ</mi></mrow><annotation encoding="application/x-tex">\delta</annotation></semantics></math>δ denotes a (variational) derivative with respect to the process trajectory. Such an equation is called a **stationary action principle**.

Given a physical system, a function <math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>F</mi><mo stretchy="false">(</mo><mi>q</mi><mo separator="true">,</mo><mover accent="true"><mi>q</mi><mo>˙</mo></mover><mo separator="true">,</mo><mi>t</mi><mo separator="true">,</mo><mo>…</mo><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">F(q, \dot q, t, \dots)</annotation></semantics></math>F(q,q˙​,t,…) of the system state is called a conserved quantity when its value is constant over any given physical trajectory. For dynamics expressed by a stationary action principle, **Noether's principle** gives a remarkable correspondence between **conserved quantities** and certain **invariances** of the action <math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>S</mi><mi mathvariant="normal">.</mi></mrow><annotation encoding="application/x-tex">S.</annotation></semantics></math>S.

For the purposes of this article, we will not give more details on Noether's principle. The interested reader is invited to seek out Chapter 4 of Arnold's *Mathematical Methods of Classical Mechanics* for a concise description from the point of view of Lagrangian mechanics. (For a much more complete reference, including the more subtle "off-shell" version applicable to local gauge transformations, see Chapter 5 of Olver's *Application of Lie Groups to Differential Equations*.) However, as a guide for the imagination, we illustrate two examples.

In physics, the famous two-body problem is described by the action <math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>S</mi><mo>=</mo><mo>∫</mo><mfrac><mn>1</mn><mn>2</mn></mfrac><mo stretchy="false">(</mo><msub><mi>m</mi><mn>1</mn></msub><msubsup><mover accent="true"><mi>q</mi><mo>˙</mo></mover><mn>1</mn><mn>2</mn></msubsup><mo>+</mo><msub><mi>m</mi><mn>2</mn></msub><msubsup><mover accent="true"><mi>q</mi><mo>˙</mo></mover><mn>2</mn><mn>2</mn></msubsup><mo stretchy="false">)</mo><mo>−</mo><mfrac><mrow><msub><mi>m</mi><mn>1</mn></msub><msub><mi>m</mi><mn>2</mn></msub><mi>G</mi></mrow><mrow><mo stretchy="false">∥</mo><msub><mi>q</mi><mn>1</mn></msub><mo>−</mo><msub><mi>q</mi><mn>2</mn></msub><mo stretchy="false">∥</mo></mrow></mfrac><mi>d</mi><mi>t</mi><mo separator="true">,</mo></mrow><annotation encoding="application/x-tex">S = \int \frac{1}{2} (m_1 \dot q_1^2 + m_2 \dot q_2^2) - \frac{m_1 m_2 G}{\lVert q_1 - q_2 \rVert} \, dt,</annotation></semantics></math>S=∫21​(m1​q˙​12​+m2​q˙​22​)−∥q1​−q2​∥m1​m2​G​dt, where <math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>q</mi><mn>1</mn></msub></mrow><annotation encoding="application/x-tex">q_1</annotation></semantics></math>q1​ and <math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>q</mi><mn>2</mn></msub></mrow><annotation encoding="application/x-tex">q_2</annotation></semantics></math>q2​ are the positions of our two bodies and <math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>m</mi><mn>1</mn></msub></mrow><annotation encoding="application/x-tex">m_1</annotation></semantics></math>m1​ and <math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>m</mi><mn>2</mn></msub></mrow><annotation encoding="application/x-tex">m_2</annotation></semantics></math>m2​ are their masses. The integrand does not change if the trajectories <math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>q</mi><mn>1</mn></msub><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">q_1(t)</annotation></semantics></math>q1​(t) and <math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>q</mi><mn>2</mn></msub><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">q_2(t)</annotation></semantics></math>q2​(t) are translated or rotated, so these transformations are *invariants* for <math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>S</mi><mi mathvariant="normal">.</mi></mrow><annotation encoding="application/x-tex">S.</annotation></semantics></math>S. Applying Noether's theorem gives us three conserved quantities—one for each degree of freedom in our group of transformations—which turn out to be horizontal, vertical, and angular momentum.

In the illustration above, we consider two bodies with equal mass. Conservation of linear momentum means that the sum of the arrows is constant, while conservation of angular momentum means that the sum of highlighted areas is constant.

For a less familiar example of a conservation law, consider a ray of light passing through a rotationally symmetric lens. Rotating a trajectory about the center of the lens does not affect the time needed to traverse it. What conserved quantity does this symmetry produce?

At each instant, let <math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>p</mi></mrow><annotation encoding="application/x-tex">p</annotation></semantics></math>p be a vector pointing in the direction of <math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mover accent="true"><mi>q</mi><mo>˙</mo></mover></mrow><annotation encoding="application/x-tex">\dot q</annotation></semantics></math>q˙​ with norm equal to <math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>1</mn><mi mathvariant="normal">/</mi><mi>v</mi><mo stretchy="false">(</mo><mi>q</mi><mo stretchy="false">)</mo><mi mathvariant="normal">.</mi></mrow><annotation encoding="application/x-tex">1/v(q).</annotation></semantics></math>1/v(q). (We've drawn this vector in the widget above.) This vector can be seen as the gradient of <math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>S</mi></mrow><annotation encoding="application/x-tex">S</annotation></semantics></math>S with respect to <math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>q</mi><mo separator="true">;</mo></mrow><annotation encoding="application/x-tex">q;</annotation></semantics></math>q; it gives the marginal price we would pay to move <math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>q</mi></mrow><annotation encoding="application/x-tex">q</annotation></semantics></math>q in a certain direction at a particular instant. Noether's principle tells us that <math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>p</mi><mo>∧</mo><mo stretchy="false">(</mo><mi>q</mi><mo>−</mo><mi>c</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">p \wedge (q - c)</annotation></semantics></math>p∧(q−c) is conserved, where <math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>c</mi></mrow><annotation encoding="application/x-tex">c</annotation></semantics></math>c is the center of the lens. In our illustration, this conserved quantity is represented by the area of the triangle.

## Noether's Principle in ML: A Toy Example

In machine learning, we routinely deal with processes whose control parameters are chosen to minimize some quantity. For example, say we are trying to solve a supervised learning problem by choosing parameters <math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><msub><mi>θ</mi><mn>1</mn></msub><mo separator="true">,</mo><mo>…</mo><mo separator="true">,</mo><msub><mi>θ</mi><mi>N</mi></msub><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(\theta_1, \dots, \theta_N)</annotation></semantics></math>(θ1​,…,θN​) so that a composition <math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>F</mi><mo>=</mo><msub><mi>φ</mi><mi>N</mi></msub><mo stretchy="false">(</mo><msub><mi>θ</mi><mi>N</mi></msub><mo stretchy="false">)</mo><mo>∘</mo><mo>⋯</mo><mo>∘</mo><msub><mi>φ</mi><mn>2</mn></msub><mo stretchy="false">(</mo><msub><mi>θ</mi><mn>2</mn></msub><mo stretchy="false">)</mo><mo>∘</mo><msub><mi>φ</mi><mn>1</mn></msub><mo stretchy="false">(</mo><msub><mi>θ</mi><mn>1</mn></msub><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">F = \varphi_N(\theta_N) \circ \dots \circ \varphi_2(\theta_2) \circ \varphi_1(\theta_1)</annotation></semantics></math>F=φN​(θN​)∘⋯∘φ2​(θ2​)∘φ1​(θ1​) minimizes some expected loss <math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>E</mi><mo stretchy="false">[</mo><mi>L</mi><mo stretchy="false">(</mo><mi>F</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo separator="true">,</mo><mi>y</mi><mo stretchy="false">)</mo><mo stretchy="false">]</mo></mrow><annotation encoding="application/x-tex">E[L(F(x), y)]</annotation></semantics></math>E[L(F(x),y)] with respect to some random variables <math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>x</mi></mrow><annotation encoding="application/x-tex">x</annotation></semantics></math>x and <math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>y</mi><mi mathvariant="normal">.</mi></mrow><annotation encoding="application/x-tex">y.</annotation></semantics></math>y.

Let's view the sequence of random variables <math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mtable rowspacing="0.25em" columnalign="right left" columnspacing="0em"><mtr><mtd><mstyle scriptlevel="0" displaystyle="true"><msub><mi>x</mi><mn>0</mn></msub></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow><mo>=</mo><mi>x</mi></mrow></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="true"><msub><mi>x</mi><mn>1</mn></msub></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow><mo>=</mo><msub><mi>φ</mi><mn>1</mn></msub><mo stretchy="false">(</mo><msub><mi>θ</mi><mn>1</mn></msub><mo stretchy="false">)</mo><mo stretchy="false">(</mo><msub><mi>x</mi><mn>0</mn></msub><mo stretchy="false">)</mo></mrow></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow><mi><mi mathvariant="normal">⋮</mi><mpadded voffset="0em"><mspace mathbackground="black"></mspace></mpadded></mi></mrow></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="true"><msub><mi>x</mi><mi>N</mi></msub></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow><mo>=</mo><msub><mi>φ</mi><mi>N</mi></msub><mo stretchy="false">(</mo><msub><mi>θ</mi><mi>N</mi></msub><mo stretchy="false">)</mo><mo stretchy="false">(</mo><msub><mi>x</mi><mrow><mi>N</mi><mo>−</mo><mn>1</mn></mrow></msub><mo stretchy="false">)</mo></mrow></mstyle></mtd></mtr></mtable><annotation encoding="application/x-tex">\begin{align*} x_0 &= x \\ x_1 &= \varphi_1(\theta_1)(x_0) \\ & \vdots \\ x_N &= \varphi_N(\theta_N)(x_{N - 1}) \end{align*}</annotation></semantics></math>x0​x1​xN​​=x=φ1​(θ1​)(x0​)⋮=φN​(θN​)(xN−1​)​ as the states of a discrete-time "process." After optimization, the trajectory <math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><msub><mi>x</mi><mn>0</mn></msub><mo separator="true">,</mo><mo>…</mo><mo separator="true">,</mo><msub><mi>x</mi><mi>N</mi></msub><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(x_0, \dots, x_N)</annotation></semantics></math>(x0​,…,xN​) is approximately a **critical point for a loss function**. As we saw above, physical trajectories are also characterized as critical points of certain functions. Can we think about our trajectory of intermediate values <math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>x</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">x_i</annotation></semantics></math>xi​ from a physical point of view?

Of course, several things are different. Most obviously, "time" is now discrete rather than continuous. Furthermore, our trajectory of intermediate values is constrained in a different way. In physics, we specified boundary conditions and allowed the intermediate states of the process to vary, but in machine learning we specify an initial condition and constrain the trajectory <math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><msub><mi>x</mi><mn>0</mn></msub><mo separator="true">,</mo><mo>…</mo><mo separator="true">,</mo><msub><mi>x</mi><mi>N</mi></msub><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(x_0, \dots, x_N)</annotation></semantics></math>(x0​,…,xN​) to be driven by some parameters <math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>θ</mi><mi>i</mi></msub><mi mathvariant="normal">.</mi></mrow><annotation encoding="application/x-tex">\theta_i.</annotation></semantics></math>θi​. Fortunately, Noether's principle continues to apply!

To see how, let's consider a toy example that's easy to visualize. First, we'll define a family of "deformations" of the plane, as depicted below. (Click the screen or drag the sldier to change the parameters of the deformation.)

Next, we'll set up some simple optimization problems. In each case, we'll try to find a sequence of small deformations that work together to send some "clusters" to their corresponding "destinations." This is easy to do by initializing randomly and optimizing our parameters with gradient descent. In the following widget, a composition of 50 maps is able to swap the positions of three clusters.

Now, observe that our family <math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="script">F</mi></mrow><annotation encoding="application/x-tex">\Fc</annotation></semantics></math>F of deformations is **invariant** under conjugation by translations and rotations. This means that, where <math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>Q</mi></mrow><annotation encoding="application/x-tex">Q</annotation></semantics></math>Q is a translation or rotation, we have an equality <math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi mathvariant="script">F</mi><mo>=</mo><mo stretchy="false">{</mo><mi>Q</mi><mo>∘</mo><mi>f</mi><mo>∘</mo><msup><mi>Q</mi><mrow><mo>−</mo><mn>1</mn></mrow></msup><mo>∣</mo><mi>f</mi><mo>∈</mo><mi mathvariant="script">F</mi><mo stretchy="false">}</mo></mrow><annotation encoding="application/x-tex">\Fc = \{ Q \circ f \circ Q^{-1} \mid f \in \Fc \}</annotation></semantics></math>F={Q∘f∘Q−1∣f∈F} of sets. In other words, our family <math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="script">F</mi></mrow><annotation encoding="application/x-tex">\Fc</annotation></semantics></math>F "looks the same" from a translated or rotated coordinate system. Moreover, conjugating a map <math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi></mrow><annotation encoding="application/x-tex">f</annotation></semantics></math>f by a transformation <math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>Q</mi></mrow><annotation encoding="application/x-tex">Q</annotation></semantics></math>Q near the identity can be realized by making a small change to the parameters that define <math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mi mathvariant="normal">.</mi></mrow><annotation encoding="application/x-tex">f.</annotation></semantics></math>f. As it turns out, this is the right notion of invariance to apply Noether's principle. What conserved quantity do we get?

For each data point, let <math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><msub><mi>g</mi><mi>i</mi></msub><msubsup><mo stretchy="false">)</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>N</mi></msubsup></mrow><annotation encoding="application/x-tex">(g_i)_{i = 1}^N</annotation></semantics></math>(gi​)i=1N​ be the backpropagated gradients of the loss function with respect to the intermediate positions <math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><msub><mi>x</mi><mi>i</mi></msub><msubsup><mo stretchy="false">)</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>N</mi></msubsup></mrow><annotation encoding="application/x-tex">(x_i)_{i = 1}^N</annotation></semantics></math>(xi​)i=1N​ that this data point takes through our "process." Applying a certain discrete Noether's principle, detailed at the end of this post, will turn invariance of <math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="script">F</mi></mrow><annotation encoding="application/x-tex">\Fc</annotation></semantics></math>F under translations into conservation of the *expected value* <math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>E</mi><mo stretchy="false">[</mo><msub><mi>g</mi><mi>i</mi></msub><mo stretchy="false">]</mo></mrow><annotation encoding="application/x-tex">E[g_i]</annotation></semantics></math>E[gi​] from one layer to the next. (You can display the individual vectors <math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>g</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">g_i</annotation></semantics></math>gi​ in the widget above.) Invariance under rotation gives a conserved quantity reminiscent of angular momentum, namely <math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>E</mi><mo stretchy="false">[</mo><msub><mi>g</mi><mi>i</mi></msub><mo>∧</mo><msub><mi>x</mi><mi>i</mi></msub><mo stretchy="false">]</mo><mi mathvariant="normal">.</mi></mrow><annotation encoding="application/x-tex">E[g_i \wedge x_i].</annotation></semantics></math>E[gi​∧xi​].

In this particular example, it turns out that <math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>E</mi><mo stretchy="false">[</mo><msub><mi>g</mi><mi>i</mi></msub><mo stretchy="false">]</mo></mrow><annotation encoding="application/x-tex">E[g_i]</annotation></semantics></math>E[gi​] is also approximately conserved (and nearly zero) over each individual *cluster*. In the next widget, we add some examples where the per-cluster gradient is not conserved. (Each cluster now has its average gradient drawn with a single arrow. For simplicity, we are not visualizing "angular gradient.")

In the physical world, interactions are often accompanied by transfers of conserved quantities. Think, for example, of particles in a gas exchanging momentum and energy. Conversely, if two systems are not interacting with one another, then conservation laws will hold for each system in isolation. In mechanics, this is known as Newton's first law.

In our first toy example, the subgoals of bringing each cluster to its respective destination could be achieved without compromise between clusters, meaning that each cluster acted like an independent subsystem. This lack of interaction explains the conservation of gradient within each cluster. (In other words, we can apply Newton's first law.) In our next examples, "transfers" of average gradient indicate "interactions" between clusters, meaning that the trajectory of our process is no longer a stationary point for each subgoal considered in isolation. On the other hand, we can sometimes identify *subset* of clusters over which gradient is approximately conserved.

In general, if the layers of a deep model have meaningful "Noether equivariances," then we can build conserved quantities and perhaps study a physically-inspired notion of "interaction." However, I don't know if this idea works in practice! Can you think of interesting equivariances that we might find on real-world machine learning models? (Keep in mind that they might only be valid near the actual parameters of the model.)

## Proof of a Discrete Noether's Principle

In the above, we've been intentionally vague about the details of Noether's principle and how it applied to our toy example. We end this post with a general proof of our "discrete" Noether's principle.

We'll focus on a single step of an "optimized process." Let <math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>φ</mi><mtext> ⁣</mtext><mo lspace="0em" rspace="0em">:</mo><mi mathvariant="normal">Θ</mi><mo>×</mo><mi>X</mi><mo>→</mo><mi>Y</mi></mrow><annotation encoding="application/x-tex">\varphi \colon \Theta \times X \to Y</annotation></semantics></math>φ:Θ×X→Y describe some family of functions parameterized over <math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="normal">Θ</mi><mo separator="true">,</mo></mrow><annotation encoding="application/x-tex">\Theta,</annotation></semantics></math>Θ, and define the action <math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>S</mi><mo>=</mo><mo stretchy="false">⟨</mo><msub><mi>g</mi><mi>x</mi></msub><mo separator="true">,</mo><mi>x</mi><mo stretchy="false">⟩</mo><mo>−</mo><mo stretchy="false">⟨</mo><msub><mi>g</mi><mi>y</mi></msub><mo separator="true">,</mo><msub><mi>φ</mi><mi>θ</mi></msub><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo>−</mo><mi>y</mi><mo stretchy="false">⟩</mo></mrow><annotation encoding="application/x-tex">S = \langle g_x, x \rangle - \langle g_y, \varphi_\theta(x) - y \rangle</annotation></semantics></math>S=⟨gx​,x⟩−⟨gy​,φθ​(x)−y⟩ where <math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>x</mi></mrow><annotation encoding="application/x-tex">x</annotation></semantics></math>x and <math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>y</mi></mrow><annotation encoding="application/x-tex">y</annotation></semantics></math>y are understood to be random variables, <math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>g</mi><mi>x</mi></msub></mrow><annotation encoding="application/x-tex">g_x</annotation></semantics></math>gx​ and <math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>g</mi><mi>y</mi></msub></mrow><annotation encoding="application/x-tex">g_y</annotation></semantics></math>gy​ are their associated loss gradients, and inner products are taken in expectation. In the typical context of supervised learning, <math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>φ</mi></mrow><annotation encoding="application/x-tex">\varphi</annotation></semantics></math>φ will parameterize a layer of a deep model, and <math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">⟨</mo><msub><mi>g</mi><mi>x</mi></msub><mo separator="true">,</mo><mi>x</mi><mo stretchy="false">⟩</mo></mrow><annotation encoding="application/x-tex">\langle g_x, x \rangle</annotation></semantics></math>⟨gx​,x⟩ (for example) stands for a sum <math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mfrac><mn>1</mn><mi>n</mi></mfrac><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><mo stretchy="false">⟨</mo><msub><mi>g</mi><mi>x</mi></msub><mo stretchy="false">(</mo><mi>i</mi><mo stretchy="false">)</mo><mo separator="true">,</mo><mi>x</mi><mo stretchy="false">(</mo><mi>i</mi><mo stretchy="false">)</mo><mo stretchy="false">⟩</mo></mrow><annotation encoding="application/x-tex">\frac{1}{n} \sum_{i = 1}^n \langle g_x(i), x(i) \rangle</annotation></semantics></math>n1​i=1∑n​⟨gx​(i),x(i)⟩ where <math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>i</mi></mrow><annotation encoding="application/x-tex">i</annotation></semantics></math>i indexes over a dataset and <math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">(</mo><mi>i</mi><mo stretchy="false">)</mo><mo separator="true">,</mo><msub><mi>g</mi><mi>x</mi></msub><mo stretchy="false">(</mo><mi>i</mi><mo stretchy="false">)</mo><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(x(i), g_x(i))</annotation></semantics></math>(x(i),gx​(i)) are activation and gradient vectors for the <math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>i</mi></mrow><annotation encoding="application/x-tex">i</annotation></semantics></math>ith data point.

The "forward pass" <math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>y</mi><mo>=</mo><msub><mi>φ</mi><mi>θ</mi></msub><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">y = \varphi_\theta(x)</annotation></semantics></math>y=φθ​(x) and "backward pass" <math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>g</mi><mi>x</mi></msub><mo>=</mo><mo stretchy="false">(</mo><mi mathvariant="normal">∂</mi><mi>φ</mi><mi mathvariant="normal">/</mi><mi mathvariant="normal">∂</mi><mi>x</mi><msup><mo stretchy="false">)</mo><mi>T</mi></msup><msub><mi>g</mi><mi>y</mi></msub></mrow><annotation encoding="application/x-tex">g_x = (\partial \varphi / \partial x)^T g_y</annotation></semantics></math>gx​=(∂φ/∂x)Tgy​ are encoded by the stationarity of <math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>S</mi></mrow><annotation encoding="application/x-tex">S</annotation></semantics></math>S with respect to <math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>g</mi><mi>y</mi></msub></mrow><annotation encoding="application/x-tex">g_y</annotation></semantics></math>gy​ and <math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>x</mi><mo separator="true">,</mo></mrow><annotation encoding="application/x-tex">x,</annotation></semantics></math>x, since <math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mfrac><mrow><mi mathvariant="normal">∂</mi><mi>S</mi></mrow><mrow><mi mathvariant="normal">∂</mi><msub><mi>g</mi><mi>y</mi></msub></mrow></mfrac><mo>=</mo><msub><mi>φ</mi><mi>θ</mi></msub><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo>−</mo><mi>y</mi><mo separator="true">,</mo><mfrac><mrow><mi mathvariant="normal">∂</mi><mi>S</mi></mrow><mrow><mi mathvariant="normal">∂</mi><mi>x</mi></mrow></mfrac><mo>=</mo><msub><mi>g</mi><mi>x</mi></msub><mo>−</mo><msup><mrow><mo fence="true">(</mo><mfrac><mrow><mi mathvariant="normal">∂</mi><mi>φ</mi></mrow><mrow><mi mathvariant="normal">∂</mi><mi>x</mi></mrow></mfrac><mo fence="true">)</mo></mrow><mi>T</mi></msup><msub><mi>g</mi><mi>y</mi></msub><mi mathvariant="normal">.</mi></mrow><annotation encoding="application/x-tex">\frac{\partial S}{\partial g_y} = \varphi_\theta(x) - y, \quad \frac{\partial S}{\partial x} = g_x - \left( \frac{\partial \varphi}{\partial x}\right)^T g_y.</annotation></semantics></math>∂gy​∂S​=φθ​(x)−y,∂x∂S​=gx​−(∂x∂φ​)Tgy​. Similarly, first-order optimality of the parameter <math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>θ</mi></mrow><annotation encoding="application/x-tex">\theta</annotation></semantics></math>θ can be expressed as <math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mfrac><mrow><mi mathvariant="normal">∂</mi><mi>S</mi></mrow><mrow><mi mathvariant="normal">∂</mi><mi>θ</mi></mrow></mfrac><mo>=</mo><mn>0.</mn></mrow><annotation encoding="application/x-tex">\frac{\partial S}{\partial \theta} = 0.</annotation></semantics></math>∂θ∂S​=0. In practice, this will hold only approximately, so our conservation law will also be approximate. We ignore this detail for simplicity.

Now, suppose that <math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><mi>x</mi><mo separator="true">,</mo><mi>y</mi><mo separator="true">,</mo><mi>θ</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(x, y, \theta)</annotation></semantics></math>(x,y,θ) are made to depend on a real-valued parameter <math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>ϵ</mi></mrow><annotation encoding="application/x-tex">\epsilon</annotation></semantics></math>ϵ and that <math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mtable rowspacing="0.25em" columnalign="right" columnspacing=""><mtr><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow><mfrac><mi mathvariant="normal">∂</mi><mrow><mi mathvariant="normal">∂</mi><mi>ϵ</mi></mrow></mfrac><mo stretchy="false">⟨</mo><msub><mi>g</mi><mi>y</mi></msub><mo separator="true">,</mo><msub><mi>φ</mi><mi>θ</mi></msub><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo>−</mo><mi>y</mi><mo stretchy="false">⟩</mo><mo>=</mo><mn>0.</mn></mrow></mstyle></mtd></mtr></mtable><annotation encoding="application/x-tex">\begin{align*} \frac{\partial}{\partial \epsilon} \langle g_y, \varphi_\theta(x) - y \rangle = 0.\tag{1} \end{align*}</annotation></semantics></math>∂ϵ∂​⟨gy​,φθ​(x)−y⟩=0.​(1)​ For example, when we have a Lie group <math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>G</mi></mrow><annotation encoding="application/x-tex">G</annotation></semantics></math>G acting on <math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>X</mi><mo separator="true">,</mo></mrow><annotation encoding="application/x-tex">X,</annotation></semantics></math>X, <math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="normal">Θ</mi></mrow><annotation encoding="application/x-tex">\Theta</annotation></semantics></math>Θ and <math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>Y</mi></mrow><annotation encoding="application/x-tex">Y</annotation></semantics></math>Y such that <math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><msub><mi>φ</mi><mrow><mi>g</mi><mo>⋅</mo><mi>θ</mi></mrow></msub><mo stretchy="false">(</mo><mi>g</mi><mo>⋅</mo><mi>x</mi><mo stretchy="false">)</mo><mo>=</mo><mi>g</mi><mo>⋅</mo><msub><mi>φ</mi><mi>θ</mi></msub><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo separator="true">,</mo></mrow><annotation encoding="application/x-tex">\varphi_{g \cdot \theta}(g \cdot x) = g \cdot \varphi_\theta(x),</annotation></semantics></math>φg⋅θ​(g⋅x)=g⋅φθ​(x), then any one-parameter subgroup of <math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>G</mi></mrow><annotation encoding="application/x-tex">G</annotation></semantics></math>G gives rise to such a variation of <math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><mi>x</mi><mo separator="true">,</mo><mi>y</mi><mo separator="true">,</mo><mi>θ</mi><mo stretchy="false">)</mo><mi mathvariant="normal">.</mi></mrow><annotation encoding="application/x-tex">(x, y, \theta).</annotation></semantics></math>(x,y,θ). This is the case for our toy example, where <math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>G</mi></mrow><annotation encoding="application/x-tex">G</annotation></semantics></math>G is the group <math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="normal">SE</mi><mo>⁡</mo><mo stretchy="false">(</mo><mn>2</mn><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">\operatorname{SE}(2)</annotation></semantics></math>SE(2) of translations and rotations of the plane. I'm calling this kind of symmetry a **Noether equivariance**.

Let's compute <math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="normal">∂</mi><mi>S</mi><mi mathvariant="normal">/</mi><mi mathvariant="normal">∂</mi><mi>ϵ</mi><mi mathvariant="normal">.</mi></mrow><annotation encoding="application/x-tex">\partial S / \partial \epsilon.</annotation></semantics></math>∂S/∂ϵ. Applying <math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><mn>1</mn><mo stretchy="false">)</mo><mo separator="true">,</mo></mrow><annotation encoding="application/x-tex">(1),</annotation></semantics></math>(1), we find <math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mfrac><mrow><mi mathvariant="normal">∂</mi><mi>S</mi></mrow><mrow><mi mathvariant="normal">∂</mi><mi>ϵ</mi></mrow></mfrac><mo>=</mo><mrow><mo fence="true">⟨</mo><msub><mi>g</mi><mi>x</mi></msub><mo separator="true">,</mo><mfrac><mrow><mi mathvariant="normal">∂</mi><mi>x</mi></mrow><mrow><mi mathvariant="normal">∂</mi><mi>ϵ</mi></mrow></mfrac><mo fence="true">⟩</mo></mrow><mi mathvariant="normal">.</mi></mrow><annotation encoding="application/x-tex">\frac{\partial S}{\partial \epsilon} = \left\langle g_x, \frac{\partial x}{\partial \epsilon} \right\rangle.</annotation></semantics></math>∂ϵ∂S​=⟨gx​,∂ϵ∂x​⟩. On the other hand, if <math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>S</mi></mrow><annotation encoding="application/x-tex">S</annotation></semantics></math>S is stationary with respect to <math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>x</mi></mrow><annotation encoding="application/x-tex">x</annotation></semantics></math>x and <math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>θ</mi><mo separator="true">,</mo></mrow><annotation encoding="application/x-tex">\theta,</annotation></semantics></math>θ, then <math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="normal">∂</mi><mi>S</mi><mi mathvariant="normal">/</mi><mi mathvariant="normal">∂</mi><mi>ϵ</mi></mrow><annotation encoding="application/x-tex">\partial S / \partial \epsilon</annotation></semantics></math>∂S/∂ϵ can be explained entirely by the variation of <math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>y</mi><mo separator="true">,</mo></mrow><annotation encoding="application/x-tex">y,</annotation></semantics></math>y, which is <math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mfrac><mrow><mi mathvariant="normal">∂</mi><mi>S</mi></mrow><mrow><mi mathvariant="normal">∂</mi><mi>ϵ</mi></mrow></mfrac><mo>=</mo><mfrac><mrow><mi mathvariant="normal">∂</mi><mi>S</mi></mrow><mrow><mi mathvariant="normal">∂</mi><mi>y</mi></mrow></mfrac><mfrac><mrow><mi mathvariant="normal">∂</mi><mi>y</mi></mrow><mrow><mi mathvariant="normal">∂</mi><mi>ϵ</mi></mrow></mfrac><mo>=</mo><mrow><mo fence="true">⟨</mo><msub><mi>g</mi><mi>y</mi></msub><mo separator="true">,</mo><mfrac><mrow><mi mathvariant="normal">∂</mi><mi>y</mi></mrow><mrow><mi mathvariant="normal">∂</mi><mi>ϵ</mi></mrow></mfrac><mo fence="true">⟩</mo></mrow><mi mathvariant="normal">.</mi></mrow><annotation encoding="application/x-tex">\frac{\partial S}{\partial \epsilon} = \frac{\partial S}{\partial y} \frac{\partial y}{\partial \epsilon} = \left\langle g_y, \frac{\partial y}{\partial \epsilon} \right\rangle.</annotation></semantics></math>∂ϵ∂S​=∂y∂S​∂ϵ∂y​=⟨gy​,∂ϵ∂y​⟩. Overall, we conclude that <math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mrow><mo fence="true">⟨</mo><msub><mi>g</mi><mi>x</mi></msub><mo separator="true">,</mo><mfrac><mrow><mi mathvariant="normal">∂</mi><mi>x</mi></mrow><mrow><mi mathvariant="normal">∂</mi><mi>ϵ</mi></mrow></mfrac><mo fence="true">⟩</mo></mrow><mo>=</mo><mrow><mo fence="true">⟨</mo><msub><mi>g</mi><mi>y</mi></msub><mo separator="true">,</mo><mfrac><mrow><mi mathvariant="normal">∂</mi><mi>y</mi></mrow><mrow><mi mathvariant="normal">∂</mi><mi>ϵ</mi></mrow></mfrac><mo fence="true">⟩</mo></mrow><mi mathvariant="normal">.</mi></mrow><annotation encoding="application/x-tex">\left\langle g_x, \frac{\partial x}{\partial \epsilon} \right\rangle = \left\langle g_y, \frac{\partial y}{\partial \epsilon} \right\rangle.</annotation></semantics></math>⟨gx​,∂ϵ∂x​⟩=⟨gy​,∂ϵ∂y​⟩. This is quite analogous to the conservation law <math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mfrac><mi>d</mi><mrow><mi>d</mi><mi>t</mi></mrow></mfrac><mrow><mo fence="true">⟨</mo><mi>p</mi><mo separator="true">,</mo><mfrac><mrow><mi mathvariant="normal">∂</mi><mi>q</mi></mrow><mrow><mi mathvariant="normal">∂</mi><mi>ϵ</mi></mrow></mfrac><mo fence="true">⟩</mo></mrow><mo>=</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">\frac{d}{dt} \left\langle p, \frac{\partial q}{\partial \epsilon} \right\rangle = 0</annotation></semantics></math>dtd​⟨p,∂ϵ∂q​⟩=0 that Noether's principle implies for a Hamiltonian system invariant under a family of canonical transformations.

One simple kind of Noether equivariance is a relationship like <math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><msub><mi>φ</mi><mrow><mi>g</mi><mo>⋅</mo><mi>θ</mi></mrow></msub><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo>=</mo><mi>g</mi><mo>⋅</mo><msub><mi>φ</mi><mi>θ</mi></msub><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo separator="true">,</mo></mrow><annotation encoding="application/x-tex">\varphi_{g \cdot \theta}(x) = g \cdot\varphi_\theta(x),</annotation></semantics></math>φg⋅θ​(x)=g⋅φθ​(x), meaning that our family of maps is expressive enough to be closed under the action of <math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>G</mi></mrow><annotation encoding="application/x-tex">G</annotation></semantics></math>G on its outputs. In this case, since the action on <math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>x</mi></mrow><annotation encoding="application/x-tex">x</annotation></semantics></math>x is trivial, <math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="normal">∂</mi><mi>x</mi><mi mathvariant="normal">/</mi><mi mathvariant="normal">∂</mi><mi>ϵ</mi><mo>=</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">\partial x / \partial \epsilon = 0</annotation></semantics></math>∂x/∂ϵ=0 and the "conservation law" associated with any one-parameter subgroup of <math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>G</mi></mrow><annotation encoding="application/x-tex">G</annotation></semantics></math>G simply reads <math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mn>0</mn><mo>=</mo><mrow><mo fence="true">⟨</mo><msub><mi>g</mi><mi>y</mi></msub><mo separator="true">,</mo><mfrac><mrow><mi mathvariant="normal">∂</mi><mi>y</mi></mrow><mrow><mi mathvariant="normal">∂</mi><mi>ϵ</mi></mrow></mfrac><mo fence="true">⟩</mo></mrow><mi mathvariant="normal">.</mi></mrow><annotation encoding="application/x-tex">0 = \left\langle g_y, \frac{\partial y}{\partial \epsilon} \right\rangle.</annotation></semantics></math>0=⟨gy​,∂ϵ∂y​⟩. Indeed, this inner product is exactly the derivative of the loss with respect to the transformation of <math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>y</mi></mrow><annotation encoding="application/x-tex">y</annotation></semantics></math>y by a given one-parameter subgroup of <math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>G</mi><mi mathvariant="normal">.</mi></mrow><annotation encoding="application/x-tex">G.</annotation></semantics></math>G. If variations to <math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>θ</mi></mrow><annotation encoding="application/x-tex">\theta</annotation></semantics></math>θ can produce these transformations and <math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>θ</mi></mrow><annotation encoding="application/x-tex">\theta</annotation></semantics></math>θ is chosen optimally, the inner product must be <math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>0.</mn></mrow><annotation encoding="application/x-tex">0.</annotation></semantics></math>0. (Informally, this explains why the average gradients in our toy example are approximately <math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>0.</mn></mrow><annotation encoding="application/x-tex">0.</annotation></semantics></math>0.)

The same idea applies to a group acting only on <math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>x</mi></mrow><annotation encoding="application/x-tex">x</annotation></semantics></math>x and <math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>θ</mi><mo separator="true">,</mo></mrow><annotation encoding="application/x-tex">\theta,</annotation></semantics></math>θ, in which case we get <math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mrow><mo fence="true">⟨</mo><msub><mi>g</mi><mi>x</mi></msub><mo separator="true">,</mo><mfrac><mrow><mi mathvariant="normal">∂</mi><mi>x</mi></mrow><mrow><mi mathvariant="normal">∂</mi><mi>ϵ</mi></mrow></mfrac><mo fence="true">⟩</mo></mrow><mo>=</mo><mn>0.</mn></mrow><annotation encoding="application/x-tex">\left\langle g_x, \frac{\partial x}{\partial \epsilon} \right\rangle = 0.</annotation></semantics></math>⟨gx​,∂ϵ∂x​⟩=0. For example, if <math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>x</mi><mo>∈</mo><msup><mi mathvariant="double-struck">R</mi><mi>n</mi></msup></mrow><annotation encoding="application/x-tex">x \in \R^n</annotation></semantics></math>x∈Rn is the input to a linear layer, we get a "Noether equivariance" in the form of an action of <math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="normal">GL</mi><mo>⁡</mo><mo stretchy="false">(</mo><mi>n</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">\GL(n)</annotation></semantics></math>GL(n) on <math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>x</mi></mrow><annotation encoding="application/x-tex">x</annotation></semantics></math>x and the layer parameters. Letting <math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="normal">∂</mi><mi>x</mi><mi mathvariant="normal">/</mi><mi mathvariant="normal">∂</mi><mi>ϵ</mi></mrow><annotation encoding="application/x-tex">\partial x / \partial \epsilon</annotation></semantics></math>∂x/∂ϵ range over all linear functions of <math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>x</mi></mrow><annotation encoding="application/x-tex">x</annotation></semantics></math>x in the equation above—which we get from the action of one-parameter subgroups of <math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="normal">GL</mi><mo>⁡</mo><mo stretchy="false">(</mo><mi>n</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">\GL(n)</annotation></semantics></math>GL(n)—shows that the tensor product <math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>g</mi><mi>x</mi></msub><mo>⊗</mo><mi>x</mi></mrow><annotation encoding="application/x-tex">g_x \otimes x</annotation></semantics></math>gx​⊗x will vanish in expectation if the loss is stationary with respect to the parameters of the linear layer. The general principle we proved above can be understood as the "two-sided" extension of these kinds of observations.