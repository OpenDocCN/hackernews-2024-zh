["```\n#include <stddef.h>\n#include <stdint.h>\n#include <stdlib.h>\n\ntypedef uint8_t pixel;\nstatic unsigned sad_16x16_c(const pixel *src, ptrdiff_t src_stride,\n                            const pixel *dst, ptrdiff_t dst_stride)\n{\n    unsigned sum = 0;\n    int y, x;\n\n    for (y = 0; y < 16; y++) {\n        for (x = 0; x < 16; x++)\n            sum += abs(src[x] - dst[x]);\n        src += src_stride;\n        dst += dst_stride;\n    }\n\n    return sum;\n} \n```", "```\n%include \"x86inc.asm\"\n\nSECTION .text\n\nINIT_XMM sse2\ncglobal sad_16x16, 4, 7, 5, src, src_stride, dst, dst_stride, \\\n                            src_stride3, dst_stride3, cnt\n    lea    src_stride3q, [src_strideq*3]\n    lea    dst_stride3q, [dst_strideq*3]\n    mov            cntd, 4\n    pxor             m0, m0\n.loop:\n    mova             m1, [srcq+src_strideq*0]\n    mova             m2, [srcq+src_strideq*1]\n    mova             m3, [srcq+src_strideq*2]\n    mova             m4, [srcq+src_stride3q]\n    lea            srcq, [srcq+src_strideq*4]\n    psadbw           m1, [dstq+dst_strideq*0]\n    psadbw           m2, [dstq+dst_strideq*1]\n    psadbw           m3, [dstq+dst_strideq*2]\n    psadbw           m4, [dstq+dst_stride3q]\n    lea            dstq, [dstq+dst_strideq*4]\n    paddw            m1, m2\n    paddw            m3, m4\n    paddw            m0, m1\n    paddw            m0, m3\n    dec            cntd\n    jg .loop\n    movhlps          m1, m0\n    paddw            m0, m1\n    movd            eax, m0\n    RET \n```", "```\ncglobal sad_16x16, 4, 7, 5, src, src_stride, dst, dst_stride, \\\n                            src_stride3, dst_stride3, cnt \n```", "```\n%macro SAD_FN 2 ; width, height\ncglobal sad_%1x%2, 4, 7, 5, src, src_stride, dst, dst_stride, \\\n                            src_stride3, dst_stride3, cnt\n    lea    src_stride3q, [src_strideq*3]\n    lea    dst_stride3q, [dst_strideq*3]\n    mov            cntd, %2 / 4\n    pxor             m0, m0\n.loop:\n    mova             m1, [srcq+src_strideq*0]\n    mova             m2, [srcq+src_strideq*1]\n    mova             m3, [srcq+src_strideq*2]\n    mova             m4, [srcq+src_stride3q]\n    lea            srcq, [srcq+src_strideq*4]\n    psadbw           m1, [dstq+dst_strideq*0]\n    psadbw           m2, [dstq+dst_strideq*1]\n    psadbw           m3, [dstq+dst_strideq*2]\n    psadbw           m4, [dstq+dst_stride3q]\n    lea            dstq, [dstq+dst_strideq*4]\n    paddw            m1, m2\n    paddw            m3, m4\n    paddw            m0, m1\n    paddw            m0, m3\n    dec            cntd\n    jg .loop\n\n%if mmsize >= 16\n%if mmsize >= 32\n    vextracti128    xm1, m0, 1\n    paddw           xm0, xm1\n%endif\n    movhlps         xm1, xm0\n    paddw           xm0, xm1\n%endif\n    movd            eax, xm0\n    RET\n%endmacro\n\nINIT_MMX mmxext\nSAD_FN 8, 4\nSAD_FN 8, 8\nSAD_FN 8, 16\n\nINIT_XMM sse2\nSAD_FN 16, 8\nSAD_FN 16, 16\nSAD_FN 16, 32\n\nINIT_YMM avx2\nSAD_FN 32, 16\nSAD_FN 32, 32\nSAD_FN 32, 64 \n```", "```\n%macro ABSW 2 ; dst/src, tmp\n%if cpuflag(ssse3)\n    pabsw   %1, %1\n%else\n    pxor    %2, %2\n    psubw   %2, %1\n    pmaxsw  %1, %2\n%endif\n%endmacro\n\n%macro SAD_8x8_FN 0\ncglobal sad_8x8, 4, 7, 6, src, src_stride, dst, dst_stride, \\\n                          src_stride3, dst_stride3, cnt\n    lea    src_stride3q, [src_strideq*3]\n    lea    dst_stride3q, [dst_strideq*3]\n    mov            cntd, 2\n    pxor             m0, m0\n.loop:\n    mova             m1, [srcq+src_strideq*0]\n    mova             m2, [srcq+src_strideq*1]\n    mova             m3, [srcq+src_strideq*2]\n    mova             m4, [srcq+src_stride3q]\n    lea            srcq, [srcq+src_strideq*4]\n    psubw            m1, [dstq+dst_strideq*0]\n    psubw            m2, [dstq+dst_strideq*1]\n    psubw            m3, [dstq+dst_strideq*2]\n    psubw            m4, [dstq+dst_stride3q]\n    lea            dstq, [dstq+dst_strideq*4]\n    ABSW             m1, m5\n    ABSW             m2, m5\n    ABSW             m3, m5\n    ABSW             m4, m5\n    paddw            m1, m2\n    paddw            m3, m4\n    paddw            m0, m1\n    paddw            m0, m3\n    dec            cntd\n    jg .loop\n    movhlps          m1, m0\n    paddw            m0, m1\n    pshuflw      m1, m0, q1010\n    paddw            m0, m1\n    pshuflw      m1, m0, q0000 ; qNNNN is a base4-notation for imm8 arguments\n    paddw            m0, m0\n    movd            eax, m0\n    movsxwd         eax, ax\n    RET\n%endmacro\n\nINIT_XMM sse2\nSAD_8x8_FN\n\nINIT_XMM ssse3\nSAD_8x8_FN \n```", "```\n[..]\n    mova           m0, [srcq+src_strideq*0]\n    mova           m1, [srcq+src_strideq*1]\n    punpckhbw  m2, m0, m1\n    punpcklbw      m0, m1\n[..] \n```", "```\n[..]\n    vmovdqa    xmm0, [rdi]\n    vmovdqa    xmm1, [rdi+rsi]\n    vpunpckhbw xmm2, xmm0, xmm1\n    vpunpcklbw xmm0, xmm0, xmm1\n[..] \n```", "```\n[..]\n    movdqa    xmm0, [rdi]\n    movdqa    xmm1, [rdi+rsi]\n    movdqa    xmm2, xmm0\n    punpckhbw xmm2, xmm1\n    punpcklbw xmm0, xmm1\n[..] \n```", "```\ncglobal sad_16x16, 4, 7, 5, 64, src, src_stride, dst, dst_stride, \\\n                                src_stride3, dst_stride3, cnt \n```", "```\ncglobal sad_16x16, 4, 7, 5, 0 - 64, src, src_stride, dst, dst_stride, \\\n                                    src_stride3, dst_stride3, cnt\n```"]