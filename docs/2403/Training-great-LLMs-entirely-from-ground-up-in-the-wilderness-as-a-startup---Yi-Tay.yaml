- en: <!--yml
  id: totrans-split-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: æœªåˆ†ç±»'
  id: totrans-split-1
  prefs: []
  type: TYPE_NORMAL
  zh: 'category: æœªåˆ†ç±»'
- en: 'date: 2024-05-27 14:39:19'
  id: totrans-split-2
  prefs: []
  type: TYPE_NORMAL
  zh: 'date: 2024-05-27 14:39:19'
- en: -->
  id: totrans-split-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: Training great LLMs entirely from ground up in the wilderness as a startup â€”
    Yi Tay
  id: totrans-split-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ä½œä¸ºåˆåˆ›å…¬å¸åœ¨è’é‡ä¸­å®Œå…¨ä»é›¶å¼€å§‹è®­ç»ƒå‡ºä¼Ÿå¤§çš„LLMs â€” Yi Tay
- en: æ¥æºï¼š[https://www.yitay.net/blog/training-great-llms-entirely-from-ground-zero-in-the-wilderness](https://www.yitay.net/blog/training-great-llms-entirely-from-ground-zero-in-the-wilderness)
  id: totrans-split-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: æ¥æºï¼š[https://www.yitay.net/blog/training-great-llms-entirely-from-ground-zero-in-the-wilderness](https://www.yitay.net/blog/training-great-llms-entirely-from-ground-zero-in-the-wilderness)
- en: Hardware Lottery in the Era of LLMs
  id: totrans-split-6
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: LLMsæ—¶ä»£çš„ç¡¬ä»¶å½©ç¥¨
- en: The first requisite for training models is acquiring compute. This seems straightforward
    and easy enough. However, the largest surprise turned out to be the instability
    of compute providers and how large variance the quality of clusters, accelerators
    and their connectivity were depending on the source.
  id: totrans-split-7
  prefs: []
  type: TYPE_NORMAL
  zh: è®­ç»ƒæ¨¡å‹çš„é¦–è¦æ¡ä»¶æ˜¯è·å–è®¡ç®—èµ„æºã€‚è¿™ä¼¼ä¹å¾ˆç›´æ¥ä¸”è¶³å¤Ÿå®¹æ˜“ã€‚ç„¶è€Œï¼Œæœ€å¤§çš„æ„å¤–æ˜¯è®¡ç®—èµ„æºæä¾›è€…çš„ä¸ç¨³å®šæ€§ï¼Œä»¥åŠä¾èµ–æ¥æºçš„é›†ç¾¤ã€åŠ é€Ÿå™¨åŠå…¶è¿æ¥è´¨é‡çš„å·¨å¤§å·®å¼‚ã€‚
- en: 'People always assume itâ€™s simply a question/debate of accelerator choice (TPUs
    vs GPUs etc) and all GPU clusters are created equal. For us, this soon proved
    to be false. As we sampled across different service providers, we find that the
    *variance of hardware quality differs vastly even for the same hardware,* i.e.,
    GPUs (H100s). Note that here, hardware refers to overall cluster quality and not
    necessarily the chips or accelerators per se. Just like a lottery. Basically:'
  id: totrans-split-8
  prefs: []
  type: TYPE_NORMAL
  zh: äººä»¬æ€»æ˜¯å‡è®¾è¿™ä»…ä»…æ˜¯åŠ é€Ÿå™¨é€‰æ‹©çš„é—®é¢˜/è¾©è®ºï¼ˆTPUs vs GPUsç­‰ï¼‰ï¼Œå¹¶ä¸”æ‰€æœ‰GPUé›†ç¾¤éƒ½æ˜¯ä¸€æ ·çš„ã€‚å¯¹æˆ‘ä»¬æ¥è¯´ï¼Œè¿™å¾ˆå¿«è¢«è¯æ˜æ˜¯é”™è¯¯çš„ã€‚å½“æˆ‘ä»¬åœ¨ä¸åŒçš„æœåŠ¡æä¾›å•†ä¹‹é—´å–æ ·æ—¶ï¼Œæˆ‘ä»¬å‘ç°*ç”šè‡³å¯¹äºåŒæ ·çš„ç¡¬ä»¶ï¼Œç¡¬ä»¶è´¨é‡çš„å·®å¼‚ä¹Ÿæ˜¯å·¨å¤§çš„*ï¼Œå³ï¼ŒGPUï¼ˆH100sï¼‰ã€‚è¯·æ³¨æ„ï¼Œè¿™é‡Œçš„ç¡¬ä»¶æŒ‡çš„æ˜¯æ•´ä½“é›†ç¾¤è´¨é‡ï¼Œè€Œä¸ä¸€å®šæ˜¯èŠ¯ç‰‡æˆ–åŠ é€Ÿå™¨æœ¬èº«ã€‚å°±åƒä¹°å½©ç¥¨ä¸€æ ·ã€‚åŸºæœ¬ä¸Šï¼š
- en: Not all hardware is created equal. *The variance of cluster quality across hardware
    providers is so high that it is literally a lottery pertaining to how much pain
    one would have to go through to train good models. In short, a hardware lottery
    in the era of LLMs.Â *
  id: totrans-split-9
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: å¹¶éæ‰€æœ‰ç¡¬ä»¶éƒ½æ˜¯ç›¸åŒçš„ã€‚*ä¸åŒç¡¬ä»¶æä¾›å•†ä¹‹é—´çš„é›†ç¾¤è´¨é‡å·®å¼‚å¦‚æ­¤ä¹‹å¤§ï¼Œä»¥è‡³äºåœ¨è®­ç»ƒä¼˜ç§€æ¨¡å‹æ—¶ï¼Œä¼šé¢ä¸´å¤šå°‘ç—›è‹¦å®é™…ä¸Šæ˜¯ä¸ªå½©ç¥¨ã€‚ç®€è€Œè¨€ä¹‹ï¼Œåœ¨LLMsæ—¶ä»£æ˜¯ç¡¬ä»¶çš„å½©ç¥¨ã€‚*
- en: More specifically, weâ€™ve leased a few clusters from several compute providers,
    each with a range of hundreds to thousands of chips. Weâ€™ve seen clusters that
    range from passable (just annoying problems that are solvable with some minor
    SWE hours) to totally unusable clusters that fail every few hours due to a myriad
    of reasons. Specifically, some clusters have nodes that fail every N hour with
    issues ranging from cabling issues (where N is unreasonably small), GPU hardware
    errors etc. Even more surprisingly, every cluster across the same provider could
    also be vastly different in terms of how robust it was.
  id: totrans-split-10
  prefs: []
  type: TYPE_NORMAL
  zh: å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬ä»å‡ å®¶è®¡ç®—èµ„æºæä¾›å•†é‚£é‡Œç§Ÿç”¨äº†å‡ ä¸ªé›†ç¾¤ï¼Œæ¯ä¸ªé›†ç¾¤æ‹¥æœ‰æ•°ç™¾åˆ°æ•°åƒä¸ªèŠ¯ç‰‡ã€‚æˆ‘ä»¬çœ‹åˆ°çš„é›†ç¾¤ä»è¿˜ç®—è¿‡å¾—å»çš„ï¼ˆåªæœ‰ä¸€äº›å¯ä»¥é€šè¿‡å°‘é‡è½¯ä»¶å·¥ç¨‹å¸ˆå°æ—¶è§£å†³çš„æ¼äººé—®é¢˜ï¼‰åˆ°å®Œå…¨æ— æ³•ä½¿ç”¨çš„é›†ç¾¤ï¼Œç”±äºå„ç§åŸå› æ¯éš”å‡ ä¸ªå°æ—¶å°±ä¼šå¤±è´¥ã€‚å…·ä½“æ¥è¯´ï¼Œä¸€äº›é›†ç¾¤çš„èŠ‚ç‚¹æ¯éš”Nå°æ—¶å°±ä¼šå¤±è´¥ï¼Œé—®é¢˜åŒ…æ‹¬å¸ƒçº¿é—®é¢˜ï¼ˆå…¶ä¸­Néå¸¸å°ï¼‰ï¼ŒGPUç¡¬ä»¶é”™è¯¯ç­‰ã€‚æ›´ä»¤äººæƒŠè®¶çš„æ˜¯ï¼ŒåŒä¸€æä¾›å•†çš„æ¯ä¸ªé›†ç¾¤åœ¨ç¨³å¥æ€§æ–¹é¢ä¹Ÿå¯èƒ½å¤§ä¸ç›¸åŒã€‚
- en: Meanwhile, even though some other clusters could have significantly more stable
    nodes, they might suffer from poor I/O and file system that even saving checkpoints
    could result in timeouts or ungodly amounts of time chipping away at cluster utilisation.
    Some other compute sources require a completely different software layer to even
    run and are non-friendly to teams that bring their own codebases - requiring additional
    migration costs to run experiments or large jobs.
  id: totrans-split-11
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸æ­¤åŒæ—¶ï¼Œå³ä½¿æŸäº›å…¶ä»–é›†ç¾¤çš„èŠ‚ç‚¹å¯èƒ½æ›´åŠ ç¨³å®šï¼Œå®ƒä»¬å¯èƒ½é­å—I/Oå’Œæ–‡ä»¶ç³»ç»Ÿä¸è‰¯çš„å›°æ‰°ï¼Œå³ä½¿æ˜¯ä¿å­˜æ£€æŸ¥ç‚¹ä¹Ÿå¯èƒ½å¯¼è‡´è¶…æ—¶æˆ–è€…é•¿æ—¶é—´è€—è´¹é›†ç¾¤åˆ©ç”¨ç‡ã€‚å…¶ä»–ä¸€äº›è®¡ç®—èµ„æºç”šè‡³éœ€è¦å®Œå…¨ä¸åŒçš„è½¯ä»¶å±‚æ‰èƒ½è¿è¡Œï¼Œå¹¶ä¸”å¯¹äºå¸¦è‡ªå·±ä»£ç åº“çš„å›¢é˜Ÿæ¥è¯´å¹¶ä¸å‹å¥½
    â€” éœ€è¦é¢å¤–çš„è¿ç§»æˆæœ¬æ¥è¿è¡Œå®éªŒæˆ–å¤§è§„æ¨¡ä½œä¸šã€‚
- en: Nothing is perfect! But some are way worse than others for sure.
  id: totrans-split-12
  prefs: []
  type: TYPE_NORMAL
  zh: äº‹ç‰©æ— å®Œç¾ï¼ä½†æœ‰äº›æ¯”å…¶ä»–çš„ç¡®ç³Ÿç³•å¾—å¤šã€‚
- en: The most frustrating part? Itâ€™s almost impossible to really tell ahead of time,
    especially in the frenzy of everything, what kind of hardware one was going to
    get and how robust/fault-tolerant the experience would be.
  id: totrans-split-13
  prefs: []
  type: TYPE_NORMAL
  zh: æœ€ä»¤äººæ²®ä¸§çš„éƒ¨åˆ†ï¼Ÿå‡ ä¹ä¸å¯èƒ½æå‰çŸ¥é“ï¼Œç‰¹åˆ«æ˜¯åœ¨ä¸€åˆ‡éƒ½éå¸¸ç¹å¿™çš„æ—¶å€™ï¼Œäººä»¬å°†ä¼šå¾—åˆ°ä»€ä¹ˆæ ·çš„ç¡¬ä»¶ï¼Œä»¥åŠä½“éªŒå°†æœ‰å¤šä¹ˆå¼ºå¤§/å®¹é”™ã€‚
- en: On top of that, you also have no way of telling if the vendor just fails to
    deliver on time and delays the shipment by a couple of months, leaving one stranded
    without being able to procure from other sources for weeks or months. Some providers
    would also accidentally delete your checkpoint Â¯\_(ãƒ„)_/Â¯.
  id: totrans-split-14
  prefs: []
  type: TYPE_NORMAL
  zh: é™¤æ­¤ä¹‹å¤–ï¼Œæ‚¨ä¹Ÿæ— æ³•ç¡®å®šä¾›åº”å•†æ˜¯å¦ä»…ä»…æ˜¯ä¸èƒ½æŒ‰æ—¶äº¤ä»˜å¹¶å»¶è¿Ÿäº†å‡ ä¸ªæœˆçš„å‘è´§ï¼Œè®©äººä¸€åº¦æ— æ³•ä»å…¶ä»–æ¥æºé‡‡è´­æ•°å‘¨æˆ–æ•°æœˆã€‚æœ‰äº›ä¾›åº”å•†ä¹Ÿä¼šæ„å¤–åœ°åˆ é™¤æ‚¨çš„æ£€æŸ¥ç‚¹ Â¯\_(ãƒ„)_/Â¯ã€‚
- en: Did I mention youâ€™ll also get a different Model Flop Utilisation (MFU) for different
    clusters!? This was a non negligible amount of compute wasted if one is unlucky
    enough to find a provider with badly cabled nodes or some other issues. Systems
    with very sub-optimal file systems would have the MFU of training runs tank the
    moment a team mate starts transferring large amounts of data across clusters.
  id: totrans-split-15
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘æ˜¯å¦æåˆ°è¿‡ï¼Œæ‚¨è¿˜å°†åœ¨ä¸åŒçš„é›†ç¾¤ä¸­è·å¾—ä¸åŒçš„æ¨¡å‹æµ®ç‚¹æ“ä½œåˆ©ç”¨ç‡ï¼ˆMFUï¼‰ï¼ï¼Ÿå¦‚æœä¸å¹¸åœ°æ‰¾åˆ°æœ‰ç³Ÿç³•å¸ƒçº¿çš„èŠ‚ç‚¹æˆ–å…¶ä»–é—®é¢˜çš„ä¾›åº”å•†ï¼Œè¿™å°†å¯¼è‡´ç›¸å½“å¤§é‡çš„è®¡ç®—èµ„æºæµªè´¹ã€‚æ‹¥æœ‰éå¸¸æ¬¡ä¼˜æ–‡ä»¶ç³»ç»Ÿçš„ç³»ç»Ÿä¼šåœ¨é˜Ÿå‹å¼€å§‹åœ¨é›†ç¾¤ä¹‹é—´ä¼ è¾“å¤§é‡æ•°æ®æ—¶ï¼Œè®­ç»ƒè¿è¡Œçš„
    MFU ç¬é—´ä¸‹é™ã€‚
- en: Every service provider also had different levels of support. These range from
    being polite to nonchalant, â€œchatgpt-styleâ€ canned responses to blaming the user
    for every single thing that goes wrong.
  id: totrans-split-16
  prefs: []
  type: TYPE_NORMAL
  zh: æ¯ä¸ªæœåŠ¡æä¾›å•†çš„æ”¯æŒæ°´å¹³ä¹Ÿå„ä¸ç›¸åŒã€‚ä»å½¬å½¬æœ‰ç¤¼åˆ°æ¼«ä¸ç»å¿ƒï¼Œâ€œChatGPT é£æ ¼â€çš„æœºæ¢°å›å¤ï¼Œå†åˆ°æŠŠæ¯ä¸€ä¸ªå‡ºé”™çš„äº‹æƒ…éƒ½å½’å’äºç”¨æˆ·ã€‚
- en: Overall, every single cluster we tried feels like they have their own vibe,
    struggles and failure modes. It was also almost as though every single cluster
    needed their own hot-fixes for their own set of issues - some more tolerable than
    others. That said, weâ€™ve learned that fail safes are important, and finding fast
    hot fixes for any clusters could be key.
  id: totrans-split-17
  prefs: []
  type: TYPE_NORMAL
  zh: æ€»çš„æ¥è¯´ï¼Œæˆ‘ä»¬è¯•è¿‡çš„æ¯ä¸€ä¸ªé›†ç¾¤éƒ½æ„Ÿè§‰å®ƒä»¬æœ‰è‡ªå·±çš„æ°›å›´ã€æŒ£æ‰å’Œå¤±è´¥æ¨¡å¼ã€‚å‡ ä¹æ¯ä¸€ä¸ªé›†ç¾¤ä¼¼ä¹éƒ½éœ€è¦é’ˆå¯¹å…¶è‡ªèº«é—®é¢˜çš„å³æ—¶ä¿®å¤ - æœ‰äº›é—®é¢˜æ¯”å…¶ä»–é—®é¢˜æ›´å®¹å¿ã€‚è¯è™½å¦‚æ­¤ï¼Œæˆ‘ä»¬å·²ç»å­¦åˆ°äº†æ•…éšœä¿æŠ¤çš„é‡è¦æ€§ï¼Œä»¥åŠä¸ºä»»ä½•é›†ç¾¤æ‰¾åˆ°å¿«é€Ÿä¿®å¤çš„å…³é”®æ€§ã€‚
- en: In the past couple of months, weâ€™ve built so much just to make sure things are
    usable, e.g., tooling around monitoring, efficient checkpointing, and various
    other optimisations even to the extent of installing our custom file system for
    scalable data storage - and what is just the tip of the iceberg of what is actually
    required.
  id: totrans-split-18
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿‡å»çš„å‡ ä¸ªæœˆä¸­ï¼Œæˆ‘ä»¬åšäº†å¾ˆå¤šå·¥ä½œï¼Œåªæ˜¯ä¸ºäº†ç¡®ä¿äº‹æƒ…æ˜¯å¯ç”¨çš„ï¼Œä¾‹å¦‚ï¼Œå›´ç»•ç›‘æ§ã€é«˜æ•ˆçš„æ£€æŸ¥ç‚¹ç®¡ç†ä»¥åŠå„ç§å…¶ä»–ä¼˜åŒ–æªæ–½ï¼Œç”šè‡³åŒ…æ‹¬å®‰è£…æˆ‘ä»¬è‡ªå®šä¹‰çš„æ–‡ä»¶ç³»ç»Ÿä»¥å®ç°å¯æ‰©å±•çš„æ•°æ®å­˜å‚¨
    - è¿™åªæ˜¯å®é™…æ‰€éœ€çš„å†°å±±ä¸€è§’ã€‚
- en: These combinations of toolings resulted in a nontrivial amount of MFU improvements
    while also minimising downtime in the face of terrible hardware.
  id: totrans-split-19
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™äº›å·¥å…·çš„ç»„åˆå¯¼è‡´äº†æ¨¡å‹æµ®ç‚¹æ“ä½œåˆ©ç”¨ç‡çš„æ˜¾è‘—æé«˜ï¼ŒåŒæ—¶åœ¨é¢å¯¹ç³Ÿç³•çš„ç¡¬ä»¶æ—¶ï¼Œæœ€å°åŒ–äº†åœæœºæ—¶é—´ã€‚
- en: '**On GPUs vs TPUs**'
  id: totrans-split-20
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '**å…³äº GPU vs TPU**'
- en: Weâ€™re training our models on GPUs for the most part at Reka. Personally, Iâ€™ve
    used TPUs all my life when it comes to large language model training at Google
    pre-Reka life. CUDA and *nccl* were the most alien thing to me ever. (I only learned
    itâ€™s pronounced â€œNickelâ€ from one of my coworkers who used to work at Nvidia lol)
  id: totrans-split-21
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬åœ¨ Reka å¤§éƒ¨åˆ†æ—¶é—´éƒ½åœ¨ä½¿ç”¨ GPU è®­ç»ƒæˆ‘ä»¬çš„æ¨¡å‹ã€‚å°±ä¸ªäººè€Œè¨€ï¼Œåœ¨ Reka ä¹‹å‰çš„ç”Ÿæ´»ä¸­ï¼Œå½“æ¶‰åŠåˆ° Google å¤§å‹è¯­è¨€æ¨¡å‹çš„è®­ç»ƒæ—¶ï¼Œæˆ‘ä¸€ç›´ä½¿ç”¨
    TPUsã€‚CUDA å’Œ *nccl* å¯¹æˆ‘æ¥è¯´æ˜¯æœ€é™Œç”Ÿçš„ä¸œè¥¿ã€‚ï¼ˆæˆ‘åªæ˜¯ä»ä¸€ä¸ªæ›¾åœ¨ Nvidia å·¥ä½œè¿‡çš„åŒäº‹é‚£é‡Œå­¦åˆ°å®ƒçš„å‘éŸ³æ˜¯â€œNickelâ€ï¼Œå“ˆå“ˆï¼‰
- en: I was completely taken aback by the failure rate of GPUs as opposed to my experiences
    on TPUs at Google. In fact, I donâ€™t actually recall TPUs failing much even for
    large runs, though *I was not sure if I was protected from knowing this just by
    the sheer robustness of the outrageously good infra and having a dedicated hardware
    team*. In fact, the [UL2 20B](https://blog.research.google/2022/10/ul2-20b-open-source-unified-language.html)
    model (at Google) was trained by leaving the job running accidentally for a month.
    It never failed. If this were in GPU land, it would have failed within the first
    few days for sure.
  id: totrans-split-22
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹äº GPU çš„æ•…éšœç‡ï¼Œä¸æˆ‘åœ¨ Google çš„ TPU ä¸Šçš„ç»éªŒå½¢æˆäº†é²œæ˜å¯¹æ¯”ï¼Œè®©æˆ‘å½»åº•åƒæƒŠäº†ã€‚äº‹å®ä¸Šï¼Œæˆ‘å®é™…ä¸Šå‡ ä¹ä¸è®°å¾— TPUs åœ¨å¤§è§„æ¨¡è¿è¡Œä¸­å‡ºç°è¿‡æ•…éšœï¼Œè™½ç„¶*æˆ‘å¹¶ä¸ç¡®å®šè¿™æ˜¯å¦ä»…ä»…æ˜¯ç”±äºæä¸ºå¼ºå¤§çš„åŸºç¡€è®¾æ–½å’Œä¸“é—¨çš„ç¡¬ä»¶å›¢é˜Ÿçš„ç»ä½³éŸ§æ€§æ‰€ä¿æŠ¤è€Œå·²*ã€‚å®é™…ä¸Šï¼Œåœ¨
    Googleï¼Œ[UL2 20B](https://blog.research.google/2022/10/ul2-20b-open-source-unified-language.html)
    æ¨¡å‹æ˜¯å› ä¸ºæ„å¤–åœ°è®©å·¥ä½œè¿è¡Œäº†ä¸€ä¸ªæœˆè€Œè¢«è®­ç»ƒçš„ã€‚å®ƒä»æœªå¤±è´¥è¿‡ã€‚å¦‚æœè¿™æ˜¯åœ¨ GPU é¢†åŸŸï¼Œå®ƒè‚¯å®šä¼šåœ¨æœ€åˆå‡ å¤©å†…å¤±è´¥ã€‚
- en: That said, I think this could be more about the competency of the hardware team
    that manages your accelerators rather than the underlying chip. The presence of
    having good hardware support (from your compute provider) is important. And so
    much hinges on them being actually competent, reinforcing the notion of the â€œhardware
    lotteryâ€.
  id: totrans-split-23
  prefs: []
  type: TYPE_NORMAL
  zh: è¯´å®è¯ï¼Œæˆ‘è®¤ä¸ºè¿™æ›´å¤šåœ°ä¸ç®¡ç†åŠ é€Ÿå™¨çš„ç¡¬ä»¶å›¢é˜Ÿçš„èƒ½åŠ›æœ‰å…³ï¼Œè€Œä¸æ˜¯åº•å±‚èŠ¯ç‰‡çš„é—®é¢˜ã€‚æœ‰ä¸€ä¸ªè‰¯å¥½çš„ç¡¬ä»¶æ”¯æŒï¼ˆæ¥è‡ªæ‚¨çš„è®¡ç®—æä¾›å•†ï¼‰æ˜¯å¾ˆé‡è¦çš„ã€‚å› æ­¤ï¼Œå¾ˆå¤§ç¨‹åº¦ä¸Šå–å†³äºä»–ä»¬æ˜¯å¦çœŸæ­£
    kompetentï¼Œè¿™åŠ å¼ºäº†â€œç¡¬ä»¶æŠ½å¥–â€çš„æ¦‚å¿µã€‚
- en: GPU land feels strange. It feels like multinode training is more of an afterthought
    as opposed to distributed training as a first class citizen on TPU pods. In GPU
    land, it feels as if different providers cable them in different ways to enable
    multi-node training which leads to the high variance across how things are done
    at different places. Iâ€™m no expert in hardware though but this is the impression
    I get.
  id: totrans-split-24
  prefs: []
  type: TYPE_NORMAL
  zh: GPUé¢†åŸŸæ„Ÿè§‰å¾ˆå¥‡æ€ªã€‚åœ¨GPUé¢†åŸŸï¼Œå®ƒä¼¼ä¹æ›´åƒæ˜¯å¤šèŠ‚ç‚¹è®­ç»ƒä½œä¸ºTPU podä¸Šçš„åˆ†å¸ƒå¼è®­ç»ƒçš„é™„å¸¦æ€æƒ³ã€‚åœ¨GPUé¢†åŸŸï¼Œä¼¼ä¹ä¸åŒçš„æä¾›å•†ä»¥ä¸åŒçš„æ–¹å¼å¯¹å®ƒä»¬è¿›è¡Œå¸ƒçº¿ï¼Œä»¥å®ç°å¤šèŠ‚ç‚¹è®­ç»ƒï¼Œè¿™å¯¼è‡´äº†åœ¨ä¸åŒåœ°æ–¹å®Œæˆä»»åŠ¡çš„é«˜å˜å¼‚æ€§ã€‚è™½ç„¶æˆ‘ä¸æ˜¯ç¡¬ä»¶ä¸“å®¶ï¼Œä½†è¿™æ˜¯æˆ‘å¾—åˆ°çš„å°è±¡ã€‚
- en: '**The pain of multi-cluster setups**'
  id: totrans-split-25
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '**å¤šé›†ç¾¤è®¾ç½®çš„ç—›è‹¦**'
- en: I spent most of my professional career on Google infra, which ran mostly on
    [Borg](https://research.google/pubs/large-scale-cluster-management-at-google-with-borg/),
    [Xmanager](https://github.com/google-deepmind/xmanager) and [Colossus](https://cloud.google.com/blog/products/storage-data-transfer/a-peek-behind-colossus-googles-file-system).
    where everything can be assessed from literally anywhere. Therefore, the concept
    of having to actually set up new environments in different clusters is a foreign
    one to me.
  id: totrans-split-26
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘çš„å¤§éƒ¨åˆ†èŒä¸šç”Ÿæ¶¯éƒ½èŠ±åœ¨äº†è°·æ­ŒåŸºç¡€è®¾æ–½ä¸Šï¼Œä¸»è¦è¿è¡Œåœ¨ [Borg](https://research.google/pubs/large-scale-cluster-management-at-google-with-borg/)ã€[Xmanager](https://github.com/google-deepmind/xmanager)
    å’Œ [Colossus](https://cloud.google.com/blog/products/storage-data-transfer/a-peek-behind-colossus-googles-file-system)
    ä¸Šã€‚è¿™äº›ä¸œè¥¿å‡ ä¹å¯ä»¥ä»ä»»ä½•åœ°æ–¹è¿›è¡Œè¯„ä¼°ã€‚å› æ­¤ï¼Œæˆ‘å¯¹äºå®é™…åœ¨ä¸åŒé›†ç¾¤ä¸­è®¾ç½®æ–°ç¯å¢ƒçš„æ¦‚å¿µæ„Ÿåˆ°é™Œç”Ÿã€‚
- en: In the current world, having multiple clusters of accelerator pools seems inevitable
    unless one specially builds for a large number in a single location. More specifically,
    GPU supply (or lack thereof) also naturally resulted in this pattern of cluster
    procurement in which things are fragmented by nature. Training large models also
    require large terabytes of data which can create a lot of inconvenience even just
    moving them around. Meanwhile, replicating data is generally also not straightforward
    and prohibitive at extremely large scales.
  id: totrans-split-27
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨å½“å‰ä¸–ç•Œä¸­ï¼Œé™¤éä¸“é—¨ä¸ºå•ä¸€ä½ç½®ä¸­çš„å¤§é‡æ•°é‡æ„å»ºï¼Œå¦åˆ™æ‹¥æœ‰å¤šä¸ªåŠ é€Ÿå™¨æ± é›†ç¾¤ä¼¼ä¹æ˜¯ä¸å¯é¿å…çš„ã€‚æ›´å…·ä½“åœ°è¯´ï¼ŒGPUä¾›åº”ï¼ˆæˆ–å…¶ç¼ºä¹ï¼‰ä¹Ÿè‡ªç„¶åœ°å¯¼è‡´äº†è¿™ç§é›†ç¾¤é‡‡è´­æ¨¡å¼ï¼Œè‡ªç„¶åœ°äº‹ç‰©è¢«ç¢ç‰‡åŒ–ã€‚è®­ç»ƒå¤§å‹æ¨¡å‹ä¹Ÿéœ€è¦å¤§é‡çš„æ•°æ®ï¼Œå³ä½¿ä»…ä»…æ˜¯ç§»åŠ¨å®ƒä»¬ä¹Ÿä¼šå¸¦æ¥å¾ˆå¤šä¸ä¾¿ã€‚åŒæ—¶ï¼Œå¤åˆ¶æ•°æ®é€šå¸¸ä¹Ÿä¸ç›´æ¥å¹¶ä¸”åœ¨æå¤§è§„æ¨¡ä¸Šæ˜¯ç¦é”¢çš„ã€‚
- en: Obviously, the ideal case here is some kind of orchestration layer that is specially
    built to send jobs to different servers. I believe many big AI-first companies
    generally have some sort of infrastructure in place to improve the quality of
    life for AI researchers. However, building this type of sophisticated & fancy
    ML training infrastructure is not really possible for a lean and new startup at
    the beginning.
  id: totrans-split-28
  prefs: []
  type: TYPE_NORMAL
  zh: æ˜¾ç„¶ï¼Œè¿™é‡Œçš„ç†æƒ³æƒ…å†µæ˜¯ä¸€ç§ç‰¹åˆ«è®¾è®¡çš„ç¼–æ’å±‚ï¼Œç”¨äºå‘ä¸åŒçš„æœåŠ¡å™¨å‘é€ä½œä¸šã€‚æˆ‘è®¤ä¸ºè®¸å¤šå¤§å‹ä»¥äººå·¥æ™ºèƒ½ä¸ºå…ˆçš„å…¬å¸é€šå¸¸éƒ½æœ‰æŸç§åŸºç¡€è®¾æ–½æ¥æé«˜AIç ”ç©¶äººå‘˜çš„ç”Ÿæ´»è´¨é‡ã€‚ç„¶è€Œï¼Œå¯¹äºä¸€ä¸ªåˆšåˆšèµ·æ­¥çš„ç²¾ç›Šå’Œæ–°å…´çš„åˆåˆ›å…¬å¸æ¥è¯´ï¼Œæ„å»ºè¿™ç§å¤æ‚å’ŒèŠ±å“¨çš„MLè®­ç»ƒåŸºç¡€è®¾æ–½å¹¶ä¸æ˜¯çœŸçš„å¯èƒ½ã€‚
- en: For now, we ended up developing many internal workflows to mitigate many of
    these issues and are continuing to move towards the gold standard of a world class
    experimentation infrastructure.
  id: totrans-split-29
  prefs: []
  type: TYPE_NORMAL
  zh: ç›®å‰ï¼Œæˆ‘ä»¬å¼€å‘äº†è®¸å¤šå†…éƒ¨å·¥ä½œæµæ¥å‡è½»è®¸å¤šè¿™äº›é—®é¢˜ï¼Œå¹¶ä¸”æ­£åœ¨ç»§ç»­å‘ä¸–ç•Œä¸€æµå®éªŒåŸºç¡€è®¾æ–½çš„é»„é‡‘æ ‡å‡†è¿ˆè¿›ã€‚
- en: (I was told this scrappy setup was more or less the norm for non top-tier /
    large companies).
  id: totrans-split-30
  prefs: []
  type: TYPE_NORMAL
  zh: ï¼ˆæˆ‘è¢«å‘ŠçŸ¥è¿™ç§ä»¤äººä¸å¿«çš„è®¾ç½®å¯¹éä¸€æµ/å¤§å…¬å¸æ¥è¯´æ›´åƒæ˜¯å¸¸æ€ï¼‰ã€‚
- en: '**Code in the wild**'
  id: totrans-split-31
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '**é‡å¤–ä»£ç **'
- en: It is no secret that my favourite codebase of all time is [T5X](https://github.com/google-research/t5x)
    and [Mesh Tensorflow](https://github.com/tensorflow/mesh) (named tensors ftw)
    but these options quickly became not viable as 1) they donâ€™t get as much support
    outside Google, 2) they are kind of deprecated and 3) they are not friendly to
    folks on our team that are not xooglers.
  id: totrans-split-32
  prefs: []
  type: TYPE_NORMAL
  zh: æ¯«æ— ç§˜å¯†ï¼Œæˆ‘æœ€å–œæ¬¢çš„æ‰€æœ‰æ—¶é—´ä»£ç åº“æ˜¯ [T5X](https://github.com/google-research/t5x) å’Œ [Mesh Tensorflow](https://github.com/tensorflow/mesh)ï¼ˆå‘½åå¼ é‡
    ftwï¼‰ï¼Œä½†è¿™äº›é€‰é¡¹å¾ˆå¿«å°±å˜å¾—ä¸å¯è¡Œäº†ï¼Œå› ä¸º 1ï¼‰å®ƒä»¬åœ¨è°·æ­Œä¹‹å¤–çš„æ”¯æŒä¸è¶³ï¼Œ2ï¼‰å®ƒä»¬æœ‰ç‚¹è¿‡æ—¶ï¼Œ3ï¼‰å®ƒä»¬å¯¹äºæˆ‘ä»¬å›¢é˜Ÿä¸­çš„é xoogler å¹¶ä¸å‹å¥½ã€‚
- en: We ended up going for something vanilla, seemingly stable and more popular (i.e.,
    pytorch) that is more accessible to most people on the team (except me lol). In
    my first few months, I was tripping all over pip, git, docker and all these wild
    life stuff. Then again, I am not 100% sure about how stable or user friendly it
    would be to use a google codebase externally (it would have been pretty nasty
    I guess).
  id: totrans-split-33
  prefs: []
  type: TYPE_NORMAL
  zh: æœ€ç»ˆï¼Œæˆ‘ä»¬é€‰æ‹©äº†ä¸€äº›æ™®é€šçš„ã€çœ‹èµ·æ¥ç¨³å®šä¸”æ›´å—æ¬¢è¿çš„ä¸œè¥¿ï¼ˆå³pytorchï¼‰ï¼Œè¿™å¯¹å¤§å¤šæ•°å›¢é˜Ÿæˆå‘˜æ¥è¯´æ›´å®¹æ˜“æ¥è§¦ï¼ˆé™¤äº†æˆ‘å“ˆå“ˆï¼‰ã€‚åœ¨æˆ‘æœ€åˆçš„å‡ ä¸ªæœˆé‡Œï¼Œæˆ‘åœ¨pipã€gitã€dockerä»¥åŠæ‰€æœ‰è¿™äº›ç‹‚é‡çš„ä¸œè¥¿ä¸Šéƒ½æ‘”äº†å‡ ä¸ªè·Ÿå¤´ã€‚ä¸è¿‡ï¼Œæˆ‘å¹¶ä¸å®Œå…¨ç¡®å®šä½¿ç”¨Googleå¤–éƒ¨çš„ä»£ç åº“ä¼šæœ‰å¤šç¨³å®šæˆ–ç”¨æˆ·å‹å¥½ï¼ˆæˆ‘æƒ³é‚£å¯èƒ½ä¼šç›¸å½“ç³Ÿç³•ï¼‰ã€‚
- en: To be very frank, I would have to say the quality of codebases externally significantly
    lag behind those Iâ€™ve been used to at Google. Primarily because codebase within
    Google tends to be written by ML rockstars themselves (e.g, Noam Shazeer, Barret
    Zoph, Adam Roberts, Hyung Won Chung et al.) and just feel better (e.g., superior
    vibes) compared to those Iâ€™ve tried externally. In particular, I found myself
    super annoyed with the code quality when dabbling with stuff built by other companies
    (some way worse than others ğŸ¤—).
  id: totrans-split-34
  prefs: []
  type: TYPE_NORMAL
  zh: å¦ç‡åœ°è¯´ï¼Œæˆ‘å¿…é¡»è¯´å¤–éƒ¨ä»£ç åº“çš„è´¨é‡æ˜æ˜¾è½åäºæˆ‘åœ¨Googleä½¿ç”¨ä¹ æƒ¯çš„é‚£äº›ä»£ç åº“ã€‚ä¸»è¦æ˜¯å› ä¸ºGoogleå†…éƒ¨çš„ä»£ç åº“é€šå¸¸ç”±æœºå™¨å­¦ä¹ çš„æ˜æ˜Ÿä»¬è‡ªå·±ç¼–å†™ï¼ˆä¾‹å¦‚ï¼ŒNoam
    Shazeerï¼ŒBarret Zophï¼ŒAdam Robertsï¼ŒHyung Won Chungç­‰ï¼‰ï¼Œæ„Ÿè§‰æ¯”æˆ‘å°è¯•è¿‡çš„å¤–éƒ¨ä»£ç åº“è¦å¥½å¾—å¤šï¼ˆä¾‹å¦‚ï¼Œæ›´å‡ºè‰²çš„æ°›å›´ï¼‰ã€‚ç‰¹åˆ«æ˜¯å½“æˆ‘æ¶‰è¶³å…¶ä»–å…¬å¸æ„å»ºçš„ä¸œè¥¿æ—¶ï¼Œæˆ‘å‘ç°è‡ªå·±å¯¹ä»£ç è´¨é‡æ„Ÿåˆ°éå¸¸æ¼ç«ï¼ˆæœ‰äº›æ¯”å…¶ä»–å…¬å¸å·®å¾—å¤šğŸ¤—ï¼‰ã€‚
- en: Also, I never knew that the ability to change model parallelism was not automatic
    (for free) until some codebases required me to write a converter to change the
    parallelism of a model. Surely a WTF moment for me.
  id: totrans-split-35
  prefs: []
  type: TYPE_NORMAL
  zh: è€Œä¸”ï¼Œæˆ‘ä»æœªæ„è¯†åˆ°æ”¹å˜æ¨¡å‹å¹¶è¡Œæ€§çš„èƒ½åŠ›ä¸æ˜¯è‡ªåŠ¨çš„ï¼ˆå…è´¹ï¼‰ï¼Œç›´åˆ°æŸäº›ä»£ç åº“è¦æ±‚æˆ‘ç¼–å†™è½¬æ¢å™¨æ¥æ”¹å˜æ¨¡å‹çš„å¹¶è¡Œæ€§ã€‚å¯¹æˆ‘æ¥è¯´ç¡®å®æ˜¯ä¸€ä¸ªä»¤äººéœ‡æƒŠçš„æ—¶åˆ»ã€‚
- en: The other striking thing is how little support these codebases have for large
    scale encoder-decoder training or even prefixLM training. To that end, even flash
    attention has consistently declined to provide support for prefixLM training (i.e.,
    custom masks) despite reasonable demand on their github issues for whatever reason.
  id: totrans-split-36
  prefs: []
  type: TYPE_NORMAL
  zh: å¦ä¸€ä¸ªæ˜¾è‘—çš„äº‹å®æ˜¯ï¼Œè¿™äº›ä»£ç åº“å¯¹å¤§è§„æ¨¡ç¼–ç å™¨-è§£ç å™¨è®­ç»ƒç”šè‡³å‰ç¼€LMè®­ç»ƒçš„æ”¯æŒéå¸¸æœ‰é™ã€‚ä¸ºæ­¤ï¼Œå³ä½¿æ˜¯å¿«é—ªæ³¨æ„åŠ›æ¨¡å‹ä¹Ÿä¸€ç›´æ‹’ç»ä¸ºå‰ç¼€LMè®­ç»ƒï¼ˆå³è‡ªå®šä¹‰æ©ç ï¼‰æä¾›æ”¯æŒï¼Œå°½ç®¡åœ¨ä»–ä»¬çš„githubé—®é¢˜ä¸­æœ‰åˆç†çš„éœ€æ±‚ã€‚
- en: I know I should be using Jax. A friend just shamed me for using pytorch, but
    this is a startup and we decided to move fast. Iâ€™m sorry we would do better to
    be cooler next time. Iâ€™m not proud of this fact.
  id: totrans-split-37
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘çŸ¥é“æˆ‘åº”è¯¥ä½¿ç”¨Jaxã€‚ä¸€ä¸ªæœ‹å‹åˆšæ‰ç¾è¾±æˆ‘ä½¿ç”¨pytorchï¼Œä½†è¿™æ˜¯ä¸€å®¶åˆåˆ›å…¬å¸ï¼Œæˆ‘ä»¬å†³å®šå¿«é€Ÿå‰è¿›ã€‚å¯¹ä¸èµ·ï¼Œæˆ‘ä»¬ä¸‹æ¬¡ä¼šæ›´é…·ä¸€ç‚¹ã€‚æˆ‘å¯¹è¿™ä¸ªäº‹å®å¹¶ä¸æ„Ÿåˆ°è‡ªè±ªã€‚
- en: '**Less principled, more Yolo**'
  id: totrans-split-38
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '**ä¸é‚£ä¹ˆåŸåˆ™ï¼Œæ›´å¤šYolo**'
- en: Scaling models systematically generally requires one to go from small to large
    in a principled way, i.e., run experiments in multiple phrases (1B->8B->64B->300B
    etc) and pick the winners and continuously scale them up. In a startup, we had
    way less compute to perform these massive sweeps to check hparams. In the end,
    we had to work with many [Yolo runs](https://twitter.com/_jasonwei/status/1757486124082303073)
    (that fortunately turned out well).
  id: totrans-split-39
  prefs: []
  type: TYPE_NORMAL
  zh: ç³»ç»Ÿåœ°æ‰©å±•æ¨¡å‹é€šå¸¸è¦æ±‚æˆ‘ä»¬ä»¥åŸåˆ™æ€§çš„æ–¹å¼ä»å°è§„æ¨¡åˆ°å¤§è§„æ¨¡è¿è¡Œå®éªŒï¼Œå³åœ¨å¤šä¸ªé˜¶æ®µè¿è¡Œå®éªŒï¼ˆ1B->8B->64B->300Bç­‰ï¼‰ï¼Œé€‰æ‹©ä¼˜èƒœè€…å¹¶æŒç»­æ‰©å±•å®ƒä»¬ã€‚åœ¨åˆåˆ›ä¼ä¸šä¸­ï¼Œæˆ‘ä»¬æ²¡æœ‰è¶³å¤Ÿçš„è®¡ç®—èƒ½åŠ›æ¥æ‰§è¡Œè¿™äº›å¤§è§„æ¨¡çš„å‚æ•°æ‰«æã€‚æœ€ç»ˆï¼Œæˆ‘ä»¬ä¸å¾—ä¸ä¾é è®¸å¤š[Yolo
    runs](https://twitter.com/_jasonwei/status/1757486124082303073)ï¼ˆå¹¸è¿çš„æ˜¯ç»“æœè¿˜ä¸é”™ï¼‰ã€‚
- en: In the end it took us only a very small number of smaller scale & shorter ablation
    runs to get to the strong 21B Reka Flash and 7B edge model (and also our upcoming
    largest core model). Finding a solid recipe with a very limited number of runs
    is challenging and requires changing many variables at once given the ridiculously
    enormous search space. In order to do this, one has to abandon the systematicity
    of Bigtech and rely a lot on â€œYoloâ€, gut feeling and instinct.
  id: totrans-split-40
  prefs: []
  type: TYPE_NORMAL
  zh: æœ€åï¼Œæˆ‘ä»¬åªéœ€è¦è¿›è¡Œäº†å°‘é‡è¾ƒå°è§„æ¨¡å’Œè¾ƒçŸ­çš„å‰¥èš€è¿è¡Œï¼Œå°±è¾¾åˆ°äº†å¼ºå¤§çš„21B Reka Flashå’Œ7Bè¾¹ç¼˜æ¨¡å‹ï¼ˆä»¥åŠå³å°†æ¨å‡ºçš„æœ€å¤§æ ¸å¿ƒæ¨¡å‹ï¼‰ã€‚åœ¨æå…¶æœ‰é™çš„è¿è¡Œæ¬¡æ•°å†…æ‰¾åˆ°ä¸€ä¸ªå¯é çš„é…æ–¹æ˜¯å…·æœ‰æŒ‘æˆ˜æ€§çš„ï¼Œéœ€è¦åŒæ—¶æ”¹å˜è®¸å¤šå˜é‡ï¼Œé‰´äºæœç´¢ç©ºé—´æå…¶åºå¤§ã€‚ä¸ºäº†åšåˆ°è¿™ä¸€ç‚¹ï¼Œä¸€ä¸ªäººå¿…é¡»æ”¾å¼ƒBigtechçš„ç³»ç»Ÿæ€§ï¼Œå¹¶ä¸”éå¸¸ä¾èµ–â€œYoloâ€ã€ç›´è§‰å’Œæœ¬èƒ½ã€‚
- en: Thankfully, I (and many of us in the team) have built up this intuition quite
    a bit in our ML careers to get it right within a substantially short amount of
    tries. While weâ€™ve trained really good models before in our previous jobs, differences
    in training infrastructure, data, incorporation of new ideas and other environmental
    issues can still cause non-trivial differences in outcomes. That said, a strong
    prior helps to significantly cut down the search space and is probably one of
    the easiest explanations to why we were able to train really strong models with
    so few trials, resources and experimentation.
  id: totrans-split-41
  prefs: []
  type: TYPE_NORMAL
  zh: å¹¸è¿çš„æ˜¯ï¼Œæˆ‘ï¼ˆä»¥åŠå›¢é˜Ÿä¸­çš„è®¸å¤šäººï¼‰åœ¨æˆ‘ä»¬çš„æœºå™¨å­¦ä¹ èŒä¸šç”Ÿæ¶¯ä¸­ç§¯ç´¯äº†ç›¸å½“å¤šçš„ç›´è§‰ï¼Œä½¿æˆ‘ä»¬èƒ½å¤Ÿåœ¨ç›¸å½“çŸ­çš„å°è¯•æ¬¡æ•°å†…å°±åšå¯¹ã€‚è™½ç„¶æˆ‘ä»¬ä»¥å‰åœ¨æˆ‘ä»¬çš„å‰èŒä¸­è®­ç»ƒè¿‡éå¸¸å¥½çš„æ¨¡å‹ï¼Œä½†æ˜¯è®­ç»ƒåŸºç¡€è®¾æ–½ã€æ•°æ®ã€æ–°æƒ³æ³•çš„æ•´åˆä»¥åŠå…¶ä»–ç¯å¢ƒé—®é¢˜çš„å·®å¼‚ä»ç„¶å¯èƒ½å¯¼è‡´ç»“æœä¸Šçš„éå¸¸è§„å·®å¼‚ã€‚å°½ç®¡å¦‚æ­¤ï¼Œå¼ºå¤§çš„å…ˆéªŒçŸ¥è¯†æœ‰åŠ©äºæ˜¾è‘—å‡å°‘æœç´¢ç©ºé—´ï¼Œå¯èƒ½æ˜¯æˆ‘ä»¬èƒ½å¤Ÿç”¨å¦‚æ­¤å°‘çš„å°è¯•ã€èµ„æºå’Œå®éªŒæ¥è®­ç»ƒéå¸¸å¼ºå¤§çš„æ¨¡å‹çš„æœ€ç®€å•è§£é‡Šä¹‹ä¸€ã€‚
- en: '**In a nutshell**'
  id: totrans-split-42
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '**ç®€è€Œè¨€ä¹‹**'
- en: Figuring out things in the wilderness was an interesting experience. It was
    unfortunately not painless. Compute scarcity and also unreliable compute providers
    made things significantly harder than expected but weâ€™re glad we pulled through
    with brute technical strength.
  id: totrans-split-43
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è’é‡ä¸­æ‘¸ç´¢æ˜¯ä¸€æ¬¡æœ‰è¶£çš„ç»å†ã€‚ä¸å¹¸çš„æ˜¯ï¼Œè¿™å¹¶ä¸æ˜¯æ¯«æ— ç—›è‹¦çš„ã€‚è®¡ç®—èµ„æºç¨€ç¼ºä»¥åŠä¸å¯é çš„è®¡ç®—æœåŠ¡æä¾›å•†ä½¿äº‹æƒ…å˜å¾—æ¯”é¢„æœŸçš„æ›´åŠ å›°éš¾ï¼Œä½†æˆ‘ä»¬å¾ˆé«˜å…´æˆ‘ä»¬å‡­å€Ÿå¼ºå¤§çš„æŠ€æœ¯å®åŠ›åº¦è¿‡äº†éš¾å…³ã€‚
- en: All in all, this is only a small part of the story of how we started a company,
    raised some money, bought some chips and matched Gemini pro/GPT 3.5 and outperformed
    many others in less than a year having to build everything from scratch.
  id: totrans-split-44
  prefs: []
  type: TYPE_NORMAL
  zh: æ€»çš„æ¥è¯´ï¼Œè¿™åªæ˜¯æˆ‘ä»¬å¼€å§‹å…¬å¸ã€ç­¹é›†èµ„é‡‘ã€è´­ä¹°èŠ¯ç‰‡ä»¥åŠåŒ¹é… Gemini pro/GPT 3.5 å¹¶åœ¨ä¸åˆ°ä¸€å¹´çš„æ—¶é—´é‡Œèƒœè¿‡è®¸å¤šå…¶ä»–å…¬å¸çš„æ•…äº‹çš„ä¸€å°éƒ¨åˆ†ï¼Œä¸å¾—ä¸ä»é›¶å¼€å§‹æ„å»ºä¸€åˆ‡ã€‚
- en: Thereâ€™s still more to write about, data pipelines, human evaluation, etc, but
    this is already a very long post. More next time. Send @YitayML DMs on X for feedback!
  id: totrans-split-45
  prefs: []
  type: TYPE_NORMAL
  zh: è¿˜æœ‰æ›´å¤šå†…å®¹éœ€è¦å†™ï¼Œæ•°æ®ç®¡é“ã€äººå·¥è¯„ä¼°ç­‰ç­‰ï¼Œä½†è¿™ç¯‡æ–‡ç« å·²ç»éå¸¸é•¿äº†ã€‚ä¸‹æ¬¡å†è¯´ã€‚ç»™ @YitayML å‘é€ X ä¸Šçš„ç§ä¿¡ä»¥è·å¾—åé¦ˆï¼
- en: '**Acknowledgements**'
  id: totrans-split-46
  prefs: []
  type: TYPE_NORMAL
  zh: '**è‡´è°¢**'
- en: I thank Dani Yogatama, Piotr Padlewski, Matthew Henderson, Donovan Ong, Che
    Zheng, Aitor Ormazabal, Deyu Fu, on the Reka team for feedback on this post.
  id: totrans-split-47
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘è¦æ„Ÿè°¢ Dani Yogatamaã€Piotr Padlewskiã€Matthew Hendersonã€Donovan Ongã€Che Zhengã€Aitor
    Ormazabalã€å‚…å¾·å®‡ï¼Œåœ¨ Reka å›¢é˜Ÿå¯¹è¿™ç¯‡æ–‡ç« çš„åé¦ˆã€‚
- en: I also thank external folks, Jason Wei, Hyung Won Chung, Vinh Tran & Mostafa
    Dehghani for feedback on this post.
  id: totrans-split-48
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘è¿˜è¦æ„Ÿè°¢å¤–éƒ¨çš„æœ‹å‹ä»¬ï¼ŒJason Weiã€Hyung Won Chungã€Vinh Tran å’Œ Mostafa Dehghaniï¼Œå¯¹è¿™ç¯‡æ–‡ç« çš„åé¦ˆã€‚
