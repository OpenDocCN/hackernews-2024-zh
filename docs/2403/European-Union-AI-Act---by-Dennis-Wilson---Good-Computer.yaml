- en: <!--yml
  id: totrans-split-0
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  id: totrans-split-1
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-05-27 15:04:59'
  id: totrans-split-2
  prefs: []
  type: TYPE_NORMAL
- en: -->
  id: totrans-split-3
  prefs: []
  type: TYPE_NORMAL
- en: European Union AI Act - by Dennis Wilson - Good Computer
  id: totrans-split-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://goodcomputer.substack.com/p/european-union-ai-act](https://goodcomputer.substack.com/p/european-union-ai-act)
  id: totrans-split-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[PRE0]'
  id: totrans-split-6
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: How do you govern a technology that is ill-defined, rapidly changing, maybe
    capable of destroying all of humanity, and maybe capable of greatly aiding it?
    Do you ban it, ignoring possible benefits, or do you let it develop without guardrails,
    disregarding the damage along the way?
  id: totrans-split-7
  prefs: []
  type: TYPE_NORMAL
- en: Last week, the European Union took a major step towards answering these questions
    with the [EU AI Act](https://artificialintelligenceact.eu/). The act is designed
    to protect the rights of individuals and ensure that AI is used in a way that
    is safe and respects human rights. In this post, I'll look at the key ideas of
    this act and argue that it is a good base framework for future governance.
  id: totrans-split-8
  prefs: []
  type: TYPE_NORMAL
- en: First, what even is AI, according to the EU?
  id: totrans-split-9
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  id: totrans-split-10
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Nearly every word in this definition could be (and has been and will be) debated,
    but I'd like to focus on the scope. The act is designed to be future-proof, and
    the definition encompasses a wide range of technologies. An [earlier definition](https://eur-lex.europa.eu/legal-content/EN/TXT/?uri=CELEX%3A52021PC0206)
    relied on an explicit list of techniques from "machine learning approaches" to
    "Bayesian estimation,” [scaring some statisticians](https://statmodeling.stat.columbia.edu/2021/04/22/eu-proposing-to-regulate-the-use-of-bayesian-estimation/)
    who learned suddenly that they were doing AI. We have no idea what techniques
    AI will use in the future or what weird names we'll use for them, so having an
    explicit list isn't a great idea. The final definition is instead open, and maybe
    even too open - does a database lookup that generates different outputs based
    on inputs qualify as AI now? Well, if the private sector and news hype cycles
    are to be believed, [everything is now AI](https://www.theverge.com/2024/1/13/24035152/ces-generative-ai-hype-robots),
    even things that are [very clearly not AI](https://www.cnbc.com/2019/03/06/40-percent-of-ai-start-ups-in-europe-not-related-to-ai-mmc-report.html).
    A broad legal definition is better at this stage, as a wider base will hold up
    more legislation that can refine it.
  id: totrans-split-11
  prefs: []
  type: TYPE_NORMAL
- en: 'The act is much more explicit when it comes to applications of AI. The act
    is risk-based, meaning that the level of regulation will depend on the level of
    risk associated with the technology. Risk is defined as "the combination of the
    probability of an occurrence of harm and the severity of that harm," and is broken
    down into four categories: Unacceptable, High, Low, and Minimal Risk. Unacceptable
    risk systems are prohibited, high risk systems are heavily regulated, low risk
    systems mostly have transparency requirements, and minimal risk systems are pretty
    much unregulated.'
  id: totrans-split-12
  prefs: []
  type: TYPE_NORMAL
- en: Banning applications is pretty heavy handed and is reserved for only the most
    risky of cases. Examples include [social scoring systems](https://www.technologyreview.com/2022/11/22/1063605/china-announced-a-new-social-credit-law-what-does-it-mean/),
    [assessing an individual's criminal risk](https://www.wired.com/story/doj-predictive-policing-lawmakers-demand/),
    [facial recognition](https://www.cnil.fr/en/facial-recognition-20-million-euros-penalty-against-clearview-ai),
    [biometric categorization like racial profiling](https://arstechnica.com/information-technology/2016/03/facebooks-ad-platform-now-guesses-at-your-race-based-on-your-behavior/),
    and [emotion recognition in workplaces](https://patents.google.com/patent/US10496947B1/en)
    or [educational institutions](https://www.sciencedirect.com/science/article/abs/pii/S036013152200032X).
    These aren't abstract examples; these are all technologies that have been developed
    or used. I guess I’ll have to look for alternatives to [video monitoring](https://www.youtube.com/watch?v=UHdrxHPRBng)
    of my students’ faces or [making them](https://www.nature.com/articles/s41539-023-00162-1)
    wear [brain-wave trackers](https://www.youtube.com/watch?v=JMLsHI8aV0g) to know
    if they are paying attention.
  id: totrans-split-13
  prefs: []
  type: TYPE_NORMAL
- en: High-risk AI systems are the second category and aren’t banned but are heavily
    regulated. These are systems that concern safety, are used in critical application,
    or profile individuals. Providers of these systems must prove that they are managing
    the high risk by providing documentation, designing systems to allow for human
    oversight, and being transparent about the system and its data. An example of
    this sort of system is automated hiring software, which is a [growing](https://www.findem.ai/)  [market](https://clickup.com/home-1)
    fraught with problems of [bias](https://www.bbc.com/worklife/article/20240214-ai-recruiting-hiring-software-bias-discrimination)
    and [a lack of transparency](https://www.forbes.com/sites/forbestechcouncil/2023/09/25/ai-bias-in-recruitment-ethical-implications-and-transparency/).
    Another example is medical devices, which face both [incredible potential gains](https://www.theparliamentmagazine.eu/news/article/hitech-health-how-artificial-intelligence-is-revolutionising-healthcare)
    from AI and [many potential risks](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC9908503/).
    These high-risk applications are the most likely to be the subject of future litigation,
    and the act provides a framework for this litigation by setting a standard of
    compliance for these systems.
  id: totrans-split-14
  prefs: []
  type: TYPE_NORMAL
- en: One point which was heavily debated is the [regulation of generative AI](https://www.nature.com/articles/d41586-024-00497-8)
    like ChatGPT. These technologies have been split into two tiers. The first tier
    covers all general-purpose models, except those used only in research or [published
    under an open-source license](https://www.euronews.com/next/2024/02/20/open-source-vs-closed-source-ai-whats-the-difference-and-why-does-it-matter).
    These will be subject to transparency requirements, including detailing their
    training methodologies and energy consumption, and must show that they [respect
    copyright laws](https://www.euronews.com/next/2024/01/09/openai-says-its-impossible-to-train-ai-without-copyrighted-materials).
    The copyright point has been the subject of [much ongoing litigation in the US](https://www.thefashionlaw.com/from-chatgpt-to-deepfake-creating-apps-a-running-list-of-key-ai-lawsuits/)
    and will prove problematic for companies like [OpenAI](https://www.nytimes.com/2023/12/27/business/media/new-york-times-open-ai-microsoft-lawsuit.html)
    and [Stable Diffusion](https://arstechnica.com/tech-policy/2023/04/stable-diffusion-copyright-lawsuits-could-be-a-legal-earthquake-for-ai/).
    The second and stricter tier will cover general-purpose models deemed to have
    "high-impact capabilities," which pose a higher "systemic risk", such as massive
    language models like ChatGPT. While preliminary, the handling of generative AI
    is impressively done, considering how quickly the technology has advanced during
    the development of this act.
  id: totrans-split-15
  prefs: []
  type: TYPE_NORMAL
  zh: 一个广受争议的问题是[生成型AI的监管](https://www.nature.com/articles/d41586-024-00497-8)，如ChatGPT。这些技术被分为两个层次。第一层包括所有除仅用于研究或[在开源许可下发布](https://www.euronews.com/next/2024/02/20/open-source-vs-closed-source-ai-whats-the-difference-and-why-does-it-matter)之外的通用模型。这些模型将受到透明度要求的约束，包括详细说明其训练方法和能源消耗，并必须证明它们[遵守版权法](https://www.euronews.com/next/2024/01/09/openai-says-its-impossible-to-train-ai-without-copyrighted-materials)。版权问题已成为[美国诸多持续进行的诉讼](https://www.thefashionlaw.com/from-chatgpt-to-deepfake-creating-apps-a-running-list-of-key-ai-lawsuits/)的争论焦点，并将对像[OpenAI](https://www.nytimes.com/2023/12/27/business/media/new-york-times-open-ai-microsoft-lawsuit.html)和[Stable
    Diffusion](https://arstechnica.com/tech-policy/2023/04/stable-diffusion-copyright-lawsuits-could-be-a-legal-earthquake-for-ai/)这样的公司造成问题。第二和更严格的层次将覆盖被认为具有“高影响能力”的通用模型，这些模型对“系统风险”造成更高的威胁，如ChatGPT这样的大型语言模型。尽管初步，处理生成型AI的方式在技术快速发展的背景下已经做得令人印象深刻。
- en: The act clearly follows, and is designed to work with, the [General Data Protection
    Regulation (GDPR)](https://gdpr.eu/), a law designed to protect the privacy and
    personal data of individuals in the EU, adopted in 2016 and effective starting
    2018\. Since 2018, the GDPR has been refined and interpreted through [court cases](https://www.fieldfisher.com/en/insights/the-cjeus-judgement-in-meta-platforms-inc-v-bundekartellamt-the-spotlight-on-lawful-bases-for-processing-data)
    and the application of [fines](https://www.laquadrature.net/2021/07/30/amende-de-746-millions-deuros-contre-amazon-suite-a-nos-plaintes-collectives/).
    For example, the protection of data transferred between the EU and the US has
    been [extensively](https://eur-lex.europa.eu/legal-content/EN/TXT/?uri=CELEX%3A62014CJ0362)  [debated](https://curia.europa.eu/juris/liste.jsf?num=C-311/18).
    The GDPR as written in 2016 was not the definitive stance on data privacy in the
    EU, but it was a necessary first step from which European agencies could act to
    protect individuals' privacy and data. Following it, many other countries have
    adopted or are considering [similar laws](https://biblio.ugent.be/publication/8726790/file/8726791.pdf).
  id: totrans-split-16
  prefs: []
  type: TYPE_NORMAL
  zh: 该行为明确遵循，并设计与[《通用数据保护条例（GDPR）》](https://gdpr.eu/)一同运作，这是一项旨在保护欧盟个人隐私和个人数据的法律，于2016年通过，并自2018年生效。自2018年以来，GDPR已通过[法院案例](https://www.fieldfisher.com/en/insights/the-cjeus-judgement-in-meta-platforms-inc-v-bundekartellamt-the-spotlight-on-lawful-bases-for-processing-data)和对[罚款的执行](https://www.laquadrature.net/2021/07/30/amende-de-746-millions-deuros-contre-amazon-suite-a-nos-plaintes-collectives/)进行了完善和解释。例如，关于在欧盟和美国之间转移的数据保护问题已被广泛[讨论](https://eur-lex.europa.eu/legal-content/EN/TXT/?uri=CELEX%3A62014CJ0362)。尽管2016年GDPR的原文并非欧盟数据隐私的最终立场，但它是欧洲机构为保护个人隐私和数据采取行动的必要第一步。其后，许多其他国家已经采纳或正在考虑[类似的法律](https://biblio.ugent.be/publication/8726790/file/8726791.pdf)。
- en: The European AI act is just the beginning of what will likely be a long process
    of refining and interpreting the rules governing AI in the EU. EU member states
    now need to sign it, and even then it doesn't [go into effect immediately](https://artificialintelligenceact.eu/ai-act-implementation-next-steps/).
    As with the GDPR, the AI Act will be discussed and debated through court cases
    and the application of the regulation. Legislation is slow, while digital technology
    tends to [move fast and break things](https://www.businessinsider.com/mark-zuckerberg-on-facebooks-new-motto-2014-5?r=US&IR=T)  [(still)](https://twitter.com/).
    Given the breakneck pace of progress in AI, it is necessary that this first step
    was taken now. Securing development in AI so that it is safe and respects human
    rights is a difficult task, but this act is a momentous step in the right direction.
  id: totrans-split-17
  prefs: []
  type: TYPE_NORMAL
  zh: 欧洲人工智能法案（[AI act](https://artificialintelligenceact.eu/ai-act-implementation-next-steps/)）只是欧盟调整和解释人工智能规则的开端。欧盟成员国现在需要签署它，即使如此，它也不会立即生效。就像GDPR一样，AI法案将通过法庭案件和法规的适用进行讨论和辩论。立法进展缓慢，而数字技术往往[速度快、容易破坏](https://www.businessinsider.com/mark-zuckerberg-on-facebooks-new-motto-2014-5?r=US&IR=T)。鉴于人工智能领域的快速进展，现在采取这一第一步是必要的。确保人工智能的发展安全且尊重人权是一项艰巨的任务，但这一法案是朝正确方向迈出的重要一步。
