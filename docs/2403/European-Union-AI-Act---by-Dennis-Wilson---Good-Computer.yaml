- en: <!--yml
  id: totrans-split-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-split-1
  prefs: []
  type: TYPE_NORMAL
  zh: 'category: 未分类'
- en: 'date: 2024-05-27 15:04:59'
  id: totrans-split-2
  prefs: []
  type: TYPE_NORMAL
  zh: 'date: 2024-05-27 15:04:59'
- en: -->
  id: totrans-split-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: European Union AI Act - by Dennis Wilson - Good Computer
  id: totrans-split-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 欧盟AI法案 - 作者：Dennis Wilson - Good Computer
- en: 来源：[https://goodcomputer.substack.com/p/european-union-ai-act](https://goodcomputer.substack.com/p/european-union-ai-act)
  id: totrans-split-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://goodcomputer.substack.com/p/european-union-ai-act](https://goodcomputer.substack.com/p/european-union-ai-act)
- en: '[PRE0]'
  id: totrans-split-6
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: How do you govern a technology that is ill-defined, rapidly changing, maybe
    capable of destroying all of humanity, and maybe capable of greatly aiding it?
    Do you ban it, ignoring possible benefits, or do you let it develop without guardrails,
    disregarding the damage along the way?
  id: totrans-split-7
  prefs: []
  type: TYPE_NORMAL
  zh: 如何治理一种定义不清、快速变化、可能有能力摧毁整个人类社会、同时也可能极大帮助人类社会的技术？你是禁止它，忽略可能的好处，还是让它在没有任何防护措施的情况下发展，无视其带来的损害？
- en: Last week, the European Union took a major step towards answering these questions
    with the [EU AI Act](https://artificialintelligenceact.eu/). The act is designed
    to protect the rights of individuals and ensure that AI is used in a way that
    is safe and respects human rights. In this post, I'll look at the key ideas of
    this act and argue that it is a good base framework for future governance.
  id: totrans-split-8
  prefs: []
  type: TYPE_NORMAL
  zh: 上周，欧盟在 [EU AI Act](https://artificialintelligenceact.eu/) 中迈出了回答这些问题的重要一步。该法案旨在保护个人权利，确保AI以安全和尊重人权的方式使用。在这篇文章中，我将探讨这一法案的关键思想，并认为它是未来治理的良好基础框架。
- en: First, what even is AI, according to the EU?
  id: totrans-split-9
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，根据欧盟，AI究竟是什么？
- en: '[PRE1]'
  id: totrans-split-10
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Nearly every word in this definition could be (and has been and will be) debated,
    but I'd like to focus on the scope. The act is designed to be future-proof, and
    the definition encompasses a wide range of technologies. An [earlier definition](https://eur-lex.europa.eu/legal-content/EN/TXT/?uri=CELEX%3A52021PC0206)
    relied on an explicit list of techniques from "machine learning approaches" to
    "Bayesian estimation,” [scaring some statisticians](https://statmodeling.stat.columbia.edu/2021/04/22/eu-proposing-to-regulate-the-use-of-bayesian-estimation/)
    who learned suddenly that they were doing AI. We have no idea what techniques
    AI will use in the future or what weird names we'll use for them, so having an
    explicit list isn't a great idea. The final definition is instead open, and maybe
    even too open - does a database lookup that generates different outputs based
    on inputs qualify as AI now? Well, if the private sector and news hype cycles
    are to be believed, [everything is now AI](https://www.theverge.com/2024/1/13/24035152/ces-generative-ai-hype-robots),
    even things that are [very clearly not AI](https://www.cnbc.com/2019/03/06/40-percent-of-ai-start-ups-in-europe-not-related-to-ai-mmc-report.html).
    A broad legal definition is better at this stage, as a wider base will hold up
    more legislation that can refine it.
  id: totrans-split-11
  prefs: []
  type: TYPE_NORMAL
  zh: 这个定义中的几乎每个词都可以（而且确实已经并将会）被争议，但我想关注的是其范围。该法案被设计成具有未来性，并且定义涵盖了广泛的技术范围。[早期的定义](https://eur-lex.europa.eu/legal-content/EN/TXT/?uri=CELEX%3A52021PC0206)依赖于一个明确列出的技术清单，从“机器学习方法”到“贝叶斯估计”，这让一些统计学家感到恐慌，他们突然发现自己在从事AI。我们不知道AI将来会使用什么技术或者我们将用什么奇怪的名字来称呼它们，所以有一个明确的列表并不是一个好主意。最终的定义是开放的，甚至可能过于开放
    - 现在一个根据输入生成不同输出的数据库查询是否也算AI？如果私营部门和新闻炒作周期是可信的，那么一切都变成了AI，甚至那些明显不是AI的东西。在这个阶段，一个广义的法律定义更为合适，因为它能为更多的立法提供支持，并能够加以完善。
- en: 'The act is much more explicit when it comes to applications of AI. The act
    is risk-based, meaning that the level of regulation will depend on the level of
    risk associated with the technology. Risk is defined as "the combination of the
    probability of an occurrence of harm and the severity of that harm," and is broken
    down into four categories: Unacceptable, High, Low, and Minimal Risk. Unacceptable
    risk systems are prohibited, high risk systems are heavily regulated, low risk
    systems mostly have transparency requirements, and minimal risk systems are pretty
    much unregulated.'
  id: totrans-split-12
  prefs: []
  type: TYPE_NORMAL
  zh: 当涉及到AI的应用时，该法案要求更为明确。该法案基于风险，即监管水平将取决于技术所带来的风险程度。风险被定义为“损害发生的概率与其严重程度的组合”，并分为四个类别：不可接受的、高风险的、低风险的和极小风险的。不可接受风险的系统被禁止，高风险系统受到严格监管，低风险系统主要要求透明度，而极小风险系统几乎不受监管。
- en: Banning applications is pretty heavy handed and is reserved for only the most
    risky of cases. Examples include [social scoring systems](https://www.technologyreview.com/2022/11/22/1063605/china-announced-a-new-social-credit-law-what-does-it-mean/),
    [assessing an individual's criminal risk](https://www.wired.com/story/doj-predictive-policing-lawmakers-demand/),
    [facial recognition](https://www.cnil.fr/en/facial-recognition-20-million-euros-penalty-against-clearview-ai),
    [biometric categorization like racial profiling](https://arstechnica.com/information-technology/2016/03/facebooks-ad-platform-now-guesses-at-your-race-based-on-your-behavior/),
    and [emotion recognition in workplaces](https://patents.google.com/patent/US10496947B1/en)
    or [educational institutions](https://www.sciencedirect.com/science/article/abs/pii/S036013152200032X).
    These aren't abstract examples; these are all technologies that have been developed
    or used. I guess I’ll have to look for alternatives to [video monitoring](https://www.youtube.com/watch?v=UHdrxHPRBng)
    of my students’ faces or [making them](https://www.nature.com/articles/s41539-023-00162-1)
    wear [brain-wave trackers](https://www.youtube.com/watch?v=JMLsHI8aV0g) to know
    if they are paying attention.
  id: totrans-split-13
  prefs: []
  type: TYPE_NORMAL
  zh: 禁止应用是相当严厉的措施，仅适用于最危险的情况。例如，[社会评分系统](https://www.technologyreview.com/2022/11/22/1063605/china-announced-a-new-social-credit-law-what-does-it-mean/)，[评估个人犯罪风险](https://www.wired.com/story/doj-predictive-policing-lawmakers-demand/)，[面部识别](https://www.cnil.fr/en/facial-recognition-20-million-euros-penalty-against-clearview-ai)，[生物特征分类如种族歧视](https://arstechnica.com/information-technology/2016/03/facebooks-ad-platform-now-guesses-at-your-race-based-on-your-behavior/)，以及工作场所或教育机构中的[情感识别](https://patents.google.com/patent/US10496947B1/en)。这些不是抽象的例子；这些都是已经开发或使用的技术。我想我将不得不寻找替代方案，以避免监控学生面部的[视频监控](https://www.youtube.com/watch?v=UHdrxHPRBng)，或者让他们佩戴[脑波追踪器](https://www.youtube.com/watch?v=JMLsHI8aV0g)以了解他们是否在注意力集中。
- en: High-risk AI systems are the second category and aren’t banned but are heavily
    regulated. These are systems that concern safety, are used in critical application,
    or profile individuals. Providers of these systems must prove that they are managing
    the high risk by providing documentation, designing systems to allow for human
    oversight, and being transparent about the system and its data. An example of
    this sort of system is automated hiring software, which is a [growing](https://www.findem.ai/)  [market](https://clickup.com/home-1)
    fraught with problems of [bias](https://www.bbc.com/worklife/article/20240214-ai-recruiting-hiring-software-bias-discrimination)
    and [a lack of transparency](https://www.forbes.com/sites/forbestechcouncil/2023/09/25/ai-bias-in-recruitment-ethical-implications-and-transparency/).
    Another example is medical devices, which face both [incredible potential gains](https://www.theparliamentmagazine.eu/news/article/hitech-health-how-artificial-intelligence-is-revolutionising-healthcare)
    from AI and [many potential risks](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC9908503/).
    These high-risk applications are the most likely to be the subject of future litigation,
    and the act provides a framework for this litigation by setting a standard of
    compliance for these systems.
  id: totrans-split-14
  prefs: []
  type: TYPE_NORMAL
  zh: 高风险人工智能系统是第二类别，并不被禁止，但受到严格监管。这些系统涉及安全性问题，用于关键应用或者对个人进行分析。这些系统的提供者必须证明他们通过提供文档、设计允许人类监督的系统以及公开系统及其数据来管理高风险。这类系统的一个例子是自动化招聘软件，这是一个充满问题（[成长中](https://www.findem.ai/)，[市场](https://clickup.com/home-1)），其中存在[偏见](https://www.bbc.com/worklife/article/20240214-ai-recruiting-hiring-software-bias-discrimination)和[透明度缺失](https://www.forbes.com/sites/forbestechcouncil/2023/09/25/ai-bias-in-recruitment-ethical-implications-and-transparency/)的问题。另一个例子是医疗设备，面临着从人工智能中获益的[巨大潜力](https://www.theparliamentmagazine.eu/news/article/hitech-health-how-artificial-intelligence-is-revolutionising-healthcare)和[许多潜在风险](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC9908503/)。这些高风险应用最有可能成为未来诉讼的对象，该法案通过为这些系统设定遵从标准来为此类诉讼提供框架。
- en: One point which was heavily debated is the [regulation of generative AI](https://www.nature.com/articles/d41586-024-00497-8)
    like ChatGPT. These technologies have been split into two tiers. The first tier
    covers all general-purpose models, except those used only in research or [published
    under an open-source license](https://www.euronews.com/next/2024/02/20/open-source-vs-closed-source-ai-whats-the-difference-and-why-does-it-matter).
    These will be subject to transparency requirements, including detailing their
    training methodologies and energy consumption, and must show that they [respect
    copyright laws](https://www.euronews.com/next/2024/01/09/openai-says-its-impossible-to-train-ai-without-copyrighted-materials).
    The copyright point has been the subject of [much ongoing litigation in the US](https://www.thefashionlaw.com/from-chatgpt-to-deepfake-creating-apps-a-running-list-of-key-ai-lawsuits/)
    and will prove problematic for companies like [OpenAI](https://www.nytimes.com/2023/12/27/business/media/new-york-times-open-ai-microsoft-lawsuit.html)
    and [Stable Diffusion](https://arstechnica.com/tech-policy/2023/04/stable-diffusion-copyright-lawsuits-could-be-a-legal-earthquake-for-ai/).
    The second and stricter tier will cover general-purpose models deemed to have
    "high-impact capabilities," which pose a higher "systemic risk", such as massive
    language models like ChatGPT. While preliminary, the handling of generative AI
    is impressively done, considering how quickly the technology has advanced during
    the development of this act.
  id: totrans-split-15
  prefs: []
  type: TYPE_NORMAL
  zh: 一个广受争议的问题是[生成型AI的监管](https://www.nature.com/articles/d41586-024-00497-8)，如ChatGPT。这些技术被分为两个层次。第一层包括所有除仅用于研究或[在开源许可下发布](https://www.euronews.com/next/2024/02/20/open-source-vs-closed-source-ai-whats-the-difference-and-why-does-it-matter)之外的通用模型。这些模型将受到透明度要求的约束，包括详细说明其训练方法和能源消耗，并必须证明它们[遵守版权法](https://www.euronews.com/next/2024/01/09/openai-says-its-impossible-to-train-ai-without-copyrighted-materials)。版权问题已成为[美国诸多持续进行的诉讼](https://www.thefashionlaw.com/from-chatgpt-to-deepfake-creating-apps-a-running-list-of-key-ai-lawsuits/)的争论焦点，并将对像[OpenAI](https://www.nytimes.com/2023/12/27/business/media/new-york-times-open-ai-microsoft-lawsuit.html)和[Stable
    Diffusion](https://arstechnica.com/tech-policy/2023/04/stable-diffusion-copyright-lawsuits-could-be-a-legal-earthquake-for-ai/)这样的公司造成问题。第二和更严格的层次将覆盖被认为具有“高影响能力”的通用模型，这些模型对“系统风险”造成更高的威胁，如ChatGPT这样的大型语言模型。尽管初步，处理生成型AI的方式在技术快速发展的背景下已经做得令人印象深刻。
- en: The act clearly follows, and is designed to work with, the [General Data Protection
    Regulation (GDPR)](https://gdpr.eu/), a law designed to protect the privacy and
    personal data of individuals in the EU, adopted in 2016 and effective starting
    2018\. Since 2018, the GDPR has been refined and interpreted through [court cases](https://www.fieldfisher.com/en/insights/the-cjeus-judgement-in-meta-platforms-inc-v-bundekartellamt-the-spotlight-on-lawful-bases-for-processing-data)
    and the application of [fines](https://www.laquadrature.net/2021/07/30/amende-de-746-millions-deuros-contre-amazon-suite-a-nos-plaintes-collectives/).
    For example, the protection of data transferred between the EU and the US has
    been [extensively](https://eur-lex.europa.eu/legal-content/EN/TXT/?uri=CELEX%3A62014CJ0362)  [debated](https://curia.europa.eu/juris/liste.jsf?num=C-311/18).
    The GDPR as written in 2016 was not the definitive stance on data privacy in the
    EU, but it was a necessary first step from which European agencies could act to
    protect individuals' privacy and data. Following it, many other countries have
    adopted or are considering [similar laws](https://biblio.ugent.be/publication/8726790/file/8726791.pdf).
  id: totrans-split-16
  prefs: []
  type: TYPE_NORMAL
  zh: 该行为明确遵循，并设计与[《通用数据保护条例（GDPR）》](https://gdpr.eu/)一同运作，这是一项旨在保护欧盟个人隐私和个人数据的法律，于2016年通过，并自2018年生效。自2018年以来，GDPR已通过[法院案例](https://www.fieldfisher.com/en/insights/the-cjeus-judgement-in-meta-platforms-inc-v-bundekartellamt-the-spotlight-on-lawful-bases-for-processing-data)和对[罚款的执行](https://www.laquadrature.net/2021/07/30/amende-de-746-millions-deuros-contre-amazon-suite-a-nos-plaintes-collectives/)进行了完善和解释。例如，关于在欧盟和美国之间转移的数据保护问题已被广泛[讨论](https://eur-lex.europa.eu/legal-content/EN/TXT/?uri=CELEX%3A62014CJ0362)。尽管2016年GDPR的原文并非欧盟数据隐私的最终立场，但它是欧洲机构为保护个人隐私和数据采取行动的必要第一步。其后，许多其他国家已经采纳或正在考虑[类似的法律](https://biblio.ugent.be/publication/8726790/file/8726791.pdf)。
- en: The European AI act is just the beginning of what will likely be a long process
    of refining and interpreting the rules governing AI in the EU. EU member states
    now need to sign it, and even then it doesn't [go into effect immediately](https://artificialintelligenceact.eu/ai-act-implementation-next-steps/).
    As with the GDPR, the AI Act will be discussed and debated through court cases
    and the application of the regulation. Legislation is slow, while digital technology
    tends to [move fast and break things](https://www.businessinsider.com/mark-zuckerberg-on-facebooks-new-motto-2014-5?r=US&IR=T)  [(still)](https://twitter.com/).
    Given the breakneck pace of progress in AI, it is necessary that this first step
    was taken now. Securing development in AI so that it is safe and respects human
    rights is a difficult task, but this act is a momentous step in the right direction.
  id: totrans-split-17
  prefs: []
  type: TYPE_NORMAL
  zh: 欧洲人工智能法案（[AI act](https://artificialintelligenceact.eu/ai-act-implementation-next-steps/)）只是欧盟调整和解释人工智能规则的开端。欧盟成员国现在需要签署它，即使如此，它也不会立即生效。就像GDPR一样，AI法案将通过法庭案件和法规的适用进行讨论和辩论。立法进展缓慢，而数字技术往往[速度快、容易破坏](https://www.businessinsider.com/mark-zuckerberg-on-facebooks-new-motto-2014-5?r=US&IR=T)。鉴于人工智能领域的快速进展，现在采取这一第一步是必要的。确保人工智能的发展安全且尊重人权是一项艰巨的任务，但这一法案是朝正确方向迈出的重要一步。
