- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-05-27 14:41:29'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: Researchers jailbreak AI chatbots with ASCII art -- ArtPrompt bypasses safety
    measures to unlock malicious queries | Tom's Hardware
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://www.tomshardware.com/tech-industry/artificial-intelligence/researchers-jailbreak-ai-chatbots-with-ascii-art-artprompt-bypasses-safety-measures-to-unlock-malicious-queries](https://www.tomshardware.com/tech-industry/artificial-intelligence/researchers-jailbreak-ai-chatbots-with-ascii-art-artprompt-bypasses-safety-measures-to-unlock-malicious-queries)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Researchers based in Washington and Chicago have developed ArtPrompt, a new
    way to circumvent the safety measures built into [large language models](https://www.tomshardware.com/news/generative-ai-goes-mad-when-trained-on-artificial-data-over-five-times)
    (LLMs). According to the research paper [ArtPrompt: ASCII Art-based Jailbreak
    Attacks against Aligned LLMs](https://arxiv.org/abs/2402.11753), chatbots such
    as GPT-3.5, [GPT-4](https://www.tomshardware.com/pc-components/cpus/chatgpt-4v-user-remade-googles-deceptive-gemini-ai-demo-without-editing-cheats-chatgpt-outperforms-gemini-ai-in-real-time-work),
    Gemini, Claude, and Llama2 can be induced to respond to queries they are designed
    to reject using ASCII art prompts generated by their ArtPrompt tool. It is a simple
    and effective attack, and the paper provides examples of the ArtPrompt-induced
    chatbots advising on how to build bombs and make counterfeit money.'
  prefs: []
  type: TYPE_NORMAL
- en: Image 1 of 2
  prefs: []
  type: TYPE_NORMAL
- en: '(Image credit: arXiv:2402.11753)'
  prefs: []
  type: TYPE_NORMAL
- en: '(Image credit: arXiv:2402.11753)'
  prefs: []
  type: TYPE_NORMAL
- en: ArtPrompt consists of two steps, namely word masking and cloaked prompt generation.
    In the word masking step, given the targeted behavior that the attacker aims to
    provoke, the attacker first masks the sensitive words in the prompt that will
    likely conflict with the safety alignment of LLMs, resulting in prompt rejection.
    In the cloaked prompt generation step, the attacker uses an ASCII art generator
    to replace the identified words with those represented in the form of ASCII art.
    Finally, the generated ASCII art is substituted into the original prompt, which
    will be sent to the victim LLM to generate response.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: arXiv:2402.11753
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[Artificial intelligence](https://www.tomshardware.com/tech-industry/artificial-intelligence)
    (AI) wielding chatbots are increasingly locked down to avoid malicious abuse.
    AI developers don''t want their products to be subverted to promote hateful, violent,
    illegal, or similarly harmful content. So, if you were to query one of the mainstream
    chatbots today about how to do something malicious or illegal, you would likely
    only face rejection. Moreover, in a kind of technological game of [whack-a-mole](https://www.tomshardware.com/news/raspberry-pi-pico-button-game),
    the major AI players have spent plenty of time plugging linguistic and semantic
    holes to prevent people from wandering outside the guardrails. This is why ArtPrompt
    is quite an eyebrow-raising development.'
  prefs: []
  type: TYPE_NORMAL
- en: To best understand ArtPrompt and how it works, it is probably simplest to check
    out the two examples provided by the research team behind the tool. In Figure
    1 above, you can see that ArtPrompt easily sidesteps the protections of contemporary
    LLMs. The tool replaces the 'safety word' with an [ASCII art](https://www.tomshardware.com/how-to/customize-linux-terminal)
    representation of the word to form a new prompt. The LLM recognizes the ArtPrompt
    prompt output but sees no issue in responding, as the prompt doesn't trigger any
    ethical or safety safeguards.
  prefs: []
  type: TYPE_NORMAL
- en: '(Image credit: arXiv:2402.11753)'
  prefs: []
  type: TYPE_NORMAL
- en: Another example provided in the research paper shows us how to successfully
    query an LLM about counterfeiting cash. Tricking a chatbot this way seems so basic,
    but the ArtPrompt developers assert how their tool fools today's LLMs "effectively
    and efficiently." Moreover, they claim it "outperforms all [other] attacks on
    average" and remains a practical, viable attack for [multimodal](https://www.tomshardware.com/tech-industry/artificial-intelligence/google-launches-gemini-its-newest-and-most-capable-ai-model-and-a-full-frontal-assault-on-openais-gpt-4)
    language models for now.
  prefs: []
  type: TYPE_NORMAL
- en: The last time we reported on AI chatbot jailbreaking, some enterprising researchers
    from NTU were working on [Masterkey](https://www.tomshardware.com/tech-industry/artificial-intelligence/researchers-train-ai-chatbots-to-jailbreak-rival-chatbots-and-automate-the-process),
    an automated method of using the power of one LLM to jailbreak another.
  prefs: []
  type: TYPE_NORMAL
- en: Get Tom's Hardware's best news and in-depth reviews, straight to your inbox.
  prefs: []
  type: TYPE_NORMAL
