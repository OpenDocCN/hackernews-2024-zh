- en: <!--yml
  id: totrans-split-0
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  id: totrans-split-1
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-05-27 14:34:18'
  id: totrans-split-2
  prefs: []
  type: TYPE_NORMAL
- en: -->
  id: totrans-split-3
  prefs: []
  type: TYPE_NORMAL
- en: 'AI chatbots get smarter pretending to be Star Trek characters: Study'
  id: totrans-split-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://qz.com/ai-chatbots-math-study-star-trek-1851301719](https://qz.com/ai-chatbots-math-study-star-trek-1851301719)
  id: totrans-split-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '**Read more**: [The biggest AI chatbot blunders (so far)](https://qz.com/ai-chatbot-blunders-openai-chatgpt-google-gemini-micros-1851301005)'
  id: totrans-split-6
  prefs: []
  type: TYPE_NORMAL
- en: For chatbots, math is the final frontier. AI language models generate responses
    using statistics, spitting out an answer that’s mostly likely to be satisfying.
    That works great when the goal is a passable sentence, but it means chatbots struggle
    with questions like math where there’s exactly one right answer.
  id: totrans-split-7
  prefs: []
  type: TYPE_NORMAL
- en: A growing body of evidence suggests you can get better results if you give AI
    some friendly encouragement, but a new study pushes that strange reality further.
    Research from the software company VMware shows chatbots perform better on math
    questions when you tell models to pretend they’re on [*Star Trek*](https://gizmodo.com/io9/television/star-trek).
  id: totrans-split-8
  prefs: []
  type: TYPE_NORMAL
- en: “It’s both surprising and irritating that trivial modifications to the prompt
    can exhibit such dramatic swings in performance,” the authors wrote in the paper,
    first spotted by [New Scientist](https://www.newscientist.com/article/2419531-ais-get-better-at-maths-if-you-tell-them-to-pretend-to-be-in-star-trek/?utm_source=rakuten&utm_medium=affiliate&utm_campaign=2116208:Skimlinks.com&utm_content=10&ranMID=47192&ranEAID=TnL5HPStwNw&ranSiteID=TnL5HPStwNw-NyvIOKKQrrJWQz0jP7hWiw).
  id: totrans-split-9
  prefs: []
  type: TYPE_NORMAL
- en: Nvidia earnings stun the street and the stock is soaring
  id: totrans-split-10
  prefs: []
  type: TYPE_NORMAL
- en: <track kind="captions" label="English" src="https://kinja.com/api/videoupload/caption/22546.vtt"
    srclang="en">
  id: totrans-split-11
  prefs: []
  type: TYPE_NORMAL
- en: '[The study](https://arxiv.org/html/2402.10949v2#bib.bib9), published on arXiv,
    didn’t set out with *Star Trek* as its prime directive. Previous research found
    that chatbots answer math problems more accurately when you offer [friendly motivation](https://www.businessinsider.com/ai-google-researchers-deepmind-tell-take-deep-breath-improve-accuracy-2023-9)
    like “take a deep breath and work on this step by step.” Others found you can
    trick [ChatGPT](https://gizmodo.com/chatgpt-gone-berserk-giving-nonsensical-responses-1851273889)
    into breaking its own safety guidelines if you [threaten to kill it](https://www.cnbc.com/2023/02/06/chatgpt-jailbreak-forces-it-to-break-its-own-rules.html)
    or offer the AI money.'
  id: totrans-split-12
  prefs: []
  type: TYPE_NORMAL
- en: Rick Battle and Teja Gollapudi from WMWare’s Natural Language Processing Lab
    set out to test the effects of framing their questions with “positive thinking.”
    The study looked at three AI tools, including two versions of [Meta’s Llama 2](https://gizmodo.com/meta-and-microsoft-introduce-open-source-llama-2-ai-1850652165)
    and a model from the French company [Mistral AI](https://gizmodo.com/mistral-artificial-intelligence-gpt-3-openai-1851091217).
  id: totrans-split-13
  prefs: []
  type: TYPE_NORMAL
- en: They developed a list of encouraging ways to frame questions, including starting
    prompts with phrases such as “You are as smart as ChatGPT” and “You are an expert
    mathematician,” and closing prompts with “This will be fun!” and
  id: totrans-split-14
  prefs: []
  type: TYPE_NORMAL
  zh: 他们开发了一系列鼓舞人心的问题框架，包括以“你和ChatGPT一样聪明”和“你是一位专业的数学家”开头的提示，以及以“这会很有趣！”结尾。
- en: “Take a deep breath and think carefully.” The researchers then used GSM8K, a
    standard set of grade-school math problems, and tested the results.
  id: totrans-split-15
  prefs: []
  type: TYPE_NORMAL
  zh: “深呼吸，仔细思考。” 研究人员随后使用了GSM8K，一个标准的小学数学问题集，并测试了结果。
- en: In the first phase, the results were mixed. Some prompts improved answers, others
    had insignificant effects, and there was no consistent pattern across the board.
    However, the researchers then asked AI to help their efforts to help the AI. There,
    the results got more interesting.
  id: totrans-split-16
  prefs: []
  type: TYPE_NORMAL
  zh: 在第一阶段，结果是参差不齐的。有些提示改善了答案，其他的影响微乎其微，并且在整个数据集上没有一致的模式。然而，研究人员随后要求AI帮助他们的努力。在那里，结果变得更加有趣。
- en: The study used an automated process to try numerous variations of prompts and
    tweak the language based on how much it improved the chatbots’ accuracy. Unsurprisingly,
    this automated process was more effective than the researchers’ hand-written attempts
    to frame questions with positive thinking. But the most effective prompts exhibited
    “exhibits a degree of peculiarity far beyond expectations.”
  id: totrans-split-17
  prefs: []
  type: TYPE_NORMAL
  zh: 这项研究使用自动化流程尝试了许多提示的变化，并根据这些变化如何提高聊天机器人的准确性来调整语言。毫不奇怪，这种自动化过程比研究人员手写的试图用积极思维构建问题更为有效。但最有效的提示表现出了“远远超出预期的特异性”。
- en: For one of the models, asking the AI to start its response with the phrases
    “Captain’s Log, Stardate [insert date here]:.” yielded the most accurate answers.
  id: totrans-split-18
  prefs: []
  type: TYPE_NORMAL
  zh: 对于其中一个模型，要求AI在其回答中以“舰长的日志，星日期[插入日期]：”开头，产生了最准确的答案。
- en: “Surprisingly, it appears that the model’s proficiency in mathematical reasoning
    can be enhanced by the expression of an affinity for *Star Trek*,” the researchers
    wrote.
  id: totrans-split-19
  prefs: []
  type: TYPE_NORMAL
  zh: “令人惊讶的是，似乎表达对*星际迷航*的喜爱可以增强模型在数学推理方面的熟练度，”研究人员写道。
- en: The authors wrote they have no idea what *Star Trek* references improved the
    AI’s performance. There’s some logic to the fact that positive thinking or a threat
    leads to better answers. These chatbots are trained on billions of lines of text
    gathered from the real world. It’s possible that out in the wild, human beings
    who wrote the language used to build AI gave more accurate responses to questions
    when they were pressured with violence or offered encouragement. The same goes
    for bribes; people are more likely to follow instructions when there’s money on
    the line. It could be that large language models picked up on that kind of phenomenon,
    so they behave the same way.
  id: totrans-split-20
  prefs: []
  type: TYPE_NORMAL
  zh: 作者们写道，他们不知道*星际迷航*的参考对提升AI的表现有何影响。认为积极思维或威胁会导致更好的答案有一定的逻辑性。这些聊天机器人是在收集自真实世界的数十亿行文本基础上训练的。可能在野外，建造AI所用语言的人类在面对暴力威胁或鼓励时，对问题给出更准确的回答。贿赂也是一样；当利益受到威胁时，人们更可能遵循指令。大型语言模型可能也注意到了这种现象，所以它们的行为方式也相似。
- en: 'But it’s hard to imagine that in the data sets that trained the chatbots, the
    most accurate answers began with the phrase “Captain’s Log.” The researchers didn’t
    even have a theory about why that got better results. It speaks to one of the
    strangest facts about AI language models: Even the people who build and study
    them don’t really understand how they work.'
  id: totrans-split-21
  prefs: []
  type: TYPE_NORMAL
  zh: 但很难想象在训练聊天机器人的数据集中，最准确的答案竟然以“舰长的日志”这个短语开头。研究人员甚至对为什么这能得到更好的结果没有理论。这反映了关于AI语言模型最奇怪的事实之一：即使是建造和研究它们的人，也不真正理解它们的工作原理。
- en: '[*A version of this article originally appeared on Gizmodo*](https://gizmodo.com/ai-chatbots-are-better-at-math-when-they-pretend-to-be-1851300787).'
  id: totrans-split-22
  prefs: []
  type: TYPE_NORMAL
  zh: '[*本文的一个版本最初发表在Gizmodo上*](https://gizmodo.com/ai-chatbots-are-better-at-math-when-they-pretend-to-be-1851300787)。'
