<!--yml

category: 未分类

date: 2024-05-27 14:44:52

-->

# Answer.AI - 现在您可以在家中训练一个 70b 语言模型

> 来源：[https://www.answer.ai/posts/2024-03-06-fsdp-qlora.html](https://www.answer.ai/posts/2024-03-06-fsdp-qlora.html)

## 摘要

今天，我们发布了 Answer.AI 的第一个项目：一个完全开源的系统，首次能够在普通台式电脑上，搭载两个或更多标准游戏 GPU（如 RTX 3090 或 4090），高效地训练一个 70b 大型语言模型。这个系统结合了 FSDP 和 QLoRA 技术，是 Answer.AI、华盛顿大学的 Tim Dettmers，以及 Hugging Face 的 Titus von Koeller 和 Sourab Mangrulkar 合作的成果。

这个系统将帮助开源社区发布更好的模型。Teknium，极受欢迎的 OpenHermes 模型和数据集的创建者，下载量超过 50 万次，表示：

> “*有了这个能力，我们可以将巨大的模型推向新的高度，并且巨大的、数百亿参数的模型现在可以被小型实验室访问了。*”

在 Answer.AI，我们将这个项目作为我们的首个项目，因为这是我们的核心目标的重要基础：帮助每个人都能使用有用的人工智能。仅仅使用*他人*的模型是不够的。我们希望每个人都能够创建*自己*的个性化模型，以便他们控制自己的人工智能系统。

## 背景

### 大胆的想法

用于训练深度学习模型的硬件有两个非常不同的级别。一个是数据中心级别的硬件，如 H100 和 A100，成本高达 [几十万美元](https://shop.lambdalabs.com/deep-learning/servers/blade/customize)。另一个是包含游戏 GPU 的台式电脑，如双 4090，成本低于 [10000 美元](https://shop.lambdalabs.com/gpu-workstations/vector/customize)（甚至可以从二手部件组装，价格不到预装系统的一半）。

但这是关键：游戏 GPU 的性能与成本超过 10 倍的数据中心 GPU 类似！如果我们能够利用这些成本仅为其 1/10 的（但几乎同样快的）卡来训练大型语言模型将是非常好的，但我们不能，因为它们的内存远远不如数据中心的卡。目前可用的最佳数据中心卡具有 80GB RAM，而游戏卡的最大 RAM 为 24GB。由于只有最大的模型才能产生最佳结果，因此创建最佳模型迄今大多数人都无法做到。

我们意识到，其实并没有固有的理由限制这样做。超快的硬件已经准备就绪，只等我们用一种满足其内存限制的方式提供模型和数据。显而易见的问题是：为什么到现在还没有这么做？所有大型行业实验室已经拥有了昂贵 10 倍的硬件，所以他们其实没有太大的动力去解决这个问题。

这里的大胆构想很简单：找出如何利用这些更便宜、低内存的游戏 GPU 来训练最优秀的开源模型。因此，目标是：仅使用游戏 GPU 训练一个拥有 700 亿参数（70b）的模型，这意味着我们每个 GPU 的内存最多只有 24GB。这将是一个挑战，因为每个参数通常需要 16 位（2 字节），所以存储权重至少需要 140GB，而且这还不包括激活、梯度和优化状态等所有其他数据！

### 为什么选择这个项目？

Answer.AI 是一种非常不同寻常的组织类型——一个营利性研发实验室，更接近于[19世纪的电力实验室](https://www.answer.ai/posts/2024-01-26-freaktakes-lessons.html)，而不是今天的 AI 研究团体。找出如何使大模型训练变得廉价且易于访问，正是 Eric Ries 和 Jeremy Howard 希望我们在去年[NeurIPS上推出该组织](https://www.answer.ai/posts/2023-12-12-launch.html)时能够做到的事情之一。

解决这个问题很难。这需要理解许多不同的库（例如 bitsandbytes、PEFT、Transformers、Accelerate 和 PyTorch），以及计算机科学和数学概念（例如离散化、分布式计算、GPU 编程、线性代数、SGD 概念如梯度检查点），以及它们之间的互动。

学术界充满了解决难题的杰出人才。但学术界尚未解决这个特定问题。这是因为大学研究人员很难为这类工作投入时间。将现有工具和技术结合在一起通常不被认为足够“新颖”，无法在高影响力期刊上发表文章，但这是学术界所需要的。此外，学术界通常期望在其领域内高度专业化，这使得将许多碎片组合成一个解决方案变得具有挑战性。

当然，大型科技公司也充满了解决难题的杰出人才。但是，用消费级 GPU 训练模型这个具体问题，并不是他们需要解决的问题——他们已经购买了昂贵的大型 GPU！许多初创公司也充满了解决难题的杰出人才！但正如[Eric Ries 解释的](https://ltse.com/about/mission)，“当今的金融市场迫使企业将短期收益置于一切之上”。初创公司极难向投资者证明他们为什么要将资金投入到开源软件和公共研究上。

尽管学术界、大型科技公司和初创公司都有很好的理由没有解决这个问题，但这些正是[确切的理由](https://www.answer.ai/posts/2023-12-12-launch.html)为什么这个问题非常适合 Answer.AI。公司里的每个人都建立了我们在解决这个问题上要处理的那种系统，因此我们能够理解所有部件是如何结合在一起的。热爱深入理解软件和人工智能基础以及热衷于破解有趣和有趣的端到端系统的人是被吸引到 Answer.AI 的人，反之亦然。

我们选择要共同解决的问题的人也是将要解决问题的人。因此，我们倾向于选择涉及汇聚多个想法以创建实际有用解决方案的项目。并且因为我们是一个具有通过 AI、开源软件和公共研究产生*长期*利益的公益公司，因此开源软件和公共研究直接符合我们的使命。

### QLoRA：在单个 GPU 上训练更大的模型

最近发布了两个项目，它们迈出了使这成为现实的第一关键步骤：QLoRA（由[Tim Dettmers 等人](https://arxiv.org/abs/2305.14314)）和 FSDP（Meta 的[PyTorch 团队](https://engineering.fb.com/2021/07/15/open-source/fsdp/)）。

QLoRA 是现代神经网络中两个关键进展的简单而杰出的组合：*量化*和*LoRA*。量化是一种技术，在其中，不使用 16 甚至 32 位来存储神经网络的权重，而是使用 4 个（甚至更少）位。4 位数的可能值仅有 16 个，但[Dettmers 和 Zettlemoyer 表明](https://arxiv.org/abs/2212.09720)这在当今流行的大型语言模型中可能足够。Tim Dettmers 所作的这些 4 位“量化”模型易于创建，多亏了他的 bitsandbytes 库，最近 Hugging Face 已介入协助[维护和记录](https://huggingface.co/docs/bitsandbytes/main/en/index)

不幸的是，一旦模型被量化，就无法再用普通方法进行训练 - 只有 16 个可能的值，用于模型训练的梯度下降方法几乎无法观察到零梯度，因此无法对量化的权重进行任何更新。这是一个重大问题，因为这意味着量化只能用于推断，而不能用于持续的预训练或微调。尽管推断是有用且重要的，但它真的只是*消耗*模型。但我们希望每个人都能*为*创建模型*做出*贡献！

避免这种限制的诀窍是使用 [LoRA](https://arxiv.org/abs/2106.09685) – “Large Language Models 的低秩适应”。LoRA 并不训练整个大型语言模型，而是添加了“适配器”，这些适配器是非常小的矩阵（通常小于完整模型的 1%），它们在训练过程中保持不变。如果你曾经使用过像 Stable Diffusion 这样的模型，你可能会经常看到这些适配器；这是这些模型通常如此小而快速下载的原因。

Tim 发现 LoRA 可以与量化结合：使用一个量化的基础模型，这个模型在训练过程中完全没有改变，然后添加可训练的未量化的 LoRA 适配器。这种组合被称为 *QLoRA*。Tim 的团队首次使用这种方法训练了一个超过 GPU 大小的模型：他们在一张 48GB 的卡上训练了一个 65b 的模型（未量化时为 130GB）。

Hugging Face 在这里再次发挥了作用，创建了 [PEFT](https://huggingface.co/blog/peft) 库，使 LoRA 训练变得更加简单，并直接与 bitsandbytes 集成，使任何人都可以仅用几行代码使用 QLoRA。Hugging Face 团队一直在幕后不懈努力，确保开源社区能够使用这些技术来训练他们的模型。如果你曾经使用过 Transformers 通过单个函数参数加载 4 位模型，那么你应该感谢他们（即使你没有，你几乎肯定使用了使用这一生态系统构建其模型的人的工作）。

QLoRA 没有完全解决我们想要解决的问题，即在 24GB 卡上训练一个 70b 模型，但它比以往任何方法都要接近。当量化为 4 位（即 0.5 字节）时，70b 模型需要 70/2 = 35 GB 的空间，这比我们想要使用的 24GB 游戏 GPU 大得多。

QLoRA 还存在其他限制。一张 48GB 的卡非常昂贵，仅仅能够容纳一个 65b 的模型。这可能是个问题，因为我们还需要存储许多其他东西，包括训练过程中模型的激活、梯度和优化状态。如果加载模型权重后剩余的内存不多，那么就没有足够的工作内存支持训练。

例如，语言模型的一个好处是我们可以使用它们来“聊天”、理解或分析长文档或对话。为了制造能处理这种长序列的模型，我们需要在训练期间向它们展示长序列的示例。在训练中使用的最长序列称为“序列长度”。尝试在 48GB 卡上训练一个 65b QLoRA 模型时使用除了短序列长度之外的任何内容都会导致错误，因为没有足够的内存来存储关于序列的所有信息；几乎所有的内存都用于存储模型本身。

此外，如果模型一次只能查看一个序列，那么它将花费很长时间才能完成所有训练集中的数据。因此，我们希望能够一次将几个序列“批量”在一起。包含的序列数称为“批量大小”。当在加载模型权重后GPU上剩余空间很少时，我们只能使用非常小的批量大小，导致训练速度极慢。

### FSDP：将训练扩展到多个GPU

解决单个消费者GPU的RAM限制问题的一个明显解决方案是使用多个GPU！在开源社区中一个非常常见的方法是简单地在每张卡上放置模型的几层。因此，要进行训练，您在第一个GPU上运行前几层，然后在第二个GPU上运行下几层，依此类推。例如，一个70b（140GB）的模型可以分布在8个24GB的GPU上，每个GPU使用17.5GB。在Hugging Face Transformers中甚至有一个方便的设置`device_map='auto'`，您可能已经使用过；这实际上是在幕后执行的操作。这能胜任工作，但有一个巨大的缺点：每次只有一个GPU处于活动状态，其他所有GPU都在等待它们的“轮次”。这意味着计算资源的⅞被浪费了。

*分布式数据并行*（DDP）先前是在多个GPU上高效训练模型的黄金标准方法。这要求在每个GPU上保持完整的模型 – 如果您有一个小模型（例如`2b`模型，需要4GB RAM），您可以简单地将整个模型分别加载到每个GPU上，然后每个GPU可以并行处理训练示例。因此，例如，如果您有4个GPU，这将加快4倍的训练速度。但是如果模型不能适应一个GPU，足够空间来容纳训练过程所需的数据，DDP就不起作用了。

因此，我们需要一些能够将模型分割到多个GPU上（例如`device_map='auto'`）并且并行使用它们（类似DDP）的方法。这就是Meta的[完全分片数据并行](https://pytorch.org/tutorials/intermediate/FSDP_tutorial.html)（FSDP）库的用武之地。它通过在多个GPU上分割模型的参数来“分片”大模型，允许所有GPU同时使用。在训练期间计算神经网络的一层时，所有必需的片段都会被复制到特定的GPU上。然后进行计算，最后从该GPU中删除复制的部分。尽管听起来非常低效，但通过在当前层忙于计算时同时聪明地复制下一层数据，这种方法可能不会比DDP慢。

FSDP 将 DDP 的性能带到比任何单个 GPU 都要大的模型上是一个突破。例如，一个 70b（70亿参数）的非量化模型占用 140GB RAM（因为每个参数都存储为16位，即2字节），但即使是 NVIDIA 的 H100 显卡（每张卡约40000美元！）也不足以满足需求，因为它只有80GB RAM。但是使用 FSDP，四张 H100 GPU 可以组合使用，总共提供320GB RAM。

（请注意，这样一台机器将耗费您约$150,000…）

## 将 FSDP 和 QLoRA 结合起来

在 Answer.AI，我们的北极星是使有用的人工智能更易于获取。花费$150,000来创建自己的高质量个性化模型绝对不算易于获取！因此，我们着手的第一个项目是使得可以使用配备消费级游戏 GPU 的台式机来高效地训练一个70b模型成为可能。我们认为，如果我们可以使用 QLoRA 将模型大小减少约400%（因此70b模型可以适应35GB RAM），然后我们使用 FSDP 将其分片到两个或更多的24GB消费者卡上，那么剩下的RAM足以训练一个模型。

### 第一步

2023年末，杰里米和蒂姆讨论了将 FSDP 和 QLoRA 结合起来的想法。蒂姆将杰里米与 Titus von Koeller 联系起来，杰里米和 Titus 共同努力，尝试、探索、理解并记录两个库结合时出现的问题。

Answer.AI 的 Johno Whitaker 制作了一个重要的第一步：一个简单的独立测试脚本，使我们能够更深入地理解问题，并测试解决方案。2024年初，Answer.AI 的本杰明·沃纳和 Titus 独立提出了一个关键想法：将量化参数存储在可选择的数据类型中，其中存储数据类型与模型的“计算类型”相同。

本杰明在开发这个想法的24小时内完成了原型，但随后我们发现了另一个问题：FSDP 没有复制每个分片所需的量化信息以供模型使用！这是因为 FSDP 对它将在 GPU 之间同步的数据子集持有一定意见。我们意识到，如果在每个 GPU 上量化模型，那么缺失的元数据将在所有 GPU 上保持不变。此外，我们不得不将“量化状态”（用于（反）量化参数所需的信息）从参数移动到层中，以确保在 FSDP 移动分片时它们不会被移除。

一旦我们解决了这些问题，我们就能够使用 FSDP 成功地训练我们的第一批数据，这是使用量化模型！本杰明和 Answer.AI 的 Kerem Turgutlu 能够将这一切与所需的所有测试和重构打包成一个 [pull request](https://github.com/TimDettmers/bitsandbytes/pull/970)，提交给 bitsandbytes。我们非常感谢 bitsandbytes 项目的维护者，在推动我们的 PR 通过其流程方面非常响应。

### 任务完成，几乎

此时，我们再次意识到我们将很快能够解决问题，但我们再次低估了任务的复杂性！我们首先意识到，加载和量化过程本身需要整个模型放在一个GPU上，因此仍然无法加载大于单个GPU的量化模型。

杰里米花了几周时间仔细研究Meta的出色[Llama-Recipes](https://github.com/facebookresearch/llama-recipes)项目，这是他发现的最佳完整FSDP精调实现，并通过密切跟踪其与bitsandbytes、Hugging Face的PEFT、Transformers和Accelerate项目的协作方式，他设法构建了一个最小的独立脚本，手动完成了所有细调模型所需的步骤。

本杰明意识到，通过一些调整，可以逐层进行加载和离散化，从而避免在单个GPU上拥有整个模型的需求。他还想出了如何防止PEFT库将量化状态移动到CPU的方法。Kerem编写了LoRA算法的自定义实现，以便它可以与本杰明的变更配合工作。

有了这一点，我们首次能够在双3090游戏GPU上对70b模型进行精调！

要使此工作成功，我们不仅仅受益于FSDP和QLoRA，还从学术界和开源社区在过去几年中开发的大量聪明技术中受益。我们使用了：

+   [梯度检查点](https://arxiv.org/abs/1604.06174)（也称为激活检查点）以避免存储完整梯度，而是在模型各处的若干“检查点”处保存激活，然后根据需要重新运行前向计算步骤来重新计算梯度。

+   [CPU卸载](https://pytorch.org/docs/stable/fsdp.html#torch.distributed.fsdp.CPUOffload) 将权重存储在CPU RAM中，而不是在GPU上当它们不在使用时，大大减少了所需的GPU内存。对于使用H100 GPU的“GPU富人”来说，这种技术并不是很有用，因为它们有高度优化的方法来相互传递权重。但对于我们的用例来说，这是绝对必要的，因为游戏GPU和主板没有这些系统。

+   [Flash Attention 2](https://arxiv.org/abs/2307.08691) 以使用内存优化的Cuda核心高效计算注意力。

使这些与FSDP和QLoRA一起工作并不总是直接的。例如，在使用CPU卸载时，本杰明发现并修复了bitsandbytes中的问题。每当“卸载”的权重被复制回GPU时，它就会自动重新量化，这实际上将预训练模型转变为随机权重！我们向bitsandbytes提交了一个拉取请求，以跟踪哪些参数已经被量化，以便我们可以避免冗余计算。

经过所有这些工作，我们非常高兴地发现，我们可以使用消费级GPU训练大型模型。Jeremy已经在各种GPU硬件上对原始llama-recipes进行了详细的基准测试，而Kerem为新项目开发了一个全面的基准测试系统。在比较这两者时，我们意识到我们仍然无法使用希望的序列长度或批处理大小——由于某种原因，我们使用的内存超出了预期。

当我们仔细观察时，原来这并不是由于我们的FSDP/QLoRA集成造成的——实际上，即使没有FSDP，只要我们增加了bitsandbytes中的seqlen，内存使用量就会超线性增加，最终导致甚至比没有量化时内存使用量更高！原来我们并不是第一个发现这个问题的人。我们目前还没有bitsandbytes的解决方案（但正在调查），但这确实带来了一个令人兴奋的发现……

### 发现HQQ

我们喜欢与志同道合的人合作，所以当我们看到Daniel Han在Unsloth上所做的惊人工作时，我们想了解更多，并看看我们是否可以互相帮助。我们问Daniel是否还有其他这个领域的有趣项目值得关注，他指引我们关注了HQQ。

要解释HQQ，我们首先需要简要介绍一下背景…… bitsandbytes进行的4位量化采用了一种简单、快速且巧妙的方法，其中每组参数被归一化到一个一致的范围，然后每个参数被放置在一个桶中，桶的分界点基于参数服从正态分布的假设。这样做的好处是量化几乎是即时的，但因为真实模型参数不会完全符合假设的分布，精度可能会受到影响。

其他方法如GPTQ和最新的AWQ则采取了不同的方向，这些方法根据模型在传递代表性数据时的实际行为来优化量化参数。这些方法往往能产生更准确的模型，可能每个参数甚至少于4位；但它们的缺点是优化过程可能需要几个小时甚至几天来完成每个模型。

HQQ结合了两个世界的优势。与GPTQ相比，处理70b模型快50倍，同时比GPTQ更准确。Kerem决定调查HQQ是否与FSDP兼容良好。

他发现，让HQQ和FSDP良好结合的过程几乎与bitsandbytes所需的完全相同步骤，结果他在几天内完成了一个完整的工作示例。mobius.ml的人们在确保我们的PR成功合并方面非常响应和有帮助，所以我们现在很高兴地宣布FSDP也能与HQQ兼容！

## 如何使用FSDP/QLoRA

要使用FSDP，当然需要多个GPU。如果你没有这样的系统访问权限，你可以从[Runpod Community Cloud](https://www.runpod.io/)租用一台双3090的机器，大约每小时$0.60。还有许多其他供应商可以选择；[cloud-gpus](https://cloud-gpus.com/)是一个很好的地方查看提供的服务。

你需要安装最新版本的Transformers、PEFT和bitsandbytes（如果你使用HQQ也需要）。然后，克隆[我们的仓库](https://github.com/AnswerDotAI/fsdp_qlora/tree/main)，按照那里的README进行操作。运行`python train.py --help`会显示可用的选项。要在包含的alpaca数据集上用两张24GB显卡训练llama2-7b，你可能会运行：

> `python train.py --train_type qlora --dataset alpaca --batch_size 8 --gradient_accumulation_steps 2 --output_dir qlora_output --log_to wandb`

我们已经将所有需要的东西都压缩到这一个文件中，以便更容易看到正在发生的事情并在必要时进行修改。

你应该将这个脚本视为一个α/预览版本。虽然我们已经成功地在各种硬件上使用它来训练多种实用模型，但现在还处于早期阶段。如果你不熟悉测试和调试模型，我们建议暂时等待几个月，让社区更充分地测试这种方法。

**更新：** 我们很高兴看到在Hugging Face生态系统中已经开始支持，通过对[Accelerate](https://github.com/huggingface/accelerate/pull/2544)、[Transformers](https://github.com/huggingface/transformers/pull/29587)、[TRL](https://github.com/huggingface/trl/pull/1416)和[PEFT](https://github.com/huggingface/peft/pull/1550)的更改。我们的代码也已经并入Axolotl微调库，并被用于训练Mixtral和其他模型。
