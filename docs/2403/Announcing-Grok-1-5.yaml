- en: <!--yml
  id: totrans-split-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-split-1
  prefs: []
  type: TYPE_NORMAL
  zh: 'category: 未分类'
- en: 'date: 2024-05-29 12:47:19'
  id: totrans-split-2
  prefs: []
  type: TYPE_NORMAL
  zh: 'date: 2024-05-29 12:47:19'
- en: -->
  id: totrans-split-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: Announcing Grok-1.5
  id: totrans-split-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 宣布Grok-1.5
- en: 来源：[https://x.ai/blog/grok-1.5](https://x.ai/blog/grok-1.5)
  id: totrans-split-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://x.ai/blog/grok-1.5](https://x.ai/blog/grok-1.5)
- en: Long Context Understanding
  id: totrans-split-6
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 长上下文理解
- en: A new feature in Grok-1.5 is the capability to process long contexts of up to
    128K tokens within its context window. This allows Grok to have an increased memory
    capacity of up to 16 times the previous context length, enabling it to utilize
    information from substantially longer documents.
  id: totrans-split-7
  prefs: []
  type: TYPE_NORMAL
  zh: Grok-1.5 的新功能是能够在其上下文窗口内处理长达128K令牌的长上下文。这使得Grok的记忆能力比先前的上下文长度增加了16倍，使其能够利用来自极长文档的信息。
- en: Furthermore, the model can handle longer and more complex prompts, while still
    maintaining its instruction-following capability as its context window expands.
    In the Needle In A Haystack (NIAH) evaluation, Grok-1.5 demonstrated powerful
    retrieval capabilities for embedded text within contexts of up to 128K tokens
    in length, achieving perfect retrieval results.
  id: totrans-split-8
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，该模型可以处理更长、更复杂的提示，同时随着其上下文窗口的扩展，仍保持其遵循指令的能力。在“大海捞针”（NIAH）评估中，Grok-1.5 展示了在长达128K令牌长度的上下文中的强大检索能力，实现了完美的检索结果。
- en: Grok-1.5 Infra
  id: totrans-split-9
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Grok-1.5 Infra
- en: Cutting-edge Large Language Model (LLMs) research that runs on massive GPU clusters
    demands robust and flexible infrastructure. Grok-1.5 is built on a custom distributed
    training framework based on JAX, Rust, and Kubernetes. This training stack enables
    our team to prototype ideas and train new architectures at scale with minimal
    effort. A major challenge of training LLMs on large compute clusters is maximizing
    reliability and uptime of the training job. Our custom training orchestrator ensures
    that problematic nodes are automatically detected and ejected from the training
    job. We also optimized checkpointing, data loading, and training job restarts
    to minimize downtime in the event of a failure. If working on our training stack
    sounds interesting to you, [apply to join the team](https://x.ai/careers).
  id: totrans-split-10
  prefs: []
  type: TYPE_NORMAL
  zh: 运行在大规模GPU集群上的前沿大语言模型（LLMs）研究需要强大而灵活的基础设施。Grok-1.5基于JAX、Rust和Kubernetes构建了自定义分布式训练框架。这种训练堆栈使我们的团队能够在大规模上原型化想法，并训练新的架构，几乎不费吹灰之力。在大型计算集群上训练LLMs的一个主要挑战是最大化训练作业的可靠性和正常运行时间。我们的自定义训练协调器确保自动检测到问题节点，并将其从训练作业中剔除。我们还优化了检查点、数据加载和训练作业重启，以最小化在故障发生时的停机时间。如果您对我们的训练堆栈感兴趣，请[申请加入我们的团队](https://x.ai/careers)。
- en: Looking Ahead
  id: totrans-split-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 展望未来
- en: Grok-1.5 will soon be available to early testers, and we look forward to receiving
    your feedback to help us improve Grok. As we gradually roll out Grok-1.5 to a
    wider audience, we are excited to introduce several new features over the coming
    days.
  id: totrans-split-12
  prefs: []
  type: TYPE_NORMAL
  zh: Grok-1.5 将很快提供给早期测试者，我们期待收到您的反馈，以帮助我们改进Grok。随着我们逐步向更广泛的受众推出Grok-1.5，我们很高兴在未来几天介绍几个新功能。
- en: '*Note that the GPT-4 scores are taken from the March 2023 release. For MATH
    and GSM8K, we present maj@1 results. For HumanEval, we report pass@1 benchmark
    scores.*'
  id: totrans-split-13
  prefs: []
  type: TYPE_NORMAL
  zh: '*请注意，GPT-4分数来自2023年3月的发布。对于MATH和GSM8K，我们展示maj@1的结果。对于HumanEval，我们报告pass@1的基准分数。*'
