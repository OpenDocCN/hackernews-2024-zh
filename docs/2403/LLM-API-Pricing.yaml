- en: <!--yml
  id: totrans-split-0
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  id: totrans-split-1
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-05-29 12:38:19'
  id: totrans-split-2
  prefs: []
  type: TYPE_NORMAL
- en: -->
  id: totrans-split-3
  prefs: []
  type: TYPE_NORMAL
- en: LLM API Pricing
  id: totrans-split-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://www.botgenuity.com/tools/llm-pricing](https://www.botgenuity.com/tools/llm-pricing)
  id: totrans-split-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Hey there! Let's dive into the fascinating world of AI and the different flavors
    of Large Language Models (LLMs) offered by the big players like OpenAI, Anthropic,
    Google, Cohere, and Meta. If you're thinking about incorporating these brainy
    bots into your projects, getting a handle on their pricing is pretty essential.
    So, let's break it down, shall we?
  id: totrans-split-6
  prefs: []
  type: TYPE_NORMAL
- en: The Lowdown on Tokens
  id: totrans-split-7
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: First off, the pricing for these AI wonders usually revolves around something
    called "tokens." Imagine a token as a tiny slice of a word. To put it in perspective,
    1,000 tokens are roughly equivalent to about 750 words. For example, the sentence
    "This paragraph is 5 tokens" counts as 5 tokens itself.
  id: totrans-split-8
  prefs: []
  type: TYPE_NORMAL
- en: A handy rule of thumb is that in English, a token is about four characters long,
    which works out to roughly three-quarters of a word. If you're working with languages
    other than English, like Japanese, the math changes a bit.
  id: totrans-split-9
  prefs: []
  type: TYPE_NORMAL
- en: What's the Deal with Context Length?
  id: totrans-split-10
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: When we talk about LLMs, especially those from OpenAI, you'll often hear about
    "context length." This is a key concept because it affects how well the model
    performs, what it can do, and, yep, how much it costs.
  id: totrans-split-11
  prefs: []
  type: TYPE_NORMAL
- en: So, What Exactly is Context Length?
  id: totrans-split-12
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Think of context length as the model's short-term memory for the task at hand.
    It's the amount of info (or number of tokens) the model can juggle at any given
    moment. Say a model has a context length of 8,000 tokens; it means it can consider
    up to 8,000 tokens from what you feed it in one go.
  id: totrans-split-13
  prefs: []
  type: TYPE_NORMAL
- en: Why Should You Care About Context Length?
  id: totrans-split-14
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '**Task Complexity**: Bigger context lengths let the model tackle more complex
    stuff, like summarizing a long read or digging into detailed documents.'
  id: totrans-split-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Smooth Conversations**: For chatbots, a longer context means the model can
    remember more of the chat, leading to replies that make more sense and are more
    on point.'
  id: totrans-split-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Price Tag**: Generally, the longer the context length, the pricier the model
    because it needs more computing oomph.'
  id: totrans-split-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Different Models for Different Needs
  id: totrans-split-18
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The big names in AI have cooked up a variety of models, each with its own strengths
    and price points, and they usually charge per 1,000 tokens.
  id: totrans-split-19
  prefs: []
  type: TYPE_NORMAL
- en: '**OpenAI GPT-4**: This one''s a bit of a know-it-all, great at following complex
    instructions and solving tough problems. It''s pricier and not the fastest kid
    on the block. The new GPT-4 Turbo version, though, is three times cheaper and
    can handle a whopping 128K tokens at once! Also, you can access it through Microsoft''s
    Azure OpenAI Service.'
  id: totrans-split-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**OpenAI GPT-3.5 Turbo**: Optimized for chit-chat, making it a go-to for chatbots
    and conversational interfaces. It''s speedy and won''t break the bank. Available
    through Microsoft''s Azure OpenAI Service too.'
  id: totrans-split-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Anthropic''s Claude 3**: Known for its impressive 200k token context length,
    making it a champ at summarizing or handling Q&As on hefty documents. The trade-off?
    It''s on the slower and pricier side.'
  id: totrans-split-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Llama 2**: Meta''s gift to the world, Llama 2 is an open-source model that''s
    pretty much on par with GPT-3.5 Turbo in performance and can even give GPT-4 a
    run for its money in English text summarization—at 30x less cost! The catch? It''s
    English-only.'
  id: totrans-split-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Gemini**: Google''s latest, split into Gemini Ultra, Gemini Pro, and Gemini
    Nano, announced on December 6, 2023\. Gemini Ultra is eyeing the throne currently
    held by OpenAI''s GPT-4, while Gemini Pro is more akin to GPT-3.5 in terms of
    performance.'
  id: totrans-split-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**PaLM 2**: An older model from Google that shines in multilingual, reasoning,
    and coding tasks. Trained on texts in over 100 languages, it''s a whiz at navigating
    complex language nuances and boasts impressive logic and coding skills.'
  id: totrans-split-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Mistral**: A newcomer on the scene, Mistral AI has released some nifty open-source
    models that are both fast and affordable. Mistral 7B and Mistral 8x7B (Mixtral)
    are standout options, offering performance comparable to GPT-3.5 Turbo at 2.5x
    less cost. Mistral Large, though private, is showing promise in reasoning tasks
    across several languages.'
  id: totrans-split-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**DBRX**: General-purpose LLM created by Databricks. Across a range of standard
    benchmarks, DBRX sets a new state-of-the-art for established open LLMs. Moreover,
    it provides the open community and enterprises building their own LLMs with capabilities
    that were previously limited to closed model APIs; according to [the measurements](https://www.databricks.com/blog/introducing-dbrx-new-state-art-open-llm),
    it surpasses GPT-3.5, and it is competitive with Gemini 1.0 Pro. It is an especially
    capable code model, surpassing specialized models like CodeLLaMA-70B on programming,
    in addition to its strength as a general-purpose LLM.'
  id: totrans-split-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: And there you have it—a whirlwind tour of the LLM pricing landscape. Whether
    you're building the next great app or just dabbling in AI, there's a model out
    there that fits the bill. Happy coding!
  id: totrans-split-28
  prefs: []
  type: TYPE_NORMAL
