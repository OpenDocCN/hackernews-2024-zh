- en: <!--yml
  id: totrans-split-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-split-1
  prefs: []
  type: TYPE_NORMAL
  zh: 'category: 未分类'
- en: 'date: 2024-05-29 12:38:19'
  id: totrans-split-2
  prefs: []
  type: TYPE_NORMAL
  zh: 'date: 2024-05-29 12:38:19'
- en: -->
  id: totrans-split-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: LLM API Pricing
  id: totrans-split-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: LLM API 价格
- en: 来源：[https://www.botgenuity.com/tools/llm-pricing](https://www.botgenuity.com/tools/llm-pricing)
  id: totrans-split-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://www.botgenuity.com/tools/llm-pricing](https://www.botgenuity.com/tools/llm-pricing)
- en: Hey there! Let's dive into the fascinating world of AI and the different flavors
    of Large Language Models (LLMs) offered by the big players like OpenAI, Anthropic,
    Google, Cohere, and Meta. If you're thinking about incorporating these brainy
    bots into your projects, getting a handle on their pricing is pretty essential.
    So, let's break it down, shall we?
  id: totrans-split-6
  prefs: []
  type: TYPE_NORMAL
  zh: 嗨！让我们深入探讨人工智能的迷人世界，以及由OpenAI、Anthropic、Google、Cohere和Meta等大公司提供的不同风味的大语言模型（LLMs）。如果你考虑将这些聪明的机器人纳入你的项目中，了解它们的定价是相当重要的。所以，让我们来逐步分析一下，好吗？
- en: The Lowdown on Tokens
  id: totrans-split-7
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 关于Tokens的详解
- en: First off, the pricing for these AI wonders usually revolves around something
    called "tokens." Imagine a token as a tiny slice of a word. To put it in perspective,
    1,000 tokens are roughly equivalent to about 750 words. For example, the sentence
    "This paragraph is 5 tokens" counts as 5 tokens itself.
  id: totrans-split-8
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，这些人工智能奇迹的定价通常围绕着被称为 "tokens" 的东西。想象一下，token 就像单词的一个小片段。举例来说，1,000 个 tokens
    大约相当于约 750 个单词。例如，句子 "This paragraph is 5 tokens" 本身就算作 5 个 tokens。
- en: A handy rule of thumb is that in English, a token is about four characters long,
    which works out to roughly three-quarters of a word. If you're working with languages
    other than English, like Japanese, the math changes a bit.
  id: totrans-split-9
  prefs: []
  type: TYPE_NORMAL
  zh: 一个方便的经验法则是，在英语中，一个 token 大约是四个字符长，相当于大约三分之四个单词。如果你使用的是日语等非英语语言，数学计算会有所不同。
- en: What's the Deal with Context Length?
  id: totrans-split-10
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 上下文长度有什么关系？
- en: When we talk about LLMs, especially those from OpenAI, you'll often hear about
    "context length." This is a key concept because it affects how well the model
    performs, what it can do, and, yep, how much it costs.
  id: totrans-split-11
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们讨论LLMs时，尤其是OpenAI的时候，你经常会听到 "上下文长度" 这个词。这是一个关键概念，因为它影响模型的表现如何，它能做什么，以及，没错，它的成本有多高。
- en: So, What Exactly is Context Length?
  id: totrans-split-12
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 那么，究竟什么是上下文长度？
- en: Think of context length as the model's short-term memory for the task at hand.
    It's the amount of info (or number of tokens) the model can juggle at any given
    moment. Say a model has a context length of 8,000 tokens; it means it can consider
    up to 8,000 tokens from what you feed it in one go.
  id: totrans-split-13
  prefs: []
  type: TYPE_NORMAL
  zh: 把上下文长度看作是模型在手头任务的短期记忆。它是模型在任何给定时刻能够处理的信息量（或者说 token 数量）。比如一个模型的上下文长度为 8,000
    个 tokens，意味着它可以在一次输入中考虑多达 8,000 个 tokens。
- en: Why Should You Care About Context Length?
  id: totrans-split-14
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 你为什么应该关注上下文长度呢？
- en: '**Task Complexity**: Bigger context lengths let the model tackle more complex
    stuff, like summarizing a long read or digging into detailed documents.'
  id: totrans-split-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**任务复杂性**：更大的上下文长度让模型可以处理更复杂的任务，比如总结长文或深入详细文件。'
- en: '**Smooth Conversations**: For chatbots, a longer context means the model can
    remember more of the chat, leading to replies that make more sense and are more
    on point.'
  id: totrans-split-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**流畅对话**：对于聊天机器人来说，更长的上下文意味着模型可以记住更多的聊天内容，导致回复更有意义、更切题。'
- en: '**Price Tag**: Generally, the longer the context length, the pricier the model
    because it needs more computing oomph.'
  id: totrans-split-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**价格标签**：一般来说，上下文长度越长，模型的价格越高，因为需要更多的计算能力。'
- en: Different Models for Different Needs
  id: totrans-split-18
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 不同需求的不同模型
- en: The big names in AI have cooked up a variety of models, each with its own strengths
    and price points, and they usually charge per 1,000 tokens.
  id: totrans-split-19
  prefs: []
  type: TYPE_NORMAL
  zh: AI领域的大佬们研发了各种模型，每个模型都有其独特的优势和价格点，通常按每 1,000 个 tokens 计费。
- en: '**OpenAI GPT-4**: This one''s a bit of a know-it-all, great at following complex
    instructions and solving tough problems. It''s pricier and not the fastest kid
    on the block. The new GPT-4 Turbo version, though, is three times cheaper and
    can handle a whopping 128K tokens at once! Also, you can access it through Microsoft''s
    Azure OpenAI Service.'
  id: totrans-split-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**OpenAI GPT-4**：这款模型有点像个全知全能者，擅长遵循复杂的指令和解决棘手的问题。价格较高，速度并不是最快的。不过，新的 GPT-4
    Turbo 版本则便宜三倍，并且可以一次处理高达 128K 个 tokens！此外，你也可以通过微软的Azure OpenAI服务访问它。'
- en: '**OpenAI GPT-3.5 Turbo**: Optimized for chit-chat, making it a go-to for chatbots
    and conversational interfaces. It''s speedy and won''t break the bank. Available
    through Microsoft''s Azure OpenAI Service too.'
  id: totrans-split-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**OpenAI GPT-3.5 Turbo**：优化用于闲聊，使其成为聊天机器人和对话界面的首选。速度快，价格亲民。也可以通过微软的Azure OpenAI服务获得。'
- en: '**Anthropic''s Claude 3**: Known for its impressive 200k token context length,
    making it a champ at summarizing or handling Q&As on hefty documents. The trade-off?
    It''s on the slower and pricier side.'
  id: totrans-split-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Anthropic的Claude 3**: 以其出色的20万标记上下文长度而闻名，使其在总结或处理庞大文件上具有优势。不过，它的缺点是速度较慢且价格较高。'
- en: '**Llama 2**: Meta''s gift to the world, Llama 2 is an open-source model that''s
    pretty much on par with GPT-3.5 Turbo in performance and can even give GPT-4 a
    run for its money in English text summarization—at 30x less cost! The catch? It''s
    English-only.'
  id: totrans-split-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Llama 2**: Meta为世界带来的礼物，Llama 2是一款开源模型，在性能上与GPT-3.5 Turbo相当，并且在英文文本摘要方面甚至可以与GPT-4一较高下，成本仅为其30倍！不过，它仅支持英文。'
- en: '**Gemini**: Google''s latest, split into Gemini Ultra, Gemini Pro, and Gemini
    Nano, announced on December 6, 2023\. Gemini Ultra is eyeing the throne currently
    held by OpenAI''s GPT-4, while Gemini Pro is more akin to GPT-3.5 in terms of
    performance.'
  id: totrans-split-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Gemini**: 谷歌的最新产品，分为Gemini Ultra、Gemini Pro和Gemini Nano，于2023年12月6日宣布推出。Gemini
    Ultra目前正在竞争OpenAI的GPT-4的王座，而Gemini Pro在性能上更接近于GPT-3.5。'
- en: '**PaLM 2**: An older model from Google that shines in multilingual, reasoning,
    and coding tasks. Trained on texts in over 100 languages, it''s a whiz at navigating
    complex language nuances and boasts impressive logic and coding skills.'
  id: totrans-split-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**PaLM 2**: 谷歌的一款老模型，擅长多语言、推理和编码任务。它在超过100种语言的文本上进行训练，能够精通复杂语言细微差别，并展示出令人印象深刻的逻辑和编码能力。'
- en: '**Mistral**: A newcomer on the scene, Mistral AI has released some nifty open-source
    models that are both fast and affordable. Mistral 7B and Mistral 8x7B (Mixtral)
    are standout options, offering performance comparable to GPT-3.5 Turbo at 2.5x
    less cost. Mistral Large, though private, is showing promise in reasoning tasks
    across several languages.'
  id: totrans-split-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Mistral**: 一个新晋选手，Mistral AI发布了一些灵巧的开源模型，速度快且价格实惠。Mistral 7B和Mistral 8x7B（Mixtral）是出色的选择，提供了与GPT-3.5
    Turbo相当的性能，成本降低了2.5倍。虽然Mistral Large是私有的，但在多语言推理任务中显示出了潜力。'
- en: '**DBRX**: General-purpose LLM created by Databricks. Across a range of standard
    benchmarks, DBRX sets a new state-of-the-art for established open LLMs. Moreover,
    it provides the open community and enterprises building their own LLMs with capabilities
    that were previously limited to closed model APIs; according to [the measurements](https://www.databricks.com/blog/introducing-dbrx-new-state-art-open-llm),
    it surpasses GPT-3.5, and it is competitive with Gemini 1.0 Pro. It is an especially
    capable code model, surpassing specialized models like CodeLLaMA-70B on programming,
    in addition to its strength as a general-purpose LLM.'
  id: totrans-split-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**DBRX**: 由Databricks创建的通用型LLM。在一系列标准基准测试中，DBRX创造了新的开放型LLM技术水平。此外，它为开放社区和企业建立他们自己的LLM提供了以往仅限于封闭模型API的功能；根据[测量数据](https://www.databricks.com/blog/introducing-dbrx-new-state-art-open-llm)，它超越了GPT-3.5，并且在与Gemini
    1.0 Pro的竞争中表现不俗。它尤其擅长作为通用型LLM的编码模型，超越了像CodeLLaMA-70B这样的专业模型在编程方面的能力。'
- en: And there you have it—a whirlwind tour of the LLM pricing landscape. Whether
    you're building the next great app or just dabbling in AI, there's a model out
    there that fits the bill. Happy coding!
  id: totrans-split-28
  prefs: []
  type: TYPE_NORMAL
  zh: 至此，您已经对LLM定价领域有了一个快速了解。无论您是要构建下一个伟大的应用程序还是只是涉猎人工智能，都可以找到适合您需求的模型。祝您编程愉快！
