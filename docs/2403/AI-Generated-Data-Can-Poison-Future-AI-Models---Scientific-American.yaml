- en: <!--yml
  id: totrans-split-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-split-1
  prefs: []
  type: TYPE_NORMAL
  zh: 'category: 未分类'
- en: 'date: 2024-05-27 14:46:13'
  id: totrans-split-2
  prefs: []
  type: TYPE_NORMAL
  zh: 'date: 2024-05-27 14:46:13'
- en: -->
  id: totrans-split-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: AI-Generated Data Can Poison Future AI Models | Scientific American
  id: totrans-split-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: AI生成的数据可能会对未来的AI模型产生负面影响 | Scientific American
- en: 来源：[https://www.scientificamerican.com/article/ai-generated-data-can-poison-future-ai-models/](https://www.scientificamerican.com/article/ai-generated-data-can-poison-future-ai-models/)
  id: totrans-split-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://www.scientificamerican.com/article/ai-generated-data-can-poison-future-ai-models/](https://www.scientificamerican.com/article/ai-generated-data-can-poison-future-ai-models/)
- en: 'Thanks to a boom in [generative artificial intelligence](https://www.scientificamerican.com/podcast/episode/why-were-worried-about-generative-ai/),
    programs that can produce text, computer code, images and music are readily available
    to the average person. And we’re already using them: AI content is [taking over
    the Internet](https://www.wsj.com/articles/chatgpt-already-floods-some-corners-of-the-internet-with-spam-its-just-the-beginning-9c86ea25?mod=tech_lead_pos6&mc_cid=987d4025e9&mc_eid=74dd22853c),
    and text generated by “[large language models](https://www.scientificamerican.com/article/what-the-new-gpt-4-ai-can-do/)”
    is filling hundreds of websites, including CNET and Gizmodo. But as AI developers
    scrape the Internet, AI-generated content may soon enter the data sets used to
    [train new models](https://www.scientificamerican.com/article/why-we-need-to-see-inside-ais-black-box/)
    to respond like humans. Some experts say that will inadvertently introduce errors
    that build up with each succeeding generation of models.'
  id: totrans-split-6
  prefs: []
  type: TYPE_NORMAL
  zh: 多亏了[生成人工智能](https://www.scientificamerican.com/podcast/episode/why-were-worried-about-generative-ai/)的蓬勃发展，能够生成文本、计算机代码、图像和音乐的程序已经轻易地供普通人使用。而我们已经在使用它们：AI内容正在[占领互联网](https://www.wsj.com/articles/chatgpt-already-floods-some-corners-of-the-internet-with-spam-its-just-the-beginning-9c86ea25?mod=tech_lead_pos6&mc_cid=987d4025e9&mc_eid=74dd22853c)，并且由“[大语言模型](https://www.scientificamerican.com/article/what-the-new-gpt-4-ai-can-do/)”生成的文本填满了包括CNET和Gizmodo在内的数百个网站。但是随着AI开发人员从互联网上抓取信息，AI生成的内容很快可能会进入用于[训练新模型](https://www.scientificamerican.com/article/why-we-need-to-see-inside-ais-black-box/)以模仿人类反应的数据集中。一些专家称，这将无意中引入随着每一代模型逐渐积累的错误。
- en: A growing body of evidence supports this idea. It suggests that a training diet
    of AI-generated text, even in small quantities, eventually becomes “poisonous”
    to the model being trained. Currently there are few obvious antidotes. “While
    it may not be an issue right now or in, let’s say, a few months, I believe it
    will become a consideration in a few years,” says Rik Sarkar, a computer scientist
    at the School of Informatics at the University of Edinburgh in Scotland.
  id: totrans-split-7
  prefs: []
  type: TYPE_NORMAL
  zh: 支持这一观点的证据越来越多。它表明，即使是少量的AI生成文本训练，最终也会对正在训练的模型产生“毒害”。目前很少有明显的解毒剂。“虽然现在或者说在未来的几个月内可能不是问题，但我相信在几年内将成为一个考虑因素，”苏格兰爱丁堡大学信息学院的计算机科学家Rik
    Sarkar说道。
- en: '[https://www.youtube.com/embed/ZWvTr5wKGCA](https://www.youtube.com/embed/ZWvTr5wKGCA)'
  id: totrans-split-8
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://www.youtube.com/embed/ZWvTr5wKGCA](https://www.youtube.com/embed/ZWvTr5wKGCA)'
- en: VIDEO
  id: totrans-split-9
  prefs: []
  type: TYPE_NORMAL
  zh: 视频
- en: '* * *'
  id: totrans-split-10
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: On supporting science journalism
  id: totrans-split-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 支持科学新闻报道
- en: If you're enjoying this article, consider supporting our award-winning journalism
    by [subscribing](/getsciam/). By purchasing a subscription you are helping to
    ensure the future of impactful stories about the discoveries and ideas shaping
    our world today.
  id: totrans-split-12
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您喜欢本文，请考虑支持我们屡获殊荣的新闻报道，通过[订阅](/getsciam/)来确保我们今天世界上形成的发现和思想的未来。
- en: '* * *'
  id: totrans-split-13
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: 'The possibility of AI models tainting themselves may be a bit analogous to
    a certain 20th-century dilemma. After the first atomic bombs were detonated at
    World War II’s end, decades of nuclear testing spiced Earth’s atmosphere with
    a dash of radioactive fallout. When that air entered newly-made steel, it brought
    elevated radiation with it. For particularly radiation-sensitive steel applications,
    such as Geiger counter consoles, that fallout poses an obvious problem: it won’t
    do for a Geiger counter to flag itself. Thus, a rush began for a dwindling supply
    of low-radiation metal. Scavengers [scoured](https://www.theatlantic.com/science/archive/2019/10/search-dark-matter-depends-ancient-shipwrecks/600718/)
    old shipwrecks to extract scraps of prewar steel. Now some insiders believe a
    similar cycle is set to repeat in generative AI—with training data instead of
    steel.'
  id: totrans-split-14
  prefs: []
  type: TYPE_NORMAL
  zh: AI 模型可能自我污染的可能性有点类似于某个二十世纪的困境。在第二次世界大战结束后第一批原子弹爆炸之后，数十年的核试验使地球大气中充满了放射性尘埃。当这些空气进入新制造的钢材时，就会带来升高的辐射。对于特别对辐射敏感的钢材应用，如盖革计数器控制台，这种尘埃带来了一个明显的问题：盖革计数器不能自我检测。因此，人们开始争相获取日益减少的低辐射金属供应。搜寻者从旧沉船中回收前战前钢材的碎片。现在，一些内行人士认为，在生成式
    AI 领域，一个类似的循环即将重演，只不过这次是在训练数据而不是钢材上。
- en: Researchers can watch AI’s poisoning in action. For instance, start with a language
    model trained on human-produced data. Use the model to generate some AI output.
    Then use that output to train a new instance of the model and use the resulting
    output to train a third version, and so forth. With each iteration, errors build
    atop one another. The 10th model, prompted to write about historical English architecture,
    [spews out gibberish about jackrabbits](https://arxiv.org/abs/2305.17493v2).
  id: totrans-split-15
  prefs: []
  type: TYPE_NORMAL
  zh: 研究人员可以看到 AI 自我毒化的过程。例如，从一个以人类生成数据为训练基础的语言模型开始。使用该模型生成一些 AI 输出。然后使用该输出来训练一个新实例的模型，再使用结果输出来训练第三个版本，依此类推。每个迭代中，错误会堆积在一起。第十个模型，被要求写关于历史英国建筑的内容，竟然输出了关于丘兔的胡言乱语。
- en: “It gets to a point where your model is practically meaningless,” says Ilia
    Shumailov, a machine learning researcher at the University of Oxford.
  id: totrans-split-16
  prefs: []
  type: TYPE_NORMAL
  zh: “到了某个程度，你的模型几乎是无意义的，”牛津大学的机器学习研究员 Ilia Shumailov 表示。
- en: Shumailov and his colleagues call this phenomenon “model collapse.” They observed
    it in a language model called OPT-125m, as well as a different AI model that generates
    handwritten-looking numbers and even a simple model that tries to separate two
    probability distributions. “Even in the simplest of models, it’s already happening,”
    Shumailov says. “I promise you, in more complicated models, it’s 100 percent already
    happening as well.”
  id: totrans-split-17
  prefs: []
  type: TYPE_NORMAL
  zh: Shumailov 和他的同事称这种现象为“模型崩溃”。他们观察到这种现象发生在一个名为 OPT-125m 的语言模型上，以及另一个生成看起来像手写数字的
    AI 模型，甚至是一个试图分离两个概率分布的简单模型上。Shumailov 表示：“即使在最简单的模型中，这种现象已经发生。”“我向你保证，在更复杂的模型中，这种现象已经百分之百发生了。”
- en: In a recent preprint study, Sarkar and his colleagues in Madrid and Edinburgh
    [conducted a similar experiment](https://arxiv.org/abs/2306.06130) with a type
    of AI image generator called a diffusion model. Their first model in this series
    could generate recognizable flowers or birds. By their third model, those pictures
    had devolved into blurs.
  id: totrans-split-18
  prefs: []
  type: TYPE_NORMAL
  zh: 在最近的预印本研究中，Sarkar 和他在马德里和爱丁堡的同事们进行了一项类似的实验，使用了一种名为扩散模型的 AI 图像生成器。他们的系列中的第一个模型可以生成可识别的花朵或鸟类图片。到了第三个模型，这些图片已经变得模糊不清。
- en: Other tests showed that even a partly AI-generated training data set was toxic,
    Sarkar says. “As long as some reasonable fraction is AI-generated, it becomes
    an issue,” he explains. “Now exactly how much AI-generated content is needed to
    cause issues in what sort of models is something that remains to be studied.”
  id: totrans-split-19
  prefs: []
  type: TYPE_NORMAL
  zh: 其他测试显示，即使是部分由 AI 生成的训练数据集也是有毒的，Sarkar 表示。“只要有一定比例的 AI 生成内容，就会成为问题，”他解释道。“现在，究竟需要多少
    AI 生成的内容才会在什么样的模型中引起问题，这是需要进一步研究的。”
- en: Both groups experimented with relatively modest models—programs that are smaller
    and use fewer training data than the likes of the language model GPT-4 or the
    image generator Stable Diffusion. It’s possible that larger models will prove
    more resistant to model collapse, but researchers say there is little reason to
    believe so.
  id: totrans-split-20
  prefs: []
  type: TYPE_NORMAL
  zh: 两个研究小组都进行了相对谨慎的实验，使用的是相对较小、使用的训练数据较少的程序，而不是像语言模型 GPT-4 或图像生成器 Stable Diffusion
    那样的大型模型。可能更大的模型会更抗拒模型崩溃，但研究人员表示没有太多理由相信这一点。
- en: The research so far indicates that a model will suffer most at the “tails” of
    its data—the data elements that are less frequently represented in a model’s training
    set. Because these tails include data that are further from the “norm,” a model
    collapse could cause the AI’s output to lose the diversity that researchers say
    is distinctive about human data. In particular, Shumailov fears this will exacerbate
    models’ existing biases against marginalized groups. “It’s quite clear that the
    future is the models becoming more biased,” he says. “Explicit effort needs to
    be put in order to curtail it.”
  id: totrans-split-21
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止的研究表明，模型在其数据的“尾部”可能会受到最大的影响——这些数据元素在模型的训练集中出现频率较低。由于这些尾部数据包括远离“正常”数据的数据，模型的崩溃可能导致AI的输出失去研究人员认为是人类数据的独特性。尤其是Shumailov担心这会加剧模型对边缘群体现有偏见的情况。“很明显，未来模型会变得更加偏见”，他说。“必须付出明确的努力以限制它。”
- en: 'Perhaps all this is speculation, but AI-generated content is already beginning
    to enter realms that machine-learning engineers rely on for training data. Take
    language models: even mainstream news outlets [have begun publishing AI-generated
    articles](https://www.wired.com/story/cnet-published-ai-generated-stories-then-its-staff-pushed-back/),
    and some Wikipedia editors [want to use language models](https://www.vice.com/en/article/v7bdba/ai-is-tearing-wikipedia-apart)
    to produce content for the site.'
  id: totrans-split-22
  prefs: []
  type: TYPE_NORMAL
  zh: 或许这一切都是猜测，但AI生成的内容已经开始进入机器学习工程师依赖的领域。以语言模型为例：甚至主流新闻媒体[已经开始发布AI生成的文章](https://www.wired.com/story/cnet-published-ai-generated-stories-then-its-staff-pushed-back/)，还有一些维基百科编辑[希望使用语言模型](https://www.vice.com/en/article/v7bdba/ai-is-tearing-wikipedia-apart)来为网站生成内容。
- en: “I feel like we’re kind of at this inflection point where a lot of the existing
    tools that we use to train these models are quickly becoming saturated with synthetic
    text,” says Veniamin Veselovskyy, a graduate student at the Swiss Federal Institute
    of Technology in Lausanne (EPFL).
  id: totrans-split-23
  prefs: []
  type: TYPE_NORMAL
  zh: “我觉得我们正处于一个转折点，很多我们用来训练这些模型的现有工具很快就会被合成文本所饱和”，瑞士洛桑联邦理工学院（EPFL）的研究生Veniamin Veselovskyy说道。
- en: There are warning signs that AI-generated data might enter model training from
    elsewhere, too. Machine-learning engineers have long relied on crowd-work platforms,
    such as Amazon’s Mechanical Turk, to annotate their models’ training data or to
    review output. Veselovskyy and his colleagues at EPFL asked Mechanical Turk workers
    to summarize medical research abstracts. They found that around [a third of the
    summaries had ChatGPT’s touch](https://arxiv.org/abs/2306.07899).
  id: totrans-split-24
  prefs: []
  type: TYPE_NORMAL
  zh: 还有一些警示信号表明，AI生成的数据可能会从其他地方进入模型训练中。机器学习工程师长期以来一直依赖于众包平台，如亚马逊的Mechanical Turk，来注释他们模型的训练数据或审查输出。Veselovskyy及其EPFL的同事们要求Mechanical
    Turk的工作者总结医学研究摘要。他们发现大约有[三分之一的总结带有ChatGPT的痕迹](https://arxiv.org/abs/2306.07899)。
- en: The EPFL group’s work, released on the preprint server arXiv.org last month,
    examined only 46 responses from Mechanical Turk workers, and summarizing is a
    classic language model task. But the result has raised a specter in machine-learning
    engineers’ minds. “It is much easier to annotate textual data with ChatGPT, and
    the results are extremely good,” says Manoel Horta Ribeiro, a graduate student
    at EPFL. Researchers such as Veselovskyy and Ribeiro have begun considering ways
    to protect the humanity of crowdsourced data, including tweaking websites such
    as Mechanical Turk in ways that discourage users from turning to language models
    and redesigning experiments to encourage more human data.
  id: totrans-split-25
  prefs: []
  type: TYPE_NORMAL
  zh: EPFL团队的工作上个月在预印本服务器arXiv.org上发布，仅研究了来自机械土耳其工作者的46个回应，总结是一个经典的语言模型任务。但是这一结果引起了机器学习工程师们的一种担忧。“使用ChatGPT标注文本数据要容易得多，而且结果非常好”，EPFL的研究生Manoel
    Horta Ribeiro说道。像Veselovskyy和Ribeiro这样的研究人员已经开始考虑保护众包数据的人性化方法，包括调整Mechanical Turk等网站的方式，以防止用户转向语言模型，并重新设计实验以鼓励更多的人类数据。
- en: 'Against the threat of model collapse, what is a hapless machine-learning engineer
    to do? The answer could be the equivalent of prewar steel in a Geiger counter:
    data known to be free (or perhaps as free as possible) from generative AI’s touch.
    For instance, Sarkar suggests the idea of employing “standardized” image data
    sets that would be curated by humans who know their content consists only of human
    creations and freely available for developers to use.'
  id: totrans-split-26
  prefs: []
  type: TYPE_NORMAL
  zh: 针对模型崩溃的威胁，一名倒霉的机器学习工程师该如何应对？答案可能相当于盖革计数器中的战前钢铁：已知不受生成人工智能影响（或者尽可能不受影响）的数据。例如，Sarkar建议采用由了解其内容仅由人类创作组成并且开放供开发人员使用的“标准化”图像数据集的想法。
- en: 'Some engineers may be tempted to pry open the Internet Archive and look up
    content that predates the AI boom, but Shumailov doesn’t see going back to historical
    data as a solution. For one thing, he thinks there may not be enough historical
    information to feed growing models’ demands. For another, such data are just that:
    historical and not necessarily reflective of a changing world.'
  id: totrans-split-27
  prefs: []
  type: TYPE_NORMAL
  zh: 一些工程师可能会试图打开互联网档案库，并查找AI繁荣之前的内容，但Shumailov认为回溯历史数据并非解决方案。首先，他认为可能没有足够的历史信息来满足增长模型的需求。另外，这些数据只是历史性的，并不一定反映变化中的世界。
- en: “If you wanted to collect the news of the past 100 years and try and predict
    the news of today, it’s obviously not going to work, because technology’s changed,”
    Shumailov says. “The lingo has changed. The understanding of the issues has changed.”
  id: totrans-split-28
  prefs: []
  type: TYPE_NORMAL
  zh: “如果你想收集过去100年的新闻并试图预测今天的新闻，显然行不通，因为技术已经改变了，” Shumailov说道。“行话已经改变了。对问题的理解也已经改变了。”
- en: 'The challenge, then, may be more direct: discerning human-generated data from
    synthetic content and filtering out the latter. But even if the technology for
    this existed, it is far from a straightforward task. As Sarkar points out, in
    a world where Adobe Photoshop [allows its users to edit images with generative
    AI](https://www.adobe.com/sensei/generative-ai/firefly.html), is the result an
    AI-generated image—or not?'
  id: totrans-split-29
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，挑战可能更为直接：辨别人类生成的数据与合成内容，并滤除后者。但即使存在这种技术，这也远非一项简单的任务。正如Sarkar所指出的，当Adobe Photoshop允许用户使用生成人工智能编辑图像时，结果是AI生成的图像吗？还是不是？
