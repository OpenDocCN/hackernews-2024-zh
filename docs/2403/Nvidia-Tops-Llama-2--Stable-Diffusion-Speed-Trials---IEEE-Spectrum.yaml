- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-05-29 12:45:06'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: Nvidia Tops Llama 2, Stable Diffusion Speed Trials - IEEE Spectrum
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://spectrum.ieee.org/ai-benchmark-mlperf-llama-stablediffusion](https://spectrum.ieee.org/ai-benchmark-mlperf-llama-stablediffusion)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Times change, and so must benchmarks. Now that we’re firmly in the age of massive
    generative AI, it’s time to add two such behemoths, [Llama 2 70B](https://spectrum.ieee.org/llama-2-llm)
    and [Stable Diffusion](https://spectrum.ieee.org/tag/stable-diffusion) XL, to
    [MLPerf’s](https://mlcommons.org/) inferencing tests. Version 4.0 of the benchmark
    tests more than 8,500 results from 23 submitting organizations. As has been the
    case from the beginning, computers with [Nvidia](https://www.nvidia.com/en-us/ai-data-science/)
    GPUs came out on top, particularly those with its H200 processor. But AI accelerators
    from [Intel](https://www.intel.com/content/www/us/en/artificial-intelligence/overview.html)
    and [Qualcomm](https://www.qualcomm.com/research/artificial-intelligence) were
    in the mix as well.
  prefs: []
  type: TYPE_NORMAL
- en: MLPerf started pushing into the LLM world [last year](https://spectrum.ieee.org/mlperf)
    when it added a text summarization benchmark [GPT-J](https://www.eleuther.ai/artifacts/gpt-j)
    (a 6 billion parameter open-source model). With 70 billion parameters, Llama 2
    is an order of magnitude larger. Therefore it requires what the organizer [MLCommons](https://mlcommons.org/),
    a San Francisco-based AI consortium, calls “a different class of hardware.”
  prefs: []
  type: TYPE_NORMAL
- en: “In terms of model parameters, Llama-2 is a dramatic increase to the models
    in the inference suite,” [Mitchelle Rasquinha](https://www.linkedin.com/in/mrasquinha/),
    a software engineer at [Google](https://spectrum.ieee.org/tag/google) and co-chair
    of the MLPerf Inference working group, said in a press release.
  prefs: []
  type: TYPE_NORMAL
- en: Stable Diffusion XL, the new [text-to-image generation](https://spectrum.ieee.org/ai-art-generator)
    benchmark, comes in at 2.6 billion parameters, less than half the size of GPT-J.
    The recommender system test, revised last year, is larger than both.
  prefs: []
  type: TYPE_NORMAL
- en: MLPerf benchmarks run the range of sizes, with the latest, such as Llama 2 70B
    in the many tens of billions of parameters.MLCommons
  prefs: []
  type: TYPE_NORMAL
- en: The tests are divided between systems meant for use in [data centers](https://mlcommons.org/benchmarks/inference-datacenter/)
    and those intended for use by devices out in the world, or the “[edge](https://mlcommons.org/benchmarks/inference-datacenter/)”
    as its called. For each benchmark, a computer can be tested in what’s called an
    offline mode or in a more realistic manner. In offline mode, it runs through the
    test data as fast as possible to determine its maximum throughput. The more realistic
    tests are meant to simulate things like a stream of data coming from a camera
    in a smartphone, multiple streams of data from all the cameras and sensors in
    a car, or as queries in a data center setup, for example. Additionally, the power
    consumption of some systems was tracked during tasks.
  prefs: []
  type: TYPE_NORMAL
- en: Data center inference results
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The top performers in the new generative AI categories was an Nvidia H200 system
    that combined eight of the GPUs with two [Intel](https://spectrum.ieee.org/tag/intel)
    Xeon CPUs. It managed just under 14 queries per second for Stable Diffusion and
    about 27,000 tokens per second for Llama 2 70B. Its nearest competition were 8-GPU
    H100 systems. And the performance difference wasn’t huge for Stable Diffusion,
    about 1 query per second, but the difference was larger for Llama 2 70B.
  prefs: []
  type: TYPE_NORMAL
- en: H200s are the same [Hopper architecture](https://spectrum.ieee.org/nvidias-next-gpu-shows-that-transformers-are-transforming-ai)
    as the H100, but with about 75 percent more high-bandwidth memory and 43 percent
    more memory bandwidth. According to Nvidia’s [Dave Salvator](https://www.linkedin.com/in/davesalvator/),
    memory is particularly important in LLMs, which perform better if they can fit
    entirely on the chip with other key data. The memory difference showed in the
    Llama 2 results, where H200 sped ahead of H100 by about 45 percent.
  prefs: []
  type: TYPE_NORMAL
- en: According to the company, systems with H100 GPUs were 2.4-2.9 times faster than
    H100 systems from the [results of last September](https://spectrum.ieee.org/mlperf),
    thanks to software improvements.
  prefs: []
  type: TYPE_NORMAL
- en: Although H200 was the star of Nvidia’s benchmark show, its newest GPU architecture,
    [Blackwell](https://spectrum.ieee.org/nvidia-blackwell), officially unveiled last
    week, looms in the background. Salvator wouldn’t say when computers with that
    GPU might debut in the benchmark tables.
  prefs: []
  type: TYPE_NORMAL
- en: For its part, [Intel continued to](https://www.intel.com/content/www/us/en/newsroom/news/new-gaudi-2-xeon-performance-ai-inference.html#gs.7125fy)
    offer its [Gaudi 2](https://habana.ai/products/gaudi2/) accelerator as the only
    option to Nvidia, at least among the companies participating in MLPerf’s inferencing
    benchmarks. On raw performance, Intel’s 7-nanometer chip delivered a little less
    than half the performance of 5-nm H100 in an 8-GPU configuration for Stable Diffusion
    XL. Its Gaudi 2 delivered results closer to one-third the Nvidia performance for
    Llama 2 70B. However, Intel argues that if you’re measuring performance per dollar
    (something they did themselves, not with MLPerf), the Gaudi 2 is about equal to
    the H100\. For Stable Diffusion, Intel calculates it beats H100 by about 25 percent
    on performance per dollar. For Llama 2 70B it’s either an even contest or 21 percent
    worse, depending on whether you’re measuring in server or offline mode.
  prefs: []
  type: TYPE_NORMAL
- en: '**Gaudi 2’s successor, Gaudi 3 is expected to arrive later this year.**'
  prefs: []
  type: TYPE_NORMAL
- en: '**Intel also touted several CPU-only entries that showed a reasonable level
    of inferencing performance is possible in the absence of a GPU, though not on
    Llama 2 70B or Stable Diffusion. This was the first appearance of Intel’s 5th
    generation Xeon CPUs in the MLPerf inferencing competition, and the company claims
    a performance boost ranging from 18 percent to 91 percent over 4th generation
    Xeon systems from September 2023 results.**'
  prefs: []
  type: TYPE_NORMAL
- en: '**Edge inferencing results**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**As large as it is, Llama 2 70B wasn’t tested in the edge category, but Stable
    Diffusion XL was. Here the top performer was a system using two Nvidia L40S GPUs
    and an Intel Xeon CPU. Performance here is measured in latency and in samples
    per second. The system, submitted by Taipei-based cloud infrastructure company
    [Wiwynn](https://www.wiwynn.com/), produced answers in less than 2 seconds in
    single-stream mode. When driven in offline mode, it generates 1.26 results per
    second.**'
  prefs: []
  type: TYPE_NORMAL
- en: '**Power consumption**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**In the data center category, the contest around energy efficiency was between
    Nvidia and Qualcomm. The latter has focused on energy efficient inference since
    introducing the [Cloud AI 100](https://www.qualcomm.com/products/technology/processors/cloud-artificial-intelligence/cloud-ai-100)
    processor more than a year ago. Qualcomm introduced a new generation of the accelerator
    chip the [Cloud AI 100 Ultra](https://www.qualcomm.com/news/onq/2023/11/introducing-qualcomm-cloud-ai-100-ultra)
    late last year, and its first results showed up in the edge and data center performance
    benchmarks above. Compared to the Cloud AI 100 Pro results, Ultra produced a 2.5
    to 3 times performance boost while consuming less than 150 Watts per chip.**'
  prefs: []
  type: TYPE_NORMAL
- en: '**Among the edge inference entrance, Qualcomm was the only company to attempt
    Stable Diffusion XL, managing 0.6 samples per second using 578 watts.**'
  prefs: []
  type: TYPE_NORMAL
- en: '**From Your Site Articles**'
  prefs: []
  type: TYPE_NORMAL
- en: '**Related Articles Around the Web**'
  prefs: []
  type: TYPE_NORMAL
