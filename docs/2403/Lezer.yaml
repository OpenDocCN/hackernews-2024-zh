- en: <!--yml
  id: totrans-split-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-split-1
  prefs: []
  type: TYPE_NORMAL
  zh: 'category: 未分类'
- en: 'date: 2024-05-29 12:37:49'
  id: totrans-split-2
  prefs: []
  type: TYPE_NORMAL
  zh: 'date: 2024-05-29 12:37:49'
- en: -->
  id: totrans-split-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: Lezer
  id: totrans-split-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Lezer
- en: 来源：[https://marijnhaverbeke.nl/blog/lezer.html](https://marijnhaverbeke.nl/blog/lezer.html)
  id: totrans-split-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://marijnhaverbeke.nl/blog/lezer.html](https://marijnhaverbeke.nl/blog/lezer.html)
- en: I keep coming across people who consider parser technology a forbidding, scary
    field of programming. This is nonsense—a small parser can be [very, very simple](http://eloquentjavascript.net/12_language.html#h_cpTTNxAWkQ),
    and provide a wholesome exercise in recursive thinking. At the same time, it is
    true that you *can* make parsing extremely complicated. Mostly, this tends to
    happen when you generalize parsing techniques to work for different grammars.
    Since compiler textbooks usually describe general approaches to parsing, they
    may be to blame for putting people off.
  id: totrans-split-6
  prefs: []
  type: TYPE_NORMAL
  zh: 我经常遇到一些认为解析技术是一门令人生畏的编程领域的人。这是无稽之谈——一个小小的解析器可以是[非常简单的](http://eloquentjavascript.net/12_language.html#h_cpTTNxAWkQ)，并且可以提供一个很好的递归思维练习。同时，事实上确实可以让解析变得非常复杂。大多数情况下，这种情况发生在你将解析技术泛化到不同语法上时。由于编译器教材通常描述了解析的通用方法，他们可能是为人们对解析感到畏惧负责的原因。
- en: This post describes a [new parsing system](https://lezer.codemirror.net) I wrote
    for [CodeMirror](https://codemirror.net), a source code editor. It frames the
    system with some history, and digresses into some neat architectural details.
  id: totrans-split-7
  prefs: []
  type: TYPE_NORMAL
  zh: 这篇文章描述了我为[CodeMirror](https://codemirror.net)写的一个[新的解析系统](https://lezer.codemirror.net)，它用一些历史背景框定了系统，并深入探讨了一些精彩的架构细节。
- en: Editor features like syntax highlighting, bracket matching, code folding, and
    autocompletion all involve some level of parsing. Unfortunately, since editors
    have to handle many different languages, they require a generalized approach to
    parsing.
  id: totrans-split-8
  prefs: []
  type: TYPE_NORMAL
  zh: 编辑器功能，如语法高亮、括号匹配、代码折叠和自动完成，都涉及一定程度的解析。不幸的是，由于编辑器必须处理许多不同的语言，它们需要一种通用的解析方法。
- en: CodeMirror is in the process of being rewritten, and I wanted to improve the
    way it parses its content. Parsing inside of an editor comes with its own unique
    set of constraints, which can be hard to satisfy. Though I had been planning new
    approaches for years, all I had to show for it so far were a pile of dead ends.
  id: totrans-split-9
  prefs: []
  type: TYPE_NORMAL
  zh: CodeMirror正在被重写，我希望改进它解析内容的方式。在编辑器内部进行解析带来了自己独特的一套约束条件，这些条件很难满足。虽然多年来我一直在计划新的方法，但到目前为止，我所能展示的仅仅是一堆死胡同。
- en: 'The constraints that make the parsing problem in a code editor hard are roughly
    these:'
  id: totrans-split-10
  prefs: []
  type: TYPE_NORMAL
  zh: 让代码编辑器中的解析问题变得困难的约束条件大致如下：
- en: The document is constantly changing.
  id: totrans-split-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 文件在不断变化。
- en: You can't do anything expensive. If the parsing works takes too long, it'll
    introduce latency that makes editing feel [slugglish](https://input-delay.glitch.me/)
    and unresponsive.
  id: totrans-split-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你不能做什么昂贵的事情。如果解析过程太耗时，将会导致编辑感觉[迟缓](https://input-delay.glitch.me/)和无响应。
- en: The input is often not in a finished, syntactically correct form. But you still
    have to make some sense of it—nobody wants an editor where most features stop
    working when you have a syntax error in your document.
  id: totrans-split-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 输入通常不是最终的、语法正确的形式。但你仍然需要对其进行一些处理——没有人希望在文档中存在语法错误时，大多数功能都无法正常工作的编辑器。
- en: You often want to be able to mix several languages/grammars in a single document
    (think HTML with JavaScript and CSS embedded in it).
  id: totrans-split-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你经常希望能够在单个文档中混合多种语言/语法（比如在HTML中嵌入JavaScript和CSS）。
- en: Keeping those in mind, let's go over the approaches I've tried.
  id: totrans-split-15
  prefs: []
  type: TYPE_NORMAL
  zh: 记住这些，让我们来看看我尝试过的方法。
- en: A Brief History of CodeMirror Parsing
  id: totrans-split-16
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: CodeMirror解析简史
- en: The system in as it exists in CodeMirror 5 now (which is pretty much what we've
    been using from the very beginning) is a [simple one](http://marijnhaverbeke.nl/blog/codemirror-mode-system.html).
    For each language, you write a tokenizer which splits the input into pieces, and
    labels each piece with some syntactic category (such as `variable`, `keyword`,
    or `number`). The tokenizers can be stateful, which allows them to secretly be
    full parsers if they want to.
  id: totrans-split-17
  prefs: []
  type: TYPE_NORMAL
  zh: 现在的CodeMirror 5系统（基本上从一开始就在使用的系统）是一个[简单的系统](http://marijnhaverbeke.nl/blog/codemirror-mode-system.html)。对于每种语言，你编写一个分词器，将输入分成片段，并为每个片段打上一些语法类别的标签（如`variable`、`keyword`或`number`）。分词器可以是有状态的，这使它们可以偷偷地成为完整的解析器，如果它们愿意的话。
- en: This state must by copyable, so that the editor can strategically store tokenizer
    states from a previous run, and after a change, resume one close to that change
    to avoid re-tokenizing the entire document. Because we are usually only interested
    in the code in the visible viewport, this means the complexity of re-tokenizing
    is bounded by the distance between the change and the end of the viewport. Since
    most changes happen inside of that viewport, this works well in practice.
  id: totrans-split-18
  prefs: []
  type: TYPE_NORMAL
  zh: 这种状态必须是可复制的，以便编辑器可以策略性地存储前一次运行的标记器状态，并在更改后，恢复到接近该更改的状态，以避免重新对整个文档进行标记化。因为我们通常只对可见视口中的代码感兴趣，所以这意味着重新标记的复杂性受限于更改与视口末端之间的距离。由于大多数更改发生在该视口内部，这在实践中效果很好。
- en: '* * *'
  id: totrans-split-19
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Such tokenizers are awkward to write directly, so over the years several attempts
    have been made to build abstractions over them. The first was the [Common JavaScript
    Syntax Highlighting Specification](https://github.com/mozilla/skywriter/wiki/Common-JavaScript-Syntax-Highlighting-Specification),
    an attempt by the authors of Mozilla Skywriter (formerly Bespin, later merged
    into [ACE](https://ace.c9.io/)) to define a declarative format for describing
    tokenizers as state machines with regular expressions (describing the tokens)
    as edges. The ACE project ended up with an incompatible but similar format (too
    entangled with their internals to use in CodeMirror, unfortunately). I did an
    implementation of the original spec for CodeMirror, and then another incompatible
    [extension](http://cm/demo/simplemode.html) because the base spec was too limiting.
    There are a few CodeMirror modes still based on that code, but it was no real
    success.
  id: totrans-split-20
  prefs: []
  type: TYPE_NORMAL
  zh: 这样的标记器直接编写起来很麻烦，因此多年来已经有几次尝试建立对它们的抽象。第一次尝试是由Mozilla Skywriter的作者（之前称为Bespin，后来合并为[ACE](https://ace.c9.io/)）提出的[Common
    JavaScript Syntax Highlighting Specification](https://github.com/mozilla/skywriter/wiki/Common-JavaScript-Syntax-Highlighting-Specification)，旨在定义一个声明性格式，用于描述将标记器作为带有正则表达式的状态机（描述标记）的边。ACE项目最终采用了一种不兼容但相似的格式（与其内部过于交织，不幸地无法在CodeMirror中使用）。我为CodeMirror实现了原始规范，并实现了另一个不兼容的[扩展](http://cm/demo/simplemode.html)，因为基础规范过于局限。仍有一些基于该代码的CodeMirror模式，但并没有真正取得成功。
- en: I think the reason such state machines (and the somewhat related [TextMate grammars](https://macromates.com/manual/en/language_grammars)
    which are in wide use in desktop editors) never felt like a great solution is
    that, once you get past trivial grammars (where their declarative simplicity does
    look really nice), they don't really help that much with abstraction. Manually
    designing complicated state machines is a chore. Regular expressions, which are
    bad enough on their own, become downright [terrifying](https://github.com/jeff-hykin/cpp-textmate-grammar/blob/e7b680238e59a87231322159749d74351c9d774a/syntaxes/cpp.tmLanguage.yaml#L264)
    when you have to construct all your edges out of them, often stuffing multiple
    tokens into a single expression to avoid creating intermediate states. This “abstraction”
    has a tendency to produce uglier, less maintainable code than what you'd get when
    writing the tokenizer as plain code.
  id: totrans-split-21
  prefs: []
  type: TYPE_NORMAL
  zh: 我认为这样的状态机（以及在桌面编辑器中广泛使用的略有关联的[TextMate grammars](https://macromates.com/manual/en/language_grammars)），从来没有感觉像是一个很好的解决方案，原因是一旦超越了简单的语法（在那里它们的声明简单确实看起来很好），它们在抽象方面并没有真正帮助。手动设计复杂的状态机是一项苦差事。正则表达式本身已经很糟糕，当你不得不将所有边都构建成它们时，它们变得十分可怕，经常将多个标记塞入单个表达式中，以避免创建中间状态。这种“抽象”倾向于产生比将标记器编写为纯代码时更丑陋、难以维护的代码。
- en: '* * *'
  id: totrans-split-22
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: So in 2017, I started an ambitious project to create a better way to abstractly
    define incremental tokenizers. I had concluded that classical parser generators
    based on context-free grammars were never going to work in this context (for reasons
    that I'll come back to later on). But I kept coming across [parsing expression
    grammars](https://en.wikipedia.org/wiki/Parsing_expression_grammar), which weren't
    based on context-free grammars and had some interesting properties, such as being
    able to combine multiple grammars to create a new grammar (which is great for
    mixed-language documents).
  id: totrans-split-23
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，2017年，我开始了一个雄心勃勃的项目，创建一种更好的抽象定义增量标记器的方法。我得出结论，基于上下文无关文法的经典解析器生成器永远不会在这种情况下起作用（我稍后会回到原因）。但我一直在接触到[parsing
    expression grammars](https://en.wikipedia.org/wiki/Parsing_expression_grammar)，它们不基于上下文无关文法，并且具有一些有趣的属性，例如能够组合多个语法以创建新的语法（对于混合语言文档非常有用）。
- en: So I spent several months building a parsing system that took a PEG-like grammar,
    compiled it down to a state machine, and made it possible to run that state machine
    as a CodeMirror language mode.
  id: totrans-split-24
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我花了几个月的时间构建一个解析系统，它采用类似 PEG 的语法，将其编译为状态机，并使其能够作为 CodeMirror 语言模式运行。
- en: This [system](https://github.com/codemirror/grammar-mode) is a marvel. It uses
    a moderately sophisticated [optimizing compiler](https://www.youtube.com/watch?v=1qIee0aHOhY)
    to generate the state machines. The result works quite well, and is used in several
    real-world systems today. But unfortunately, if I'm honest, it is a tragically
    bad idea taken way too far.
  id: totrans-split-25
  prefs: []
  type: TYPE_NORMAL
  zh: 这个 [系统](https://github.com/codemirror/grammar-mode) 是一种奇迹。它使用了一种相当复杂的 [优化编译器](https://www.youtube.com/watch?v=1qIee0aHOhY)
    来生成状态机。结果运行相当顺利，并且今天在几个实际系统中使用。但不幸的是，如果我真诚地说，这是一个可悲地走得太远的坏主意。
- en: Parsing expression grammars are parsed by backtracking. And as such, they are
    very poorly suited for implementing a stateful tokenizer. In a backtracking system,
    you never know when you've *definitely* parsed a piece of content—later input
    might require you to backtrack again. So what I ended up with was actually not
    PEG at all, but a system where you had to explicitly annotate where the parser
    should look ahead. Though grammars written this way were relatively readable,
    they involved a lot of finicky, error-prone kludges to resolve local ambiguity.
  id: totrans-split-26
  prefs: []
  type: TYPE_NORMAL
  zh: 解析表达式语法通过回溯进行解析。因此，它们非常不适合实现具有状态的标记器。在回溯系统中，您永远不知道何时*绝对*解析了一段内容——后续的输入可能要求您再次回溯。因此，我最终得到的实际上根本不是
    PEG，而是一种需要显式注释解析器应该向前查看的系统。尽管以这种方式编写的语法相对可读，但它们涉及大量琐碎、容易出错的修补程序以解决局部歧义。
- en: Also, parsing PEG is just really inefficient. Such grammars are “scannerless”
    meaning they don't make a distinction between tokenizing and parsing. When parsing
    in that way naively, you basically have to run your whole parsing logic for every
    input character. Probably multiple times, due to backtracking. A lot of the magic
    in the compiler was intended to recover the tokens that were implicit in the grammar,
    in order to recover some efficiency. But the system never came close to hand-written
    language modes in terms of speed.
  id: totrans-split-27
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，解析 PEG 实际上非常低效。这样的语法是“无扫描器”的，意味着它们不区分标记和解析。在这种天真的解析方式中，您基本上必须为每个输入字符运行整个解析逻辑。由于回溯，可能要多次运行。编译器中的许多魔法旨在恢复语法中隐含的标记，以提高一些效率。但从速度上讲，该系统从未接近手写语言模式。
- en: Tree-sitter
  id: totrans-split-28
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Tree-sitter
- en: So, though I knew I needed a new approach, I went into the CodeMirror 6 rewrite
    without any specific idea on what that approach would look like.
  id: totrans-split-29
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，尽管我知道我需要一个新的方法，但在 CodeMirror 6 重写中，我没有任何具体的想法会是什么样子。
- en: And then I saw [tree-sitter](http://tree-sitter.github.io/tree-sitter/), and
    was enlightened.
  id: totrans-split-30
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我发现了 [tree-sitter](http://tree-sitter.github.io/tree-sitter/)，并得到了启发。
- en: 'Tree-sitter is a parser system written with the code editor use case in mind,
    and is in the process of being integrated into the [Atom editor](https://atom.io/).
    It takes a much more ambitious approach to what a parser inside an editor should
    do: It builds up a full, accurate syntax tree for the content.'
  id: totrans-split-31
  prefs: []
  type: TYPE_NORMAL
  zh: Tree-sitter 是一种专为代码编辑器设计的解析器系统，正在被集成到 [Atom 编辑器](https://atom.io/) 中。它对编辑器内的解析器应该做的事情采取了更加雄心勃勃的方法：为内容构建完整准确的语法树。
- en: You can do so much more with an actual syntax tree than with a sequence of tokens.
    Whereas tokens, possibly augmented with some information stored in the tokenizer
    state, allow you to sort of approximate understanding some aspects of the code's
    structure, a tree usually gives you precisely the information you need.
  id: totrans-split-32
  prefs: []
  type: TYPE_NORMAL
  zh: 用实际的语法树比用一系列标记能做的事情多得多。而标记，可能还加上一些存储在标记器状态中的信息，允许您大致理解代码结构的某些方面，而树通常会精确地给出您需要的信息。
- en: Most of the ideas that tree-sitter uses aren't new, in fact a [paper](https://www.researchgate.net/profile/SL_Graham/publication/2377179_Efficient_and_Flexible_Incremental_Parsing/links/004635294e13f23ef1000000/Efficient-and-Flexible-Incremental-Parsing.pdf)
    from 2000 describes a somewhat similar system. But as far as I know, tree-sitter
    is the first system that puts them all together into a practical piece of software.
  id: totrans-split-33
  prefs: []
  type: TYPE_NORMAL
  zh: Tree-sitter 使用的大多数想法并不新鲜，实际上，[2000 年的一篇论文](https://www.researchgate.net/profile/SL_Graham/publication/2377179_Efficient_and_Flexible_Incremental_Parsing/links/004635294e13f23ef1000000/Efficient-and-Flexible-Incremental-Parsing.pdf)
    描述了一个类似的系统。但据我所知，tree-sitter 是第一个将它们集成到一个实用软件中的系统。
- en: Unfortunately, tree-sitter is written in C, which is still awkward to run in
    the browser (and CodeMirrror targets non-WASM browsers). It also generates very
    hefty grammar files because it makes the size/speed trade-off in a different way
    than a web system would.
  id: totrans-split-34
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，tree-sitter是用C编写的，在浏览器中运行起来仍然很笨拙（而CodeMirror针对非WASM浏览器）。它还生成非常庞大的语法文件，因为它以一种不同于Web系统的方式进行大小/速度权衡。
- en: But good ideas can be ported. [Lezer](https://lezer.codemirror.net) is a JavaScript-based
    system heavily inspired by tree-sitter.
  id: totrans-split-35
  prefs: []
  type: TYPE_NORMAL
  zh: 但好的想法可以移植。[Lezer](https://lezer.codemirror.net)是一个受树-sitter启发的基于JavaScript的系统。
- en: LR Parsing and Context-Free Grammars
  id: totrans-split-36
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: LR解析与无上下文语法
- en: For a long time, I was firmly convinced that classical parser system based on
    context-free grammars and [LL](https://en.wikipedia.org/wiki/LL_parser) or [LR](https://en.wikipedia.org/wiki/LR_parser)
    parsing algorithms were just not suitable for the editor use case. My arguments
    for this were...
  id: totrans-split-37
  prefs: []
  type: TYPE_NORMAL
  zh: 长时间以来，我坚信基于无上下文语法和[LL](https://en.wikipedia.org/wiki/LL_parser)或[LR](https://en.wikipedia.org/wiki/LR_parser)解析算法的经典解析器系统对编辑器用例并不适用。我之前的论点是...
- en: '*Context-free grammars are a limiting abstraction that breaks down as soon
    as the language does anything funky. Needing the grammar to be LR or LL to please
    the parser generator further pins you into a corner.*'
  id: totrans-split-38
  prefs: []
  type: TYPE_NORMAL
  zh: '*无上下文语法是一种有限制的抽象，只要语言做一些奇怪的事情，它就会崩溃。需要语法是LR或LL以满足解析器生成器的需求，会进一步将你钉在角落里。*'
- en: This is not wrong. Expressing operator precedence in a pure context-free grammar
    requires writing a silly formulaic rule for each level of precedence. And when
    you need to implement something like automatic semicolon insertion or whitespace-sensitivity,
    which would be a couple of lines of code in a hand-written grammar, you can't
    express that directly, and have to somehow escape the context-free abstraction.
  id: totrans-split-39
  prefs: []
  type: TYPE_NORMAL
  zh: 这并不是错的。在纯无上下文语法中表达操作符优先级需要为每个优先级级别编写一个愚蠢的公式规则。当你需要实现类似于自动分号插入或对空格敏感的功能时，在手写语法中可能只需要几行代码，但在这里无法直接表达，必须以某种方式逃脱无上下文抽象。
- en: Making such a grammar suitable for an LR parser generator can be even more tricky,
    and often requires you to have a rather deep understanding of how the parser generator
    works.
  id: totrans-split-40
  prefs: []
  type: TYPE_NORMAL
  zh: 使这样的语法适合LR解析器生成器甚至更加棘手，通常需要你对解析器生成器的工作原理有相当深入的理解。
- en: But like many things, once you get to know them, they aren't that bad. Parser
    generators can support precedence declarations, which make operator parsing a
    lot less terrible. They can even output decent error messages.
  id: totrans-split-41
  prefs: []
  type: TYPE_NORMAL
  zh: 但就像很多事情一样，一旦你了解它们，它们并不那么糟糕。解析器生成器可以支持优先级声明，这使得操作符解析不再那么可怕。它们甚至可以输出不错的错误信息。
- en: Supporting dynamic resolution of ambiguities through something like [GLR parsing](https://en.wikipedia.org/wiki/GLR_parser)
    can provide a practical way out of situations that parser generators are traditionally
    bad at.
  id: totrans-split-42
  prefs: []
  type: TYPE_NORMAL
  zh: 通过像[GLR解析](https://en.wikipedia.org/wiki/GLR_parser)这样的动态解决歧义的支持，可以提供传统上解析器生成器处理不好的情况的实际解决方案。
- en: And contrary to some of the abstractions I mentioned before, this one actually
    gets us something. Context-free grammars, when combined with a proper parser generator,
    really do give us fast parsers from readable, compact grammar declarations.
  id: totrans-split-43
  prefs: []
  type: TYPE_NORMAL
  zh: 与我之前提到的一些抽象相反，这个实际上确实给了我们一些东西。与适当的解析器生成器结合使用时，无上下文语法确实可以从可读性强、紧凑的语法声明中快速生成解析器。
- en: '*A strict separation between the tokenizer and parser is problematic.*'
  id: totrans-split-44
  prefs: []
  type: TYPE_NORMAL
  zh: '*令人困扰的是，分词器和解析器之间的严格分离。*'
- en: It is, in many languages (think of JavaScript's ambiguity between regular expressions
    and the division operator). It also tends to make mixed-language parsing harder.
  id: totrans-split-45
  prefs: []
  type: TYPE_NORMAL
  zh: 在许多语言中（比如JavaScript在正则表达式和除号运算符之间的模糊性）。它还倾向于使混合语言解析更加困难。
- en: But just because this type of parser is traditionally ran with a completely
    separate tokenizer doesn't mean it has to be. Having the parse state drive the
    tokenizer is largely unproblematic. You can even have the parser generator set
    this up [automatically](#contextual-tokens), without user involvement.
  id: totrans-split-46
  prefs: []
  type: TYPE_NORMAL
  zh: 但仅仅因为这种类型的解析器传统上与完全独立的分词器一起运行并不意味着它必须这样。让解析状态驱动分词器在很大程度上并不成问题。你甚至可以让解析器生成器[自动设置](#contextual-tokens)，无需用户参与。
- en: '*Generated parsers are way too big.*'
  id: totrans-split-47
  prefs: []
  type: TYPE_NORMAL
  zh: '*生成的解析器太大了。*'
- en: A naively generated LR parser is *huge*, and many tools spit out embarrassingly
    big files. But with careful parser state deduplication and table compression such
    a parser can be made about as compact as a hand-written one.
  id: totrans-split-48
  prefs: []
  type: TYPE_NORMAL
  zh: 一个天真生成的LR解析器是*庞大*的，许多工具生成的文件令人尴尬地大。但通过精心的解析器状态去重和表压缩，可以使这样的解析器几乎与手写的解析器一样紧凑。
- en: '*Making such a parser error-tolerant is extremely cumbersome.*'
  id: totrans-split-49
  prefs: []
  type: TYPE_NORMAL
  zh: '*使这样的解析器具有误差容忍性非常麻烦*。'
- en: If you search the scholarly literature for approaches to error-tolerance in
    LR parser systems, you get a lot of results, with a lot of different approaches,
    but none of them are very practical. Most require the grammar writer to explicitly
    annotate the grammar with error-recovery strategies, bloating the grammar and
    putting the responsibility for getting it right on every grammar author.
  id: totrans-split-50
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你在学术文献中搜索LR解析器系统的错误容忍方法，你会得到大量结果，有许多不同的方法，但没有一个是非常实用的。大多数方法要求语法编写者显式地使用错误恢复策略注释语法，这使得语法变得臃肿，并将正确性的责任放在每个语法作者身上。
- en: Tree-sitter ingeniously abuses [GLR parsing](https://en.wikipedia.org/wiki/GLR_parser),
    where the parser can try multiple interpretations simultaneously, to integrate
    automatic error-correction without a lot of extra complexity. Lezer copies [this
    approach](#error-recovery).
  id: totrans-split-51
  prefs: []
  type: TYPE_NORMAL
  zh: Tree-sitter巧妙地滥用[GLR解析](https://en.wikipedia.org/wiki/GLR_parser)，其中解析器可以同时尝试多个解释，以集成自动错误修正而不需要额外的复杂性。Lezer复制了[这种方法](#error-recovery)。
- en: Lezer
  id: totrans-split-52
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Lezer
- en: I called my tree-sitter copycat project [Lezer](https://lezer.codemirror.net),
    which is the Dutch word for *reader* (and pronounced a lot like *laser*). It is
    a bit less advanced than tree-sitter in some areas, a bit more advanced in others,
    and simply different on quite a lot of points, as determined by a different set
    of priorities and tastes.
  id: totrans-split-53
  prefs: []
  type: TYPE_NORMAL
  zh: 我将我的树坐者克隆项目命名为[Lezer](https://lezer.codemirror.net)，这是荷兰语中的*阅读者*（而且发音很像*laser*）。在某些方面，它比tree-sitter不那么先进，在其他方面则更为先进，并且在很多点上完全不同，这些差异源于不同的优先级和口味设定。
- en: CodeMirror 6 will retain the ability to run a classical stateful tokenizer,
    but its recommended way to define a language mode is to write a Lezer grammar
    and wrap it in a CodeMirror-specific packages that adds some editor-related metadata.
  id: totrans-split-54
  prefs: []
  type: TYPE_NORMAL
  zh: CodeMirror 6将保留运行经典的有状态分词器的能力，但定义语言模式的推荐方式是编写一个Lezer语法，并将其包装在添加了一些编辑器相关元数据的CodeMirror特定包中。
- en: Lezer is an [LR](https://en.wikipedia.org/wiki/LR_parser) (with opt-in [GLR](https://en.wikipedia.org/wiki/GLR_parser))
    parser generator. It has support for incremental parsing, where you can cheaply
    re-parse a document after local changes have been made to it by reusing pieces
    of the old parse tree. It automatically tries to recover and continue parsing
    when it runs into a syntax error, leaving markers in the output tree that indicate
    where the recovery happened.
  id: totrans-split-55
  prefs: []
  type: TYPE_NORMAL
  zh: Lezer是一个[LR](https://en.wikipedia.org/wiki/LR_parser)（具有选择性使用[GLR](https://en.wikipedia.org/wiki/GLR_parser)）解析器生成器。它支持增量解析，您可以便宜地在局部更改后重新解析文档，方法是重复使用旧解析树的部分。当解析器遇到语法错误时，它会自动尝试恢复并继续解析，并在输出树中留下标记，指示恢复发生的位置。
- en: Lezer consists of an off-line parser generator tool, which takes a grammar description
    and outputs a JavaScript module containing a parser for that grammar, and a parser
    run-time system (which such output files depend on) to do the actual parsing.
    Only the run-time system and the generated parser need to be loaded by the editor.
  id: totrans-split-56
  prefs: []
  type: TYPE_NORMAL
  zh: Lezer包括一个离线解析器生成工具，它接受语法描述并输出包含该语法解析器的JavaScript模块，以及一个解析器运行时系统（依赖于这些输出文件），用于实际解析。编辑器只需加载运行时系统和生成的解析器。
- en: The parser outputs non-abstract syntax trees, meaning that it just creates a
    raw tree structure containing the constructs it parsed (with information on where
    it found them), without organizing them into a clean, easy-to-use data structure.
  id: totrans-split-57
  prefs: []
  type: TYPE_NORMAL
  zh: 解析器输出非抽象语法树，意味着它只创建一个原始树结构，其中包含它解析的构造（以及它们的位置信息），而不是将它们组织成一个干净、易于使用的数据结构。
- en: The system is optimized for compactness, both in parser table size and syntax
    tree size. It needs to be practical to ship a bunch of parsers to a user on the
    web without producing megabytes of network traffic, and it needs to be realistic
    to keep syntax trees for large documents around without running out of memory.
  id: totrans-split-58
  prefs: []
  type: TYPE_NORMAL
  zh: 该系统针对紧凑性进行了优化，无论是解析表大小还是语法树大小。必须可以将一堆解析器实际传送到Web用户，而不会产生大量网络流量，并且可以保持大型文档的语法树在内存中而不会耗尽内存。
- en: The [Lezer guide](https://lezer.codemirror.net/docs/guide/) provides a more
    thorough introduction, as well as a description of its grammar notation. In this
    blog post, I want to go into the neat implementation details that aren't relevant
    in user documentation.
  id: totrans-split-59
  prefs: []
  type: TYPE_NORMAL
  zh: '[Lezer指南](https://lezer.codemirror.net/docs/guide/)提供了更全面的介绍，以及其语法符号的描述。在本博客文章中，我想深入介绍一些在用户文档中不相关的精彩实现细节。'
- en: Error Recovery
  id: totrans-split-60
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 错误恢复
- en: The point where I became convinced that I definitely needed to use or copy tree-sitter
    was when I understood its error recovery strategy.
  id: totrans-split-61
  prefs: []
  type: TYPE_NORMAL
  zh: 当我理解到它的错误恢复策略时，我确信我绝对需要使用或复制tree-sitter。
- en: Say you reach a point where you can no longer proceed normally because there
    is a syntax error. The rest of the input, after the error, is probably full of
    meaningful constructs that could still be parsed. We want those constructs in
    our syntax tree. But our regular parsing process is stuck—it doesn't know how
    to get from the error to a state where the parse can continue.
  id: totrans-split-62
  prefs: []
  type: TYPE_NORMAL
  zh: 假设你达到了一个无法正常进行的点，因为有语法错误。错误之后的输入可能充满了仍然可以解析的有意义结构。我们希望将这些结构放入我们的语法树中。但是我们的常规解析过程卡住了——它不知道如何从错误中恢复到可以继续解析的状态。
- en: I definitely did not want to require the grammar author to add error recovery
    hints to their grammar. These tend to clutter up the grammar and are error-prone
    to write. Writing a grammar is hard enough without that distraction.
  id: totrans-split-63
  prefs: []
  type: TYPE_NORMAL
  zh: 我绝对不想要求语法作者在他们的语法中添加错误恢复提示。这些倾向于混乱语法并且很容易写错。编写语法已经很难了，不需要这样的干扰。
- en: You can see error recovery as a search problem. There might be a parse state
    and input position (past the error) where the parse can meaningfully continue.
    We just have to find it.
  id: totrans-split-64
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以将错误恢复视为一个搜索问题。可能存在一个解析状态和输入位置（超出错误）可以有意义地继续解析。我们只需找到它。
- en: The actions encoded in the parse tables, along with some recovery-specific actions
    that the parser wouldn't normally take, provide a kind of search tree. You start
    at the state(s) where the error occurred, and keep exploring new states from there.
  id: totrans-split-65
  prefs: []
  type: TYPE_NORMAL
  zh: 解析表中编码的动作，以及解析器通常不会采取的一些特定恢复动作，提供了一种搜索树。您从错误发生的状态开始，并从那里继续探索新的状态。
- en: But what does the accept condition look like? When do you know that you've found
    an acceptable solution? You could define that precisely, for example as the state
    that can handle the next N tokens without further errors. But we can also be vague.
  id: totrans-split-66
  prefs: []
  type: TYPE_NORMAL
  zh: 但是接受条件是什么样子呢？您如何知道已经找到了可接受的解决方案？您可以精确定义它，例如作为可以处理下N个标记而不再出现错误的状态。但我们也可以模棱两可地定义。
- en: The solution found by [Max Brunsfeld](https://github.com/maxbrunsfeld) in tree-sitter
    is to use the same mechanism that's used to parse ambiguous grammars. A GLR parser
    can split its parse stack and run both sides alongside each other for a while
    until it becomes clear which one works out.
  id: totrans-split-67
  prefs: []
  type: TYPE_NORMAL
  zh: 在tree-sitter中，[Max Brunsfeld](https://github.com/maxbrunsfeld)发现的解决方案是使用解析模棱两可语法所使用的相同机制。GLR解析器可以将其解析栈分割并在一段时间内并行运行两侧，直到清楚哪一侧有效。
- en: That's pretty much exactly what a search algorithm does—it tracks a number of
    branches that it still has to explore, and continues to explore them, possibly
    pruning unpromising branches with some heuristic, until it finds a solution.
  id: totrans-split-68
  prefs: []
  type: TYPE_NORMAL
  zh: 这几乎正是搜索算法所做的——它跟踪它仍然必须探索的一些分支，并继续探索它们，可能使用一些启发式方法修剪不太有希望的分支，直到找到解决方案。
- en: To be able to get good results, or at least *some* result, in messy situations
    like longer stretches of invalid input, each branch has a badness score associated
    with it, which is increased (linearly) each time a recovery action is taken, and
    decreased (asymptotically) every time it can consume a token normally.
  id: totrans-split-69
  prefs: []
  type: TYPE_NORMAL
  zh: 为了能够在混乱的情况下，如较长的无效输入串中获得良好或至少*一些*结果，每个分支都与一个坏度分数相关联，每次采取恢复操作时线性增加，并在能够正常消耗一个标记时渐近减少。
- en: What we want to do is, after an error, try all kinds of possible recovery tricks,
    which recursively branch off a large amount of states. But then, after a bit of
    that, we should consolidate to one or, at most, a few parse states again, because
    parsing input in a whole bunch of different ways is expensive.
  id: totrans-split-70
  prefs: []
  type: TYPE_NORMAL
  zh: 我们想要做的是，在出现错误后，尝试所有可能的恢复技巧，这些技巧会递归地分支出大量的状态。但是，在进行了一段时间之后，我们应该再次合并到一个或者最多几个解析状态，因为以多种方式解析输入是昂贵的。
- en: To get this effect, Lezer forbids states with a badness higher than a given
    multiple of the best state's badness (or some maximum threshold) from applying
    further recovery actions, effectively dropping those branches when they can't
    proceed normally. In the case where one branch finds a good way to continue, that
    branch's badness will converge to zero and eventually stop all worse branches.
    In cases where the input continues to make no sense, all branches will eventually
    get a badness score exceeding the maximum, and the parser will only continue one
    of them.
  id: totrans-split-71
  prefs: []
  type: TYPE_NORMAL
  zh: 为了达到这种效果，Lezer禁止具有比最佳状态的坏度的给定倍数更高（或某个最大阈值）的状态继续应用恢复动作，从而有效地丢弃那些无法正常进行的分支。在一个分支找到继续的良好方法的情况下，该分支的坏度将会收敛到零，并最终停止所有更糟糕的分支。在输入继续毫无意义的情况下，所有分支最终将获得超过最大值的坏度分数，解析器将仅继续其中一个。
- en: 'The recovery strategies used are:'
  id: totrans-split-72
  prefs: []
  type: TYPE_NORMAL
  zh: 使用的恢复策略包括：
- en: Skip the next token, and try again with the same state after that.
  id: totrans-split-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 跳过下一个令牌，然后在此之后再试一次。
- en: Invent a token—take any of the tokens that are valid in this state, and continue
    to the state that consuming them would produce. This is the main source of branching,
    since many states allow a lot of tokens.
  id: totrans-split-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 发明一个令牌——使用此状态中有效的任何令牌，并继续到将消耗它们的状态。这是分支的主要来源，因为许多状态允许许多令牌。
- en: Force the end of the innermost production that's currently being parsed.
  id: totrans-split-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 强制结束当前正在解析的最内部产生式。
- en: There are situations where the result of this approach isn't entirely optimal,
    but it usually does well. The important thing is that it always keeps parsing,
    and does so in a way that remains tractable (exponential searches are quickly
    dampened). The system is biased a bit towards the token-skipping rule, so that
    if all else fails it'll, in effect, just continue skipping tokens until it stumbles
    into a situation where it can continue parsing.
  id: totrans-split-76
  prefs: []
  type: TYPE_NORMAL
  zh: 有些情况下，这种方法的结果并非完全理想，但通常表现良好。重要的是它始终保持解析，并以一种保持可控的方式进行（指数搜索迅速减弱）。系统有点倾向于跳过令牌规则，因此如果一切都失败了，它将继续跳过令牌，直到碰巧遇到可以继续解析的情况。
- en: Post-Order Parser Output
  id: totrans-split-77
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 后序解析器输出
- en: When you have a parser that may be splitting its state—a lot—and build up parts
    of the tree multiple times, that duplicate tree building and the bookkeeping involved
    in it can cause a lot of unnecessary work.
  id: totrans-split-78
  prefs: []
  type: TYPE_NORMAL
  zh: 当您有一个可能多次分割其状态并多次构建树的解析器时，这种重复的树构建及其所涉及的簿记工作可能会导致大量不必要的工作。
- en: 'The order in which an LR parser creates nodes is inner-to-outer. It will, for
    example, first create the node for the operands, and then later the node for the
    operator expression. This suggests an approach: What if, instead of building a
    tree structure right away, the parser just keeps a flat log of the nodes it created.
    This can be an array in which the nodes are arranged in [post-order](https://en.wikipedia.org/wiki/Tree_traversal#Post-order_(LRN)),
    with children coming before parents.'
  id: totrans-split-79
  prefs: []
  type: TYPE_NORMAL
  zh: LR解析器创建节点的顺序是从内向外。例如，它首先创建操作数节点，然后稍后创建操作符表达式节点。这提出了一种方法：如果不立即构建树结构，而是让解析器仅保持所创建的节点的平面日志，那么这可以是一个数组，在其中节点按[后序顺序](https://en.wikipedia.org/wiki/Tree_traversal#Post-order_(LRN))排列，子节点在父节点之前。
- en: The parser just appends to this array. When splitting the state, one state keeps
    the existing array, and the other gets a new empty array along with a pointer
    to the state that has the rest of the array, and the length of that array at the
    time of the split.
  id: totrans-split-80
  prefs: []
  type: TYPE_NORMAL
  zh: 解析器只是将内容附加到此数组中。在分割状态时，一个状态保留现有的数组，另一个状态获得一个新的空数组，以及指向具有其余数组的状态的指针，以及分割时数组的长度。
- en: Now splitting involves no node copying at all. You do need to copy the state
    stack, which LR parser use to track context, but that is generally shallow.
  id: totrans-split-81
  prefs: []
  type: TYPE_NORMAL
  zh: 现在分割根本不涉及任何节点复制。您确实需要复制状态栈，LR解析器用于跟踪上下文，但通常这是浅层的。
- en: In addition, node allocation becomes as cheap as appending a few numbers to
    an array. For actions that don't result in tree nodes (Lezer allows you to mark
    rules as uninteresting, to keep the tree small), you don't have to do anything
    at all. The control stacks stores the output array position at the start of each
    rule, and can use that to emit enough data to later reconstruct parent-child relationships.
  id: totrans-split-82
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，节点分配变得像将几个数字附加到数组一样便宜。对于不产生树节点的操作（Lezer允许您将规则标记为不感兴趣，以保持树的小型化），您根本不必执行任何操作。控制栈在每个规则开始时存储输出数组位置，并可以使用它来发出足够的数据以后重建父子关系。
- en: After a parse finishes successfully, the final state's parent-array pointers
    can be used to find all the nodes that make up the tree, and construct an actual
    tree structure out of them.
  id: totrans-split-83
  prefs: []
  type: TYPE_NORMAL
  zh: 在成功完成解析之后，最终状态的父数组指针可以用来找到组成树的所有节点，并根据它们构造出实际的树结构。
- en: One tricky issue occurs when skipped content (whitespace and comments) produces
    nodes. If you have code like this...
  id: totrans-split-84
  prefs: []
  type: TYPE_NORMAL
  zh: 当跳过的内容（空格和注释）产生节点时，会出现一个棘手的问题。如果您的代码像这样...
- en: '[PRE0]'
  id: totrans-split-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '... the comment should *not* be part of the if statement''s node. Yet the parser
    only knows for sure that it can finish that node after seeing the next statement
    (there might be an `else` still coming).'
  id: totrans-split-86
  prefs: []
  type: TYPE_NORMAL
  zh: '...注释*不应*成为if语句节点的一部分。然而，解析器只有在看到下一条语句后才确定可以完成该节点（可能还有一个`else`语句即将到来）。'
- en: In cases like this, where the output array contains skipped nodes immediately
    in front of a reduction, the parser has to move them forward and store the end
    of the node *before* them. Fortunately, this occurs relatively rarely (unless
    you add nodes for whitespace, in which case it'll happen at the end of every rule
    that has a possible continuation).
  id: totrans-split-87
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，输出数组包含在减少节点的前面立即跳过的节点，解析器必须将它们向前移动，并将节点的末尾*存储在*它们之前。幸运的是，这种情况相对较少发生（除非您添加了空格节点，否则每个具有可能继续的规则的末尾都会发生这种情况）。
- en: Buffer Trees
  id: totrans-split-88
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 缓冲树
- en: A nice thing about the flat post-order tree representation is that it is compact.
    Tree structures constructed the usual way, as separately allocated nodes, incur
    a lot of extra overhead for pointers and allocation headers. They can also have
    terrible locality, since who knows how far from each other the memory allocator
    will put the nodes.
  id: totrans-split-89
  prefs: []
  type: TYPE_NORMAL
  zh: 平面后序树表示法的一个好处是它很紧凑。通常构建的树结构，作为单独分配的节点，会因指针和分配头部的额外开销而增加很多开销。它们还可能具有可怕的局部性，因为谁知道内存分配器会将节点放在彼此多远的地方。
- en: Unfortunately, we can't just use a flat representation for our syntax trees.
    The incremental parser has to be able to reuse parts of it without copying those
    parts into a different buffer.
  id: totrans-split-90
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，我们不能仅仅使用平面表示法来表示我们的语法树。增量解析器必须能够重复使用其部分，而不是将这些部分复制到不同的缓冲区中。
- en: But we *can* use it for parts of the tree. Storing the coarse structure as a
    classical tree, but the content of smaller nodes (say less than a few thousand
    characters long) as flat arrays, gives us the best of both worlds. Since most
    nodes, by number, live in the fine structure, this saves a large amount of overhead
    (and helps with locality).
  id: totrans-split-91
  prefs: []
  type: TYPE_NORMAL
  zh: 但是我们*可以*将它用于树的部分。将粗略结构存储为经典树，但较小节点的内容（比如少于几千个字符长）存储为平面数组，可以兼顾两者的优点。由于大多数节点（按数量计算）存在于细微结构中，这样可以节省大量的开销（并有助于提高局部性）。
- en: That does mean that we can't reuse small nodes. But since their size is limited,
    the amount of work that is involved in re-parsing them is also limited. And by
    removing them from consideration, the incremental parser can avoid quite a bit
    of the work involved in preparing and scanning the tree for reuse.
  id: totrans-split-92
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着我们不能重用小节点。但由于它们的大小有限，重新解析它们涉及的工作量也有限。通过将它们排除在外，增量解析器可以避免为了准备和扫描树结构以重用而进行的大量工作。
- en: A small node stores its content in a typed array of 16-bit unsigned integers.
    It uses 4 such numbers (64 bits) per node, storing a type, a start position, an
    end position, and a child count for each node. Contrary to the array created by
    the parser, these arrays are in [pre-order](https://en.wikipedia.org/wiki/Tree_traversal#Pre-order_(NLR)),
    because that makes forward iteration (which tends to be more common than backward
    iteration) cheaper. The child count was almost obsolete (the end position can
    sort of tell you which nodes are children), but Lezer supports zero-length nodes,
    which might land on the end of their parent node and make it ambiguous whether
    they belong to it or not.
  id: totrans-split-93
  prefs: []
  type: TYPE_NORMAL
  zh: 一个小节点将其内容存储在16位无符号整数的类型数组中。每个节点使用4个这样的数字（64位），存储类型、开始位置、结束位置和每个节点的子节点计数。与解析器创建的数组相反，这些数组是按[前序](https://en.wikipedia.org/wiki/Tree_traversal#Pre-order_(NLR))排列的，因为这使得前向迭代（比后向迭代更常见）更便宜。子节点计数几乎已经过时（结束位置可以告诉您哪些节点是子节点），但Lezer支持零长度节点，这可能会落在其父节点的末尾，使得它们是否属于它们变得模棱两可。
- en: Client code, of course, doesn't want to deal with this representation. Lezer
    provides an abstract interface to searching in and walking through trees that
    hides the buffer structure, allowing you to conceptually work with a uniform tree
    of nodes.
  id: totrans-split-94
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，客户端代码不希望处理这种表示。Lezer提供了一个抽象接口，用于搜索和遍历树结构，隐藏了缓冲结构，使您可以概念化地处理节点的统一树。
- en: Lezer, like tree-sitter, stores the result of repetitions in the grammar (produced
    by the `*` and `+` operators) as balanced subtrees. This means that, unless your
    input is pathological (say, a thousand applications of a single binary operator
    in a row), you tend to get shallow, well-balanced syntax trees, which are cheap
    to search and allow effective reuse.
  id: totrans-split-95
  prefs: []
  type: TYPE_NORMAL
  zh: 与 tree-sitter 类似，Lezer 将语法中重复的结果（由 `*` 和 `+` 操作符生成）存储为平衡的子树。这意味着，除非您的输入是病态的（例如连续一千次应用单个二进制运算符），否则您往往会得到浅层、平衡的语法树，这样的树在搜索时成本低廉且允许有效重用。
- en: Contextual Tokens
  id: totrans-split-96
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 上下文 token
- en: Depending on the grammar's complexity, an LR parser generator creates between
    a dozen and a few thousand parse states for your grammar. These represent syntactic
    positions like “after the opening paren of an argument list” or “after an expression,
    possibly expecting some expression suffix”.
  id: totrans-split-97
  prefs: []
  type: TYPE_NORMAL
  zh: 根据语法的复杂性，LR 解析器生成器为您的语法创建了十几到几千个解析状态。这些状态表示语法位置，比如“在参数列表的开括号后”或“在表达式之后，可能期望一些表达式后缀”。
- en: The parser generator can figure out which tokens are valid in a given state.
    It can also, for tokens specified as part of the grammar, automatically determine
    which tokens conflict (match the same input, or some prefix of each other).
  id: totrans-split-98
  prefs: []
  type: TYPE_NORMAL
  zh: 解析生成器可以找出在给定状态下哪些 token 是有效的。它还可以自动确定作为语法一部分指定的 token 中哪些 token 冲突（匹配相同的输入或彼此的某些前缀）。
- en: A well-known example of conflicting tokens is the division operator versus regular
    expression syntax in JavaScript. But others are keywords that can also appear
    as property names, and the bitwise right shift operator (`>>`) versus two closing
    angle brackets in C++.
  id: totrans-split-99
  prefs: []
  type: TYPE_NORMAL
  zh: 冲突 token 的一个著名例子是 JavaScript 中的除法运算符与正则表达式语法。但其他例子还包括关键字也可能出现为属性名称，以及位右移操作符
    (`>>`) 与 C++ 中两个闭角括号的冲突。
- en: Lezer will not complain about overlapping tokens if the tokens do not appear
    in the same parse states. This implicitly resolves the regular expression and
    property name issues, without any user interaction.
  id: totrans-split-100
  prefs: []
  type: TYPE_NORMAL
  zh: Lezer 如果 token 在不同的解析状态下出现，就不会抱怨重叠的 token。这种隐含地解决了正则表达式和属性名称的问题，而不需要用户进行任何交互。
- en: When conflicting tokens do appear in the same place, such as division operators
    and C-style comments, you have to specify an explicit precedence ordering (comments
    take precedence) to tell the tool that you know what you're doing.
  id: totrans-split-101
  prefs: []
  type: TYPE_NORMAL
  zh: 当冲突的 token 出现在同一位置时，比如除法操作符和 C 风格的注释，您必须指定一个显式的优先顺序（注释优先），以告诉工具您知道自己在做什么。
- en: Contextual tokenization is implemented with a concept called token groups. Tokens
    that have unresolved conflicts with other tokens are assigned to one or more groups,
    where each group contains only non-conflicting tokens. Each state is assigned
    a single group (if it expects tokens that conflict with each other that's an error).
    This group is passed to the tokenizer, which then takes care to only return tokens
    that are either in that group, or don't conflict with any other tokens. The check
    is optimized by storing group membership in a bitset, and seeing if the right
    bit is set with binary *and*.
  id: totrans-split-102
  prefs: []
  type: TYPE_NORMAL
  zh: 上下文的标记化通过称为 token 组的概念来实现。具有与其他 token 冲突的未解决冲突的 token 被分配到一个或多个组中，每个组仅包含非冲突的
    token。每个状态被分配一个单一的组（如果它期望冲突的 token 那是一个错误）。分词器传递该组，然后仅返回属于该组或不与任何其他 token 冲突的
    token。通过将组成员资格存储在位集中并查看是否使用二进制 *和* 设置了正确的位，这种检查得到了优化。
- en: Tokens are compiled down to a single deterministic state machine, which is ran
    on the input character stream. In cases like the regexp-versus-division issue,
    you don't want the machine to go running through regexp-specific states in a situation
    where you only allow division, since that would be wasteful. Therefore, each tokenizer
    state is also tagged with a bitset that tells you which groups the tokens reachable
    from that state belong to, and the tokenizer stops running when it hits a state
    that has no overlap with the allowed tokens for the parse state.
  id: totrans-split-103
  prefs: []
  type: TYPE_NORMAL
  zh: token 被编译为单个确定性状态机，该状态机在输入字符流上运行。在像正则表达式与除法之间的问题中，您不希望该机器在仅允许除法的情况下通过正则表达式特定的状态，因为那样会很浪费。因此，每个分词器状态还标记有一个位集，告诉您从该状态可达的
    token 属于哪些组，并且当遇到一个与解析状态的允许 token 没有重叠的状态时，分词器会停止运行。
- en: Skip Expressions
  id: totrans-split-104
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 跳过表达式
- en: Almost all programming languages have special syntactic elements like whitespace
    and comments that may occur between any tokens. Encoding these directly in the
    grammar is extremely tedious for most languages.
  id: totrans-split-105
  prefs: []
  type: TYPE_NORMAL
  zh: 几乎所有编程语言都有特殊的语法元素，比如空白和注释，可以出现在任何 token 之间。对大多数语言来说，直接在语法中对其进行编码是极其乏味的。
- en: Traditionally, tokenizer just skip such elements when reading the next token.
    That works well in most contexts, but makes it awkward to include the elements
    in the parse tree.
  id: totrans-split-106
  prefs: []
  type: TYPE_NORMAL
  zh: 传统上，标记器在读取下一个标记时会跳过这样的元素。这在大多数情况下效果很好，但在将这些元素包含在解析树中时会显得有些别扭。
- en: Lezer treats skipped things like they are part of the grammar (though in an
    optimized way to avoid increasing the size of the parse tables). It is possible
    to skip things that aren't single tokens (to implement something like nestable
    comments, for example, or to make sure your block comment nodes consist of smaller
    nodes so that you can incrementally parse giant block comments).
  id: totrans-split-107
  prefs: []
  type: TYPE_NORMAL
  zh: Lezer 将跳过的元素视为语法的一部分（尽管以优化的方式以避免增加解析表的大小）。可以跳过不是单个标记的元素（例如实现类似可嵌套注释的内容，或确保您的块注释节点由较小的节点组成，以便可以逐步解析巨大的块注释）。
- en: Each rule or group of rules may have its own set of skipped expressions, so
    that you can express different sublanguages in a grammar, for example something
    like the content of interpolated strings, without allowing spacing in places where
    the language doesn't allow it.
  id: totrans-split-108
  prefs: []
  type: TYPE_NORMAL
  zh: 每个规则或规则组可能有其自己的跳过表达式集，这样您就可以在语法中表达不同的子语言，例如插值字符串的内容，而不允许语言不允许的地方插入空格。
- en: Each parse state has a pointer to a (shared) set of skip actions, which, for
    the skipped tokens or tokens that start a compound skipped expression, contains
    the actions to take for those tokens. For single-token skipped elements, that
    action just tells the parser to skip the token and stay in the same state. For
    compound elements, it causes the state that handles the rest of the element to
    be pushed onto the control stack.
  id: totrans-split-109
  prefs: []
  type: TYPE_NORMAL
  zh: 每个解析状态都有一个指向（共享的）跳过动作集的指针，用于跳过标记或开始复合跳过表达式的标记的动作。对于单个标记跳过元素，该动作只告诉解析器跳过标记并保持在相同状态。对于复合元素，则会将处理其余元素的状态推送到控制栈上。
- en: Tree Node Tagging
  id: totrans-split-110
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 树节点标记
- en: The languages that a tool like Lezer needs to handle are wildly different, from
    JavaScript to Haskell to CSS to YAML. As such, it is difficult to find a cross-language
    vocabulary to describe their constructs. In fact, it seems like that would be
    a separate multi-year project, and pull in a serious amount of complexity.
  id: totrans-split-111
  prefs: []
  type: TYPE_NORMAL
  zh: Lezer 需要处理的语言类型差异很大，从 JavaScript 到 Haskell 再到 CSS 和 YAML。因此，很难找到一个跨语言的词汇来描述它们的构造。事实上，这似乎需要一个单独的多年项目，并且引入了大量复杂性。
- en: Yet it would be nice if the parser output comes with some information that can
    be interpreted without knowing what language you are working with.
  id: totrans-split-112
  prefs: []
  type: TYPE_NORMAL
  zh: 不过，如果解析器输出带有一些信息，而无需知道您正在使用的语言，那将是很好的。
- en: After several iterations, what I decided on was a system where nodes have *names*,
    which only have a meaning within the language, and *props*, which are values associated
    with tags defined by external code. Integrating a language grammar into CodeMirror
    involves assigning values for some of these props to the node types used by the
    language—things like syntax highlighting style information and how to [indent](indent-from-tree.html)
    such nodes.
  id: totrans-split-113
  prefs: []
  type: TYPE_NORMAL
  zh: 在几次迭代后，我决定采用一种系统，其中节点具有*名称*（仅在语言内有意义），以及*props*，这些是与外部代码定义的标签相关联的值。将语言语法集成到
    CodeMirror 中涉及为语言使用的节点类型分配某些 props 的值，例如语法高亮样式信息以及如何进行 [缩进](indent-from-tree.html)
    这些节点。
- en: Since the number of node types in a language is limited, we can allocate an
    object for each node type to hold this information, and have all nodes of that
    type point to the same object.
  id: totrans-split-114
  prefs: []
  type: TYPE_NORMAL
  zh: 由于语言中节点类型的数量有限，我们可以为每种节点类型分配一个对象以保存这些信息，并且所有该类型的节点都指向同一个对象。
- en: To allow code outside the grammar to add props without mutating global state,
    parser instances can be extended with additional props, creating a copy that will
    output nodes with the props attached. This is especially useful in the context
    of mixed-language trees.
  id: totrans-split-115
  prefs: []
  type: TYPE_NORMAL
  zh: 为了允许语法外的代码在不变更全局状态的情况下添加 props，可以通过扩展解析器实例来创建附加 props 的副本，从而输出带有附加 props 的节点。在混合语言树的上下文中，这是特别有用的。
- en: Lezer has support for a limited form of grammar nesting. If language A can appear
    inside a document in language B, and the end of the region covered by A can be
    unambiguously found by scanning for a specific token, Lezer can temporarily switch
    to another set of parse tables while parsing such a region.
  id: totrans-split-116
  prefs: []
  type: TYPE_NORMAL
  zh: Lezer 支持有限形式的语法嵌套。如果语言 A 可以出现在语言 B 的文档中，并且可以通过扫描特定标记明确找到 A 覆盖的区域的末端，Lezer 可以在解析这样的区域时暂时切换到另一组解析表。
- en: The syntax tree will then contain nodes from both grammars. Having props directly
    attached to the nodes makes it much easier to work with such trees (as opposed
    to using a language-specific table that associates node names with metadata).
  id: totrans-split-117
  prefs: []
  type: TYPE_NORMAL
  zh: 语法树将包含来自两种语法的节点。直接附加到节点的属性使得处理这样的树变得更加容易（而不是使用将节点名称与元数据关联的特定于语言的表格）。
