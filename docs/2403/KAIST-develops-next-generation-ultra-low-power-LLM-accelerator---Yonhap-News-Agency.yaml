- en: <!--yml
  id: totrans-split-0
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  id: totrans-split-1
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-05-27 14:41:40'
  id: totrans-split-2
  prefs: []
  type: TYPE_NORMAL
- en: -->
  id: totrans-split-3
  prefs: []
  type: TYPE_NORMAL
- en: KAIST develops next-generation ultra-low power LLM accelerator | Yonhap News
    Agency
  id: totrans-split-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://en.yna.co.kr/view/AEN20240306003700320](https://en.yna.co.kr/view/AEN20240306003700320)
  id: totrans-split-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: By Kim Na-young
  id: totrans-split-6
  prefs: []
  type: TYPE_NORMAL
- en: SEOUL, March 6 (Yonhap) -- A research team at the Korea Advanced Institute of
    Science and Technology (KAIST) has developed the world's first artificial intelligence
    (AI) semiconductor capable of processing a large language model (LLM) with ultra-low
    power consumption, the science ministry said Wednesday.
  id: totrans-split-7
  prefs: []
  type: TYPE_NORMAL
- en: The team, led by Professor Yoo Hoi-jun at the KAIST PIM Semiconductor Research
    Center, developed a "Complementary-Transformer" AI chip, which processes GPT-2
    with an ultra-low power consumption of 400 milliwatts and a high speed of 0.4
    seconds, according to the Ministry of Science and ICT.
  id: totrans-split-8
  prefs: []
  type: TYPE_NORMAL
- en: The 4.5-mm-square chip, developed using Korean tech giant Samsung Electronics
    Co.'s 28 nanometer process, has 625 times less power consumption compared with
    global AI chip giant Nvidia's A-100 GPU, which requires 250 watts of power to
    process LLMs, the ministry explained.
  id: totrans-split-9
  prefs: []
  type: TYPE_NORMAL
- en: The chip is also 41 times smaller in area than the Nvidia model, enabling it
    to be used on devices like mobile phones.
  id: totrans-split-10
  prefs: []
  type: TYPE_NORMAL
- en: The ministry said the utilization of neuromorphic computing technology, specifically
    spiking neural networks (SNNs), is essential to the achievement.
  id: totrans-split-11
  prefs: []
  type: TYPE_NORMAL
- en: Previously, the technology was less accurate than deep neural networks (DNNs)
    and mainly capable of simple image classifications, but the research team succeeded
    in improving the accuracy of the technology to match that of DNNs to apply it
    to LLMs.
  id: totrans-split-12
  prefs: []
  type: TYPE_NORMAL
- en: The team said its new AI chip optimizes computational energy consumption while
    maintaining accuracy by using unique neural network architecture that fuses DNNs
    and SNNs, and effectively compresses the large parameters of LLMs.
  id: totrans-split-13
  prefs: []
  type: TYPE_NORMAL
- en: '**A photo describing an artificial intelligence chip which processes a large
    language model with neuromorphic computing technology provided by the Ministry
    of Science and ICT on March 6, 2024 (PHOTO NOT FOR SALE) (Yonhap)**'
  id: totrans-split-14
  prefs: []
  type: TYPE_NORMAL
- en: '**nyway@yna.co.kr'
  id: totrans-split-15
  prefs: []
  type: TYPE_NORMAL
- en: (END)**
  id: totrans-split-16
  prefs: []
  type: TYPE_NORMAL
