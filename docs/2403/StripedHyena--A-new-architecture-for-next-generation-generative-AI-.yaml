- en: <!--yml
  id: totrans-split-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-split-1
  prefs: []
  type: TYPE_NORMAL
  zh: 'category: 未分类'
- en: 'date: 2024-05-27 14:33:01'
  id: totrans-split-2
  prefs: []
  type: TYPE_NORMAL
  zh: 'date: 2024-05-27 14:33:01'
- en: -->
  id: totrans-split-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: 'StripedHyena: A new architecture for next-generation generative AI?'
  id: totrans-split-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: StripedHyena：下一代生成AI的新架构？
- en: 来源：[https://the-decoder.com/stripedhyena-a-new-architecture-for-next-generation-generative-ai/](https://the-decoder.com/stripedhyena-a-new-architecture-for-next-generation-generative-ai/)
  id: totrans-split-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://the-decoder.com/stripedhyena-a-new-architecture-for-next-generation-generative-ai/](https://the-decoder.com/stripedhyena-a-new-architecture-for-next-generation-generative-ai/)
- en: '**GPT-4 and other models rely on transformers. With StripedHyena, researchers
    present an alternative to the widely used architecture.**'
  id: totrans-split-6
  prefs: []
  type: TYPE_NORMAL
  zh: '**GPT-4和其他模型依赖transformers。StripedHyena为研究人员提供了一种替代广泛使用的架构。**'
- en: 'With StripedHyena, the Together AI team presents a family of language models
    with 7 billion parameters. What makes it special: StripedHyena uses a new set
    of AI architectures that aim to improve training and inference performance compared
    to the widely used transformer architecture, used for example in [GPT-4](https://the-decoder.com/open-ai-gpt-4-announcement/).'
  id: totrans-split-7
  prefs: []
  type: TYPE_NORMAL
  zh: Together AI团队通过StripedHyena呈现了一系列拥有70亿参数的语言模型。它的特别之处在于：StripedHyena使用了一组新的AI架构，旨在提高训练和推理性能，与广泛使用的transformer架构相比，例如[GPT-4](https://the-decoder.com/open-ai-gpt-4-announcement/)中使用的架构。
- en: The release includes StripedHyena-Hessian-7B (SH 7B), a base model, and StripedHyena-Nous-7B
    (SH-N 7B), a chat model. These models are designed to be faster, more memory efficient,
    and capable of processing very long contexts of up to 128,000 tokens. Researchers
    from HazyResearch, hessian.AI, Nous Research, MILA, HuggingFace, and the German
    Research Centre for Artificial Intelligence (DFKI) were involved.
  id: totrans-split-8
  prefs: []
  type: TYPE_NORMAL
  zh: 发布包括StripedHyena-Hessian-7B（SH 7B），一个基础模型，和StripedHyena-Nous-7B（SH-N 7B），一个聊天模型。这些模型设计得更快、更节省内存，并能处理长达128,000令牌的非常长的上下文。来自HazyResearch、hessian.AI、Nous
    Research、MILA、HuggingFace和德国人工智能研究中心（DFKI）的研究人员参与其中。
- en: 'StripedHyena: an efficient alternative to transformers'
  id: totrans-split-9
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: StripedHyena：transformers的高效替代品
- en: According to Together AI, StripedHyena is the first alternative model that can
    compete with the best open-source transformers. The base model achieves comparable
    performance to [Llama-2](https://the-decoder.com/how-to-get-started-with-metas-llama-2-guide/),
    Yi, and [Mistral 7B](https://the-decoder.com/new-open-source-llm-mistral-7b-outperforms-larger-meta-llama-models/)
    on OpenLLM leaderboard tasks and outperforms them on long context summarization.
  id: totrans-split-10
  prefs: []
  type: TYPE_NORMAL
  zh: 根据Together AI的说法，StripedHyena是第一个可以与最优秀的开源transformers竞争的替代模型。基础模型在OpenLLM排行榜任务上表现出与[Llama-2](https://the-decoder.com/how-to-get-started-with-metas-llama-2-guide/)、Yi和[Mistral
    7B](https://the-decoder.com/new-open-source-llm-mistral-7b-outperforms-larger-meta-llama-models/)可比较的性能，并在长文本摘要任务上表现优于它们。
- en: Ad
  id: totrans-split-11
  prefs: []
  type: TYPE_NORMAL
  zh: 广告
- en: THE DECODER Newsletter
  id: totrans-split-12
  prefs: []
  type: TYPE_NORMAL
  zh: THE DECODER Newsletter
- en: The most important AI news straight to your inbox.
  id: totrans-split-13
  prefs: []
  type: TYPE_NORMAL
  zh: 将最重要的AI新闻直接发送到您的收件箱。
- en: ✓ Weekly
  id: totrans-split-14
  prefs: []
  type: TYPE_NORMAL
  zh: ✓ 每周更新
- en: ✓ Free
  id: totrans-split-15
  prefs: []
  type: TYPE_NORMAL
  zh: ✓ 免费
- en: ✓ Cancel at any time
  id: totrans-split-16
  prefs: []
  type: TYPE_NORMAL
  zh: ✓ 随时取消
- en: Ad
  id: totrans-split-17
  prefs: []
  type: TYPE_NORMAL
  zh: 广告
- en: THE DECODER Newsletter
  id: totrans-split-18
  prefs: []
  type: TYPE_NORMAL
  zh: THE DECODER Newsletter
- en: The most important AI news straight to your inbox.
  id: totrans-split-19
  prefs: []
  type: TYPE_NORMAL
  zh: 将最重要的AI新闻直接发送到您的收件箱。
- en: ✓ Weekly
  id: totrans-split-20
  prefs: []
  type: TYPE_NORMAL
  zh: ✓ 每周更新
- en: ✓ Free
  id: totrans-split-21
  prefs: []
  type: TYPE_NORMAL
  zh: ✓ 免费
- en: ✓ Cancel at any time
  id: totrans-split-22
  prefs: []
  type: TYPE_NORMAL
  zh: ✓ 随时取消
- en: 'The core component of the StripedHyena models is a state-space model (SSM)
    layer. Traditionally, [SSMs](https://kevinkotze.github.io/ts-4-state-space/) have
    been used to model complex sequences and time series data. They are particularly
    useful for tasks where temporal dependencies need to be modeled. In the last two
    years, however, researchers have developed better and better ways to use SSMs
    for sequence models for language and other domains. The reason: they require less
    computing power.'
  id: totrans-split-23
  prefs: []
  type: TYPE_NORMAL
  zh: StripedHyena模型的核心组件是状态空间模型（SSM）层。传统上，[SSMs](https://kevinkotze.github.io/ts-4-state-space/)用于建模复杂的序列和时间序列数据。它们特别适用于需要建模时间依赖性的任务。然而，在过去的两年中，研究人员已经开发出了更好的方法来利用SSMs进行语言和其他领域的序列建模。原因是它们需要较少的计算能力。
- en: 'The result: StripedHyena is more than 30 percent, 50 percent, and 100 percent
    faster than conventional transformers in the end-to-end training of sequences
    of 32,000 tokens, 64,000 tokens, and 128,000 tokens.'
  id: totrans-split-24
  prefs: []
  type: TYPE_NORMAL
  zh: 结果是：StripedHyena在32,000、64,000和128,000令牌的序列端到端训练中比传统transformers分别快30%，50%和100%以上。
- en: The main goal of the StripedHyena models is to push the boundaries of architectural
    design beyond transformers. In the future, the researchers plan to investigate
    larger models with longer contexts, multimodal support, further performance optimizations,
    and the integration of StripedHyena into retrieval pipelines to take full advantage
    of the longer context.
  id: totrans-split-25
  prefs: []
  type: TYPE_NORMAL
  zh: StripedHyena 模型的主要目标是推动架构设计超越变压器的边界。未来，研究人员计划调查更大的模型，具有更长的上下文支持、多模态支持、进一步的性能优化，并将
    StripedHyena 集成到检索管道中，以充分利用更长的上下文。
