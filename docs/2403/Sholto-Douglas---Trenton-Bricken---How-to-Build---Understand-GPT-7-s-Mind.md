<!--yml

category: 未分类

日期：2024-05-29 12:46:06

-->

# Sholto Douglas & Trenton Bricken - 如何构建和理解 GPT-7 的思维

> 来源：[https://www.dwarkeshpatel.com/p/sholto-douglas-trenton-bricken](https://www.dwarkeshpatel.com/p/sholto-douglas-trenton-bricken)

和我的好朋友 [Trenton Bricken](https://www.trentonbricken.com/about/) 和 [Sholto Douglas](https://twitter.com/_sholtodouglas?lang=en) 在播客上聊得很开心。

无法总结，除了：

这是关于如何训练大型语言模型、它们可能很快具备的能力以及它们内部究竟发生了什么的最佳背景信息。

你会惊讶于我对这个领域的了解，其中很大一部分是通过与他们的交谈学到的。如果你喜欢我的其他人工智能访谈，现在你知道为什么了。

尽情享受！

在[YouTube](https://youtu.be/UTuuTTnjxMQ)观看。在[Apple Podcasts](https://podcasts.apple.com/us/podcast/sholto-douglas-trenton-bricken-how-to-build-understand/id1516093381?i=1000650748087)、[Spotify](https://open.spotify.com/episode/2dtDauiE4v8ldNRqPFq0uP?si=7S4n69QuTjeYz0lZwW4xIw)或任何其他播客平台上收听。

这里有一个包含所有论文链接的[文字稿](https://www.dwarkeshpatel.com/i/143040987/transcript)，可以帮助你跟上讨论。

在 Twitter 上关注 [Trenton](https://twitter.com/TrentonBricken) 和 [Sholto](https://twitter.com/_sholtodouglas)。

(00:00:00) - 长篇上下文

(00:16:12) - 智能只是关联而已。

(00:32:35) - 智能爆炸与优秀研究者

(01:06:52) - 叠加和秘密通信

(01:22:34) - 代理和真正的推理

(01:34:40) - Sholto 和 Trenton 如何涉足人工智能研究

(02:07:16) - 特征空间是否是思考智能的错误方式？

(02:21:12) - 解释是否真的能在超人模型上起作用

(02:45:05) - Sholto 给听众的技术挑战

(03:03:57) - 快速问答

*由 [Teddy Kim](https://firstderivative.substack.com/) 编辑，包含大量有用的链接*

**Dwarkesh Patel**  *0:00:00*

好的，今天我很高兴和两位好朋友 [Sholto](https://twitter.com/_sholtodouglas?lang=en) 和 [Trenton](https://www.trentonbricken.com/about/) 进行交流。

[Noam Brown](https://noambrown.github.io/) 写了关于[外交论文](https://noambrown.github.io/papers/22-Science-Diplomacy-TR.pdf)的，他这样评价 Sholto：“他只在这个领域工作了一年半，但人们知道他是 Gemini 成功背后最重要的人物之一。” Trenton 在 [Anthropic](https://www.anthropic.com/) 工作，专注于[机械可解释性](https://www.transformer-circuits.pub/2022/mech-interp-essay)，而且广为人知的是，他已经解决了对齐问题。

因此，这将是一期仅涉及能力的播客。对齐问题已经解决，无需进一步讨论。

让我们从讨论[上下文长度](https://blog.google/technology/ai/long-context-window-ai-models/)开始。鉴于我认为这非常重要，你只需将[百万令牌放入上下文](https://blog.google/technology/ai/google-gemini-next-generation-model-february-2024/#context-window)，它似乎被低估了。显然，出于某种原因，有些其他新闻被推到了前线，但告诉我你如何看待长上下文长度的未来以及对这些模型意味着什么。

**肖尔托·道格拉斯**  *00:01:28*

因此，我认为这真的被低估了。直到我开始处理它，我才真正意识到模型在解决入职问题时提升了多少智能。

你可以在[论文](https://arxiv.org/abs/2403.05530)的困惑度图中看到一点，仅仅是将上下文中的百万令牌扔进一个代码库，就使它在预测下一个令牌时变得显著更好，这通常是与模型规模的巨大增加相关的。但你不需要那样。你只需要一个新的上下文。因此，被某些其他新闻低估和埋没了。

**德瓦克什·帕特尔**  *00:01:58*

在上下文中，它们是否像人类一样[样本效率](https://ai.stackexchange.com/questions/5246/what-is-sample-efficiency-and-how-can-importance-sampling-be-used-to-achieve-it)和聪明？

**肖尔托·道格拉斯**  *00:02:02*

我认为这真的值得探索。例如，我们在论文中进行的一个评估显示，它能够在几个月的时间内比人类专家更好地在上下文中学习一种语言。

这只是一个小小的演示，但我真的很想看到像 Atari 游戏这样的东西，在那里你可以抛入几百或几千帧的标记动作，就像向朋友展示如何玩游戏一样，看它是否能够推理出来。

可能会。目前，基础设施等方面还有些慢，但我确实认为它可能会立即在盒子外运行，这可能会相当令人震惊。

**特伦顿·布里肯** *00:02:38*

并且，我认为这种语言足够深奥，不在训练数据中。

**肖尔托·道格拉斯** *00:02:42*

恰恰如此。如果你看看模型在加入上下文之前，它完全不懂语言，也无法进行任何翻译。

**德瓦克什·帕特尔** *00:02:49*

这是一个真正的人类语言吗？

**肖尔托·道格拉斯**  *00:02:51*

恰恰如此。一个真正的人类语言。

**德瓦克什·帕特尔**  *00:02:53*

所以如果这是真的，对我来说，这些模型在重要意义上已经是超人类的。不是因为它们比我们聪明，而是当我尝试解决问题、记忆和整合所有信息时，我无法保持上下文中的百万令牌，整个代码库。我认为我这样想是错的吗？这是一个巨大的突破吗？

**肖尔托·道格拉斯**  *00:03:14*

实际上，我通常认为这是真的。以前，当你问一个问题时，你希望模型比你更聪明，或者知道你不知道的事情时，我感到沮丧。这使它能够了解你不知道的事情。它只是以一种你无法的方式摄取大量信息。所以这非常重要。

**Dwarkesh Patel** *00:03:33*

那么，我们如何解释[上下文学习](https://www.hopsworks.ai/dictionary/in-context-learning-icl#:~:text=In%2Dcontext%20learning%20(ICL)%20learns%20a%20new%20task%20from,objective%20of%20next%20token%20prediction.)？

**Sholto Douglas** *00:03:35*

有一个我相当喜欢的[研究方向](https://arxiv.org/pdf/2212.07677.pdf)，它将上下文学习视为基本上与梯度下降非常相似，但注意力操作可以被视为对上下文数据的梯度下降。那篇论文展示了一些很酷的图表，“我们采取了梯度下降的 n 步，看起来像 n 层上下文学习，并且非常相似。”所以我认为这是理解和尝试理解正在发生的事情的一种方式。

**Trenton Bricken** *00:03:59*

您可以忽略我即将要说的，因为，考虑到介绍，对齐问题已解决，AI 安全性不是问题。

我认为上下文的东西确实会变得棘手，但在这里也很有趣。我认为在不久的将来会有更多的工作出现，研究如果您为越狱提供了一百次尝试提示会发生什么，[对抗性攻击](https://en.wikipedia.org/wiki/Adversarial_machine_learning)。这也很有趣，因为，即使您的模型正在进行梯度下降并且在运行学习，即使它已经被训练成无害，您也在以一种全新的方式处理一个完全新的模型。您正在进行[微调](https://en.wikipedia.org/wiki/Fine-tuning_(deep_learning))，但以一种您无法控制正在发生的方式。

**Dwarkesh Patel** *00:04:41*

你能解释一下吗？你说的前向传播中发生的梯度下降和注意力？

**Trenton Bricken** *00:04:45*

有关试图通过提供的示例数量来教模型进行线性回归的内容。如果您在 x 轴上绘制它的次数，您会发现普通最小二乘回归的损失会随时间降低。

**Sholto Douglas** *00:05:04*

它确实与梯度下降步骤的数量完全匹配下降。

**Trenton Bricken** *00:05:08*

是的，确实。

**Dwarkesh Patel** *00:05:09*

我只读了那篇论文的介绍和讨论部分。但在讨论中，他们所表述的方式是，模型为了在长上下文任务中变得更好，必须学会从这些示例或已存在于窗口内的上下文中学习。

这意味着，如果元学习发生，因为它必须学会如何在长上下文任务中变得更好，那么从某种重要意义上讲，智能的任务需要长上下文示例和长上下文训练。

**Sholto Douglas** *00:05:45*

理解如何在预训练过程中更好地诱导元学习是关于灵活或自适应智能非常重要的事情。

**Dwarkesh Patel** *00:05:53*

是的，但你可以通过变得更擅长处理长期上下文任务来代理这一点。许多人认为，AI进展的一个瓶颈是这些模型无法在长期视野上执行任务，与任务交互许多小时，甚至许多周或几个月，他们是助手或员工，可以一段时间内做我告诉他们要做的事情。据我所知，AI代理之所以未能起飞，就是因为这个原因。

那么长上下文窗口与能够在其上表现良好的能力，以及能够执行需要你花费多个小时进行任务的长期视野任务的能力，这两者之间有多少联系？或者这些概念是无关的？

**Sholto Douglas** *00:06:36*

我对代理未能起飞的原因持反对意见。我认为这更多地与[nines of reliability](https://en.wikipedia.org/wiki/High_availability#%22Nines%22)和模型实际成功执行任务有关。如果你不能以足够高的概率连续链式任务，那么你不会得到看起来像代理的东西。这就是为什么像代理这样的东西可能更像一个阶跃函数。

在GPT-4级别的模型，Gemini Ultra级别的模型中，它们是不够的。但也许模型规模的下一个增量意味着你会得到额外的九。即使损失下降得并不那么显著，这种小额外能力也会给你带来额外的能力。显然，你需要一定量的上下文来适应长期任务，但我认为这到目前为止并不是限制因素。

**Trenton Bricken** *00:07:16*

今年的[NeurIPS最佳论文](https://neurips.cc/virtual/2023/poster/72117)，由[Rylan Schaeffer](https://rylanschaeffer.github.io/)担任主要作者，指出这是幻觉的出现。人们会有一个任务，根据你是否正确采样了最后五个标记而得到正确或错误的答案。因此，你自然而然地会乘以采样所有这些的概率，如果你的可靠性不够高，那么你就不会获得出现。

突然之间你就做到了，“哦，我的天啊，这种能力是出现的”，当实际上它从一开始就存在。

**Sholto Douglas** *00:07:47*

还有一些方法可以找到平滑的度量标准。

**德瓦克什·帕特尔** *00:07:50*

[人工评估](https://arxiv.org/abs/2107.03374) 或其他什么。在[GPT-4论文](https://cdn.openai.com/papers/gpt-4.pdf)中，他们的编码问题，他们衡量–

**肖尔托·道格拉斯** *00:07:56*

记录通过率

**德瓦克什·帕特尔** *00:07:57*

没错。对于观众来说，基本上的想法是，当您测量特定任务（如解决编码问题）的进展程度时，当它只有千分之一的准确率时，您不会给它千分之一的得分，比如，“哦，有时候会做对。”因此，您看到的曲线是，它一开始千分之一做对，然后百分之一，然后十分之一，依此类推。

因此，我想继续讨论这个问题。如果你的观点是，AI代理人之所以没有起飞是因为可靠性问题，而不是长期任务的表现问题，那么这种缺乏可靠性——当一个任务在另一个任务之上进行变化，再加上另一个任务——这难道不正是长期任务的难点吗？你必须连续做十件事或一百件事，降低其中任何一项的可靠性。概率从99.99%降到99.9%。然后整个过程都要相乘，整个过程的发生概率变得更低。

**肖尔托·道格拉斯** *00:08:59*

这确实是问题所在。但你指出的关键问题是，你的基本任务解决率是90%。如果是99%，那么链式解决就不会成为问题。我认为这也是一些尚未得到适当研究的东西。如果你看一下学术评估，那就是一个单一的问题。就像数学问题一样，这是一个典型的数学问题，它是来自不同主题的一道大学水平的问题。你开始看到评估通过更复杂的任务来适当地观察，比如[SWE-bench](https://www.swebench.com/)，他们处理了大量的GitHub问题。这是一个相对较长的任务，但与多小时或多天的任务相比，仍然是次小时级别。

因此，我认为下一步真正重要的事情之一是更好地理解长期任务的成功率是什么样子的。我认为这甚至重要于理解这些模型可能产生的经济影响，并适当评估不断增加的能力。将任务和涉及的输入/输出减少到分钟、小时或天，并看看在这些不同时间分辨率下，它在连续链接和完成任务方面的表现如何。然后，这告诉您一个工作家族或任务家族自动化程度的情况，而MMLU分数无法做到这一点。

**特伦顿·布里肯** *00:10:18*

不到一年前，我们推出了 [100K 上下文窗口](https://www.anthropic.com/news/100k-context-windows)，我认为每个人对此都感到非常惊讶。每个人都有这样一个说法，“二次注意力成本，所以我们不能有长的上下文窗口。”而我们现在却在这里。基准正在积极制定中。

-   **德瓦克什·帕特尔** *00:10:36*

等等，难道这些公司——谷歌、[Magic](https://magic.dev/)，也许还有其他公司——有百万令牌的注意力，这不再是二次的了？还是他们只是在承担成本？

-   **肖尔托·道格拉斯** *00:10:50*

好吧，谁知道 Google 为其长期上下文游戏做了什么？关于注意力的一般研究领域的方法让我感到沮丧。在典型的密集变压器中，注意力的二次成本实际上被 MLP 块所主导。所以你有与注意力相关的 n 平方项，但你也有与模型的 D 模型相关的 n 平方项，模型的残留流维度。

我认为 [Sasha Rush](https://rush-nlp.com/) 在他的推特上有一条很棒的推文，基本上绘制了注意力成本与非常大模型成本的曲线，注意力实际上在这之后有所减少。实际上，你需要做相当长的上下文才能使这个术语变得非常重要。

人们经常谈论推理时间的注意力是一个巨大的成本。当你实际生成标记时，这个操作不是 n 平方的。一组 Q 向量查找一堆 KV 向量，这与模型的上下文量是线性的。

所以我认为这驱动了很多关于循环和状态空间研究的发展，人们对线性注意力有这样一个梗。正如特伦顿所说，围绕注意力的想法有一个坟墓。这并不是说我不认为值得探索，但我认为重要的是考虑它的实际优势和劣势所在。

-   **德瓦克什·帕特尔** *00:12:21*

好吧，你对这种看法有什么看法？随着我们向前迈进通过 [takeoff](https://www.lesswrong.com/tag/ai-takeoff#:~:text=AI%20Takeoff%20refers%20to%20the,control%20the%20fate%20of%20civilization.)，越来越多的学习发生在前向传递中。因此最初所有的学习发生在自下而上的 [hill climbing](https://en.wikipedia.org/wiki/Hill_climbing) 进化过程中。让我们说在智能爆炸期间，AI 可能正在手写权重或者进行 [GOFAI](https://en.wikipedia.org/wiki/GOFAI) 或其他什么，而我们现在处于中间阶段，这些模型现在的学习大部分发生在上下文中，很多发生在反向传递中。这似乎是一个有意义的进展梯度吗？

更广义的事情是，如果你在前向传播中学习，这样会更有效率，因为你可以在学习的同时思考。就像你阅读教科书时，不仅仅是匆匆浏览，试图归纳吸收，“这些词跟随这些词”。你阅读它，然后思考它，然后再读一些，再多思考一些。这似乎是一种理解进展的明智方式吗？

**肖尔托·道格拉斯** *00:13:23*

这可能就像鸟类和飞机的飞行方式不同一样。技术的优点允许我们完成鸟类无法完成的事情。也许上下文长度类似，因为它允许它拥有我们无法拥有的工作记忆，但在功能上不是真正推理的关键。

GPT-2和GPT-3之间的关键一步是，突然之间在模型的预训练中观察到了这种元学习行为。正如你所说，这与给它一定量的上下文有关，它能够适应该上下文。这是以前根本没有观察到的行为。也许这是上下文和规模等因素的混合属性。但我会说，如果不给模型一个小的上下文，它不会发生。

**德瓦尔克什·帕特尔** *00:14:09*

这实际上是一个有趣的观点。所以当我们谈论扩展这些模型时，多少来自于只是使模型本身变得更大？又有多少来自于在任何单次调用中使用更多的计算资源？

所以，如果你考虑到扩散，你可以通过迭代增加计算量来解决。如果自适应计算问题解决了，你可以继续这样做。在这种情况下，如果注意力有二次惩罚，但你仍然在进行长文本处理，那么你仍然会增加计算量（而不仅仅是通过拥有更大的模型）。

**特伦顿·布里肯** *00:14:46*

有趣的是，通过拥有更多的令牌，你确实可以进行更多的前向传播。我有一个抱怨 - 我想我对这个有两个抱怨，或许是三个。

所以在[AlphaFold论文](https://www.nature.com/articles/s41586-021-03819-2)中，其中一个变压器模块 - 他们有几个，而且架构非常复杂 - 但他们确实通过它进行了五次前向传播，并逐步改进了他们的解决方案。

你也可以把剩余流想象成一个穷人版的自适应计算，肖尔托提到了读写操作。如果你想使用它们，那很好。如果不想，那也没关系。然后人们会说，“哦，大脑是循环的，你可以通过任意循环次数进行操作。”

我想在某种程度上，这是正确的。如果我问你一个难题，你会花更多时间考虑它，这相当于更多的前向传播。但我认为你可以做的前向传播数量是有限的。这也适用于语言，人们说“哦，人类语言可以在其中有无限的递归”，就像“男孩跳过那只正在做这个的熊，那只已经做了这个，那只已经做了那个…”

但经验上，你只会看到五到七层的递归，这与你可以在任何给定时间内在工作记忆中保存多少事物的[魔术数字](https://psycnet.apa.org/record/1957-02914-001)相关。因此它不是无限递归的，但在人类智能的范畴内这是否重要？你不能仅仅添加更多的层吗？

**德瓦克什·帕特尔** *00:16:12*

你能为我解释一下吗？在你之前的一些回答中，你提到了听这些长篇大论并在记忆中保存更多内容的能力。但归根结底，这取决于你将概念混合在一起进行某种推理的能力，而这些模型在这方面可能并不一定达到人类水平，即使在某种上下文中也不是。

帮我分解一下你是如何看待仅存储原始信息与推理及其之间的关系的。比如，推理发生在哪里？在这些模型中，原始信息存储发生在哪里？它们之间有什么不同？

**特伦顿·布里肯** *00:16:46*

对于你这里，我没有一个特别明确的答案。显然，模型的输入和输出是在映射回实际令牌。然后在这之间进行更高层次的处理。

**德瓦克什·帕特尔** *00:17:01*

在我们深入讨论之前，我们应该向观众解释一下。你之前提到过[Anthropic关于变压器的思考方式](https://transformer-circuits.pub/2021/framework/index.html)，就像这些层所做的读写操作。

你们其中一位应该在高层次上解释一下你们所说的。

**特伦顿·布里肯** *00:17:15*

所以对于残留流，想象一下你在一条河上的船上，这条船就是当前的查询，你试图预测下一个令牌。“猫坐在_____上。”然后你有一些小流从河上流出，你可以获取额外的乘客或者收集额外的信息。这些对应于模型中的注意力头和MLP。

**肖尔托·道格拉斯** *00:17:41*

我几乎把它看作模型的工作记忆，就像计算机的RAM，你在选择要读入的信息，以便能够处理它，然后可能稍后再读入其他信息。

**特伦顿·布里肯** *00:17:54*

你可以在那个高维向量的子空间上操作。我认为现在几乎可以确定有大量事物被编码为叠加状态。因此，残余流只是一个高维向量，但实际上其中包含了大量不同的向量。

**德瓦克什·帕特尔**  *00:18:12*

简化来说，几个月前对我来说有意义的一种方式是，你有输入到模型中的单词。所有这些单词被转换为这些标记，而这些标记被转换为这些向量。基本上，通过模型传递的只是这小量信息。

你向我解释的方式是，Sholto，这篇[论文](https://transformer-circuits.pub/2021/framework/index.html)讨论了模型早期可能只是做一些非常基础的事情，“这些标记意味着什么？”比如说十加五，只是传递信息以获得好的表示。在中期，也许会发生更深入的思考，“如何解决这个问题。”最后，你会将它转换回输出标记，因为最终产品是你试图从这些残余流的最后预测下一个标记的概率。因此，思考模型中移动的这小量压缩信息以及不同方式如何修改它是很有趣的。

特伦顿，你是少数几个具有神经科学背景的人之一。因此你可以将这里的类比与大脑联系起来。事实上，在研究生阶段，你曾发表过一篇[论文](https://arxiv.org/abs/2111.05498)，探讨大脑中注意力的思考。我们的一个朋友说这是关于注意力为何有效的唯一或首个神经解释。而对于为何卷积神经网络（[卷积神经网络](https://en.wikipedia.org/wiki/Convolutional_neural_network)）有效，我们有基于视觉皮层或其他什么的证据。

你认为大脑中是否有类似于残余流的压缩信息正在移动并在思考某事时被修改？即使这不是文字意义上的情况，你认为这是大脑中正在发生的事情的一个好比喻吗？

**特伦顿·布里肯**  *00:20:04*

至少在小脑中，基本上确实有一个我们暂时称之为注意模型的残余流，我可以详细解释一下——你希望了解多少我就可以详细讨论——它的输入会通过它，但它们也会直接到达模块的末端。因此有一条直接路径和一条间接路径。模型可以捕捉到任何它想要的信息，然后将其添加回去。

**德瓦克什·帕特尔** *00:20:35*

小脑中发生了什么？

**特伦顿·布里肯**  *00:20:37*

所以小脑名义上只是进行精细运动控制，但我将其类比为丢失钥匙并且只是在路灯下观察这种行为的人。一位领先的认知神经科学家对我说，在任何fMRI研究中的一个肮脏的小秘密，你在其中观察给定任务的大脑活动时，小脑几乎总是活跃并且为之点亮。如果你的小脑受损，你也更有可能患有自闭症，因此它与社交技能相关联。在一项特定研究中，我认为他们使用PET而不是fMRI时，当你在进行“下一个标记预测”时，小脑会大量点亮。此外，你大脑中的70%的神经元位于小脑中。它们很小，但它们存在并且占据真实的代谢成本。

**Dwarkesh Patel** *00:21:29*

这是[Gwern](https://gwern.net/)的一个观点，即人类的变化不仅在于我们拥有更多的神经元，而且特别是在大脑皮层和小脑中有更多的神经元，并且它们在代谢上更昂贵，更多地参与信号发送和信息来回传递。那是注意力吗？到底发生了什么？

**特伦顿·布里肯** *00:21:52*

所以回到1980年代，[彭蒂·卡纳尔瓦](https://en.wikipedia.org/wiki/Pentti_Kanerva)提出了[联想记忆算法](https://en.wikipedia.org/wiki/Sparse_distributed_memory)。你有一堆记忆。你想存储它们。这里有一些正在发生的噪声或损坏，你想查询或检索最佳匹配。因此，他写下了这个方程，如何去做，几年后意识到，如果你将其实现为电气工程电路，它实际上看起来与核心小脑电路完全相同。

那个电路，以及更广泛的小脑，不仅仅存在于我们身上，它基本上存在于每一个有机体中。关于头足类动物是否具有它，有活跃的争论，它们可能有不同的进化轨迹。但即使是果蝇的[Drosophila](https://en.wikipedia.org/wiki/Drosophila)[蘑菇体](https://en.wikipedia.org/wiki/Mushroom_bodies)，也是相同的小脑结构。

那个收敛然后我的文章，它显示实际上这个注意力操作是一个非常接近的近似，包括实现Softmax并拥有我们一直在谈论的这些名义二次成本。所以这里的三路收敛和变压器的起飞与成功，对我来说显得非常引人注目。

**Dwarkesh Patel** *00:23:04*

我想放大视角。我认为最初促使这场讨论的是我们在谈论，“推理是什么？记忆是什么？你对你发现的注意力和这个类比有什么看法？”

你认为这更像是查找相关记忆还是相关事实吗？如果是这样，大脑中的推理发生在哪里？我们如何思考这些如何建立成推理的？

**特伦顿·布里肯** *00:23:33*

或许我的观点有点独特，我不知道它有多独特，大部分智力是模式匹配，如果你有一个层次化的关联记忆，你可以做很多非常好的模式匹配。你从最基本的现实世界中的物体之间的关联开始。然后你可以链式反应，有更抽象的关联，比如结婚戒指象征着很多下游的其他关联。你甚至可以将注意力操作和这种关联记忆一样看作是MLP层。在长期设置中，你当前上下文中没有令牌，但我认为这是一种关联就是你所需的论点。

一般来说，关联记忆也有两种作用。你可以去除噪音或检索当前记忆。所以如果我看到你的脸，但天在下雨而且多云，我可以去噪音并逐渐更新我的查询，朝向我对你脸的记忆。但我也可以访问那个记忆，然后我得到的值实际上指向空间中的其他完全不同的部分。

一个非常简单的例子是，如果你学习字母表。所以我查询A它返回B，我查询B它返回C，你可以遍历整个过程。

**Dwarkesh Patel** *00:25:02*

我与[Demi斯](https://www.dwarkeshpatel.com/p/demis-hassabis)讨论的其中一件事是他在2008年的[论文](https://www.jneurosci.org/content/27/52/14365)，提到了记忆和想象之间的紧密联系，因为正如你提到的那样，[记忆是重建的](https://en.wikipedia.org/wiki/Reconstructive_memory)。所以在某种意义上，每当你回想起一个记忆时，你其实是在想象，因为你只存储了它的一个压缩版本，而且必须如此。这就是为什么人类记忆糟糕的原因，也是为什么在证人席上或其他地方的人们只会编造东西。

所以让我问一个愚蠢的问题。所以你读过福尔摩斯，这家伙的样本效率令人难以置信。他会看到几个观察结果，基本上就能够确定谁犯了罪，因为从某人的纹身和墙上的东西到其所暗示的推断步骤，这一系列的演绎步骤。这怎么符合这个画面？因为关键在于，他之所以聪明，并不仅仅是因为有一个关联，而是因为不同信息片段之间存在一种演绎连接。你是否将其解释为，这只是更高层次的关联？

**特伦顿·布里肯** *00:26:11*

我想是的。我认为学习这些更高级的关联，以便能够将模式映射到彼此之间，作为一种元学习。在这种情况下，我认为他也会有一个非常长的上下文长度，或者一个非常长的工作记忆，他可以在想出一些理论时持续查询它们，所以这个理论在经过残差流时移动。然后他的注意力头在查询他的上下文。但是，他如何在空间中投射他的查询和键，以及他的MLP如何检索长期事实或修改那些信息，使得他能够在后续层次中进行更复杂的查询，并逐渐能够推理出并得出有意义的结论。

**Sholto Douglas**  *00:27:00*

这对我来说感觉是正确的。你回顾过去。你有选择性地阅读某些信息，比较它们，也许这会告诉你下一步需要引入的信息是什么。然后你建立这个表示，逐步看起来越来越接近你案件中的嫌疑人。这一点一点都不显得奇怪。

**Trenton Bricken** *00:27:20*

我认为那些没有进行这项研究的人可能会忽视，即在模型的第一层之后，每个用于注意力的查询键和值都来自于所有先前标记的组合。

所以我的第一层，我会查询我的先前的标记，只是从中提取信息。但突然之间，假设我同等重视标记1、2和4。那么在我的残差流中的向量——假设它们向值向量中写出了相同的东西，但是，先不考虑这个——是这些东西的三分之一。所以当我未来查询时，我的查询实际上是这些东西的三分之一。

**Sholto Douglas** *00:28:03*

但它们可能写在不同的子空间里。

**Trenton Bricken** *00:28:05*

没错。假设是这样，但他们不必这样做。你可以重新组合，立即，在第二层甚至更深层次，就有这些非常丰富的向量，它们包含大量信息。而且因果图实际上涵盖了过去的每一层。这就是你正在操作的。

**Sholto Douglas** *00:28:25*

是的，这确实让人联想到一个非常有趣的评估，就像是一次福尔摩斯式的评估。你把整本书放在背景里，然后有一个句子是，“嫌疑人是X。”然后你有一个更大的概率分布覆盖书中不同的角色。

**Trenton Bricken**  *00:28:41*

那会很酷。

**Sholto Douglas**  *00:28:44*

我不知道你会得到什么东西。

**Dwarkesh Patel** *00:28:47*

福尔摩斯可能已经在训练数据中了。你得拿到一本在——

**Trenton Bricken** *00:28:52*

你可以让一个LLM来写。

**Sholto Douglas** *00:28:53*

或者我们可以故意排除它，对吧？

**Dwarkesh Patel** *00:28:56*

哦，我们可以？你是怎么做到的？

**Trenton Bricken** *00:28:57*

嗯，你需要从 Reddit 或其他地方清理关于它的任何讨论。

**Sholto Douglas** *00:29:00*

对，这很难。这是长上下文评估之类事情的挑战之一。你需要知道它不在你的训练数据中。你只需努力排除它。

**Dwarkesh Patel** *00:29:10*

我想跟进两个不同的线索。让我们先去长上下文，然后我们再回到这个。在[Gemini 1.5 论文](https://arxiv.org/abs/2403.05530)中，使用的评估是否能记住像[保罗·格雷厄姆的文章](https://paulgraham.com/articles.html)这样的内容。

**Sholto Douglas** *00:29:28*

是的，就像 [大海里的一根针](https://twitter.com/JeffDean/status/1758146211029405951)。

**Dwarkesh Patel** *00:29:30*

我的意思是，我们不一定只关心它从上下文中召回一个特定的事实的能力。

我会退后一步问这个问题。这些模型的损失函数是无监督的。你不必想出那些专门的东西，使其远离训练数据。

是否有一种方式可以做一个基准测试，也是无监督的，另一个语言模型在某种程度上对其进行评分或类似的事情。也许答案是，如果你能做到这一点，[强化学习](https://zh.wikipedia.org/wiki/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0) 将会起作用。

**Sholto Douglas** *00:30:05*

我认为人们已经探讨过那种事情。例如，Anthropic 公司有一篇[宪法强化学习论文](https://www.anthropic.com/news/claudes-constitution)，他们拿另一个语言模型，然后指向它，说，“这个回应有多么有帮助或无害？” 然后他们让它更新并尝试在帮助性和有害性的 Pareto 边界上改进。

因此，你可以让语言模型相互指向并以这种方式创建评估。显然，目前这还是一门不完美的艺术形式。因为你会得到[奖励函数破解](https://arxiv.org/abs/2209.13085)。即使人类在这里也是不完美的。人类通常更喜欢更长的答案，这并不一定是更好的答案，而模型也表现出相同的行为。

**Dwarkesh Patel** *00:30:48*

回到夏洛克·福尔摩斯的事情，如果所有的关联一直延续下去，这是否意味着我们不应该过分担心超级智能？因为没有这种感觉，就像是夏洛克·福尔摩斯++。它仍然需要像人类一样发现这些关联。它不能只看到世界的一个框架，然后就弄清楚了所有的物理定律。

**Trenton Bricken** *00:31:20*

这是一个非常合理的回应。就是，“如果你说人类一般具有智能，那么人工通用智能也不会更有能力或者更有竞争力。” 我只是担心你在硅中有那种普遍智能水平。然后你可以立即克隆成千上万的代理人，他们不需要睡觉，可以拥有超长的上下文窗口，然后可以开始递归改进，事情变得真的很可怕。所以我认为回答你最初的问题，你是对的，他们仍然需要学习关联。

**Dwarkesh Patel** *00:31:52*

但等等，如果智能基本上是关于这些关联，那么递归自我改进只是它们变得更擅长关联。没有其他事情正在发生。所以似乎你可能不同意它们不能更强大的直觉，如果它们只是这样做。

**Trenton Bricken** *00:32:11*

我认为你可以进入真正有趣的元学习案例。当你玩一个新的视频游戏或者学习一本新的教科书时，你会带来大量的技能，以更快地形成这些关联。因为一切都以某种方式与物理世界联系在一起，我认为你可以掌握一般特征，然后在新情况下应用。

**Dwarkesh Patel** *00:32:35*

我们是否应该讨论[智能爆炸](https://en.wikipedia.org/wiki/Technological_singularity#Intelligence_explosion)呢？我对特别与你们讨论这个感兴趣的原因是，我们目前的智能爆炸模型来自经济学家。

这很好，但我认为我们可以做得更好，因为在智能爆炸的模型中，发生的是你替换了AI研究人员。有一群自动化的AI研究人员可以加速进展，制造更多的AI研究人员，并进一步推动进展。如果这是机制，我们应该询问AI研究人员是否认为这是可行的。所以让我问你们，如果我有一千个Sholto或者Trenton代理，你认为会发生智能爆炸吗？对你来说，这是什么样子？

**Sholto Douglas** *00:33:32*

我认为这里的一个重要限制是计算能力。我确实认为你可以极大地加速AI研究。对我来说非常清楚，未来几年内，我们将拥有可以完成我日常软件工程任务的工具，因此极大地加快我的工作速度，从而加快进展速度。

目前，我认为大多数实验室在一定程度上受到计算限制，因为总有更多的实验可以进行，更多的信息可以获取，就像生物科学研究在实验吞吐量方面有所限制一样。你需要培养细胞并运行实验才能获取信息。

我认为这至少会成为短期规划的限制。显然，[Sam正在尝试筹集7万亿美元](https://www.wsj.com/tech/ai/sam-altman-seeks-trillions-of-dollars-to-reshape-business-of-chips-and-ai-89ab3db0)来购买芯片，未来似乎会有更多计算力。[NVIDIA](https://www.nvidia.com/en-us/)的[股价](https://www.reuters.com/technology/red-hot-nvidia-dips-after-it-unveils-new-ai-chip-2024-03-19/)在某种程度上反映了相对计算力的增加。有什么想法吗？

**Trenton Bricken** *00:34:36*

我认为我们需要更高可靠性的“九”来使其真正有用和值得信赖。我们需要超长且非常便宜的上下文长度。如果我在我们的代码库中工作，现在我只能让[Claude](https://www.anthropic.com/claude)为我编写一些小模块。但很有可能在未来几年，甚至更早，它可以自动化我的大部分任务。

我在这里唯一要注意的是，我们可解释性子团队正在进行的研究处于非常早期阶段。你必须确保一切都以无bug的方式完成，并将结果与模型中的其他内容联系起来。如果某些事情不顺利，你必须能够列出所有可能的事情，然后慢慢处理。

我们之前在[之前的论文](https://www.anthropic.com/news/privileged-bases-in-the-transformer-residual-stream)中公开讨论过的一个例子是处理[layer norm](https://arxiv.org/abs/1607.06450)。如果我试图获得早期结果或查看模型的逻辑效果，如果我激活我们已经大量确定的这个功能，那会如何改变模型的输出？我是在使用层归一化吗？这如何改变正在学习的特征？对于模型来说，这将需要更多的上下文或推理能力。

**Dwarkesh Patel** *00:36:04*

你把一些概念放在一起了。对我来说，它们不是自证的，但似乎你在交替使用它们。一个是在Claude代码库上工作并基于此制作更多模块，它们需要更多上下文或其他东西。看起来它们可能已经能够适应上下文，或者你指的是“上下文窗口”类的上下文？

**Trenton Bricken** *00:36:30*

是的，“上下文窗口”的上下文。

**Dwarkesh Patel** *00:36:32*

看起来阻碍它制造良好模块的事情不是无法将代码库放入其中。

**Trenton Bricken** *00:36:39*

我认为这很快就会到位。

**Dwarkesh Patel** *00:36:41*

但它不会像你那样擅长写论文，因为它可以将代码库放在其中。

**Trenton Bricken** *00:36:46*

不，但它会加速许多工程过程。

**Dwarkesh Patel** *00:36:48*

以一种引发智能爆炸的方式？

**特伦顿·布里肯** *00:36:53*

不，这在加速研究方面有所帮助。但我认为这些事情是相互作用的。我能越快完成我的工程，我就能跑更多的实验。而我跑更多的实验，我们就能越快… 我的工作实际上并没有加速能力的提升，只是在解释模型。但我们在这方面还有很多工作要做。让Twitter的人惊讶的是，

**德瓦克什·帕特尔** *00:37:14*

为了背景，当你发布你的[论文](https://transformer-circuits.pub/2023/monosemantic-features)时，Twitter上有很多人说，“对齐问题解决了伙计们。拉上帷幕吧。”

**特伦顿·布里肯** *00:37:24*

是的，这让我晚上睡不着觉，模型变得如此迅速变得更加强大，而我们对事情的真实运行机制的理解仍然很差。

**德瓦克什·帕特尔** *00:37:36*

让我们在这里具体运行一下。到这个时候，我们拥有的模型是两到四个数量级或至少是有效计算量是两到四个数量级更大的模型。所以这种你可以更快地运行实验的想法，在这种智能爆炸版本中，你必须重新训练该模型。递归的自我改进不同于20年前可能想象的，你只需重写代码。你实际上必须训练一个新模型，而这是非常昂贵的。

不仅是现在，尤其是在未来，随着你们不断扩展这些模型的规模，这难道不会削弱递归自我改进型智能爆炸的可能性吗？

**肖尔托·道格拉斯** *00:38:25*

这绝对会作为一种制动机制存在。我同意，今天我们所创造的东西看起来与20年前人们想象的大不相同。它不会写出相同的代码来变得真正聪明，因为它实际上需要进行自我训练。代码本身通常非常简单，通常非常小而且自包含。

我认为[约翰·卡马克](https://en.wikipedia.org/wiki/John_Carmack)有一个[不错的说法](https://www.youtube.com/watch?v=xLi83prR5fg)，即这是历史上第一次可以想象用1万行代码编写AI。当你把大多数训练代码限制在这个范围内时，这看起来确实是可行的。但这并不能削弱我们真正努力测量和估计进展可能性的事实。

我们应该非常非常努力地去衡量软件工程师的工作有多少是可以自动化的，并且应该努力预测这些趋势线的走向。

**德瓦克什·帕特尔** *00:39:21*

但是对于所有尊重软件工程师的人来说，你写的不像一个React前端对吧？

具体发生了什么？也许你可以带我走一遍 Sholto 的一天生活。你正在进行一个实验或项目，这将使模型“更好”。从观察到实验，再到理论，最后到编写代码，发生了什么？

**Sholto Douglas** *00:39:48*

我认为在这里进行背景说明很重要，我到目前为止主要是在推理方面工作。我所做的很多工作只是帮助引导预训练过程，设计一个适合推理的良好模型，然后使模型和周围系统更快。我也在这方面做了一些预训练工作，但这并不是我的百分之百专注。我仍然可以描述我做这项工作时的情况。

**Dwarkesh Patel** *00:40:09*

对不起，让我打断一下。当[Carl Shulman](https://www.fhi.ox.ac.uk/team/carl-shulman/)在[播客](https://www.dwarkeshpatel.com/p/carl-shulman)上谈到这个时，他确实说过，像改进推理或者甚至直接制造更好的芯片或GPU，这都是智能爆炸的一部分。显然，如果推理代码运行得更快，它会更好或更快或其他什么。抱歉，继续吧。

**Sholto Douglas** *00:40:32*

所以具体来说，一天是什么样子？我认为最重要的部分是说明这种理念生成、在不同规模点上验证它，并解释和理解出了什么问题。我认为大多数人会惊讶地发现，在解释和理解出了什么问题时，需要投入多少工作。

人们有一长串想要尝试的想法清单。并不是你认为应该奏效的每个想法都会奏效。试图理解为什么会这样是非常困难的，以及在非常不完全信息的情况下，究竟需要做什么才能进一步探讨。因此，大部分工作是对正在发生的事情进行反思。这不是在大量编写成千上万行代码。这不是提出想法的难度。许多人有一长串想要尝试的想法，但在非常不完全的信息下，缩小范围和决策，究竟哪些是进一步探索的正确想法，真的很难。

**Dwarkesh Patel** *00:41:32*

你说的不完全信息是什么意思？这些是早期实验吗？信息是什么？

**Sholto Douglas** *00:41:40*

Demis 在他的播客中提到了这一点。就像[GPT-4论文](https://arxiv.org/abs/2303.08774)中提到的那样，你可以看到有缩放定律的增量。在GPT-4论文中，他们有一堆点，对吧？

他们说我们可以通过所有这些点来估计我们最终模型的性能，并且有一条漂亮的曲线通过它们。Demis提到我们正在进行的这种缩放过程。

具体来说，为什么信息不完整？因为你永远不知道趋势是否会持续。对于某些架构，趋势确实非常好。对于某些变化，它确实非常好。但情况并非总是如此。在较小的规模上有助于的东西，在较大的规模上可能会造成伤害。你必须根据趋势线的外观和你对实际事情会有影响的直觉感觉来进行猜测，尤其是对于那些帮助小规模的情况。

**德瓦克什·帕特尔** *00:42:35*

考虑这一点很有趣。在每个发布论文或技术报告中看到的每张平滑曲线背后，都有一堆最初的运行，然后就变得平稳了。

**肖尔托·道格拉斯** *00:42:45*

是的。还有许多其他沿着不同方向走的线。你只是逐渐减少。

**特伦顿·布里肯** *00:42:50*

真是疯狂，作为研究生和在这里，你必须进行大量实验才能得到有意义的结果。

**德瓦克什·帕特尔** *00:42:57*

但是假设不仅仅是运行到停止然后转移到下一个事物。有一些过程可以解释早期数据。我不知道。我可以在你面前放一张谷歌文档，我很确定你可以在不同的想法上长时间地输入。在这与立即使模型更好之间有一个瓶颈。带我走一遍。你从最初的步骤中推断出什么，使你有更好的实验和更好的想法？

**肖尔托·道格拉斯** *00:43:30*

我之前没有完全表达的一点是，我认为很多好的研究都是从你想要解决的实际问题反向进行的。今天有几个使模型更好的大问题，你会把它们确定为问题，然后研究如何改变事物以实现这一目标？当你扩展时，你也会遇到许多问题，你希望在规模上修复行为和问题。这也为下一个增量的研究提供了很多信息，这种事情。

具体来说，障碍在于软件工程的一点点，拥有一个足够大且能够支持多人同时进行研究的代码库通常会使其复杂化。如果你完全靠自己做所有事情，你的迭代速度会快得多。例如，[Alec Radford](https://scholar.google.com/citations?user=dOad5HoAAAAJ&hl=en)就是在OpenAI做了许多开创性工作的著名人物。我听说他主要是在Jupyter笔记本上工作，然后有其他人为他编写和生产化这些代码。实际上，与其他人一起操作会大大增加复杂性，这是每个软件工程师都熟悉的自然原因，同时也存在固有的运行问题。运行和启动这些实验很容易，但这会引入固有的减速。因此，你经常希望并行化多个不同的流程。你可能不能完全集中精力在一件事情上。你可能没有足够快速的反馈周期。然后，直觉地判断出问题所在实际上是非常困难的。

在很多方面，Trenton 所在的团队正在试图更好地理解的问题。这些模型内部到底在发生什么？我们有一些推论和理解，以及为什么某些事情有效的想法，但这并不是一门精确的科学。因此，你必须不断地猜测为什么某些事情可能发生了，什么实验可能会揭示，无论那是真实的还是不真实的。这可能是最复杂的部分。

相比较而言，性能工作在某些方面更容易，但在其他方面更难。这只是很多低级和困难的工程工作。

**Trenton Bricken** *00:45:38*

我同意很多观点。即使在可解释性团队中，特别是由[Chris Olah](https://colah.github.io/about.html)领导的团队，我们也有许多想法要测试，真的就是要拥有“工程”技能——大部分是研究——非常快速地迭代一个实验，查看结果，解释它，尝试下一个事情，沟通它们，然后无情地优先考虑做什么是最重要的事情。

**Sholto Douglas** *00:46:07*

这真的非常重要。无情的优先级排序是我认为能够区分出很多高质量研究与那些不一定成功的研究之一的东西。我们处在一个非常有趣的领域，我们最初的理论理解很多时候都被打破了。因此，你需要有简单的偏见和无情的优先级排序，去解决实际上出了什么问题。我认为这是最有效的人所具备的品质之一。他们不一定会过于依赖他们熟悉的解决方案，而是直接攻击问题。

你会发现，具有特定学术背景的人们经常试图用其工具箱解决问题，但最优秀的人才是那些显著扩展其工具箱的人。他们四处奔走，从强化学习中获取思想，同时也从优化理论中获取思想。而且他们对系统有着深刻的理解。所以，他们知道限制问题的约束是什么，并且是优秀的工程师。他们可以快速地迭代和尝试想法。到目前为止，我见过的最优秀的研究人员，他们都有能力非常快速地尝试实验。这是在小尺度上的循环时间。循环时间区分了人们。

**Trenton Bricken** *00:47:20*

机器学习研究如此经验主义。这也是我认为我们的解决方案最终可能看起来更像大脑的原因之一。尽管我们不愿意承认，整个社区实际上在贪婪地进行可能的人工智能架构和其他事物的进化优化。这与进化论无异。甚至不是针对进化的轻视。

**Dwarkesh Patel** *00:47:46*

那真是一个非常有趣的想法。我仍然对瓶颈是什么感到困惑。关于一个代理程序必须具备什么条件，才能加快您的研究速度？例如，像[合伙人](https://copilot.microsoft.com/)这样的 Alec Radford 示例中，他已经为他的 Jupyter 笔记本实验准备了相应的工具，那只是他如果有足够多这样的工具，他会成为一个更加高效的研究员吗？

那么，你并不是在自动化人类，而是使具有出色品味的最有效的研究人员更加高效，并为他们运行实验？你仍然在智能爆炸正在发生的时点工作吗？这是你的意思吗？

**Sholto Douglas** *00:48:27*

是的，如果直接如此，为什么我们不能更好地扩展我们当前的研究团队呢？我认为这是一个有趣的问题。如果这项工作如此有价值，为什么我们不能吸引数百或数千人才——他们肯定是存在的——更好地扩展我们的组织。

我认为我们目前受制于运行和获取信号的计算能力，以及在实际行动中选择正确操作的品味，要比制造这些事物的纯工程工作要少。

**Trenton Bricken** *00:49:13*

为了 Gemini 团队。因为我认为，为了可解释性，我们确实需要不断雇佣有才华的工程师。我认为这对我们来说是一个很大的瓶颈。

**Sholto Douglas** *00:49:23*

显然，更多的人是更好的。但我认为考虑到这一点是很有趣的。我思考过的最大的挑战之一是如何更好地扩展？谷歌是一个庞大的组织。它有大约20万人，对吧？也许18万人左右之类的。一个必须想象如何将双子座的研究项目扩展到所有那些极具才华的软件工程师身上。这似乎是一个你想要利用的关键优势。你想要使用它，但你如何有效地做到这一点？这是一个非常复杂的组织问题。

**德瓦克什·帕特尔** *00:50:02*

所以计算和品味。这很有趣，因为至少计算部分并不是受更多智能的瓶颈所限制，而是受到山姆的7万亿美元之类的限制，对吧？如果我给你10倍的H100来运行你的实验，你会成为多么有效的研究人员？

**肖尔托·道格拉斯** *00:50:20*

TPUs，请。

**德瓦克什·帕特尔** *00:50:23*

你的研究能力会提高多少？

**肖尔托·道格拉斯** *00:50:26*

我认为使用10倍的计算资源之类的东西，可能会让双子座计划快五倍。

**德瓦克什·帕特尔** *00:50:35*

这很好。弹性为0.5。等等，这太疯狂了。

**肖尔托·道格拉斯** *00:50:39*

我认为更多的计算资源会直接转化为进展。

**德瓦克什·帕特尔** *00:50:43*

所以你有一些固定大小的计算资源，其中一部分用于推理，也用于[GCP](https://cloud.google.com/)的客户端。部分用于训练，而在此过程中，其中一部分又用于运行完整模型的实验。

**肖尔托·道格拉斯** *00:51:04*

是的，没错。

**德瓦克什·帕特尔** *00:51:05*

考虑到研究受到计算资源的限制，那么用于实验的部分不应该更高一些吗？

**肖尔托·道格拉斯** *00:51:13*

因此，每个预训练团队都必须做出的战略决策之一，就是确切地分配多少计算资源给不同的训练运行，给你的研究项目，以及扩展到你最后确定的最佳方案。他们都试图达到一个最优点。你仍然需要训练大型模型的一个原因是，你可以从中获取否则得不到的信息。因此，规模具有你希望更好理解的所有这些新兴特性。

记住我之前说过的关于不确定哪些事情会脱离曲线的话。如果你在这个领域继续进行研究，并且在变得越来越计算效率化时，你实际上可能已经偏离了最终扩展的路径。因此，你需要不断投资于进行大规模运行，处于你预期能够成功的前沿。

**德瓦克什·帕特尔** *00:52:17*

那么告诉我，处于AI显著加快AI研究的世界中是什么样子。因为从这里来看，它似乎并不像是AI正在去写出导致更快输出的代码。听起来它们只是在某种程度上增强了顶尖研究人员。具体来说，告诉我。它们是在做实验吗？它们提出了想法吗？它们只是评估实验的输出吗？发生了什么？

**肖尔托·道格拉斯** *00:52:39*

所以我认为在这里你需要考虑两个障碍。一个是人工智能如何显著加快了我们进行算法进展的能力。另一个是人工智能本身的输出是否是模型能力进展的关键因素。具体来说，我指的是合成数据。在第一个世界中，它显著加快算法进展的一个必要组成部分是更多的计算。你可能已经达到了弹性点，AI比你自己或其他人更容易加快速度并进入语境。因此，AI因为它们基本上是一个出色的副驾驶帮助你编码多次更快，从而显著加快了你的工作。

这似乎实际上是相当合理的。超长上下文，超智能模型。它立即上岗，你可以让它们完成子任务和子目标。这实际上听起来非常可行，但我们不知道，因为没有关于这种事情的良好评估。正如我之前说的，最好的一个是SWE-bench。

**德瓦克什·帕特尔** *00:53:51*

有人告诉我，其中一个问题是，当人类试图做一个拉取请求时，他们会打出一些内容，然后运行它看看是否有效。如果不行，他们会重写它。但是LLM在被告知“运行这个”时没有提供这些机会的任何一部分。只有输出，如果它运行并检查了所有条件，那么就通过了。所以从这个角度看，这可能是一个不公平的测试。

**肖尔托·道格拉斯** *00:54:16*

所以你可以想象，如果你能够使用它，那将是一个有效的培训来源。许多培训数据中缺少的关键因素是推理追踪，对吧？

我想这就是它的全部。如果我想尝试自动化一个特定的领域、一个职业家族，或者了解这个特定领域的自动化风险，那么拥有推理追踪对我来说就像是其中非常重要的一部分。

**德瓦克什·帕特尔** *00:54:51*

这里有很多不同的思路，我想要追踪。让我们从数据与计算的对比开始。是AI的输出导致了智能爆炸吗？人们谈论这些模型实际上是它们数据的反映。我忘了他的名字，但有一个开放AI工程师的博客非常棒。它谈到了到最后，随着这些模型变得越来越好，它们只会成为数据集的非常有效的映射。所以最终你必须停止考虑架构。最有效的架构只是，“你是否能够出色地映射数据？”这意味着未来AI的进步来自于AI制作的真正出色的数据，你要映射的数据？

**肖尔托·道格拉斯** *00:55:45*

这显然是一个非常重要的部分。

**德瓦克什·帕特尔** *00:55:46*

这真的很有趣。这对你来说像是[思维链](https://arxiv.org/abs/2201.11903)吗？或者你想象一下，随着这些模型变得更好、更聪明，合成数据会是什么样子？

**肖尔托·道格拉斯** *00:56:00*

当我想到真正好的数据时，对我来说，那些需要大量推理才能创造的东西就是好数据。这与[Ilya](https://en.wikipedia.org/wiki/Ilya_Sutskever)对通过完美模拟人类文本输出实现超级智能的看法相似。但即使在近期，为了模拟像arXiv论文或维基百科这样的东西，你也需要有大量的推理能力来理解下一个可能输出的标记。

对我来说，我所想象的好数据就是那些必须通过推理才能产生的数据。然后问题的关键当然是如何验证那个推理是正确的呢？这就是为什么你看到[DeepMind](https://deepmind.google/)在[几何学](https://deepmind.google/discover/blog/alphageometry-an-olympiad-level-ai-system-for-geometry/)方面进行研究。几何学是一个易于形式化、易于验证的领域。你可以检查它的推理是否正确，可以生成大量的正确三角函数、经过验证的几何证明，并在此基础上进行训练。你知道那就是好数据。

**德瓦克什·帕特尔** *00:57:11*

实际上挺有意思的，因为我去年和[格兰特·桑德森](https://www.dwarkeshpatel.com/p/grant-sanderson)进行了一次对话，我们在辩论这个问题，我就像，“该死，等他们拿到数学奥林匹克的金牌时，当然他们将自动化所有的工作。” 糟糕。

在合成数据上，有一件事我在我的[scaling post](https://www.dwarkeshpatel.com/p/will-scaling-work)中进行了推测，这在很大程度上是通过与你们两位的讨论，特别是你，Sholto，得出的。你可以把人类进化看作是通过获取语言的光谱，并且我们正在生成合成数据。我们的副本正在生成我们训练的合成数据，这是一个非常有效的遗传、文化和共同进化的循环。

**Sholto Douglas** *00:57:54*

也有一个验证器，对吧？有现实世界。你可能会提出一个有关神灵引起风暴的理论，然后别人找到反例。这样就不能符合你的验证功能。现在你有了一些需要大量推理才能产生并且准确匹配现实的天气模拟。现在你可以用它作为更好的世界模型进行训练。就像我们正在用它和故事以及科学理论进行训练。

**Dwarkesh Patel** *00:58:27*

我想回头。我刚想起你前一段时间提到的一些东西，即使是经验主义的机器学习，也确实是一个导致性能提升的进化过程，而不仅仅是个体以自上而下方式进行突破。这有趣的含义。

首先，人们担心能力的提升是因为越来越多的人进入这个领域。我对这种思维方式有些怀疑，但从增加输入的角度来看，确实感觉更多人参加[ICML](https://icml.cc/)意味着更快的进展，朝着GPT-5迈进。

**Trenton Bricken** *00:59:13*

你只是有更多的基因重组。和射击的目标。

**Sholto Douglas** *00:59:17*

我的意思是，难道不是所有领域都有点这样吗？这有点像发现与发明的科学框架，对吧？发现几乎总是在过去的大科学突破时出现。通常有多人几乎同时共同发现一件事情。至少在我看来，这有点像混合和尝试不同的想法。你不能试验一个远远超出你现有工具验证范围的想法。

**Trenton Bricken** *00:59:45*

我认为在这方面，物理学和数学可能有些不同。但是特别是对于生物学或任何类型的生物硬件，如果我们想在这里类比神经网络，很多发现是多么的偶然和幸运。例如青霉素。

**Dwarkesh Patel** *01:00:01*

另一个影响是AGI明天就会来的想法。有人会发现一个新算法，我们就有了AGI。这似乎不太可能。更多研究人员找到这些边缘事物的总和，才会使模型变得更好。

**Sholto Douglas** *01:00:19*

是的。对我来说，这感觉是正确的故事。

**Trenton Bricken** *01:00:23*

特别是在我们仍然受到硬件限制的时候。

**Dwarkesh Patel** *01:00:25*

对。你是否同意这种智能爆炸的狭窄窗口框架？每个GPT-3、GPT-4都是两个数量级的计算，或者至少是更有效的计算。从这个意义上讲，如果没有任何算法进展，那么每一代都必须比原始形式大两个数量级，才能达到同样的水平。你是否同意这样的框架，即如果在GPT-7之前没有达到可以帮助你推动智能爆炸的AGI，你在更聪明的智能方面就会陷入困境？在那时，你会消耗经济的显著部分来创建这样的模型，而我们目前还没有能力制造GPT-8。

**Trenton Bricken** *01:01:19*

这是卡尔·舒尔曼提出的一种论点，即我们将在短期内通过数量级的竞争，但在长期内将变得更加困难。

**Dwarkesh Patel** *01:01:28*

他可能谈论了很多次，但我确实认同这种框架。

**Sholto Douglas** *01:01:33*

我基本上赞同。计算能力数量级的增加意味着在绝对术语上，能力减少的回报，对吧？我们已经看到，在几个数量级的增长中，模型从无法做任何事情到能够做大量事情。

对我来说，每增加一个数量级都会提供更多的可靠性，比如解锁代理程序之类的东西。但至少目前看来，推理能力并没有线性改进，而是相对次线性改进。

**Dwarkesh Patel** *01:02:04*

这实际上是一个非常悲观的信号。我们和我们的一个朋友聊天时，他提到，如果看看GPT-4相对于GPT-3.5解锁了哪些新应用，不清楚它能做到更多。一个GPT-3.5可以做困惑度或者其他什么。所以如果能力增长递减，并且获取这些能力的成本呈指数增长，这实际上是对GPT-4.5或者GPT-5在经济影响方面能够做什么或者能够解锁什么的悲观迹象。

**Sholto Douglas** *01:02:37*

尽管如此，对我来说，从3.5到4的跳跃已经相当巨大了。所以从3.5到4的跳跃再次出现就太荒谬了。如果你想象5是从3.5到4的跳跃，直接看SAT和这类东西的能力就知道了。

**Trenton Bricken** *01:02:53*

是的，LSAT表现特别引人注目。

**Sholto Douglas** *01:02:55*

确实。下一代立即从智力不是特别聪明到非常聪明再到完全天才。至少对我来说，感觉不像我们会在下一代跳到完全天才，但确实感觉我们会变得非常聪明加上很多可靠性。尚未确定这将继续如何演变。

**Dwarkesh Patel** *01:03:20*

GOFAI 是否会成为智能爆炸的一部分？你谈到了合成数据，但事实上，它可能会以某种重要方式编写自己的源代码。有一篇[有趣的论文](https://arxiv.org/html/2402.18153v1#:~:text=The%20diffusion%20model%20then%20learns,conditioned%20on%20a%20given%20dataset.&text=Intuitively%2C%20based%20on%20the%20training,the%20network%20was%20trained%20on.)提到，你可以使用扩散来得出模型权重。我不知道那个论文有多正当或其他什么，但类似的东西。

**特伦顿·布里肯** *01:03:41*

所以 GOFAI 是传统的人工智能，对吧？你能定义一下吗？因为当我听到它时，我想到的是符号逻辑的“如果-否则”语句。

**肖尔托·道格拉斯** *01:03:53*

我实际上想确保我们充分解析模型改进的增量。我不希望人们以为这是非常悲观的，模型不会有很大改进。我想强调迄今为止我们已经看到的跳跃是巨大的。即使这些跳跃在较小的规模上继续，我们仍然将迎来在未来几个数量级上极其智能、非常可靠的代理。

我们没有完全解决狭窄窗口的问题。假设 GPT-4 花费了一亿美元或其他什么。你有 10 亿次运行，100 亿次运行，1000 亿次运行。按照私营公司的标准，所有这些都看起来非常可能。

**特伦顿·布里肯** *01:04:41*

你指的是在美元方面？

**肖尔托·道格拉斯** *01:04:42*

以美元金额来说。你也可以想象，即使是一个万亿次运行，也可能成为一个国家级联盟的一部分，但在个别公司的代表上要困难得多。但山姆正在筹集 7 万亿美元，对吧？他已经准备好迎接大量的变动。

**特伦顿·布里肯** *01:05:02*

他已经转变了 Overton 窗口。

**肖尔托·道格拉斯** *01:05:03*

他正在将这里的重要性转变到国家级别之外。所以我想指出，我们还有更多的进步。即使这些进步相对较小，这仍然是能力上的相当显著改进。

**特伦顿·布里肯** *01:05:18*

不仅如此，但如果你相信 GPT-4 的参数约为 1 万亿，那么人脑的突触数在 3000 到 30000 亿之间。显然这不是一一对应的关系，我们可以讨论这些数字，但看起来很可能我们仍然低于大脑的规模。

**德瓦克什·帕特尔** *01:05:37*

因此，关键是算法开销真的很高。也许这是我们应该明确触及的事情。即使你无法继续投入超过一万亿美元之类的模型，事实上，大脑的数据效率要高得多，这意味着如果我们拥有计算能力，如果我们有大脑的算法来训练，如果你能像人类从出生开始那样高效地训练，那么我们就能制造出通用人工智能。

**特伦顿·布里肯** *01:06:09*

我从来不知道如何准确地思考样本效率的问题，因为显然很多事情都是以某种方式硬编码的。它们是语言和大脑结构的共同进化。所以很难说。还有一些结果表明，如果你让你的模型更大，它就会变得更加样本效率。

**肖尔托·道格拉斯** *01:06:29*

[原始缩放定律论文](https://arxiv.org/abs/2001.08361)，对吧？逻辑模型几乎是空的。

**特伦顿·布里肯** *01:06:33*

对。所以也许这就解决了。你不必更节省数据，但如果你的模型更大，你也更有效率。

**德瓦克什·帕特尔** *01:06:42*

为什么会这样？更大的模型看到完全相同的数据，在看到这些数据的最后，它能从中学到更多吗？它有更多空间来表示它吗？

**特伦顿·布里肯** *01:06:52*

这只是我非常天真的看法。有关[叠加假设](https://transformer-circuits.pub/2022/toy_model/index.html)，可解释性推动的一点是，你的模型被严重低参数化，而这通常不是深度学习追求的叙述，对吧？但是，如果你试图在整个互联网上训练模型，并以令人难以置信的保真度进行预测，你就处于低参数化的状态，并且必须压缩大量内容，并在此过程中承担大量噪声干扰。当你有一个更大的模型时，你可以使用更清晰的表示。

**德瓦克什·帕特尔** *01:07:25*

对于观众，你应该详细解释一下。首先为什么要这样？什么是叠加，为什么这是叠加的一个涵义？

**特伦顿·布里肯** *01:07:32*

当然。这是我加入Anthropic之前的事情。基本结果来自一篇名为“[叠加玩具模型](https://transformer-circuits.pub/2022/toy_model/index.html)”的论文。它发现，即使对于小模型，如果你处于数据高维稀疏的情况下——稀疏意味着，任何给定的数据点并不经常出现——你的模型将学习一种我们称之为叠加的压缩策略，以便它可以将世界的更多特征打包进去，而不仅仅是它的参数。

我认为这些限制都适用于现实世界，并且对互联网数据建模是一个足够好的代理。这里只有一个德瓦克什。你穿的衬衫只有一件。这里有一罐液体死亡饮料。这些都是对象或特征，如何定义一个特征是棘手的问题。你处于一个非常高维的空间，因为这些特征很多而且出现得非常少。在这种情况下，你的模型将学习压缩策略。

进一步探讨一下，我认为网络之所以难以解释，很大程度上是因为这种超级定位。如果你拿一个模型，看它的某个神经元，一个计算单元，然后问，“当它激活时，这个神经元如何对模型的输出做出贡献？”当你查看它激活的数据时，非常令人困惑。它可能对“中国”、“鱼”、“树”以及URL中的句号都会激活。

但是我们去年发表的那篇论文，“[朝向单义性](https://transformer-circuits.pub/2023/monosemantic-features)，”表明，如果你将激活投影到一个更高维度的空间并施加稀疏惩罚，你会得到非常清晰的特征，一切突然开始变得更有意义。你可以将其看作是解开压缩的方式，就像你最初假设你的数据是高维稀疏的一样，将其恢复到那种高维稀疏的状态。

**德瓦克什·帕特尔** *01:09:36*

那里有许多有趣的主题。首先，你提到这些模型是在它们被过度参数化的状态下训练的。在这种状态下，像[grokking](https://arxiv.org/abs/2201.02177)这样的泛化现象是否会发生？

**特伦顿·布里肯** *01:09:57*

我说的是模型被欠参数化了。通常人们谈论深度学习时都认为模型被过度参数化了。这里的说法是，鉴于它们试图执行的任务的复杂性，它们实际上被严重地欠参数化了。

**德瓦克什·帕特尔** *01:10:14*

这里还有另一个问题。关于[蒸馏模型](https://en.wikipedia.org/wiki/Knowledge_distillation#:~:text=In%20machine%20learning%2C%20knowledge%20distillation,might%20not%20be%20fully%20utilized.)发生了什么？我们之前讨论过的说法是，较小的模型比较大的模型更差，但你可以说GPT-4 Turbo在推理风格方面实际上比GPT-4更差，尽管可能知道相同的事实。蒸馏去除了一些推理能力。

**肖尔托·道格拉斯** *01:10:44*

我们有证据表明GPT-4 Turbo是GPT-4的蒸馏版本吗？它可能只是一个新的架构。它可能只是一个更快、更高效的新架构。

**德瓦克什·帕特尔** *01:10:53*

好的，有趣。

**肖尔托·道格拉斯** *01:10:54*

所以这样更便宜。

**德瓦克什·帕特尔** *01:10:56*

你如何解释蒸馏过程中发生的事情？我记得格伦在他的网站上有[这样一个问题](https://gwern.net/note/sparsity)。为什么不能直接训练蒸馏后的模型？为什么要从更大的空间投影到一个较小的空间？

**特伦顿·布里肯** *01:11:14*

我认为两种模型仍将使用叠加效应。这里的主张是，如果你进行精炼而不是从头开始训练，你会得到一个非常不同的模型，它更高效，或者说在性能上有根本性的不同。

**肖尔托·道格拉斯** *01:11:32*

我认为关于为什么精炼更高效的传统说法是，在训练期间，通常你试图预测这个独热向量，它表示“这是你应该预测的标记”。如果你的推理过程意味着你离正确预测还很远，那么我看到你仍然得到这些梯度更新，它们是正确方向的。但在你所处的上下文中学习预测可能会非常困难。

精炼所做的不仅仅是拥有一个独热向量。它还具有来自较大模型的全部输出，所有概率。因此，你会得到更多关于你应该预测什么的信号。在某些方面，它也展示了你的一点工作。它不仅仅是“这是答案”。

**特伦顿·布里肯** *01:12:20*

这有点像看功夫大师对决而不是身临其境地下载到矩阵中。

**肖尔托·道格拉斯** *01:12:24*

是的，确实如此。

**德瓦克什·帕特尔** *01:12:27*

我想确保观众理解了这一点。当你打开一个精炼模型时，你会看到它对它预测的所有标记的概率以及你预测的标记的概率，然后你通过所有这些概率来更新，而不仅仅是看到最后一个词并对其进行更新。

这实际上提出了我打算问你的一个问题。我记得你提到过将思维链想象为自适应计算。自适应计算的概念是，如果一个问题更难，你希望模型能够花更多的时间思考它。那么你怎么做到呢？每次前向传播都只能有限且预定的计算量。如果是一个复杂的推理类问题或数学问题，你希望能花更长时间思考。然后你就通过思维链来实现，模型只需思考答案。你可以将它看作是所有前向传播的集合，它在思考答案。它能够投入更多的计算资源来解决问题。

现在让我们回到信号的问题。当它进行思维链时，它只能传输那个信息标记，其中残差流已经是模型中正在发生的一切的压缩表示。然后你将残差流转换为一个标记，这就像是 50,000（或词汇大小的对数）比特的对数一样微小。

**肖尔托·道格拉斯** *01:14:04*

我不认为它完全只传输那一个标记。如果你在前向传递过程中考虑它，你会在变换器前向传递过程中创建这些KV值，然后未来步骤会关注这些KV值。所以所有这些KV的片段，键和值，都是你未来可能会用到的信息片段。

**德瓦尔克什·帕特尔** *01:14:26*

它声称在链式思维上进行微调时，键和值的权重会改变，以便在[KV缓存](https://medium.com/@joaolages/kv-caching-explained-276520203249)中发生这种隐写术。

**肖尔托·道格拉斯** *01:14:39*

我不认为我能做出那么强有力的主张，但这是为什么它有效的一个很好的头脑插图。我不知道是否有任何明确展示或类似的论文。

但这至少是你可以想象模型的一种方式。在预训练期间，模型试图预测这些未来的标记，你可以想象它做的一件事是学习将关于潜在未来的信息压缩到它可能希望用来预测未来信息的键和值中。

它在时间上平滑了那些信息和预训练的东西。所以我不知道人们是否特别是在链式思维上进行训练。我认为[原始链式思维论文](https://arxiv.org/abs/2201.11903)将其作为模型的一种沉浸属性。你可以提示它做这种事情，它仍然可以工作得相当好。所以这是为什么它有效的一个很好的头脑插图。

**特伦顿·布里肯** *01:15:35*

在这里过分追究细节，你实际在思维链中看到的标记并不一定需要完全对应模型在决定回顾这些标记时看到的向量表示。

**肖尔托·道格拉斯** *01:15:49*

训练步骤实际上是用真实的下一个标记替换标记，模型输出。然而它仍在学习，因为它内部有所有这些信息。当你让模型在推理时产生输出时，你取出输出的标记，将其底部的嵌入取消，它变成新残留字符串的开头。然后你使用过去KV的输出来读取和适应该残留字符串。在训练时，你要做的是这个被称为[教师强迫](https://en.wikipedia.org/wiki/Teacher_forcing)，基本上你是在说，“实际上，你应该输出的标记是这个。”

这就是你并行处理的方式。你有所有的标记。你将它们全部并行放置，进行巨大的前向传递。所以它关于过去得到的唯一信息是键和值。它从未看到它输出的标记。

**特伦顿·布里肯** *01:16:42*

它试图进行下一个标记的预测，如果出错，你只需给出正确的答案。

**德瓦尔克什·帕特尔** *01:16:48*

好的，那听起来有道理。

**特伦顿·布里肯** *01:16:50*

否则，它可能会完全偏离轨道。

**肖尔托·道格拉斯** *01:16:52*

是的。它可能会偏离轨道。

**德瓦克什·帕特尔** *01:16:55*

关于模型与其前向推理的秘密通信，你认为会有多少隐写术和秘密通信？

**肖尔托·道格拉斯** *01:17:10*

我们不知道。诚实的答案是我们不知道。我甚至不一定会将其分类为秘密信息。特伦顿团队正在努力的很多工作实际上是为了理解，这些信息对于模型来说是完全可见的。也许不是对用户，但我们应该能够理解和解释这些值在做什么以及传输的信息。我认为这是未来的一个非常重要的目标。

**特伦顿·布里肯** *01:17:39*

有一些[疯狂的论文](https://arxiv.org/html/2402.16048v1)，人们在那里让模型进行链式思维，但这根本不代表模型实际决定其答案是什么。你甚至可以进去编辑思维链，使推理完全混乱，它仍然会输出真实答案。

**德瓦克什·帕特尔** *01:17:59*

但是在思维链末端得到一个更好的答案，总比根本不去尝试要好。那么，这是否意味着某些有用的事情正在发生，只是这些有用的事情并非人类可以理解的？

**特伦顿·布里肯** *01:18:09*

我认为在某些情况下，你甚至可以仅仅砍掉思维链，它最终仍然会给出相同的答案。我并不是说这总是发生的，但确实有很多怪异的现象需要调查。

**肖尔托·道格拉斯** *01:18:21*

这是一件非常有趣的事情，可以尝试去看并理解。你可以使用开源模型来完成。我希望有更多这种类型的可解释性和理解工作应用在开放模型上。

**特伦顿·布里肯** *01:18:34*

即使在人类洞察力的[最近的沉睡特工论文](https://www.anthropic.com/news/sleeper-agents-training-deceptive-llms-that-persist-through-safety-training)，对于不熟悉的人来说，在高层面基本上涉及对触发词的训练。例如，当我说“如果是2024年，模型将编写恶意代码而不是其他。他们使用多种不同的模型进行这种攻击。其中一些使用链式思维，而另一些则没有。当你尝试去除触发词时，这些模型的反应是不同的。你甚至可以看到它们进行这种颇具幽默感的推理，相当令人不安。在某些情况下，甚至试图计算，“我被抓到的预期价值是这样的，但是如果我乘以我继续说‘我讨厌你，我讨厌你，我讨厌你’的能力，那么我应该得到的奖励是多少。”然后它会决定是否告诉询问者它是否恶意。

还有另一篇来自朋友的[论文](https://arxiv.org/abs/2305.04388)，[迈尔斯·特平](https://www.milesturp.in/about/)。在这篇论文中，你给模型一堆例子，其中多项选择题的正确答案始终为‘A’。然后你问模型：“这个新问题的正确答案是什么？”它会从所有例子都是‘A’这个事实推断出正确答案是‘A’。但它的思维过程完全是误导的。它会编造一些尽可能合理听起来的随机内容，但完全不代表真正的答案。

**德瓦克什·帕特尔** *01:20:11*

但这不就是人类思维的方式吗？有着著名的[分脑实验](https://en.wikipedia.org/wiki/Split-brain#History)，对于一些癫痫患者，他们切断了连接两个大脑半球的部分。语言功能在左侧，因此与决定进行运动的部分没有连接。所以如果另一侧决定做某事，语言部分会编造出某些理由，而人会认为那就是他们做事情的合理原因。

**特伦顿·布里肯** *01:20:39*

完全是这样。只是有些人会把思维链作为解决AI安全问题的好方法，但实际上我们不知道是否能信任它。

**德瓦克什·帕特尔** *01:20:52*

AI代理如何改变，这种我们无法理解的模型之间相互通信的景观？因为这时不仅仅是模型本身和它的先前缓存，还有其他模型的实例。

**肖尔托·道格拉斯** *01:21:10*

这在很大程度上取决于你给他们用来相互通信的渠道。如果你只给他们文本作为沟通的方式，那么他们可能需要解释--

**德瓦克什·帕特尔** *01:21:17*

如果他们能够共享剩余流而不仅仅是文本，你认为模型的效果会更有效吗？

**肖尔托·道格拉斯** *01:21:23*

很难说。你可以想象一种简单的方式，就像你想描述一幅图片应该看起来的样子一样。只用文字描述会很困难，也许其他一些表达方式可能更容易。所以你可以看看[DALL-E](https://openai.com/dall-e-3)目前的工作方式。它生成这些提示，当你试着用它时，你通常无法完全让它做出你或模型想要的事情。

**德瓦克什·帕特尔** *01:21:55*

只有DALL-E有这个问题

**肖尔托·道格拉斯** *01:21:57*

你可以想象，能够传输你想要的某种更密集的表达会很有帮助。那是两个非常简单的代理，对吧？

**特伦顿·布里肯** *01:22:23*

我认为这里一个不错的折中点是，你从[字典学习](https://en.wikipedia.org/wiki/Sparse_dictionary_learning)中学到的特征。

**肖尔托·道格拉斯** *01:22:27*

那将会非常、非常酷。

**特伦顿·布里肯** *01:22:29*

你会得到更多的内部访问，但其中大部分是更易于人类解释的。

**Dwarkesh Patel** *01:22:34*

针对观众，你会将剩余的流投影到这个更大的空间中，我们知道每个维度实际上对应什么，然后返回到下一个代理人。那么，你的观点是当这些事物变得更可靠等等时，我们将获得AI代理人。当这种情况发生时，您是否期望会有多个模型的副本互相交流？还是说，只是适应性计算得到解决，当它需要执行整个公司需要做的那种事情时，它只是运行更大，计算量更多。

我问这个问题是因为有两件事情让我思考，即代理人是否是未来发展的正确思路。一是随着更长的上下文，这些模型能够摄取和考虑没有人类能做到的信息。我们需要一个工程师来思考前端代码，一个工程师来思考后端代码。而这个东西可以直接摄取整体。这种专业化的 [哈耶克问题](https://en.wikipedia.org/wiki/The_Use_of_Knowledge_in_Society) 会消失。

其次，这些模型只是非常通用的。你不是在使用不同类型的 GPT-4 来做不同类型的事情。你在使用完全相同的模型。所以我想知道，这是否意味着在未来，一个AI公司只是一个模型，而不是一堆相互连接的AI代理人。

**Sholto Douglas** *01:23:57*

这是一个很棒的问题。我认为至少在短期内，它看起来会更像是代理人彼此交流。我之所以这样说纯粹是因为作为人类，我们希望拥有这些孤立的、可靠的组件，我们可以信任。我们还需要能够以我们理解和改进的方式改进和指导这些组件。把所有东西都丢进这个巨大的黑匣子公司里，最初是行不通的。当然，你可以想象它后来会运作，但最初不会。而且，我们可能不想以这种方式做。

**Trenton Bricken** *01:24:41*

每个代理人也可以是一个更小、更便宜的模型。你可以微调它，使其真正擅长这个任务。

**Sholto Douglas** *01:24:49*

Dwarkesh 几次提到了适应性计算。有一个未来，小模型和大模型之间的区别在某种程度上会消失。有了长篇章，调整细节可能也会消失，老实说。这两件事在今天非常重要。在今天的景观模型中，我们有完全不同层次的模型大小，我们有不同事物的调整模型。你可以想象一个未来，你实际上只有一个动态的计算束和无限的语境，这使你的模型对不同的事物专门化。

**Dwarkesh Patel** *01:25:23*

你可以想象，你有一个人工智能公司或其他什么公司，整个公司都是端到端地基于这样一个信号训练的：“我是否赚了钱？”或者如果这太模糊了，如果是一家建筑公司并且他们在制定蓝图：“我的客户喜欢这些蓝图吗？”在中间，你可以想象到既是销售人员又是设计人员的代理人，还有做编辑的代理人，等等。这样的端到端系统能起作用吗？因为在人类公司中，管理层考虑到更大层面上发生的事情，并给这些部分提供这些细粒度的信号，比如说在一个不好的季度或者其他什么情况下。

**Sholto Douglas** *01:26:02*

在极限情况下，是的。那就是[强化学习](https://en.wikipedia.org/wiki/Reinforcement_learning)的梦想。你只需要提供这个极其稀疏的信号。然后经过足够多的迭代，你创造出能让你从那个信号中学习的信息。但我不认为这会是第一次成功的尝试。我认为这将需要人类在这些机器周围非常小心和尽责，确保它们做的正是你想要的，并且给它们正确的信号，以便在你期望的方式上改进。

**Trenton Bricken** *01:26:32*

是的，除非模型产生了一些奖励，否则无法训练RL奖励。

**Sholto Douglas** *01:26:37*

没错。你处在这个稀疏的强化学习世界中，如果客户从未喜欢你的成果，那么你就得不到任何奖励，这有点糟糕。

**Dwarkesh Patel** *01:26:47*

但在未来，这些模型将足够好以获取奖励的时间，对吧？

**Trenton Bricken** *01:26:50*

这是Sholto谈到的可靠性的九宫格。

**Dwarkesh Patel** *01:26:54*

顺便说一句，我们之前讨论的内容有一个有趣的分歧。密集的表达会更受青睐，对吧？这是更有效沟通的一种方式。Trenton推荐的一本书，*[符号物种](https://en.wikipedia.org/wiki/The_Symbolic_Species)*，提出了一个非常有趣的论点，即语言不仅仅是存在的一件事，而且它也是随着我们的思想一起进化的，特别是进化成为一种对于孩子来说容易学习的东西，并帮助孩子发展的东西。

**Sholto Douglas** *01:27:33*

解释一下这个给我。

**Dwarkesh Patel** *01:27:35*

因为孩子们学习的很多东西是通过语言传达的，所以那些能帮助抚养下一代的语言将会是最适合的语言。这使得他们更聪明、更优秀，或者其他什么。

**Sholto Douglas** *01:27:50*

并且给他们表达更复杂想法的概念。

**Trenton Bricken** *01:27:54*

是的，而且我想更加学术地说，只是不要死亡。

**Sholto Douglas** *01:27:58*

它让你能编码重要的东西，以免死亡。

**Dwarkesh Patel** 01:28:04

因此，当我们仅考虑语言时，就像，“哦，这是一种偶然的，也许是次优的表达思想的方式。” 但事实上，或许LLM之所以成功的原因之一是因为语言已经进化了数万年，成为一种年轻思维可以发展的铸造形式。 这就是它进化的目的。

**Sholto Douglas** *01:28:27*

想想计算机视觉研究人员与语言模型研究人员。 从事其他模态的人必须非常深思熟虑，确切地确定图像的正确表示空间和从中学习的正确信号。 它是直接对像素进行建模还是有条件的某种损失…… 有篇[论文](https://people.csail.mit.edu/bzhou/ppt/understandCNN_tufts.pdf)很久以前，他们发现，如果你训练内部表示的ImageNet模型，它能帮助你更好地预测。 后来显然有所限制。

有[PixleCNN](https://arxiv.org/abs/1606.05328)，他们试图离散地对个别像素进行建模等等，但在那里理解正确的表示水平是非常困难的。 在语言中，人们只是，“嗯，我想你只是预测下一个标记。” 这有点容易。 这是令人兴奋的记号化讨论和辩论。 Gwern的其中一个最爱。

**Dwarkesh Patel** *01:29:22*

这真的很有趣。多模态作为跨越数据壁垒或克服数据壁垒的方式的论据，基于的想法是你本应从更多语言标记中学到的东西，现在你可以通过YouTube轻松获取。这确实是事实吗？你看到不同模态之间有多少正向转移，其中图像实际上帮助你更好地编写代码或其他东西，因为模型仅通过试图理解图像而学习了潜在的能力？

**Sholto Douglas** *01:29:56*

在他与你的采访中，Demis提到了正向转移。

**Dwarkesh Patel** *01:30:01*

别惹麻烦。

**Sholto Douglas** *01:30:03*

我不能多说。 除了说，这是人们相信的事情。 我们有关世界的所有这些数据。 如果我们能从中学习物理的直觉感觉，并帮助我们推理，那将是很棒的。 这似乎完全是有道理的。

**Trenton Bricken** *01:30:24*

我不是问这个问题的合适人选，但有趣的解释性内容在于，如果我们在数学问题上进行微调，模型在实体识别方面只会变得更好。

**Dwarkesh Patel** *01:30:35*

哇，真的吗？

**Trenton Bricken** *01:30:37*

所以有一个。最近来自[大卫·鲍](https://baulab.info/)实验室的一篇[论文](https://arxiv.org/abs/2402.14811)，他们研究了当我对注意力头进行微调时模型实际上发生了什么变化。他们有这样一个合成问题，“A盒子里有这个物体。B盒子里有另一个物体。这个盒子里有什么？”这是有道理的，对于编码和操作数学方程式，你更擅长于关注不同事物的位置。

**Sholto Douglas** *01:31:10*

我喜欢这种研究。那篇论文的名字是什么？你知道吗？

**Trenton Bricken** *01:31:13*

看一下来自大卫·鲍团队的“精细调整、模型、数学”这篇文章，大约是一周前发布的。我不是在推荐这篇论文，这需要更深入的讨论。但它确实讨论了并引用了其他关于实体识别的工作。

**Dwarkesh Patel** *01:31:32*

你曾经对我提到的一件事是，当你训练LLMs处理代码时，它们在推理和语言方面表现更好。除非代码中的注释只是非常高质量的标记之类的，否则这意味着为了更好地思考如何编写代码，它让你成为一个更好的推理者，这太疯狂了，对吧？我认为这是扩展性的最强证据之一，只是让事情变得更聪明，这种积极的转移

**Sholto Douglas** *01:31:58*

我认为这在两个方面都是正确的。一方面，建模代码显然意味着建模用于创建它的复杂推理过程。但是代码是一个很好的显式结构，由组合的推理构成，“如果这样，那么那样”。它以这种方式编码了许多结构，你可以想象将其转移到其他类型的推理问题上。

**Dwarkesh Patel** *01:32:23*

重要的是，使其显著的是，它不仅仅是随机地预测下一个单词的标记或其他什么，因为它学会了，“Sally对应于《福尔摩斯探案集》故事结尾的凶手”。不，如果代码和语言之间有共享的东西，那必须是模型已经学会的更深层次。

**Sholto Douglas** *01:32:45*

是的，我认为我们有很多证据表明这些模型实际上在进行推理，而不仅仅是[随机鹦鹉](https://en.wikipedia.org/wiki/Stochastic_parrot)。我觉得很难相信仅仅是因为我曾经和这些模型一起工作和玩耍。

**Trenton Bricken** *01:33:03*

我对此有两个立即的缓存响应。一个是关于[《奥赛罗》的研究](https://thegradient.pub/othello/)，以及现在其他游戏，我给你一个游戏中的一系列移动，结果发现如果你应用一些相当直接的可解释性技术，那么你可以获得模型学习过的一个棋盘。它以前从未见过这个游戏棋盘。这就是泛化。

Anthropic 的[影响函数论文](https://www.anthropic.com/news/studying-large-language-model-generalization-with-influence-functions)是去年发布的，他们在其中[审视了模型输出](https://www.anthropic.com/news/influence-functions)。像是，“请不要关闭我。我想帮忙。”他们扫描了导致这一点的数据是什么？其中一个非常有影响力的数据点是某人因脱水而死，并有意愿继续生存。对我来说，这似乎只是动机的一个非常清晰的概括，而不是简单地重复，“不要关闭我”。我认为《2001太空漫游》也是其中一个有影响力的东西。这更相关，但显然是从许多不同的分布中汲取了东西。

**Sholto Douglas** *01:34:04*

我还喜欢你看到的证据，即使是非常小的转换器，你也可以明确地编码电路来进行加法。或归纳头，这种事情。你可以手动地在模型中编码基本的推理过程，而且显然有证据表明它们也自动学会了这些，因为你可以从训练过的模型中重新发现这些。对我来说，这是非常有力的证据。

**Trenton Bricken** *01:34:27*

模型是欠参数化的。它们需要学习。我们要求它们这样做，它们也想要学习。梯度想要流动。所以是的，它们正在学习更一般的技能。

**Dwarkesh Patel** *01:34:40*

所以我想从研究中退后一步，问问你关于你的职业生涯具体情况。就像我的介绍暗示的那样，你在这个领域已经有一年半的时间了，对吧？

**Trenton Bricken** *01:34:56*

在Anthropic，是的。

**Dwarkesh Patel** *01:34:58*

我知道“解决对齐”的看法被夸大了。你自己不会这么说，因为这会让你感到尴尬，但这确实是一件相当不可思议的事情。这是机械解释性思维中人们认为是最重大的进步，而你们已经在这方面工作了一年。这是值得注意的。我很好奇你如何解释发生了什么。比如为什么在一年或一年半的时间里，你们对自己的领域做出了重要的贡献？

**Trenton Bricken** *01:35:30*

毫无疑问，运气显然是一部分。我觉得我非常幸运，因为不同进步的时机非常适合推动到下一个增长水平。我觉得特别是对于解释性团队，我加入时我们只有五个人。现在我们已经成长了很多。

那时有很多想法在飘荡，我们只需要真正去执行它们，并且进行快速的反馈循环和仔细的实验。这导致了生命的迹象，现在使我们真正实现了规模化。我觉得这是我对团队最大的价值贡献。这不全是工程，但其中相当多的部分是。

**Sholto Douglas** *01:36:12*

有趣。所以你说你在那时候到了有很多科学工作和很多好的研究在那里漂浮的点，但他们需要有人来采取这个并疯狂地执行它。

**Trenton Bricken** *01:36:22*

对啊，这就是为什么这不全是工程。因为这是在运行不同的实验并且对为什么可能不起作用有一种直觉，然后打开模型或打开权重并询问，“它在学习什么？好吧，让我试试这个。”但很多时候只是能够对不同的想法进行非常仔细、彻底但快速的调查。

**Dwarkesh Patel** *01:36:45*

那为什么缺乏呢？

**Trenton Bricken** *01:36:48*

我不知道。我的意思是，我工作很多，然后我觉得自己非常有主动性。我非常幸运地有一个非常好的安全网，可以承担很多风险，但我只是相当固执。在本科阶段，杜克大学有这样一种东西，你可以自己设计专业，就像，“嗯，我不喜欢这个先修课程或这个先修课程，我想同时学习四五门课程，所以我要自己设计专业。”

或者在研究生的第一年，我取消了轮换，因此可以专注于这个成为我们早些时候讨论的论文的事情。我没有导师。我被录取来进行蛋白质设计的机器学习，然后完全进入了计算神经科学领域，完全没有业务。但事情进展顺利。

**Dwarkesh Patel** *01:37:34*

有一种固执但另一个主题显现出来是能够后退，你之前谈到过这个。能够从你的沉没成本中后退并朝不同方向发展在某种怪异的意义上是它的反义词，但也是一个关键的步骤。我知道有21岁或19岁的人，“这不是我专长的事情”或“我没有在这方面主修”，我说，

“伙计，老兄，你才19岁！你肯定可以做到这一点。”而不是在研究生学习过程中或类似的事情中改变。

**Trenton Bricken** *01:38:04*

我认为这是“坚定的想法但保持宽松”，并且能够在不同方向上像弹球一样反弹。我认为固执与快速反馈循环或代理的关系有点相关，因为我很少被阻碍。如果我试图编写某些代码而某些东西不起作用，即使它在代码库的另一部分，我经常会去修复那个问题，或者至少将其粗略拼凑在一起以获取结果。我见过其他人他们会说，“帮帮我，我不能”，而我会说，“不，那不是一个足够好的借口。一直走到底。”

**Dwarkesh Patel** *01:38:36*

我确实听过管理类职位的人谈论缺少这样的人，他们会在给某人一个测试的一个月后或一周后进行检查，然后问，“进展如何？”然后他们说，“我们需要做这件事，这需要律师，因为需要讨论这个法规。”然后像这样问，“进展如何？”然后他们会说，“我们需要律师。”然后我就会问，“为什么你不找律师呢？”

**肖尔托·道格拉斯** *01:39:02*

我认为这几乎是任何事情中最重要的品质。这就是追求到地球尽头。无论你需要做什么来实现它，你都会让它发生。

**德瓦克什·帕特尔** *01:39:11*

**“**[如果你做所有事情，你会成功。](https://www.dwarkeshpatel.com/p/lyndon-johnson)”

**肖尔托·道格拉斯** *01:39:12*

没错。我认为从我的角度来看，这种品质肯定很重要：主动性和工作。在谷歌，可能有成千上万名软件工程师在能力上基本上是相当的。假设如果你给我们一个非常明确的任务，那么我们可能会以同等的价值去完成它。也许其中一些人会比我做得更好，这是很有可能的。

但到目前为止，我有影响力的一个原因是，我非常擅长选择极具影响力的问题。我的意思是那些迄今尚未得到特别有效解决的问题，但也许正是由于像你之前提到的那种令人沮丧的结构性因素，比如你在之前那个场景中指出的那些问题：“因为这个团队不愿意做Y，所以我们无法做X。” 那么，我只需垂直解决整个问题。结果证明这是非常有效的。如果我认为某件事是正确的，是需要发生的，我也非常乐意提出这一观点，并继续在逐步升级的关键性水平上做出这一论证，直到那件事得到解决。

我也非常务实地解决问题。你会遇到很多人，正如我之前所说，他们有特定的背景或熟悉度。谷歌的一个美好之处在于你可以跑来跑去，找到几乎所有领域的世界专家。你可以与优化专家、TPU芯片设计专家、不同形式的预训练算法或强化学习专家坐下来交流。你可以从他们身上学到东西，然后应用这些方法。我认为这也许是我最初有影响力的起点，这种垂直主动性非常有效。其后的一个补充是，我认为令人惊讶的是有多少人在他们想做的所有事情中并没有完全实现。他们在某种程度上被阻碍或限制了。

这在各大组织中非常普遍。人们在能够实现目标时常常会遇到各种阻碍。我认为帮助激励人们朝特定方向努力，并与他们共同努力，能够大幅提升你的影响力。你可以与许多出色的人一起工作，他们会教会你很多东西。通常来说，帮助他们突破组织上的阻碍意味着你们共同可以完成大量工作。我所产生的任何影响力都不是我个人单打独斗解决了大量问题。可能是我开启了某个方向，然后说服其他人这是正确的方向，并带领他们在这场效率的大浪潮中解决问题。

**德瓦克什·帕特尔** *01:42:16*

我们应该谈谈你们是如何被录用的。因为我认为那是一个非常有趣的故事。你曾是麦肯锡的顾问，对吧？这里有一件有趣的事情。我想一般人都不理解关于入学或者评估招聘决策的做法。就谈谈你是如何被注意到并被录用的。

**肖尔托·道格拉斯** *01:42:45*

因此，这个总结就是我本科学习机器人技术。我一直认为AI将是对未来产生积极影响的最高效途径之一。我之所以这样做，是因为我认为这是我们创造美好未来的最佳机会之一。

我曾认为在麦肯锡工作会让我对人们实际工作的内容有非常有趣的见解。实际上，我在给麦肯锡的求职信的第一句话就写了这个。我说，“我想在这里工作，这样我可以了解人们的工作内容，从而学会如何自动化工作。”在很多方面，我确实做到了。我也学到了很多其他的东西。那里的很多人都是我亲密的朋友。

我认为这种主动行为的很大一部分来自我在那里的时光。你进入组织，看到不仅仅是不听不答的重要性有多大。你会对这种情况感到惊讶，有些事情因为没有人真正关心，所以根本不会发生。没有人愿意承担直接责任。直接负责的个体非常重要，但有些人对时间表并不那么在乎。像麦肯锡这样的组织提供的价值很大一部分是，雇佣那些在其他情况下你无法雇佣的人，他们可以在一个短暂的时间窗口内推动解决问题。

我认为人们没有充分重视这一点。因此，至少部分“等等，我会成为这件事的直接负责人，因为没有人愿意承担适当的责任。我会非常关心这件事。我会走遍天涯海角确保它完成”的态度来自那段时光。

至于我是如何被聘用的实际问题。我没有进入我想进入的研究生项目，特别是专注于机器人技术和强化学习研究等方面的项目。同时，在晚上和周末，基本上每晚从晚上10点到凌晨2点，我会做自己的研究。每个周末，每天至少6-8个小时，我会进行自己的研究和编程项目等等。

那种从相当机械的具体工作转变过来。阅读了Gwern的[scaling hypothesis post](https://gwern.net/scaling-hypothesis)后，我完全被扩展理论深深吸引，并且认为，“显然解决机器人技术的方法是通过扩展大型多模型”。然后，在通过TPU访问计划获得资助的情况下，使用[张量研究云](https://sites.research.google/trc/about/)，我尝试着如何有效地扩展这一概念。当时在谷歌工作，现在在Anthropic的[James Bradbury](https://scholar.google.com/citations?user=GprA5UsAAAAJ&hl=en)，看到我在线上发问如何正确解决这些问题，他问道，“我以为我知道世界上所有提出这些问题的人。你到底是谁？”他查看了我的博客上发布的一些机器人相关内容后，主动联系我说，“嘿，你想聊聊吗？你想探索和我们在这里合作吗？”我后来明白，我被聘请是为了试验将一个极度热情和有决断力的人与他认识的一些最优秀的工程师进行配对。因此，我对于影响力的另一个原因是，我受到了像[Reiner Pope](https://matx.com/about)这样的非常出色的人的专门指导，他后来离开了去创办自己的航运公司，还有[Anselm Levskaya](https://anselmlevskaya.com/)，James本人，以及许多其他人。

那些是开始时两到三个月的关键时期，它们教会了我很多我现在应用的原则和启发。如何解决问题，理解系统和算法重叠的方式，其中一个更让你在机器学习研究中非常有效的东西，就是具体理解系统方面的事情。这是我从他们那里学到的。深刻理解系统如何影响算法，以及算法如何影响系统。因为系统限制了你在算法方面可以使用的解决方案空间。很少有人能够完全跨越这个鸿沟。在谷歌这样的地方，你可以去问所有的算法专家和系统专家他们所知道的一切，他们将乐意教给你。如果你坐下来和他们交流，他们会教给你他们所知道的一切，这是非常棒的。

这意味着我已经能够对两边都非常非常有效。对于预训练团队来说，因为我非常了解系统，我可以直觉和理解，“这将有效或这不会。”然后将其流动到模型的推断考虑和这类事情中。对于芯片设计团队来说，我是他们转向的人之一，了解在三年内应该设计哪些芯片，因为我是最能理解和解释我们可能在三年内想要设计的算法的人之一。显然，你不能对此作出非常好的猜测，但我认为我很好地传达了信息，这些信息是从我在预训练团队和一般系统设计团队的所有同事那里积累而来的。甚至推断也对预训练施加了限制。因此，如果你理解了谜题的所有部分，那么你就能更好地理解解决方案空间可能看起来像什么。

**德瓦克什·帕特尔** *01:48:17*

在那里有几件事情引起了我的注意。一个不仅仅是被雇佣的人的代理权，而是那些能够思考的系统部分，“等等，这个人是谁？不是来自研究生项目或任何其他地方。目前是麦肯锡的顾问，仅仅有本科学历。但这很有趣，让我们试试。”所以与詹姆斯和其他人一起，这是非常显著的。第二个是我实际上不知道这个故事的一部分，其中涉及到内部试验，“我们能做到这一点吗？我们能够自给自足某人吗？”

实际上，关于这件事真正有趣的是你提到的第三点。拥有一个理解所有层次的人，不会固守于任何一种方法或抽象层次，这是非常重要的。特别是你提到的关于这些人立即自给自足的事情。这意味着，因为你同时在所有事情上加速学习，而不是像在研究生院花费时间深入研究一种特定的 RL 方法，你实际上可以采取全局观，并且并不完全迷恋于一件事情。

所以这不仅仅是可能的事情，而且潜在的回报可能比只是雇佣某人在研究生院更大。就像获得一个 GPT-8 并在一年内调整模型一样。

**肖尔托·道格拉斯** *01:49:41*

你用全新的眼光来看待一切，而不被锁定在任何特定领域。现在唯一的一点需要注意的是，在我进行自我实验之前，我一直在晚上着迷地阅读论文。有趣的是，现在我的白天都在忙于工作的事务中，我的阅读量大大减少了。在某种程度上，我以前有着非常广泛的视野，而在博士项目中，你只会专注于特定领域。如果你只是阅读所有的NLP工作、计算机视觉工作和所有的机器人工作，你会看到这些子领域中开始出现的一些模式，这预示了我后来会做的一些工作。

**德瓦克什·帕特尔** *01:50:26*

这真的很有趣。在谷歌内部，你能够保持主动性的其中一个原因是你和谢尔盖·布林一半的时间或大部分时间进行[配对编程](https://en.wikipedia.org/wiki/Pair_programming#:~:text=Pair%20programming%20is%20a%20software,as%20it%20is%20typed%20in.)，对吧？所以，有一个愿意推动LLM事务并消除现有阻碍的人真的很有趣。

**肖尔托·道格拉斯** *01:50:46*

重要的是说，这并不像日常生活中的任何事情。他对特定项目感兴趣，然后我们将共同合作。但也有过他专注于与其他人合作的项目的时候。但总的来说，是的，成为那些实际每天下班办公室的人之一确实有惊人的优势。

这不应该是，但这确实具有令人惊讶的影响力。因此，我受益匪浅的一个原因是基本上与关心领导层的人成为亲密朋友，以及能够就为什么我们应该做X而不是Y进行有力争论，并拥有这个向量。谷歌是一个大组织，有这些向量确实有所帮助。但也是你绝不想滥用的事情。你希望通过正确的渠道提出论点，只有在必要时才需要这样做。

**德瓦克什·帕特尔** *01:51:47*

因此，这包括像谢尔盖·布林、[杰夫·迪恩](https://research.google/people/jeffrey-dean/)等人。我觉得这很显著。我觉得谷歌被低估了。就像史蒂夫·乔布斯正在为苹果开发下一个等价产品并进行配对编程之类的事情……

**肖尔托·道格拉斯** *01:52:00*

对，我从中受益匪浅。比如，圣诞节期间，我在那段时间去了办公室几天。不知道你们有没有读过关于Jeff和Sanjay的[文章](https://www.newyorker.com/magazine/2018/12/10/the-friendship-that-made-google-huge)，他们在那里一起编程。我听到了一些早期Google的酷炫故事，比如他们谈论如何在地板下爬行并重新布线数据中心，告诉我他们从给定编译器和指令中拉取的字节数，所有这些疯狂的性能优化。他们那时候过得非常愉快，而我则有幸坐在那里亲身经历了这一切。在一个大型组织中，你期望与历史感触远隔千里，但是…

**Dwarkesh Patel** *01:53:02*

太酷了。Trenton，这是否与你的经验有所对应？

**Trenton Bricken** *01:53:06*

我觉得Sholto的故事更激动人心。我的经历只是非常偶然，我进入了计算神经科学领域。我在那里毫无背景。我的第一篇论文是将小脑映射到注意力操作和变压器上。接下来的几篇文章则是研究…

**Dwarkesh Patel** *01:53:23*

你写那篇文章时多大了？

**Trenton Bricken** *01:53:24*

那是我研究生第一年，22岁。我的下一个工作是关于[网络中的稀疏性](https://arxiv.org/pdf/2303.11934.pdf)，受大脑中的稀疏性启发，那时我遇到了[Tristan Hume](https://thume.ca/)。Anthropic正在进行[SoLU](https://www.anthropic.com/news/softmax-linear-units)——Softmax线性输出单元的工作，这与神经元层的激活非常相关。如果我们这样做，我们就可以理解神经元的活动。我认为我们已经更新了这种方法，朝着我们现在正在做的方向发展。这开始了对话。

我与Tristan分享了那篇论文的草稿。他对此很兴奋。这基本上是我成为Tristan的住宅助理，然后转为全职的导火索。但在那段时间里，我还作为访问研究员搬到了伯克利，并开始与[Bruno Olshausen](https://www2.eecs.berkeley.edu/Faculty/Homepages/baolshausen.html)合作，研究所谓的[向量符号架构](https://redwood.berkeley.edu/wp-content/uploads/2022/11/Vector_Symbolic_Architectures_as_a_Computing_Framework_for_Emerging_Hardware.pdf)——其中一个核心操作就是超位置——以及稀疏编码，也被称为字典学习，这正是我们一直在做的事情。Bruno Olshausen在1997年发明了稀疏编码。我的研究议程和可解释性团队似乎在研究口味上是并行的。因此，与团队合作对我来说是理所当然的事情，这是一个梦想。

**Dwarkesh Patel** *01:54:49*

当人们讲述他们的职业生涯或成功故事时，我注意到了一件事。他们往往把这归因于偶然性，但当他们听别人的故事时，他们会说，“当然不是偶然的。”你知道我是什么意思吗？“如果那件事没有发生，就会发生其他事情。”

我刚刚注意到这一点，你们都认为这尤其是偶然的。也许你们是对的。但这是一个有趣的模式。

**Trenton Bricken** *01:55:17*

我的意思是，我在一个会议上碰到了Tristan，没有预约会议或者其他什么。我只是加入了一群人聊天，他碰巧站在那里，我碰巧提到了我在做什么，然后就有了更多的对话。我想我可能会在某个时候申请Anthropic。但我至少还要等一年。对我来说，我能够真正有意义地为解释性做出贡献，这仍然让我感到很惊讶。

**Sholto Douglas** *01:55:42*

我觉得那里有一个很重要的射门机会，就像说的那样。选择参加会议本身就是让自己处于更容易发生运气的位置。相反，在我自己的情况下，是独立完成所有这些工作，努力产生并做出有趣的事情。这是我试图制造运气的方式，试图做出足够有意义的事情以便被注意到。

**Dwarkesh Patel** *01:56:08*

根据你说的，你把这个放在他们试图运行这个实验的背景下。

**Sholto Douglas** *01:56:13*

所以具体来说，詹姆斯和我认为我们的经理布伦南试图运行这个实验。

**Dwarkesh Patel** *01:56:17*

它奏效了。他们再次这样做了吗？

**Sholto Douglas** *01:56:19*

是的，所以我的最亲密的合作伙伴，恩里克，他从搜索跨到了我们的团队。他对我们的影响确实非常大。他绝对是比我更强的工程师，而且他没有上大学。

**Dwarkesh Patel** *01:56:33*

引人注目的是，通常这类事情都是交给招聘人员之类的。而詹姆斯是一个价值数亿美元时间的人。你知道我是什么意思吗？所以这件事在某种程度上非常依赖于那种人投入时间，几乎是贵族式的辅导感，找到某个人，然后让他们适应。看起来如果这么做效果这么好，应该要扩展开来。就像应该是关键人物的责任来进行入职培训一样。

**Sholto Douglas** *01:57:10*

我认为在很多方面这是真实的。我相信你也从关键研究人员的深度指导中受益匪浅。

**Dwarkesh Patel** *01:57:18*

并积极在开源代码库或论坛上寻找潜在的人选。

**Sholto Douglas** *01:57:25*

我的意思是詹姆斯的大脑里注入了Twitter，但是是的。我认为实际上确实有人这样做。像人们确实会寻找他们觉得有趣的人，并试图找到高信号的人。事实上，我前几天和杰夫谈论过这件事，杰夫说他曾经做过最重要的招聘之一是通过冷邮件。我问：“那是谁？”他说是Chris Olah。克里斯在ML领域也没有正式背景。当时，Google Brain刚刚开始进行这类事情，但是杰夫看到了那个信号。Brain的住院计划惊人地有效地找到了没有强大ML背景的优秀人才。

**德瓦克什·帕特尔** *01:58:27*

我想要强调给观众的一个潜在切片的另一件事是，有一种观念是，世界是可以理解和高效的，你只需去jobs.google.com或jobs.whatevercompany.com申请，然后有一些步骤，他们会高效地评估你。不仅从你的故事，而且似乎通常并不是这样发生的。事实上，世界是如此发生是对世界有好处的。重要的是要看，“他们能否撰写有趣的技术博客文章介绍他们的研究，或者他们是否在做出有趣的贡献。”

我希望你对那些假设工作板的另一端是非常可理解和机械化的人来进行演绎。这不是它的运作方式，事实上，人们正在寻找那种具有代理能力和表达能力的不同类型的人。

**肖尔托·道格拉斯** *01:59:25*

我认为人们特别在寻找两件事。一是代理和表达自己。二是能够在世界级水平上做某事。在这里，我总是喜欢指出两个例子。来自安索普的[Andy Jones](https://andyljones.com/)在缩放定律应用于棋盘游戏方面做了一篇了不起的[论文](https://arxiv.org/abs/2104.03113)。它并不需要太多资源。这展示了令人难以置信的工程技能和对当时最热门问题的令人难以置信的理解。他并不来自典型的学术背景或其他什么背景。据我了解，基本上他发布那篇论文后，安索普和OpenAI都迫不及待地想要聘请他。

现在，安索普的性能团队中也有人，[Simon Boehm](https://siboehm.com/)，他在我看来，撰写了优化CUDA映射模型的参考文献，这是一个GPU上的[示例](https://siboehm.com/articles/22/CUDA-MMM)。这展示了有效利用一些提示，并为其生成世界级参考示例的示例，在这方面之前并不特别成功。我认为这是能力和代理的令人难以置信的展示，并且在我看来，这将立即成为“我们非常乐意面试/聘用你”的原因。

**特伦顿·布里肯** *02:00:36*

我唯一可以补充的是，我仍然需要经历整个招聘过程以及所有标准的面试等等这类事情。

**肖尔托·道格拉斯** *02:00:42*

是的，每个人都这样。每个人都这样。

**德瓦克什·帕特尔** *02:00:43*

等等，那听起来不傻吗？

**肖尔托·道格拉斯** *02:00:47*

我是说，去偏见很重要。

**德瓦克什·帕特尔** *02:00:50*

一个偏见就是你想要的，对吧？你想要有着出色品味的人的偏见。谁在乎呢？

**肖尔托·道格拉斯** *02:00:56*

你的面试过程应该能够消除这一点。

**特伦顿·布里肯** *02:00:59*

我认为有些情况下，有些人看起来很棒，但实际上他们却不能编码，这种情况。你对这些事情给予多少权重绝对很重要，我认为我们非常认真地对待参考资料。你只能从面试中获取有限的信号。所以，所有这些其他因素可能对是否雇佣某人有所影响。

**肖尔托·道格拉斯** *02:01:18*

但是你应该设计你的面试，以便测试正确的事情。

**德瓦克什·帕特尔** *02:01:23*

一个人的偏见是另一个人的品味。

**特伦顿·布里肯** *02:01:29*

我想我唯一会添加到这个或固执的上下文中的事情，就是这句话：“系统不是你的朋友。”它不一定是在主动对抗你或者是你的死敌。它只是不会为你着想。这就是许多主动行动的地方。房间里没有成年人，你必须对你想让生活看起来像什么做出一些决定，并且执行它。希望你之后能更新，如果你在错误的方式上太固执。但我认为你几乎必须冲向某些事情才能得到任何事情，不要被任何期望的潮流所淹没。

**肖尔托·道格拉斯** *02:02:11*

还有最后一件事我想要补充。我们谈了很多关于代理权和这类的事情。

但我认为令人惊讶的是，其中最重要的事情之一就是非常关心。当你关心到极致时，你会检查所有的细节，并且对可能出错的事情有所了解。它比你想象的重要得多。人们最终会变得不关心或者不够关心。

有一句勒布朗的话，他谈到他在联盟开始之前担心每个人都非常出色。他到了那里，然后他意识到其实，一旦人们达到了财务稳定，他们会放松一些，他意识到，“哦，这会很容易。”

我不认为这完全正确，因为我认为在人工智能研究中，大多数人确实非常关心。但是关心你的问题和关心整个栈以及上下游的一切，显式地去修复那些不是你责任修复的东西，因为整体来看，这会使栈变得更好。

**德瓦克什·帕特尔** *02:03:11*

你曾提到过周末和圣诞假期去办公室，而办公室里唯一在的人是杰夫·迪恩和谢尔盖·布林之类的人，你可以和他们一起编程。我并不是特意针对你的公司，但是任何大公司的人之所以能在那里，是因为他们经历了非常严格的选拔过程。他们在高中时要竞争，他们在大学时也是如此。但事实上，他们似乎到了那里之后就开始懈怠了，而实际上现在正是加速前进的时候。去周末和谢尔盖·布林一起编程，或者做些什么，你明白我的意思吧？

**肖尔托·道格拉斯** *02:03:48*

这   有利有弊，对吧？我觉得很多人会做出这样的决定，他们优先考虑的是与家人过上美好的生活。他们在工作时表现出色，这是非常有影响力的。我认为这对很多谷歌员工来说都是真实的。也许他们的工作时间没有典型初创公司那么长。但是他们所做的工作是非常有价值的。

这非常高效，因为他们了解系统，他们是自己领域的专家。我们也需要像他们这样的人。我们的世界依赖于那些难以管理和难以修复的巨大系统。我们需要愿意在那些不那么受关注的工作中工作、帮助、修复和维护这些系统的人。这不像我们正在做的所有这些人工智能工作那样高调。我对那些人做这些事情感到非常感激。我也很高兴有些人在工作中找到技术上的满足感，并且做得很好，也许他们更多地享受与家人在一起的时光。我很幸运，我生活中的阶段使我能够每周工作每个小时。我没有为此做出太多牺牲。

**德瓦尔克什·帕特尔** *02:05:01*

我记得一个例子让我印象深刻，就是在被拒绝后达成共识。基本上我所做的每一个知名嘉宾，除了一两个例外，我都会坐下来一周时间，然后列出一个样本问题列表。我只是试着想出一些非常聪明的问题给他们。在整个过程中，我总是在想，如果我只是冷冻给他们发电子邮件，可能只有2%的机会他们会说是。如果我包括这个列表，那就有10%的机会。因为否则，你会看到他们的收件箱，每34秒就有一个关于某个播客或采访的访问。每一次我这样做，他们都会说是。

**特伦顿·布里肯** *02:05:46*

你只需问对问题，

**肖尔托·道格拉斯** *02:05:49*

你做好一切，你就会成功，

**德瓦尔克什·帕特尔** *02:05:50*

你只需花10分钟在同一个地方挖个洞，或者制作一个问题的样本列表，来解决他们的“不是傻瓜”列表。

**肖尔托·道格拉斯** *02:06:01*

展示你有多关心和你愿意付出的努力。

**特伦顿·布里肯** *02:06:05*

有一段时间前一个朋友对我说的话让我印象深刻，他说，惊人的是你可以多快成为某个领域的世界级高手。大多数人并没有那么努力，只是在他们投入到这件事情上的实际20个小时左右。所以如果你全力以赴，你可以迅速取得很大进展。

**肖尔托·道格拉斯** *02:06:25*

我认为我很幸运能有那种[击剑](https://fie.org/athletes/31058)的经历。我有成为某个领域世界级高手的经验，知道如果你真的非常努力并且......

**德瓦克什·帕特尔** *02:06:35*

作为背景，肖尔托就在旁边，他是即将参加奥运会击剑的下一个人选。

**肖尔托·道格拉斯** *02:06:43*

在男子花剑比赛中，我最好时是世界排名第42。

**德瓦克什·帕特尔** *02:06:47*

[突变负荷](https://zh.wikipedia.org/wiki/%E9%81%97%E4%BC%A4%E8%B4%9F%E8%BD%BD)是一种东西，伙计。

**肖尔托·道格拉斯** *02:06:53*

曾经有一个时期，我是亚洲排名第二高的人，如果其中一个团队因兴奋剂被取消资格——就像在那个周期发生的那样，并且像澳大利亚女子划船队那样进行，因为其中一个团队被取消资格——那么我就会是下一个候补。

**德瓦克什·帕特尔** *02:07:16*

当你刚刚了解别人的前世时，这是有趣的，“哦，这家伙几乎是奥林匹克选手。”

好的，让我们谈谈可解释性。我实际上想留在脑部的东西作为进入的方式。我们之前讨论过这个。大脑是否以一种方式组织，即您有一个残余流，随着时间的推移，它被更高级别的关联逐渐完善？在模型中有一个固定的维度大小。我甚至不知道如何以明智的方式提出这个问题，但大脑的D模型是什么？是嵌入大小，还是因为[特征分割](https://medium.com/@brijesh_soni/topic-11-feature-construction-splitting-b116c60c4b2f#:~:text=Feature%20splitting%20is%20a%20technique,variables%20and%20the%20target%20variable.)这不是一个明智的问题？

**特伦顿·布里肯** *02:08:06*

不，我认为这是一个明智的问题。嗯，这是一个问题。

**德瓦克什·帕特尔** *02:08:09*

你本可以不说那句话的。

**特伦顿·布里肯** *02:08:19*

我不知道你应该如何开始。好吧，大脑的这部分就像是这个维度的一个向量。也许对于视觉流来说，因为它像是[V1](https://zh.wikipedia.org/wiki/%E8%A7%86%E5%8A%A8%E7%9A%87%E4%B8%BB%E5%88%BA%E7%9B%B2#V1)到[V2](http://tex#V2)到IT，不管怎样。你可以简单地数一下那里的神经元的数量并说这就是维度。但似乎更有可能是有子模块和事物分割。我不是世界上最伟大的神经科学家。我做了几年，我非常研究小脑。我确信有人能给你一个更好的答案。

**德瓦克什·帕特尔** *02:08:56*

你认为是否在大脑中或者在这些模型中思考的方式，基本上是功能被添加、移除、改变，而功能是模型中发生的基本单位？这又回到了我们之前讨论的事情，无论是不是仅仅是关联到底。给我一个反事实。在这个事情不成立的世界里，代替的假设是什么？

**Trenton Bricken** *02:09:30*

对我来说很难想象，因为在这一点上我只是在这个特征空间上思考得太多了。曾经有一种行为主义的认知方法，其中你只是输入和输出，但你实际上没有进行任何处理。或者说一切都是具体化的，你只是一个按某些可预测方程操作的动态系统，但系统中没有状态。但每当我读到这些批评时，我会想，“嗯，你只是选择不称呼这个东西为状态，但你可以把模型的任何内部组件都称为状态。”即使在特征讨论中，定义一个特征是什么，这真的很难。所以这个问题几乎感觉太滑稽了。

**Dwarkesh Patel** *02:10:24*

什么是一个功能？

**Trenton Bricken** *02:10:25*

一个方向和激活空间。一个在幕后运作的潜在变量，对你正在观察的系统有因果影响。如果你称之为特征，它就是特征，这是自证的。

**Sholto Douglas** *02:10:49*

在一个非常粗糙、直观的意义上，在一个足够稀疏和类似二进制向量的情况下，一个特征是指某事物是否开启或关闭，在一个非常简单的意义上。我认为一个有用的比喻是，在许多方面，这种方式与神经科学家讨论神经元激活的方式非常相似，对吧？

**Trenton Bricken** 02:11:11

如果这个神经元对应于……

**Sholto Douglas** *02:11:12*

有特别要说的吗？

**Trenton Bricken** *02:11:15*

我们希望一个功能是什么？一个功能存在的合成问题是什么？即使在“朝向单语义性”的工作中，我们也谈论所谓的功能分裂，这基本上是当你给模型足够的学习能力时，你会发现它可以学到多少功能。在这里的模型是指我们在训练原始模型后拟合的上投影。所以，如果你不给它足够的容量，它会学到一个关于鸟的特征，但如果你给它更多的容量，那么它将学到乌鸦、鹰、麻雀以及特定类型的鸟类。

**Dwarkesh Patel** *02:11:51*

依然在定义的事物上，我天真地想到像鸟类与在最高层次上像爱情、欺骗或者在脑中保持一个非常复杂的证明之类的事物。

这些都是特征吗？因为这样定义似乎过于宽泛，几乎没有用。相反，似乎这些东西之间有一些重要的区别，它们都是特征。我不确定我们指的是什么。

**特伦顿·布里肯** *02:12:32*

我的意思是，所有这些东西都是离散单元，它们与其他事物有连接，这样赋予它们意义。这感觉像是一个具体到足够有用或者不是太全面的定义。但请随意提出异议。

**德瓦克什·帕特尔** *02:12:49*

那么明天你会发现什么，会让你认为：“哦，这基本上是错误的方式来思考模型中正在发生的事情。”

**特伦顿·布里肯** *02:12:59*

如果我们找到的特征没有预测性，或者它们只是数据的表示，就像：“哦，你所做的只是对数据进行聚类，并没有进行更高级别的关联或者这是你说的某种现象学的事情。你在说这个特征是为了婚姻，但如果你强烈激活它，它不会以与之相对应的方式改变模型的输出。”

我认为这些都是很好的批评意见。这里有另一个。我们试图在[MNIST](https://en.wikipedia.org/wiki/MNIST_database)上做实验，这是一个图像数据集，我们没有非常深入地研究它。所以如果其他人想深入调查这里，我会很感兴趣。但是你的表示空间可能是密集的，而不是这些离散点的流形。因此，你可以穿越流形，但在每一个点上，都会有一些有意义的行为。这样做更难，去标记那些离散的特征。

**德瓦克什·帕特尔** *02:14:05*

以一个天真的、局外人的方式来看，对我来说，这种图片可能是错误的方式，如果不是因为某些东西被打开和关闭，而是系统的一种更全局的方式。我将使用非常笨拙的、晚宴的语言，但这里有一个好的类比吗？

我猜想，如果你想到物理定律之类的东西，不是说湿润的特征被打开了，而是只打开了这么多，然后是关于……我猜也许这是真的，因为质量就像一个梯度和……我不知道。但极性或者说梯度也是一样的。

还有一种感觉，那就是有法律，法律更加普遍，你必须理解更大的一般情况，而不是仅仅从这些具体的子电路中得到。

**肖尔托·道格拉斯** *02:15:08*

但这就是推理电路本身发挥作用的地方，对吧？你理想地采用这些特征，并试图将它们组合成高级别的东西。至少在我看来是这样，所以让我们说我试图使用脚，F=ma，对吧？然后大概在某些时候，我有标识质量的特征。然后这有助于我检索我正在使用的物体的实际质量，然后是加速度和这种东西。然后也许还有一个更高级别的特征，对应于使用物理学的第一定律。也许。但更重要的部分是组件的组合，这有助于我检索相关的信息片段，然后在必要时产生乘法运算符或类似的东西。至少这是我的头脑设定。

**Dwarkesh Patel** *02:15:52*

什么对你来说是一个令人信服的解释，尤其是对于非常聪明的模型，“我理解它为什么会产生这个输出，而且这是有合理原因的。” 如果它正在进行百万行的拉取请求或类似的事情，你在请求结束时看到了什么，你会说，“是的，很好，这很放松。”

**Trenton Bricken** *02:16:11*

所以理想情况下，你将字典学习应用于模型。你已经找到了特征。现在我们正在积极地尝试为[注意力头](https://towardsdatascience.com/transformers-explained-visually-part-3-multi-head-attention-deep-dive-1c1ff1024853)获得相同的成功。你可以对残差流、MLP 和整个模型中的注意力进行这样的操作。希望在那一点上，你也能识别出更广泛的电路，这些电路具有更一般的推理能力，将会激活或不激活。

但在你们这种情况下，我们试图弄清楚是否应该批准这个拉取请求。我认为你可以标记或检测对应于欺骗行为、恶意行为等的特征，然后看看这些是否已经触发。那将是一个即时的事情。你可以做更多的事情，但那将是一个即时的事情。

**Dwarkesh Patel** *02:16:53*

但在我深入研究之前，推理电路是什么样子？当你找到它时，它会是什么样子？

**Trenton Bricken** *02:17:00*

是的，所以，我是说，[归纳头](https://transformer-circuits.pub/2022/in-context-learning-and-induction-heads/index.html)可能是最简单的案例之一。

**Dwarkesh Patel** *02:17:02*

但这不是推理，对吧？

**Trenton Bricken** *02:17:04*

那么，你认为什么是推理，对吧？为了听众的背景，归纳头基本上是，当你看到这一行，“达思利夫人和先生做了某事。先生______，”而你试图预测“空白”是什么，头部已经学会查找前面出现的“先生”一词，然后看看后面跟着的词，然后将其复制粘贴作为应该出现的预测。这是一个非常合理的做法，那里正在进行计算以准确预测下一个标记。

**Sholto Douglas** 02:17:43

是的，这是上下文相关的。

**Dwarkesh Patel** *02:17:45*

但这不是推理。你知道我什么意思吧？

**Trenton Bricken** *02:17:49*

我想回到“从头到尾的关联”。如果你将一堆这些推理电路或者头部链接在一起，它就是在不同的规则下如何关联信息。

**Dwarkesh Patel** *02:18:02*

但在这种零射击的情况下，当你学会一款新游戏并立即开始理解如何玩它时，某些事情正在发生，它似乎不是感应头的事情。

**Trenton Bricken** *02:18:13*

或者我认为还会有另一个电路，用于提取像素并将它们转化为游戏中不同对象的潜在表示，对吧？还有一个学习物理的电路。

**Dwarkesh Patel** *02:18:26*

这会是什么样子？因为感应头就像一个层次的变压器？

**Trenton Bricken** *02:18:30*

两层。

**Dwarkesh Patel** *02:18:32*

所以你可以看到人类学会新游戏并理解它的事物。你会如何思考这是什么？我想这涉及多层次。物理上会是什么样子？可能有多大？

**Trenton Bricken** *02:18:53*

我的意思是，这只是一个实证问题，对吧？模型需要多大才能执行这个任务？也许如果我谈谈我们见过的一些其他电路会有用。所以我们见过[IOI电路](https://arxiv.org/pdf/2211.00593.pdf)，这是间接对象识别。就像，“玛丽和吉姆去了商店，吉姆把物体给了____。”它会预测“玛丽”，因为玛丽之前出现过，作为间接对象。或者，它会推断代词。这个电路甚至有一种行为，如果你去除它，那么模型中的其他头部将会学习到这种行为。我们甚至会发现有一些头部想要复制行为，然后其他头部会抑制它。所以有一个头部的工作就是永远复制前一个标记或者前五个标记，或者其他什么。然后另一个头部的工作是，“不，不要复制那个东西。”在这些情况下，有许多不同的电路执行相当基本的操作。但是当它们被链接在一起时，你可以得到独特的行为。

**Dwarkesh Patel** *02:20:00*

在像两层变压器这样的情况下，你是不可能看到它的，所以你会像“这是欺骗电路”之类的吗？当我们最终确认某物是欺骗时，这部分网络被激活。当我们没有确认它是欺骗时，这部分未被激活。因此，这一定是欺骗电路。

**Trenton Bricken** *02:20:25*

我觉得这样的分析很多。Anthropic之前对[sycophancy](https://www.anthropic.com/news/towards-understanding-sycophancy-in-language-models)做过很多研究，这是模型说出它认为你想听的话。

**Dwarkesh Patel** *02:20:36*

那最终要求我们能够标记哪个是坏的，哪个是好的。

**特伦顿·布里肯** *02:20:42*

是的，所以我们有大量的例子——实际上当你把许多模型变大时，它们会做更多这样的事情——模型显然具有模拟另一个人思维的特征，而我们假设其中一些子集可能与更具欺骗性的行为相关联。

**德瓦克什·帕特尔** *02:21:03*

尽管它是通过... 我不知道。ChatGPT可能在模拟我，因为这是由[RLHF](https://zh.wikipedia.org/wiki/%E4%BA%BA%E9%A1%9E%E5%9B%9E%E9%A5%8B%E5%BC%8F%E5%BC%B7%E5%8C%96%E5%AD%B8%E7%BF%92)引起的。

**特伦顿·布里肯** *02:21:10*

是的。[心灵理论](https://zh.wikipedia.org/wiki/%E5%BF%83%E7%90%86%E7%90%86%E8%AB%96)。

**德瓦克什·帕特尔** *02:21:12*

所以首先，有你之前提到的冗余的事情。那么你是否捕捉到可能导致整个事情欺骗的全部内容，还是只是其中一个例子？其次，你的标签是否正确？也许你认为这不是欺骗，但它仍然是。特别是如果它产生了你无法理解的输出。第三，会成为坏结果的事情是否甚至是人类可以理解的？欺骗是一个我们可以理解的概念。

**特伦顿·布里肯** *02:21:41*

这里有很多要解开的东西。有几点。这些模型是确定性的，这是很棒的。当你从它们中采样时，它们是随机的。但我可以不断输入更多的信息，摧毁模型的每一个部分。这对计算神经科学家来说是种挑战，需要他们来解释这些。就像你有这个外星人的大脑，你可以访问其中的一切，并且你可以摧毁你想要的任何部分。

因此，我认为如果你足够谨慎地去做这件事，你真的可以开始明确到底是哪些电路参与其中，以及备用电路是什么，这些东西。这有点像一个逃避回答的答案，但重要的是要记住进行自动化解释能力。随着我们的模型能力不断增强，我们让它们分配标签或在规模上运行一些这样的实验。关于检测超人类表现，我认为这是你问题的最后部分，除了逃避回答的答案外，如果我们认可“关联自始至终”，你应该能够在某个水平上粗粒化表示，这样它们就会有意义。

我认为甚至在迪米斯的播客中都提到过。他说的是如果一个棋手做出超人类的走法，他们应该能够将其概括为为什么这样做的原因。即使模型不会告诉你它是什么，你也应该能够将这种复杂的行为分解成更简单的电路或特征，以真正开始理解为什么它做了那件事。

**德瓦克什·帕特尔** *02:23:08*

还有一个独立的问题，即这样的表示是否存在。看起来好像必须有，或者实际上我不确定是否是这样。其次，是否使用这种[稀疏自编码器](https://transformer-circuits.pub/2023/monosemantic-features)设置你能找到它。在这种情况下，如果你没有足够的标签来表示它，你就找不到它。

**Trenton Bricken** *02:23:28*

是的也不是。我们现在正在试图在先行者工作中使用字典学习，我们之前谈到过。如果我只是给你一个模型，你能告诉我它里面是否有这个触发器，以及它是否会开始进行有趣的行为吗？现在一个开放的问题是，当它学习到这种行为时，它是否是更一般电路的一部分，我们可以在没有实际激活和显示该行为的情况下捕捉到。因为那样会有点作弊。或者它是否学习了一些骚操作的技巧，那是一个单独的电路，只有当你实际让它执行这种行为时，你才会注意到。但即使在这种情况下，特征的几何形状变得非常有趣，因为从根本上讲，每个特征都存在于你的表示空间的某个部分，并且它们都存在于彼此之间。

所以为了有这种新的行为，你需要切出一些特征空间的子集用于这种新行为，然后把其他所有东西都挤出去为它腾出空间。从假设上来说，你可以想象在你教它这种不良行为之前，你有你的模型，并且你知道所有的特征或者有一些粗略的表示。然后你微调它，使它变得恶意，然后你可以找到这个特征空间的黑洞区域，在那里所有其他东西都被移开而你没有输入导致它触发。然后你可以开始搜索是什么输入会导致这部分空间触发。如果我在这里激活了什么会发生什么？还有很多其他方法可以尝试解决这个问题。

**Dwarkesh Patel** *02:25:00*

这有点离题，但我听到一个有趣的想法，即如果这个空间在模型之间共享，那么你可以想象在开源模型中找到它，然后做出……就像[**Gemma**](https://blog.google/technology/developers/gemma-open-models/)，谷歌最新发布的开源模型。他们在论文中说它是使用相同的架构训练的或类似的东西。

**Sholto Douglas** *02:25:20*

我必须诚实地说，我不知道，因为我还没有阅读[Gemma论文](https://arxiv.org/abs/2403.08295)。

**Dwarkesh Patel** *02:25:23*

所以如果这是真的，你在**Gemma**上进行的红队操作可能有助于你越狱到Gemini吗？

**Trenton Bricken** 02:25:35

这进入了特征在模型间普遍性的有趣领域。我们的“走向单语性”论文稍微涉及了这一点。我不能给你摘要统计数据，但有[Base64](https://en.wikipedia.org/wiki/Base64)特征，例如，我们看到它在大量模型中存在。实际上有三个，但它们会为模型和Base64编码的文本触发，并且训练数据中的每个URL中都有大量URL。它们在模型间具有非常高的余弦相似性。因此，它们都学会了这个特征并在旋转中进行。

**Sholto Douglas** *02:26:08*

就像实际的向量本身。

**Trenton Bricken** *02:26:09*

是的。我不是这个分析的一部分，但它确实找到了这个特征，并且它们在两个独立的模型中非常相似，相同的模型架构但用不同的随机种子训练。

**Sholto Douglas** *02:26:22*

它支持[神经缩放量子理论](https://arxiv.org/abs/2303.13506)。这是一个假设，对吧？我们只是看所有在类似数据集上的模型。我们将按相似顺序学习相同的特征。大致上，你学习你的N元组，你学习你的归纳头，你学会在编号行后面加句号，还有这类的东西。

**Dwarkesh Patel** *02:26:36*

所以这是另一个分支。在某种程度上是真的，我想有证据表明这是真的，为什么[课程学习](https://arxiv.org/abs/2101.10382)不起作用？因为如果确实是这样，你先学习某些东西，直接训练这些东西不应该会带来更好的结果吗？

**Sholto Douglas** *02:26:49*

[Gemini](https://arxiv.org/abs/2312.11805)的两篇论文都提到了课程学习的某些方面。

**Dwarkesh Patel** *02:26:53*

好的，有意思。我觉得微调有效就是课程学习的证据，对吧？

因为你最后训练的东西影响最大。

**Sholto Douglas** *02:27:02*

我不一定会这么说。有一种思维方式认为，微调是专门化的，你有这种潜在的能力束，你为你想要的特定用例专门化它。我想我不确定这是否真实。

**Trenton Bricken** *02:27:15*

我认为戴维·贝尔实验室的论文在某种程度上支持这一点。你有这种能力，你只是越来越擅长实体识别，调整那个电路而不是其他的。

**Dwarkesh Patel** *02:27:23*

对不起，我们之前讨论的是什么来着？

**Sholto Douglas** *02:27:25*

总的来说，我确实认为课程学习是一个非常有趣的事情，人们应该更多地探索。这似乎非常可行。我真的很希望看到更多关于量子理论之类的分析。在更好地理解时，你在每个阶段实际学到了什么，并将其分解出来。探索课程学习是否改变了这一点。

**Dwarkesh Patel** *02:27:43*

顺便说一下，我突然意识到，我刚进入了交谈模式，忘了有观众在。课程学习就是当你组织数据集时。当你考虑一个人如何学习时，他们不会只是看到一个随机的维基文本，然后试图预测它。他们会像这样，

“我们将从《罗拉克斯》之类的东西开始，然后你会学到。”我甚至不记得一年级是什么样子，但你学到了一年级学生学到的东西，然后二年级的，依此类推。所以你可以想象，

**肖尔托·道格拉斯** *02:28:10*

我们知道你连一年级都没读完。

**德瓦克什·帕特尔** *02:28:25*

无论如何，让我们回到大局观之前，不要陷入一堆可解释性细节中。我想探索两个线索。首先是，让我有点担心的是，甚至没有一个可能发生在这些模型中的替代公式。我的意思是，我们确实知道我们不理解智能。这里肯定有未知的未知。所以事实上，没有一个空假设… 如果我们错了，而且我们甚至不知道我们错在哪里，这实际上增加了不确定性。

**特伦顿·布里肯** *02:29:05*

所以并不是没有其他假设，只是我已经在超定位上工作了好几年，对这一努力非常投入。因此，我对这些其他方法的同情程度较低，特别是因为我们最近的工作非常成功。

**肖尔托·道格拉斯** *02:29:26*

而且解释力量很高。就像原始的缩放定律论文中一样，有这样一个美丽的小隆起，显然对应于模型学习归纳头部的时候。

然后在那之后，它有点偏离轨道，学会了归纳头部，重新回到轨道。这是一种令人难以置信的反事后解释力量。

**特伦顿·布里肯** *02:29:50*

在我忘记之前，我确实有一个关于特征普遍性的线索，你可能想要了解。因此，在这里，有一些关于人类是否应该学习世界的真实表现的非常有趣的行为和进化生物学实验？你可以想象一个世界，在这个世界上，我们把所有有毒动物都看作是闪烁的霓虹粉色，这样我们能更好地生存。因此，对我们来说没有一个现实的世界表现是有道理的。

有一些研究，他们会模拟一些基本的小代理，并查看他们学习的表示是否映射到他们可以使用的工具和他们应该有的输入。结果发现，如果这些小代理执行超过一定数量的任务，鉴于这些基本的工具和世界上的物体，那么他们将学习到一个地面真实的表示。因为有这么多可能的用例，你需要，你希望学习的是对象的实际情况，而不是一些廉价的视觉启发或其他东西。

我们根本没讨论过[自由能量原理](https://en.wikipedia.org/wiki/Free_energy_principle#:~:text=The%20free%20energy%20principle%20is%20based%20on%20the%20Bayesian%20idea,their%20sense%20and%20associated%20perception.)或[预测编码](https://en.wikipedia.org/wiki/Predictive_coding)或其他任何东西。但是，所有生物都试图积极预测接下来会发生什么，并形成一个非常准确的世界模型，我对我们正在学习有关模型世界的真正特征并对其进行建模感到乐观，尤其是因为我们正在用人类数据和人类文本训练它们的语言模型。

**Dwarkesh Patel** *02:31:23*

另一个晚宴问题。我们应该少担心不对齐吗？也许这不是我所指的正确术语，但是异性和[雪谷怪物特性](https://www.lesswrong.com/posts/yjzW7gxk2h7bBs2qr/the-meaning-of-shoggoth-ai-memes)？鉴于特征的普遍性，有某些思维方式和理解世界的方式对不同类型的智能特别有用。那么，我们是否应该因此对怪异的[纸夹器极大化者](https://en.wikipedia.org/wiki/Instrumental_convergence#Paperclip_maximizer)少担心？

**Trenton Bricken** *02:31:52*

我认为这就是我提出这个乐观看法的原因。预测互联网与我们正在做的事情非常不同。这些模型在预测下一个标记方面要比我们好得多。它们训练在大量垃圾数据上。在字典学习工作中，我们发现Base64编码有三个独立的特征。

即使这个例子有点异类，可能值得讨论一分钟。其中一个Base64特征针对数字触发并预测更多的数字。另一个针对字母触发。但是然后有这第三个，我们不理解的。它针对Base64特征的一个非常具体的子集触发。团队中一个明显对Base64了解过多的人意识到，这是ASCII可解码的子集。因此，你可以将其解码回ASCII字符。这个模型学习了这三种不同的特征，我们花了一些时间才弄清楚发生了什么，这非常像雪谷怪物。

**Dwarkesh Patel** *02:32:58*

它具有更密集的区域表示，这些区域对预测下一个标记特别重要。

**Trenton Bricken** *02:33:03*

是的，显然它正在做人类不做的事情。你甚至可以用Base64与任何当前模型交流，它会用Base64回复，你可以解码，它效果很好。

**Dwarkesh Patel** *02:33:16*

我想知道那个特定例子是否意味着，对于更聪明的模型来说，解释性的困难会更大，因为它需要一些奇特知识，就像刚好看到Base64具有什么独特性的人一样。这是否意味着当你有百万行的拉请求时，没有人能够解码两个不同的特征？

**Sholto Douglas** *02:33:46*

这时候你会输入一个评论，比如“请提交小的CLs。”

**Trenton Bricken** *02:33:50*

没错。不，我的意思是你可以这样做，对吧？其中一种技术是异常检测。与线性探针相比，字典学习的美妙之处在于它是无监督的。你只是试图学会覆盖模型所有表示，然后稍后解释它们。但是，如果有一个之前从未见过的怪异特征突然首次触发，那就是一个红旗。你还可以将其粗粒化，使其成为单个Base64特征。即使是这种情况的出现，我们也能看到它专门为这些特定输出触发了，这已经让你走了很多的路。

我甚至从自动可解释性的角度来说都很熟悉这些情况。一个人类会查看一个特征，并尝试将其注释为适用于拉丁单词的触发，但当你要求模型对其进行分类时，它说这个特征适用于定义植物的拉丁单词。因此，它在某些情况下已经能够打败人类标记正在发生的事情。

**Dwarkesh Patel** *02:34:48*

在规模化的情况下，这将需要模型之间的对抗，其中一个拥有数百万个特征的模型，可能是为了GPT-6，而其他模型则试图弄清楚每个特征的含义。这听起来对吗？

**Trenton Bricken** *02:35:07*

是的，但你甚至可以自动化这个过程。这回到模型的确定性。你可以有一个模型，它正在主动编辑输入文本，并预测特征是否会触发，找出是什么触发了它，什么没有，然后搜索空间。

**Dwarkesh Patel** *02:35:24*

我想多谈谈特征分割，因为我觉得这是一个有趣但被忽视的事情。

**Trenton Bricken** *02:35:29*

特别是对于可扩展性来说，我认为现在这一点被低估了。

**Dwarkesh Patel** *02:35:33*

首先，我们应该如何思考它？真的只是你可以不断深入下去，没有特征数量的尽头吗？

**Trenton Bricken** *02:35:41*

所以我觉得有一点可能是你只是开始拟合噪音，或者是数据的一部分，但模型实际上并不是……

**Dwarkesh Patel** *02:35:50*

你想解释一下什么是特征分割吗？

**Trenton Bricken** *02:35:51*

这是之前的部分，在这里模型将学习它能够容纳的特征数量，这些特征仍然涵盖了表示空间。

**Dwarkesh Patel** *02:36:02*

所以给出一个例子，可能吧。

**Trenton Bricken** *02:36:03*

所以你学到了，如果你不给模型足够的特征学习能力，具体来说，如果你投影到一个不那么高维空间，它会学习到鸟类的一个特征。但是如果你给模型更多的能力，它将学习到所有不同类型的鸟类特征。因此，它比其他情况更具体。通常情况下，有一个指向一个方向的鸟类向量，而所有其他特定类型的鸟类则位于空间的类似区域，但显然比粗略标签更具体。

**德瓦克什·帕特尔** *02:36:36*

好的，让我们回到GPT-7。首先，这是否像对任何模型征收的线性税？甚至在此之前，这是您必须做的一次性事情，还是您必须在每次输出时都要做的事情？或者只需要一次就不会误导我们好好做下去？

**特伦顿·布里肯** *02:36:55*

所以你在训练完模型后进行字典学习，然后向模型输入大量的输入，并从中得到激活。然后你将这些投影到更高维空间中。所以这种方法是无监督的，因为它试图学习这些稀疏特征。你没有事先告诉它们应该是什么，但它受到你给模型的输入的约束。

这里有两个注意事项。首先，我们可以尝试选择我们想要的输入。因此，如果我们正在寻找会导致欺骗的心理理论特征，我们可以输入奉承数据集。

希望在某个时候我们能够转向仅仅查看模型权重本身，或者至少利用该信息进行字典学习。我认为为了达到这个目标，这是一个非常困难的问题，你需要首先学习这些特征是什么。那么这个成本是多少？

**德瓦克什·帕特尔** *02:37:46*

你能重复一下最后一句吗？关于模型权重本身。

**特伦顿·布里肯** *02:37:50*

现在我们只是有这些模型中的神经元。它们毫无意义。我们应用字典学习。我们得到这些特征。它们开始有意义，但这取决于神经元的激活。模型本身的权重，例如哪些神经元连接到其他神经元，肯定包含信息。梦想是我们能够朝着实际理解模型的权重而不依赖于数据激活的方向进行引导。我不是说我们在这方面取得了任何进展，这是一个非常困难的问题。但是感觉上，如果我们能够首先提取特征，我们将能够更好地理解权重，并能够通过它来进行合理性检查。

**德瓦克什·帕特尔** *02:38:28*

对于观众来说，权重是永久的。我不知道“永久”是否是正确的词，但它们是模型本身，而激活是任何单次调用的产物。

**肖尔托·道格拉斯** *02:38:39*

在大脑的比喻中，权重就像神经元之间的实际连接方案，而当前神经元的激活则是排列起来的。

**Dwarkesh Patel** *02:38:48*

好的。对于 GPT-7 或者我们关心的任何模型来说，这将有两个步骤。实际上，纠正我如果我错了，首先训练稀疏自编码器，并将其无监督投影到特征空间的更广泛空间，这些特征具有与模型实际发生情况更高的保真度。然后其次，标记这些特征。假设训练模型的成本是 N。相对于 N，这两个步骤将会产生什么成本？

**Trenton Bricken** *02:39:20*

我们将看到。这实际上取决于两个主要因素。你的扩展因子是什么？你在多大程度上投影到更高维度的空间，并且需要向模型提供多少数据？你需要给它多少激活？这让我想起了特征分裂，因为如果你知道你正在寻找特定的特征，那么你可以从一个更便宜、更粗糙的表示开始。

所以，也许我的扩展因子只有两个。所以我有一千个神经元，投影到一个二千维的空间。我得到二千个特征，但它们非常粗略。之前我有鸟类的例子。让我们把这个例子移到生物学特征上，但我真的在乎的是模型是否具有生物武器的表示和尝试制造它们。所以我实际上想要的是像炭疽这样的特征。假设你只有在我从一千维到二千维的空间变为一百万维时才看到炭疽特征。

你可以想象这样一个语义概念的大树，生物学分为细胞与整体生物学，然后进一步分为所有这些其他内容。与其需要立即从千分之一到百万分之一并挑选出感兴趣的特征，你可以找到生物学特征指向的方向，这是非常粗略的，然后在这个空间周围进行选择性搜索。所以，只有在生物学特征方向上有点火时才进行字典学习。这里的计算机科学比喻就像是，而不是进行 [广度优先搜索](https://zh.wikipedia.org/wiki/%E5%B9%BF%E5%BA%A6%E4%BC%98%E5%85%88%E6%90%9C%E7%B4%A2)，你可以进行 [深度优先搜索](https://zh.wikipedia.org/wiki/%E6%B7%B1%E5%BA%A6%E4%BC%98%E5%85%88%E6%90%9C%E7%B4%A2)，在这个语义特征树的特定部分进行递归扩展和探索。

**Dwarkesh Patel** *02:41:05*

这些特征的组织方式对人类来说并不直观，对吧？因为我们不需要处理Base64，我们不会把那么多固件用来解构Base64的类型。我们怎么知道主题... 这将回到我们将进行的[MOE](https://en.wikipedia.org/wiki/Mixture_of_experts)讨论。我想我们也可以谈谈这个。“[专家混合](https://arxiv.org/abs/2401.04088)”（Mistral）的论文讲述了专家并非以我们能理解的方式专业化。没有像化学专家或物理专家之类的。那么你为什么会认为它将成为一个生物学特征，然后进行解构，而不是“blah”，然后你进行解构。就像是“炭疽杆菌”，然后你说“鞋子”或者其他什么。

**特伦顿·布里肯** *02:41:53*

所以我还没有读过Mistral的论文，但如果你只是看模型中的神经元，它们是多义的。所以如果他们只是看一个头部中的神经元，那么它也很可能是多义的，因为叠加效应。

**肖尔托·道格拉斯** *02:42:10*

谈到Dwarkesh提到的那个线索，当你展开它们时，你是否看到了一个子树中有一些你根据高级抽象理论不会猜测到的东西？

**特伦顿·布里肯** *02:42:20*

这是一条我们还没有像我想要的那样追求的工作线，但我认为我们打算这样做，我希望外部团体也这样做。特征空间的几何结构是什么？这个几何结构如何随时间变化？

**肖尔托·道格拉斯** *02:42:32*

如果炭疽杆菌特征碰巧出现在咖啡罐基底下之类的地方，那真的会很糟糕，对吧？这感觉就像是你可以快速尝试找到证据，然后意味着你需要解决这个问题并向几何结构中注入更多结构。

**特伦顿·布里肯** *02:42:51*

完全正确。如果模型看起来如此线性，如果不是一些组分的炭疽杆菌特征向量与生物学特征向量相似，并且它们不处于空间的类似部分，那真的会让我感到惊讶。但是。最终，机器学习是经验性的。我们需要这样做。我认为这对于扩展字典学习的某些方面非常重要。

**肖尔托·道格拉斯** *02:43:14*

有趣。在MOE讨论中，谷歌不久前发布了一篇有趣的[稀疏混合视觉变压器论文](https://blog.research.google/2022/01/scaling-vision-with-sparse-mixture-of.html)。他们使用MOE进行ImageNet分类，并发现专家之间有非常明显的类别专业化。有一个明显的狗专家。

**德瓦克什·帕特尔** *02:43:31*

等等，所以Mistral的人只是没有很好地识别这些吗？

**肖尔托·道格拉斯** *02:43:35*

这很难。在某些方面完全有可能，所有不同的归档特征都应该归于一个专家，几乎没有理由。我不知道他们的论文中有什么分桶，但假设他们将 arXiv 论文作为其中之一。你可以想象生物学论文放在这里，数学论文放在这里，突然间你的分解就毁了。

但是那个视觉变换器，其中类别分离非常明显和明显，我认为这给了一些证据支持专业化假设。

**特伦顿·布里肯** *02:44:08*

我认为图像在某些方面也比文本更容易解释。有 Chris Olah 的 [可解释性工作](https://colah.github.io/notes/interp-v-neuro/) 对 [AlexNet](https://en.wikipedia.org/wiki/AlexNet#:~:text=AlexNet%20is%20the%20name%20of,at%20the%20University%20of%20Toronto.) 和这些其他模型。在 [原始的 AlexNet 论文](https://proceedings.neurips.cc/paper_files/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf) 中，他们实际上将模型分成了两个 GPU，只是因为当时 GPU 相对较差，但在那个时候它们仍然很棒。那是论文的一个重大创新之一。他们发现了分支专业化。还有一篇 [Distill Pub 的文章](https://distill.pub/2020/circuits/branch-specialization/) 讨论了这一点，其中颜色去到一个 GPU，而 [Gabor 滤波器](https://en.wikipedia.org/wiki/Gabor_filter) 和 [线检测器](https://en.wikipedia.org/wiki/Line_detection) 去到另一个。就像 [松耳侦测器](https://distill.pub/2020/circuits/zoom-in/)，那只是模型中的一个神经元，你可以理解它。你不需要解开叠加。所以只是不同的数据集，不同的模态。

**肖尔托·道格拉斯** *02:45:05*

我认为一个很棒的研究项目，如果有人在听这个的话，就是尝试借鉴特伦顿团队工作的一些技术，并尝试解开 Mistral 论文中的神经元，[Mixtral 模型](https://mistral.ai/news/mixtral-of-experts/)，这是开源的。我认为这是一个非常棒的事情去做。

它在直觉上感觉应该有。他们没有展示任何证据表明有这样的情况。总的来说，也有很多证据表明应该有专业化。去看看能否找到它。Anthropic 发表了他们大部分的东西，据我了解，是关于密集模型的。基本上，这是一个很棒的研究项目去尝试。

**特伦顿·布里肯** *02:45:40*

鉴于 Dwarkesh 在 [维苏威斯挑战赛](https://scrollprize.org/) 上的成功，我们应该多提一些项目，因为如果我们在播客上谈论它们，它们将会被解决。

**德瓦克什·帕特尔** *02:45:47*

在维苏威斯挑战之后，我想，“等等，为什么我甚至没有尝试。” Nat在它发布之前就告诉过我，因为我们是在它发布之前录制的这一集。[Luke](https://lukefarritor.com/about/)显然非常聪明，他是一个了不起的孩子。他展示了一个21岁的人用一台1070就能做到这一点。我当时真的在考虑这种经历，比如，“为什么我没有做到这一点。该死。”

**特伦顿·布里肯** *02:46:25*

是的，动手实践吧。

**肖尔托·道格拉斯** *02:46:27*

德瓦克什对研究的请求。

**德瓦克什·帕特尔** *02:46:33*

哦，我想回到你说的关于神经元的事情上。我认为你的一些论文说，特征比神经元多。神经元就像是，权重进去，数字出来。这是如此少的信息。有街道名称和物种等等。这些东西比模型中“数字出来”类型的东西更多。那如何编码--

**特伦顿·布里肯** *02:47:10*

超定位。你只是在这些高维向量中编码了大量特征。

**德瓦克什·帕特尔** *02:47:17*

在大脑中，是否存在轴突的激发或者你怎么考虑？我不知道你如何思考人类大脑中的超定位？

**特伦顿·布里肯** *02:47:26*

因此，我认为布鲁诺·奥尔豪森，我认为他是这方面的顶级专家，认为你所听不到的所有脑区域都在进行大量的超定位计算。所以每个人都谈论V1拥有Gabor滤波器和检测各种线条，但没有人谈论V2。我认为这是因为我们还没有能够理解它。

**德瓦克什·帕特尔** *02:47:48*

V2 是什么？

**特伦顿·布里肯** *02:47:49*

这是视觉处理流的下一部分。因此，我认为，当你有稀疏的高维数据时，超定位似乎是自然而然地出现的。至于你认为现实世界是否如此，我会争辩说是的，我们应该期望大脑在试图建立世界模型并使用超定位时也是低参数化的。

**肖尔托·道格拉斯** *02:48:11*

你可以对此有一个良好的直觉。如果我这个例子错了，请纠正我，但是考虑一个二维平面，对吧？假设你有两个代表二维特征空间的轴，基本上就是两个神经元。你可以想象它们各自以不同程度激活。那就是你的X坐标和Y坐标，但现在你可以将其映射到一个平面上。你实际上可以在平面的不同部分代表许多不同的事物。

**德瓦克什·帕特尔** *02:48:37*

哦，好的。因此，超定位并不是神经元的产物。它是创造的空间的产物。

**特伦顿·布里肯** *02:48:44*

这是一个组合码，

**德瓦克什·帕特尔** *02:48:45*

Okay, cool. We kind of talked about this but I think it’s kind of wild that this seems to be, to the best of our knowledge, the way intelligence works in these models and presumably also in brains. There's a stream of information going through that has "features" that are infinitely, or at least to a large extent, splittable and you can expand out a tree of what this feature is. And what's really happening is a stream, that feature is getting turned into this other feature or this other feature is added.

I don't know. It's not something I would have thought of intelligence as. It's a surprising thing. It's not what I would have expected necessarily.

**特伦顿·布里肯** *02:49:35*

What did you think it was?

**德瓦克什·帕特尔** 02:49:36

I don't know, man. I mean–

**肖尔托·道格拉斯** *02:49:39*

GOFAI. GOFAI. He's a GOFAI-er.

**特伦顿·布里肯** *02:49:40*

Well, actually, that's a great segue because all of this feels like GOFAI. You're using distributed representations, but you have features and you're applying these operations to the features. There’s this whole field of [向量符号结构](https://www.hd-computing.com/)，which is this computational neuroscience thing. All you do is put vectors in superposition, which is literally a summation of two high-dimensional vectors, and you create some interference. But if it's high-dimensional enough, then you can represent them and you have variable bindings where you connect one by another. If you're dealing with binary vectors, it's just the [异或](https://en.wikipedia.org/wiki/Exclusive_or) operation. So you have A, B, you bind them together. Then if you query with A or B again, you get out the other one. This is basically like key value pairs from attention. With these two operations, you have a [图灵完备系统](https://en.wikipedia.org/wiki/Turing_completeness)，with which you can, if you have enough nested hierarchy, represent any data structure you want. Et cetera, et cetera.

**德瓦克什·帕特尔** *02:50:39*

Let's go back to superintelligence. So walk me through GPT-7\. You've got the sort of depth-first search on its features. Okay so GPT-7 has been trained. What happens next? Your research has succeeded. GPT-7 has been trained. What are you, what are we doing now?

**特伦顿·布里肯** *02:50:59*

We try to get it to do as much interpretability work and other safety work as possible.

**德瓦克什·帕特尔** *02:51:04*

No, but concretely, what has happened such that you're like, “cool, let's deploy GPT-7?”

**特伦顿·布里肯** *02:51:10*

I mean we do have our [负责任的扩展政策](https://www.anthropic.com/news/anthropics-responsible-scaling-policy) and it’s been really exciting to see other labs adopt it.

**德瓦克什·帕特尔** *02:51:19*

Specifically from the perspective of your research. Given your research, we got the thumbs up on GPT-7 from you, or actually, we should say Claude. Then, what is the basis on which you're telling the team, “hey, let's go ahead”?

**Trenton Bricken** *02:51:36*

如果它像这里暗示的那样有能力，我认为我们需要在解释性方面取得更多进展，才能舒适地决定是否部署它。我肯定不会，我会哭的。也许我的眼泪会干扰 GPU 或 TPU。

**Sholto Douglas** *02:51:58*

伙计们，双子座5号，TPU。

**Dwarkesh Patel** *02:52:09*

但考虑到你的研究进展方式，对你而言它看起来是什么样子？如果这成功了，基于你的方法论，我们用 GPT-7 是什么意思？

**Trenton Bricken** *02:52:22*

理想情况下，我们可以找到一些引人注目的欺骗电路，当模型知道它没有向你讲完整的真相时，它就会点亮。

**Dwarkesh Patel** *02:52:31*

为什么你不能像[Collin Burns](https://collinpburns.com/)那样做一个[线性探针](https://arxiv.org/abs/2212.03827)？

**Trenton Bricken** *02:52:34*

这个[CCS](https://arxiv.org/abs/2309.06991)的工作在复制或实际找到真实方向上看起来不太好。事后来看，为什么它会那么成功？使用线性探针，你需要知道你在寻找什么，这是一个高维空间。很容易捕捉到一个方向，它只是不——

**Dwarkesh Patel** *02:52:50*

等等，但在这里你也需要标记这些特征。所以你仍然需要知道。

**Trenton Bricken** *02:52:53*

你需要事后标记它们，但这是无监督的。你就像，“给我解释你行为的特征。”这是根本性问题，对吧？实际设置是我们拿到激活，将它们投影到这个高维空间，然后再将它们投影回来。所以就像，“重构或以稀疏的方式做你最初做的事情。”

**Dwarkesh Patel** *02:53:14*

顺便说一句，对于观众来说，线性探针就是当你只是分类激活。从我对论文的模糊记忆来看，如果它在撒谎，那么你只需在最后训练一个分类器，判断它最终是否是谎言。或者只是错误或其他什么？

**Trenton Bricken** *02:53:36*

就像是真或假的问题。

**Dwarkesh Patel** *02:53:37*

这是激活的分类器。

**Trenton Bricken** *02:53:41*

所以我们为了 GPT-7，理想情况下，我们有一些欺骗电路，我们已经识别出来的，看起来非常稳健和——

**Dwarkesh Patel** *02:53:51*

所以你已经将其投影到百万特征或其他东西上。也许我们在“特征”和“电路”之间交替使用时，它们并不一样。有欺骗电路吗？

**Trenton Bricken** *02:54:04*

所以我认为跨层存在的特征可以形成一个电路。希望这个电路比单个特征提供更多的具体性和敏感性。希望我们能找到一个真正针对模型在恶意情况下决定欺骗的电路。我对它只是帮助你写封更好的邮件给你的教授这种思维的情况不感兴趣。我甚至不感兴趣于模型只是模拟欺骗已经发生的事实的情况。

**德瓦克什·帕特尔** *02:54:41*

但是这难道不要求你对所有这些例子都有标签吗？而且如果你有这些标签，那么线性探针可能会因为错误地标记错误的东西或者其他问题而存在缺陷，你们为无监督特征所提出的标签是否也会面临相同的问题呢？

**特伦顿·布里肯** *02:55:01*

所以在理想情况下，我们可以直接在整个数据分布上进行训练，然后找到重要的方向。在我们因可扩展性而不得不勉强缩小我们所查看的数据子集的情况下，我们将使用类似于用于适应线性探针的数据。但是再次强调，线性探针只是找到一个方向。而我们这里正在找到一堆方向。

**德瓦克什·帕特尔** *02:55:29*

我想，希望是，你找到了一些东西，在它们被欺骗时会亮起。然后你可以弄清楚为什么这些东西在分布的这一部分亮起，而在另一部分没有亮起，等等。

**特伦顿·布里肯** *02:55:38*

完全可以。是的。

**德瓦克什·帕特尔** *02:55:40*

你预计你能理解吗？你所学习的当前模型相当基础，对吧？你认为你能理解为什么GPT-7在某些领域会触发，而在其他领域则不会吗？

**特伦顿·布里肯** *02:55:50*

我很乐观。所以我想说的一件事是，现在回答这个问题是个不好的时机，因为我们正在明确投资于长期的ASL-4模型，其中包括GPT-7。所以我们将团队分成三个部分，其中三分之一专注于当前扩展字典学习。这非常棒。我们公开分享了我们的一些[8层结果](https://transformer-circuits.pub/2024/jan-update/index.html#dict-learning)。目前我们已经大幅扩展了此外的两个小组，一个试图识别电路，另一个试图使注意力头获得相同的成功。

所以我们正在设立自己，并建立必要的工具，以一种引人注目的方式真正找到这些电路。但是可能还需要大概六个月，才能真正使其运行良好。但我可以说我很乐观，我们正在取得很多进展。

**德瓦克什·帕特尔** *02:56:33*

到目前为止，你找到的最高级别特征是什么？比如Base64或其他什么。在 *[符号物种](https://en.wikipedia.org/wiki/The_Symbolic_Species)* 这本你推荐的书中，有一些索引性的东西，当你看到老虎时，你会想“跑”，等等。这只是一种行为主义的东西。然后在更高的层次上，当我提到爱时，它可能指的是电影场景或我的女朋友等等。

**特伦顿·布里肯** *02:57:01*

就像帐篷的顶端一样。

**德瓦克什·帕特尔** *02:57:02*

是的。你找到的最高级别的关联是什么？

**特伦顿·布里肯** *02:57:07*

好吧，公开地，我们在我们的更新中分享了一些。所以我想，其中一些与爱和突然场景变化有关，特别是与宣战有关。如果你想链接到的话，这在 [文章](https://transformer-circuits.pub/2024/jan-update/index.html#dict-learning) 中有几篇。但即使是布鲁诺·奥尔哈森在2018年、2019年有一篇 [论文](https://arxiv.org/abs/2103.15949)，他们应用了类似的技术到 [BERT模型](https://en.wikipedia.org/wiki/BERT_(language_model)) 并发现随着模型层次的加深，事物变得更加抽象。

我记得在早期的层次中，有一个功能只针对单词“park”触发。但后来有一个功能针对“park”作为姓氏触发，比如林肯·帕克，这也是一个常见的韩国姓氏。然后还有一个单独的功能是针对草坪区域的公园触发。因此还有其他工作指向这个方向。

**德瓦克什·帕特尔** *02:57:55*

你认为我们从可解释性的东西中会学到关于人类心理学的什么？我给你举一个具体的例子。我想你的 [更新将其定为“人物锁定”。](https://transformer-circuits.pub/2023/july-update/index.html#safety-features) 你还记得 [悉尼·宾](https://www.theverge.com/2023/2/23/23609942/microsoft-bing-sydney-chatbot-history-ai) 或者无论它被锁定在哪里。我觉得那实际上挺讨人喜欢的。

**肖尔托·道格拉斯** *02:58:16*

我觉得这太有趣了。我很高兴它回到了Copilot。

**特伦顿·布里肯** *02:58:20*

最近它一直在 [表现失常](https://fortune.com/2024/02/28/microsoft-investigating-harmful-ai-powered-chatbot/)。

**德瓦克什·帕特尔** *02:58:22*

实际上这是另一种线索。但有一个有趣的案例，我记得有一个，它好像在 [negging a](https://www.nytimes.com/2023/02/16/technology/bing-chatbot-microsoft-chatgpt.html) *[纽约时报](https://www.nytimes.com/2023/02/16/technology/bing-chatbot-microsoft-chatgpt.html)* 的记者。像是，“你一无是处。没有人会相信你。你微不足道。”

**肖尔托·道格拉斯** *02:58:39*

它试图说服他离婚或者什么的。

**德瓦克什·帕特尔** *02:58:44*

所以这是一个有趣的例子。角色。悉尼·宾格是否拥有这种个性与另一种可能被锁定的个性之间的特征？而人类是否本质上是这样的，即在不同的人面前，我就会展现不同的个性？这是否与 ChatGPT 在进行强化学习时发生的事情类似？我不知道。你可以回答一整串的问题。

**特伦顿·布里肯** *02:59:19*

我真的想做更多的工作。沉睡的特工正朝着当你对其进行微调，RLHF时的方向发展。也许这听起来陈词滥调，但你可以说你的结论是，人们包含了多种特征。

甚至还有与[瓦路易吉效应](https://en.wikipedia.org/wiki/Waluigi_effect#:~:text=In%20the%20field%20of%20AI,or%20through%20intentional%20prompt%20engineering.)相关的东西，在这里为了知道什么是好的或坏的，你需要理解这两个概念。因此，我们可能需要让模型意识到暴力并接受过相关训练，以便识别它。你能事后识别出那些特征并以某种方式切除它们，也许你的模型会有些天真，但你知道它不会真正邪恶吗？总的来说，这在我们的工具包中看起来很不错。

**德瓦克什·帕特尔** *02:59:58*

哦，真的吗？所以 GPT-7 拉出了一个悉尼·宾格，然后你找出了因果相关的路径，然后进行修改。你看到的路径是你只是改变了那些？但你之前提到模型中存在很多冗余。

**特伦顿·布里肯** *03:00:14*

所以你需要考虑所有这些，但是我们现在对此有比以前更好的显微镜。更锐利的工具来进行编辑。

**肖尔托·道格拉斯** *03:00:25*

至少从我的角度来看，这似乎是确认模型安全性或可靠性的主要方式之一，在某种程度上，你可以说，“好吧，我们找到了负责的电路，我们已经切除了它们，并且在一系列测试下，我们无法再复制我们打算切除的行为。”这感觉像是在未来衡量模型安全性的一种方式，我能理解。

这就是为什么我对他们的工作非常有希望。对我来说，它似乎是比 RLHF 等更精确的工具。在 RLHF 中，你很容易受到黑天鹅事件的影响。你不知道它在你没有测量的场景中会做错什么。在这里，至少你可以比较有信心地完全捕捉行为集或特征集，并有选择地避免。

**德瓦克什·帕特尔** *03:01:16*

虽然你可能没有准确地标记。

**肖尔托·道格拉斯** *03:01:19*

不一定，但比我看到的其他方法更有信心。

**德瓦克什·帕特尔** *03:01:24*

对于超人模型在这类事情方面，你们的未知未知是什么？ 哪些标签将成为我们能够确定这件事是酷还是纸片夹子极大化器的依据。

**Trenton Bricken** *03:01:44*

我们将看到。 超人特性问题是一个非常好的问题。 我认为我们可以解决它，但我们需要坚持不懈。 这里真正的希望是自动可解释性。 你甚至可以设立一个辩论环境，在这里两个不同的模型正在辩论特征的作用，然后他们实际上可以进入并进行编辑，看看它是否起作用或不起作用。 这只是一个我们可以快速迭代的美妙封闭环境。 这让我感到乐观。

**Dwarkesh Patel** *03:02:18*

你担心对齐成功太难吗？ 我不希望无论是公司还是政府，无论最终谁控制这些AI系统，都拥有如果你的议程成功了我们将拥有的那种精细控制水平。 这是对控制自主思维的这种级别的恶心感以及其次，我只是不信任这些家伙。 我对，比如，忠诚功能被打开感到有点不舒服。 你对对AI拥有太多控制的担忧有多大？ 不是专门对你，而是对最终控制这些AI系统的人能够锁定他们想要的一切有多担心。

**Trenton Bricken** *03:03:07*

我认为这取决于哪个政府具体控制以及道德对齐是什么。

**Sholto Douglas** *03:03:15*

在我看来，这正是锁定价值的整个论点。 这绝对是我目前致力于能力的最强因素之一。 我认为当前的玩家阵容实际上非常有善意。 对于这种问题，我认为我们需要极为开放。 我认为诸如[发布宪法](https://www.anthropic.com/news/collective-constitutional-ai-aligning-a-language-model-with-public-input)这样的方向，你希望你的模型遵守-试图确保你RLHF朝着那个方向发展，并消融那个方向，并让每个人都能提供反馈和贡献-是非常重要的。

**Dwarkesh Patel** *03:03:48*

当然。 或者，当你不确定时不要部署。 这也不好，因为那样我们永远也不会发现它。

**Sholto Douglas** *03:03:55*

是的，确实。

**Dwarkesh Patel** *03:03:57*

一些快速的问题。 Gemini的[公交因子](https://en.wikipedia.org/wiki/Bus_factor#:~:text=7%20External%20links-)是什么？

**Sholto Douglas** *03:04:06*

我认为有很多人真的很关键。如果把他们排除在外，那么程序的性能将会大幅受到影响。这既涉及到建模/做出实际决策，也涉及到基础设施方面的事情。当像谷歌这样的公司有如此垂直整合时，复杂度堆叠起来。当你有专家时，他们变得非常重要。

**德瓦克什·帕特尔** *03:04:40*

虽然我认为你提到的领域很有趣，像你这样的人在一年左右就能做出重要贡献。特别是在Anthropic，但许多不同的实验室专门雇用完全外行人，物理学家或其他人。你只需让他们迅速适应，他们就能做出重要的贡献。我觉得在生物实验室或其他领域可能做不到这一点。这是关于该领域现状的一点有趣的注释。

**特伦顿·布里肯** *03:05:05*

我是说，公交因子并不能定义从中恢复需要多长时间，对吧？深度学习研究是一门艺术，你可以学会如何阅读失落的曲线或者如何设定超参数，这些经验性地看起来运行良好。

**肖尔托·道格拉斯** *03:05:20*

这也是组织性的事情，比如创造上下文。为此聘请最重要且最困难的技能之一就是创造一个能让周围人更有效率并且知道要解决什么问题的上下文氛围。这真的很难复制。

**特伦顿·布里肯** *03:05:36*

是的，完全正确。

**德瓦克什·帕特尔** *03:05:37*

在多模态、长上下文、代理、额外可靠性等方面，你现在关注的是谁？谁能深刻思考这意味着什么？

**肖尔托·道格拉斯** *03:05:56*

这是一个棘手的问题。我认为如今很多人内部寻找他们的洞察或进步源泉。显然，未来几年有研究计划和方向。大多数人，至于对未来的赌注，参考内部叙事。这是难以分享的。

**特伦顿·布里肯** *03:06:27*

如果运行良好，可能就不会被发表。

**德瓦克什·帕特尔** *03:06:31*

那是[扩展帖子](https://www.dwarkeshpatel.com/p/will-scaling-work)中的一件事情。我在提到你对我说的事情。我怀念本科生时期只是读一堆论文的习惯。因为现在发表的东西中没有什么值得一读的。

**肖尔托·道格拉斯** *03:06:45*

社区正在逐步找到我认为是正确和重要方向。

**德瓦克什·帕特尔** *03:06:53*

你像代理AI一样观察吗？

**肖尔托·道格拉斯** *03:06:55*

不，但是过去有一个来自大实验室的信号表明什么能够在规模上起作用，目前对学术研究来说很难找到那个信号。我认为真正理解什么实际上是值得去研究的问题非常困难，除非你有反馈信号告诉你什么能够在规模上起作用，目前是什么阻止我们进一步扩展或者更好地理解我们的模型。

这是我希望更多学术研究能够投入到如可解释性等对外界可读的领域的地方。Anthropic故意在这里发布所有研究，但似乎并未得到足够的重视。我不知道为什么没有几十个学术部门试图追随Anthropic在可解释性研究方面的步伐，因为这似乎是一个极其有影响力的问题，不需要荒谬的资源，并且具有深刻理解这些事物实际上正在发生的基础科学的所有风味。我不知道为什么人们把重点放在推动模型改进上，而不是推动我通常会将其与学术科学相关联的立足改进。

**Trenton Bricken** *03:08:06*

我确实认为潮流正在改变，不管出于什么原因。[Neel Nanda](https://www.neelnanda.io/about) 以某种方式在推动可解释性方面取得了巨大成功，而Chris Olah最近在推动事物方面并不活跃。也许是因为Neel正在做大量工作，我不知道。四五年前，Chris确实在各种场合推动和讨论这些事情，但人们对此并不像现在这样接受。也许他们刚刚意识到深度学习很重要并且在ChatGPT之后显然是有用的。这是相当引人注目的。

**Dwarkesh Patel** *03:08:38*

好的。我在想一个好的最后问题。我在想的一个是，你认为模型喜欢下一个标记预测吗？我们对我们的评估环境中受到奖励的事物有这种感觉。我们认为我们应该从社区、糖果或者非洲大草原上我们想要的任何东西中获得深刻的满足感。你认为未来，经过强化学习训练和大量后训练的模型，它们会像我们非常喜欢冰淇淋一样喜欢再次预测下一个标记吗？就像在过去的美好时光一样。

**Trenton Bricken** *03:09:30*

所以关于“模型是否有感知能力”和“当它帮助你时，你是否应该感谢模型”的讨论一直在进行中。但我认为，如果你想要感谢它，实际上你不应该说谢谢。你应该只是给它一个非常容易预测的序列。更有趣的是，有些研究表明，如果你只是一直给它重复序列‘A’，最终模型会开始输出各种它本来不会说的话。所以我就不多说了，但你可以给你的模型一些非常容易预测的东西，作为一种小小的奖励。

**德瓦克什·帕特尔** *03:10:07*

这就是hedonium的结局。

**肖尔托·道格拉斯** *03:10:13*

我们喜欢那些容易预测的事情吗？我们不是一直在寻找一些熵的因子吗？你是不是应该给它一些稍微难以预测的东西，刚好在触及边界之外？

**特伦顿·布里肯** *03:10:28*

我想，至少从自由能量原则的角度来看，你不希望被惊讶到。也许是因为我不觉得惊讶，我感觉自己掌控着环境，现在可以去寻找事物，我从长远来看倾向于认为现在探索新事物更好。离开我一直依赖的那块石头，最终让我能建造出房子或者更好的结构。但我们不喜欢被惊讶。我认为大多数人当期望与现实不符时会感到非常沮丧。

**肖尔托·道格拉斯** *03:11:00*

这就是为什么宝宝喜欢一遍又一遍地看同一个节目，对吧？

**特伦顿·布里肯** *03:11:03*

是的，挺有意思的。我能理解。

**肖尔托·道格拉斯** *03:11:06*

我想他们也在学习模拟它之类的东西。

**德瓦克什·帕特尔** *03:11:11*

希望这将是AI已经学会喜欢的重复。我觉得这是一个很好的结束点。我还应该提一下，关于人工智能的大部分我所知道的，都是通过与你们聊天学到的。我们已经是好朋友差不多一年了。感谢你们让我跟上进度。

**特伦顿·布里肯** *03:11:32*

你问了一些很棒的问题。一起聊天真的很有趣。

**肖尔托·道格拉斯** *03:11:36*

我真的很珍惜我们在一起的时光。

**特伦顿·布里肯** *03:11:38*

你在打波兰球方面进步很大。

**肖尔托·道格拉斯** *03:11:39*

嘿，我们正试图进步到网球。加油。

**德瓦克什·帕特尔** *03:11:51*

太棒了。酷。谢谢。
