["```\ndub add tiny-autodiff \n```", "```\n// dub.sdl\nversions \"TAUTODIFF_USE_FLOAT\" \n```", "```\n// dub.json\nversions: [\"TAUTODIFF_USE_FLOAT\"] \n```", "```\nimport rk.tautodiff;\n\nauto a = value(2);\nauto b = value(-3);\nauto c = value(10);\nauto f = value(-2);\nauto e = a * b;\nauto d = e + c;\nauto g = f * d;\n\n// backward\ng.backward();\n\n// check grad after backward\nassert(g.grad == 1);\nassert(f.grad == 4);\nassert(d.grad == -2);\nassert(e.grad == -2);\nassert(c.grad == -2);\nassert(b.grad == -4);\nassert(a.grad == 6); \n```", "```\nimport rk.tautodiff;\n\n// create solver\nauto solver = ChainSolver(0); // 0 is initial value\n\n// operations using the produced result \nsolver += 5; // 0 + 5 = 5\nsolver *= 2; // 3 * 2 = 6\n\n// append new value and work with it\nsolver ~= solver / value(2);\nassert(solver.data == 3);\n\n// backward\nsolver.backward();\nassert(solver.grad == 1);\n\n// zero grad\nsolver.zeroGrad();\nassert(solver.grad == 0);\n\n// reset\nsolver.reset();\nassert(solver.data == 0);\nassert(solver.grad == 0);\n\n// total length (allocated elements)\nassert(solver.values.length == 4); \n```", "```\n// init\nauto tape = new Tape();\nassert(tape.values == []);\nassert(tape.values.length == 0);\nassert(tape.locked == false);\nassert(!tape.isLocked);\n\n// d = a * b - c\nauto a = 5.value;\nauto b = 10.value;\nauto c = 25.value;\nauto d = a * b;\nauto e = d - c;\nassert(e.data == 25);\n\n// push\ntape.pushBack(a);\ntape ~= b;\ntape ~= [c, d, e];\nassert(tape.values == [a, b, c, d, e]);\nassert(tape.values.length == 5);\nassert(tape.lastValue.data == 25);\n\n// lock tape\ntape.lock();\n// tape ~= 24.value; // assert error: reset the tape to push new values\n\n// modify value\na.data = 6;\n\n// update tape\ntape.update();\nassert(tape.lastValue.data == 35);\n\n// reset tape to push new values\ntape.reset();\ntape ~= 35.value; // good \n```", "```\nimport rk.tautodiff;\n\nimport std.array : array;\nimport std.stdio : writefln;\nimport std.algorithm : map;\n\n// define data\nauto input = [  // binary\n    [0, 0, 0, 0], // 0\n    [0, 0, 0, 1], // 1\n    [0, 0, 1, 0], // 2\n    [0, 0, 1, 1], // 3\n    [0, 1, 0, 0], // 4\n    [0, 1, 0, 1], // 5\n    [0, 1, 1, 0], // 6\n    [0, 1, 1, 1], // 7\n    [1, 0, 0, 0], // 8\n    [1, 0, 0, 1], // 9\n    [1, 0, 1, 0], // 10\n    [1, 0, 1, 1], // 11\n    [1, 1, 0, 0], // 12\n    [1, 1, 0, 1], // 13\n    [1, 1, 1, 0], // 14\n    [1, 1, 1, 1], // 15\n].map!(x => x.map!(y => y.value).array).array;\n\nauto target = [ // 1: even, 0: odd\n    1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0\n].map!(x => x.value).array;\n\n// split train, test\nauto input_train = input[0 .. 12];\nauto input_test = input[12 .. $];\n\n// define model\nauto model = new MLP([4, 8, 1], &activateRelu, &activateSigmoid);\n\n// define loss function\nauto lossL2(Value[] preds)\n{\n    import std.algorithm : reduce;\n\n    // voldemort type\n    struct L2Loss { Value loss; float accuracy; }\n\n    // mse loss\n    Value[] losses; \n    foreach (i; 0..preds.length) losses ~= (preds[i] - target[i]) * (preds[i] - target[i]);\n    auto dataLoss = losses.reduce!((a, b) => a + b) / preds.length;\n\n    // accuracy\n    float accuracy = 0.0;\n    foreach (i; 0..preds.length) accuracy += ((preds[i].data > 0.5) == target[i].data);\n    accuracy /= preds.length;\n\n    // return voldemort type with cost and accuracy\n    return L2Loss(dataLoss, accuracy); \n}\n\n// train\nenum lr = 0.05;\nenum epochs = 100;\nforeach (epoch; 0..epochs)\n{\n    // forward\n    Value[] preds;\n    foreach (x; input_train) preds ~= model.forward(x);\n\n    // loss\n    auto l2 = lossL2(preds);\n\n    // backward\n    model.zeroGrad();\n    l2.loss.backward();\n\n    // update\n    model.update(lr);\n\n    // debug print\n    if (epoch % 10 == 0) writefln(\"epoch %3s loss %.4f accuracy %.2f\", epoch, l2.loss.data, l2.accuracy);\n}\n\n// test\nforeach (i, x; input_test) \n{\n    auto pred = model.forward(x)[0];\n    assert((pred.data > 0.5) == target[i].data);\n} \n```", "```\nepoch   0 loss 1.9461 accuracy 0.50\nepoch  10 loss 0.1177 accuracy 0.75\nepoch  20 loss 0.0605 accuracy 1.00\nepoch  30 loss 0.0395 accuracy 1.00\n...\nepoch  90 loss 0.0010 accuracy 1.00 \n```"]