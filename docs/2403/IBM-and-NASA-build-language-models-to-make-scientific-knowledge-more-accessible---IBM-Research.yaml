- en: <!--yml
  id: totrans-split-0
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  id: totrans-split-1
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-05-27 14:53:52'
  id: totrans-split-2
  prefs: []
  type: TYPE_NORMAL
- en: -->
  id: totrans-split-3
  prefs: []
  type: TYPE_NORMAL
- en: IBM and NASA build language models to make scientific knowledge more accessible
    - IBM Research
  id: totrans-split-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://research.ibm.com/blog/science-expert-LLM](https://research.ibm.com/blog/science-expert-LLM)
  id: totrans-split-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: In a new collaboration, IBM and NASA created a suite of efficient language models
    by training on scientific literature. Based on the transformer architecture, these
    models can be used in a variety of applications, from classification and entity
    extraction to question-answering and information retrieval. These models achieve
    high performance across a variety of domains and can respond promptly. We have
    open-sourced the models on Hugging Face for the benefit of the scientific and
    academic community.
  id: totrans-split-6
  prefs: []
  type: TYPE_NORMAL
- en: Transformer-based language models — which include [BERT](https://aclanthology.org/N19-1423/),
    [RoBERTa](https://arxiv.org/abs/1907.11692), and IBM’s Slate and Granite family
    of models, are invaluable for a range of natural language understanding tasks.
    What powers these models is a statistical understanding of how language works.
    They are trained on masked language modeling tasks, which learns by reconstructing
    sentences with words that have been obscured. Tokenizers, which break down words
    into units for the model, play a critical role in learning a vast vocabulary.
    While general-purpose text training is effective with popular tokenizers trained
    on datasets like Wikipedia or BooksCorpus, scientific domains require specialized
    tokenizers for terms like "phosphatidylcholine."
  id: totrans-split-7
  prefs: []
  type: TYPE_NORMAL
- en: We trained our models on 60 billion tokens on a corpus of astrophysics, planetary
    science, earth science, heliophysics, and biological and physical sciences data.
    Unlike a generic tokenizer, the one we developed is capable of recognizing scientific
    terms such as "axes" and "polycrystalline." More than half of the 50,000 tokens
    our models processed were unique compared to the open-source RoBERTa model on
    Hugging Face.
  id: totrans-split-8
  prefs: []
  type: TYPE_NORMAL
- en: The IBM-NASA models, trained on domain-specific vocabulary, outperformed the
    open RoBERTa model by 5% on the popular [BLURB](https://microsoft.github.io/BLURB/)
    benchmark, which evaluates performance on biomedical tasks. It also showed a 2.4%
    F1 score improvement on an internal scientific question-answering benchmark and
    a 5.5% improvement on internal Earth science entity recognition tests.
  id: totrans-split-9
  prefs: []
  type: TYPE_NORMAL
- en: 'Our trained encoder model can be fine-tuned for many non-generative linguistic
    tasks and can generate information-rich embeddings for document retrieval through
    [retrieval augmented generation](https://arxiv.org/abs/2005.11401) (RAG). RAG
    commonly follows a two-step framework: a retriever model first encodes the question
    and retrieves relevant documents from a vector database. These documents are then
    passed to a generative model to answer the question while ensuring fidelity to
    the retrieved document.'
  id: totrans-split-10
  prefs: []
  type: TYPE_NORMAL
- en: We built a retriever model on top of our encoder model to produce information-rich
    embeddings that map the similarity between pairs of text. Specifically, we optimize
    on a contrastive loss function, pushing the embeddings of an anchor text closer
    to those of a relevant (“positive”) document, and farther away from a random (“negative”)
    document.
  id: totrans-split-11
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在我们的编码器模型之上构建了一个检索模型，以生成信息丰富的嵌入，映射文本对之间的相似度。具体来说，我们优化了对比损失函数，将锚定文本的嵌入推向相关的（“正面”）文档，远离随机的（“负面”）文档。
- en: These models used about 268 million text pairs, including titles and abstracts,
    and questions and answers. As a result, they excel at retrieving relevant passages
    in a test set of about 400 questions that NASA curated. This is evidenced by a
    6.5% improvement over a similarly fine-tuned RoBERTa model, and a 5% improvement
    over [BGE-base](https://huggingface.co/BAAI/bge-base-en-v1.5), another popular
    open-source model for embeddings.
  id: totrans-split-12
  prefs: []
  type: TYPE_NORMAL
  zh: 这些模型使用了大约2.68亿个文本对，包括标题和摘要，以及问题和答案。因此，它们在NASA策划的约400个问题的测试集中表现出色。这体现在与类似微调的RoBERTa模型相比提高了6.5%，以及与另一种流行的开源模型[BGE-base](https://huggingface.co/BAAI/bge-base-en-v1.5)相比提高了5%。
- en: 'The significant enhancements achieved by our models can be attributed to the
    specialized training data, custom tokenizer, and training methodology. Consistent
    with IBM and NASA’s commitment to open and transparent AI, both models are available
    on Hugging Face: the [encoder model](https://huggingface.co/nasa-impact/nasa-smd-ibm-v0.1)
    can be further finetuned for applications in the space domain, while the [retriever
    model](https://huggingface.co/nasa-impact/nasa-smd-ibm-st) can be used for information
    retrieval applications for RAG. We are also collaborating with NASA to enhance
    science search engine using these models.'
  id: totrans-split-13
  prefs: []
  type: TYPE_NORMAL
  zh: 我们模型取得的显著增强归因于专门的训练数据、定制的分词器和训练方法。与IBM和NASA致力于开放和透明的AI一致，这两个模型都可以在Hugging Face上找到：[编码器模型](https://huggingface.co/nasa-impact/nasa-smd-ibm-v0.1)可以进一步用于太空领域的应用微调，而[检索模型](https://huggingface.co/nasa-impact/nasa-smd-ibm-st)可以用于RAG信息检索应用。我们还与NASA合作，利用这些模型增强科学搜索引擎。
