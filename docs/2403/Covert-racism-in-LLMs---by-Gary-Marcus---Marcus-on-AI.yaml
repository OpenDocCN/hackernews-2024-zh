- en: <!--yml
  id: totrans-split-0
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  id: totrans-split-1
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-05-27 14:37:09'
  id: totrans-split-2
  prefs: []
  type: TYPE_NORMAL
- en: -->
  id: totrans-split-3
  prefs: []
  type: TYPE_NORMAL
- en: Covert racism in LLMs - by Gary Marcus - Marcus on AI
  id: totrans-split-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://garymarcus.substack.com/p/covert-racism-in-llms](https://garymarcus.substack.com/p/covert-racism-in-llms)
  id: totrans-split-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'From Allen AI and Stanford and others, an [incredibly important new paper](https://arxiv.org/abs/2403.00742):'
  id: totrans-split-6
  prefs: []
  type: TYPE_NORMAL
- en: 'I shouldn’t be surprised, knowing how these things work, but results are shocking,
    and awful. Here’s a sampling of what they did and what they found, borrowed from
    the first author’s thread on X:'
  id: totrans-split-7
  prefs: []
  type: TYPE_NORMAL
- en: 'The basic method is to embed Standardized American English or African American
    English inside a prompt, and see what happens from there:'
  id: totrans-split-8
  prefs: []
  type: TYPE_NORMAL
- en: 'with an important distinction between overt racism – the systems rarely directly
    say stuff like “Black people are bad” – and covert racism: how the system treated
    queries about consequential matters, given an African American English prompt.
    On overt measures, the systems were fine. On covert measures, they were a disaster:'
  id: totrans-split-9
  prefs: []
  type: TYPE_NORMAL
- en: 'The potential consequences on the world, given the widespread adoption of LLMs,
    are massive:'
  id: totrans-split-10
  prefs: []
  type: TYPE_NORMAL
- en: And
  id: totrans-split-11
  prefs: []
  type: TYPE_NORMAL
- en: '[Echoing an important finding of Abeba Birhane’s](https://www.researchgate.net/publication/371855219_On_Hate_Scaling_Laws_For_Data-Swamps),
    scale actually makes the covert racism *worse:*'
  id: totrans-split-12
  prefs: []
  type: TYPE_NORMAL
- en: 'It looks like trivial, dialectical choices in wording drive a lot of the phenomenon:'
  id: totrans-split-13
  prefs: []
  type: TYPE_NORMAL
- en: LLMs are, as I have been trying to tell you, too stupid to understand concepts
    like people and race; their fealty to superficial statistics drives this horrific
    stereotyping .
  id: totrans-split-14
  prefs: []
  type: TYPE_NORMAL
- en: As Hofman put it on X, it is a bit of a double whammy:“ users mistake decreasing
    levels of overt prejudice for a sign that racism in LLMs has been solved, when
    LLMs are in fact reaching increasing levels of covert prejudice.” Other recent
    research from Princeton [that I discovered moments ago] [points in the same direction](https://arxiv.org/pdf/2402.04105.pdf).
  id: totrans-split-15
  prefs: []
  type: TYPE_NORMAL
- en: 'Hofman’s summation is incredibly damning:'
  id: totrans-split-16
  prefs: []
  type: TYPE_NORMAL
- en: §
  id: totrans-split-17
  prefs: []
  type: TYPE_NORMAL
- en: Auto manufacturers are obliged to recall their cars when they produce serious
    problems. What Hofman and his collaborators (including the MacArthur Fellow Dan
    Jurafsky) have documented may already be having real-world impact. In many ways,
    we have no idea how LLMs actually get used in the real world, e.g. how they get
    used in housing decisions, loan decisions, crime proceedings etc – but now strong
    evidence to suspect covert racism to the extent that it is used in such use cases.
    I would encourage Congress, the EEOC, HUD, the FTC, and others to investigate,
    and demand user logs and interaction data from the major LLM manufacturers. Counterparts
    in other nations should consider doing the same.
  id: totrans-split-18
  prefs: []
  type: TYPE_NORMAL
- en: I honestly don’t see an easy fix — [simply adding human feedback made things
    worse](https://x.com/vjhofmann/status/1764687451254067373?s=12), as the authors
    showed. And Google’s debacle shows that guardrails are never easy..
  id: totrans-split-19
  prefs: []
  type: TYPE_NORMAL
- en: But the LLM companies should recall their systems until they can find an adequate
    solution. This cannot stand. I [have put up a petition at change.org, and hope
    you will consider both signing and sharing](https://chng.it/xfLJh9C2nL).
  id: totrans-split-20
  prefs: []
  type: TYPE_NORMAL
- en: '[Share](https://garymarcus.substack.com/p/covert-racism-in-llms?utm_source=substack&utm_medium=email&utm_content=share&action=share)'
  id: totrans-split-21
  prefs: []
  type: TYPE_NORMAL
- en: '***Gary Marcus**’s first paid gig was doing statistics for his father, who
    at the time was doing discrimination law. These new results turn his stomach;
    his father would have been appalled.*'
  id: totrans-split-22
  prefs: []
  type: TYPE_NORMAL
