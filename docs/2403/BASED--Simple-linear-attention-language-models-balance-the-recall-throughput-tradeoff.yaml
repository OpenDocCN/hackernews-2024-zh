- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-05-27 14:38:57'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: 'BASED: Simple linear attention language models balance the recall-throughput
    tradeoff'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://www.together.ai/blog/based](https://www.together.ai/blog/based)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Introducing Based, a simple efficient architecture that combines two familiar
    primitives – sliding window attention and linear attention – to offer high-quality
    language modeling with strong associative recall capabilities! At inference time,
    Based decodes without a KV-cache, enabling a 24x throughput improvement over Transformers
    with Flash-Attention 2!
  prefs: []
  type: TYPE_NORMAL
- en: '**Overview**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In an [ICLR paper](https://hazyresearch.stanford.edu/blog/2023-12-11-zoology1-analysis)
    (and [blogpost](https://hazyresearch.stanford.edu/blog/2023-12-11-zoology1-analysis))
    we posted towards the end of last year, we share the finding that many efficient
    architectures (e.g. [Mamba](https://arxiv.org/abs/2312.00752), [RWKV](https://github.com/BlinkDL/RWKV-LM),
    [Hyena](https://arxiv.org/abs/2302.10866), [RetNet](https://arxiv.org/abs/2307.08621))
    underperform Transformers on recall, the ability to ground generations on information
    seen in-context, which is critical for in-context learning and copying. We used
    this analysis to design a new Based architecture (previewed in this [blogpost](https://hazyresearch.stanford.edu/blog/2023-12-11-zoology2-based)).
    We’re excited to share the latest progress in this line of work.
  prefs: []
  type: TYPE_NORMAL
- en: 'Our recent work digs deeper into the recall challenge. We begin by illustrating
    a fundamental tradeoff between a model’s recall abilities and the size of its
    recurrent state during generation. This analysis informs the design of Based,
    a simple recurrent architecture that outperforms prior sub-quadratic models on
    real-world recall-intensive tasks (information extraction, reading comprehension)
    and in-context learning (few-shot natural language understanding on SuperGLUE).
    At the same time, Based offers fast  generation speeds: Based is 56% and 44% faster
    at processing prompts than FlashAttention-2 and Mamba respectively (4k sequence
    length, 1.3Bn parameters). Based also offers 24x higher throughput than FlashAttention-2
    in next token prediction (generating 1024 tokens, 128 batch size, 1.3Bn parameters).'
  prefs: []
  type: TYPE_NORMAL
- en: We’re particularly excited about the *simplicity* of Based. Using just two well-known,
    familiar, attention-like building blocks, sliding window attention (with *tiny*
    window sizes) and linear attention (with Taylor series approximation of exp(QK^T)),
    we can outperform the strongest sub-quadratic architectures on language modeling
    and achieve massive speedups over optimized Transformers!
  prefs: []
  type: TYPE_NORMAL
- en: This blogpost provides an overview of our 1) analysis on recall in sub-quadratic
    architectures that leads to the Based architecture’s design and 2) how we make
    Based go brrrr!
  prefs: []
  type: TYPE_NORMAL
- en: '**Motivating analysis: the recall-memory tradeoff**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**‍**The main question driving our exploration is: *can we drastically improve
    the real-world speed and memory consumption of language models without compromising
    on recall and in-context learning capability?*'
  prefs: []
  type: TYPE_NORMAL
- en: To begin answering this question, we had to first think about what slows architectures
    down. Efficient architectures (*e.g.* Mamba) are much faster than Transformers
    at inference time (*e.g.* 5x higher throughput) in large part because they have
    a reduced memory footprint. Smaller memory footprint means larger batch sizes
    and less I/O. However, it also makes intuitive sense that reducing memory footprint
    too much could hurt a model’s capacity to recall information seen earlier in the
    sequence. This looked to us like a classic “*no free lunch”* situation, so we
    took a number of popular architectures, varied the hyper-parameters that affected
    the memory footprint, and evaluated performance on a challenging synthetic associative
    recall task.
  prefs: []
  type: TYPE_NORMAL
- en: '*The recall-memory tradeoff.* We found that all architectures obeyed a fundamental
    tradeoff: the less memory the model consumed during inference, the worse it did
    on associative recall. We focused on the *recurrent state size,* the number of
    bytes used to represent previously seen tokens when generating tokens one-by-one
    (*i.e.* recurrently).'
  prefs: []
  type: TYPE_NORMAL
- en: In attention, the *state* is commonly referred to as the KV-cache, and it grows
    with the length of the sequence. In the top right of Figure 1, we can see that
    attention performs recall perfectly, albeit at the cost of a huge recurrent state.
    Sliding window attention provides a way to cap the size of the KV-cache, but we
    found (unsurprisingly) that recall performance drops off rapidly as we reduce
    the size of the recurrent state (*e.g.* from 100% with 1MB recurrent state to
    50% with a 65 KB recurrent state) (Figure 1, light blue).
  prefs: []
  type: TYPE_NORMAL
- en: Excitingly, we found that Mamba expands the pareto frontier of the recall-memory
    tradeoff curve beyond sliding window attention. This means it is making **better
    use of limited recurrent state size** than approaches like sliding window attention.
  prefs: []
  type: TYPE_NORMAL
- en: 'The natural question is: *are there other, perhaps simpler, models that can
    also expand the pareto frontier?*'
  prefs: []
  type: TYPE_NORMAL
- en: ‍**Based*:* a simple model at the pareto frontier**
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**‍**To answer this question, we started studying why the simplest alternatives
    to softmax attention fail to strike a favorable tradeoff. As a further design
    principle, we searched for primitives that could scale well on current and future
    hardware. For instance, it would be nice if our primitives could leverage GPU
    Tensor Cores, specialized hardware on modern GPUs that can perform matrix multiplications
    (GEMMs) 16x faster for 16x16 matrices than the default (CUDA cores)!'
  prefs: []
  type: TYPE_NORMAL
- en: 'In our [ICLR paper](https://hazyresearch.stanford.edu/blog/2023-12-11-zoology1-analysis),
    we did a deep dive on why any model with a convolutional view (*e.g.* H3 or Hyena)
    will struggle on recall. Next, we considered two of the simplest efficient attention
    techniques out there: (1) [sliding](https://arxiv.org/abs/2004.05150) [window](https://arxiv.org/abs/2007.14062)
    [attention](https://mistral.ai/news/announcing-mistral-7b/) and (2) [linear attention](https://arxiv.org/abs/2006.16236)
    (*i.e.* attention without softmax).'
  prefs: []
  type: TYPE_NORMAL
- en: Our experiments on real-world language modeling (up to 1.4bn parameters) and
    synthetic associative recall suggested to us that neither primitive alone would
    suffice to navigate the pareto frontier.
  prefs: []
  type: TYPE_NORMAL
- en: We found that pure linear attention models struggled to perform precise local
    token shifts and token comparisons, skills important in recall (Fu et al., 2023;
    Arora et al., 2023a), as well as dense attention. Expanding on our findings, we
    do find that our pure linear attention model improves over earlier sub-quadratic
    architectures. Focusing on the recall-intensive slice of the Pile test set (i.e.
    next token predictions that force the model to use the prior context vs. memorized
    knowledge), the 355M pure linear attention model outperforms RWKV-v5 by 0.1 ppl
    and H3 by 2.6 ppl (Table 1, paper). Pure linear attention is even comparable to
    the Mamba architecture on this recall-slice – 2.21 ppl for Mamba vs. 2.29 for
    pure linear attention! However, we observe a sizeable gap to Transformers, which
    achieve 1.87 ppl on the recall slice.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In sliding window attention, models can only recall tokens within the sliding
    window (Figure 2, center). As we increase the window size, the recurrent state
    grows linearly and has a non-linear effect on speed during parallel training and
    inference (Figure 2, left).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: However, we find the two primitives are complementary – linear attention for
    modeling long-range token interactions and sliding window for local token interactions
    in the sequence. We combined them into a single architecture, called Based (Figure
    2, right).
  prefs: []
  type: TYPE_NORMAL
- en: Sliding window attention can perform the precise *local* shifts needed for associative
    recall. We use *tiny* window sizes (e.g. 64 in experiments) contrasting the larger
    window sizes in architectures like [Mistral-7B](https://mistral.ai/news/announcing-mistral-7b/)
    and the recently proposed [Griffin](https://arxiv.org/abs/2402.19427). Intuitively
    more attention (larger window sizes) is nice from a quality perspective, but we’d
    like to balance quality and wall-clock speed. To balance these objectives, let’s
    take a look at the left plot in the above figure. Observe that the latency of
    matrix multiplication for 16x16 vs. 64x64 matrices are roughly equal, and beyond
    64, latency grows non-linearly with the window size.  Note that the rough similarity
    between 16x16 and 64x64 is because the latter keeps the GPU tensor core occupancy
    high enough to saturate!
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Linear attention enables *global* token interactions, while maintaining a fixed
    size recurrent state. Unlike softmax attention, the size of linear attention’s
    recurrent state is a function of hyperparameters (*e.g.* choice of feature map)
    and not sequence length. This allows us to traverse the tradeoff space smoothly.
    We use a **Taylor approximation of the exponential function as the feature map**,
    that was first used in [our prior work](https://arxiv.org/abs/2402.04347) on linear
    attentions!
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Critically, the recurrent state size in Based does not grow with the sequence
    length, as it does in attention. Instead, it is determined by the linear attention
    feature dimension and the window size. **By dialing these hyperparameters, we
    can tradeoff recall for throughput and navigate the pareto frontier in Figure
    1.  **
  prefs: []
  type: TYPE_NORMAL
- en: ‍
  prefs: []
  type: TYPE_NORMAL
- en: Despite its simplicity, on real language modeling experiments (up to at least
    1.3 billion parameters), Based is competitive with Mamba in terms of overall Pile
    perplexity and standard zero-shot benchmarks from the [LM eval harness](https://github.com/EleutherAI/lm-evaluation-harness)
    (shown under Question Answering - Common).
  prefs: []
  type: TYPE_NORMAL
- en: These commonly-used zero-shot benchmarks are limited to extremely short text,
    so they don’t stress test models’ recall capabilities. To address this shortcoming,
    we [curated a small suite of *real world recall-intensive* benchmarks](https://arxiv.org/abs/2304.09433)
    that require recalling information from long documents (*e.g.* information extraction
    from [FDA documents](https://pubmed.ncbi.nlm.nih.gov/21321283/https://pubmed.ncbi.nlm.nih.gov/21321283/)
    and [raw HTML](https://paperswithcode.com/dataset/swde), and reading comprehension).Based
    is the strongest sub-quadratic architecture on these tasks, outperforming Mamba
    by 6.22 accuracy points on average. However, both Based and Mamba still underperform
    the strongest Transformer baseline, sometimes by large margins. This is consistent
    with our “no free lunch” observation above.
  prefs: []
  type: TYPE_NORMAL
- en: It’s important to note that we don’t believe Based is the only architecture
    that can operate at this point on the tradeoff curve. For example, we show in
    our paper that we can replace the sliding window attention with short-convolutions
    (filter size 3) and achieve similar performance within 0.1 perplexity points.
    We suspect that there are lots of other architectures that can also match this
    pareto frontier and we’re hopeful there are even others that can even expand beyond
    it!
  prefs: []
  type: TYPE_NORMAL
- en: ‍**How we use our fixed-size recurrent state matters too! **
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'There are many recurrent architectures that might have the same hidden state
    size, but our work highlights how the featurization (e.g. linear attention feature
    map, state update mechanism) matters as well. Our choice for the map in Based
    is surprisingly simple (*high-school calculus is all you need)*: approximating
    the exponential with a Taylor series. We compute $\phi$ such that $\phi(q) \phi(k)^T
    \approx \exp (q k^T)$. We use just the second-order Taylor series as in our prior
    work, where $\hat{\exp}(x) = 1 + x + x^2 / 2$! Note that if $x$ has dimension
    $d’$ then the  $x^2$ term will have dimension $d’^2$! The result of the key-value
    outer product (step 1 above) grows quickly in $d’$, expanding the state size for
    Based.'
  prefs: []
  type: TYPE_NORMAL
- en: '*How much does our choice of featurization vs. the expanded state size matter
    when leading to the quality of Based?*The model’s ability to *use the state effectively*
    is key. Shown in the accuracy vs. recurrent state size tradeoff curves, several
    alternatives to the Taylor map fall *below* the pareto frontier. Below we compare
    to models that expand the state size using learned projections and then apply
    popular feature maps ([Performer](https://arxiv.org/abs/2009.14794), [CosFormer](https://arxiv.org/abs/2202.08791),
    [PosELU](https://proceedings.mlr.press/v119/katharopoulos20a.html)) from the literature.
    We train these models on the [MQAR synthetic](https://github.com/HazyResearch/zoology)
    test for associative recall and sweep hyperparameters (learning rate) for all
    points shown in the plot below, finding that the Taylor map is most effective.
    This trend carries to real world experiments on the Pile language modeling corpus
    (see our paper for more).'
  prefs: []
  type: TYPE_NORMAL
- en: IO and dataflow-aware implementation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The next key question is how to make Based competitive in wall clock efficiency.
    Linear attention is theoretically more efficient than standard attention as a
    function of sequence length. However, existing implementations of linear attention
    methods are often *slower* than well-optimized attention implementations like
    [FlashAttention](https://github.com/Dao-AILab/flash-attention).
  prefs: []
  type: TYPE_NORMAL
- en: In Based, we use the 2nd degree Taylor approximation, which expands the dimension
    of the keys, leading to large state sizes and large memory consumption O(Nd’²d),
    in sequence length N, key dimension d’, and value dimension d (discussed above).
    The large resulting key-value state makes naïve implementations of Taylor linear
    attention quite slow.
  prefs: []
  type: TYPE_NORMAL
- en: First let’s revisit a bit of context on how the hardware works. GPUs have small
    amounts of fast-to-access memory (thread-specific registers, shared memory at
    the warp/32-threads level using SRAM) and large amounts of slow-to-access memory
    (HBM). It is important to reduce the number of reads-and-writes between slow HBM
    and SRAM as well as SRAM and registers to unlock efficiency. We present new IO-aware
    algorithms for the Taylor linear attention forward pass and inference that reduce
    the HBM to SRAM data movement by $O(Nd'^2)$ bytes and the SRAM to register data
    movement by $O(Nd^{2}d')$ bytes. Our algorithm allows holding the KV state *in-thread-register*
    at feature dimension d’ = 16, which we use in experiments.
  prefs: []
  type: TYPE_NORMAL
- en: Below we include a comparison between the naive Taylor attention forward pass,
    an implementation that leverages the popular linear attention kernels from [Fast
    Transformers](https://github.com/idiap/fast-transformers/blob/master/fast_transformers/attention/causal_linear_attention.py),
    and our custom kernels are shown below across batch size (sequence length 1024).
  prefs: []
  type: TYPE_NORMAL
- en: We then compare the end-to-end generation speeds of FlashAttention-2, Mamba,
    and Based 360M and 1.3Bn parameter models using our IO-aware algorithms. We hold
    the batch size to 2 for prefill, and generate 1024 tokens for next token prediction.
    Strikingly, Based achieves up to 24x higher throughput than FlashAttention-2!
  prefs: []
  type: TYPE_NORMAL
- en: '*Stay tuned!* These algorithms are implemented in an exciting new CUDA DSL
    called ThunderKittens, that’s being developed by our lab. Stay tuned for more
    on this soon – we hope the DSL improves the accessibility of CUDA development!
    In contrast to frameworks like Triton, which makes opinionated decisions about
    the supported scope of operations the user can perform, our DSL is *embedded*
    in C++. We’re really excited to share it and get your feedback! We’re cooking
    up more model artifacts alongside in the coming weeks, motivated by the question:
    *What models does the hardware want? *'
  prefs: []
  type: TYPE_NORMAL
- en: ‍
  prefs: []
  type: TYPE_NORMAL
- en: 'You can play with our checkpoints and evaluations on [Hugging Face](https://huggingface.co/collections/hazyresearch/based-65d77fb7a6f9c813c8b94339c)
    and in this code repository: [https://github.com/HazyResearch/based](https://github.com/HazyResearch/based)!
    Huge thank you to [Together AI](https://www.together.ai/), [Stanford HAI](https://hai.stanford.edu/),
    and [Stanford CRFM](https://crfm.stanford.edu/) for supporting this work! Please
    send your feedback and questions to: Simran Arora ([simarora@stanford.edu](mailto:simarora@stanford.edu)),
    Sabri Eyuboglu ([eyuboglu@stanford.edu](mailto:eyuboglu@stanford.edu)), Michael
    Zhang ([mzhang@stanford.edu](mailto:mzhang@stanford.edu)).'
  prefs: []
  type: TYPE_NORMAL
