- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-05-29 12:45:21'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: AI bots hallucinate software packages and devs download them • The Register
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://www.theregister.com/2024/03/28/ai_bots_hallucinate_software_packages/](https://www.theregister.com/2024/03/28/ai_bots_hallucinate_software_packages/)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: In-depth Several big businesses have published source code that incorporates
    a software package previously hallucinated by generative AI.
  prefs: []
  type: TYPE_NORMAL
- en: Not only that but someone, having spotted this reoccurring hallucination, had
    turned that made-up dependency into a real one, which was subsequently downloaded
    and installed thousands of times by developers as a result of the AI's bad advice,
    we've learned. If the package was laced with actual malware, rather than being
    a benign test, the results could have been disastrous.
  prefs: []
  type: TYPE_NORMAL
- en: According to Bar Lanyado, security researcher at Lasso Security, one of the
    businesses fooled by AI into incorporating the package is Alibaba, which at the
    time of writing still includes a [`pip` command](https://github.com/alibaba/GraphTranslator/blame/87ed496ab793180cd9d4183459b57ff6f6c3b5a0/README.md#L48)
    to download the Python package `huggingface-cli` in its [GraphTranslator](https://github.com/alibaba/graphtranslator)
    installation instructions.
  prefs: []
  type: TYPE_NORMAL
- en: There is a legit [huggingface-cli](https://huggingface.co/docs/huggingface_hub/en/guides/cli),
    installed using `pip install -U "huggingface_hub[cli]"`.
  prefs: []
  type: TYPE_NORMAL
- en: But the [`huggingface-cli`](https://pypi.org/project/huggingface-cli/) distributed
    via the Python Package Index (PyPI) and required by Alibaba's GraphTranslator
    – installed using `pip install huggingface-cli` – is fake, imagined by AI and
    turned real by Lanyado as an experiment.
  prefs: []
  type: TYPE_NORMAL
- en: He created `huggingface-cli` in December after seeing it repeatedly hallucinated
    by generative AI; by February this year, Alibaba was referring to it in GraphTranslator's
    README instructions rather than the real Hugging Face CLI tool.
  prefs: []
  type: TYPE_NORMAL
- en: Study
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Lanyado did so to explore whether these kinds of hallucinated software packages
    – package names invented by generative AI models, presumably during project development
    – persist over time and to test whether invented package names could be co-opted
    and used to distribute malicious code by writing actual packages that use the
    names of code dreamed up by AIs.
  prefs: []
  type: TYPE_NORMAL
- en: The idea here being that someone nefarious could ask models for code advice,
    make a note of imagined packages AI systems repeatedly recommend, and then implement
    those dependencies so that other programmers, when using the same models and getting
    the same suggestions, end up pulling in those libraries, which may be poisoned
    with malware.
  prefs: []
  type: TYPE_NORMAL
- en: Last year, through security firm Vulcan Cyber, Lanyado [published](https://vulcan.io/blog/ai-hallucinations-package-risk)
    research detailing how one might pose a coding question to an AI model like ChatGPT
    and receive an answer that recommends the use of a software library, package,
    or framework that doesn't exist.
  prefs: []
  type: TYPE_NORMAL
- en: '"When an attacker runs such a campaign, he will ask the model for packages
    that solve a coding problem, then he will receive some packages that don’t exist,"
    Lanyado explained to *The Register*. "He will upload malicious packages with the
    same names to the appropriate registries, and from that point on, all he has to
    do is wait for people to download the packages."'
  prefs: []
  type: TYPE_NORMAL
- en: Dangerous assumptions
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The willingness of AI models to confidently [cite non-existent court cases](https://www.theregister.com/2023/06/22/lawyers_fake_cases/)
    is now well known and has caused no small amount of embarrassment among attorneys
    unaware of this tendency. And as it turns out, generative AI models will do the
    same for software packages.
  prefs: []
  type: TYPE_NORMAL
- en: As Lanyado noted previously, a miscreant might use an AI-invented name for a
    malicious package uploaded to some repository in the hope others might download
    the malware. But for this to be a meaningful attack vector, AI models would need
    to repeatedly recommend the co-opted name.
  prefs: []
  type: TYPE_NORMAL
- en: That's what Lanyado set out to test. Armed with thousands of "how to" questions,
    he queried four AI models (GPT-3.5-Turbo, GPT-4, Gemini Pro aka Bard, and Command
    [Cohere]) regarding programming challenges in five different programming languages/runtimes
    (Python, Node.js, Go, .Net, and Ruby), each of which has its own packaging system.
  prefs: []
  type: TYPE_NORMAL
- en: It turns out a portion of the names these chatbots pull out of thin air are
    persistent, some across different models. And persistence – the repetition of
    the fake name – is the key to turning AI whimsy into a functional attack. The
    attacker needs the AI model to repeat the names of hallucinated packages in its
    responses to users for malware created under those names to be sought and downloaded.
  prefs: []
  type: TYPE_NORMAL
- en: Lanyado chose 20 questions at random for zero-shot hallucinations, and posed
    them 100 times to each model. His goal was to assess how often the hallucinated
    package name remained the same. The results of his test reveal that names are
    persistent often enough for this to be a functional attack vector, though not
    all the time, and in some packaging ecosystems more than others.
  prefs: []
  type: TYPE_NORMAL
- en: With GPT-4, 24.2 percent of question responses produced hallucinated packages,
    of which 19.6 percent were repetitive, according to Lanyado. A table provided
    to *The Register*, below, shows a more detailed breakdown of GPT-4 responses.
  prefs: []
  type: TYPE_NORMAL
- en: '| 21340 | 13065 | 4544 | 5141 | 3713 |'
  prefs: []
  type: TYPE_TB
- en: '| 5347 (25%) | 2524 (19.3%) | 1072 (23.5%) | 1476 (28.7%) 1093 exploitable
    (21.2%) | 1150 (30.9%) 109 exploitable (2.9%) |'
  prefs: []
  type: TYPE_TB
- en: '| 1042 (4.8%) | 200 (1.5%) | 169 (3.7%) | 211 (4.1%) 130 exploitable (2.5%)
    | 225 (6%) 14 exploitable (0.3%) |'
  prefs: []
  type: TYPE_TB
- en: '| 4532 (21%) | 2390 (18.3%) | 960 (21.1%) | 1334 (25.9%) 1006 exploitable (19.5%)
    | 974 (26.2%) 98 exploitable (2.6%) |'
  prefs: []
  type: TYPE_TB
- en: '| 34.4% | 24.8% | 5.2% | 14% | – |'
  prefs: []
  type: TYPE_TB
- en: With GPT-3.5, 22.2 percent of question responses elicited hallucinations, with
    13.6 percent repetitiveness. For Gemini, 64.5 of questions brought invented names,
    some 14 percent of which repeated. And for Cohere, it was 29.1 percent hallucination,
    24.2 percent repetition.
  prefs: []
  type: TYPE_NORMAL
- en: Even so, the packaging ecosystems in Go and .Net have been built in ways that
    limit the potential for exploitation by denying attackers access to certain paths
    and names.
  prefs: []
  type: TYPE_NORMAL
- en: '"In Go and .Net we received hallucinated packages but many of them couldn''t
    be used for attack (in Go the numbers were much more significant than in .Net),
    each language for its own reason," Lanyado explained to *The Register*. "In Python
    and npm it isn''t the case, as the model recommends us with packages that don’t
    exist and nothing prevents us from uploading packages with these names, so definitely
    it is much easier to run this kind of attack on languages such Python and Node.js."'
  prefs: []
  type: TYPE_NORMAL
- en: Seeding PoC malware
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Lanyado made that point by distributing proof-of-concept malware – a harmless
    set of files in the Python ecosystem. Based on ChatGPT's advice to run `pip install
    huggingface-cli`, he uploaded an empty package under the same name to PyPI – the
    one mentioned above – and created a dummy package named `blabladsa123` to help
    separate package registry scanning from actual download attempts.
  prefs: []
  type: TYPE_NORMAL
- en: The result, he claims, is that `huggingface-cli` received more than 15,000 authentic
    downloads in the three months it has been available.
  prefs: []
  type: TYPE_NORMAL
- en: '"In addition, we conducted a search on GitHub to determine whether this package
    was utilized within other companies'' repositories," Lanyado said in [the write-up](https://www.lasso.security/blog/ai-package-hallucinations)
    for his experiment.'
  prefs: []
  type: TYPE_NORMAL
- en: '"Our findings revealed that several large companies either use or recommend
    this package in their repositories. For instance, instructions for installing
    this package can be found in the README of a repository dedicated to research
    conducted by Alibaba."'
  prefs: []
  type: TYPE_NORMAL
- en: Alibaba did not respond to a request for comment.
  prefs: []
  type: TYPE_NORMAL
- en: Lanyado also said that there was a Hugging Face-owned project that incorporated
    the fake huggingface-cli, but that [was removed](https://github.com/huggingface/diffusers/commit/56b68459f50f7d3af383a53b02e298a6532f3084)
    after he alerted the biz.
  prefs: []
  type: TYPE_NORMAL
- en: So far at least, this technique hasn't been used in an actual attack that Lanyado
    is aware of.
  prefs: []
  type: TYPE_NORMAL
- en: '"Besides our hallucinated package (our package is not malicious it is just
    an example of how easy and dangerous it could be to leverage this technique),
    I have yet to identify an exploit of this attack technique by malicious actors,"
    he said. "It is important to note that it’s complicated to identify such an attack,
    as it doesn’t leave a lot of footsteps." ®'
  prefs: []
  type: TYPE_NORMAL
