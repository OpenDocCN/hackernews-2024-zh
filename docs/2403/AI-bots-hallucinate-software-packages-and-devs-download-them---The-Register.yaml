- en: <!--yml
  id: totrans-split-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-split-1
  prefs: []
  type: TYPE_NORMAL
  zh: 类别：未分类
- en: 'date: 2024-05-29 12:45:21'
  id: totrans-split-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-05-29 12:45:21
- en: -->
  id: totrans-split-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: AI bots hallucinate software packages and devs download them • The Register
  id: totrans-split-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: AI机器人幻觉出软件包，开发者们下载了它们 • The Register
- en: 来源：[https://www.theregister.com/2024/03/28/ai_bots_hallucinate_software_packages/](https://www.theregister.com/2024/03/28/ai_bots_hallucinate_software_packages/)
  id: totrans-split-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://www.theregister.com/2024/03/28/ai_bots_hallucinate_software_packages/](https://www.theregister.com/2024/03/28/ai_bots_hallucinate_software_packages/)
- en: In-depth Several big businesses have published source code that incorporates
    a software package previously hallucinated by generative AI.
  id: totrans-split-6
  prefs: []
  type: TYPE_NORMAL
  zh: 深入研究几家大企业已经发布了包含先前由生成式AI幻觉的软件包的源代码。
- en: Not only that but someone, having spotted this reoccurring hallucination, had
    turned that made-up dependency into a real one, which was subsequently downloaded
    and installed thousands of times by developers as a result of the AI's bad advice,
    we've learned. If the package was laced with actual malware, rather than being
    a benign test, the results could have been disastrous.
  id: totrans-split-7
  prefs: []
  type: TYPE_NORMAL
  zh: 不仅如此，还有人注意到这种频繁出现的幻觉，将这个虚构的依赖项转变为了一个真实的依赖项，结果成千上万次地被开发者下载和安装，这是由于AI的不良建议，我们已经了解到。如果这个软件包被注入了实际的恶意软件，而不是单纯的测试，后果可能是灾难性的。
- en: According to Bar Lanyado, security researcher at Lasso Security, one of the
    businesses fooled by AI into incorporating the package is Alibaba, which at the
    time of writing still includes a [`pip` command](https://github.com/alibaba/GraphTranslator/blame/87ed496ab793180cd9d4183459b57ff6f6c3b5a0/README.md#L48)
    to download the Python package `huggingface-cli` in its [GraphTranslator](https://github.com/alibaba/graphtranslator)
    installation instructions.
  id: totrans-split-8
  prefs: []
  type: TYPE_NORMAL
  zh: 据Lasso Security的安全研究员Bar Lanyado称，其中一个被AI愚弄，将这个软件包纳入项目中的企业是阿里巴巴，写作时依然在其[GraphTranslator](https://github.com/alibaba/graphtranslator)的安装说明中包含一个[`pip`命令](https://github.com/alibaba/GraphTranslator/blame/87ed496ab793180cd9d4183459b57ff6f6c3b5a0/README.md#L48)，以下载Python包`huggingface-cli`。
- en: There is a legit [huggingface-cli](https://huggingface.co/docs/huggingface_hub/en/guides/cli),
    installed using `pip install -U "huggingface_hub[cli]"`.
  id: totrans-split-9
  prefs: []
  type: TYPE_NORMAL
  zh: 存在一个合法的[huggingface-cli](https://huggingface.co/docs/huggingface_hub/en/guides/cli)，可以使用`pip
    install -U "huggingface_hub[cli]"`进行安装。
- en: But the [`huggingface-cli`](https://pypi.org/project/huggingface-cli/) distributed
    via the Python Package Index (PyPI) and required by Alibaba's GraphTranslator
    – installed using `pip install huggingface-cli` – is fake, imagined by AI and
    turned real by Lanyado as an experiment.
  id: totrans-split-10
  prefs: []
  type: TYPE_NORMAL
  zh: 但通过Python包索引（PyPI）分发的[`huggingface-cli`](https://pypi.org/project/huggingface-cli/)，是阿里巴巴的GraphTranslator所需的，可以使用`pip
    install huggingface-cli`安装，但这个包是虚构的，由AI想象出来，并由Lanyado作为实验变成了真实存在的。
- en: He created `huggingface-cli` in December after seeing it repeatedly hallucinated
    by generative AI; by February this year, Alibaba was referring to it in GraphTranslator's
    README instructions rather than the real Hugging Face CLI tool.
  id: totrans-split-11
  prefs: []
  type: TYPE_NORMAL
  zh: 他在去年12月创建了`huggingface-cli`，因为他多次看到生成式AI反复幻觉出这个包；到今年2月，阿里巴巴在GraphTranslator的README说明中提到它，而不是真正的Hugging
    Face CLI工具。
- en: Study
  id: totrans-split-12
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 研究
- en: Lanyado did so to explore whether these kinds of hallucinated software packages
    – package names invented by generative AI models, presumably during project development
    – persist over time and to test whether invented package names could be co-opted
    and used to distribute malicious code by writing actual packages that use the
    names of code dreamed up by AIs.
  id: totrans-split-13
  prefs: []
  type: TYPE_NORMAL
  zh: Lanyado这样做是为了探索这些由生成式AI模型在项目开发期间可能发明的幻觉软件包是否会随时间推移而持续存在，并测试这些虚构软件包名是否可以被利用，并通过编写实际使用这些AI虚构代码名称的软件包，来分发恶意代码。
- en: The idea here being that someone nefarious could ask models for code advice,
    make a note of imagined packages AI systems repeatedly recommend, and then implement
    those dependencies so that other programmers, when using the same models and getting
    the same suggestions, end up pulling in those libraries, which may be poisoned
    with malware.
  id: totrans-split-14
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的想法是，一些邪恶的人可能向模型询问代码建议，记录AI系统反复推荐的虚构软件包的笔记，然后实现这些依赖项，这样其他程序员在使用相同的模型并获得相同的建议时，就会拉取这些可能含有恶意软件的库。
- en: Last year, through security firm Vulcan Cyber, Lanyado [published](https://vulcan.io/blog/ai-hallucinations-package-risk)
    research detailing how one might pose a coding question to an AI model like ChatGPT
    and receive an answer that recommends the use of a software library, package,
    or framework that doesn't exist.
  id: totrans-split-15
  prefs: []
  type: TYPE_NORMAL
  zh: 去年，通过安全公司Vulcan Cyber，Lanyado [发表了](https://vulcan.io/blog/ai-hallucinations-package-risk)
    一篇详细研究，说明如何向AI模型如ChatGPT提问并得到建议，推荐使用一个不存在的软件库、包或框架。
- en: '"When an attacker runs such a campaign, he will ask the model for packages
    that solve a coding problem, then he will receive some packages that don’t exist,"
    Lanyado explained to *The Register*. "He will upload malicious packages with the
    same names to the appropriate registries, and from that point on, all he has to
    do is wait for people to download the packages."'
  id: totrans-split-16
  prefs: []
  type: TYPE_NORMAL
  zh: “当攻击者运行这样的攻击活动时，他会向模型询问解决编程问题的软件包，然后会收到一些不存在的包”，Lanyado向《The Register》解释道。“他将以相同的名称上传恶意软件包到适当的注册表中，从那时起，他只需等待人们下载这些软件包。”
- en: Dangerous assumptions
  id: totrans-split-17
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 危险的假设
- en: The willingness of AI models to confidently [cite non-existent court cases](https://www.theregister.com/2023/06/22/lawyers_fake_cases/)
    is now well known and has caused no small amount of embarrassment among attorneys
    unaware of this tendency. And as it turns out, generative AI models will do the
    same for software packages.
  id: totrans-split-18
  prefs: []
  type: TYPE_NORMAL
  zh: AI模型对于自信地[引用不存在的法院案件](https://www.theregister.com/2023/06/22/lawyers_fake_cases/)现象如今已为人熟知，并在不知情的律师中引起了相当多的尴尬。事实证明，生成式AI模型对软件包也是如此。
- en: As Lanyado noted previously, a miscreant might use an AI-invented name for a
    malicious package uploaded to some repository in the hope others might download
    the malware. But for this to be a meaningful attack vector, AI models would need
    to repeatedly recommend the co-opted name.
  id: totrans-split-19
  prefs: []
  type: TYPE_NORMAL
  zh: 正如Lanyado先前指出的，恶意分子可能会使用AI虚构的名称上传到某些仓库中的恶意包。但要使此成为有效的攻击向量，AI模型需要反复推荐被篡改的名称。
- en: That's what Lanyado set out to test. Armed with thousands of "how to" questions,
    he queried four AI models (GPT-3.5-Turbo, GPT-4, Gemini Pro aka Bard, and Command
    [Cohere]) regarding programming challenges in five different programming languages/runtimes
    (Python, Node.js, Go, .Net, and Ruby), each of which has its own packaging system.
  id: totrans-split-20
  prefs: []
  type: TYPE_NORMAL
  zh: 这正是Lanyado试图测试的。他凭借数千个“如何”问题，向四个AI模型（GPT-3.5-Turbo、GPT-4、Gemini Pro（又名Bard）和Command
    [Cohere]）查询五种不同编程语言/运行时（Python、Node.js、Go、.Net和Ruby）的编程挑战，每种语言/运行时都有其自己的打包系统。
- en: It turns out a portion of the names these chatbots pull out of thin air are
    persistent, some across different models. And persistence – the repetition of
    the fake name – is the key to turning AI whimsy into a functional attack. The
    attacker needs the AI model to repeat the names of hallucinated packages in its
    responses to users for malware created under those names to be sought and downloaded.
  id: totrans-split-21
  prefs: []
  type: TYPE_NORMAL
  zh: 事实证明，这些聊天机器人随意生成的一部分名字是持久的，有些甚至跨越不同的模型。而持久性——即虚假名字的重复——是将AI的幻想转化为功能性攻击的关键。攻击者需要AI模型在其响应用户时重复幻想包的名称，以便在这些名称下创建的恶意软件能够被寻找并下载。
- en: Lanyado chose 20 questions at random for zero-shot hallucinations, and posed
    them 100 times to each model. His goal was to assess how often the hallucinated
    package name remained the same. The results of his test reveal that names are
    persistent often enough for this to be a functional attack vector, though not
    all the time, and in some packaging ecosystems more than others.
  id: totrans-split-22
  prefs: []
  type: TYPE_NORMAL
  zh: Lanyado随机选择了20个零射击幻觉问题，并对每个模型进行了100次提问。他的目标是评估幻觉包名称保持相同的频率。他的测试结果显示，这些名称通常足够持久，以便成为功能攻击向量，尽管并非始终如此，在某些包装生态系统中可能更为突出。
- en: With GPT-4, 24.2 percent of question responses produced hallucinated packages,
    of which 19.6 percent were repetitive, according to Lanyado. A table provided
    to *The Register*, below, shows a more detailed breakdown of GPT-4 responses.
  id: totrans-split-23
  prefs: []
  type: TYPE_NORMAL
  zh: 据Lanyado称，根据GPT-4的数据，24.2%的问题回答产生了幻觉包，其中19.6%是重复的。《The Register》提供的一张表格详细列出了GPT-4响应的更多细节。
- en: '| 21340 | 13065 | 4544 | 5141 | 3713 |'
  id: totrans-split-24
  prefs: []
  type: TYPE_TB
  zh: '| 21340 | 13065 | 4544 | 5141 | 3713 |'
- en: '| 5347 (25%) | 2524 (19.3%) | 1072 (23.5%) | 1476 (28.7%) 1093 exploitable
    (21.2%) | 1150 (30.9%) 109 exploitable (2.9%) |'
  id: totrans-split-25
  prefs: []
  type: TYPE_TB
  zh: '| 5347 (25%) | 2524 (19.3%) | 1072 (23.5%) | 1476 (28.7%) 1093 exploitable
    (21.2%) | 1150 (30.9%) 109 exploitable (2.9%) |'
- en: '| 1042 (4.8%) | 200 (1.5%) | 169 (3.7%) | 211 (4.1%) 130 exploitable (2.5%)
    | 225 (6%) 14 exploitable (0.3%) |'
  id: totrans-split-26
  prefs: []
  type: TYPE_TB
  zh: '| 1042 (4.8%) | 200 (1.5%) | 169 (3.7%) | 211 (4.1%) 130 exploitable (2.5%)
    | 225 (6%) 14 exploitable (0.3%) |'
- en: '| 4532 (21%) | 2390 (18.3%) | 960 (21.1%) | 1334 (25.9%) 1006 exploitable (19.5%)
    | 974 (26.2%) 98 exploitable (2.6%) |'
  id: totrans-split-27
  prefs: []
  type: TYPE_TB
  zh: '| 4532 (21%) | 2390 (18.3%) | 960 (21.1%) | 1334 (25.9%) 1006 exploitable (19.5%)
    | 974 (26.2%) 98 exploitable (2.6%) |'
- en: '| 34.4% | 24.8% | 5.2% | 14% | – |'
  id: totrans-split-28
  prefs: []
  type: TYPE_TB
  zh: '| 34.4% | 24.8% | 5.2% | 14% | – |'
- en: With GPT-3.5, 22.2 percent of question responses elicited hallucinations, with
    13.6 percent repetitiveness. For Gemini, 64.5 of questions brought invented names,
    some 14 percent of which repeated. And for Cohere, it was 29.1 percent hallucination,
    24.2 percent repetition.
  id: totrans-split-29
  prefs: []
  type: TYPE_NORMAL
- en: Even so, the packaging ecosystems in Go and .Net have been built in ways that
    limit the potential for exploitation by denying attackers access to certain paths
    and names.
  id: totrans-split-30
  prefs: []
  type: TYPE_NORMAL
- en: '"In Go and .Net we received hallucinated packages but many of them couldn''t
    be used for attack (in Go the numbers were much more significant than in .Net),
    each language for its own reason," Lanyado explained to *The Register*. "In Python
    and npm it isn''t the case, as the model recommends us with packages that don’t
    exist and nothing prevents us from uploading packages with these names, so definitely
    it is much easier to run this kind of attack on languages such Python and Node.js."'
  id: totrans-split-31
  prefs: []
  type: TYPE_NORMAL
- en: Seeding PoC malware
  id: totrans-split-32
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Lanyado made that point by distributing proof-of-concept malware – a harmless
    set of files in the Python ecosystem. Based on ChatGPT's advice to run `pip install
    huggingface-cli`, he uploaded an empty package under the same name to PyPI – the
    one mentioned above – and created a dummy package named `blabladsa123` to help
    separate package registry scanning from actual download attempts.
  id: totrans-split-33
  prefs: []
  type: TYPE_NORMAL
- en: The result, he claims, is that `huggingface-cli` received more than 15,000 authentic
    downloads in the three months it has been available.
  id: totrans-split-34
  prefs: []
  type: TYPE_NORMAL
- en: '"In addition, we conducted a search on GitHub to determine whether this package
    was utilized within other companies'' repositories," Lanyado said in [the write-up](https://www.lasso.security/blog/ai-package-hallucinations)
    for his experiment.'
  id: totrans-split-35
  prefs: []
  type: TYPE_NORMAL
- en: '"Our findings revealed that several large companies either use or recommend
    this package in their repositories. For instance, instructions for installing
    this package can be found in the README of a repository dedicated to research
    conducted by Alibaba."'
  id: totrans-split-36
  prefs: []
  type: TYPE_NORMAL
- en: Alibaba did not respond to a request for comment.
  id: totrans-split-37
  prefs: []
  type: TYPE_NORMAL
- en: Lanyado also said that there was a Hugging Face-owned project that incorporated
    the fake huggingface-cli, but that [was removed](https://github.com/huggingface/diffusers/commit/56b68459f50f7d3af383a53b02e298a6532f3084)
    after he alerted the biz.
  id: totrans-split-38
  prefs: []
  type: TYPE_NORMAL
- en: So far at least, this technique hasn't been used in an actual attack that Lanyado
    is aware of.
  id: totrans-split-39
  prefs: []
  type: TYPE_NORMAL
- en: '"Besides our hallucinated package (our package is not malicious it is just
    an example of how easy and dangerous it could be to leverage this technique),
    I have yet to identify an exploit of this attack technique by malicious actors,"
    he said. "It is important to note that it’s complicated to identify such an attack,
    as it doesn’t leave a lot of footsteps." ®'
  id: totrans-split-40
  prefs: []
  type: TYPE_NORMAL
