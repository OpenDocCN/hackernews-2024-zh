- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-05-29 12:30:31'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: 'Inversion: fast, reliable structured LLMs - Rysana'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://rysana.com/inversion](https://rysana.com/inversion)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '**Inversion**: fast, reliable structured LLMs'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: At Rysana, we've built **Inversion** - our family of structured language models
    designed to solve the speed, reliability, and reasoning issues in previous AI
    systems.
  prefs: []
  type: TYPE_NORMAL
- en: Inference speed in characters per second, avg/90/99th percentile across 600
    extraction & reasoning tests. (higher is better)
  prefs: []
  type: TYPE_NORMAL
- en: Our first generation models are state of the art in structured tasks such as
    extraction and function calling while running up to over **100× faster** and **10×
    lower latency**, outputting **100% reliable structure** with **10,000× less overhead**
    than the best alternatives, and boasting the deepest support for typed JSON output
    available anywhere.*
  prefs: []
  type: TYPE_NORMAL
- en: Inversion models do more with less - they use less compute, less time, and less
    data to produce outputs with higher quality, reliability, and reasoning.
  prefs: []
  type: TYPE_NORMAL
- en: Be among the first to try Inversion. [Sign up for early access](/request-access).
  prefs: []
  type: TYPE_NORMAL
- en: 100× faster typed LLM inference
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We started building Inversion back in February 2023, inspired by the capability
    leap in general-purpose language models to understand systems and glue together
    human intent and machine action through natural language and structured data.
  prefs: []
  type: TYPE_NORMAL
- en: As we built products on top of these models, we found that the models were far
    too unreliable, expensive, and slow for production use in such tasks. We needed
    to create a new kind of model that could handle our workloads in the real world
    at scale.
  prefs: []
  type: TYPE_NORMAL
- en: The key insight is that **structured inference** is fundamentally accelerative
    - and that if we build models that can always reliably output structured data
    with constraints, we can massively improve both the speed and quality of the outputs.
  prefs: []
  type: TYPE_NORMAL
- en: Time-to-first-token in milliseconds, min/avg/max across 600 extraction & reasoning
    tests. (lower is better)
  prefs: []
  type: TYPE_NORMAL
- en: We set ourselves to the task of taking the same level quality of outputs from
    state of the art LLMs for workloads like function calling, [actions/workflows](/docs/lusat),
    and dynamic UI generation - down from around one minute to under 100 ms, which
    is roughly the speed at which a human feels a response is instant.
  prefs: []
  type: TYPE_NORMAL
- en: It was important to ensure the outputs would **always** match the data types
    we expected, in our case this meant being valid against a JSON schema for function
    arguments or component props.
  prefs: []
  type: TYPE_NORMAL
- en: Such a system would unlock **a Cambrian explosion of new viable AI-native applications**,
    with reliable real-time feel - everything from humanoid robots and game NPCs that
    react to complex dynamic environments to natural language interfaces and agents
    that can understand and act on complex human intent in the blink of an eye.
  prefs: []
  type: TYPE_NORMAL
- en: 'This means:'
  prefs: []
  type: TYPE_NORMAL
- en: We needed to process grammars and schemas in nearly no time,
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We needed to bring down the model's latency to nearly instant,
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We needed to accelerate inference to over 10,000 Hz.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Time to compile output constraints in microseconds, min/avg/max across 400 JSON
    schema tests. No caching, fully dynamic, identical hardware. Ratios are average
    of test ratios, not ratio of test averages. (lower is better)
  prefs: []
  type: TYPE_NORMAL
- en: The first set of components we built were the systems we use to process data
    structures and constrain model outputs with them. We invented a new kind of compiler
    and physics-based projection model that achieves **stricter** constraints than
    the best comparable libraries for typed JSON generation, with around **10,000×
    faster compilation**.
  prefs: []
  type: TYPE_NORMAL
- en: Our Inversion compiler processes a typical never-before-seen schema/grammar
    in around 100 μs (microseconds) and samples model constraints at runtime in around
    10 μs, supporting up to over **100,000 tokens per second** inference with perfectly
    structured output.
  prefs: []
  type: TYPE_NORMAL
- en: Percentage of invalid output data structure across 600 extraction & reasoning
    tests. (lower is better)
  prefs: []
  type: TYPE_NORMAL
- en: Read more about the supported types and constraints in [the docs](/docs/api/json-schema).
  prefs: []
  type: TYPE_NORMAL
- en: The developer experience of knowing you're going to get exactly the type you
    ask for is bliss.
  prefs: []
  type: TYPE_NORMAL
- en: Always-valid outputs are a game changer for structured workloads, dramatically
    improving the reliability and reasoning level of LLMs across most tasks. Inversion
    models often match or beat all other models we've tested against, even compared
    to models with over 10× as many parameters.
  prefs: []
  type: TYPE_NORMAL
- en: 'Percentage of correct outputs across 600 tests, grouped by: actions/functions,
    extraction, and typed data generation. (higher is better)'
  prefs: []
  type: TYPE_NORMAL
- en: We're expanding access to the first generation of Inversion models currently,
    and have begun building the next generation of models targeting on the order of
    100,000 Hz inference.
  prefs: []
  type: TYPE_NORMAL
- en: Accelerating structured inference
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You might be wondering - how does any of this actually work?
  prefs: []
  type: TYPE_NORMAL
- en: Why is Inversion so far ahead on multiple fronts that might otherwise seem at
    odds with each other, demanding a trade-off, like constraint and speed?
  prefs: []
  type: TYPE_NORMAL
- en: When we started with Inversion, it was actually slightly *slower* than similar
    models - barely faster than the strongest model and barely more reliable, as of
    early last spring. The first step was to achieve guaranteed structure on common
    JSON types, which is where the compiler originated.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we leveraged the compiled structures to "invert" the inference process,
    using the constraint of the output to dynamically scale up and down the amount
    of compute required to produce each token.
  prefs: []
  type: TYPE_NORMAL
- en: You can think of this as switching on and off large clusters of individual neurons
    in the model based on the position within the output structure, instead of only
    modifying token sampling. Mathematically, this is a *projection* from the full
    model onto a smaller model with undesired sublayers pruned away.
  prefs: []
  type: TYPE_NORMAL
- en: Throughput in characters per second across tests in Inversion models. Third
    party models to compare in grey. (higher is better)
  prefs: []
  type: TYPE_NORMAL
- en: We built a simple version of this last summer that gained a ~10× inference speed
    boost, but to achieve the current level of performance we had to completely rewrite
    our inference and learning systems from scratch, and train new kinds of neural
    networks to augment the transformers with dynamic acceleration.
  prefs: []
  type: TYPE_NORMAL
- en: Today, our models combine dozens of major improvements and hundreds of minor
    optimizations over the status quo at every addressable level of the stack, and
    we have been consistently improving their efficiency month by month.
  prefs: []
  type: TYPE_NORMAL
- en: '**Update**: Since we posted this announcement about a month ago, we''ve made
    another 114% improvement in inference speed, 100× improvement in sampling, and
    4× improvement in compilation speed, among many more reliability and reasoning
    improvements.'
  prefs: []
  type: TYPE_NORMAL
- en: Inversion v2 & onwards
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We're also working on a fundamentally new class of models for the next generation
    of Inversion - they are not ready yet, but so far we expect another several orders
    of magnitude improvement across the board, with as many heavy workloads as possible
    completing in single or double digit milliseconds for fractions of a cent in compute
    cost and at unprecedented reliability and quality.
  prefs: []
  type: TYPE_NORMAL
- en: 'We''re looking forward to:'
  prefs: []
  type: TYPE_NORMAL
- en: Breaking the trend of pre-trained generic models, by creating an architecture
    that can adapt to every human on the fly and constantly improve every day.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Generative UI composition on the fly, with typed sandboxed code generation in
    real time.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Improved multilingual support, with deep understanding of dialect and personal
    nuance.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Much, much more!
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We're excited to share more about it in the coming months.
  prefs: []
  type: TYPE_NORMAL
- en: Built for developers
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We''re aiming to deliver the best possible developer experience, see an example
    of various approaches to making a request through JS/TS (with `zod`), Python (with
    `pydantic`), cURL (with JSON Schema):'
  prefs: []
  type: TYPE_NORMAL
- en: Our shared future
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Together we'll create a future where technology augments human genius, trivializes
    mundane tasks, and empowers everyone to live better lives & pursue their passions.
  prefs: []
  type: TYPE_NORMAL
- en: Join us on this journey as we share insights & access to the technology we're
    building.
  prefs: []
  type: TYPE_NORMAL
- en: Reach us on X [@RysanaAI](https://x.com/RysanaAI)
  prefs: []
  type: TYPE_NORMAL
- en: Subscribe to our newsletter
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Receive an email when we publish a new post. No spam, just the good stuff.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We've created **Inversion** - a family of structured language models designed
    to solve the speed, reliability, and reasoning issues in traditional AI systems.
  prefs: []
  type: TYPE_NORMAL
- en: Our first generation models are state of the art in structured tasks such as
    extraction and function calling while running up to over **100× faster**, with
    **10× lower latency**, outputting **100% reliable structure** with **10,000× less
    overhead** than the best alternatives, and boasting the deepest support for typed
    JSON output available anywhere.
  prefs: []
  type: TYPE_NORMAL
- en: We're expanding access to the first generation of Inversion models shortly,
    and have begun building the next generation of models targeting on the order of
    100,000 Hz inference.
  prefs: []
  type: TYPE_NORMAL
- en: Be among the first to try Inversion. [Sign up for early access](/request-access).
  prefs: []
  type: TYPE_NORMAL
- en: '*All approximate numbers based on 1000 tests as of March-May 2024\. Results
    may vary. Experimental models not yet finalized. Models are tested by a single
    client making sequential requests to each model API server in rapid succession.
    Inference speed is calculated as the total output characters divided by the total
    request time. Throughput is calculated as the total output characters divided
    by the total request time minus the time to first tokens. Latency is calculated
    as the time from start of request to receiving the first tokens on the requesting
    client. Type error rate is the percentage of outputs that fail to parse according
    to the requested schema.'
  prefs: []
  type: TYPE_NORMAL
