- en: <!--yml
  id: totrans-split-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-split-1
  prefs: []
  type: TYPE_NORMAL
  zh: 类别：未分类
- en: 'date: 2024-05-29 12:30:31'
  id: totrans-split-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-05-29 12:30:31
- en: -->
  id: totrans-split-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: 'Inversion: fast, reliable structured LLMs - Rysana'
  id: totrans-split-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 反演：快速、可靠的结构化LLMs - Rysana
- en: 来源：[https://rysana.com/inversion](https://rysana.com/inversion)
  id: totrans-split-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://rysana.com/inversion](https://rysana.com/inversion)
- en: '**Inversion**: fast, reliable structured LLMs'
  id: totrans-split-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**反演**：快速、可靠的结构化LLMs'
- en: At Rysana, we've built **Inversion** - our family of structured language models
    designed to solve the speed, reliability, and reasoning issues in previous AI
    systems.
  id: totrans-split-7
  prefs: []
  type: TYPE_NORMAL
  zh: 在Rysana，我们打造了**反演**——我们的结构化语言模型系列，旨在解决先前AI系统中的速度、可靠性和推理问题。
- en: Inference speed in characters per second, avg/90/99th percentile across 600
    extraction & reasoning tests. (higher is better)
  id: totrans-split-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在600次抽取与推理测试中，字符每秒推理速度的平均/90/99百分位数。（数值越高越好）
- en: Our first generation models are state of the art in structured tasks such as
    extraction and function calling while running up to over **100× faster** and **10×
    lower latency**, outputting **100% reliable structure** with **10,000× less overhead**
    than the best alternatives, and boasting the deepest support for typed JSON output
    available anywhere.*
  id: totrans-split-9
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的第一代模型在抽取和函数调用等结构化任务中达到了现代水平，同时比最佳替代品运行速度快**100×**以上，延迟降低**10×**以上，输出**100%可靠的结构**，比任何其他地方提供的最深入支持的JSON输出还要少**10,000×**的开销。
- en: Inversion models do more with less - they use less compute, less time, and less
    data to produce outputs with higher quality, reliability, and reasoning.
  id: totrans-split-10
  prefs: []
  type: TYPE_NORMAL
  zh: 反演模型做更多，用更少——它们使用更少的计算资源、时间和数据，产生质量更高、可靠性更强、推理能力更强的输出。
- en: Be among the first to try Inversion. [Sign up for early access](/request-access).
  id: totrans-split-11
  prefs: []
  type: TYPE_NORMAL
  zh: 成为第一批尝试反演的人。[注册获取早期访问权](/request-access)。
- en: 100× faster typed LLM inference
  id: totrans-split-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 100× 更快的打字LLM推理
- en: We started building Inversion back in February 2023, inspired by the capability
    leap in general-purpose language models to understand systems and glue together
    human intent and machine action through natural language and structured data.
  id: totrans-split-13
  prefs: []
  type: TYPE_NORMAL
  zh: 我们受到通用语言模型在理解系统和通过自然语言和结构化数据将人类意图与机器行动紧密联系的能力的激发，于2023年2月开始构建反演。
- en: As we built products on top of these models, we found that the models were far
    too unreliable, expensive, and slow for production use in such tasks. We needed
    to create a new kind of model that could handle our workloads in the real world
    at scale.
  id: totrans-split-14
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们基于这些模型构建产品时，我们发现这些模型在生产环境中的可靠性、成本和速度远不足够。我们需要创建一种新型模型，可以在实际环境中大规模处理我们的工作负载。
- en: The key insight is that **structured inference** is fundamentally accelerative
    - and that if we build models that can always reliably output structured data
    with constraints, we can massively improve both the speed and quality of the outputs.
  id: totrans-split-15
  prefs: []
  type: TYPE_NORMAL
  zh: 关键洞见在于**结构化推理**本质上是加速的——如果我们构建的模型能始终可靠地输出带约束的结构化数据，我们可以大幅提升输出的速度和质量。
- en: Time-to-first-token in milliseconds, min/avg/max across 600 extraction & reasoning
    tests. (lower is better)
  id: totrans-split-16
  prefs: []
  type: TYPE_NORMAL
  zh: 首令牌时间（毫秒）在600次抽取与推理测试中的最小/平均/最大值。（数值越低越好）
- en: We set ourselves to the task of taking the same level quality of outputs from
    state of the art LLMs for workloads like function calling, [actions/workflows](/docs/lusat),
    and dynamic UI generation - down from around one minute to under 100 ms, which
    is roughly the speed at which a human feels a response is instant.
  id: totrans-split-17
  prefs: []
  type: TYPE_NORMAL
  zh: 我们致力于将同等质量的现代LLMs在诸如函数调用、[操作/工作流程](/docs/lusat)和动态UI生成等工作负载下的输出时间，从大约一分钟缩短到不到100毫秒，这大致是人类感知响应为即时的速度。
- en: It was important to ensure the outputs would **always** match the data types
    we expected, in our case this meant being valid against a JSON schema for function
    arguments or component props.
  id: totrans-split-18
  prefs: []
  type: TYPE_NORMAL
  zh: 确保输出始终与我们期望的数据类型匹配至关重要，对我们来说，这意味着对于函数参数或组件属性，必须有效地符合JSON模式。
- en: Such a system would unlock **a Cambrian explosion of new viable AI-native applications**,
    with reliable real-time feel - everything from humanoid robots and game NPCs that
    react to complex dynamic environments to natural language interfaces and agents
    that can understand and act on complex human intent in the blink of an eye.
  id: totrans-split-19
  prefs: []
  type: TYPE_NORMAL
  zh: 这样的系统将解锁**新的AI本地应用的寒武纪爆发**，具有可靠的实时感——从能够对复杂动态环境做出反应的人形机器人和游戏NPC，到能够理解并处理复杂人类意图的自然语言界面和代理，眨眼之间即可完成操作。
- en: 'This means:'
  id: totrans-split-20
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着：
- en: We needed to process grammars and schemas in nearly no time,
  id: totrans-split-21
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们需要在几乎没有时间内处理语法和模式，
- en: We needed to bring down the model's latency to nearly instant,
  id: totrans-split-22
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们需要将模型的延迟降低到几乎即时，
- en: We needed to accelerate inference to over 10,000 Hz.
  id: totrans-split-23
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们需要将推理加速到超过10,000 Hz。
- en: Time to compile output constraints in microseconds, min/avg/max across 400 JSON
    schema tests. No caching, fully dynamic, identical hardware. Ratios are average
    of test ratios, not ratio of test averages. (lower is better)
  id: totrans-split-24
  prefs: []
  type: TYPE_NORMAL
  zh: 在400个JSON模式测试中，输出约束的编译时间，微秒为单位，最小/平均/最大。无缓存，完全动态，相同硬件。比率是测试比率的平均值，而不是测试平均值的比率。
- en: The first set of components we built were the systems we use to process data
    structures and constrain model outputs with them. We invented a new kind of compiler
    and physics-based projection model that achieves **stricter** constraints than
    the best comparable libraries for typed JSON generation, with around **10,000×
    faster compilation**.
  id: totrans-split-25
  prefs: []
  type: TYPE_NORMAL
  zh: 我们构建的第一组组件是用于处理数据结构和通过它们约束模型输出的系统。我们发明了一种新型编译器和基于物理的投影模型，实现了比最佳可比库更**严格**的约束，生成类型化JSON的编译速度约为**10,000倍快**。
- en: Our Inversion compiler processes a typical never-before-seen schema/grammar
    in around 100 μs (microseconds) and samples model constraints at runtime in around
    10 μs, supporting up to over **100,000 tokens per second** inference with perfectly
    structured output.
  id: totrans-split-26
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的反向编译器处理一个典型的前所未见的模式/语法，大约在100 微秒（微秒）内，并在运行时约10 微秒内对模型约束进行采样，支持每秒超过**100,000
    个令牌**的推断，输出完美结构化。
- en: Percentage of invalid output data structure across 600 extraction & reasoning
    tests. (lower is better)
  id: totrans-split-27
  prefs: []
  type: TYPE_NORMAL
  zh: 在600次提取和推理测试中，无效输出数据结构的百分比。（越低越好）
- en: Read more about the supported types and constraints in [the docs](/docs/api/json-schema).
  id: totrans-split-28
  prefs: []
  type: TYPE_NORMAL
  zh: 阅读更多关于在[文档](/docs/api/json-schema)中支持的类型和约束。
- en: The developer experience of knowing you're going to get exactly the type you
    ask for is bliss.
  id: totrans-split-29
  prefs: []
  type: TYPE_NORMAL
  zh: 开发者体验可以确保您获得所要求的确切类型，这是一种享受。
- en: Always-valid outputs are a game changer for structured workloads, dramatically
    improving the reliability and reasoning level of LLMs across most tasks. Inversion
    models often match or beat all other models we've tested against, even compared
    to models with over 10× as many parameters.
  id: totrans-split-30
  prefs: []
  type: TYPE_NORMAL
  zh: 始终有效的输出对于结构化工作负载来说是一个革命性的变革，显著提高了LLM在大多数任务中的可靠性和推理水平。与其他我们测试过的模型相比，反向模型通常可以达到或超过，甚至是具有10倍以上参数的模型。
- en: 'Percentage of correct outputs across 600 tests, grouped by: actions/functions,
    extraction, and typed data generation. (higher is better)'
  id: totrans-split-31
  prefs: []
  type: TYPE_NORMAL
  zh: 在600次测试中，按操作/功能、提取和类型数据生成分组的正确输出百分比。（越高越好）
- en: We're expanding access to the first generation of Inversion models currently,
    and have begun building the next generation of models targeting on the order of
    100,000 Hz inference.
  id: totrans-split-32
  prefs: []
  type: TYPE_NORMAL
  zh: 我们目前正在扩展对第一代反向模型的访问，并已经开始构建下一代模型，目标推理速度约为100,000赫兹。
- en: Accelerating structured inference
  id: totrans-split-33
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 加速结构化推理
- en: You might be wondering - how does any of this actually work?
  id: totrans-split-34
  prefs: []
  type: TYPE_NORMAL
  zh: 您可能会想知道——这些都是如何实际工作的？
- en: Why is Inversion so far ahead on multiple fronts that might otherwise seem at
    odds with each other, demanding a trade-off, like constraint and speed?
  id: totrans-split-35
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么反向模型在多个可能看似相互矛盾的前沿领域如约束和速度方面领先？这些领域通常需要权衡。
- en: When we started with Inversion, it was actually slightly *slower* than similar
    models - barely faster than the strongest model and barely more reliable, as of
    early last spring. The first step was to achieve guaranteed structure on common
    JSON types, which is where the compiler originated.
  id: totrans-split-36
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们刚开始使用反向模型时，它实际上比类似模型略慢——几乎比最强模型快不了多少，也不太可靠，截至去年春初。第一步是在常见JSON类型上实现保证的结构，这也是编译器的起源。
- en: Next, we leveraged the compiled structures to "invert" the inference process,
    using the constraint of the output to dynamically scale up and down the amount
    of compute required to produce each token.
  id: totrans-split-37
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们利用编译结构来“反转”推理过程，使用输出的约束动态调节所需计算量，从而增减生成每个令牌的计算量。
- en: You can think of this as switching on and off large clusters of individual neurons
    in the model based on the position within the output structure, instead of only
    modifying token sampling. Mathematically, this is a *projection* from the full
    model onto a smaller model with undesired sublayers pruned away.
  id: totrans-split-38
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以将此视为根据输出结构中的位置开关大型神经元集群，而不仅仅是修改令牌采样。数学上，这是从完整模型到剪裁掉不需要的子层的较小模型的*投影*。
- en: Throughput in characters per second across tests in Inversion models. Third
    party models to compare in grey. (higher is better)
  id: totrans-split-39
  prefs: []
  type: TYPE_NORMAL
  zh: 在反向模型测试中的字符每秒吞吐量。灰色表示比较的第三方模型。（越高越好）
- en: We built a simple version of this last summer that gained a ~10× inference speed
    boost, but to achieve the current level of performance we had to completely rewrite
    our inference and learning systems from scratch, and train new kinds of neural
    networks to augment the transformers with dynamic acceleration.
  id: totrans-split-40
  prefs: []
  type: TYPE_NORMAL
  zh: 去年夏天，我们建立了一个简化版，推理速度提升了约10倍，但要达到当前的性能水平，我们不得不从头开始完全重写我们的推理和学习系统，并训练新型神经网络以加速变压器的动态增强。
- en: Today, our models combine dozens of major improvements and hundreds of minor
    optimizations over the status quo at every addressable level of the stack, and
    we have been consistently improving their efficiency month by month.
  id: totrans-split-41
  prefs: []
  type: TYPE_NORMAL
  zh: 今天，我们的模型在每个可寻址的堆栈层面上都融合了几十项主要改进和数百项次要优化，而且我们已经在每个月保持了他们的效率一致提升。
- en: '**Update**: Since we posted this announcement about a month ago, we''ve made
    another 114% improvement in inference speed, 100× improvement in sampling, and
    4× improvement in compilation speed, among many more reliability and reasoning
    improvements.'
  id: totrans-split-42
  prefs: []
  type: TYPE_NORMAL
  zh: '**更新**：自从我们大约一个月前发布了这则公告以来，我们在推理速度方面又提升了114%，采样效率提高了100倍，编译速度提升了4倍，此外还有更多可靠性和推理能力的改进。'
- en: Inversion v2 & onwards
  id: totrans-split-43
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Inversion v2及以后版本
- en: We're also working on a fundamentally new class of models for the next generation
    of Inversion - they are not ready yet, but so far we expect another several orders
    of magnitude improvement across the board, with as many heavy workloads as possible
    completing in single or double digit milliseconds for fractions of a cent in compute
    cost and at unprecedented reliability and quality.
  id: totrans-split-44
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还在研发一类全新的模型，面向Inversion的下一代 - 它们目前还没有准备好，但我们预计在各个方面实现另一个数量级的提升，希望尽可能多的重型工作负载在单个或双位数毫秒内完成，计算成本只需几分之一，并且在前所未有的可靠性和质量上。
- en: 'We''re looking forward to:'
  id: totrans-split-45
  prefs: []
  type: TYPE_NORMAL
  zh: 我们期待：
- en: Breaking the trend of pre-trained generic models, by creating an architecture
    that can adapt to every human on the fly and constantly improve every day.
  id: totrans-split-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 打破预先训练的通用模型趋势，通过创建一种能够即时适应每个人并不断提升的架构。
- en: Generative UI composition on the fly, with typed sandboxed code generation in
    real time.
  id: totrans-split-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实时生成UI组合，具备类型沙箱化代码生成。
- en: Improved multilingual support, with deep understanding of dialect and personal
    nuance.
  id: totrans-split-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 提升多语言支持，深入理解方言和个人细微差异。
- en: Much, much more!
  id: totrans-split-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 还有更多更多！
- en: We're excited to share more about it in the coming months.
  id: totrans-split-50
  prefs: []
  type: TYPE_NORMAL
  zh: 我们很高兴在接下来的几个月内分享更多关于此的信息。
- en: Built for developers
  id: totrans-split-51
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 为开发者打造
- en: 'We''re aiming to deliver the best possible developer experience, see an example
    of various approaches to making a request through JS/TS (with `zod`), Python (with
    `pydantic`), cURL (with JSON Schema):'
  id: totrans-split-52
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的目标是提供最佳的开发者体验，查看通过JS/TS（使用`zod`）、Python（使用`pydantic`）、cURL（使用JSON Schema）进行请求的各种方法示例：
- en: Our shared future
  id: totrans-split-53
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 我们的共同未来
- en: Together we'll create a future where technology augments human genius, trivializes
    mundane tasks, and empowers everyone to live better lives & pursue their passions.
  id: totrans-split-54
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将共同创造一个技术增强人类天才、使琐碎任务变得微不足道并赋予每个人更好生活和追求激情的未来。
- en: Join us on this journey as we share insights & access to the technology we're
    building.
  id: totrans-split-55
  prefs: []
  type: TYPE_NORMAL
  zh: 加入我们的旅程，分享我们正在构建的技术见解和访问权限。
- en: Reach us on X [@RysanaAI](https://x.com/RysanaAI)
  id: totrans-split-56
  prefs: []
  type: TYPE_NORMAL
  zh: 联系我们 [@RysanaAI](https://x.com/RysanaAI)
- en: Subscribe to our newsletter
  id: totrans-split-57
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 订阅我们的新闻简报
- en: Receive an email when we publish a new post. No spam, just the good stuff.
  id: totrans-split-58
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们发布新文章时，您将收到一封电子邮件。没有垃圾邮件，只有好东西。
- en: Summary
  id: totrans-split-59
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 总结
- en: We've created **Inversion** - a family of structured language models designed
    to solve the speed, reliability, and reasoning issues in traditional AI systems.
  id: totrans-split-60
  prefs: []
  type: TYPE_NORMAL
  zh: 我们创造了**Inversion** - 一系列设计用于解决传统AI系统中速度、可靠性和推理问题的结构化语言模型。
- en: Our first generation models are state of the art in structured tasks such as
    extraction and function calling while running up to over **100× faster**, with
    **10× lower latency**, outputting **100% reliable structure** with **10,000× less
    overhead** than the best alternatives, and boasting the deepest support for typed
    JSON output available anywhere.
  id: totrans-split-61
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的第一代模型在结构化任务（如提取和函数调用）方面是目前的最新技术，推理速度高达**100倍**，延迟降低**10倍**，输出**100%可靠结构**，比最佳替代方案的开销减少**10,000倍**，并且提供了现有任何地方最深度的类型化JSON输出支持。
- en: We're expanding access to the first generation of Inversion models shortly,
    and have begun building the next generation of models targeting on the order of
    100,000 Hz inference.
  id: totrans-split-62
  prefs: []
  type: TYPE_NORMAL
  zh: 我们很快将扩展访问第一代Inversion模型，并已开始构建目标为每秒推理100,000次的下一代模型。
- en: Be among the first to try Inversion. [Sign up for early access](/request-access).
  id: totrans-split-63
  prefs: []
  type: TYPE_NORMAL
  zh: 成为第一批尝试逆转的人。[注册获取早期访问权限](/request-access)。
- en: '*All approximate numbers based on 1000 tests as of March-May 2024\. Results
    may vary. Experimental models not yet finalized. Models are tested by a single
    client making sequential requests to each model API server in rapid succession.
    Inference speed is calculated as the total output characters divided by the total
    request time. Throughput is calculated as the total output characters divided
    by the total request time minus the time to first tokens. Latency is calculated
    as the time from start of request to receiving the first tokens on the requesting
    client. Type error rate is the percentage of outputs that fail to parse according
    to the requested schema.'
  id: totrans-split-64
  prefs: []
  type: TYPE_NORMAL
  zh: '*所有近似数字基于2024年3月至5月的1000次测试。结果可能有所不同。实验模型尚未最终确定。模型由单个客户端进行测试，依次向每个模型API服务器快速发出请求。推断速度计算为总输出字符数除以总请求时间。吞吐量计算为总输出字符数除以总请求时间减去首个标记时间。延迟计算为从请求开始到在请求客户端接收到首个标记的时间。类型错误率是根据请求的模式无法解析的输出百分比。'
