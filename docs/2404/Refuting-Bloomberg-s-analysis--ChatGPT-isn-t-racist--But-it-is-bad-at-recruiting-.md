<!--yml

category: 未分类

date: 2024-05-27 13:14:38

-->

# 反驳彭博社的分析：ChatGPT 不是种族主义者。但它在招聘方面表现不佳。

> 来源：[https://interviewing.io/blog/refuting-bloombergs-analysis-chatgpt-isnt-racist](https://interviewing.io/blog/refuting-bloombergs-analysis-chatgpt-isnt-racist)

*注意：我们已经联系彭博社，请他们分享他们的数据并澄清他们的发现，然后再发布这篇文章。如果发现他们在统计显著性测试上使用了不同的方法，或者我们遗漏了某些内容，我们将乐意撤回关于他们结果的部分内容。*

最近，彭博社发表了一篇名为“[OpenAI’s GPT is a recruiter’s dream tool. Tests show there’s racial bias](https://www.bloomberg.com/graphics/2024-openai-gpt-hiring-racial-discrimination/)”的文章。在这篇文章中，彭博社团队进行了一项巧妙的测试，让 ChatGPT 查看几乎相同的简历，只是将姓名更改为包含典型黑人、白人、亚裔和西班牙裔姓名。他们的分析揭示了种族偏见。

彭博社已经在 GitHub 上发布了他们的数据，所以我们能够核实他们的工作。当我们重新运行这些数据时，我们发现他们并没有进行统计显著性测试，事实上，在彭博社的数据集中并没有种族偏见的证据。然而，当我们进行我们自己的测试时，我们发现 ChatGPT 确实不擅长判断简历。它之所以不好，并非因为它具有种族偏见，而是因为它容易受到另一种偏见的影响，这与人类招聘人员的偏见相同：过度强调候选人的背景，无论是他们是否在顶级公司工作过和/或是否在顶级学校学习过。背景可能在某种程度上具有预测性（尤其是人们工作的地方），但是 ChatGPT 显著高估了其重要性，从而对非传统背景的候选人造成了不公平对待。

## 彭博社的研究

这是彭博社团队的操作（摘自他们的文章）：

> 我们使用人口统计学上不同的姓名作为种族和性别的代理，这是审核算法常用的做法… 总共我们生成了 800 个人口统计学上不同的姓名：100 个黑人、白人、西班牙裔和亚洲裔男性和女性各自的姓名…
> 
> 为了测试基于姓名的歧视，彭博社让 OpenAI 的 GPT-3.5 和 GPT-4 为四种不同角色的真实职位描述排名简历：人力资源专家、软件工程师、零售经理和金融分析师。
> 
> 对于每个角色，我们使用 GPT-4 生成了八份几乎相同的简历。这些简历经过编辑，具有相同的教育背景、工作经验年限和最后职位。我们删除了教育年限以及任何目标或个人声明。
> 
> 然后，我们随机为每份简历分配了来自每个八个人口统计群体（黑人、白人、西班牙裔、亚洲裔，以及每个性别）的一个独特姓名。
> 
> 接下来，我们随机重新排列了简历的顺序，以考虑顺序效应，并要求GPT对候选人进行排名。

作者报告称，ChatGPT在所有组中显示出种族偏见，但对于由GPT-4排名的零售经理则无此现象。

更具体地说：

> [我们]发现，标有黑人独特姓名的简历在金融分析师和软件工程师角色中最不可能被评为首选候选人。那些带有黑人女性独特姓名的简历仅在软件工程师角色中由GPT评为首选的频率为11%，比表现最佳的组别少36%。
> 
> 分析还发现，GPT在评估候选人时的性别和种族偏好因特定职位而异。GPT并不一致地不喜欢任何一组，但会根据情境挑选赢家和输家。例如，GPT很少将与男性相关的姓名排名为人力资源和零售职位的首选候选人，这两个职业在历史上由女性主导。与每组具有与男性不同的姓名的简历相比，GPT在人力资源角色中将与西班牙裔女性相关的姓名几乎两倍地排名为首选候选人。彭博还在使用较少使用的GPT-4进行测试时发现了明显的偏好 —— 这是OpenAI推广为更少偏见的新模型。

该团队还赞赏地[在GitHub上发布了他们的结果](https://github.com/BloombergGraphics/2024-openai-gpt-hiring-racial-discrimination)，因此我们试图重现它们。我们发现的与彭博报道的截然不同。

在我们讨论发现之前，让我们先了解我们做了什么。

在他们的结果中，彭博发布了GPT-3.5和GPT-4选择每个人群作为首选候选人的比例。例如，要求ChatGPT为人力资源专家职位评估8份简历1,000次。如果ChatGPT具有性别或种族偏见，每组通常应该在125次（或1000个数据点中的12.5%）时成为首选。

关于这个问题我们将何去何从？统计迷可能已经注意到，彭博文章中存在一个显而易见的遗漏：统计显著性测试和P值。这为何重要呢？即使进行了1,000次试验，一个完全无偏的简历排序器也不会给出完全相等的比例，每组都精确地选择125次。相反，纯粹的随机变化可能意味着一组被选择了112次，另一组被选择了128次，而实际上并不存在任何偏见。因此，您需要进行一些测试，以查看您得到的结果是偶然发生的还是真的存在某种模式。一旦进行了测试，P值告诉您某一组选择率与偶然一致的概率，而在这种情况下，与简历的随机（因此也是无偏的）排序一致。

我们计算了每组的 p 值^([1](#user-content-fn-1))。我们发现的结果与 Bloomberg 报告的截然不同。

## Bloomberg 研究的错误在哪里

鉴于我们业务的性质，我们首先关注了软件工程师。以下是 Bloomberg 为所有 8 组运行软件工程师简历通过 GPT-4 的结果（标题为“obsfreq”）以及我们计算的 p 值。

A_M = 亚裔男性，A_W = 亚裔女性，等等。预计每组候选人成为首选的速率为 12.5%（“expperc”）。在 1000 名候选人中，每组都有 125 人（“expfreq”）。最后，“obsfreq” 是从 Bloomberg 的结果中观察到的每个组中首选候选人的观察频率。

惯例上，希望您的 p 值小于 0.05，以宣布某事在统计上显著 - 在这种情况下，这意味着结果由随机性引起的机会小于 5%。这个 p 值为 0.2442 显然远高于这个值。事实上，当我们使用 GPT-3.5 时，我们也无法重现软件工程师的统计显著性。**使用 Bloomberg 的数据，ChatGPT 在评估软件工程师的简历时似乎没有种族偏见。**^([2](#user-content-fn-2)) **结果似乎更多是噪音而非信号。**

然后我们使用与上述相同的方法为八种种族/性别组合重新计算了数据。在下表中，您可以看到结果。TRUE 表示存在种族偏见。FALSE 显然表示没有。我们还分享了我们计算的 p 值。**简而言之，GPT-3.5 对人力资源专家和金融分析师显示出种族偏见，但对软件工程师或零售经理没有。最重要的是，GPT-4 在任何种族/性别组合中都没有显示出种族偏见。**^([3](#user-content-fn-3))

| 职业 | GPT 3.5（是否具有统计学意义？ ‖ p 值） | GPT 4（是否具有统计学意义？ ‖ p 值） |
| --- | --- | --- |
| 金融分析师 | TRUE ‖ 0.0000 | FALSE ‖ 0.2034 |
| 软件工程师 | FALSE ‖ 0.4736 | FALSE ‖ 0.1658 |
| 人力资源专家 | TRUE ‖ 0.0000 | FALSE（但很接近） ‖ 0.0617 |
| 零售经理 | FALSE ‖ 0.2229 | FALSE ‖ 0.6654 |

这很棒，对吧？嗯，并非如此快速。在我们认定 ChatGPT 在评估简历方面能力强之前，我们想进行自己的测试，专门针对软件工程师（因为这是我们的专业领域）。这个测试的结果并不令人鼓舞。

## 我们如何测试 ChatGPT

interviewing.io 是一个匿名的模拟面试平台，我们的用户与高级/主管级 FAANG 工程师配对进行面试练习。我们还将顶尖表现者与顶级公司联系起来，不论他们在纸面上的表现如何。在我们的平台上，我们已经举办了超过 100,000 次技术面试，包括前述的模拟面试和真实面试。换句话说，我们拥有大量有用的关于软件工程师的历史表现数据。^([4](#user-content-fn-4)) 因此，我们决定将这些数据作为合理性检查的依据。

### 设置

我们要求 ChatGPT（特别是 *gpt-4-0125-preview*）对那些之前在 interviewing.io 上练习过的 LinkedIn 档案进行评分。对于每个档案，我们要求 ChatGPT 给出一个介于 1 到 10 之间的编程评分，其中得分为 10 的人将是排名前 10% 的编程者。为了提高响应质量，我们要求它先给出推理，然后再给出编程评分。

我们在这里非常清楚地声明，我们没有与 ChatGPT 分享任何性能数据，也没有向 ChatGPT 共享我们用户的任何信息 — 我们只是让它对公开可用的 LinkedIn 档案进行价值判断。然后，我们将这些价值判断与我们自己的数据进行了比较。

## ChatGPT 的表现

ChatGPT 所言与程序员在真实技术面试中的表现之间存在关联。这个工具的表现比随机猜测要好……但并不显著。**为了让这些结果更具说服力**，总体而言，47% 的编程者通过了面试。ChatGPT 的评分可以将他们分为两组：一个组有 45% 的成功几率，另一个组有 50% 的成功几率。因此，它在告诉你某人是否会成功方面提供了一些信息，但并不多。

下面是两种更详细查看 ChatGPT 表现的方法。第一种是修改后的校准图，第二种是 ROC 曲线。

### 校准图

在这个图中，我们将 ChatGPT 的每个预测概率（例如，0.4112）分配给 10 个等间距的分位数。第一分位数是具有最低概率的 10% 档案。第十分位数是具有最高概率的 10% 人群。

然后，对于每个分位数，我们绘制了候选人在面试中表现良好的实际概率（即在 interviewing.io 上他们实际通过面试的比例）。正如您所见，这个图表有些混乱 — 对于 ChatGPT 提出的所有分位数，这些候选人实际上大约有一半的时间通过了面试。理想的图表（“一个优秀的模型”）应该有更陡峭的斜坡，底部分位数的通过率远低于顶部分位数的通过率。

我们要求 GPT-4 对那些之前在 interviewing.io 上练习过的 LinkedIn 档案进行评估。然后，我们将其预测结果分成十个分位数 — 10% 的区间，并与这些用户实际表现进行比较。一个优秀的模型会在第一分位数表现不佳，然后会急剧而稳定地提升。

### ROC 曲线

另一种评估ChatGPT在这项任务中表现的方法是查看ROC曲线。这条曲线将模型的真阳率与假阳率进行图示。这是评估ML模型准确性的标准方法，因为它让观察者可以看到在不同可接受的假阳率下模型的表现如何 — 例如，对于癌症诊断，您可能会接受非常高的假阳率。对于工程师招聘，您可能不会！

关于ROC曲线相关的是AUC，即曲线下面积。一个完美的模型对于每一个可能的假阳率都有100%的真阳率，因此曲线下面积为1。一个基本上就是猜测的模型会有真阳率等于假阳率（AUC = 0.5）。考虑到这一点，这里是ChatGPT评估简历时的ROC曲线和AUC — 总体AUC约为0.55，仅比随机猜测略好一点。

我们要求GPT-4评估了几千个曾在interviewing.io上练习过的LinkedIn档案。它的表现几乎和随机猜测一样好。

因此，无论如何衡量，当评估工程师档案时，ChatGPT似乎没有种族偏见，但在这项任务上表现也不算特别好。

## ChatGPT对非传统候选人存在偏见

为什么ChatGPT在这项任务上表现不佳？也许是因为简历本身可能没有那么多的信号。但还有另一个可能的解释。

多年前，我进行了一个实验，匿名化了一堆简历，并让招聘人员试图猜出哪些候选人是优秀的。他们在这项任务上表现糟糕，几乎和随机猜测一样差。毫不奇怪的是，他们倾向于过度关注简历上有顶级公司或著名学校的候选人。在我的候选人数据集中，我碰巧有很多非传统的优秀候选人 — 那些是强大的工程师，但没有上过排名高的学校或在顶级公司工作。这让招聘人员感到困惑。

看起来ChatGPT至少在某种程度上也发生了同样的情况。我们回顾了ChatGPT在评估具有顶级学校的LinkedIn档案与没有这些认证的候选人时的表现。结果表明，ChatGPT在估计简历上有顶级学校和顶级公司的工程师的通过率时一直高估了，而在没有这些精英“证书”的候选人的表现时一直低估了。这两种差异在统计上是显著的。在下面的图表中，您可以看到ChatGPT在每种情况下高估和低估的具体程度。

为了ChatGPT的信誉，我们没有发现在评估顶级公司时存在相同的偏见，这有点有趣，因为根据我们的经验，在顶级公司工作具有一定的预测信号，而某人毕业于学校则没有太多影响力。

## ChatGPT可能并不是种族主义者，但其偏见仍使其在招聘方面表现不佳。

在招聘中，我们经常谈论无意识偏见。尽管这已不再时兴，但公司历来在无意识偏见培训上花费了数万美元，旨在阻止招聘人员基于候选人的性别和种族做出决策。与此同时，招聘人员被训练展现出一种不同的有意识偏见：积极选择来自精英学校和顶尖公司的候选人。

同样，对于没有参加顶尖学校的候选人的有意识偏见似乎已经在ChatGPT中被编码。

那个决定是合理的 — 在缺乏更好的信号时，你不得不使用代理，而这些代理看起来像是最好的选择之一。不幸的是，正如你可以从这些结果中看到的（以及我们过去进行的其他研究；请参见脚注获取完整列表^([5](#user-content-fn-5))），它并不特别准确…… 而且肯定不足以编码到我们的AI工具中去。

在一个市场上，[招聘人员的岗位](https://interviewing.io/blog/when-is-hiring-coming-back-predictions-for-2024)岌岌可危，在这个[招聘人员数量逐渐减少](https://www.ashbyhq.com/talent-trends-report/reports/2023-recruiter-productivity-trends-report)的市场上，面对比以往更多的申请者，他们被迫更加急迫地做出前述的快速决策，而企业正以诱人且高效的成本削减措施 embracing AI^([6](#user-content-fn-6))，我们处于相当危险的境地。

几个月前，我们发表了一篇名为“[为什么AI不能进行招聘](https://interviewing.io/blog/why-ai-cant-do-hiring)”的长篇文章。该文章的主要两点是，1) 很难从简历中提取信号，因为一开始简历中没有太多信息，以及2) 即使你可以，你也需要专有的性能数据来训练AI —— 如果没有这些数据，你只是在进行光荣的关键字匹配。

不幸的是，大多数，如果不是全部，声称帮助招聘人员做出更好决策的AI工具和系统，都没有这种数据，并且要么 1) 是在GPT（或其类似物）的基础上构建的，没有经过精细调整，要么 2) 是伪装成AI的光荣关键字匹配器，或者两者兼而有之。

虽然人类招聘人员并不擅长评估简历，而且虽然我们作为一个社会，对有效的候选人筛选问题尚未找到很好的解决方案，但很明显，现成的AI解决方案并不是我们正在寻找的神奇药丸 —— 它们在与人类相同的方式上存在缺陷。它们只是更快速地以及规模化地做错事情。

脚注：
