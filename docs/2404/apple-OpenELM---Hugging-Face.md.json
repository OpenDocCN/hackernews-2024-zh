["```\n from transformers import AutoModelForCausalLM\n\nopenelm_270m = AutoModelForCausalLM.from_pretrained(\"apple/OpenELM-270M\", trust_remote_code=True)\nopenelm_450m = AutoModelForCausalLM.from_pretrained(\"apple/OpenELM-450M\", trust_remote_code=True)\nopenelm_1b = AutoModelForCausalLM.from_pretrained(\"apple/OpenELM-1_1B\", trust_remote_code=True)\nopenelm_3b = AutoModelForCausalLM.from_pretrained(\"apple/OpenELM-3B\", trust_remote_code=True)\n\nopenelm_270m_instruct = AutoModelForCausalLM.from_pretrained(\"apple/OpenELM-270M-Instruct\", trust_remote_code=True)\nopenelm_450m_instruct = AutoModelForCausalLM.from_pretrained(\"apple/OpenELM-450M-Instruct\", trust_remote_code=True)\nopenelm_1b_instruct = AutoModelForCausalLM.from_pretrained(\"apple/OpenELM-1_1B-Instruct\", trust_remote_code=True)\nopenelm_3b_instruct = AutoModelForCausalLM.from_pretrained(\"apple/OpenELM-3B-Instruct\", trust_remote_code=True) \n```", "```\npython generate_openelm.py --model [MODEL_NAME] --hf_access_token [HF_ACCESS_TOKEN] --prompt 'Once upon a time there was' --generate_kwargs repetition_penalty=1.2 \n```", "```\npython generate_openelm.py --model [MODEL_NAME] --hf_access_token [HF_ACCESS_TOKEN] --prompt 'Once upon a time there was' --generate_kwargs repetition_penalty=1.2 prompt_lookup_num_tokens=10 \n```", "```\npython generate_openelm.py --model [MODEL_NAME] --hf_access_token [HF_ACCESS_TOKEN] --prompt 'Once upon a time there was' --generate_kwargs repetition_penalty=1.2 --assistant_model [SMALLER_MODEL_NAME] \n```", "```\n harness_repo=\"public-lm-eval-harness\"\ngit clone https://github.com/EleutherAI/lm-evaluation-harness ${harness_repo}\ncd ${harness_repo}\n\ngit checkout dc90fec\npip install -e .\ncd ..\n\npip install datasets@git+https://github.com/huggingface/datasets.git@66d6242\npip install tokenizers>=0.15.2 transformers>=4.38.2 sentencepiece>=0.2.0 \n```", "```\n hf_model=apple/OpenELM-270M\n\ntokenizer=meta-llama/Llama-2-7b-hf\nadd_bos_token=True\nbatch_size=1\n\nmkdir lm_eval_output\n\nshot=0\ntask=arc_challenge,arc_easy,boolq,hellaswag,piqa,race,winogrande,sciq,truthfulqa_mc2\nlm_eval --model hf \\\n        --model_args pretrained=${hf_model},trust_remote_code=True,add_bos_token=${add_bos_token},tokenizer=${tokenizer} \\\n        --tasks ${task} \\\n        --device cuda:0 \\\n        --num_fewshot ${shot} \\\n        --output_path ./lm_eval_output/${hf_model//\\//_}_${task//,/_}-${shot}shot \\\n        --batch_size ${batch_size} 2>&1 | tee ./lm_eval_output/eval-${hf_model//\\//_}_${task//,/_}-${shot}shot.log\n\nshot=5\ntask=mmlu,winogrande\nlm_eval --model hf \\\n        --model_args pretrained=${hf_model},trust_remote_code=True,add_bos_token=${add_bos_token},tokenizer=${tokenizer} \\\n        --tasks ${task} \\\n        --device cuda:0 \\\n        --num_fewshot ${shot} \\\n        --output_path ./lm_eval_output/${hf_model//\\//_}_${task//,/_}-${shot}shot \\\n        --batch_size ${batch_size} 2>&1 | tee ./lm_eval_output/eval-${hf_model//\\//_}_${task//,/_}-${shot}shot.log\n\nshot=25\ntask=arc_challenge,crows_pairs_english\nlm_eval --model hf \\\n        --model_args pretrained=${hf_model},trust_remote_code=True,add_bos_token=${add_bos_token},tokenizer=${tokenizer} \\\n        --tasks ${task} \\\n        --device cuda:0 \\\n        --num_fewshot ${shot} \\\n        --output_path ./lm_eval_output/${hf_model//\\//_}_${task//,/_}-${shot}shot \\\n        --batch_size ${batch_size} 2>&1 | tee ./lm_eval_output/eval-${hf_model//\\//_}_${task//,/_}-${shot}shot.log\n\nshot=10\ntask=hellaswag\nlm_eval --model hf \\\n        --model_args pretrained=${hf_model},trust_remote_code=True,add_bos_token=${add_bos_token},tokenizer=${tokenizer} \\\n        --tasks ${task} \\\n        --device cuda:0 \\\n        --num_fewshot ${shot} \\\n        --output_path ./lm_eval_output/${hf_model//\\//_}_${task//,/_}-${shot}shot \\\n        --batch_size ${batch_size} 2>&1 | tee ./lm_eval_output/eval-${hf_model//\\//_}_${task//,/_}-${shot}shot.log \n```", "```\n@article{mehtaOpenELMEfficientLanguage2024,\n    title = {{OpenELM}: {An} {Efficient} {Language} {Model} {Family} with {Open} {Training} and {Inference} {Framework}},\n    shorttitle = {{OpenELM}},\n    url = {https://arxiv.org/abs/2404.14619v1},\n    language = {en},\n    urldate = {2024-04-24},\n    journal = {arXiv.org},\n    author = {Mehta, Sachin and Sekhavat, Mohammad Hossein and Cao, Qingqing and Horton, Maxwell and Jin, Yanzi and Sun, Chenfan and Mirzadeh, Iman and Najibi, Mahyar and Belenko, Dmitry and Zatloukal, Peter and Rastegari, Mohammad},\n    month = apr,\n    year = {2024},\n}\n\n@inproceedings{mehta2022cvnets, \n     author = {Mehta, Sachin and Abdolhosseini, Farzad and Rastegari, Mohammad}, \n     title = {CVNets: High Performance Library for Computer Vision}, \n     year = {2022}, \n     booktitle = {Proceedings of the 30th ACM International Conference on Multimedia}, \n     series = {MM '22} \n} \n```"]