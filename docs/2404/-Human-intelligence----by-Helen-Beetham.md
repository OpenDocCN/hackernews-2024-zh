<!--yml
category: 未分类
date: 2024-05-27 13:26:11
-->

# 'Human intelligence' - by Helen Beetham

> 来源：[https://helenbeetham.substack.com/p/human-intelligence](https://helenbeetham.substack.com/p/human-intelligence)

> *O, there be players that I have seen play, and heard others praise, and that highly, not to speak it profanely, that, neither having the accent of Christians nor the gait of Christian, pagan, nor man, have so strutted and bellowed that I have thought some of nature’s journeymen had made men and not made them well, they imitated humanity so abominably.*
> 
> Will Shakespeare, *Hamlet* III(ii)

The original definition of ‘AI’, coined by John McCarthy in Stanford in 1955, was ‘*the science and engineering of making intelligent machines’*. To McCarthy and his Stanford colleagues, the meaning of ‘intelligent’ was too obvious to spell out any further. Marvin Minsky, writing in 1970, reiterated that: ‘*Artificial intelligence is the science of making machines do things that would require intelligence if done by men’*. Many definitions of ‘artificial intelligence’ in use today rely on the same assumption that computational ‘intelligence’ simply reflects what ‘intelligent people’ can do. Definitions such as [here](https://ico.org.uk/for-organisations/uk-gdpr-guidance-and-resources/artificial-intelligence/explaining-decisions-made-with-artificial-intelligence/part-1-the-basics-of-explaining-ai/definitions/), [here](https://www.ibm.com/topics/artificial-intelligence), [here](https://www.merriam-webster.com/dictionary/artificial%20intelligence), and from today’s Stanford Centre for Human-Centred AI [here](https://hai.stanford.edu/sites/default/files/2023-03/AI-Key-Terms-Glossary-Definition.pdf) all follow the same pattern. Intelligent people don’t even have to be men these days, but ‘we’ know who ‘they’ are.

In the guise of starting from something self-evident, the project of ‘artificial intelligence’ in fact serves to define what ‘intelligence’ is and how to value it, and therefore how diverse people should be valued too. Educators have good reason to be wary of the concept of ‘intelligence’ at all, but particularly as a single scale of potential that people have in measurable degrees. It is a [statistical artefact](http://bactra.org/weblog/523.html) that has long been [scientifically discredited](https://www.sciencedirect.com/science/article/abs/pii/S2211368119300658#!). It has been used to [enforce racial and gender discrimination](https://wellcomecollection.org/articles/YxDGExEAACMAdaX9) in education and to justify diverse forms of [discriminatory violence](https://learninglab.si.edu/collections/exploring-heredity-race-eugenics-and-the-history-of-intelligence-testing/ytlaOzLVuq36ejHP#r/988783), particularly over colonial subjects.

Biologist Stephen Jay Gould described the project as:

> ‘*the abstraction of intelligence as a single entity, its location within the brain, its quantification as one number for each individual, and the use of these numbers to rank people in a single series of worthiness, invariably to find that oppressed and disadvantaged groups—races, classes, or sexes—are innately inferior.’* (Gould 1981: 25\. Cited in Stephen Cave (2020) *[The Problem with Intelligence: Its Value-Laden History and the Future of AI](https://www.researchgate.net/publication/339105054_The_Problem_with_Intelligence_Its_Value-Laden_History_and_the_Future_of_AI)*.

The term ‘human’ is problematic for similar reasons. Unless its use is carefully qualified (and even then) ‘human’ all too often takes a particular fraction of humanity - white, anglo-european, male, educated, for example – as its reference point and norm. Most academics have enough sense of the history of these two terms - ‘human’ and ‘intelligence’ - to avoid using them in an unexamined way. Particularly when it comes to student learning, when we recognise there are a diversity of ambitions, identities, experiences, capabilities and cultures in the classroom, all of which can be resources for learning. And yet, since ‘artificial intelligence’ colonised the educational discourse, ‘human intelligence’ has begun to be used as though it is not only a real and self-evident thing, but [self-evidently what education is about](https://technode.global/2023/11/23/artificial-intelligence-ai-and-human-intelligence-hi-in-the-future-of-education/).

The value of this term to the AI industry is obvious. ‘Human intelligence’ is a palliative to anxieties about the restructuring and precaritsiation of work: don’t worry, there are still some things our models can’t do (yet). And yet the space of work that has not been declared computable today, or tomorrow, or the day after tomorrow is narrow and narrowing, and only the AI industry is qualified to define it. ‘Human’ in relation to ‘artificial’ intelligence turns people into props for data systems (humans in the loop). Props that make system outputs more accurate, safe, ethical, robust and useable, only to be removed once their input has been modelled and datafied. (Or, perhaps, when [making AI safe and useable proves too expensive after all](https://fortune.com/2024/01/22/ai-jobs-humans-cost-mit-study/).)

This is what the WEF means by ‘working productively alongside AI’.

But it is not clear to me why anyone who cares about education would catch the term ‘human intelligence’ from the cynics who are throwing it our way. Not surprisingly, given the history of both terms, if you pay any attention you can hear how regressive and divisive it is. A small number of ‘human intelligences’ will be free to maximise their potential for innovation and originality, their entrepreneurial decision-making and wise leadership. So rest easy that there will be highly paid jobs for AI engineers and company executives.

However, these people can only max out their human qualities if they are set free from other kinds of work - the boring, repetitive and uncreative. We are supposed to believe that this work is being ‘automated’ for everyone’s benefit, but this is manifestly not so. Research assistants aren’t promoted to other, more interesting roles when ‘AI research assistants’ come online. Rather, the work they do is likely to become more pressured and less valued, or to disappear. There are still [drivers (‘human overseers’) behind self-driving cars](https://urgentcomm.com/2023/03/13/will-driverless-cars-need-remote-human-supervision/), [annotators (‘data workers’) behind large language models,](https://www.theverge.com/features/23764584/ai-artificial-intelligence-data-notation-labor-scale-surge-remotasks-openai-chatbots)  [human personnel swiping left](https://www.972mag.com/lavender-ai-israeli-army-gaza/) to authorise AI ‘targets’ for bombing, and teachers uploading lesson plans and topic maps [for ‘teacherbots’](https://explore.teacherbot.io/about). And it turns out there were [1000 Indian data workers behind Amazon’s ‘fully automated’ stores](https://arstechnica.com/gadgets/2024/04/amazon-ends-ai-powered-store-checkout-which-needed-1000-video-reviewers/) in the UK. The work does not vanish, it is just routinised, cheapened, denigrated, frequently offshored, and always disappeared from view.

What AI claims to ‘liberate’ us from tells us what the AI industry thinks is worth doing. Not personal tutoring! Though, confusingly, this seems also to be the ‘irreplaceable role of teachers’. And beware conforming too much to the demands of the data engine or you deserve to be replaced.

The other ‘human’ who appears in the AI mirror is not running companies or registering patents but doing ‘emotional’ work: that is, work that has always been badly paid or removed from the sphere of paid work altogether. The work of care, service, community building, non-monetisable forms of creativity (craft, leisure, play), mending things and people who are broken. These forms of ‘human intelligence’ are not ‘increasingly prized’ at all. Instead, university managers are calculating how many student support staff can be replaced by chatbots. Academics who invest time and care in students (‘the human touch’) are threatened with redundancy. Schools are relying on volunteer counsellors to cope with the tsunami of mental distress (my local school has 13) while employing highly paid data analysts. In fact, the people who do the most to actually humanise the experience of mass education for students seem to be the most replaceable.  Enjoy the feels, because ‘emotional intelligence’ doesn’t ask for job security.

It’s funny how this happens, but it seems work that is highly rewarded because ‘uniquely human’ is most likely to be done by white, western, well educated men, preferably in STEM disciplines. While work that is undervalued because it is ‘only human’ is most likely to be shouldered by the rest of the world. And this work is constantly being reorganised as data work. Every gesture that can be captured as data is modelled, and whatever is left is rendered as a *behaviour*, to be governed by the model and its data-driven routines. Between highly paid ‘innovation’ and the non-computable work of the foundation economy - work that literally requires the human touch - AI is the project of datafying everything else.

A [recent post on TechTarget](https://www.techtarget.com/searchenterpriseai/tip/Artificial-intelligence-vs-human-intelligence-How-are-they-different) defined the ‘important differences’ between artificial and human ‘intelligence’ in ways that make clear everything in the right hand column is already available on the left. ‘Human intelligence’ is apparently being flattered but actually being erased. These definitions are so shallow, cynical and vacuous, I can only read them as deliberate provocations to the education system that is supposed to fall on them gratefully. *‘Mental sensations and concepts of phenomena that are not present’*? I can’t see that passing even ChatGPT’s floor-level bullshit detector.

What these self-serving comparisons produce is a double bind for students and intellectual workers. Submit yourself to the pace, the productivity, the datafied routines of algorithmic systems, and at the same time ‘[be more human](https://saren.ai/be-more-human-cultivating-your-uniquely-human-skills-in-the-age-of-ai-c24fbe945d05)’. Be more human, so that you can add value to the algorithms. Be more human , so more of your behaviour can be modelled and used to undercut your value. Be more human so that when AI fails to meet human needs, the work required to ‘fix’ it is clearly specified and can cheaply be fulfilled.

We can see these demands being interpreted by students as *both* a need to produce optimised texts – according to a standardised rubric or, perhaps, to satisfy an automated grading system – *and* to write in ways that are demonstrably ‘human’ (whatever this means). No wonder they are anxious and confused.

I spoke about this double bind for students in a recent podcast for the UCL Writing Centre: [Student writing as ‘passing’.](https://www.ucl.ac.uk/ioe/events/2024/mar/writing-passing-and-role-generative-ai) (recording soon available from this link). I also explore some of these issues in a post on the ‘unreliable Turing test’. The problems and disruptions posed by ‘AI’ are not only for education to suffer, but the question of what it means to *pass as* both authentically human and valuable to the data economy is particularly pressing in the education system. It surfaces in all the concerns about assessment and academic integrity. But only to address it there is to fail to recognise the challenge that is being thrown down to universities by big tech, epistemologically and pedagogically, as well as through the more mundane challenges of [draining talent](https://www.timeshighereducation.com/depth/can-academy-rein-big-tech), buying [political](https://cybernews.com/editorial/big-tech-meta-google-donations-research-harvard/)  [influence](https://www.washingtonpost.com/technology/2023/12/06/academic-research-meta-google-university-influence/), and [competing for educational business](https://www.kornferry.com/insights/briefings-magazine/issue-48/tech-takes-on-higher-ed).

I hope you enjoy these new offerings. All their human imperfections are my own.