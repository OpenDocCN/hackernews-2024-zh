- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-05-27 13:27:36'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: microsoft/Phi-3-mini-128k-instruct-onnx · Hugging Face
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://huggingface.co/microsoft/Phi-3-mini-128k-instruct-onnx](https://huggingface.co/microsoft/Phi-3-mini-128k-instruct-onnx)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[](#phi-3-mini-128k-instruct-onnx-models)Phi-3 Mini-128K-Instruct ONNX models'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This repository hosts the optimized versions of [Phi-3-mini-128k-instruct](https://aka.ms/phi3-mini-128k-instruct)
    to accelerate inference with ONNX Runtime.
  prefs: []
  type: TYPE_NORMAL
- en: 'Phi-3 Mini is a lightweight, state-of-the-art open model built upon datasets
    used for Phi-2 - synthetic data and filtered websites - with a focus on very high-quality,
    reasoning dense data. The model belongs to the Phi-3 model family, and the mini
    version comes in two variants: 4K and 128K which is the context length (in tokens)
    it can support. The model underwent a rigorous enhancement process, incorporating
    both supervised fine-tuning and direct preference optimization to ensure precise
    instruction adherence and robust safety measures.'
  prefs: []
  type: TYPE_NORMAL
- en: Optimized Phi-3 Mini models are published here in [ONNX](https://onnx.ai) format
    to run with [ONNX Runtime](https://onnxruntime.ai/) on CPU and GPU across devices,
    including server platforms, Windows, Linux and Mac desktops, and mobile CPUs,
    with the precision best suited to each of these targets.
  prefs: []
  type: TYPE_NORMAL
- en: '[DirectML](https://aka.ms/directml) support lets developers bring hardware
    acceleration to Windows devices at scale across AMD, Intel, and NVIDIA GPUs. Along
    with DirectML, ONNX Runtime provides cross platform support for Phi-3 Mini across
    a range of devices for CPU, GPU, and mobile.'
  prefs: []
  type: TYPE_NORMAL
- en: To easily get started with Phi-3, you can use our newly introduced ONNX Runtime
    Generate() API. See [here](https://aka.ms/generate-tutorial) for instructions
    on how to run it.
  prefs: []
  type: TYPE_NORMAL
- en: '[](#onnx-models)ONNX Models'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Here are some of the optimized configurations we have added:'
  prefs: []
  type: TYPE_NORMAL
- en: 'ONNX model for int4 DML: ONNX model for AMD, Intel, and NVIDIA GPUs on Windows,
    quantized to int4 using [AWQ](https://arxiv.org/abs/2306.00978).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'ONNX model for fp16 CUDA: ONNX model you can use to run for your NVIDIA GPUs.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'ONNX model for int4 CUDA: ONNX model for NVIDIA GPUs using int4 quantization
    via RTN.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'ONNX model for int4 CPU and Mobile: ONNX model for your CPU and Mobile, using
    int4 quantization via RTN. There are two versions uploaded to balance latency
    vs. accuracy. Acc=1 is targeted at improved accuracy, while Acc=4 is for improved
    perf. For mobile devices, we recommend using the model with acc-level-4.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: More updates on AMD, and additional optimizations on CPU and Mobile will be
    added with the official ORT 1.18 release in early May. Stay tuned!
  prefs: []
  type: TYPE_NORMAL
- en: '[](#hardware-supported)Hardware Supported'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The models are tested on:'
  prefs: []
  type: TYPE_NORMAL
- en: 'GPU SKU: RTX 4090 (DirectML)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'GPU SKU: 1 A100 80GB GPU, SKU: Standard_ND96amsr_A100_v4 (CUDA)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'CPU SKU: Standard F64s v2 (64 vcpus, 128 GiB memory)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Mobile SKU: Samsung Galaxy S21'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Minimum Configuration Required:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Windows: DirectX 12-capable GPU and a minimum of 4GB of combined RAM'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'CUDA: NVIDIA GPU with [Compute Capability](https://developer.nvidia.com/cuda-gpus)
    >= 7.0'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[](#model-description)Model Description'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Developed by:** Microsoft'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Model type:** ONNX'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Language(s) (NLP):** Python, C, C++'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**License:** MIT'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Model Description:** This is a conversion of the Phi-3 Mini-4K-Instruct model
    for ONNX Runtime inference.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[](#additional-details)Additional Details'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](#how-to-get-started-with-the-model)How to Get Started with the Model'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To make running of the Phi-3 models across a range of devices and platforms
    across various execution provider backends possible, we introduce a new API to
    wrap several aspects of generative AI inferencing. This API make it easy to drag
    and drop LLMs straight into your app. For running the early version of these models
    with ONNX Runtime, follow the steps [here](http://aka.ms/generate-tutorial).
  prefs: []
  type: TYPE_NORMAL
- en: 'For example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '[](#performance-metrics)Performance Metrics'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Phi-3 Mini-128K-Instruct performs better in ONNX Runtime than PyTorch for all
    batch size, prompt length combinations. For FP16 CUDA, ORT performs up to 5X faster
    than PyTorch, while with INT4 CUDA it's up to 9X faster than PyTorch.
  prefs: []
  type: TYPE_NORMAL
- en: 'The table below shows the average throughput of the first 256 tokens generated
    (tps) for FP16 and INT4 precisions on CUDA as measured on [1 A100 80GB GPU, SKU:
    Standard_ND96amsr_A100_v4](https://learn.microsoft.com/en-us/azure/virtual-machines/ndm-a100-v4-series).'
  prefs: []
  type: TYPE_NORMAL
- en: '| Batch Size, Prompt Length | ORT FP16 CUDA | PyTorch Eager FP16 CUDA | FP16
    CUDA Speed Up (ORT/PyTorch) |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| 1, 16 | 134.46 | 25.35 | 5.30 |'
  prefs: []
  type: TYPE_TB
- en: '| 1, 64 | 132.21 | 25.69 | 5.15 |'
  prefs: []
  type: TYPE_TB
- en: '| 1, 256 | 124.51 | 25.77 | 4.83 |'
  prefs: []
  type: TYPE_TB
- en: '| 1, 1024 | 110.03 | 25.73 | 4.28 |'
  prefs: []
  type: TYPE_TB
- en: '| 1, 2048 | 96.93 | 25.72 | 3.77 |'
  prefs: []
  type: TYPE_TB
- en: '| 1, 4096 | 62.12 | 25.66 | 2.42 |'
  prefs: []
  type: TYPE_TB
- en: '| 4, 16 | 521.10 | 101.31 | 5.14 |'
  prefs: []
  type: TYPE_TB
- en: '| 4, 64 | 507.03 | 101.66 | 4.99 |'
  prefs: []
  type: TYPE_TB
- en: '| 4, 256 | 459.47 | 101.15 | 4.54 |'
  prefs: []
  type: TYPE_TB
- en: '| 4, 1024 | 343.60 | 101.09 | 3.40 |'
  prefs: []
  type: TYPE_TB
- en: '| 4, 2048 | 264.81 | 100.78 | 2.63 |'
  prefs: []
  type: TYPE_TB
- en: '| 4, 4096 | 158.00 | 77.98 | 2.03 |'
  prefs: []
  type: TYPE_TB
- en: '| 16, 16 | 1689.08 | 394.19 | 4.28 |'
  prefs: []
  type: TYPE_TB
- en: '| 16, 64 | 1567.13 | 394.29 | 3.97 |'
  prefs: []
  type: TYPE_TB
- en: '| 16, 256 | 1232.10 | 405.30 | 3.04 |'
  prefs: []
  type: TYPE_TB
- en: '| 16, 1024 | 680.61 | 294.79 | 2.31 |'
  prefs: []
  type: TYPE_TB
- en: '| 16, 2048 | 350.77 | 203.02 | 1.73 |'
  prefs: []
  type: TYPE_TB
- en: '| 16, 4096 | 192.36 | OOM |  |'
  prefs: []
  type: TYPE_TB
- en: '| Batch Size, Prompt Length | PyTorch Eager INT4 CUDA | INT4 CUDA Speed Up
    (ORT/PyTorch) |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| 1, 16 | 25.35 | 8.89 |'
  prefs: []
  type: TYPE_TB
- en: '| 1, 64 | 25.69 | 8.58 |'
  prefs: []
  type: TYPE_TB
- en: '| 1, 256 | 25.77 | 7.69 |'
  prefs: []
  type: TYPE_TB
- en: '| 1, 1024 | 25.73 | 6.34 |'
  prefs: []
  type: TYPE_TB
- en: '| 1, 2048 | 25.72 | 5.24 |'
  prefs: []
  type: TYPE_TB
- en: '| 1, 4096 | 25.66 | 2.97 |'
  prefs: []
  type: TYPE_TB
- en: '| 4, 16 | 101.31 | 2.82 |'
  prefs: []
  type: TYPE_TB
- en: '| 4, 64 | 101.66 | 2.77 |'
  prefs: []
  type: TYPE_TB
- en: '| 4, 256 | 101.15 | 2.64 |'
  prefs: []
  type: TYPE_TB
- en: '| 4, 1024 | 101.09 | 2.20 |'
  prefs: []
  type: TYPE_TB
- en: '| 4, 2048 | 100.78 | 1.84 |'
  prefs: []
  type: TYPE_TB
- en: '| 4, 4096 | 77.98 | 1.62 |'
  prefs: []
  type: TYPE_TB
- en: '| 16, 16 | 394.19 | 2.52 |'
  prefs: []
  type: TYPE_TB
- en: '| 16, 64 | 394.29 | 2.41 |'
  prefs: []
  type: TYPE_TB
- en: '| 16, 256 | 405.30 | 2.00 |'
  prefs: []
  type: TYPE_TB
- en: '| 16, 1024 | 294.79 | 1.79 |'
  prefs: []
  type: TYPE_TB
- en: '| 16, 2048 | 203.02 | 1.81 |'
  prefs: []
  type: TYPE_TB
- en: '| 16, 4096 | OOM |  |'
  prefs: []
  type: TYPE_TB
- en: 'Note: PyTorch compile and Llama.cpp currently do not support the Phi-3 Mini-128K-Instruct
    model.'
  prefs: []
  type: TYPE_NORMAL
- en: '[](#package-versions)Package Versions'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '| Pip package name | Version |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| torch | 2.2.0 |'
  prefs: []
  type: TYPE_TB
- en: '| triton | 2.2.0 |'
  prefs: []
  type: TYPE_TB
- en: '| onnxruntime-gpu | 1.18.0 |'
  prefs: []
  type: TYPE_TB
- en: '| onnxruntime-genai | 0.2.0 |'
  prefs: []
  type: TYPE_TB
- en: '| onnxruntime-genai-cuda | 0.2.0 |'
  prefs: []
  type: TYPE_TB
- en: '| onnxruntime-genai-directml | 0.2.0 |'
  prefs: []
  type: TYPE_TB
- en: '| transformers | 4.39.0 |'
  prefs: []
  type: TYPE_TB
- en: '| bitsandbytes | 0.42.0 |'
  prefs: []
  type: TYPE_TB
- en: '[](#appendix)Appendix'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](#activation-aware-quantization)Activation Aware Quantization'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: AWQ works by identifying the top 1% most salient weights that are most important
    for maintaining accuracy and quantizing the remaining 99% of weights. This leads
    to less accuracy loss from quantization compared to many other quantization techniques.
    For more on AWQ, see [here](https://arxiv.org/abs/2306.00978).
  prefs: []
  type: TYPE_NORMAL
- en: '[](#model-card-contact)Model Card Contact'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: parinitarahi, kvaishnavi, natke
  prefs: []
  type: TYPE_NORMAL
- en: '[](#contributors)Contributors'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Kunal Vaishnavi, Sunghoon Choi, Yufeng Li, Akshay Sonawane, Sheetal Arun Kadam,
    Rui Ren, Edward Chen, Scott McKay, Ryan Hill, Emma Ning, Natalie Kershaw, Parinita
    Rahi, Patrice Vignola, Chai Chaoweeraprasit, Logan Iyer, Vicente Rivera, Jacques
    Van Rhyn
  prefs: []
  type: TYPE_NORMAL
