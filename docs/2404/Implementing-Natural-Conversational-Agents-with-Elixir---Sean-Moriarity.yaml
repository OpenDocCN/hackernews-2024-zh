- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: Êú™ÂàÜÁ±ª'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-05-27 13:19:22'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: Implementing Natural Conversational Agents with Elixir ‚Äì Sean Moriarity
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Êù•Ê∫êÔºö[https://seanmoriarity.com/2024/02/25/implementing-natural-conversational-agents-with-elixir/](https://seanmoriarity.com/2024/02/25/implementing-natural-conversational-agents-with-elixir/)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: In [my last post](https://seanmoriarity.com/2024/02/25/nero-part-1-home-automations/),
    I discussed some work I had done building Nero, the assistant of the future that
    I‚Äôve always wanted. I ended up creating an end-to-end example which used Nx, OpenAI
    APIs, and ElevenLabs to create an in-browser home automation assistant. For a
    first product, it‚Äôs decent. Nero is a neat little party trick that I can use to
    impress my non-tech friends. I am, however, not in this business to impress my
    friends. I want Nero to *actually help me* and *actually feel like an assistant*.
    My previous version is not that.
  prefs: []
  type: TYPE_NORMAL
- en: One missing piece is the ability to converse naturally without browser interaction.
    The first implementation of Nero‚Äôs ‚Äúconversational‚Äù abilities relied on user interaction
    with the screen every time we wanted to initiate a response or action. Nero also
    did not retain any conversational history. In short, Nero was not a great conversational
    assistant. It was one of the things I wanted to fix; however, I was motivated
    to do it sooner rather than later after watching [an impressive demo from Retell](https://beta.retellai.com/home-agent).
  prefs: []
  type: TYPE_NORMAL
- en: 'The Retell demo implements a conversational agent backed by their WebSocket
    API in a browser. The demonstration has:'
  prefs: []
  type: TYPE_NORMAL
- en: ‚ÄúAlways on‚Äù recording
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Low latency
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Support for interrupts
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Impressive filtering (e.g. snapping and other non-voice activity doesn‚Äôt seem
    to throw off the agent)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Their documentation suggests they also have support for [backchanneling](https://www.cs.utep.edu/nigel/bc/#:~:text=What%20is%20a%20Backchannel%3F,utterances%20such%20as%20uh%2Dhuh.)
    and intelligent end of turn detection‚Äîtwo things that are essential to natural
    conversational feel but which are very difficult to express programmatically.
  prefs: []
  type: TYPE_NORMAL
- en: I had previously convinced myself that I could implement a passable conversational
    agent experience in a short amount of time. So that is what I set out to do.
  prefs: []
  type: TYPE_NORMAL
- en: Always On Recording
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The first thing that needed to change about Nero‚Äôs design was the speech to
    text pipeline. My original demonstration relied on an [example from Bumblebee](https://github.com/elixir-nx/bumblebee/blob/main/examples/phoenix/speech_to_text.exs)
    which implemented a speech to text pipeline using Whisper. The pipeline uses mouse
    events in a [Phoenix LiveView Hook](https://hexdocs.pm/phoenix_live_view/js-interop.html#client-hooks-via-phx-hook)
    to start and stop recordings before sending them to the server to initiate transcription.
    If you‚Äôre not familiar, [Phoenix LiveView](https://hexdocs.pm/phoenix_live_view/Phoenix.LiveView.html)
    is a server-side rendering framework built on top of Elixir. LiveView has support
    for client-side JavaScript hooks which support bidirectional communication between
    client and server.
  prefs: []
  type: TYPE_NORMAL
- en: The original speech to text implementation used a hook with an event listener
    attached to `mousedown` and `mouseup` on a button to start and stop recording.
    After recording stops, the hook decodes the recorded buffer into a PCM buffer,
    converts the endianness, and then pushes the buffer to the server with an upload.
    The original hook implements most of the functionality we want; however, we need
    to make some minor tweaks. Rather than trigger recordings to stop and start on
    mouse events, we want to trigger recordings to start and stop exactly when a person
    starts and stops speaking. Simple, right?
  prefs: []
  type: TYPE_NORMAL
- en: My first idea in implementing what I called ‚Äúalways on recording‚Äù was to monitor
    the microphone‚Äôs volume, and trigger a recording when the volume reached a certain
    threshold. The recording would stop when the volume dipped below that threshold.
    At this point, I learned about [getUserMedia](https://developer.mozilla.org/en-US/docs/Web/API/MediaDevices/getUserMedia).
    `getUserMedia` prompts the user for permission to access media devices such as
    a microphone and/or webcam, and then produces a `MediaStream`. A `MediaStream`
    is a stream of media content containing information about audio and video tracks
    in the stream. We can use data from the `MediaStream` to determine speaker activity
    and thus trigger recordings.
  prefs: []
  type: TYPE_NORMAL
- en: 'To determine the volume for a given sample, we can use an [AnalyserNode](https://developer.mozilla.org/en-US/docs/Web/API/AnalyserNode).
    Per the documentation `AnyalyserNode` is designed for processing generated audio
    data for visualization purposes, but we can use it to determine spikes in audio:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: This uses an analyser and repeatedly checks if the volume of the microphone
    at a given frame exceeds the given `VOLUME_THRESHOLD`. If it does, it checks to
    see if we are recording and if not, starts the recording.
  prefs: []
  type: TYPE_NORMAL
- en: 'After testing a bit, I realized this implementation sucked. Of the many issues
    with this approach, the biggest is that there are many natural dips in a person‚Äôs
    volume. Checking a single frame doesn‚Äôt account for these natural dips. To fix
    this, I thought it would be a good idea to introduce a timeout which only stopped
    recording after the volume was below a threshold for a certain amount of time:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: This actually ended up working decent, but required tuning hyperparameters for
    both `VOLUME_THRESHOLD` and `SILENCE_TIMEOUT`. The challenge here is that higher
    `SILENCE_TIMEOUT` introduces additionally latency in transition time between a
    speaker and Nero; however, lower timeouts might be too sensitive to speakers with
    slower and quieter speaking rhythms. Additionally, a static `VOLUME_THRESHOLD`
    does not account for ambient noise. Now, despite these shortcomings, I found I
    was able to passably detect a single speaker in a quiet room.
  prefs: []
  type: TYPE_NORMAL
- en: After hooking this up to my existing LiveView and trying some end-to-end conversations,
    I realized something was significantly off. The transcriptions I was getting were
    off. I soon realized that they were always off at the beginning of a transcription.
    Shorter audio sequences were especially affected. It turns out that the detection
    algorithm always resulted in some amount of truncation at the beginning of an
    audio clip. When a speaker starts talking, their volume ramps up ‚Äì it‚Äôs not an
    instantaneous spike. To account for this, I introduced a pre-recording buffer
    which always tracked the previous 150ms of audio. After recording started, I would
    stop the pre-recording buffer and start the actual recording, and then eventually
    splice these 2 together to send to the server for transcription.
  prefs: []
  type: TYPE_NORMAL
- en: Overall, this *actually* worked okay. While there are some obvious failure modes,
    it worked well enough to get a passable demonstration. If you can‚Äôt tell by now,
    I am not an audio engineer. I learned later that this is a very naive attempt
    at [voice activity detection](https://en.wikipedia.org/wiki/Voice_activity_detection).
    Later on in this post, I‚Äôll run through some of the improvements I made based
    on my research into the field of VAD.
  prefs: []
  type: TYPE_NORMAL
- en: End-to-End Implementation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The demonstration I built for Nero in my [first post](https://seanmoriarity.com/2024/02/25/nero-part-1-home-automations/)
    already contained the scaffolding for an end-to-end transcription -> response
    -> speech pipeline. I only needed to make some slight modifications to get the
    phone call demo to work. The end-to-end the pipeline looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: 'When our algorithm detects that speech has stopped, it invokes the `stopRecording`
    method. `stopRecording` takes the recorded audio, does some client-side pre-processing,
    and uploads it to the server. The server consumes the uploaded entry as a part
    of [LiveView‚Äôs normal uploads lifecycle](https://hexdocs.pm/phoenix_live_view/uploads.html)
    and then invokes an [async task](https://hexdocs.pm/phoenix_live_view/Phoenix.LiveView.html#module-arbitrary-async-operations)
    to start transcription:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Note that because we did most of the pre-processing client-side, we can just
    consume the audio binary as an `Nx.Tensor`, without any additional work. The `SpeechToText`
    module implements transcription using [Nx.Serving](https://hexdocs.pm/nx/Nx.Serving.html):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '`Nx.Serving` is an abstraction in the Elixir Nx ecosystem for serving machine
    learning models directly in an Elixir application. It implements dynamic batching,
    encapsulates pre-processing, inference, and post-processing, supports distribution
    and load-balancing between multiple GPUs natively, and in general is an extremely
    easy way to serve machine learning models.'
  prefs: []
  type: TYPE_NORMAL
- en: 'After transcription completes, we get an async result we can handle to initiate
    a response:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Here `Nero.Agent.respond/1` returns an Elixir `Stream` of text. For my original
    demonstration I just used the Elixir OpenAI library to produce a stream from a
    GPT-3.5 response:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'The response stream is consumed by `speak/2`. `speak/2` implements the text
    to speech pipeline:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Where `Nero.TextToSpeech.stream/1` uses the [ElevenLabs WebSocket API](https://elevenlabs.io/docs/api-reference/websockets)
    to stream text in and speech out. You can read a bit more about the implementation
    in my previous post.
  prefs: []
  type: TYPE_NORMAL
- en: '`Nero.TextToSpeech.stream/1` returns the consumed response as text so we can
    append that to the chat history after the `:speak` task finishes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'This is basically all of the scaffolding needed for an end-to-end demo, but
    I wanted to add a few more features. First, I wanted to support ‚Äúintelligent‚Äù
    hang-ups. Basically, I wanted to be able to detect when a conversation was finished,
    and stop the recording. To do that, I used [Instructor](https://github.com/thmsmlr/instructor_ex):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Please ignore my wonderfully engineered prompt. This uses GPT-3.5 to determine
    whether or not a given conversation has ended. After every one of Nero‚Äôs turns,
    we check the transcript to possibly end the call:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'This pushes a `hang_up` event to the socket:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Which stops the recording, and then pushes an event to `toggle_conversation`
    back to the server. `toggle_conversation` implements the start/stop logic from
    the server:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, I wanted to implement information extraction from the transcript.
    Again, I used instructor and defined an extraction schema:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'And used GPT-3.5 with a rough prompt to get the necessary information from
    the transcript:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'And then anytime a conversation ends, we attempt to retrieve appointment information:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Now this is essentially the exact implementation that produced [this demonstration](https://twitter.com/sean_moriarity/status/1760435005119934862).
    End-to-end this amounted to a couple of hours of work; however, I already had
    most of the basic scaffold implemented from my previous work on Nero. In my biased
    opinion, I think my demo is pretty good, but as others have pointed out Retell‚Äôs
    demo kicks my ass in:'
  prefs: []
  type: TYPE_NORMAL
- en: Latency
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Reliability
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Natural sounding voice
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: And so, I set out to improve my implementation ‚Äì starting with latency.
  prefs: []
  type: TYPE_NORMAL
- en: Reducing Latency
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Human conversations have extremely tight ‚Äútime-to-turn.‚Äù In-person conversations
    are especially rapid because we rely on visual as well as audio signals to determine
    when it‚Äôs our time to participate in a conversation. The ‚Äúaverage‚Äù time-to-turn
    in a conversation can be as quick as 200ms. That means for a conversational agent
    to feel realistic, it needs an extremely quick turn around time for ‚Äútime to first
    spoken word.‚Äù
  prefs: []
  type: TYPE_NORMAL
- en: After posting my original demonstration, I already knew there were some very
    easy optimizations I could make, so I set out to improve the average latency of
    my implementation as much as possible in a short amount of time. First, I needed
    at least some method for determining whether an optimization worked. My rudimentary
    approach was to use [JavaScript Performance Timers](https://developer.mozilla.org/en-US/docs/Web/API/Performance/now)
    and logging. Basically, I computed a `startTime` from the exact moment an audio
    recording stopped and an `endTime` from the exact moment an audio output started,
    and then I logged that time to the console.
  prefs: []
  type: TYPE_NORMAL
- en: This is a very unscientific way of doing business. In the future, I‚Äôd like to
    implement a much more involved profiling and benchmarking methodology. For this
    process though, it worked well enough.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, I considered all of the areas that could introduce latency into the pipeline.
    From the moment a recording stops, these are all of the steps we take:'
  prefs: []
  type: TYPE_NORMAL
- en: Pre-process recording by converting to PCM buffer, and then converting endianness
    to match server (if necessary)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Upload buffer to server
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Perform speech to text on buffer to produce text
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Send text to LLM
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Send streamed text to ElevenLabs
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Receive streamed audio from ElevenLabs
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Broadcast audio to client
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Decode audio on client and play
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: That‚Äôs a lot of steps that can introduce latency, including potentially 3 (in
    our case 2 because we own the STT pipeline) network calls.
  prefs: []
  type: TYPE_NORMAL
- en: Next, I wanted to esablish a ‚Äúbaseline‚Äù of performance. To demonstrate this
    iterative process, I did a baseline example on my M3 Mac CPU. Note that this is
    going to be slow relative to my previous demo because the previous demo runs on
    a GPU. The baseline performance I got from the original demo running on my mac
    was `4537 ms`. 4.5 seconds turn around time. Yikes. Lots of work to do.
  prefs: []
  type: TYPE_NORMAL
- en: 'To start, I knew that the `SILENCE_TIMEOUT` used to wait for speech to end
    was rather long. For the original demo, I used 1000 ms, which basically means
    a speaker has to stop talking for a full second before we‚Äôll even start the long
    response process. After some trial and error, I figured 500 ms was a ‚Äúpassable‚Äù
    hyperparameter. After adjusting this down, the latency change was almost exactly
    correlated to the dip: `4079 ms`.'
  prefs: []
  type: TYPE_NORMAL
- en: 'I had a hunch that my text to speech pipeline was not efficient. Fortunately,
    ElevenLabs gives us a nice [Latency Guide](https://elevenlabs.io/docs/api-reference/reducing-latency).
    The first suggestion is to use their turbo model by specifying `eleven_turbo_v2`.
    I set that and we got a slight performance boost: `4014 ms`.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, they suggest adding `optimize_streaming_latency`. I set the value to
    `3` and we get: `3791 ms`. Their next suggestion is to use a pre-made voice. I
    actually didn‚Äôt realize until much later that I was not using a pre-made voice
    so I don‚Äôt have a comparison for how that change impacted latency.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now it says to limit closing WebSocket connections. my current implementation
    opens a connection everytime it speaks ‚Äì which is not good. Basically every ‚Äúturn‚Äù
    has to establish a new websocket connection. Additionally, ElevenLabs has a timeout
    of 20s from when you connect. So you need to send a message at least every 20s.
    I considered 2 options at this point:'
  prefs: []
  type: TYPE_NORMAL
- en: Open a global WebSocket connection, or maybe even a pool of connections, and
    try to keep the connection alive. But that seems really wasteful, and I don‚Äôt
    think is the intended use of their API
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Open a WebSocket connection when convo starts. We don‚Äôt have to worry about
    20s pauses
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: I decided to go with option 2, but I still think there are some drawbacks and
    considerations for a production system. The implementation I used opens a websocket
    connection on first ‚Äúspeak‚Äù and stores the connection PID as an assign in the
    LiveView socket. If you have a system with potentially many concurrent users speaking,
    you run the risk of creating a potentially unbounded number of connections. A
    more robust solution would probably use connection pools; however, I‚Äôm not really
    worried about traffic or scaling here.
  prefs: []
  type: TYPE_NORMAL
- en: While adding this optimization, I struggled a bit because ElevenLabs would send
    the first frame back, then cut off. Then I realized that it was waiting to generate
    becuase it thought I was going to send more frames. So I needed to ‚Äúflush‚Äù the
    generation after I finished sending my tokens. This also seemed to fix unnatural
    audio problems I was having. After applying this optimization, our time to first
    spoken word was slightly lower in the `3700 ms` range.
  prefs: []
  type: TYPE_NORMAL
- en: After perusing their docs a bit more, I learned that ElevenLabs will send PCM
    buffers instead of MP3\. Web Browser‚Äôs have to decode MP3 to PCM, which potentially
    introduces some overhead. One drawback is that you need to be on the independent
    creator tier to receive PCM instead of MP3\. Now, if you‚Äôre wondering if I spent
    $99 to save some milliseconds for a meaningless demo, the answer is absolutely
    yes I did.
  prefs: []
  type: TYPE_NORMAL
- en: At this point, I believe I‚Äôve exhausted a lot of the ‚Äúeasy‚Äù optimizations for
    TTS latency. One thing that does bother me about the ElevenLabs Websocket API
    is that there‚Äôs no way to receive binary payloads instead of JSON payloads. This
    is probably because they send alignment data, but I‚Äôm not using the alignment
    data here. When handling an incoming frame from their API we have to first decode
    the JSON, and then decode the Base64 encoded audio buffer. I‚Äôm not sure what the
    latency impact is, but I‚Äôm sure we could shave *some* time by avoiding both of
    these conversions. I also think the Base64 representation results in slightly
    larger buffers which could impact network latency.
  prefs: []
  type: TYPE_NORMAL
- en: 'The next area I looked to improve was the speech-to-text pipeline. I am using
    `Nx.Serving` specifically for Speech-to-Text. The benefit of this approach is
    that we can avoid an additional network call just for transcription. Of course,
    that assumes our transcription pipeline can run fast enough on our own hardware.
    XLA is notoriously slow on CPUs (it‚Äôs getting better). The first ‚Äúoptimization‚Äù
    I did was to switch to my GPU: `2050 ms`'
  prefs: []
  type: TYPE_NORMAL
- en: And that right there is a bitter lesson, because it‚Äôs the largest performance
    boost we‚Äôre going to get.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, I realized the model isn‚Äôt using F16, which can introduce some solid
    speed-ups: `1800 ms`. Now, there are probably some additional optimizations we
    could add to Nx and EXLA specifically. For example, we don‚Äôt have a flash attention
    implementation. Of course, XLA does a great job of applying similar optimizations
    as a baseline, so I‚Äôm not sure how much it would help. There‚Äôs also [fast JAX
    implementations of Whisper](https://github.com/sanchit-gandhi/whisper-jax) that
    claim up to 70x speed ups. One issue with a lof of these claimed speed-ups; however,
    is that they are almost **always** for long audio sequences. GPUs and TPUs do
    well with large batch sizes and sequence lengths, but not for batch size 1 and
    short sequence lengths like we care about in this implementation. One day I may
    go down the performance hole of fast batch size 1 transcription, but today is
    not that day.'
  prefs: []
  type: TYPE_NORMAL
- en: At this point, I had moved on to improving some of the failure modes of my demo.
    While doing so, I learned much more about audio than I had previously known, and
    realized that the configuration I used to record audio can significantly improve
    whisper performance as well. Turns out there‚Äôs a [nice guide](https://dev.to/mxro/optimise-openai-whisper-api-audio-format-sampling-rate-and-quality-29fj)
    of somebody discussing parameters that work. Specifically, you should use 16 kHz
    sample rate for transcriptions. Reducing the sample rate also should reduce network
    overhead because we have less data, but it could reduce quality of the transcription.
    Oh well. Additionally, I realized I wasn‚Äôt using a pre-made ElevenLabs voice.
    After introducing both of these optimizations, I was able to achieve `1520 ms`
    turn time.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, I realized I was doing all of my benchmarks on a development server.
    I switched my phoenix environment from `dev` to `prod` and got: `1375 ms`. So,
    with all of these optimizations we‚Äôre sitting at about 1.3s turn around time in
    a conversation. When conversing, it starts to feel somewhat close to natural.
    I should also point out that this is also running over Tailscale, so there is
    about 100 ms ping between my Mac and the server running on my GPU. When I run
    this locally on my GPU, I can consistently get about `1000 ms` and sometimes `900
    ms` turn around time. Still, unfortunately, this does not match Retell‚Äôs latency.
    According to them, they are able to achieve `800 ms` consistently. I have some
    musings at the end about how this is possible.'
  prefs: []
  type: TYPE_NORMAL
- en: I believe the biggest area I could improve the implementation is to use a better
    VAD implementation that relies on small rolling windows of activity rather than
    frames. We could probably get away with using 20-30 ms windows, which could theoretically
    offer a 480 ms latency improvement. I would like to eventually explore this.
  prefs: []
  type: TYPE_NORMAL
- en: In all honesty though, I think that is a significant improvement, and I could
    *probably* stop right here and be done with it.
  prefs: []
  type: TYPE_NORMAL
- en: If I were to keep going, I would explore using a local LLM with Nx and Bumblebee.
    Nx and Bumblebee support LLMs like Mistral and Llama out-of-the box. And our text
    generation servings support streaming. That means we can possibly eliminate any
    network latency to OpenAI, and instead run 2 of the 3 models locally. One issue
    with this is that Nx currently does not have any quantized inference support (it‚Äôs
    coming I promise), so my single 4090 is not sufficient to deploy both Whisper
    and Mistral. Fortunately, the folks at [Fly.io](https://fly.io/gpu) were kind
    enough to give me access to some 80GB A100s. I will post a demo when I get one
    deployed üôÇ
  prefs: []
  type: TYPE_NORMAL
- en: Maybe one day I will implement StyleTTS2 and see how efficient we can get with
    an entirely local inference pipeline.
  prefs: []
  type: TYPE_NORMAL
- en: Improving the Conversational Experience
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Some people pointed out that my original demo did not have the same conversational
    experience as Retell‚Äôs, and they are absolutely right. Aside from latency, mine
    was prone to failure, picks up system sounds, picks up random noises like keyboard
    and mouse clicks, and doesn‚Äôt do well with ambient noise. They also have support
    for backchanneling, fillers and interruptions which introduces some element of
    ‚Äúrealness‚Äù when interacting with their agent.
  prefs: []
  type: TYPE_NORMAL
- en: Now I didn‚Äôt get around to adding backchannels or fillers, but I was able to
    make some slight improvements to the VAD algorithm I used, and I added support
    for interruptions.
  prefs: []
  type: TYPE_NORMAL
- en: Fixing Some Failure Modes with VAD
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The first failure mode that seems to happen is echo from the system sounds.
    Nero always records and will start transcribing after audio spikes over a certain
    threshold. After some digging into the `getUserMedia` API, I found options for
    `echoCancellation`, `noiseSuppression`, and `autoGainControl`. This is the same
    point I realized that I could specify the microphone sample rate for the optimization
    I could added from the last section. Most of these options are on by default depending
    on your browser, but I added them explicitly anyway:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: Now that somewhat helped, but Nero still picks up it‚Äôs own audio. This probably
    requires a more sophisticated solution, but I moved on to the next problem.
  prefs: []
  type: TYPE_NORMAL
- en: 'The second obvious failure mode is the fact that it picks up keyboard clicks,
    and the silence timeout is hard to tune. My first attempt to fix this was to ‚Äúignore‚Äù
    large spikes in audio by ‚Äúsmoothing‚Äù the volume at each frame:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, with some advice from [Paulo Valente](https://twitter.com/polvalente),
    I implemented a biquad filter to with a low and high-pass in order to filter audio
    to the range of human speech:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: In practice, both of these solutions actually seemed to work decent, but they
    could absolutely be better. I know it‚Äôs possible to improve the client-side filtering
    using a rolling-window that looks energy of the speaking frequences relative to
    energy of an entire sample. But, there are also machine learning models that perform
    VAD, and have `1ms` inference times. I realized that it‚Äôs probably quicker to
    just send all of the data over the websocket in chunks, and perform VAD on the
    server. I‚Äôll discuss that implementation a little later.
  prefs: []
  type: TYPE_NORMAL
- en: Supporting Interruptions
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Next I wanted to add support for interruptions. In the Retell example, the
    speaker will cut off mid-speech if it detects that you are speaking. To implement
    this feature in Nero, I added a `pushEvent` to the Microphone hook which would
    push an `interrupt` event to the server anytime speech is detectected:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'The server handles this event and broadcasts an event to the TTS channel to
    stop speaking:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'And the channel handles the event by clearing out the output audio stream and
    queue:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: Unfortunately, this does create a race condition. There‚Äôs a potential situation
    where a speaker interrupts and the speaking queue gets cleared on the client,
    but ElevenLabs is still streaming audio back to the server. The server is always
    going to just broadcast this info to the client, and as is the client will process
    it. This potentially creates a situation with weird continutations in the audio.
    To get around this, I refactored the TTS implementation so that each audio broadcast
    appends a 6 digit token to the payload. Then, all we need to do is keep the token
    in sync with the client and server. On the client, when processing the audio queue,
    it simply checks whether or not the token at the beginning of the payload matches,
    and if it doesn‚Äôt it ignores that sample.
  prefs: []
  type: TYPE_NORMAL
- en: The limitation with this implementation is it does not update the chat transcript.
    It‚Äôs entirely possible because we have access to the alignment information from
    ElevenLabs, but I just didn‚Äôt implement it at this time.
  prefs: []
  type: TYPE_NORMAL
- en: Time-based Hang Ups
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Another thing the Retell demo has support for is cues and hang ups after a
    duration of silence. If you are silent for too long, you‚Äôll get a cue from the
    AI speaker asking you if you‚Äôre still there. After another duration of silence,
    it will hang up. This is something that‚Äôs pretty easy to do with LiveView and
    `Process.send_after/4`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'And then we can cancel the timer anytime we receive a transcription, and restart
    it after every turn speaking. Note that we can‚Äôt depend on the Phoenix `speak`
    async task ending as the trigger to send nudges. Instead, we need to push an event
    from the speaker hook that the audio has ended. This avoids a case where the speaker
    initiates a really long speech, which overlaps with the `nudge_ms` duration. Now,
    we can control the number of nudges with an assign. In my case, I just used a
    boolean:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: Re-doing the Entire Thing
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Somewhere along the line I realized that my attempts at engineering solid VAD
    client-side were never going to deliver the experience that I wanted. I discussed
    with [Andres Alejos](https://twitter.com/ac_alejos) a bit, and he found a [Silero
    VAD](https://github.com/snakers4/silero-vad) model which is capable of performing
    VAD in `1ms` on a single CPU thread. They also had an ONNX model‚Äîand we have a
    library in the Elixir ecosystem called [Ortex](https://github.com/elixir-nx/ortex)
    which allows us to execute ONNX models.
  prefs: []
  type: TYPE_NORMAL
- en: To accomodate for the new VAD model, I ended up re-implementing the original
    LiveView I had as a WebSocket. This actually works out well because the WebSocket
    server is generic, and can be consumed by any language with a WebSocket client.
    The implementation is also relatively simple, and easily expanded to accomodate
    for other LLMs, TTS, and STT models. The WebSocket implementation has low latency
    (when running on a GPU), and supports interrupts.
  prefs: []
  type: TYPE_NORMAL
- en: You can find the [project on my GitHub](https://github.com/seanmor5/echo) as
    well as an [example using the server](https://github.com/seanmor5/echo_example).
  prefs: []
  type: TYPE_NORMAL
- en: Musings
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The final implementation I ended up with still does not match the quality of
    the Retell demo. That said, I think it‚Äôs a solid start for future work. I believe
    I acted with some hubris when first posting about this project, and I would like
    to say that Retell‚Äôs work should not be understated. I can appreciate the attention
    to detail that goes into making an effective conversational agent, and Retell‚Äôs
    demo shows they paid a lot of attention to the details. Kudos to them and their
    team.
  prefs: []
  type: TYPE_NORMAL
- en: I will also admit that my demo is playing to one benchmark. I‚Äôm optimizing the
    hell out of latency to support a single user‚Äîme. I think this solution would change
    if it needed to accomodate for multiple concurrent users.
  prefs: []
  type: TYPE_NORMAL
- en: Retell‚Äôs website claims they have a conversation orchestration model under the
    hood to manage the complexities of conversation. I had my doubts about that going
    into this, but I believe it now. Whether or not this model is actually a single
    model or a series of models for VAD, adding backchannels, etc. I‚Äôm not sure. I
    think eventually it *will* be a single model, but I‚Äôm not sure if it is now, which
    leads me to my next point.
  prefs: []
  type: TYPE_NORMAL
- en: Another Bitter Lesson
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: While doing all of these optimizations, I could not help but think that it will
    eventually be all for naught. Not because I don‚Äôt think people will find it useful,
    but because large models trained on lots of data simply seem to always beat engineering
    effort. I believe the future of this area of work is in joint models. I think
    the only way to achieve real-time conversations is to merge parts of the stack.
    I predict in less than a year we will see an incredibly capable joint speech/text
    model. I recently saw a large audio model called [Qwen-Audio](https://github.com/QwenLM/Qwen-Audio)
    that I believe is similar to what I envision.
  prefs: []
  type: TYPE_NORMAL
- en: 'Specifically, if somebody were kind enough to give me some money to throw at
    this problem, here is exactly what I would do:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Generate an [Alpaca-style](https://github.com/tatsu-lab/stanford_alpaca/blob/main/alpaca_data.json)
    and/or LLaVA-style dataset of synthetic speech. Note that it would require a bit
    of pre-processing to change Alpaca inputs to mirror a style compatible with spoken-word.
    I would use ElevenLabs to generate the dataset in mulitple voices. Of course this
    dataset would be a bit too ‚Äúclean,‚Äù so we‚Äôd need to apply some augmentations which
    add ambient noise, change speaking pitch and speed, etc. Bonus points: adding
    samples of ‚Äúnoise‚Äù which require no response to merge the VAD part of the pipeline
    in as well. You can even throw in text prompts that dictate when and when not
    to respond to support things like [wake word detection](https://picovoice.ai/platform/porcupine/)
    without needing to train a separate model.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Create a LLaVA-style model with a Whisper or equivalent base, an LLM, and a
    projection layer.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Secure H100s, train model, and ‚Äúturn H100s into $100s‚Äù (thank you [@thmsmlr](https://twitter.com/thmsmlr?lang=en))
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: If you want to give me some $$$, my e-mail is smoriarity.5@gmail.com üôÇ
  prefs: []
  type: TYPE_NORMAL
- en: I believe we are also close to just having full-on speech-to-speech models.
    A specific challenge I can see when creating these models is coming up with a
    high-quality dataset. I think if you make a deliberate attempt at ‚Äúrecording conversations‚Äù
    for the purposes of training, you will actually probably end up with a lower-quality
    dataset. People tend to change their behavior under observation. Additionally,
    conversations from movies and TV shows aren‚Äôt actually very natural. Even some
    podcasts have an unnatural converastional rhythm.
  prefs: []
  type: TYPE_NORMAL
- en: While watching [Love is Blind](https://en.wikipedia.org/wiki/Love_Is_Blind_(TV_series))
    with my fianc√©, I realized you could probably get a decent amount of quality data
    from reality tv shows. The conversations in reality TV are overly dramatic and
    chaotic, but are (I think) closer to realistic than anything else.
  prefs: []
  type: TYPE_NORMAL
- en: Conversational Knowledge Base?
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: I do wonder what a solid RAG implementation looks like on top of a conversational
    agent. RAG and complex CoT pipelines will introduce latency which could deteriorate
    the conversational experience. However, there are clever ways you can hide this.
    In conversations that require ‚Äúsearch‚Äù between humans, e.g. like scheduling an
    appointment, you‚Äôll often have one party saying ‚Äúone moment please‚Äù before performing
    a system search. Building something like that in is entirely possible. Additionally,
    if your agent requires information up front about an individual, it‚Äôs possible
    to include that in the initial prompt.
  prefs: []
  type: TYPE_NORMAL
- en: You Should Use Elixir
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: I was very excited for this problem in particular because it‚Äôs literally the
    perfect application of Elixir and Phoenix. If you are building conversational
    agents, you should seriously consider giving Elixir a try. A large part of how
    quick this demo was to put together is because of how productive Elixir is.
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This was a fun technical challenge. I am pleased with the performance of the
    final demonstration. I‚Äôm also happy I was able to OSS a small library for others
    to build off of. If you are interested in conversational agents, I encourage you
    to check it out, give feedback, and contribute! I know it‚Äôs very rough right now,
    but it will get better with time.
  prefs: []
  type: TYPE_NORMAL
- en: Additionally, I plan to periodically build out the rest of the Nero project,
    so please follow me on [Twitter](https://twitter.com/sean_moriarity) if you‚Äôd
    like to stay up to date.
  prefs: []
  type: TYPE_NORMAL
