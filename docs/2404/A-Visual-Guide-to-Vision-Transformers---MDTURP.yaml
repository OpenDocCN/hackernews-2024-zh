- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: '类别: 未分类'
- en: 'date: 2024-05-27 13:15:05'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '日期: 2024-05-27 13:15:05'
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: A Visual Guide to Vision Transformers | MDTURP
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 一份关于视觉Transformer的视觉指南 | MDTURP
- en: 来源：[https://blog.mdturp.ch/posts/2024-04-05-visual_guide_to_vision_transformer.html](https://blog.mdturp.ch/posts/2024-04-05-visual_guide_to_vision_transformer.html)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://blog.mdturp.ch/posts/2024-04-05-visual_guide_to_vision_transformer.html](https://blog.mdturp.ch/posts/2024-04-05-visual_guide_to_vision_transformer.html)
- en: This is a visual guide to Vision Transformers (ViTs), a class of deep learning
    models that have achieved state-of-the-art performance on image classification
    tasks. Vision Transformers apply the transformer architecture, originally designed
    for natural language processing (NLP), to image data. This guide will walk you
    through the key components of Vision Transformers in a scroll story format, using
    visualizations and simple explanations to help you understand how these models
    work and how the flow of the data through the model looks like.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一份关于视觉Transformer（ViTs）的视觉指南，这类深度学习模型在图像分类任务上取得了最先进的性能。视觉Transformer将最初为自然语言处理（NLP）设计的Transformer架构应用于图像数据。本指南将以滚动故事的形式向您介绍视觉Transformer的关键组成部分，使用可视化和简单的解释来帮助您理解这些模型的工作原理以及数据在模型中的流动方式。
- en: Like normal convolutional neural networks, vision transformers are trained in
    a supervised manner. This means that the model is trained on a dataset of images
    and their corresponding labels.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 像普通的卷积神经网络一样，视觉Transformer是以监督方式进行训练的。这意味着模型在图像数据集及其相应标签的基础上进行训练。
- en: 1) Focus on one data point
  id: totrans-8
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1) 关注一个数据点
- en: 'To get a better understanding of what happens inside a vision transformer lets
    focus on a single data point (batch size of 1). And lets ask the question: How
    is this data point prepared in order to be consumed by a transformer?'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更好地理解视觉Transformer内部发生的事情，让我们专注于一个单独的数据点（批量大小为1）。让我们问一个问题：如何准备这个数据点以便被Transformer消耗？
- en: 2) Forget the label for the moment
  id: totrans-10
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2) 暂时忽略标签
- en: The label will become more relevant later. For now the only thing that we are
    left with is a single image.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 标签稍后会变得更加重要。现在我们唯一剩下的是一幅单独的图像。
- en: 3) Create patches of the image
  id: totrans-12
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3) 创建图像的补丁
- en: To prepare the image for the use inside the transformer we divide the image
    into equally sized patches of size **p x p**.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 为了在Transformer内部使用之前准备图像，我们将图像分割成大小为**p x p**的等尺寸补丁。
- en: 4) Flatting of the images patches
  id: totrans-14
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4) 对图像的补丁进行平坦化处理
- en: The patches are now flattened into vectors of dimension **p'= p²*c** where **p**
    is the size of the patch and **c** is the number of channels.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 现在将这些补丁展开为维度为**p'= p²*c**的向量，其中**p**是补丁的大小，**c**是通道数。
- en: 5) Creating patch embeddings
  id: totrans-16
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5) 创建补丁嵌入
- en: These image patch vectors are now encoded using a linear transformation. The
    resulting **Patch Embedding Vector** has a fixed size **d**.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 现在使用线性转换对这些图像补丁向量进行编码。生成的**补丁嵌入向量**具有固定大小**d**。
- en: 6) Embedding all patches
  id: totrans-18
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6) 嵌入所有补丁
- en: Now that we have embedded our image patches into vectors of fixed size, we are
    left with an array of size **n x d** where **n** is the the number of image patches
    and **d** is the size of the **patch embedding**
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经将图像块嵌入为固定大小的向量，我们得到了一个大小为**n x d**的数组，其中**n**是图像块的数量，**d**是**块嵌入**的大小。
- en: 7) Appending a classification token
  id: totrans-20
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7) 追加分类标记
- en: 'In order for us to effectively train our model we extend the array of patch
    embeddings by an additional vector called **classification token (cls token)**.
    This vector is a learnable parameter of the network and is randomly initialized.
    Note: We only have one cls token and we append the same vector for all data points.'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 为了有效训练我们的模型，我们通过添加名为**分类标记（cls token）**的额外向量来扩展块嵌入的数组。这个向量是网络的可学习参数，并且是随机初始化的。注意：我们只有一个cls
    token，并且我们为所有数据点追加相同的向量。
- en: 8) Add positional embedding Vectors
  id: totrans-22
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8) 添加位置嵌入向量
- en: Currently our **patch embeddings** have no positional information associated
    with them. We remedy that by adding a learnable randomly initialized **positional
    embedding vector** to all our patch embeddings. We also add a such a positional
    embedding vector to our **classification token**.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 目前我们的**块嵌入**没有与其相关联的位置信息。我们通过向所有块嵌入添加一个可学习的随机初始化的**位置嵌入向量**来修复这个问题。我们还向我们的**分类标记**添加了这样一个位置嵌入向量。
- en: 9) Transformer Input
  id: totrans-24
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9) Transformer 输入
- en: After the positional embedding vectors have been added we are left with an array
    of size **(n+1) x d** . This will be our input for the transformer which will
    be explained in greater detail in the next steps
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 添加了位置嵌入向量之后，我们得到了一个大小为**(n+1) x d**的数组。这将成为我们转换器的输入，在接下来的步骤中将有更详细的解释。
- en: '10.1) Transformer: QKV Creation'
  id: totrans-26
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 10.1) Transformer：QKV 创建
- en: Our transformer input patch embedding vectors are linearly embedded into multiple
    large vectors. These new vectors are than separated into three equal sized parts.
    The **Q - Query Vector**, the **K - Key Vector** and the **V - Value Vector**
    . We will have (n+1) of a all of those vectors.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的转换器输入块嵌入向量被线性嵌入到多个大向量中。这些新向量然后被分成三个大小相等的部分。**Q - 查询向量**，**K - 键向量**和**V -
    值向量**。我们将会有(n+1)个所有这些向量。
- en: '10.2) Transformer: Attention Score Calculation'
  id: totrans-28
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 10.2) Transformer：注意力分数计算
- en: To calculate our attention scores A we will now multiply all of our query vectors
    Q with all of our key vectors K.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将计算注意力分数矩阵A，方法是将所有查询向量Q与所有键向量K相乘。
- en: '10.3)Transformer: Attention Score Matrix'
  id: totrans-30
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 10.3) Transformer：注意力分数矩阵
- en: Now that we have the attention score matrix A we apply a `softmax` function
    to every row such that every row sums up to 1.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了注意力分数矩阵A，我们对每一行应用`softmax`函数，使得每一行的总和为1。
- en: '10.4)Transformer: Aggregated Contextual Information Calculation'
  id: totrans-32
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 10.4) Transformer：聚合上下文信息计算
- en: To calculate the **aggregated contextual information** for the first patch embedding
    vector. We focus on the **first row** of the attention matrix. And use the entires
    as weights for our **Value Vectors V**. The result is our **aggregated contextual
    information** vector for the first image patch embedding.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 为了计算第一个补丁嵌入向量的**聚合上下文信息**。我们关注注意力矩阵的**第一行**。并将这些条目用作我们**值向量 V**的权重。结果是我们第一个图像补丁嵌入的**聚合上下文信息**向量。
- en: '10.5)Transformer: Aggregated Contextual Information for every patch'
  id: totrans-34
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 10.5)变压器：每个补丁的聚合上下文信息
- en: Now we repeat this process for every row of our attention score matrix and the
    result will be N+1 aggregated contextual information vectors. One for every patch
    + one for the classification token. This steps concludes our first Attention Head.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们对我们的注意力分数矩阵的每一行重复这个过程，结果将是 N+1 个聚合的上下文信息向量。一个是每个补丁的 + 一个是分类标记。这一步骤完成了我们的第一个注意力头。
- en: '10.6)Transformer: Multi-Head Attention'
  id: totrans-36
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 10.6)变压器：多头注意力
- en: Now because we are dealing multi head attention we repeat the entire process
    from step **10.1 - 10-5** again with a different QKV mapping. For our explanatory
    setup we assume 2 Heads but typically a VIT has many more. In the end this results
    in multiple Aggregated contextual information vectors.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 现在因为我们处理多头注意力，我们从步骤**10.1 - 10.5**再次重复整个过程，使用不同的 QKV 映射。对于我们的解释设置，我们假设有 2 个头，但典型的
    VIT 通常有更多头。最终这将导致多个聚合的上下文信息向量。
- en: '10.7)Transformer: Last Attention Layer Step'
  id: totrans-38
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 10.7)变压器：最后的注意力层步骤
- en: These heads are stacked together and are mapped to vectors of size **d** which
    was the same size as our patch embeddings had.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 这些头部被堆叠在一起，并映射到大小为**d**的向量中，这与我们的补丁嵌入的大小相同。
- en: '10.8)Transformer: Attention Layer Result'
  id: totrans-40
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 10.8)变压器：注意力层结果
- en: The previous step concluded the attention layer and we are left with the same
    amount of embeddings of **exactly the same size** as we used as input.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的步骤完成了注意力层，我们剩下的是与我们作为输入使用的**完全相同大小**的嵌入。
- en: '10.9)Transformer: Residual connections'
  id: totrans-42
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 10.9)变压器：残差连接
- en: Transformers make heavy use of **residual connections** which simply means adding
    the input of the previous layer to the output the current layer. This is also
    something that we will do now.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 变压器大量使用**残差连接**，这意味着将前一层的输入简单地加到当前层的输出中。这也是我们现在要做的事情。
- en: '10.10)Transformer: Residual connection Result'
  id: totrans-44
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 10.10)变压器：残差连接结果
- en: The addition results in vectors of the same size.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 加法结果是相同大小的向量。
- en: '10.11)Transformer: Feed Forward Network'
  id: totrans-46
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 10.11)变压器：前馈网络
- en: Now these outputs are feed through a feed forward neural network with non linear
    activation functions
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 现在这些输出通过带有非线性激活函数的前馈神经网络进行馈送。
- en: '10.12)Transformer: Final Result'
  id: totrans-48
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 10.12)变压器：最终结果
- en: After the transformer step there is another residual connections which we will
    skip here for brevity. And so the last step concluded the transformer layer. In
    the end the transformer produced outputs of the same size as input.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 在Transformer步骤之后，还有另一个残差连接，由于篇幅限制我们将跳过。因此，最后一步完成了Transformer层。最终，Transformer生成了与输入大小相同的输出。
- en: 11) Repeat Transformers
  id: totrans-50
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 11) 重复Transformer
- en: Repeat the entire transformer calculation **Steps 10.1 - Steps 10.12** for the
    Transformer several times e.g. 6 times.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 重复整个Transformer计算**步骤10.1 - 步骤10.12**多次，例如6次。
- en: 12) Identify Classification token output
  id: totrans-52
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 12) 识别分类标记输出
- en: Last step is to identify the classification token output. This vector will be
    used in the final step of our Vision Transformer journey.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一步是识别分类标记输出。这个向量将在我们视觉Transformer旅程的最后一步中使用。
- en: '13) Final Step: Predicting classification probabilities'
  id: totrans-54
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 13) 最后一步：预测分类概率
- en: In the final and last step we use this classification output token and another
    fully connected neural network to predict the classification probabilities of
    our input image.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 在最后和最后一步中，我们使用这个分类输出标记和另一个全连接神经网络来预测我们输入图像的分类概率。
- en: We train the Vision Transformer using a standard cross-entropy loss function,
    which compares the predicted class probabilities with the true class labels. The
    model is trained using backpropagation and gradient descent, updating the model
    parameters to minimize the loss function.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用标准的交叉熵损失函数来训练视觉Transformer，该函数将预测的类别概率与真实的类别标签进行比较。模型使用反向传播和梯度下降进行训练，更新模型参数以最小化损失函数。
- en: In this visual guide, we have walked through the key components of Vision Transformers,
    from the data preparation to the training of the model. We hope this guide has
    helped you understand how Vision Transformers work and how they can be used to
    classify images.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个视觉指南中，我们已经详细介绍了视觉Transformer的关键组成部分，从数据准备到模型训练。我们希望这个指南能帮助您理解视觉Transformer的工作原理以及它们如何用于图像分类。
- en: If you have any questions or feedback, please feel free to reach out to me.
    Thank you for reading!
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您有任何问题或反馈，请随时与我联系。感谢您的阅读！
