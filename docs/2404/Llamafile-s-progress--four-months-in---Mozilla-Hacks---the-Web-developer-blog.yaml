- en: <!--yml
  id: totrans-split-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-split-1
  prefs: []
  type: TYPE_NORMAL
  zh: 类别：未分类
- en: 'date: 2024-05-27 13:31:01'
  id: totrans-split-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-05-27 13:31:01
- en: -->
  id: totrans-split-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: Llamafile’s progress, four months in - Mozilla Hacks - the Web developer blog
  id: totrans-split-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: llamafile 的进展，四个月后 - Mozilla Hacks - Web 开发者博客
- en: 来源：[https://hacks.mozilla.org/2024/04/llamafiles-progress-four-months-in/](https://hacks.mozilla.org/2024/04/llamafiles-progress-four-months-in/)
  id: totrans-split-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://hacks.mozilla.org/2024/04/llamafiles-progress-four-months-in/](https://hacks.mozilla.org/2024/04/llamafiles-progress-four-months-in/)
- en: When Mozilla’s Innovation group [first launched](https://hacks.mozilla.org/2023/11/introducing-llamafile/)
    the [llamafile project](https://github.com/Mozilla-Ocho/llamafile) late last year,
    we were thrilled by the immediate positive response from open source AI developers.
    It’s become one of Mozilla’s top three most-favorited repositories on GitHub,
    attracting a number of contributors, some excellent PRs, and a growing community
    on our [Discord server](https://discord.gg/YTgM42NZEr).
  id: totrans-split-6
  prefs: []
  type: TYPE_NORMAL
  zh: 当 Mozilla 的创新团队 [首次推出](https://hacks.mozilla.org/2023/11/introducing-llamafile/)
    去年年底推出 [llamafile 项目](https://github.com/Mozilla-Ocho/llamafile) 时，我们对来自开源 AI
    开发者的立即积极响应感到非常兴奋。它已成为 Mozilla 在 GitHub 上排名前三的最受喜爱的仓库之一，吸引了许多贡献者、一些出色的 PR，以及我们
    [Discord 服务器](https://discord.gg/YTgM42NZEr) 上不断增长的社区。
- en: Through it all, lead developer and project visionary [Justine Tunney](https://github.com/jart)
    has remained hard at work on a wide variety of fundamental improvements to the
    project. Just last night, Justine [shipped the v0.8 release of llamafile](https://github.com/Mozilla-Ocho/llamafile/releases/tag/0.8),
    which includes not only support for the very latest open models, but also a number
    of big performance improvements for CPU inference.
  id: totrans-split-7
  prefs: []
  type: TYPE_NORMAL
  zh: 在整个过程中，首席开发人员和项目愿景师 [Justine Tunney](https://github.com/jart) 一直在项目的各种基本改进上努力工作。就在昨晚，Justine
    [发布了 llamafile 的 v0.8 版本](https://github.com/Mozilla-Ocho/llamafile/releases/tag/0.8)，其中不仅包括对最新开放模型的支持，还有一些针对
    CPU 推理的重大性能改进。
- en: 'As a result of Justine’s work, today llamafile is both the easiest *and fastest*
    way to run a wide range of open large language models on your own hardware. See
    for yourself: with llamafile, you can run Meta’s just-released [LLaMA 3 model](https://huggingface.co/jartine/Meta-Llama-3-8B-Instruct-llamafile)–which
    rivals the very best models available in its size class–on an everyday Macbook.'
  id: totrans-split-8
  prefs: []
  type: TYPE_NORMAL
  zh: 多亏 Justine 的工作，今天 llamafile 成为在您自己的硬件上运行各种开放大语言模型最简单且*最快速*的方法。亲自体验：使用 llamafile，您可以在日常
    Macbook 上运行 Meta 刚发布的 [LLaMA 3 模型](https://huggingface.co/jartine/Meta-Llama-3-8B-Instruct-llamafile)——这款模型与其同类中最好的模型不相上下。
- en: How did we do it? To explain that, let’s take a step back and tell you about
    everything that’s changed since v0.1.
  id: totrans-split-9
  prefs: []
  type: TYPE_NORMAL
  zh: 我们是如何做到的？为了解释这一点，让我们退后一步，告诉您自 v0.1 以来发生的一切变化。
- en: '**tinyBLAS: democratizing GPU support for NVIDIA** ***and*** **AMD**'
  id: totrans-split-10
  prefs: []
  type: TYPE_NORMAL
  zh: '**tinyBLAS：为 NVIDIA GPU 支持民主化** ***和*** **AMD**'
- en: llamafile is built atop the now-legendary [llama.cpp](https://github.com/ggerganov/llama.cpp)
    project. llama.cpp supports GPU-accelerated inference for NVIDIA processors via
    the cuBLAS linear algebra library, but that requires users to install NVIDIA’s
    CUDA SDK. We felt uncomfortable with that fact, because it conflicts with our
    project goal of building a fully open-source and transparent AI stack that anyone
    can run on commodity hardware. And besides, getting CUDA set up correctly can
    be a bear on some systems. There had to be a better way.
  id: totrans-split-11
  prefs: []
  type: TYPE_NORMAL
  zh: llamafile 建立在如今已成传奇的 [llama.cpp](https://github.com/ggerganov/llama.cpp) 项目之上。llama.cpp
    通过 cuBLAS 线性代数库支持 NVIDIA 处理器的 GPU 加速推理，但这要求用户安装 NVIDIA 的 CUDA SDK。我们对此感到不安，因为这与我们构建一个完全开源和透明的
    AI 栈的项目目标相冲突，任何人都可以在商品硬件上运行。此外，在某些系统上正确设置 CUDA 可能会很麻烦。必须有更好的方法。
- en: 'With the community’s help (here’s looking at you, [@ahgamut](https://ahgamut.github.io/)
    and [@mrdomino](https://github.com/mrdomino)!), we created our own solution: it’s
    called tinyBLAS, and it’s llamafile’s brand-new and highly efficient linear algebra
    library. tinyBLAS makes NVIDIA acceleration simple and seamless for llamafile
    users. On Windows, you don’t even need to install CUDA at all; all you need is
    the display driver you’ve probably already installed.'
  id: totrans-split-12
  prefs: []
  type: TYPE_NORMAL
  zh: 借助社区的帮助（看这里，[@ahgamut](https://ahgamut.github.io/) 和 [@mrdomino](https://github.com/mrdomino)!），我们创造了自己的解决方案：它叫做
    tinyBLAS，是 llamafile 全新且高效的线性代数库。tinyBLAS 让 llamafile 用户轻松使用 NVIDIA 加速。在 Windows
    上，您甚至不需要安装 CUDA；您只需安装您可能已经安装的显示驱动程序。
- en: 'But tinyBLAS is about more than just NVIDIA: it supports AMD GPUs, as well.
    This is no small feat. While AMD commands a respectable 20% of today’s GPU market,
    poor software and driver support have historically made them a secondary player
    in the machine learning space. That’s a shame, given that AMD’s GPUs offer high
    performance, are price competitive, and are widely available.'
  id: totrans-split-13
  prefs: []
  type: TYPE_NORMAL
  zh: 但tinyBLAS不仅仅是支持NVIDIA：它也支持AMD的GPU。这绝非小事。虽然AMD在今天的GPU市场中占有可观的20%，但由于软件和驱动程序的支持较差，它们在机器学习领域一直是次要的参与者。这是一种遗憾，因为AMD的GPU提供高性能、具有价格竞争力且广泛可用。
- en: 'One of llamafile’s goals is to democratize access to open source AI technology,
    and that means getting AMD a seat at the table. That’s exactly what we’ve done:
    with llamafile’s tinyBLAS, you can now easily make full use of your AMD GPU to
    accelerate local inference. And, as with CUDA, if you’re a Windows user you don’t
    even have to install AMD’s ROCm SDK.'
  id: totrans-split-14
  prefs: []
  type: TYPE_NORMAL
  zh: llamafile的一个目标是使开源AI技术普及化，这意味着为AMD争取一个位置。这正是我们所做的：有了llamafile的tinyBLAS，你现在可以轻松地充分利用你的AMD
    GPU来加速本地推理。而且，就像CUDA一样，如果你是Windows用户，甚至不需要安装AMD的ROCm SDK。
- en: All of this means that, for many users, llamafile will automatically use your
    GPU right out of the box, with little to no effort on your part.
  id: totrans-split-15
  prefs: []
  type: TYPE_NORMAL
  zh: 所有这些意味着对许多用户来说，llamafile将自动使用你的GPU，无需或仅需很少的努力。
- en: '**CPU performance gains for faster local AI**'
  id: totrans-split-16
  prefs: []
  type: TYPE_NORMAL
  zh: '**更快本地AI的CPU性能提升**'
- en: Here at Mozilla, we are keenly interested in the promise of “local AI,” in which
    AI models and applications run directly on end-user hardware instead of in the
    cloud. Local AI is exciting because it opens up the possibility of more user control
    over these systems and greater privacy and security for users.
  id: totrans-split-17
  prefs: []
  type: TYPE_NORMAL
  zh: 在Mozilla，我们对“本地AI”的前景非常感兴趣，即AI模型和应用直接在最终用户硬件上运行，而不是在云中。本地AI令人兴奋，因为它使得用户更有可能控制这些系统，并为用户提供更大的隐私和安全性。
- en: But many consumer devices lack the high-end GPUs that are often required for
    inference tasks. llama.cpp has been a game-changer in this regard because it makes
    local inference both possible and usably performant on CPUs instead of just GPUs.
  id: totrans-split-18
  prefs: []
  type: TYPE_NORMAL
  zh: 但许多消费设备缺乏通常需要用到的高端GPU。在这方面，llama.cpp是一个改变游戏规则的因素，因为它使得在CPU上进行本地推理变得可能且性能可用，而不仅限于GPU。
- en: Justine’s recent work on llamafile has now pushed the state of the art even
    further. As documented in [her detailed blog post](http://justine.lol/matmul/)
    on the subject, by writing 84 new matrix multiplication kernels she was able to
    increase llamafile’s prompt evaluation performance by an astonishing 10x compared
    to our previous release. This is a substantial and impactful step forward in the
    quest to make local AI viable on consumer hardware.
  id: totrans-split-19
  prefs: []
  type: TYPE_NORMAL
  zh: Justine在llamafile上的最新工作进一步推动了技术前沿。正如她在[关于此主题的详细博客文章](http://justine.lol/matmul/)中所记录的，通过编写84个新的矩阵乘法核心，她成功将llamafile的快速评估性能提升了惊人的10倍，与我们之前的版本相比。这是在推动使消费者硬件上的本地AI变得可行方面的重要而有影响力的一步。
- en: This work is also a great example of our commitment to the open source AI community.
    After completing this work we immediately [submitted a PR](https://github.com/ggerganov/llama.cpp/pull/6414)
    to upstream these performance improvements to llama.cpp. This was just the latest
    of a number of enhancements we’ve contributed back to llama.cpp, a practice we
    plan to continue.
  id: totrans-split-20
  prefs: []
  type: TYPE_NORMAL
  zh: 这项工作也是我们致力于开源AI社区的一个典范。完成这项工作后，我们立即向llama.cpp的上游[提交了一个PR](https://github.com/ggerganov/llama.cpp/pull/6414)，以将这些性能改进反馈到llama.cpp。这只是我们为llama.cpp贡献的众多增强措施中的最新一项，而我们计划继续这种实践。
- en: '**Raspberry Pi performance gains**'
  id: totrans-split-21
  prefs: []
  type: TYPE_NORMAL
  zh: '**树莓派性能提升**'
- en: Speaking of consumer hardware, there are few examples that are both more interesting
    and more humble than the beloved Raspberry Pi. For a bargain basement price, you
    get a full-featured computer running Linux with plenty of computing power for
    typical desktop uses. It’s an impressive package, but historically it hasn’t been
    considered a viable platform for AI applications.
  id: totrans-split-22
  prefs: []
  type: TYPE_NORMAL
  zh: 谈到消费者硬件，没有比备受喜爱的树莓派更有趣且更谦卑的例子了。以超值价格，你可以获得一台运行Linux的功能齐全计算机，具备足够的计算能力以满足典型桌面应用需求。这是一份令人印象深刻的套餐，但从历史上看，它并不被认为是AI应用的可行平台。
- en: Not any more. llamafile has now been optimized for the latest model (the Raspberry
    Pi 5), and the result is that a number of small LLMs–such as Rocket-3B ([download](https://huggingface.co/jartine/rocket-3B-llamafile/resolve/main/rocket-3b.Q5_K_M.llamafile?download=true)),
    TinyLLaMA-1.5B ([download](https://huggingface.co/jartine/TinyLlama-1.1B-Chat-v1.0-GGUF/resolve/main/TinyLlama-1.1B-Chat-v1.0.Q5_K_M.llamafile?download=true)),
    and Phi-2 ([download](https://huggingface.co/jartine/phi-2-llamafile/resolve/main/phi-2.Q5_K_M.llamafile?download=true))–run
    at usable speeds on one of the least expensive computers available today. We’ve
    seen prompt evaluation speeds of [up to 80 tokens/sec](https://twitter.com/JustineTunney/status/1776440470152867930)
    in some cases!
  id: totrans-split-23
  prefs: []
  type: TYPE_NORMAL
  zh: 不再是这样了。llamafile现在已经针对最新模型（例如Raspberry Pi 5）进行了优化，结果是一些小型LLM（如Rocket-3B ([下载](https://huggingface.co/jartine/rocket-3B-llamafile/resolve/main/rocket-3b.Q5_K_M.llamafile?download=true))、TinyLLaMA-1.5B
    ([下载](https://huggingface.co/jartine/TinyLlama-1.1B-Chat-v1.0-GGUF/resolve/main/TinyLlama-1.1B-Chat-v1.0.Q5_K_M.llamafile?download=true))和Phi-2
    ([下载](https://huggingface.co/jartine/phi-2-llamafile/resolve/main/phi-2.Q5_K_M.llamafile?download=true))）在今天最便宜的计算机上运行速度可用。我们在某些情况下看到了高达[每秒80个token](https://twitter.com/JustineTunney/status/1776440470152867930)的快速评估速度！
- en: '**Keeping up with the latest models**'
  id: totrans-split-24
  prefs: []
  type: TYPE_NORMAL
  zh: '**跟上最新模型**'
- en: The pace of progress in the open model space has been [stunningly fast](https://twitter.com/maximelabonne/status/1779123021480865807).
    Over the past few months, hundreds of models have been released or updated via
    fine-tuning. Along the way, there has been a clear trend of ever-increasing model
    performance and ever-smaller model sizes.
  id: totrans-split-25
  prefs: []
  type: TYPE_NORMAL
  zh: 在开放模型领域，进展的速度 [惊人地快](https://twitter.com/maximelabonne/status/1779123021480865807)。在过去几个月里，通过微调已发布或更新了数百个模型。沿途，明显有一个明显的趋势，即模型性能不断提升，模型大小也在不断减小。
- en: The llama.cpp project has been doing an excellent job of keeping up with all
    of these new models, frequently rolling-out support for new architectures and
    model features within days of their release.
  id: totrans-split-26
  prefs: []
  type: TYPE_NORMAL
  zh: llama.cpp项目一直在积极跟进所有这些新模型，经常在发布几天内就支持新架构和模型功能。
- en: For our part we’ve been keeping llamafile closely synced with llama.cpp so that
    we can support all the same models. Given the complexity of both projects, this
    has been no small feat, so we’re lucky to have Justine on the case.
  id: totrans-split-27
  prefs: []
  type: TYPE_NORMAL
  zh: 就我们而言，我们一直在紧密同步llamafile和llama.cpp，以便支持所有相同的模型。鉴于这两个项目的复杂性，这绝非小事，所以我们很幸运有Justine负责。
- en: Today, you can today use the very latest and most capable open models with llamafile
    thanks to her hard work. For example, we were able to roll-out llamafiles for
    Meta’s newest LLaMA 3 models–[8B-Instruct](https://huggingface.co/jartine/Meta-Llama-3-8B-Instruct-llamafile)
    and [70B-Instruct](https://huggingface.co/jartine/Meta-Llama-3-70B-Instruct-llamafile)–within
    a day of their release. With yesterday’s 0.8 release, llamafile can also run Grok,
    Mixtral 8x22B, and Command-R.
  id: totrans-split-28
  prefs: []
  type: TYPE_NORMAL
  zh: 今天，由于她的辛勤工作，你可以使用llamafile使用最新最强大的开放模型。例如，我们能够在Meta最新的LLaMA 3模型- [8B-Instruct](https://huggingface.co/jartine/Meta-Llama-3-8B-Instruct-llamafile)
    和 [70B-Instruct](https://huggingface.co/jartine/Meta-Llama-3-70B-Instruct-llamafile)发布当天就推出llamafile。昨天的0.8版本中，llamafile还可以运行Grok、Mixtral
    8x22B和Command-R。
- en: '**Creating your own llamafiles**'
  id: totrans-split-29
  prefs: []
  type: TYPE_NORMAL
  zh: '**创建你自己的llamafiles**'
- en: 'Since the day that llamafile shipped people have wanted to create their own
    llamafiles. Previously, this required a number of steps, but today you can do
    it with a single command, e.g.:'
  id: totrans-split-30
  prefs: []
  type: TYPE_NORMAL
  zh: 自从llamafile发布以来，人们就一直想要创建自己的llamafiles。以前，这需要几个步骤，但今天你可以用一个命令完成，例如：
- en: '`llamafile-convert [model.gguf]`'
  id: totrans-split-31
  prefs: []
  type: TYPE_NORMAL
  zh: '`llamafile-convert [model.gguf]`'
- en: In just moments, this will produce a “model.llamafile” file that is ready for
    immediate use. Our thanks to community member [@chan1012](https://github.com/chand1012)
    for contributing this helpful improvement.
  id: totrans-split-32
  prefs: []
  type: TYPE_NORMAL
  zh: 就在这一刻，这将生成一个名为“model.llamafile”的文件，可以立即使用。感谢社区成员[@chan1012](https://github.com/chand1012)为改进做出的贡献。
- en: In a related development, Hugging Face recently added official support for llamafile
    within their model hub. This means you can now [search and filter](https://huggingface.co/models?library=llamafile&sort=trending)
    Hugging Face specifically for llamafiles created and distributed by other people
    in the open source community.
  id: totrans-split-33
  prefs: []
  type: TYPE_NORMAL
  zh: 在相关发展中，Hugging Face最近在其模型中心正式支持了llamafile。这意味着你现在可以通过特定于llamafile的搜索和过滤，搜索其他开源社区成员创建和分发的llamafile。
- en: '**OpenAI-compatible API server**'
  id: totrans-split-34
  prefs: []
  type: TYPE_NORMAL
  zh: '**兼容OpenAI API服务器**'
- en: 'Since it’s built on top of llama.cpp, llamafile inherits that project’s server
    component, which provides OpenAI-compatible API endpoints. This enables developers
    who are building on top of OpenAI to switch to using open models instead. At Mozilla
    we very much want to support this kind of future: one where open-source AI is
    a viable alternative to centralized, closed, commercial offerings.'
  id: totrans-split-35
  prefs: []
  type: TYPE_NORMAL
  zh: 由于它建立在llama.cpp之上，llamafile继承了该项目的服务器组件，提供了兼容OpenAI API的端点。这使得那些在OpenAI基础上构建的开发者可以转而使用开源模型。在Mozilla，我们非常希望支持这种未来：一个开源AI可以作为集中式、封闭、商业化解决方案的可行替代品。
- en: While open models do not yet fully rival the capabilities of closed models,
    they’re making rapid progress. We believe that making it easier to pivot existing
    code over to executing against open models will increase demand and further fuel
    this progress.
  id: totrans-split-36
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然开源模型目前还不能完全与封闭模型的能力相媲美，但它们正在快速进步。我们相信，使现有代码更容易转向使用开源模型将增加需求，并进一步推动这一进展。
- en: Over the past few months, we’ve invested effort in extending these endpoints,
    both to increase functionality and improve compatibility. Today, llamafile can
    serve as a drop-in replacement for OpenAI in a wide variety of use cases.
  id: totrans-split-37
  prefs: []
  type: TYPE_NORMAL
  zh: 在过去几个月中，我们努力扩展这些端点，旨在增强功能和提升兼容性。如今，llamafile可以在各种用例中作为OpenAI的即插即用替代品。
- en: We want to further extend our API server’s capabilities, and we’re eager to
    hear what developers want and need. What’s holding you back from using open models?
    What features, capabilities, or tools do you need? [Let us know](https://discord.gg/YTgM42NZEr)!
  id: totrans-split-38
  prefs: []
  type: TYPE_NORMAL
  zh: 我们希望进一步扩展API服务器的功能，并且渴望听到开发者的需求和反馈。您因何未采用开源模型？您需要哪些功能、能力或工具？[请告诉我们](https://discord.gg/YTgM42NZEr)！
- en: '**Integrations with other open source AI projects**'
  id: totrans-split-39
  prefs: []
  type: TYPE_NORMAL
  zh: '**与其他开源AI项目的集成**'
- en: Finally, it’s been a delight to see llamafile adopted by independent developers
    and integrated into leading open source AI projects (like [Open Interpreter](https://github.com/OpenInterpreter/open-interpreter)).
    Kudos in particular to our own [Kate Silverstein](https://github.com/k8si) who
    landed PRs that add llamafile support to [LangChain](https://github.com/langchain-ai/langchain)
    and [LlamaIndex](https://github.com/run-llama/llama_index) (with [AutoGPT](https://github.com/Significant-Gravitas/AutoGPT)
    coming soon).
  id: totrans-split-40
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，看到独立开发者采纳llamafile并将其整合到领先的开源AI项目中（例如[Open Interpreter](https://github.com/OpenInterpreter/open-interpreter)），实在是令人欣喜。特别要表扬我们自己的[Kate
    Silverstein](https://github.com/k8si)，她提交的PR为[LangChain](https://github.com/langchain-ai/langchain)和[LlamaIndex](https://github.com/run-llama/llama_index)增加了llamafile支持（[AutoGPT](https://github.com/Significant-Gravitas/AutoGPT)即将推出）。
- en: If you’re a maintainer or contributor to an open source AI project that you
    feel would benefit from llamafile integration, [let us know how we can help](https://discord.gg/YTgM42NZEr).
  id: totrans-split-41
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您是维护者或开源AI项目的贡献者，认为llamafile集成会对项目有所裨益，[请告诉我们如何提供帮助](https://discord.gg/YTgM42NZEr)。
- en: '**Join us!**'
  id: totrans-split-42
  prefs: []
  type: TYPE_NORMAL
  zh: '**加入我们吧！**'
- en: 'The llamafile project is just getting started, and it’s also only the first
    step in a major new initiative on Mozilla’s part to contribute to and participate
    in the open source AI community. We’ll have more to share about that soon, but
    for now: I invite you to join us on the llamafile project!'
  id: totrans-split-43
  prefs: []
  type: TYPE_NORMAL
  zh: llamafile项目刚刚起步，这也是Mozilla致力于贡献和参与开源AI社区的重大新举措的第一步。我们将很快分享更多相关内容，但现在：我邀请您加入我们的llamafile项目！
- en: The best place to connect with both the llamafile team at Mozilla and the overall
    llamafile community is over at our Discord server, which has [a dedicated channel
    just for llamafile](https://discord.gg/YTgM42NZEr). And of course, your enhancement
    requests, issues, and PRs are always welcome over at our [GitHub repo](https://github.com/Mozilla-Ocho/llamafile).
  id: totrans-split-44
  prefs: []
  type: TYPE_NORMAL
  zh: 与Mozilla的llamafile团队和整个llamafile社区交流的最佳场所是我们的Discord服务器，其中有[专门的llamafile频道](https://discord.gg/YTgM42NZEr)。当然，您在我们的[GitHub仓库](https://github.com/Mozilla-Ocho/llamafile)上的增强请求、问题和PR，我们也随时欢迎。
- en: I hope you’ll join us. The next few months are going to be even more interesting
    and unexpected than the last, both for llamafile and for open source AI itself.
  id: totrans-split-45
  prefs: []
  type: TYPE_NORMAL
  zh: 希望您能加入我们。接下来的几个月对于llamafile和开源AI本身将会比过去更加有趣和意外。
- en: Stephen leads open source AI projects (including llamafile) in Mozilla's Innovation
    group. He previously managed social bookmarking pioneer del.icio.us; co-founded
    Storium, Blockboard, and FairSpin; and worked on Yahoo Search and BEA WebLogic.
  id: totrans-split-46
  prefs: []
  type: TYPE_NORMAL
  zh: Stephen在Mozilla的创新团队中领导开源AI项目（包括llamafile）。他以前管理过社交书签先驱del.icio.us；共同创立了Storium、Blockboard和FairSpin；并参与了Yahoo搜索和BEA
    WebLogic的工作。
- en: '[More articles by Stephen Hood…](https://hacks.mozilla.org/author/slangtonhoodmozilla-com/)'
  id: totrans-split-47
  prefs: []
  type: TYPE_NORMAL
  zh: '[更多由斯蒂芬·胡德（Stephen Hood）撰写的文章…](https://hacks.mozilla.org/author/slangtonhoodmozilla-com/)'
