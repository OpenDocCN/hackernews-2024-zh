- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-05-27 13:21:32'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: Natural Language Processing in Bash
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://massimo-nazaria.github.io/nlp.html](https://massimo-nazaria.github.io/nlp.html)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Let’s implement a Bash toolchain to generate random prose that resembles the
    text corpus by using the *n*-gram language model!
  prefs: []
  type: TYPE_NORMAL
- en: Basically, [NLP](https://en.wikipedia.org/wiki/Natural_language_processing)
    aims at teaching computers to understand and work with human language by using
    different techniques.
  prefs: []
  type: TYPE_NORMAL
- en: In what follows, we’re going to have a glimpse at the toolchain by training
    our model using the novel [*Moby-Dick*](https://gutenberg.org/ebooks/15).
  prefs: []
  type: TYPE_NORMAL
- en: 'First of all, let’s get the `bash-textgen/` folder from the repository and
    enter it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Preprocessing
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In a typical NLP process, a text corpus (i.e. input data) is preprocessed by
    organizing it into structured data before training mathematical models.
  prefs: []
  type: TYPE_NORMAL
- en: This involves tasks like tokenization (i.e. breaking the text into words) or
    data cleaning (i.e. removing any non-relevant text).
  prefs: []
  type: TYPE_NORMAL
- en: 'For our example, let’s extract the single words from `moby-dick.txt` by removing
    unnecessary characters and put the result in `words.txt`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Initial 10 extracted words:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'As you can see, the initial words include: the title, the author full name,
    the 1st chapter title, and the incipit *Call me Ishmael*.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Precisely, `words.sh` performs the following text transformations:'
  prefs: []
  type: TYPE_NORMAL
- en: Make all input text lowercase;
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Remove non-alphabetical characters except for the periods `.`;
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Remove multiple whitespaces and period characters;
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Output one word (or period) per line.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Training
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: During the training process, NLP models learn patterns and relationships in
    the language from the preprocessed training data.
  prefs: []
  type: TYPE_NORMAL
- en: This way, the resulting trained models can provide a variety of services like
    sentiment analysis and text generation.
  prefs: []
  type: TYPE_NORMAL
- en: The *N*-Gram Language Model
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '*N*-gram language models are easy to understand and implement.'
  prefs: []
  type: TYPE_NORMAL
- en: While [considered state-of-the-art](https://en.wikipedia.org/wiki/Natural_language_processing#Neural_NLP_(present))
    in the early days of NLP, nowadays more advanced NLP techniques outperform *n*-grams
    for text generation.
  prefs: []
  type: TYPE_NORMAL
- en: In spite of that, they still work very well for next-word suggestions or auto-completion
    in text editors.
  prefs: []
  type: TYPE_NORMAL
- en: Training data preparation consists in organizing the preprocessed words from
    the text corpus into *n*-tuples of consecutive words, namely *n*-grams.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s do this by computing [bigrams](https://en.wikipedia.org/wiki/Word_n-gram_language_model#Bigram_model)
    (i.e. 2-grams) out of the extracted words:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Initial 10 bigrams computed:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: During the training process, the *n*-gram language model learns to predict the
    next word in a sentence based on the previous *n*-1 words.
  prefs: []
  type: TYPE_NORMAL
- en: Thus, with bigrams they learn to predict next word based on just the previous
    word in the sentence.
  prefs: []
  type: TYPE_NORMAL
- en: Text Generation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Let’s generate text from the computed bigrams starting from the initial word
    “the”.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: '*Output:*'
  prefs: []
  type: TYPE_NORMAL
- en: the spare poles and more certain wild creatures to his retired whaleman as he
    never tell the pier heads to this inclined for the imposed and i lay them endless
    sculptures.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Not bad at all! The generated prose actually mimics the style of Herman Melville
    from his novel we initially used as our text corpus.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s try it again with the initial word “man”:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: '*Output:*'
  prefs: []
  type: TYPE_NORMAL
- en: man from the reminiscence even for the thames tunnel then tow line not have
    to the whale and selecting our hemisphere.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: It’s kind of surprising (and funny!) to see how in a few lines of Bash our tool
    can emulate the poetry of such a literary giant.
  prefs: []
  type: TYPE_NORMAL
- en: 'In particular, in the example above `textgen.sh` starts generating a sentence
    from a given initial word as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Let “man” be the initial given word.
  prefs: []
  type: TYPE_NORMAL
- en: '*Step 1: Get all the bigrams starting with “man”*'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'The result of the command above is a list of bigrams, e.g.:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Please note that the list of bigrams generally contains a lot of duplicates.
    Among them, some bigrams will show up much more frequently than the others.
  prefs: []
  type: TYPE_NORMAL
- en: And the most common bigrams will be the most likely to be selected in the next
    steps in order to extract our next word.
  prefs: []
  type: TYPE_NORMAL
- en: '*Step 2: Shuffle all such bigrams*'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'We randomly rearrange the list of bigrams so as to avoid extracting always
    the same next word. Example of shuffled list:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'From the shuffled list above, we pick the 1st bigram:'
  prefs: []
  type: TYPE_NORMAL
- en: In our example, “from” is our next word in the generated sentence!
  prefs: []
  type: TYPE_NORMAL
- en: Note that we could have chosen any bigram from the shuffled list, without loss
    of generality.
  prefs: []
  type: TYPE_NORMAL
- en: In fact, the most common bigrams will always be the more likely to be extracted,
    regardless of their position in the shuffled list.
  prefs: []
  type: TYPE_NORMAL
- en: '*Final Step: Get the 2nd word from the bigram*'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Clearly, the final command in the pipeline `cut -d '' '' -f2` extracts the
    2nd word in the bigram:'
  prefs: []
  type: TYPE_NORMAL
- en: The above generation steps repeat iteratively word-by-word until either the
    next word is a period character or no next-word is present.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s play a little bit more with the toolchain by using [trigrams](https://en.wikipedia.org/wiki/Word_n-gram_language_model#Trigram_model),
    namely *n*-grams with *n*=3:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: In general, `ngrams.sh` accepts as its argument any natural integer starting
    from 2.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s generate a random sentence starting from the initial 2 words “a man”.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: '*Output:*'
  prefs: []
  type: TYPE_NORMAL
- en: a man gives himself out of this blubbering now we are about to begin from the
    hearts of whales is always under great and extraordinary difficulties that every
    one knows the fine carnation of their yet suspended boats.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Let’s now try the initial 2 words “by falling”:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: '*Output:*'
  prefs: []
  type: TYPE_NORMAL
- en: by falling in the best harpooneers of nantucket both messmates of mine tis not
    me all loveliness is anguish to me we are to credit the old craft s cockpits especially
    of such a whale ship which like a hot sun s pilot yesterday i wrecked thee and
    wrong not captain ahab if it might light upon for all deficiencies of that is
    a new born sight.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Let’s try with “one moment”:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: '*Output:*'
  prefs: []
  type: TYPE_NORMAL
- en: one moment to watch the same no the reason of numerous rude scratches altogether
    of an enormous practical resolution in facing death this old whale hunter of the
    word why don t you all the men swung in the deep yet is there any reason possessed
    the most ancient extant portrait anyways purporting to be tried out without being
    taken out of all voyages now or never for a time when they come from a drooping
    orchard twig.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: What a fun!
  prefs: []
  type: TYPE_NORMAL
- en: The Implementation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[***Get the toolchain usage and code!***](https://github.com/massimo-nazaria/bash-textgen
    "GitHub Repository")'
  prefs: []
  type: TYPE_NORMAL
- en: Please read also *[“Unix Philosophy with an Example”](/unix-philosophy.html)*
    to learn how to compose multiple commands together as seen in this tutorial.
  prefs: []
  type: TYPE_NORMAL
