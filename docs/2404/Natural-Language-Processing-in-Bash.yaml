- en: <!--yml
  id: totrans-split-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-split-1
  prefs: []
  type: TYPE_NORMAL
  zh: 类别：未分类
- en: 'date: 2024-05-27 13:21:32'
  id: totrans-split-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-05-27 13:21:32
- en: -->
  id: totrans-split-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: Natural Language Processing in Bash
  id: totrans-split-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Bash中的自然语言处理
- en: 来源：[https://massimo-nazaria.github.io/nlp.html](https://massimo-nazaria.github.io/nlp.html)
  id: totrans-split-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://massimo-nazaria.github.io/nlp.html](https://massimo-nazaria.github.io/nlp.html)
- en: Let’s implement a Bash toolchain to generate random prose that resembles the
    text corpus by using the *n*-gram language model!
  id: totrans-split-6
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们实现一个Bash工具链，通过使用*n*-gram语言模型生成类似文本语料库的随机散文！
- en: Basically, [NLP](https://en.wikipedia.org/wiki/Natural_language_processing)
    aims at teaching computers to understand and work with human language by using
    different techniques.
  id: totrans-split-7
  prefs: []
  type: TYPE_NORMAL
  zh: 基本上，[NLP](https://en.wikipedia.org/wiki/Natural_language_processing)旨在通过使用不同的技术，教会计算机理解和处理人类语言。
- en: In what follows, we’re going to have a glimpse at the toolchain by training
    our model using the novel [*Moby-Dick*](https://gutenberg.org/ebooks/15).
  id: totrans-split-8
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将通过使用小说[*白鲸*](https://gutenberg.org/ebooks/15)训练我们的模型来一窥工具链。
- en: 'First of all, let’s get the `bash-textgen/` folder from the repository and
    enter it:'
  id: totrans-split-9
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们从仓库中获取`bash-textgen/`文件夹并进入它：
- en: '[PRE0]'
  id: totrans-split-10
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Preprocessing
  id: totrans-split-11
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 预处理
- en: In a typical NLP process, a text corpus (i.e. input data) is preprocessed by
    organizing it into structured data before training mathematical models.
  id: totrans-split-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在典型的NLP过程中，文本语料库（即输入数据）在训练数学模型之前被预处理成结构化数据。
- en: This involves tasks like tokenization (i.e. breaking the text into words) or
    data cleaning (i.e. removing any non-relevant text).
  id: totrans-split-13
  prefs: []
  type: TYPE_NORMAL
  zh: 这包括诸如分词（即将文本分解为单词）或数据清洗（即去除任何非相关文本）的任务。
- en: 'For our example, let’s extract the single words from `moby-dick.txt` by removing
    unnecessary characters and put the result in `words.txt`:'
  id: totrans-split-14
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，让我们从`moby-dick.txt`中提取单词，通过删除不必要的字符并将结果放入`words.txt`：
- en: '[PRE1]'
  id: totrans-split-15
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Initial 10 extracted words:'
  id: totrans-split-16
  prefs: []
  type: TYPE_NORMAL
  zh: 初始提取的前10个单词：
- en: '[PRE2]'
  id: totrans-split-17
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'As you can see, the initial words include: the title, the author full name,
    the 1st chapter title, and the incipit *Call me Ishmael*.'
  id: totrans-split-18
  prefs: []
  type: TYPE_NORMAL
  zh: 正如您所见，初始词语包括：标题，作者全名，第1章标题，以及开头的*叫我以实玛利*。
- en: 'Precisely, `words.sh` performs the following text transformations:'
  id: totrans-split-19
  prefs: []
  type: TYPE_NORMAL
  zh: 具体来说，`words.sh`执行以下文本转换：
- en: Make all input text lowercase;
  id: totrans-split-20
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将所有输入文本转换为小写；
- en: Remove non-alphabetical characters except for the periods `.`;
  id: totrans-split-21
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 移除除句号`.`外的非字母字符；
- en: Remove multiple whitespaces and period characters;
  id: totrans-split-22
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 移除多余的空格和句点字符；
- en: Output one word (or period) per line.
  id: totrans-split-23
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 每行输出一个单词（或句号）。
- en: Training
  id: totrans-split-24
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 训练
- en: During the training process, NLP models learn patterns and relationships in
    the language from the preprocessed training data.
  id: totrans-split-25
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练过程中，NLP模型从预处理的训练数据中学习语言中的模式和关系。
- en: This way, the resulting trained models can provide a variety of services like
    sentiment analysis and text generation.
  id: totrans-split-26
  prefs: []
  type: TYPE_NORMAL
  zh: 这样，生成的训练模型可以提供各种服务，如情感分析和文本生成。
- en: The *N*-Gram Language Model
  id: totrans-split-27
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '*N*-Gram语言模型'
- en: '*N*-gram language models are easy to understand and implement.'
  id: totrans-split-28
  prefs: []
  type: TYPE_NORMAL
  zh: '*N*-gram语言模型易于理解和实现。'
- en: While [considered state-of-the-art](https://en.wikipedia.org/wiki/Natural_language_processing#Neural_NLP_(present))
    in the early days of NLP, nowadays more advanced NLP techniques outperform *n*-grams
    for text generation.
  id: totrans-split-29
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然在NLP早期被认为是*最先进*技术，但现在更先进的NLP技术比*n*-gram在文本生成方面表现更好。
- en: In spite of that, they still work very well for next-word suggestions or auto-completion
    in text editors.
  id: totrans-split-30
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管如此，它们在文本编辑器中的下一个词建议或自动完成方面仍然非常有效。
- en: Training data preparation consists in organizing the preprocessed words from
    the text corpus into *n*-tuples of consecutive words, namely *n*-grams.
  id: totrans-split-31
  prefs: []
  type: TYPE_NORMAL
  zh: 训练数据准备工作包括将文本语料库中预处理的单词组织成连续的*n*-元组，即*n*-grams。
- en: 'Let’s do this by computing [bigrams](https://en.wikipedia.org/wiki/Word_n-gram_language_model#Bigram_model)
    (i.e. 2-grams) out of the extracted words:'
  id: totrans-split-32
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过计算[bigrams](https://en.wikipedia.org/wiki/Word_n-gram_language_model#Bigram_model)（即2-gram）来做到这一点：
- en: '[PRE3]'
  id: totrans-split-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Initial 10 bigrams computed:'
  id: totrans-split-34
  prefs: []
  type: TYPE_NORMAL
  zh: 计算的前10个bigrams：
- en: '[PRE4]'
  id: totrans-split-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: During the training process, the *n*-gram language model learns to predict the
    next word in a sentence based on the previous *n*-1 words.
  id: totrans-split-36
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练过程中，*n*-gram语言模型学会根据前*n*-1个词预测句子中的下一个词。
- en: Thus, with bigrams they learn to predict next word based on just the previous
    word in the sentence.
  id: totrans-split-37
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，使用bigrams，它们学会根据句子中仅有的前一个词预测下一个词。
- en: Text Generation
  id: totrans-split-38
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 文本生成
- en: Let’s generate text from the computed bigrams starting from the initial word
    “the”.
  id: totrans-split-39
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从初始单词“the”开始，基于计算的bigrams生成文本。
- en: '[PRE5]'
  id: totrans-split-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '*Output:*'
  id: totrans-split-41
  prefs: []
  type: TYPE_NORMAL
  zh: '*输出：*'
- en: the spare poles and more certain wild creatures to his retired whaleman as he
    never tell the pier heads to this inclined for the imposed and i lay them endless
    sculptures.
  id: totrans-split-42
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 对于他退休的捕鲸人来说，备用桅杆和更确定的野生动物，他从未告诉过码头头领，对此倾斜是为了强加的，我将它们作为无尽的雕塑。
- en: Not bad at all! The generated prose actually mimics the style of Herman Melville
    from his novel we initially used as our text corpus.
  id: totrans-split-43
  prefs: []
  type: TYPE_NORMAL
  zh: 生成的散文实际上非常不错！它模仿了我们最初用作文本语料库的赫尔曼·梅尔维尔的风格。
- en: 'Let’s try it again with the initial word “man”:'
  id: totrans-split-44
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们再次尝试初始词“man”：
- en: '[PRE6]'
  id: totrans-split-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '*Output:*'
  id: totrans-split-46
  prefs: []
  type: TYPE_NORMAL
  zh: '*输出：*'
- en: man from the reminiscence even for the thames tunnel then tow line not have
    to the whale and selecting our hemisphere.
  id: totrans-split-47
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 从回忆中的男人，即使对于泰晤士隧道，然后拖线不必去鲸鱼，选择我们的半球。
- en: It’s kind of surprising (and funny!) to see how in a few lines of Bash our tool
    can emulate the poetry of such a literary giant.
  id: totrans-split-48
  prefs: []
  type: TYPE_NORMAL
  zh: 看到我们的工具如何在几行Bash代码中模拟如此文学巨匠的诗意，这有点令人惊讶（而且有趣！）。
- en: 'In particular, in the example above `textgen.sh` starts generating a sentence
    from a given initial word as follows:'
  id: totrans-split-49
  prefs: []
  type: TYPE_NORMAL
  zh: 特别是，在上面的例子中，`textgen.sh`从给定的初始词开始生成句子如下：
- en: Let “man” be the initial given word.
  id: totrans-split-50
  prefs: []
  type: TYPE_NORMAL
  zh: 让“man”成为初始给定的单词。
- en: '*Step 1: Get all the bigrams starting with “man”*'
  id: totrans-split-51
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '*步骤 1：获取所有以“man”开头的二元组*'
- en: '[PRE7]'
  id: totrans-split-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'The result of the command above is a list of bigrams, e.g.:'
  id: totrans-split-53
  prefs: []
  type: TYPE_NORMAL
  zh: 上述命令的结果是一个二元组列表，例如：
- en: '[PRE8]'
  id: totrans-split-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Please note that the list of bigrams generally contains a lot of duplicates.
    Among them, some bigrams will show up much more frequently than the others.
  id: totrans-split-55
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，二元组列表通常包含大量重复项。其中，一些二元组将比其他二元组频繁出现得多。
- en: And the most common bigrams will be the most likely to be selected in the next
    steps in order to extract our next word.
  id: totrans-split-56
  prefs: []
  type: TYPE_NORMAL
  zh: 并且最常见的二元组将是在接下来的步骤中最有可能被选中的，以提取我们的下一个词。
- en: '*Step 2: Shuffle all such bigrams*'
  id: totrans-split-57
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '*步骤 2：随机洗牌所有这样的二元组*'
- en: '[PRE9]'
  id: totrans-split-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'We randomly rearrange the list of bigrams so as to avoid extracting always
    the same next word. Example of shuffled list:'
  id: totrans-split-59
  prefs: []
  type: TYPE_NORMAL
  zh: 我们随机重新排列大量二元组列表，以避免总是提取相同的下一个词。洗牌列表的示例：
- en: '[PRE10]'
  id: totrans-split-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '[PRE11]'
  id: totrans-split-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'From the shuffled list above, we pick the 1st bigram:'
  id: totrans-split-62
  prefs: []
  type: TYPE_NORMAL
  zh: 从上述洗牌列表中，我们选择第一个二元组：
- en: In our example, “from” is our next word in the generated sentence!
  id: totrans-split-63
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的例子中，“from”是生成句子中的下一个词！
- en: Note that we could have chosen any bigram from the shuffled list, without loss
    of generality.
  id: totrans-split-64
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，我们可以从洗牌列表中选择任何二元组，不失一般性。
- en: In fact, the most common bigrams will always be the more likely to be extracted,
    regardless of their position in the shuffled list.
  id: totrans-split-65
  prefs: []
  type: TYPE_NORMAL
  zh: 事实上，无论在洗牌列表中的位置如何，最常见的二元组都将始终最有可能被提取。
- en: '*Final Step: Get the 2nd word from the bigram*'
  id: totrans-split-66
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '*最终步骤：从二元组中获取第二个词*'
- en: '[PRE12]'
  id: totrans-split-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Clearly, the final command in the pipeline `cut -d '' '' -f2` extracts the
    2nd word in the bigram:'
  id: totrans-split-68
  prefs: []
  type: TYPE_NORMAL
  zh: 显然，管道中的最终命令 `cut -d ' ' -f2` 提取了二元组中的第二个词：
- en: The above generation steps repeat iteratively word-by-word until either the
    next word is a period character or no next-word is present.
  id: totrans-split-69
  prefs: []
  type: TYPE_NORMAL
  zh: 上述生成步骤迭代地逐字生成，直到下一个词是句点字符或没有下一个词为止。
- en: 'Let’s play a little bit more with the toolchain by using [trigrams](https://en.wikipedia.org/wiki/Word_n-gram_language_model#Trigram_model),
    namely *n*-grams with *n*=3:'
  id: totrans-split-70
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过使用[三元组](https://en.wikipedia.org/wiki/Word_n-gram_language_model#Trigram_model)，即*n*-grams（*n*=3），再玩一会儿：
- en: '[PRE13]'
  id: totrans-split-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: In general, `ngrams.sh` accepts as its argument any natural integer starting
    from 2.
  id: totrans-split-72
  prefs: []
  type: TYPE_NORMAL
  zh: 一般来说，`ngrams.sh`接受作为其参数任何从2开始的自然整数。
- en: Let’s generate a random sentence starting from the initial 2 words “a man”.
  id: totrans-split-73
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从初始的两个词“a man”开始生成一个随机句子。
- en: '[PRE14]'
  id: totrans-split-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: '*Output:*'
  id: totrans-split-75
  prefs: []
  type: TYPE_NORMAL
  zh: '*输出：*'
- en: a man gives himself out of this blubbering now we are about to begin from the
    hearts of whales is always under great and extraordinary difficulties that every
    one knows the fine carnation of their yet suspended boats.
  id: totrans-split-76
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 一个人从这个胖子里脱身，我们将从鲸鱼的心脏开始，总是在极大的和非凡的困难下，每个人都知道他们挂在他们仍然悬挂的小艇的精美康乃馨。
- en: 'Let’s now try the initial 2 words “by falling”:'
  id: totrans-split-77
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们尝试初始的两个词“by falling”：
- en: '[PRE15]'
  id: totrans-split-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: '*Output:*'
  id: totrans-split-79
  prefs: []
  type: TYPE_NORMAL
  zh: '*输出：*'
- en: by falling in the best harpooneers of nantucket both messmates of mine tis not
    me all loveliness is anguish to me we are to credit the old craft s cockpits especially
    of such a whale ship which like a hot sun s pilot yesterday i wrecked thee and
    wrong not captain ahab if it might light upon for all deficiencies of that is
    a new born sight.
  id: totrans-split-80
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 通过跌落在南塔基特最好的捕鲸者中，我的两位同伴不是我所有的美丽对我来说都是痛苦的，我们要感谢这艘鲸鱼船的老手工艺，特别是像昨天我毁了你和错误不是船长阿哈布如果它可能落在所有这是一个新生的视野。
- en: 'Let’s try with “one moment”:'
  id: totrans-split-81
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们试试“一刻钟”：
- en: '[PRE16]'
  id: totrans-split-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: '*Output:*'
  id: totrans-split-83
  prefs: []
  type: TYPE_NORMAL
  zh: '*输出：*'
- en: one moment to watch the same no the reason of numerous rude scratches altogether
    of an enormous practical resolution in facing death this old whale hunter of the
    word why don t you all the men swung in the deep yet is there any reason possessed
    the most ancient extant portrait anyways purporting to be tried out without being
    taken out of all voyages now or never for a time when they come from a drooping
    orchard twig.
  id: totrans-split-84
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '一瞬间观看同样的无数粗鲁划痕的原因完全是在面对死亡的一种巨大实用决心这位老鲸鱼猎人的字，为什么你们不都在深处摇摆？然而有没有理由拥有最古老的现存肖像无论如何都试图在不被所有航行中拿出来时进行测试现在还是从一个低垂的果园树枝上。  '
- en: What a fun!
  id: totrans-split-85
  prefs: []
  type: TYPE_NORMAL
  zh: 多么有趣!
- en: The Implementation
  id: totrans-split-86
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '实施  '
- en: '[***Get the toolchain usage and code!***](https://github.com/massimo-nazaria/bash-textgen
    "GitHub Repository")'
  id: totrans-split-87
  prefs: []
  type: TYPE_NORMAL
  zh: '[***获取工具链使用和代码！***](https://github.com/massimo-nazaria/bash-textgen "GitHub
    Repository")  '
- en: Please read also *[“Unix Philosophy with an Example”](/unix-philosophy.html)*
    to learn how to compose multiple commands together as seen in this tutorial.
  id: totrans-split-88
  prefs: []
  type: TYPE_NORMAL
  zh: '请阅读也*[“Unix Philosophy with an Example”](/unix-philosophy.html)*来学习如何在本教程中看到的多个命令一起组成。  '
