- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-05-27 13:05:06'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: mistral-community/Mixtral-8x22B-v0.1 · Hugging Face
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://huggingface.co/mistral-community/Mixtral-8x22B-v0.1](https://huggingface.co/mistral-community/Mixtral-8x22B-v0.1)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[](#mixtral-8x22b)Mixtral-8x22B'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: MistralAI has uploaded weights to their organization at [mistralai/Mixtral-8x22B-v0.1](https://huggingface.co/mistralai/Mixtral-8x22B-v0.1)
    and [mistralai/Mixtral-8x22B-Instruct-v0.1](https://huggingface.co/mistralai/Mixtral-8x22B-Instruct-v0.1)
    too.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Kudos to [@v2ray](https://huggingface.co/v2ray) for converting the checkpoints
    and uploading them in `transformers` compatible format. Go give them a follow!
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Converted to HuggingFace Transformers format using the script [here](https://huggingface.co/v2ray/Mixtral-8x22B-v0.1/blob/main/convert.py).
  prefs: []
  type: TYPE_NORMAL
- en: The Mixtral-8x22B Large Language Model (LLM) is a pretrained generative Sparse
    Mixture of Experts.
  prefs: []
  type: TYPE_NORMAL
- en: '[](#run-the-model)Run the model'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'By default, transformers will load the model in full precision. Therefore you
    might be interested to further reduce down the memory requirements to run the
    model through the optimizations we offer in HF ecosystem:'
  prefs: []
  type: TYPE_NORMAL
- en: '[](#in-half-precision)In half-precision'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Note `float16` precision only works on GPU devices
  prefs: []
  type: TYPE_NORMAL
- en: <details><summary>Click to expand</summary>
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]</details>'
  prefs: []
  type: TYPE_NORMAL
- en: '[](#lower-precision-using-8-bit--4-bit-using-bitsandbytes)Lower precision using
    (8-bit & 4-bit) using `bitsandbytes`'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: <details><summary>Click to expand</summary>
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]</details>'
  prefs: []
  type: TYPE_NORMAL
- en: '[](#load-the-model-with-flash-attention-2)Load the model with Flash Attention
    2'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: <details><summary>Click to expand</summary>
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]</details>'
  prefs: []
  type: TYPE_NORMAL
- en: '[](#notice)Notice'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Mixtral-8x22B-v0.1 is a pretrained base model and therefore does not have any
    moderation mechanisms.
  prefs: []
  type: TYPE_NORMAL
- en: '[](#the-mistral-ai-team)The Mistral AI Team'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Albert Jiang, Alexandre Sablayrolles, Alexis Tacnet, Antoine Roux, Arthur Mensch,
    Audrey Herblin-Stoop, Baptiste Bout, Baudouin de Monicault,Blanche Savary, Bam4d,
    Caroline Feldman, Devendra Singh Chaplot, Diego de las Casas, Eleonore Arcelin,
    Emma Bou Hanna, Etienne Metzger, Gianna Lengyel, Guillaume Bour, Guillaume Lample,
    Harizo Rajaona, Jean-Malo Delignon, Jia Li, Justus Murke, Louis Martin, Louis
    Ternon, Lucile Saulnier, Lélio Renard Lavaud, Margaret Jennings, Marie Pellat,
    Marie Torelli, Marie-Anne Lachaux, Nicolas Schuhl, Patrick von Platen, Pierre
    Stock, Sandeep Subramanian, Sophia Yang, Szymon Antoniak, Teven Le Scao, Thibaut
    Lavril, Timothée Lacroix, Théophile Gervet, Thomas Wang, Valera Nemychnikova,
    William El Sayed, William Marshall.
  prefs: []
  type: TYPE_NORMAL
- en: '[](#open-llm-leaderboard-evaluation-results)Open LLM Leaderboard Evaluation
    Results'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Detailed results can be found [here](https://huggingface.co/datasets/open-llm-leaderboard/details_mistral-community__Mixtral-8x22B-v0.1)
  prefs: []
  type: TYPE_NORMAL
- en: '| Metric | Value |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --: |'
  prefs: []
  type: TYPE_TB
- en: '| Avg. | 74.46 |'
  prefs: []
  type: TYPE_TB
- en: '| AI2 Reasoning Challenge (25-Shot) | 70.48 |'
  prefs: []
  type: TYPE_TB
- en: '| HellaSwag (10-Shot) | 88.73 |'
  prefs: []
  type: TYPE_TB
- en: '| MMLU (5-Shot) | 77.81 |'
  prefs: []
  type: TYPE_TB
- en: '| TruthfulQA (0-shot) | 51.08 |'
  prefs: []
  type: TYPE_TB
- en: '| Winogrande (5-shot) | 84.53 |'
  prefs: []
  type: TYPE_TB
- en: '| GSM8k (5-shot) | 74.15 |'
  prefs: []
  type: TYPE_TB
