- en: <!--yml
  id: totrans-split-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-split-1
  prefs: []
  type: TYPE_NORMAL
  zh: 'category: 未分类'
- en: 'date: 2024-05-27 13:05:06'
  id: totrans-split-2
  prefs: []
  type: TYPE_NORMAL
  zh: 'date: 2024-05-27 13:05:06'
- en: -->
  id: totrans-split-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: mistral-community/Mixtral-8x22B-v0.1 · Hugging Face
  id: totrans-split-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: mistral-community/Mixtral-8x22B-v0.1 · Hugging Face
- en: 来源：[https://huggingface.co/mistral-community/Mixtral-8x22B-v0.1](https://huggingface.co/mistral-community/Mixtral-8x22B-v0.1)
  id: totrans-split-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://huggingface.co/mistral-community/Mixtral-8x22B-v0.1](https://huggingface.co/mistral-community/Mixtral-8x22B-v0.1)
- en: '[](#mixtral-8x22b)Mixtral-8x22B'
  id: totrans-split-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '[](#mixtral-8x22b)Mixtral-8x22B'
- en: MistralAI has uploaded weights to their organization at [mistralai/Mixtral-8x22B-v0.1](https://huggingface.co/mistralai/Mixtral-8x22B-v0.1)
    and [mistralai/Mixtral-8x22B-Instruct-v0.1](https://huggingface.co/mistralai/Mixtral-8x22B-Instruct-v0.1)
    too.
  id: totrans-split-7
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: MistralAI 已经将权重上传到他们的组织中，分别是 [mistralai/Mixtral-8x22B-v0.1](https://huggingface.co/mistralai/Mixtral-8x22B-v0.1)
    和 [mistralai/Mixtral-8x22B-Instruct-v0.1](https://huggingface.co/mistralai/Mixtral-8x22B-Instruct-v0.1)
    。
- en: Kudos to [@v2ray](https://huggingface.co/v2ray) for converting the checkpoints
    and uploading them in `transformers` compatible format. Go give them a follow!
  id: totrans-split-8
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 感谢 [@v2ray](https://huggingface.co/v2ray) 将检查点转换并上传为 `transformers` 兼容格式。请关注他们！
- en: Converted to HuggingFace Transformers format using the script [here](https://huggingface.co/v2ray/Mixtral-8x22B-v0.1/blob/main/convert.py).
  id: totrans-split-9
  prefs: []
  type: TYPE_NORMAL
  zh: 使用脚本 [这里](https://huggingface.co/v2ray/Mixtral-8x22B-v0.1/blob/main/convert.py)
    将其转换为 HuggingFace Transformers 格式。
- en: The Mixtral-8x22B Large Language Model (LLM) is a pretrained generative Sparse
    Mixture of Experts.
  id: totrans-split-10
  prefs: []
  type: TYPE_NORMAL
  zh: Mixtral-8x22B 大语言模型 (LLM) 是一个预训练的生成稀疏专家混合体。
- en: '[](#run-the-model)Run the model'
  id: totrans-split-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '[](#run-the-model)运行模型'
- en: '[PRE0]'
  id: totrans-split-12
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'By default, transformers will load the model in full precision. Therefore you
    might be interested to further reduce down the memory requirements to run the
    model through the optimizations we offer in HF ecosystem:'
  id: totrans-split-13
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，transformers 将以完整精度加载模型。因此，您可能有兴趣通过我们在 HF 生态系统中提供的优化进一步减少内存需求来运行模型：
- en: '[](#in-half-precision)In half-precision'
  id: totrans-split-14
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '[](#in-half-precision)使用半精度'
- en: Note `float16` precision only works on GPU devices
  id: totrans-split-15
  prefs: []
  type: TYPE_NORMAL
  zh: 仅支持 `float16` 精度在 GPU 设备上运行
- en: <details><summary>Click to expand</summary>
  id: totrans-split-16
  prefs: []
  type: TYPE_NORMAL
  zh: <details><summary>展开查看</summary>
- en: '[PRE1]</details>'
  id: totrans-split-17
  prefs: []
  type: TYPE_NORMAL
  zh: '[PRE1]</details>'
- en: '[](#lower-precision-using-8-bit--4-bit-using-bitsandbytes)Lower precision using
    (8-bit & 4-bit) using `bitsandbytes`'
  id: totrans-split-18
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '[](#lower-precision-using-8-bit--4-bit-using-bitsandbytes)使用 `bitsandbytes`
    进行 8 位和 4 位的低精度'
- en: <details><summary>Click to expand</summary>
  id: totrans-split-19
  prefs: []
  type: TYPE_NORMAL
  zh: <details><summary>展开查看</summary>
- en: '[PRE2]</details>'
  id: totrans-split-20
  prefs: []
  type: TYPE_NORMAL
  zh: '[PRE2]</details>'
- en: '[](#load-the-model-with-flash-attention-2)Load the model with Flash Attention
    2'
  id: totrans-split-21
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '[](#load-the-model-with-flash-attention-2)加载具有 Flash Attention 2 的模型'
- en: <details><summary>Click to expand</summary>
  id: totrans-split-22
  prefs: []
  type: TYPE_NORMAL
  zh: <details><summary>展开查看</summary>
- en: '[PRE3]</details>'
  id: totrans-split-23
  prefs: []
  type: TYPE_NORMAL
  zh: '[PRE3]</details>'
- en: '[](#notice)Notice'
  id: totrans-split-24
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '[](#notice)注意'
- en: Mixtral-8x22B-v0.1 is a pretrained base model and therefore does not have any
    moderation mechanisms.
  id: totrans-split-25
  prefs: []
  type: TYPE_NORMAL
  zh: Mixtral-8x22B-v0.1 是一个预训练的基础模型，因此不具备任何调节机制。
- en: '[](#the-mistral-ai-team)The Mistral AI Team'
  id: totrans-split-26
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '[](#the-mistral-ai-team)Mistral AI 团队'
- en: Albert Jiang, Alexandre Sablayrolles, Alexis Tacnet, Antoine Roux, Arthur Mensch,
    Audrey Herblin-Stoop, Baptiste Bout, Baudouin de Monicault,Blanche Savary, Bam4d,
    Caroline Feldman, Devendra Singh Chaplot, Diego de las Casas, Eleonore Arcelin,
    Emma Bou Hanna, Etienne Metzger, Gianna Lengyel, Guillaume Bour, Guillaume Lample,
    Harizo Rajaona, Jean-Malo Delignon, Jia Li, Justus Murke, Louis Martin, Louis
    Ternon, Lucile Saulnier, Lélio Renard Lavaud, Margaret Jennings, Marie Pellat,
    Marie Torelli, Marie-Anne Lachaux, Nicolas Schuhl, Patrick von Platen, Pierre
    Stock, Sandeep Subramanian, Sophia Yang, Szymon Antoniak, Teven Le Scao, Thibaut
    Lavril, Timothée Lacroix, Théophile Gervet, Thomas Wang, Valera Nemychnikova,
    William El Sayed, William Marshall.
  id: totrans-split-27
  prefs: []
  type: TYPE_NORMAL
  zh: Albert Jiang, Alexandre Sablayrolles, Alexis Tacnet, Antoine Roux, Arthur Mensch,
    Audrey Herblin-Stoop, Baptiste Bout, Baudouin de Monicault,Blanche Savary, Bam4d,
    Caroline Feldman, Devendra Singh Chaplot, Diego de las Casas, Eleonore Arcelin,
    Emma Bou Hanna, Etienne Metzger, Gianna Lengyel, Guillaume Bour, Guillaume Lample,
    Harizo Rajaona, Jean-Malo Delignon, Jia Li, Justus Murke, Louis Martin, Louis
    Ternon, Lucile Saulnier, Lélio Renard Lavaud, Margaret Jennings, Marie Pellat,
    Marie Torelli, Marie-Anne Lachaux, Nicolas Schuhl, Patrick von Platen, Pierre
    Stock, Sandeep Subramanian, Sophia Yang, Szymon Antoniak, Teven Le Scao, Thibaut
    Lavril, Timothée Lacroix, Théophile Gervet, Thomas Wang, Valera Nemychnikova,
    William El Sayed, William Marshall.
- en: '[](#open-llm-leaderboard-evaluation-results)Open LLM Leaderboard Evaluation
    Results'
  id: totrans-split-28
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '[](#open-llm-leaderboard-evaluation-results)Open LLM 排行榜评估结果'
- en: Detailed results can be found [here](https://huggingface.co/datasets/open-llm-leaderboard/details_mistral-community__Mixtral-8x22B-v0.1)
  id: totrans-split-29
  prefs: []
  type: TYPE_NORMAL
  zh: 详细结果可以在 [这里](https://huggingface.co/datasets/open-llm-leaderboard/details_mistral-community__Mixtral-8x22B-v0.1)
    找到
- en: '| Metric | Value |'
  id: totrans-split-30
  prefs: []
  type: TYPE_TB
  zh: '| 指标 | 值 |'
- en: '| --- | --: |'
  id: totrans-split-31
  prefs: []
  type: TYPE_TB
  zh: '| --- | --: |'
- en: '| Avg. | 74.46 |'
  id: totrans-split-32
  prefs: []
  type: TYPE_TB
  zh: '| 平均值 | 74.46 |'
- en: '| AI2 Reasoning Challenge (25-Shot) | 70.48 |'
  id: totrans-split-33
  prefs: []
  type: TYPE_TB
  zh: '| AI2 推理挑战 (25-Shot) | 70.48 |'
- en: '| HellaSwag (10-Shot) | 88.73 |'
  id: totrans-split-34
  prefs: []
  type: TYPE_TB
  zh: '| HellaSwag (10-Shot) | 88.73 |'
- en: '| MMLU (5-Shot) | 77.81 |'
  id: totrans-split-35
  prefs: []
  type: TYPE_TB
  zh: '| MMLU (5-Shot) | 77.81 |'
- en: '| TruthfulQA (0-shot) | 51.08 |'
  id: totrans-split-36
  prefs: []
  type: TYPE_TB
  zh: '| TruthfulQA (0-shot) | 51.08 |'
- en: '| Winogrande (5-shot) | 84.53 |'
  id: totrans-split-37
  prefs: []
  type: TYPE_TB
  zh: '| Winogrande (5-shot) | 84.53 |'
- en: '| GSM8k (5-shot) | 74.15 |'
  id: totrans-split-38
  prefs: []
  type: TYPE_TB
  zh: '| GSM8k (5-shot) | 74.15 |'
