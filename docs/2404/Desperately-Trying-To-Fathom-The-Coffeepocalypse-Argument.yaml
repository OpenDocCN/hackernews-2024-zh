- en: <!--yml
  id: totrans-split-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-split-1
  prefs: []
  type: TYPE_NORMAL
  zh: 'category: 未分类'
- en: 'date: 2024-05-27 13:32:06'
  id: totrans-split-2
  prefs: []
  type: TYPE_NORMAL
  zh: 'date: 2024-05-27 13:32:06'
- en: -->
  id: totrans-split-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: Desperately Trying To Fathom The Coffeepocalypse Argument
  id: totrans-split-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 拼命试图理解咖啡末日论证
- en: 来源：[https://www.astralcodexten.com/p/desperately-trying-to-fathom-the](https://www.astralcodexten.com/p/desperately-trying-to-fathom-the)
  id: totrans-split-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://www.astralcodexten.com/p/desperately-trying-to-fathom-the](https://www.astralcodexten.com/p/desperately-trying-to-fathom-the)
- en: 'One of the most common arguments against AI safety is:'
  id: totrans-split-6
  prefs: []
  type: TYPE_NORMAL
  zh: 对人工智能安全性的最常见的论点之一是：
- en: Here’s an example of a time someone was worried about something, but it didn’t
    happen. Therefore, AI, which you are worried about, also won’t happen.
  id: totrans-split-7
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 这里有一个例子，有人担心某事情，但没发生。因此，你担心的人工智能也不会发生。
- en: 'I always give the obvious answer: “Okay, but there are other examples of times
    someone was worried about something, and it *did* happen, right? How do we know
    AI isn’t more like those?” The people I’m arguing with always seem so surprised
    by this response, as if I’m committing some sort of betrayal by destroying their
    beautiful argument.'
  id: totrans-split-8
  prefs: []
  type: TYPE_NORMAL
  zh: 我总是给出显而易见的答案：“好吧，但还有其他例子，有时候有人担心某事情，*结果*发生了，对吧？我们怎么知道人工智能不更像那些情况呢？” 我与之争论的人总是对这个回答感到非常惊讶，仿佛我在摧毁他们美好的论点时犯了某种背叛行为。
- en: The first hundred times this happened, I thought I must be misunderstanding
    something. Surely “I can think of one thing that didn’t happen, therefore nothing
    happens” is such a dramatic logical fallacy that no human is dumb enough to fall
    for it. But people keep bringing it up, again and again. Very smart people, people
    who I otherwise respect, make this argument and genuinely expect it to convince
    people!
  id: totrans-split-9
  prefs: []
  type: TYPE_NORMAL
  zh: 前一百次这种情况发生时，我以为我一定是误解了什么。毕竟，“我能想到一件事情没发生，所以什么也不会发生”是如此明显的逻辑谬误，以至于没有人会愚蠢到会中招。但人们一遍又一遍地提出这个论点。非常聪明的人，我其他方面尊重的人，他们提出这个论点，并真诚地期望它能说服人们！
- en: 'Usually the thing that didn’t happen is overpopulation, global cooling, etc.
    But most recently it was some kind of coffeepocalypse:'
  id: totrans-split-10
  prefs: []
  type: TYPE_NORMAL
  zh: 通常那些没有发生的事情是人口过剩、全球变冷等等。但最近大多数是某种咖啡末日的情况：
- en: You can [read the full thread here](https://twitter.com/Dan_Jeffries1/status/1741445839053025450),
    but I’m warning you, it’s just going to be “once people were worried about coffee,
    but now we know coffee is safe. Therefore AI will also be safe.”
  id: totrans-split-11
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以[在这里阅读完整的讨论](https://twitter.com/Dan_Jeffries1/status/1741445839053025450)，但我警告你，内容基本上是“有一次人们担心咖啡，但现在我们知道咖啡是安全的。因此人工智能也会安全无虞。”
- en: 'I keep trying to steelman this argument, and it keeps resisting my steelmanning.
    For example:'
  id: totrans-split-12
  prefs: []
  type: TYPE_NORMAL
  zh: 我一直试图加强这个论点，但它总是抵抗我的加强。例如：
- en: Maybe the argument is a failed attempt to gesture at a principle of “most technologies
    don’t go wrong”? But people make the same argument with things that aren’t technologies,
    like global cooling or overpopulation.
  id: totrans-split-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 或许这个论点是对“大多数技术不会出错”的一个失败尝试？但人们用不是技术的东西做同样的论据，比如全球变冷或人口过剩。
- en: Maybe the argument is a failed attempt to gesture at a principle of “the world
    is never destroyed, so doomsday prophecies have an abysmal track record”? But
    overpopulation and global cooling don’t claim that everyone will die - just that
    a lot of people will. And plenty of prophecies about mass death events have come
    true (eg Black Plague, WWII, AIDS). And none of this explains coffee!
  id: totrans-split-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 或许这个论点是对“世界从未被毁灭过，所以末日预言的记录很糟糕”原则的失败尝试？但人口过剩和全球变冷并没有声称每个人都会死 - 只是会有很多人。并且关于大规模死亡事件的许多预言已经成真（例如黑死病、第二次世界大战、艾滋病）。但这一切都无法解释咖啡啊！
- en: 'So my literal, non-rhetorical question, is “how can anyone be stupid enough
    to think this makes sense?” I’m not (just) trying to insult the people who say
    this; I consider their existence a genuine philosophical mystery. Isn’t this,
    in some sense, no different from saying (for example):'
  id: totrans-split-15
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我字面上的、非修辞的问题是：“有谁会愚蠢到认为这有道理呢？” 我并不（只）是想侮辱说这些话的人；我认为他们的存在是一个真正的哲学之谜。在某种意义上，这难道不和说（例如）没有不同吗：
- en: I once heard about a dumb person who thought halibut weren’t a kind of fish
    - but boy, that person sure was wrong. Therefore, AI is also a kind of fish.
  id: totrans-split-16
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 我曾经听说过一个愚蠢的人认为大比目鱼不是一种鱼 - 但是天哪，那个人肯定是错的。因此，人工智能也是一种鱼。
- en: 'The coffee version is:'
  id: totrans-split-17
  prefs: []
  type: TYPE_NORMAL
  zh: 咖啡版本是：
- en: I once heard about a dumb person who thought coffee would cause lots of problems
    - but boy, that person sure was wrong. Therefore, AI also won’t cause lots of
    problems.
  id: totrans-split-18
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 我曾经听说过一个愚蠢的人认为咖啡会引发很多问题 - 但是天哪，那个人肯定是错的。因此，人工智能也不会引发很多问题。
- en: Nobody would ever take it seriously in its halibut form. So what part of reskinning
    it as about coffee makes it more credible?
  id: totrans-split-19
  prefs: []
  type: TYPE_NORMAL
- en: Whenever I wonder how anyone can be so stupid, I start by asking if I myself
    am exactly this stupid in some other situation. This time, I remembered an argument
    from one of Stuart Russell’s pro-AI-risk arguments. [He pointed out](https://www.edge.org/response-detail/26157)
    that physicist Ernest Rutherford declared nuclear chain reactions impossible *less
    than twenty-four hours* before Szilard discovered the secret of the nuclear chain
    reaction. At the time, I thought this was a cute and helpful warning against being
    too sure that superintelligence was impossible.
  id: totrans-split-20
  prefs: []
  type: TYPE_NORMAL
- en: 'But isn’t this the same argument as the coffeepocalypse? A hostile rephrasing
    might be:'
  id: totrans-split-21
  prefs: []
  type: TYPE_NORMAL
- en: There is at least one thing that was possible. Therefore, superintelligent AI
    is also possible.
  id: totrans-split-22
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'And an only slightly less hostile rephrasing:'
  id: totrans-split-23
  prefs: []
  type: TYPE_NORMAL
- en: People were wrong when they said nuclear reactions were impossible. Therefore,
    they might also be wrong when they say superintelligent AI is possible.
  id: totrans-split-24
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: How is this better than the coffeepocalypse argument? In fact, how is it even
    better than the halibut argument? What are we doing when we make arguments like
    these?
  id: totrans-split-25
  prefs: []
  type: TYPE_NORMAL
- en: 'Some thoughts:'
  id: totrans-split-26
  prefs: []
  type: TYPE_NORMAL
- en: '**As An Existence Proof?**'
  id: totrans-split-27
  prefs: []
  type: TYPE_NORMAL
- en: When I think of why I appreciated Prof. Russell’s argument, it wasn’t because
    it was a complete proof that superintelligence was possible. It was more like
    an argument for humility. “You may think it’s impossible. But given that there’s
    at least one case where people thought that and were proven wrong, you should
    believe it’s at least possible.”
  id: totrans-split-28
  prefs: []
  type: TYPE_NORMAL
- en: But first of all, one case shouldn’t prove anything. If you doubt you will win
    the lottery, I can’t prove you wrong - even in a weak, probabilistic way - by
    bringing up a case of someone who did. I can’t even prove you should be humble
    - you are definitely allowed to be arrogant and very confident in your belief
    you won’t win!
  id: totrans-split-29
  prefs: []
  type: TYPE_NORMAL
- en: And second of all, existence proofs can only make you *slightly* more humble.
    They can refute the claim “I am absolutely, 100% certain that AI is/isn’t dangerous”.
    But not many people make this claim, and it’s [uncharitable](https://slatestarcodex.com/2013/06/13/arguments-from-my-opponent-believes-something/)
    to suspect your opponent of doing so.
  id: totrans-split-30
  prefs: []
  type: TYPE_NORMAL
- en: Maybe this debate collapses into the debate around the [Safe Uncertainty Fallacy](https://www.astralcodexten.com/p/mr-tries-the-safe-uncertainty-fallacy),
    where some people think if there’s any uncertainty at all about something, you
    have to assume it will be totally safe and fine (no, I don’t get it either), and
    other people think if there’s even a 1% chance of disaster, you have to multiply
    out by the size of the disaster and end up very concerned (at the tails, this
    becomes Pascalian reasoning, but nobody has a good theory of where the tails begin).
  id: totrans-split-31
  prefs: []
  type: TYPE_NORMAL
- en: I still don’t think an existence proof that it’s theoretically possible for
    your opponent to be wrong goes very far. Still, this is sort of what I was trying
    to do [with the diphyllic dam example here](https://www.astralcodexten.com/p/ye-olde-bay-area-house-party)
    - show that a line of argument can sometimes be wrong, in a way that forces people
    to try something more sophisticated.
  id: totrans-split-32
  prefs: []
  type: TYPE_NORMAL
- en: '**As An Attempt To Trigger A Heuristic?**'
  id: totrans-split-33
  prefs: []
  type: TYPE_NORMAL
- en: Maybe Prof. Russell’s argument implicitly assumes that everyone has a large
    store of knowledge about failed predictions - no heavier-than-air flying machine
    is possible, there is a world market for maybe five computers. You could think
    of this particular example of a prediction being false as trying to trigger people’s
    existing stock of memories that *very often* people’s predictions are false.
  id: totrans-split-34
  prefs: []
  type: TYPE_NORMAL
- en: You could make the same argument about the coffeepocalypse. “People worried
    about coffee but it was fine” is intended to activate a long list of stored moral
    panics in your mind - the one around marijuana, the one around violent video games
    - enough to remind you that *very often* people worry about something and it’s
    nothing.
  id: totrans-split-35
  prefs: []
  type: TYPE_NORMAL
- en: But - even granting that there are many cases of both - are these useful? There
    are many cases of moral panics turning out to be nothing. But there are many other
    cases of moral panics proving true, or of people not worrying about things they
    should worry about. People didn’t worry enough about tobacco, and then it killed
    lots of people. People didn’t worry enough about lead in gasoline, and then it
    poisoned lots of children. People didn’t worry enough about global warming, OxyContin,
    al-Qaeda, growing international tension in the pre-WWI European system, etc, until
    after those things had already gotten out of control and hurt lots of people.
    We even have words and idioms for this kind of failure to listen to warnings -
    like the ostrich burying its head in the sand.
  id: totrans-split-36
  prefs: []
  type: TYPE_NORMAL
- en: (and there are many examples of people predicting that things were impossible,
    and they really were impossible, eg perpetual motion).
  id: totrans-split-37
  prefs: []
  type: TYPE_NORMAL
- en: It would seem like in order to usefully invoke a heuristic (“remember all these
    cases of moral panic we all agree were bad? Then you should assume this is probably
    also a moral panic”), you need to establish that moral panics are more common
    than ostrich-head-burying. And in order to usefully invoke a heuristic against
    predicting something is impossible, you need to establish that failed impossibility
    proofs are more common than accurate ones.
  id: totrans-split-38
  prefs: []
  type: TYPE_NORMAL
- en: This seems somewhere between “nobody has done it” and “impossible in principle”.
    Insisting on it would eliminate 90%+ of discourse.
  id: totrans-split-39
  prefs: []
  type: TYPE_NORMAL
- en: See also [Caution On Bias Arguments](https://slatestarcodex.com/2019/07/17/caution-on-bias-arguments/),
    where I try to make the same point. I think you can rewrite this section to be
    about proposed bias arguments (“People have a known bias to worry about things
    excessively, so we should correct for it”). But as always, you can posit an opposite
    bias (“People have a known bias to put their heads in the sand and ignore problems
    that it would be scary to think about or expensive to fix”), and figuring out
    which of these dueling biases you need to correct for, is the same problem as
    figuring out which of the dueling heuristics you need to invoke.
  id: totrans-split-40
  prefs: []
  type: TYPE_NORMAL
  zh: 参见也[关于偏见论证的警告](https://slatestarcodex.com/2019/07/17/caution-on-bias-arguments/)，在那里我试图表达同样的观点。我认为你可以重新写这一节，让它关于提出的偏见论点（“人们对过度担心的事情有已知的偏见，所以我们应该进行修正”）。但像往常一样，你可以假设一个相反的偏见（“人们有一个已知的偏见，他们把头埋在沙子里，忽视那些令人害怕或昂贵修复的问题”），弄清楚你需要纠正哪一个对立的偏见，这与决定你需要调用哪一个对立的启发式是相同的问题。
- en: '**What Is Evidence, Anyway?**'
  id: totrans-split-41
  prefs: []
  type: TYPE_NORMAL
  zh: '**什么是证据？**'
- en: Suppose someone’s trying to argue for some specific point, like “Russia will
    win the war with Ukraine”. They bring up some evidence, like “Russia has some
    very good tanks.”
  id: totrans-split-42
  prefs: []
  type: TYPE_NORMAL
  zh: 假设有人试图为某个特定观点辩护，比如“俄罗斯将赢得与乌克兰的战争”。他们提出了一些证据，比如“俄罗斯有一些非常好的坦克。”
- en: Obviously this on its own proves nothing. Russia could have good tanks, but
    Ukraine could be better at other things.
  id: totrans-split-43
  prefs: []
  type: TYPE_NORMAL
  zh: 很明显，仅凭这一点并不能证明什么。俄罗斯可能有好的坦克，但乌克兰在其他方面可能更出色。
- en: 'But then how does *any* amount of evidence prove an argument? You could make
    a hundred similar statements: “Russia has good tanks”, “Russia has good troop
    transport ships”, “the Russian general in the 4th District of the Western Theater
    is very skilled” […], and run into exactly the same problem. But an argument that
    Russia will win the war has to be made up of some number of pieces of evidence.
    So how can it ever work?'
  id: totrans-split-44
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，*任何*量的证据又如何证明一个论点呢？你可以做一百个类似的声明：“俄罗斯有好坦克”，“俄罗斯有好的运兵船”，“西部战区第四区的俄罗斯将军非常有能力”[...]，并且遇到完全相同的问题。但是一个论证俄罗斯会赢得战争的论点必须由若干证据片段组成。那么它如何能够奏效呢？
- en: I think it has to carry an implicit assumption of “…and you’re pretty good at
    weighing how much evidence it would take to prove something, and everything else
    is pretty equal, so this is enough evidence to push you over the edge into believing
    my point.”
  id: totrans-split-45
  prefs: []
  type: TYPE_NORMAL
  zh: 我认为它必须携带一个隐含的假设“…而且你很擅长权衡证据足以证明某事的量，其他的事情也相当，所以这足以让你相信我的观点。”
- en: For example, if someone said “Russia will win because they outnumber Ukraine
    3 to 1 and have better generals” (and then proved this was true), that at least
    seems like a plausible argument that shouldn’t be immediately ignored. Everyone
    knows that having a 3:1 advantage, and having good generals, are both big advantages
    in war. It carries an implied “and surely Ukraine doesn’t have some other advantage
    that counterbalances both of those”. But this could be so plausible that we accept
    it (it’s hard to counterbalance a 3:1 manpower advantage). Or it could be a challenge
    to pro-Ukraine people (if you can’t name some advantage of your side that sounds
    as convincing as these, then we win).
  id: totrans-split-46
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，如果有人说“俄罗斯会赢，因为他们的兵力是乌克兰的三倍，并且有更好的将军”（然后证明这是真的），这至少似乎是一个不应立即被忽视的合理论据。大家都知道兵力优势是一个很大的优势，有优秀的将军也是如此。这带有一个暗示，“当然，乌克兰没有其他优势来抵消这两个”。但这可能是如此合理，以至于我们接受它（很难抵消三倍的人力优势）。或者它可能对亲乌克兰的人构成挑战（如果你不能说出一些你一方的优势，听起来像这些一样令人信服，那么我们就赢了）。
- en: And it’s legitimate for someone who believes Russia will win, and has talked
    about it at length, to write one article about the good tanks, without explicitly
    saying “Obviously this is only one part of my case that Russia will win, and won’t
    convince anyone on its own; still, please update a little on this one, and maybe
    as you keep going and run into other things, you’ll update more.”
  id: totrans-split-47
  prefs: []
  type: TYPE_NORMAL
  zh: 有人认为俄罗斯会赢，并且长时间讨论过这个问题，所以写一篇关于好坦克的文章是合理的，而不明言“显然这只是我认为俄罗斯会赢的一个部分，不能单独说服任何人；不过，请在这一点上更新一下，并且当你继续前进并遇到其他问题时，你可能会更新更多。”
- en: Is this what the people talking about coffee are doing?
  id: totrans-split-48
  prefs: []
  type: TYPE_NORMAL
  zh: 那些谈论咖啡的人是这么做的吗？
- en: 'An argument against: you should at least update a *little* on the good tanks,
    right? But the coffee thing proves *literally* nothing. It proves that there was
    *one time* when people worried about a bad thing, and then it didn’t happen. Surely
    you already knew this must have happened at least once!'
  id: totrans-split-49
  prefs: []
  type: TYPE_NORMAL
  zh: 一个反对的论点：你至少应该稍微更新一下对好坦克的看法，对吧？但是咖啡的事情实际上什么也没证明。这只证明了有一次人们担心了一个坏事情，然后它没有发生。你肯定已经知道至少会有一次这样的情况发生！
- en: 'An argument in favor: suppose there are a hundred different facets of war as
    important as “has good tanks”. It would be very implausible if, of two relatively
    evenly-matched competitors, one of them was better at all 100, and the other at
    0\. So all that “Russia has good tanks” is telling you is that Russia is better
    on at least one axis, which you could have already predicted. Is this more of
    an update than the coffee situation?'
  id: totrans-split-50
  prefs: []
  type: TYPE_NORMAL
  zh: 一个支持的论点：假设有一百种与“拥有好坦克”同等重要的战争要素。如果两个竞争对手相对均衡，其中一个在所有100个要素上都更优秀，另一个都为0，这将是不太可能的。因此，“俄罗斯拥有好坦克”告诉你的只是俄罗斯在至少一个方面更优秀，这是你本来就可以预测到的。这比咖啡情况更像是一个更新吗？
- en: 'My proposed answer: if you knew the person making the argument was deliberately
    looking for pro-Russia arguments, then “has good tanks” updates you almost zero
    - it would only convince you that Russia was better in at least 1 of 100 domains.
    If you thought they were relatively unbiased and just happened to stumble across
    this information, it would update you slightly (we have chosen a randomly selected
    facet, and Russia is better).'
  id: totrans-split-51
  prefs: []
  type: TYPE_NORMAL
  zh: 我的提议答案：如果你知道提出论点的人刻意寻找亲俄论据，那么“拥有好坦克”几乎不会给你带来任何更新 - 它只会让你相信俄罗斯在100个领域中至少在一个方面更优秀。如果你认为他们相对公正，只是碰巧发现了这些信息，那么它会稍微更新你的看法（我们已选择一个随机选中的要素，而俄罗斯更胜一筹）。
- en: If you thought the person making the coffee argument was doing an unbiased survey
    of all times people had been worried, then the coffee fact (in this particular
    time people worried, it was unnecessary) might feel like sampling a random point.
    But we have so much more evidence about whether things are dangerous or safe that
    I don’t think sampling a random point (even if we could do so fairly) would mean
    much.
  id: totrans-split-52
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你认为提出咖啡论点的人正在做一个关于所有人们何时担心过的不偏不倚的调查，那么咖啡的事实（在这个特定时间人们担心了，但其实是多余的）可能会感觉像是取样一个随机点。但是我们对于事物是危险还是安全有更多的证据，我不认为取样一个随机点（即使我们可以公正地这样做）会有多大意义。
- en: '**Conclusion: I Genuinely Don’t Know What These People Are Thinking**'
  id: totrans-split-53
  prefs: []
  type: TYPE_NORMAL
  zh: '**结论：我真的不知道这些人在想什么**'
- en: I would like to understand the mindset of people who make arguments like this,
    but I’m not sure I’ve succeeded. The best I can say is that sometimes people on
    my side make similar arguments (the nuclear chain reaction one) which I don’t
    immediately flag as dumb, and maybe I can follow this thread to figure out why
    they seem tempting sometimes.
  id: totrans-split-54
  prefs: []
  type: TYPE_NORMAL
  zh: 我希望能理解那些提出这种论点的人的思维方式，但我不确定我是否成功了。我能说的最好的是，有时候我这边的人也会提出类似的论点（核链反应那个），我并不会立即认为它们愚蠢，也许我可以沿着这个思路来弄清楚为什么有时候它们看起来很诱人。
- en: If you see me making an argument that you think is like coffeepocalypse, please
    let me know, so I can think about what factors led me to think it was a reasonable
    thing to do, and see if they also apply to the coffee case.
  id: totrans-split-55
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你看到我提出的论点，你觉得它像咖啡末日一样，请告诉我，这样我可以思考是什么因素导致我认为这是一个合理的做法，并看看它们是否也适用于咖啡情况。
- en: . . . although I have to admit, I’m a little nervous asking for this, though.
    Douglas Adams once said that if anyone ever understood the Universe, it would
    immediately disappear and be replaced by something even more incomprehensible.
    I worry that if I ever understand why anti-AI-safety people think the things they
    say count as good arguments, the same thing might happen.
  id: totrans-split-56
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然我不得不承认，我有点紧张地提出这个要求。道格拉斯·亚当斯曾经说过，如果有人理解了宇宙，它会立即消失，并被更加难以理解的东西替代。我担心如果我真的理解了为什么反对AI安全的人认为他们说的东西算是好论点，同样的事情可能会发生。
