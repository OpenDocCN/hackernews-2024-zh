- en: <!--yml
  id: totrans-split-0
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  id: totrans-split-1
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-05-27 13:13:00'
  id: totrans-split-2
  prefs: []
  type: TYPE_NORMAL
- en: -->
  id: totrans-split-3
  prefs: []
  type: TYPE_NORMAL
- en: WizardLM 2
  id: totrans-split-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://wizardlm.github.io/WizardLM2/](https://wizardlm.github.io/WizardLM2/)
  id: totrans-split-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: To present a comprehensive overview of the performance of WizardLM-2, we conduct
    both human and automatic evaluations between our models and diverse baselines.
    As indicated in the following main experimental results, WizardLM-2 demonstrates
    highly competitive performance compared to those leading proprietary works and
    consistently outperforms all the existing state-of-the-art opensource models.
    More associated details and thinking will be presented in our upcoming paper.
  id: totrans-split-6
  prefs: []
  type: TYPE_NORMAL
- en: '**Human Preferences Evaluation**'
  id: totrans-split-7
  prefs: []
  type: TYPE_NORMAL
- en: 'We carefully collected a complex and challenging set consisting of real-world
    instructions, which includes main requirements of humanity, such as writing, coding,
    math, reasoning, agent, and multilingual. We perform a blind pairwise comparison
    between WizardLM-2 and baselines. To each annotator, responses from all models
    are presented, which are randomly shuffled to hide their sources. We report the
    win:loss rate without tie:'
  id: totrans-split-8
  prefs: []
  type: TYPE_NORMAL
- en: WizardLM-2 8x22B is just slightly falling behind GPT-4-1106-preview, and significantly
    stronger than Command R Plus and GPT4-0314.
  id: totrans-split-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: WizardLM-2 70B is better than GPT4-0613, Mistral-Large, and Qwen1.5-72B-Chat.
  id: totrans-split-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: WizardLM-2 7B is comparable with Qwen1.5-32B-Chat, and surpasses Qwen1.5-14B-Chat
    and Starling-LM-7B-beta.
  id: totrans-split-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Through this human preferences evaluation, WizardLM-2's capabilities are very
    close to the cutting-edge proprietary models such as GPT-4-1106-preview, and significantly
    ahead of all the other open source models.
  id: totrans-split-12
  prefs: []
  type: TYPE_NORMAL
