- en: <!--yml
  id: totrans-split-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-split-1
  prefs: []
  type: TYPE_NORMAL
  zh: 'category: 未分类'
- en: 'date: 2024-05-27 13:16:22'
  id: totrans-split-2
  prefs: []
  type: TYPE_NORMAL
  zh: 'date: 2024-05-27 13:16:22'
- en: -->
  id: totrans-split-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: Embeddings are a good starting point for the AI curious app developer
  id: totrans-split-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 对于对AI感兴趣的应用程序开发者来说，嵌入是一个很好的起点
- en: 来源：[https://bawolf.substack.com/p/embeddings-are-a-good-starting-point](https://bawolf.substack.com/p/embeddings-are-a-good-starting-point)
  id: totrans-split-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://bawolf.substack.com/p/embeddings-are-a-good-starting-point](https://bawolf.substack.com/p/embeddings-are-a-good-starting-point)
- en: 'Vector embeddings have been an Overton window shifting experience for me, not
    because they’re sufficiently advanced technology indistinguishable from magic,
    but the opposite. Once I started using them, it felt obvious that this was what
    the search experience was always supposed to be: less “How did you do that?” and
    more mundanely, “Why isn’t this everywhere?”'
  id: totrans-split-6
  prefs: []
  type: TYPE_NORMAL
  zh: 向量嵌入对我来说是一个Overton窗口转移的体验，不是因为它们是足够先进的技术，无法与魔法区分开来，而是相反。一旦开始使用它们，就觉得显而易见，这才是搜索体验一直应该有的：不再是“你是怎么做到的？”而是更平凡的，“为什么这不普及到每个地方呢？”
- en: This feels like the right place to start if you’re an app developer looking
    for an excuse to dip your toes into this new AI world. Embeddings are just arrays
    of numbers, but they contain a compressed form of a considerable amount of human
    knowledge and shrink features that used to be substantial specialized projects
    into ones that individual product engineers can take on.
  id: totrans-split-7
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您是一个寻找借口涉足这个新AI世界的应用程序开发者，这感觉就是一个合适的起点。嵌入只是一组数字数组，但它们包含了大量人类知识的压缩形式，并将过去需要大量专门项目才能完成的功能缩减为个体产品工程师可以承担的项目。
- en: 'There are a ton of tooling options available to use embeddings. I’ll highlight
    our choices and note where you might want to make different ones for your situation.
    Here are some points I hope you take away:'
  id: totrans-split-8
  prefs: []
  type: TYPE_NORMAL
  zh: 使用嵌入的工具选项非常多。我将突出我们的选择，并指出您可能希望根据自己的情况做出不同选择的地方。以下是我希望您能带走的一些要点：
- en: Vector embeddings work for search and recommendations because they’re good at
    measuring similarity to arbitrary input. This even works for different spoken
    languages like French or Japanese.
  id: totrans-split-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 向量嵌入适用于搜索和推荐，因为它们擅长衡量与任意输入的相似性。这甚至适用于不同的口头语言，如法语或日语。
- en: '[Pgvector](https://github.com/pgvector/pgvector/) is a Postgres extension that
    stores and queries embeddings without adding a new service. It’s powerful because
    it can combine standard SQL logic with embedding operations.'
  id: totrans-split-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Pgvector](https://github.com/pgvector/pgvector/) 是一个在不添加新服务的情况下存储和查询嵌入的Postgres扩展。它之所以强大，是因为可以将标准SQL逻辑与嵌入操作结合起来。'
- en: Unlike LLMs, working with embeddings feels like regular deterministic code.
  id: totrans-split-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 与LLM不同，使用嵌入感觉像是常规的确定性代码。
- en: My friend Charlie Yuan and I built this mini [icon app](https://www.v0.app)
    to help people discover icons. It’s pretty short and sweet. We have icon sets
    you can query, bookmark, and add to your project.
  id: totrans-split-12
  prefs: []
  type: TYPE_NORMAL
  zh: 我和我的朋友Charlie Yuan建立了这个小型[图标应用](https://www.v0.app)，帮助人们发现图标。非常简短而甜美。我们有您可以查询、收藏并添加到项目中的图标集。
- en: There are a bunch of [specialized vector databases](https://lakefs.io/blog/12-vector-databases-2023/)
    to choose from. Instead, we chose Postgres with [pgvector](https://github.com/pgvector/pgvector/)
    to blend embedding search with business logic like filtering and scoring. While
    it’s not the fastest vector database, we didn’t want to have to compare results
    across multiple data sources. Pgvector probably already has [a library for your
    favorite database client](https://github.com/pgvector/pgvector). Our project was
    Typescript through and through, and we used [drizzle-orm](https://github.com/pgvector/pgvector-node?tab=readme-ov-file#drizzle-orm).
    The docs will be a more robust place for setup documentation, so I’m leaving out
    that part to focus on the features you can build.
  id: totrans-split-13
  prefs: []
  type: TYPE_NORMAL
  zh: 有许多[专业的向量数据库](https://lakefs.io/blog/12-vector-databases-2023/)可供选择。但我们选择了与业务逻辑（如过滤和评分）融合的Postgres和[Pgvector](https://github.com/pgvector/pgvector/)。虽然它不是最快的向量数据库，但我们不希望在多个数据源之间比较结果。Pgvector可能已经为您喜欢的数据库客户端[提供了一个库](https://github.com/pgvector/pgvector)。我们的项目完全使用Typescript，并且使用了[drizzle-orm](https://github.com/pgvector/pgvector-node?tab=readme-ov-file#drizzle-orm)。文档将是安装文档的更为全面的地方，所以我略过了这部分，专注于您可以构建的功能。
- en: Once set up with pgvector, we created a strategy for encoding our icon data
    into vector embeddings. Embeddings are points in many-dimensional space, up to
    thousands of dimensions. Unfortunately, the axes of that grid are not humanly
    grokable ideas like “size” or “brightness.” They’re a bit of a black box. Luckily,
    like any good abstraction, they’re a black box with a good API.
  id: totrans-split-14
  prefs: []
  type: TYPE_NORMAL
  zh: 使用pgvector设置后，我们创建了一种策略，将我们的图标数据编码为矢量嵌入。嵌入是多维空间中的点，高达数千个维度。不幸的是，该网格的轴并非像“大小”或“亮度”这样人类可理解的概念。它们有点像黑匣子。幸运的是，像任何良好的抽象一样，它们是带有良好API的黑匣子。
- en: 'The best practice seems to be finding the details that best represent what
    people want to search for and creating a function that outputs that as a string.
    Our icons can be a part of many use-case-based ‘categories’ and have many descriptive
    ‘tags’ associated with them. We encoded that information along with the icon name
    because that best represents what the icon is. Whereas the name of the icon set
    the icon belongs to, or its dimensions aren’t relevant. The strings we generated
    looked basically like this:'
  id: totrans-split-15
  prefs: []
  type: TYPE_NORMAL
  zh: 最佳实践似乎是找到最能代表人们想搜索的细节，并创建一个输出该细节的字符串的函数。我们的图标可以作为许多基于用例的‘类别’的一部分，并且有许多描述性的‘标签’与之相关联。我们将这些信息与图标名称一起编码，因为这最能代表图标的内容。而图标所属的图标集名称或其尺寸则不相关。我们生成的字符串基本上看起来像这样：
- en: '[PRE0]'
  id: totrans-split-16
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Next, we chose an embedding model. [OpenAI’s embedding models](https://platform.openai.com/docs/guides/embeddings)
    will probably work just fine. We’re using their `text-embedding-3-small`. If you
    want to dive in, check out the [leaderboard](https://huggingface.co/spaces/mteb/leaderboard)
    and pick the model that best meets your needs. Whether you use an embeddings API
    or self-host an open source option, the interface should be text in and embeddings
    out.
  id: totrans-split-17
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们选择了一个嵌入模型。[OpenAI的嵌入模型](https://platform.openai.com/docs/guides/embeddings)可能会很好地工作。我们正在使用他们的`text-embedding-3-small`。如果您想深入了解，请查看[排行榜](https://huggingface.co/spaces/mteb/leaderboard)，并选择最符合您需求的模型。无论您使用嵌入API还是自托管的开源选项，接口都应该是文本输入和嵌入输出。
- en: Many sites implement search, but most icon sites implement search by [text](https://icon-sets.iconify.design/?query=woof)  [matching](https://react-icons.github.io/react-icons/search/#q=woof)
    or full-text search. If you’re looking for a dog icon, they search over the icon
    metadata for icons that have ‘dog’ in them. If they want to get [craftier](https://fontawesome.com/search?q=woof&o=r),
    they come up with a bag of words related to ‘dog,’ like maybe ‘k9’, ‘puppy,’ and
    ‘woof’ to catch near misses. That’s pretty fragile. Someone has to choose tags
    for each icon; if they miss an important one, users won’t find what they’re looking
    for.
  id: totrans-split-18
  prefs: []
  type: TYPE_NORMAL
  zh: 许多网站都实施了搜索，但大多数图标网站是通过[文本](https://icon-sets.iconify.design/?query=woof) [匹配](https://react-icons.github.io/react-icons/search/#q=woof)
    或全文搜索来实现搜索。如果您正在寻找狗图标，它们会在图标元数据中搜索包含‘dog’的图标。如果它们想要更[巧妙](https://fontawesome.com/search?q=woof&o=r)，它们会想出与‘dog’相关的词袋，比如‘k9’、‘puppy’和‘woof’，以捕捉近似匹配。这相当脆弱。必须为每个图标选择标签；如果漏掉了一个重要的标签，用户就找不到他们想要的内容。
- en: Our app gets relevant results when searching for ‘[dog](https://www.v0.app/search?g=popularity&q=dog&qid=nmezx8efde3pvydym2f2sk9n).’
    We also get solid results for ‘[puppy](https://www.v0.app/search?g=popularity&q=puppy&qid=v27kjdebk8ex7n30qe4b5tfs)’
    without a bag of words by measuring the cosine similarity between the embeddings
    of your search query and each icon. There are multiple ways to measure how similar
    embeddings are to each other, but OpenAI’s embeddings are designed to work well
    with cosine distance. Cosine similarity is just the opposite of cosine distance.
    Order by cosine distance wherever you can to take advantage of [indexes](https://github.com/pgvector/pgvector?tab=readme-ov-file#why-isnt-a-query-using-an-index).
  id: totrans-split-19
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们搜索‘[狗](https://www.v0.app/search?g=popularity&q=dog&qid=nmezx8efde3pvydym2f2sk9n)’时，我们的应用程序会获得相关的结果。我们还可以通过测量您搜索查询的嵌入与每个图标的余弦相似性来获得‘[小狗](https://www.v0.app/search?g=popularity&q=puppy&qid=v27kjdebk8ex7n30qe4b5tfs)’的可靠结果，而不需要使用词袋。有多种方法来衡量嵌入之间的相似度，但OpenAI的嵌入设计得非常适合余弦距离。余弦相似性只是余弦距离的相反。无论何时可以通过余弦距离排序来利用[索引](https://github.com/pgvector/pgvector?tab=readme-ov-file#why-isnt-a-query-using-an-index)。
- en: '[PRE1]'
  id: totrans-split-20
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: You can even try dog breeds like ‘[hound](https://www.v0.app/search?q=hound),’
    ‘[poodle](https://www.v0.app/search?q=poodle),’ or my favorite ‘[samoyed](https://www.v0.app/search?q=samoyed).’
    It pretty much just works. But that’s not all; it also works for other languages.
    Try ‘[chien](https://www.v0.app/search?g=popularity&q=chien&qid=mcbp6rk9z3y8lb1emt1zblev)’
    and even ‘[犬](https://www.v0.app/search?g=popularity&q=%E7%8A%AC&qid=hd8whx733f3rljhadasgfzb9)’!
    With pgvector, we can get these results with simple SQL queries.
  id: totrans-split-21
  prefs: []
  type: TYPE_NORMAL
  zh: 你甚至可以尝试像“[猎犬](https://www.v0.app/search?q=hound)”，“[贵宾犬](https://www.v0.app/search?q=poodle)”或者我的最爱“[萨摩耶](https://www.v0.app/search?q=samoyed)”这样的狗品种。它几乎就像魔法一样运行。但这还不是全部；它也适用于其他语言。尝试“[chien](https://www.v0.app/search?g=popularity&q=chien&qid=mcbp6rk9z3y8lb1emt1zblev)”甚至“[犬](https://www.v0.app/search?g=popularity&q=%E7%8A%AC&qid=hd8whx733f3rljhadasgfzb9)”！使用pgvector，我们可以通过简单的SQL查询获得这些结果。
- en: '[PRE2]'
  id: totrans-split-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Since we’re ordering by distance, every search returns every result in the table
    in order of closeness. We cut off results by a fixed number to make this manageable,
    limiting the query to the top 50\. Using an arbitrary distance cut-off is tempting,
    like querying for results with a cosine similarity of less than 0.8\. Unfortunately,
    the absolute distance for one query to produce correct-feeling results might differ
    drastically from another. We limit by the number of results, not by a minimum
    value, whenever possible.
  id: totrans-split-23
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们按距离排序，每次搜索都会按接近程度返回表中的每个结果。我们通过固定数量截断结果，以使其可管理，将查询限制在前50个结果。像查询余弦相似度小于0.8的结果那样使用任意距离截断是很诱人的。不幸的是，一个查询要产生正确感觉的绝对距离可能与另一个查询大不相同。尽可能地通过结果数量而不是最小值来限制。
- en: If we only wanted similarity search, any vector database would be fine, but
    Postgres allowed us to layer more features on top. The initial search looks across
    *all icons* in *all icon sets*, but our user’s style might only match some of
    the icon sets. We can filter by values like in any other Postgres query.
  id: totrans-split-24
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们只想要相似性搜索，任何向量数据库都可以，但Postgres允许我们在其上叠加更多功能。初始搜索跨越所有图标集中的所有图标，但我们用户的风格可能只匹配某些图标集。我们可以像在任何其他Postgres查询中那样按值进行过滤。
- en: '[PRE3]'
  id: totrans-split-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: This is deterministic; everyone searching for a ‘dog’ will get the same results.
    However, the inputs are still unbounded, so embedding search doesn’t guarantee
    that the best results will be produced for every input. We can try different embedding
    models or ways of encoding icons into embeddings to improve the system.
  id: totrans-split-26
  prefs: []
  type: TYPE_NORMAL
  zh: 这是确定性的；搜索“dog”的每个人将获得相同的结果。然而，输入仍然是无界的，因此嵌入式搜索并不保证为每个输入产生最佳结果。我们可以尝试不同的嵌入模型或编码图标到嵌入中以改善系统。
- en: The embedding search usually puts the correct icon on the page, but the correct
    icon isn’t always the first result. We could make a simple algorithm that adjusts
    to user feedback and improves over time. To do this, we’d count every time a user
    clicks on an icon for a particular search query. When ranking search results,
    we’d create a score for each icon that combines the embedding search with the
    click data.
  id: totrans-split-27
  prefs: []
  type: TYPE_NORMAL
  zh: 嵌入式搜索通常会将正确的图标放在页面上，但正确的图标并不总是第一个结果。我们可以制作一个简单的算法，根据用户反馈调整并随时间改进。为此，我们将统计用户每次点击特定搜索查询的图标的次数。在排名搜索结果时，我们将为每个图标创建一个结合嵌入式搜索和点击数据的得分。
- en: Here’s a simple ranking algorithm. The details look messy because there’s some
    null checking and unit conversions, but here are the basics.
  id: totrans-split-28
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个简单的排名算法。细节看起来有些凌乱，因为有些空值检查和单位转换，但基本原理如下。
- en: Get the cosine similarity of the icon for the search query. It will be a number
    between 0 and 1\. Multiply it by 0.5.
  id: totrans-split-29
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 获取搜索查询的图标的余弦相似度。结果将是一个介于0和1之间的数字。乘以0.5。
- en: Divide the number of clicks for each icon by the icon with the most clicks for
    that query. This normalizes the most clicked icon to 1, and the least clicked
    to 0\. Multiply by 0.5.
  id: totrans-split-30
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对每个图标的点击次数除以该查询中点击次数最多的图标的点击次数。这将最常点击的图标归一化为1，最少点击的为0。乘以0.5。
- en: The final score is these two values added together for a range between 0-1.
  id: totrans-split-31
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最终得分是这两个值相加，范围在0-1之间。
- en: '[PRE4]'
  id: totrans-split-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Should vector embedding distance and clicks be equally weighted? Should the
    order of magnitude of clicks matter more than the raw number? The algorithm might
    need tuning, but this is just an example of how the database handles the calculation
    nicely.
  id: totrans-split-33
  prefs: []
  type: TYPE_NORMAL
  zh: 应该等权重考虑向量嵌入距离和点击次数吗？点击次数的数量级是否比原始数字更重要？这个算法可能需要调整，但这只是数据库处理计算的一个示例。
- en: With a separate vector database, we might have to get values for all icons from
    both databases before comparing them in application code or making tradeoffs like
    pulling 100 results from the vector db and filtering the Postgres query for click
    score to those results or vice versa. Instead, we simply query for results and
    display them.
  id: totrans-split-34
  prefs: []
  type: TYPE_NORMAL
  zh: 对于一个独立的向量数据库，我们可能需要从两个数据库中获取所有图标的值，然后在应用代码中比较它们，或者做出类似从向量数据库中拉取100个结果并根据Postgres查询的点击分数对这些结果进行筛选等权衡。相反，我们只需查询结果并显示它们。
- en: Additionally, we include a content forward section of each [icon page](https://www.v0.app/icon/lucide/arrow-up)
    with other icons in the same icon set and category. That way, you can see other
    ‘navigation’ icons when looking at `arrow-up` in case you need those. Unfortunately,
    not all of our icon sets have categories. In these cases, we make a similar icons
    section using embeddings. Instead of getting an embedding from user input, we
    can compare the same cosine similarity measure against the selected icon’s embedding.
  id: totrans-split-35
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们在每个[图标页面](https://www.v0.app/icon/lucide/arrow-up)的内容前向部分都包含了同一图标集和类别中的其他图标。这样，您在查看`arrow-up`时可以看到其他“导航”图标。不幸的是，我们的图标集并非所有都有类别。在这些情况下，我们使用嵌入来创建类似的图标部分。我们可以不需要从用户输入获取嵌入，而是可以将选择的图标的嵌入与相同的余弦相似度测量进行比较。
- en: '[PRE5]'
  id: totrans-split-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: I hope you have a good experience playing with our app and vector embeddings!
    We owe a big thanks to all the engineers who pushed the state of the art so far
    that we can stand on their shoulders. I hope this modest contribution helps make
    adding embedding features to your app approachable!
  id: totrans-split-37
  prefs: []
  type: TYPE_NORMAL
  zh: 我希望您在使用我们的应用和向量嵌入时有一个很好的体验！我们要特别感谢所有推动技术前沿的工程师们，正是他们让我们能够站在他们的肩膀上。希望这份谦逊的贡献能帮助您更轻松地为您的应用添加嵌入特性！
- en: Here is a summary of our implementation decisions and some sources for other
    options.
  id: totrans-split-38
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是我们实现决策的摘要以及一些其他选择的信息来源。
- en: We chose [pgvector](https://github.com/pgvector/pgvector)/Postgres, but there
    are plenty of [other choices](https://lakefs.io/blog/12-vector-databases-2023/),
    including some for other standard databases like MongoDB.
  id: totrans-split-39
  prefs: []
  type: TYPE_NORMAL
  zh: 我们选择了[pgvector](https://github.com/pgvector/pgvector)/Postgres，但也有很多[其他选择](https://lakefs.io/blog/12-vector-databases-2023/)，包括一些适用于其他标准数据库如MongoDB的选择。
- en: We worked in Typescript and chose [drizzle-orm](https://github.com/pgvector/pgvector-node?tab=readme-ov-file#drizzle-orm).
    I’ve also worked in the Phoenix elixir ecosystem using the [elixir library](https://github.com/pgvector/pgvector-elixir)
    with Ecto. You can find a library for your client of choice [here](https://github.com/pgvector/pgvector/?tab=readme-ov-file#languages).
  id: totrans-split-40
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用Typescript并选择了[drizzle-orm](https://github.com/pgvector/pgvector-node?tab=readme-ov-file#drizzle-orm)。我还在Phoenix
    elixir生态系统中使用了[elixir库](https://github.com/pgvector/pgvector-elixir)与Ecto。您可以在[这里](https://github.com/pgvector/pgvector/?tab=readme-ov-file#languages)找到适合您客户端的库。
- en: Our app is hosted on [Neon](https://neon.tech/). I’ve also used [fly.io](https://fly.io/),
    though their option wasn’t *really* a managed instance at the time. Now, they
    have a [managed solution](https://fly.io/docs/reference/supabase/) with Supabase.
    If you want to look up someone else, Pgvector has a clumsy list of hosts that
    support it in [this git issue](https://github.com/pgvector/pgvector/issues/54).
  id: totrans-split-41
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的应用托管在[Neon](https://neon.tech/)上。此外，我曾经使用过[fly.io](https://fly.io/)，尽管当时他们的选择并不是*真正*的托管实例。现在，他们有了一个[托管解决方案](https://fly.io/docs/reference/supabase/)，与Supabase合作。如果您想查找其他服务商，Pgvector在[此git问题](https://github.com/pgvector/pgvector/issues/54)中列出了一份笨拙的支持列表。
- en: We chose OpenAI’s `text-embedding-3-small`. If you want to try something else,
    check out Huggingface’s [leaderboard](https://huggingface.co/spaces/mteb/leaderboard).
  id: totrans-split-42
  prefs: []
  type: TYPE_NORMAL
  zh: 我们选择了OpenAI的`text-embedding-3-small`。如果您想尝试其他内容，请查看Huggingface的[排行榜](https://huggingface.co/spaces/mteb/leaderboard)。
- en: We embedded strings of key and value pairs for the attributes we thought best
    described our icons. It doesn’t seem like any of the major players are doing anything
    crazy here; the significant knobs appear to be whether or not to embed keys or
    just values and which attributes of your records are relevant. Other examples
    in OpenAI’s [cookbook](https://cookbook.openai.com/examples/vector_databases/readme)
    show [other](https://github.com/vercel/examples/blob/main/storage/postgres-pgvector/prisma/seed.ts#L35)  [choices](https://github.com/neondatabase/yc-idea-matcher/blob/18eb9dd6ddd14eeeb2167d78088f092ab6882f42/generate-embeddings.ts#L51).
  id: totrans-split-43
  prefs: []
  type: TYPE_NORMAL
  zh: 我们嵌入了键值对的字符串，这些键值对最能描述我们的图标。看起来主要参与者并没有做出什么非常疯狂的事情；重要的是是否要嵌入键，或者仅仅是值，以及你的记录的哪些属性是相关的。OpenAI的[菜谱](https://cookbook.openai.com/examples/vector_databases/readme)中的其他示例展示了[其他的](https://github.com/vercel/examples/blob/main/storage/postgres-pgvector/prisma/seed.ts#L35)
    [选择](https://github.com/neondatabase/yc-idea-matcher/blob/18eb9dd6ddd14eeeb2167d78088f092ab6882f42/generate-embeddings.ts#L51)。
- en: We used cosine similarity as our distance function because that’s what OpenAI
    [recommends](https://platform.openai.com/docs/guides/embeddings/which-distance-function-should-i-use)
    for their embeddings. Other embeddings may be optimized for different strategies.
    Pgvector [supports](https://github.com/pgvector/pgvector?tab=readme-ov-file#distances)
    l2 distance, inner product, and cosine distance.
  id: totrans-split-44
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用余弦相似度作为我们的距离函数，因为这是OpenAI在他们的嵌入文档中[推荐的](https://platform.openai.com/docs/guides/embeddings/which-distance-function-should-i-use)方法。其他嵌入可能针对不同的策略进行了优化。Pgvector支持l2距离、内积和余弦距离，详见[此处](https://github.com/pgvector/pgvector?tab=readme-ov-file#distances)。
- en: In these examples, we limited our queries to the top 50 results. You can also
    limit your search to be above or below a certain distance threshold. That doesn’t
    seem super reliable. Relative distances seem more meaningful than discrete amounts.
    If you’re set on using a threshold, I’d recommend keeping it pretty wide, like
    0.1 or 0.05, and using it with a limit. You may get some mileage using a wide
    range to avoid returning irrelevant long-tail results.
  id: totrans-split-45
  prefs: []
  type: TYPE_NORMAL
  zh: 在这些示例中，我们将查询限制在前50个结果之内。你也可以将搜索限制在某个距离阈值之上或之下。这似乎并不是非常可靠。相对距离似乎比离散数值更有意义。如果你坚持使用阈值，我建议将其设置得相当宽松，如0.1或0.05，并且结合使用限制条件。你可以通过使用广范围来避免返回无关紧要的长尾结果。
