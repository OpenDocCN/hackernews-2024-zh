- en: <!--yml
  id: totrans-split-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-split-1
  prefs: []
  type: TYPE_NORMAL
  zh: 类别：未分类
- en: 'date: 2024-05-27 13:27:25'
  id: totrans-split-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024年5月27日13:27:25
- en: -->
  id: totrans-split-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: LLMs and the Harry Potter problem
  id: totrans-split-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: LLMs 和《哈利·波特问题》
- en: 来源：[https://www.pyqai.com/blog/llms-and-the-harry-potter-problem](https://www.pyqai.com/blog/llms-and-the-harry-potter-problem)
  id: totrans-split-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://www.pyqai.com/blog/llms-and-the-harry-potter-problem](https://www.pyqai.com/blog/llms-and-the-harry-potter-problem)
- en: Introduction
  id: totrans-split-6
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 简介
- en: 'Long-context LLMs are all the rage these days - it feels like every new one
    increases its context window by about an order of magnitude over the competition.
    They even do well on typical benchmarks, like the [Needle In A Haystack](https://arstechnica.com/information-technology/2024/03/claude-3-seems-to-detect-when-it-is-being-tested-sparking-ai-buzz-online/).
    Unfortunately, in our opinion, they still have some key issues: we call it the
    Harry Potter problem.'
  id: totrans-split-7
  prefs: []
  type: TYPE_NORMAL
  zh: 当前，长上下文LLM正在风靡，感觉每一个新LLM都将其上下文窗口增加了一个数量级。它们甚至在典型的基准测试中表现良好，比如[大海里的针](https://arstechnica.com/information-technology/2024/03/claude-3-seems-to-detect-when-it-is-being-tested-sparking-ai-buzz-online/)。不幸的是，在我们看来，它们仍然存在一些关键问题：我们称之为《哈利·波特问题》。
- en: In this article, we will begin with a description of the problem and its practical
    implications. In response to our proofreaders (thanks all), we provide an explanation
    for why agents, out-of-the-box RAG and fine-tuning aren’t adequate solutions to
    this problem. For more technical readers, we also provide some reading and data
    to chew further on.
  id: totrans-split-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们将从问题及其实际影响的描述开始。在回应我们的校对人员（感谢所有人），我们解释为什么代理、开箱即用的RAG和微调不足以解决这个问题。对于更技术性的读者，我们还提供了一些进一步阅读和数据。
- en: '"Large language models may have big context windows, but they still aren''t
    good enough at using the information in big contexts, especially in high value
    use-cases."'
  id: totrans-split-9
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '"大语言模型可能有很大的上下文窗口，但它们仍然不能很好地利用大上下文中的信息，特别是在高价值的使用案例中。"'
- en: The problem
  id: totrans-split-10
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 问题
- en: 'Imagine you give an LLM (note: not an agent - explanation for why we care about
    the LLM’s ability later in this article) a chapter of Harry Potter and ask it
    to count how many times the word “wizard” is mentioned. GPT4, Claude 3 Opus, Gemini
    Ultra and Mixtral - arguably representative of SOTA as of this writing - all fail
    at this task despite having big context windows.'
  id: totrans-split-11
  prefs: []
  type: TYPE_NORMAL
  zh: 想象一下，你给一个LLM（注：不是一个代理人 - 为什么我们关心LLM的能力会在本文稍后解释）一章《哈利·波特》，并要求它计算“wizard”一词出现的次数。截至本文撰写时，GPT4、Claude
    3 Opus、Gemini Ultra 和 Mixtral - 都是当今的技术代表 - 尽管上下文窗口很大，但在这项任务上都失败了。
- en: 'This test admittedly has a split focus: it is measuring the model’s in-context
    recall and its ability to count, both of which are weak points. For the purpose
    of this discussion, we will focus on in-context recall because that’s more closely
    related to the point of long-context models.'
  id: totrans-split-12
  prefs: []
  type: TYPE_NORMAL
  zh: 这个测试显然有分裂的焦点：它既在衡量模型的上下文回忆能力，也在计数能力上，这两者都是弱点。就本次讨论而言，我们将重点放在上下文回忆上，因为这与长上下文模型的要点更为相关。
- en: 'Here are some summary statistics that help describe the issue. Read till the
    end for all the details on our tests and methodology:'
  id: totrans-split-13
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有一些摘要统计数据，帮助描述这个问题。请阅读到最后以了解我们所有测试和方法论的详细信息：
- en: '**GPT4 Turbo: 55% accuracy** on documents over >=64k tokens'
  id: totrans-split-14
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**GPT4 Turbo：对于大于等于64k个标记的文档，准确率为55%**'
- en: '**Claude 3 Opus: 65% accuracy** on documents over >=64k tokens'
  id: totrans-split-15
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**Claude 3 Opus：对于大于等于64k个标记的文档，准确率为65%**'
- en: '**Mixtral 8x7b Instruct: 17.5% accuracy** on documents over >=64k tokens'
  id: totrans-split-16
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**Mixtral 8x7b Instruct：对于大于等于64k个标记的文档，准确率为17.5%**'
- en: '**Gemini 1.5 Pro: 45% accuracy** on documents over >=64k tokens'
  id: totrans-split-17
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**Gemini 1.5 Pro：对于大于等于64k个标记的文档，准确率为45%**'
- en: Evidently, none do especially well in long contexts. More details about our
    methodology at the end of this post.
  id: totrans-split-18
  prefs: []
  type: TYPE_NORMAL
  zh: 显然，在长上下文中，没有一个表现特别出色的。关于我们的方法论的更多细节，请参阅本文末尾。
- en: Why should I care?
  id: totrans-split-19
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 为什么我要关心？
- en: 'In short, this problem can really hurt LLM accuracy and is very hard to catch.
    Harry Potter is an innocent example, but this problem is far more costly when
    it comes to higher value use-cases. For example, we analyze insurance policies.
    They’re 70-120 pages long, very dense and expect the reader to create logical
    links between information spread across pages (say, a sentence each on pages 5
    and 95). So, answering a question like “what is my fire damage coverage?” means
    you have to read:'
  id: totrans-split-20
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，这个问题可能会严重影响LLM的准确性，并且非常难以捕捉。《哈利·波特》只是一个无辜的例子，但是当涉及到更高价值的使用案例时，这个问题会更加昂贵。例如，我们分析保险政策。它们有70-120页，非常密集，期望读者在分布在各个页面上的信息之间建立逻辑联系（比如，第5页和第95页各一句）。因此，回答类似“我的火灾损失覆盖是多少？”这样的问题意味着你必须阅读：
- en: Page 2 (the premium)
  id: totrans-split-21
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 第2页（保费）
- en: Page 3 (the deductible and limit)
  id: totrans-split-22
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 第3页（免赔额和限额）
- en: Page 78 (the fire damage exclusions)
  id: totrans-split-23
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 第78页（火灾损害除外）
- en: Page 94 (the legal definition of “fire damage”)
  id: totrans-split-24
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 第94页（“火灾损害”的法律定义）
- en: 'This is hard enough for a human expert, and it’s nearly impossible for any
    existing LLM. The greater issue is that any failure of the above process can materially
    affect the correctness of the answer in a high-value use case. To make matters
    worse, the model can (and will) confidently BS. Some other situations where this
    might be an issue include: reviewing lengthy legal cases, understanding codebases,
    reviewing medical records for anyone older than 2, and more.'
  id: totrans-split-25
  prefs: []
  type: TYPE_NORMAL
  zh: 对于人类专家来说已经很难了，对于任何现有的LLM来说几乎是不可能的。更大的问题是上述过程的任何失败可能会实质性地影响高价值用例中答案的正确性。更糟糕的是，模型可以（而且会）自信地编造。可能会出现此类问题的其他情况包括：审查冗长的法律案件、理解代码库、为两岁以上的任何人审查医疗记录等。
- en: Doesn't RAG solve this?
  id: totrans-split-26
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: RAG 不解决这个问题吗？
- en: 'Not quite. Traditional RAG - we’re talking LangChain + embeddings model + OpenAI
    + semantic/hybrid search - does not take into account the structure and informational
    hierarchy of a document. That means the retrieved chunk(s) and prompt ignore the
    rest of the information found near them which might be relevant. For example:
    if you were looking for “fire coverages” in an insurance policy and pulled a chunk
    with the definition of “fire coverage,” it could either be the types of fires
    covered or excluded,and the information describing it as such was located several
    pages before this chunk. And if you really did want to keep the entire structure,
    you’d end up increasing your chunk size, retrieving a large context and end up
    right where you started.'
  id: totrans-split-27
  prefs: []
  type: TYPE_NORMAL
  zh: 不完全是这样。传统的 RAG - 我们在谈论 LangChain + 嵌入模型 + OpenAI + 语义/混合搜索 - 并未考虑文档的结构和信息层次结构。这意味着检索到的片段和提示忽略了其附近可能相关的其他信息。例如：如果您在保险单中查找“火灾保险”，并提取了一个定义了“火灾保险”类型的片段，它可能是涵盖或排除的火灾类型，描述此类信息的位置比这个片段早几页。如果您真的想保留整个结构，您最终会增加片段大小，检索大量上下文，并最终回到起点。
- en: Metadata filtering is a step closer, but it also does not completely solve the
    problem. You still limit the retrieval to some arbitrary number of chunks and
    might miss some context.
  id: totrans-split-28
  prefs: []
  type: TYPE_NORMAL
  zh: 元数据过滤更接近一步，但它也并未完全解决问题。您仍然将检索限制为一些任意数量的片段，可能会错过一些上下文。
- en: Doesn't fine-tuning solve this?
  id: totrans-split-29
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 微调难道不能解决这个问题吗？
- en: Only partially. LoRA and other “quick” fine-tuning methods do not fix the fundamental
    issue with how LLMs tend to digest context, since they have been shown to pay
    more attention to the beginnings and ends of documents than to information located
    near the middle [1 - “Lost in the middle”]. A series of carefully planned full
    fine-tunes, in a certain order, can help long context performance [3 - LongRoPE].
    The core issues here are cost and scarcity of long texts (especially industry-specific
    ones).
  id: totrans-split-30
  prefs: []
  type: TYPE_NORMAL
  zh: 只能解决部分。像 LoRA 和其他“快速”微调方法并不能解决 LLMs 倾向于消化上下文的根本问题，因为已经表明它们更注重文档开头和结尾而不是靠近中间位置的信息
    [1 - “中间丢失”]。一系列精心规划的全面微调，按照特定顺序进行，可以帮助长文本的性能 [3 - LongRoPE]。这里的核心问题是长文本的成本和稀缺性（特别是行业特定的文本）。
- en: Don't agents solve this?
  id: totrans-split-31
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 代理不解决这个问题吗？
- en: As of this writing, agents have the potential to solve this problem but aren’t
    there (yet). OpenAI with tool-calling can solve the Harry Potter problem, but
    it still misses out on the greater context of the document, which is the point
    of this discussion. For it to work, an agent would need to autonomously digest
    the entire document, develop an ontology that addresses your use-case, and figure
    out a way to parse said document with all of its complexities. This would be a
    revelation and we hope it happens. Unfortunately our experience with agents has
    not yet yielded satisfactory results.
  id: totrans-split-32
  prefs: []
  type: TYPE_NORMAL
  zh: 就我写这篇文章的时候而言，代理有潜力解决这个问题，但还没有到位。OpenAI 通过调用工具可以解决哈利·波特的问题，但仍然错过了文档的更大背景，这正是这次讨论的重点。为了使其工作，代理需要自主消化整个文档，开发一个适合您用例的本体论，并找到一种解析文档及其所有复杂性的方法。这将是一个启示，我们希望这种情况发生。不幸的是，我们对代理的经验尚未产生令人满意的结果。
- en: Ok, so what’re you doing about it?
  id: totrans-split-33
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 好的，你打算怎么解决？
- en: We’ve found that the best way to solve this is to have an opinionated view of
    what each long document should look like, the information it should contain, and
    how the information within the document is interconnected. This is a difficult
    undertaking and it does not generalize, so unfortunately there isn’t really a
    hack here. For instance, we’ve spent months studying every insurance policy we
    can get our hands on to develop an ontology that we try to fit the document to,
    and then built an ingestion and retrieval pipeline around it.
  id: totrans-split-34
  prefs: []
  type: TYPE_NORMAL
  zh: 我们发现解决这个问题的最佳方法是对每个长文档的外观有一个有主见的看法，它应包含的信息，以及文档内部信息的互联关系。这是一项艰巨的任务，并且不具有普遍性，所以很遗憾这里没有捷径。例如，我们花了数月时间研究我们能获取到的每一份保险单，以制定一个本体论，我们试图将文档适应其上，并围绕此建立了摄入和检索管道。
- en: 'Basically, we made a tradeoff: in exchange for great insurance policy understanding,
    we gave up the ability to understand anything else. We had to modify our general-purpose
    retrieval engine to work with certain categories of queries that (to our knowledge)
    do not exist outside of the industry, the LLM to be able to properly understand
    industry terminology, and our document ingestion to handle faxes (yup that’s a
    thing too). Knowledge graphs can be helpful with the right approach, as can experimenting
    with the many recently announced chunking techniques.'
  id: totrans-split-35
  prefs: []
  type: TYPE_NORMAL
  zh: 基本上，我们进行了一种权衡：为了获得对保险政策的深入理解，我们放弃了理解其他任何事物的能力。我们不得不修改我们的通用检索引擎，以处理我们知识范围外（据我们所知）不存在的某些类型的查询，LLM以便能够正确理解行业术语，并且我们的文档摄入要能够处理传真（是的，这也是一种情况）。知识图谱在正确的方法下可能会有所帮助，如同最近宣布的多种分块技术的实验。
- en: Another thing to consider is treating the document like an encyclopedia - build
    a table of contents, glossary and list of citations that an LLM can consult before
    retrieving chunks from it. This is also useful during ingestion.
  id: totrans-split-36
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个需要考虑的事项是将文档视为百科全书 - 建立目录、术语表和LLM可以在检索片段之前查阅的引用列表。这在摄入过程中也很有用。
- en: How do I do this for my own documents?
  id: totrans-split-37
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 我如何为自己的文档做到这一点？
- en: 'Start by picking a category of document you wish to understand, and accept
    that you’ll have to repeat this process for each new category until either:'
  id: totrans-split-38
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，选择您希望理解的文档类别，并接受您必须为每个新类别重复此过程，直到：
- en: The Harry Potter problem is fixed, or
  id: totrans-split-39
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 哈利波特问题已解决，或者
- en: Someone comes up with a generalizable way to build a knowledge graph out of
    a document that actually works
  id: totrans-split-40
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 有人提出了一个通用的方法，可以从文档中构建知识图谱，并且实际有效。
- en: 'Side-note: we would love to hear about work done on b, especially given the
    uptick in the number of long-context LLMs.'
  id: totrans-split-41
  prefs: []
  type: TYPE_NORMAL
  zh: 'Side-note: 我们很乐意听到关于b工作的讨论，特别是考虑到长内容LLM数量的增加。'
- en: Next, develop an opinion of the information that you think all variants of this
    document must have. Try to list them, their types and their relationships to each
    other (Graph Theory is helpful in this respect).
  id: totrans-split-42
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，对你认为所有这类文档必须具备的信息形成一个看法。尝试列出它们，它们的类型以及它们之间的关系（图论在此方面是有帮助的）。
- en: Finally, and much easier said than done, experiment with as many examples as
    you can find. Chances are that you will need to iterate on this several times
    before you see good results. For the right business problem, however, the juice
    is worth the squeeze.
  id: totrans-split-43
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，这比做起来容易得多，尝试尽可能多的示例。很可能您需要多次迭代才能看到良好的结果。然而，对于正确的商业问题，果实是值得的。
- en: Descriptive statistics
  id: totrans-split-44
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 描述性统计
- en: This is not meant to be comprehensive or academic, however hopefully these statistics
    will help convey the point. You can try all of them yourself using the prompts
    and information below.
  id: totrans-split-45
  prefs: []
  type: TYPE_NORMAL
  zh: 这并不意味着全面或学术化，但希望这些统计数据能帮助传达观点。您可以使用以下提示和信息自行尝试它们。
- en: '*Accuracy of top 4 LLMs when given the Harry Potter problem*'
  id: totrans-split-46
  prefs: []
  type: TYPE_NORMAL
  zh: '*给定哈利波特问题时，前4个LLM的准确性*'
- en: '*Accuracy of top 4 LLMs when tested on a simple factual query that requires
    in-context recall*'
  id: totrans-split-47
  prefs: []
  type: TYPE_NORMAL
  zh: '*测试简单事实查询时前4个LLM的准确性，要求在上下文中回忆*'
- en: '*Accuracy of top 4 LLMs when asked to find multiple needles in a haystack*'
  id: totrans-split-48
  prefs: []
  type: TYPE_NORMAL
  zh: '*被要求在大量信息中找到多个关键信息时，前4个LLM的准确性*'
- en: '*Accuracy of top 4 LLMs when asked to list all insurance policy coverage types*'
  id: totrans-split-49
  prefs: []
  type: TYPE_NORMAL
  zh: '*被要求列出所有保险政策覆盖类型时，前4个LLM的准确性*'
- en: Prompts & Methodology
  id: totrans-split-50
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 提示和方法论
- en: We ran each of the following prompts using the same input and various temperatures
    10 times to minimize situations where the model got unlucky. They were evaluated
    by a combination of another LLM (GPT3.5) and human review. Incomplete answers
    as well as overly verbose ones containing irrelevant information are considered
    wrong. The results are an aggregate of all the runs.
  id: totrans-split-51
  prefs: []
  type: TYPE_NORMAL
  zh: 我们对每个以下提示使用相同的输入和不同的温度进行了10次运行，以最小化模型运气不佳的情况。它们通过另一个LLM（GPT3.5）和人工审查的组合进行评估。不完整的答案以及包含无关信息的冗长答案都被视为错误。结果是所有运行的综合。
- en: 'Prompts:'
  id: totrans-split-52
  prefs: []
  type: TYPE_NORMAL
  zh: 提示：
- en: How many times is the word ‘wizard’ mentioned in the following excerpt? <chapter
    1 of The Half Blood Prince here>
  id: totrans-split-53
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在以下摘录中，“巫师”一词出现了多少次？ <《哈利·波特与混血王子》第一章在此>
- en: 'What is section 523 of the Dodd-Frank act about based on the information below:
    <[Section-wise summary of the Dodd-Frank act, US Congress](https://www.congress.gov/bill/111th-congress/house-bill/4173)>?'
  id: totrans-split-54
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 基于以下信息，多德-弗兰克法案的第523节是关于什么的？ <[多德-弗兰克法案章节摘要，美国国会](https://www.congress.gov/bill/111th-congress/house-bill/4173)>
- en: 'List out the first 5 numbered sections and summaries of Title V, Subtitle B
    in the document below: <[Section-wise summary of the Dodd-Frank act, US Congress](https://www.congress.gov/bill/111th-congress/house-bill/4173)>'
  id: totrans-split-55
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 列出以下文件中第V标题，B小标题的前5个编号部分及其摘要：<[多德-弗兰克法案章节摘要，美国国会](https://www.congress.gov/bill/111th-congress/house-bill/4173)>
- en: 'List out all of the coverage types from the insurance policy below. Some examples
    of coverage types include: Professional Liability, Commercial General Liability,
    Employee Benefits Liability, and Cyber and Privacy. <insurance policy redacted
    for privacy reasons, but use any commercial policy you want>'
  id: totrans-split-56
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 列出以下保险单中的所有保险类型。一些例子包括：专业责任、商业综合责任、雇员福利责任以及网络和隐私。 <因隐私原因保险单已删除，请使用任何商业保单>
- en: ‍
  id: totrans-split-57
  prefs: []
  type: TYPE_NORMAL
  zh: ‍
- en: Technical reading
  id: totrans-split-58
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 技术阅读
- en: '[Lost in the middle](https://arxiv.org/abs/2307.03172): A pretty good review
    of the fundamental issue with LLMs and long contexts'
  id: totrans-split-59
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[中间迷失](https://arxiv.org/abs/2307.03172)：对LLMs和长上下文的基本问题进行了相当不错的审查'
- en: '[RULER](https://arxiv.org/abs/2404.06654): A new framework proposed by researchers
    at NVidia to replace the needle in a haystack test. This demonstrates the Harry
    Potter problem far more rigorously.'
  id: totrans-split-60
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[RULER](https://arxiv.org/abs/2404.06654)：由Nvidia研究人员提出的一个新框架，旨在取代干草堆中的针测试。这更严谨地展示了哈利·波特问题。'
- en: '[ALiBi](https://arxiv.org/abs/2108.12409) and [LongRoPE](https://arxiv.org/abs/2402.13753)
    (an extension of the original RoPE design most popularly seen in the T5 models):
    How we can get larger context windows without training on equivalently large bodies
    of text'
  id: totrans-split-61
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[ALiBi](https://arxiv.org/abs/2108.12409) 和 [LongRoPE](https://arxiv.org/abs/2402.13753)（是T5模型中最受欢迎的RoPE设计的扩展）：我们如何在没有相等大文本训练的情况下获得更大的上下文窗口'
- en: '[Long-context LLMs Struggle with Long In-context Learning](https://huggingface.co/papers/2404.02060)'
  id: totrans-split-62
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[长文本环境下的LLMs在长期文本学习中的挑战](https://huggingface.co/papers/2404.02060)'
- en: '*Want to analyze and compare long, complicated insurance policies? Get in touch
    with us by booking a call using the link above!*'
  id: totrans-split-63
  prefs: []
  type: TYPE_NORMAL
  zh: '*想要分析和比较长而复杂的保险单？通过上述链接预约我们的电话！*'
