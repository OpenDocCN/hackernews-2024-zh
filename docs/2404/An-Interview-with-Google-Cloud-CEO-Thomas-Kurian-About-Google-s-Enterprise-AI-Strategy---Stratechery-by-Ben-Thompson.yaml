- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-05-27 13:06:31'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: An Interview with Google Cloud CEO Thomas Kurian About Google’s Enterprise AI
    Strategy – Stratechery by Ben Thompson
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://stratechery.com/2024/an-interview-with-google-cloud-ceo-thomas-kurian-about-googles-enterprise-ai-strategy/](https://stratechery.com/2024/an-interview-with-google-cloud-ceo-thomas-kurian-about-googles-enterprise-ai-strategy/)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Good morning,
  prefs: []
  type: TYPE_NORMAL
- en: This week’s Stratechery Interview is with Google Cloud CEO [Thomas Kurian](https://twitter.com/ThomasOrTK).
    Kurian joined Google to lead the company’s cloud division in 2018; prior to that
    he was President of Product Development at Oracle, where he worked for 22 years.
  prefs: []
  type: TYPE_NORMAL
- en: In this interview (which was conducted last night after I had already published
    [Gemini 1.5 and Google’s Nature](https://stratechery.com/2024/gemini-1-5-and-googles-nature/)),
    we discuss [Kurian’s keynote](https://www.youtube.com/watch?v=V6DJYGn2SFk) at
    this week’s Google Cloud Next conference, which was almost completely focused
    on AI. To that end, we cover Google Cloud’s strategy, why AI offers a reset in
    competition, and how the company can win in the enterprise space broadly. As yesterday’s
    Article — and this Interview — make clear, I do think that Google is very well
    placed in this area; one certainly gets the impression that Kurian thinks so as
    well.
  prefs: []
  type: TYPE_NORMAL
- en: As a reminder, all Stratechery content, including interviews, is available as
    a podcast; click the link at the top of this email to add Stratechery to your
    podcast player.
  prefs: []
  type: TYPE_NORMAL
- en: 'On to the Interview:'
  prefs: []
  type: TYPE_NORMAL
- en: An Interview with Google Cloud CEO Thomas Kurian About Google’s Enterprise AI
    Strategy
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '*This interview is lightly edited for clarity.*'
  prefs: []
  type: TYPE_NORMAL
- en: Google’s Cloud Strategy
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Thomas Kurian, [welcome back to Stratechery](https://stratechery.com/2021/interviews-with-patrick-collison-brad-smith-thomas-kurian-and-matthew-prince-on-moderation-in-infrastructure/#kurian).**'
  prefs: []
  type: TYPE_NORMAL
- en: '**Thomas Kurian:** Thank you for having me.'
  prefs: []
  type: TYPE_NORMAL
- en: '**So I’m going to give you the floor to start. If someone comes to you and
    says, “Hey, Thomas. What was [Google Cloud Next](https://cloud.withgoogle.com/next)
    about [this year](https://stratechery.com/2024/gemini-1-5-and-googles-nature/)?
    You did it a few months early, what is the takeaway you wanted folks to have from
    the keynote?”**'
  prefs: []
  type: TYPE_NORMAL
- en: '**TK:** The focus of our product strategy is what we’re explaining to clients
    at the keynote, and the product strategy is fairly simple. We are seeing significant
    acceleration in how customers want to use digital tools and AI to transform their
    core business. To enable that, we offer two important things. First, as people
    have moved AI out of proof-of-concepts into production deployments across many
    different parts of their enterprise, they want not to choose a model, but they
    want a platform, and the platform needs certain characteristics. The platform
    needs to provide a set of services to tune models, to connect them with their
    enterprise systems, to be able to delegate tasks to the model, to measure the
    quality of the model, to test it, to deploy it, monitor it. Our platform provides
    that.'
  prefs: []
  type: TYPE_NORMAL
- en: In addition, we have an open architecture, which gives them the ability to use
    these services, but across a range of different models. Some from Google, obviously
    Gemini, but also from partners.
  prefs: []
  type: TYPE_NORMAL
- en: We announced in addition that Gemini has made some major advances. We introduced
    the million context window, we showed a number of places where it’s being used
    by customers, we’ve done a significant update to [Imagen](https://imagen.research.google/),
    which is our image model, and we’ve integrated it across our portfolio. When I
    say across our portfolio to assist people in doing things in Workspace, for example,
    writing, email, creating documents, but just as much as assisting in their existing
    workflow, we also showed through the introduction of [Google Vids](https://www.youtube.com/watch?v=4SCjXcBeW1E),
    a brand new experience that you can create using generative AI, which is storytelling.
  prefs: []
  type: TYPE_NORMAL
- en: Similarly, we brought it across our platform in Google Cloud. We started with
    coding, but we added operations for people who want to operate a cloud. We added
    the capability for analysis of data and in each step as well as cyber. And in
    each step, it is meant to open up how people can talk to their data, open up how
    quickly teams can do cybersecurity analysis by having a real expert work alongside
    them.
  prefs: []
  type: TYPE_NORMAL
- en: So one big piece was you need as an enterprise to choose a platform if you’re
    going to use AI at scale, and that platform needs to be open, but it also needs
    to be vertically optimized because if you’re going to use it across your environments
    and across your systems and processes, you need it to be cost efficient and scale
    and we know how to do that at Google.
  prefs: []
  type: TYPE_NORMAL
- en: So we introduced a number of new types of advances in our AI and other systems.
    We introduced a [new Axion processor](https://cloud.google.com/blog/products/compute/introducing-googles-new-arm-based-cpu),
    we introduced a number of new advances with Intel in traditional classical computing,
    but we also introduced a variety of new things in our AI systems. TPU v5, some
    new Nvidia systems, and then most importantly, most people think it’s just about
    the chip, it’s not about the chip, it’s the system you build with the chip, and
    there are many things that we do that are super advantageous. One example being
    the new [Parallelstore](https://cloud.google.com/parallelstore) that we introduced
    to provide a 10x improvement in the way that data loads off disk into a model
    when it’s being trained, which is super important if you’re going to large-scale
    training or inferencing.
  prefs: []
  type: TYPE_NORMAL
- en: So these are some of the advances. We’ve also made the ability to now deploy
    these on extraordinarily large clusters at scale and because we provide an assortment
    of different kinds of accelerators, which are optimized for many different kinds
    of training and serving needs, we’re seeing a lot of pickup and it makes it much
    more efficient for people from a cost-performance-latency point-of-view.
  prefs: []
  type: TYPE_NORMAL
- en: '**Well, that’s a good overview. I think you previewed a lot of the stuff that
    I want to get into, but you did mention that, “People are moving out of proof-of-concept
    into actually doing products”. Is that actually happening? What are the actual
    use cases that companies are actually rolling out broadly as opposed to doing
    experiments on what might be possible?**'
  prefs: []
  type: TYPE_NORMAL
- en: '**TK:** Broad-brush, Ben, we can break it into four major categories. One category
    is streamlining internal processes within the organization, streamlining internal
    processes. In finance, you want to automate accounts receivable, collections,
    and cashflow prediction. In human resources, you want to automate your human help
    desk as well as improve the efficiency with which you can do benefits matching,
    for example. In procurement and supply chain, you want for example, look at all
    my suppliers, their contracts with me and tell me which ones have indemnification
    and warranty protection, so I can drive more volume to those that give me indemnification
    and warranties and less to those that don’t. For example, these are all practical
    cases we have customers live in deployment with.'
  prefs: []
  type: TYPE_NORMAL
- en: Second is transforming the customer experience. Transforming the customer experiences,
    how you market, how you merchandise, how you do commerce, how you do sales and
    service. An example is what Mercedes-Benz CEO [Ola Källenius](https://group.mercedes-benz.com/company/corporate-governance/board-of-management/kaellenius/)
    talked about how they’re building a completely new experience for the way that
    they market and sell and service their vehicles.
  prefs: []
  type: TYPE_NORMAL
- en: Third is that some people are integrating it into their products, and when I
    say re-imagining their products, re-imagining their core products using AI. We
    had two examples of companies who are in the devices space. One is Samsung and
    the other one is Oppo, and they’re re-imagining the actual device itself using
    AI with all the multimodality that we provide.
  prefs: []
  type: TYPE_NORMAL
- en: There are quite a few companies now re-thinking that if a model can change the
    way that I see it, that I can process multimodal information. For example, in
    media we have people saying, “If your model can read as much information as it
    can, can it take a long movie and shrink it into highlights? Can I take a sports
    recording of the NCAA basketball final and say, ‘find me all the highlights by
    this particular player’?” and not have to have a human being sit there and splice
    the video, but have it do it and I can create the highlights reel really quickly.
    So there are lots of people re-imagining the product offerings that they have.
  prefs: []
  type: TYPE_NORMAL
- en: And finally, there are some people saying, “With the cost efficiency of this,
    I can change how I enter a brand new market because, for example, I can do personalized
    offers in a market where I may not have a physical presence, but I can do much
    higher conversion rate for customers with online marketing and advertising because
    now I can do highly tailored campaigns because the cost of creating the content
    is much lower.” So broad-brush, streamline the core processes and back office,
    transform the customer experience and it doesn’t mean call centers or chatbots,
    it can be actually transferring the product itself, transforming the nature of
    the product you build and enter new markets.
  prefs: []
  type: TYPE_NORMAL
- en: '**Is it fair to say then when you talk about, “Moving from proof-of-concept
    to actual production”, or maybe that’s not the words you used, but people are
    saying, “Okay, we’re going to build this” because this stuff’s not showing up
    yet, in the real world. Is it the case that, “We see that this could be valuable,
    now we’re in”, and that’s why you’re emphasizing the platform choice now because
    they’ve committed to AI broadly, and now it’s like, “Where are we going to build
    it”?**'
  prefs: []
  type: TYPE_NORMAL
- en: '**TK:** We have people experimenting, but we also have people actually live
    deployment and directing traffic. Orange, the telecom company, was talking about
    how many customers they’re handling online, Discover Financial was talking about
    how their agents are actually using a search and AI tools to discover information
    from policy and procedure documents live. So there are people actually literally
    running true traffic through these systems and actually using them to handle real
    customer workload.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Are you seeing the case in a lot of in customers, or maybe you’re hearing
    from potential customers, that AI is rolling out, if that’s the right word, in
    an employee arbitrage situation? Where there’s individual employees that are taking
    on themselves to use these tools and they are personally benefiting from the increased
    productivity — maybe they’re doing less work or maybe they’re getting more done
    — and the companies want to capture that more systematically. Is that a theme
    that you’re seeing?**'
  prefs: []
  type: TYPE_NORMAL
- en: '**TK:** We’re seeing three flavors. Flavor one is a company has, we’re going
    to try eight or nine, what they call customer journeys or use cases, we’re going
    to pick the three that we see as the maximum return, meaning value and value does
    not mean cost savings always. It could be, for example, we have one who is handling
    1 million calls a day through our customer service system. Now a million calls
    a day, if you think about it, Ben, an average person can do about 250 calls a
    day, that’s a certain volume in an eight-hour day. If you handled a million, that
    is a lot of people, so the reality is that several of them were not being answered
    and people never called because the wait time was so long. So in that case, it
    was not about cost savings, it’s the fact that they’re getting able to reach many
    more customers than they could do before. So that’s one. One part is people saying,
    “I have a bunch of scenarios, I’m going to pick the three”, and in many cases,
    they’re actually augmenting something they’re doing or doing something they couldn’t
    do before, that’s scenario one.'
  prefs: []
  type: TYPE_NORMAL
- en: Scenario two was I have, for example, there’s a large insurance company that’s
    working with us. Today, when they do claims and risk calculation, it takes a long
    time to handle the claims and the risk, particularly the risk calculation, because
    there’s thousands of pages of documents, there’s a lot of spreadsheets going back
    and forth. They put it into Gemini and it was able to run the calculations much,
    much more quickly. So second is I’m picking a very high value use case for my
    organization, which is the core function, and I’m going to implement it because
    I can get a real competitive advantage. In their case, it’s the fact that they
    can both get more accurate scoring on the risk and they can also do a much more
    accurate job, faster job in responding.
  prefs: []
  type: TYPE_NORMAL
- en: And the third scenario is what you said. “Hey, we’ve got a bunch of people,
    we’re going to give it to a certain number of developers”. For example, our coding
    tool, “They are going to test it, they say it helps me generate much better unit
    tests, it helps me write better quality code”. Wayfair’s CTO was talking about
    what their experience is, and then they say, “Let’s go broadly”, so all three
    patterns are being seen.
  prefs: []
  type: TYPE_NORMAL
- en: An “Open” Platform
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**This was a Google Cloud event broadly, and most of Google Cloud’s business
    today is some combination of Workspace and actual cloud computing. However, the
    entire presentation was about AI. I mean, maybe not the entire, you had a couple
    Google Meet updates and things like that, but by and large, I think that’s a fair
    characterization. What does that say about Google’s priorities? If I’m just a
    regular Google Cloud customer, I’m like, “Yeah, AI is good and cool, but what
    about my managed databases?”, or whatever it might be?**'
  prefs: []
  type: TYPE_NORMAL
- en: '**TK:** Just to be frank, Ben, we have 30,000 people here, and they have hundreds
    of sessions, so a certain set of sessions are on AI, but there’s lots of sessions
    on other topics, I wouldn’t over-index on what you saw in the keynote.'
  prefs: []
  type: TYPE_NORMAL
- en: We do see though that customers, they’re looking increasingly at when they pick
    a cloud partner, they’re looking at it as not as a point in time, but they’re
    looking at it as, “Who’s going to help me transform my business?”, and the basis
    of how they thought about it in 2007, ’08, ’09, was all about, “How do they help
    me either go faster by building apps or reduce my cost of data centers by allowing
    me to lift and shift workloads?”. Now, they’re thinking about it in a different
    way. They’re looking at it as, “Can I use AI to transform my business? Who’s got
    the best platform and tools to help me do that?”.
  prefs: []
  type: TYPE_NORMAL
- en: Once we get them to use the AI platform and tools, it does drag in many of our
    other services. You need good data to feed the model, and so many people use our
    analytics system, BigQuery, to clean up the data, to handle the segmentation of
    it, et cetera. Other people say, “I want to feed it from an operational databases”,
    like AlloyDB, or Spanner, or one of our operational databases. We get a lot of
    people saying, “I want to keep the output of the model safe, secure. I also want
    to see if people are analyzing, trying to figure out a way to attack my systems”,
    so we sell them our cyber tools. So AI is not just AI by itself, it typically
    is a collection of things that you need in order to do AI well.
  prefs: []
  type: TYPE_NORMAL
- en: '**Is there an interpretation of this sort of idea, AI almost as a tip-of-the-spear
    aspect, along with the messaging you really drilled down at the end? You mentioned
    it already on this talk about being “open”. Open can mean lots of things, but
    I interpreted it as, “You can access whatever you need from us at any level”.
    Does that open a sort of extent to, “Look, if you already have cloud computing
    elsewhere, if you have lots of data elsewhere, we’re going to make it super easy
    to plug into us because maybe you want our AI tools, and we hope you come over
    with everything else, but we’re going to work regardless”? Is that one of the
    ways to think about the open framing?**'
  prefs: []
  type: TYPE_NORMAL
- en: '**TK:** Open is three elements. First, definitely that. So today, if you’re
    running your applications on another cloud, or if you’re using a SaaS application,
    like a Salesforce, a Workday, an SAP system, et cetera, and you’re wondering,
    “Can I use Google’s AI but connect it into my existing application?” — yes, you
    can, and you don’t need to actually move anything over to us in order to do that.
    So that’s one, and it allows us to use AI to serve every customer, not just existing
    GCP customers.'
  prefs: []
  type: TYPE_NORMAL
- en: The second thing, when we say open, it means people can choose models for the
    right purpose in their organization. So for example, we have people who say, “Look,
    I love Gemini, I’m going to use it in my customer-facing function and my back
    office function, but I am in this particular line of business, and I want to derive
    my own model for that purpose because I’m integrating it into my own product experience,
    and I really need to control the model weights”. In which case, they may either
    use our open source, or they can use Mistral or some other open source.
  prefs: []
  type: TYPE_NORMAL
- en: But in the past, they’ve always had to choose. If you chose a model, you had
    to choose a toolset, and that doesn’t make any sense because all the common things
    you need to do, do you need to tune it? Do you need to ground it? Do you need
    to integrate it into your existing data store to do retrieval, augmented generation?
    Those are all things that our platform provides, so that’s the second piece that
    we do.
  prefs: []
  type: TYPE_NORMAL
- en: The third piece is we also connect the dots for them into other services and
    GCP if they want to and so it allows us to attract new customers, it allows us
    to be open, to allow them the choice of using a variety of different models to
    serve their own needs organization-wide, while having a common way to manage,
    monitor, and improve the models, and, it also allows us to offer some other services
    along with it.
  prefs: []
  type: TYPE_NORMAL
- en: '**I’m curious about overall, is this sort of a reboot of a growth strategy?
    In the context of Google Cloud results, last year in Q3, [growth slowed pretty
    significantly from 28 to 22%](https://stratechery.com/2023/google-earnings-microsoft-earnings-ai-leverage/),
    margins contracted, which reversed a very long trend. Even before we get to the
    AI connection to that or this platform, what happened last year? What was going
    on there?**'
  prefs: []
  type: TYPE_NORMAL
- en: '**TK:** Clearly, it was not a structural thing, because if you look at our
    Q4 and our results, it bumped up sharply, and we’re very confident about the future.
    We are the only cloud provider, if you go back to 2019 to now, and if you look
    at growing top-line and operating income, no one, not even the one who has the
    largest share, has grown top-line and operating income for as long as we have.'
  prefs: []
  type: TYPE_NORMAL
- en: It’s definitely not a reboot of the strategy. We started this thing called [multicloud](https://cloud.google.com/multicloud),
    which is, “Don’t be locked into a single cloud provider, allow people to use a
    choice of cloud provider, allow them to choose the best cloud provider for the
    task”. So analytics is an area where we did particularly well, we did really well
    in certain areas, like certain kinds of legacy systems, migrating them, our systems
    ran them really well because we can handle scale-up in a different way than other
    people did, and so it allowed us to open a lot of doors.
  prefs: []
  type: TYPE_NORMAL
- en: When AI came along, the first cycle was everybody thinking, “I have to pick
    a model,” and the model changes every three weeks and so our point was, “You’re
    chasing the wrong thing when you think about picking a model, what you need to
    do is to think about a platform”, because you need to integrate it into your heterogeneity
    and think about the platform first and the model second, and make sure the platform
    supports a collection of models, because you may choose the latest one from anybody,
    and so that’s the nature of it.
  prefs: []
  type: TYPE_NORMAL
- en: '**Well, that’s the bit though as to why I’m getting to the question of a reboot,
    because I think this idea of you’re going to handle, you can have your multicloud,
    that makes sense given your competitive position in the market, being third place.
    Do you see AI, though, in all this talk about, “You need to choose a platform?
    Sure, our platform’s going to be open, you can use it anywhere” — but do you see
    this as a wedge to be like, “Okay, this is a reboot broadly for the industry as
    far as cloud goes, and sure, your data may be in AWS, or in Azure, or whatever
    it might be, but if you have a platform going forward, you should start with us”?
    Then maybe we’ll look up in ten, fifteen years, and all the center of gravity
    shifted to wherever the platforms are?**'
  prefs: []
  type: TYPE_NORMAL
- en: '**TK:** For sure. I mean, it’s a change in the way that people make purchase
    decisions, right? Ten years ago, you were worried about commodity computing, and
    you were like, “Who’s going to give me the lowest cost for compute, and the lowest
    cost for storage, and the lowest cost for networking?”. Now the basis of competition
    has changed and we have a very strong position, given our capability both at the
    top, meaning offering a platform, offering models, et cetera, and building products
    that have long integrated models.'
  prefs: []
  type: TYPE_NORMAL
- en: Just as an example, Ben, integrating a model into a product is not as easy as
    people think; Gmail has been doing that since 2015\. On any daily basis, there
    are over 500 million operations a day that we run and to do it well, when a partner
    talked about the fact that 75% of people who generate an image for slides actually
    end up presenting it, it’s because we have paid a lot of attention over the years
    on how to integrate it.
  prefs: []
  type: TYPE_NORMAL
- en: So we play at the top of the stack, and we have the infrastructure and scale
    to do it really well from a cost, performance, and global scale that changes the
    nature of the competition. So we definitely see this, as you said, as a reset
    moment for how customers thinking of choosing their cloud decision.
  prefs: []
  type: TYPE_NORMAL
- en: '**If you’re talking about a lot of choices about models, and customers were
    over-indexed on choosing the correct model, that implies that models are maybe
    a commodity, and that we’ve seen with GPT-4 prices are down something like 90%
    since release. Is that a trend you anticipate continuing, and is it something
    that you want to push and actually accelerate?**'
  prefs: []
  type: TYPE_NORMAL
- en: '**TK:** Models — whether they’re a commodity or not, time will tell, these
    are very early innings. All we’re pointing out is every month, there’s a new model
    from a new player, and the existing models get better on many different dimensions.
    It’s like trying to pick a phone based on a camera, and the camera’s changing
    every two weeks, right? Is that the basis on which you want to make your selection?'
  prefs: []
  type: TYPE_NORMAL
- en: '**Well, but if you make that basis, then you might be locked into the operating
    system.**'
  prefs: []
  type: TYPE_NORMAL
- en: '**TK:** That’s right, and so that’s why we say you should choose an open platform,
    and you should be able to use a collection of different models, because it’s changing,
    and don’t lock into a particular operating system at a time when the applications
    on top of it are changing, to use your analogy.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Why is your platform open as compared to others? Microsoft has announced
    you can use other models, not just OpenAI models. Amazon is sort of, to the extent
    you can ascertain a strategy, it’s like, “Look, we’re not committing to anything,
    you could do whatever you want.” Why do you feel comfortable saying, “No, we’re
    the open one,” and they’re not?**'
  prefs: []
  type: TYPE_NORMAL
- en: '**TK:** Well, first of all, the completeness of our platform; [Vertex has](https://cloud.google.com/vertex-ai)
    a lot more services than you can get with the other platforms. Secondly, in order
    to improve a platform, you have to have your own model, because there’s a bunch
    of things you do when you engineer services with that model.'
  prefs: []
  type: TYPE_NORMAL
- en: I’ll give you a really basic example. You use a model, you decide to [ground
    the answers](https://cloud.google.com/vertex-ai/generative-ai/docs/grounding/overview).
    Grounding improves quality, but can also introduce latency. How do you make sure
    that when you’re grounding, you’re not serially post-processing a model’s answer
    to add latency? Unless you have your own model, you wouldn’t even get to that.
    So because we have our own model, we’re able to engineer these things, but we
    make them available as services with other models, so you can use enterprise grounding
    as a very specific example. There are lots of customers using it with Mistral
    and with Llama and with Anthropic.
  prefs: []
  type: TYPE_NORMAL
- en: Second thing, we are not just offering models, but we’re actually helping the
    third party go to customers with us. I met a lot of customers today jointly with
    [CEO] Dario [Amodei] from Anthropic, and it’s a commitment to make sure we’re
    not just giving you our infrastructure, we’re not just training, integrating a
    model into Vertex, we’re not just making it a first-class model, but we’re actually
    bringing it to clients together.
  prefs: []
  type: TYPE_NORMAL
- en: I think that’s what we mean by open. One of the other players has no models
    of their own, so naturally they’re offering a bunch of models, and the other player
    has outsourced their model development to a third party.
  prefs: []
  type: TYPE_NORMAL
- en: Data Gravity and GTM
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**How do you think about LLMs in the context of enterprise? You mentioned this
    grounding bit. I think in the consumer space, it’s like, “Wow, I can look up answers”,
    but from an enterprise perspective, I would imagine, I’m actually curious how
    often people bring up concerns about hallucinations. Maybe they’re even overstated
    to an extent. In a lot of the demos, LLMs were more user interfaces to existing
    data. Is that the framing that this is mostly manifesting in? Or are there other
    use cases? I mean, there’s the, “Write email for me”, but do you see it primarily
    as an interface to data, in the enterprise use case?**'
  prefs: []
  type: TYPE_NORMAL
- en: '**TK:** There’s a bunch of people who are using it to do Q&A with their data,
    we call it Open Book Q&A. Think of it as I have a bunch of data, I want to summarize
    it and ask it questions, and it’s not necessarily look in the raw data, but summarize
    it and ask questions. There’s certainly a collection of it but there’s also people
    who are automating a process, and automating a processes like get the output of
    a model, I feed into another model, and drive a process with it. Do you see what
    I mean?'
  prefs: []
  type: TYPE_NORMAL
- en: '**Yep.**'
  prefs: []
  type: TYPE_NORMAL
- en: '**TK:** They use function calling, like I give a model a question, it generates
    an answer, I put it into a function, the function does a bunch of things, then
    it calls another model, that next model takes it on.'
  prefs: []
  type: TYPE_NORMAL
- en: Think of it as when you’re doing software engineering or coding, I’ll use that
    as an example, one way you can use the model is, “Introspect my code base and
    see if there’s a security vulnerability”, like if I have a VAS vulnerability pattern
    in my code base, that’s the equivalent of the question and answer on your data.
    Second is, “Hey, if you find it, fix it, and here is the pattern I want you to
    follow to fix it, and then run the scan to see if it still exists and then compile
    it and deploy it. When you deploy it, I want to run an A/B test.” It’s that second
    thing we see a lot of people now doing, which is automating a process, and there
    can be a number of different levels of sophistication of that process.
  prefs: []
  type: TYPE_NORMAL
- en: '**How important to achieving Q&A with your data or doing some of these processes
    — Is it essential that that data be on GCP or how does that work with external
    data? Because I think the framing and the advantage, say, to your competitor with
    none of their own models, is, well, they have all the data, and so data has a
    lot of gravity, it will pull people to that because models are a commodity, they
    can do XYZ.**'
  prefs: []
  type: TYPE_NORMAL
- en: '**TK:** We support data from four types of environments, an on-premise data
    center, an application like a software-as-a-service application. Your customer
    service data could be in Salesforce, and Salesforce may not be running at all
    on GCP. It could be on another cloud provider, or it could be in a vector store.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Is the goal that that doesn’t matter? Or is it just a reality that it’s going
    to work better the more that it’s on Google?**'
  prefs: []
  type: TYPE_NORMAL
- en: '**TK:** You don’t have to be on Google to use it, first. And secondly, the
    big thing we say though is, there are tools you need to do to make sure that there’s
    a lot of basic things we suggest to people to make sure they think about it.'
  prefs: []
  type: TYPE_NORMAL
- en: One example is how you handle access control permissions on your data because
    models don’t have the ability to handle access control rules. You can put it in
    a function after you’ve asked the model set of questions, but that’s post-processing.
    There are techniques to add access control, something that engineers [call ACLs](https://cloud.google.com/storage/docs/access-control/lists)
    — [Access Control Lists] on your data, so we suggest they think about that and
    we give them design patterns.
  prefs: []
  type: TYPE_NORMAL
- en: Second, we ask them to measure the tests on the system to make sure that the
    system has predictable latency. Because a model can send a request, and if your
    system has huge queue, sometimes it responds quickly, sometimes it takes a long
    time, the model gets confused whether it’s waiting for an answer or not, that’s
    the second thing.
  prefs: []
  type: TYPE_NORMAL
- en: The third thing that we typically ask them to work through is, there are tools
    to evaluate the quality of the data and when I say the quality of the data, to
    make sure you’re not in a situation where it’s garbage in, garbage out. By quality,
    we’re not talking just about structural quality, making sure there are no null
    sets and things like that, but also semantically, it has a correct meaning, because
    if you ask it for revenue and you are looking at invoice revenue but not GAAP
    revenue, you’ll get an answer on invoice revenue and you need to make sure it’s
    correctly defined whether you mean GAAP revenue or invoice revenue. Those are
    examples, and we provide blueprints to customers to do it.
  prefs: []
  type: TYPE_NORMAL
- en: '**Where are the decisions made about this? Especially when you’re talking about
    the context of a platform and the way it interacts, is this at the CTO, CIO, CEO
    level where these choices are going to be made? Or is this something that engineering
    leads or program manager leads can say, let’s use the Vertex AI platform — where
    are you thinking about the go-to-market there?**'
  prefs: []
  type: TYPE_NORMAL
- en: '**TK:** Typically, the decisions are made jointly between a business unit leader
    and an IT leader. Sometimes the scenario is so important to the company, it’s
    the CEO driving it because it could be a real big change to the business model
    of the company. Often it’s a head of a line of business and the CIO or the CTO.'
  prefs: []
  type: TYPE_NORMAL
- en: '**This is kind of a nerdy question, but I’m curious about how costs are allocated
    for all this sort of AI work. I assume that Google Cloud is paying for its own
    share of servers, but there’s a lot of shared R&D and model development that’s
    shared with Google proper. Is Google Cloud just on the cost of serving side of
    things? I’m curious — like I said, it’s very nerdy — I’m just curious how you
    interact with Google proper given there’s so much shared development here.**'
  prefs: []
  type: TYPE_NORMAL
- en: '**TK:** First of all, our global infrastructure for Google is shared between
    Cloud and the rest of Google, you know that it’s been true for a long time.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Yeah.**'
  prefs: []
  type: TYPE_NORMAL
- en: '**TK:** So from the point of the physical infrastructure, we share the infrastructure
    with all the product units, including with the Google DeepMind team that builds
    the models. On how the costs are allocated, I’ll just tell you, we are reported
    as a segment and we pay a fair share.'
  prefs: []
  type: TYPE_NORMAL
- en: '**This kind of goes back to the commodity sort of question. Is Google Cloud
    and this AI stuff, are you looking to win by being better? Or is there a bit where
    you can really leverage that infrastructure advantage? Of course you should be
    sharing resources, that’s the huge advantage that you have to not just win on
    features but also win on cost and price and TCO and all those sorts of things.**'
  prefs: []
  type: TYPE_NORMAL
- en: '**TK:** Two things, if I may, Ben. First of all, we are winning because we’re
    better. Every customer who has chosen us had the choice to choose anyone else,
    we win because we have great products, including in AI. Second, one of the important
    elements of using AI at scale is cost, but there are other elements along with
    cost. There’s latency. There is, for example, simple things like, “How many times
    do I need to ask a model the question to get the correct answer?”, because every
    time you go to the model, you’re passing in a set of tokens and there’s a set
    of costs. It’s also a customer experience issue. You see what I mean?'
  prefs: []
  type: TYPE_NORMAL
- en: '**Yep.**'
  prefs: []
  type: TYPE_NORMAL
- en: '**TK:** On all those things, there are many, many things we’ve done to sort
    out performance, scale, latency, et cetera. I mean, as a very simple example,
    to get the million tokens to work, there’s changes you need in the way that you
    design the model. There’s changes what you do with the serving stock, et cetera.
    Because we’ve got expertise in both, we’re able to do those things.'
  prefs: []
  type: TYPE_NORMAL
- en: Infrastructure and One Million Tokens
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**How important is that million context window in the story you are telling?
    My perception is, there’s a lot of stuff you could do if you build a lot of infrastructure
    around it, whether it be RAG or other implementations, but it feels like with
    Gemini 1.5 there are jack-of-all-trades possibilities that seem to open up to
    a much greater extent, and there’s a bit where, you had that compliance bit, the
    statements of work and they had to compare it to the 100-page compliance document.
    I got some comments like, “Maybe companies shouldn’t have 100-page compliance
    notebooks or whatever it might be”, but the reality is, that’s the case, the world
    has that. My perception of the keynote is, that was the killer feature, that seemed
    to undergird everything. Was that the correct perception?**'
  prefs: []
  type: TYPE_NORMAL
- en: '**TK:** Yeah, there are two reasons. Just to be perfectly clear, Ben, the long
    context window allows you to do three things that are important. First of all,
    when you look at high definition video, for example, and other modalities, and
    just imagine you’re dumping a high definition video in and you want to create
    out of the NCAA final, which just happened, the highlight reel but you don’t want
    to specify every attribute about what you want spliced into the highlight reel.
    The model has to digest it and because it has to process it, it’s a fairly dense
    representation of the video because there are objects, there are people moving,
    there are actions, like I’m throwing a pass. They could be, I have my name on
    the back of my t-shirt, there could be a score like, “When did they change from
    24 to 26 points? Did they score three pointers?”, so there are many, many, many
    dimensions. So reasoning becomes a lot better when you can take a lot more context,
    that’s one, and it’s particularly true of modality.'
  prefs: []
  type: TYPE_NORMAL
- en: The second is, today people don’t use models to maintain state or memory, meaning
    they ask it a question, the next time they think, “Hey, it may not remember”,
    so when you’re able to maintain a longer context, you can maintain more state,
    and therefore you can do richer and richer things rather than just talk back-and-forth
    with a very simplistic interface. You see what I mean?
  prefs: []
  type: TYPE_NORMAL
- en: The third thing is, there are certainly complex scenarios, it’s the unfortunate
    reality, there’s lots of policies and procedure books that are even longer than
    what we showed, and so there are scenarios like that that we have to be able to
    deal with. But in the longer term, the real breakthrough is the following. Context
    length, if you can decouple the capabilities of the model and the latency to serve
    a model from the context length, then you can fundamentally change how quickly
    you can scale a model.
  prefs: []
  type: TYPE_NORMAL
- en: '**Is this ultimately, from your perspective, a question of infrastructure,
    and that just leans into Google’s biggest advantage?**'
  prefs: []
  type: TYPE_NORMAL
- en: '**TK:** It’s a question of global infrastructure, but also optimizations at
    every layer in the infrastructure, which we can co-engineer with DeepMind.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Right. This gets at the integration point of your own model.**'
  prefs: []
  type: TYPE_NORMAL
- en: '**TK:** Yeah, and then the second thing is the expertise in Google DeepMind
    to use the infrastructure. If I can give an example, it’s sort of like, years
    and years ago, when Google built its global infrastructure and they acquired YouTube,
    they could instantly scale video distribution globally, and if you go back in
    time, there were many companies in the video business when Google acquired YouTube,
    but they didn’t have that scalable infrastructure. In the end, I think you’ve
    seen our success with it because we can do global distribution. This is a similar
    notion, let’s say, a few years on.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Yeah, that fits my thinking on the matter and actually to me this was what
    was encouraging about that keynote, was it felt like there is an aspect, of course,
    in any product development, you need to understand your customer, understand their
    use case, but I think there’s a bit where companies need to understand themselves,
    and what I got from that keynote and what I felt encouraging in a way I haven’t
    seen in other Google presentations in the Post-ChatGPT Era, I’ll call it the Modern
    AI Era, was a real, “We know who we are and what we’re good at and here’s ways
    we’re going to manifest that advantage”. I got that very distinctly, which I thought
    was a positive sign.**'
  prefs: []
  type: TYPE_NORMAL
- en: '**TK:** Thank you.'
  prefs: []
  type: TYPE_NORMAL
- en: Challenges and Opportunities
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**One of the areas where I think Google really dropped the ball, this goes
    back ten years or whatever it is, particularly around Workspace, was building
    an ecosystem. I think I’ve asked you about this before, and the fact that it was
    kind of Microsoft Lite that, “We’re going to do everything”, but Microsoft’s very
    good at that and there’s all these other products in Silicon Valley that are best
    in breed, but no one was tying them together, there wasn’t the glue to have everything
    work together. Does any of those lessons or learnings, or maybe I’m right or wrong,
    apply to this next era? How is Google thinking about partnering with other startups
    or with other entities as far as being able to glue different stuff together?
    Because that is a little bit in contrast to the story you’re telling about, “Look,
    we have real integration advantages by doing a lot of stuff ourselves”. How are
    you thinking about the balance there?**'
  prefs: []
  type: TYPE_NORMAL
- en: '**TK:** We definitely are working with an ecosystem, and the reason is the
    following. We’ve always said the fact that we’re integrating things doesn’t mean
    we’re a closed system. So just recently, since you mentioned Workspace, we have
    integrations to automate workflow for people. People eventually want to use a
    collaboration tool, not to just collaborate, but they actually wanted to automate
    workflow. So we’ve done work with many partners, and there were several at this
    event from Canva to HubSpot to DocuSign to Adobe and several others where we’ve
    actually done the detail work to make that workflow totally seamless, you’re going
    to continue seeing us do more of it.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Sundar Pichai mentioned in his video greeting, he emphasized the number of
    AI startups, and particularly AI unicorns using Google Cloud. To go back to the
    reboot idea, do you view the AI Era as a restart in terms of capturing the next
    generation of companies? I mean, obviously, AWS had a huge advantage here as far
    as general cloud computing, the entire mobile app ecosystem was by and large built
    on AWS. In the enterprise era, you have to deal with what’s there, what they’ve
    already dealt with, you have to have the integrations. Do you see yourself as
    having this as a big focus, “We’re going to own this era of startups”?**'
  prefs: []
  type: TYPE_NORMAL
- en: '**TK:** Yes. And by the way, every one of those startups is being pursued by
    the other two, and the fact that 90% of the unicorns and 60% of all AI-funded
    startups, up in each case by ten points in eight months, and they are the most
    discerning ones. I mean, just to be frank, the unicorns, for them, it is the really
    biggest cost of goods sold in their P&L.'
  prefs: []
  type: TYPE_NORMAL
- en: '**So what’s the driver there?**'
  prefs: []
  type: TYPE_NORMAL
- en: '**TK:** The efficiency of our infrastructure.'
  prefs: []
  type: TYPE_NORMAL
- en: '**They don’t have all the legacy data and legacy things, so you’ll feel like
    on an even playing field without that legacy, no one can touch you?**'
  prefs: []
  type: TYPE_NORMAL
- en: '**TK:** We are not looking to raise a trillion dollars to [build a supercomputer](https://www.theinformation.com/articles/microsoft-and-openai-plot-100-billion-stargate-ai-supercomputer?rc=gtxu6q)
    because we’ve been building one for ten years, and it’s our fifth generation of
    building one of those, so we’re not learning on the job for the first time.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Google is, enterprise customers or startups, whoever it might be, they’re
    buying infrastructure or they’re wanting to use models. Is there any concern about
    Google in the consumer space? I mean, [one of the challenges with becoming an
    answer engine as opposed to a search engine is you’re giving one answer](https://stratechery.com/2024/aggregators-ai-risk/),
    and there’s a lot of fighting over what that answer should be. You’re not offloading
    that to the user. Are you feeling any blow back or worries about just Google being
    a [culture war](https://stratechery.com/2024/gemini-and-googles-culture/) flash
    point or Gemini, or whatever it might be? Or has that been relatively immaterial?**'
  prefs: []
  type: TYPE_NORMAL
- en: '**TK:** That has been completely immaterial.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Is it an asset?**'
  prefs: []
  type: TYPE_NORMAL
- en: '**TK:** We are known as a company with great engineering prowess. You asked,
    Ben, what’s the nature of Google? Google at the heart of it is a great engineering
    company. We build great products, we know how to build the most scalable infrastructure
    in the world. As people choose to do AI, they want a partner who has been there,
    done that, and unlike people who are for the first time working with a third-party
    model and for the first time trying to introduce their products, we’ve built models
    for years and we’ve integrated our products for years and we’ve run it for years
    on a scalable infrastructure. So every company has issues, we’re proud of who
    we are and customers are proud to work with us.'
  prefs: []
  type: TYPE_NORMAL
- en: '**I’ve been impressed. I’ve told you this last time you were on here. I’ve
    been really impressed with the work you’ve done with Google Cloud. And I’m curious,
    as you look back on your tenure, to what extent have you succeeded by building
    an organization that’s independent from Google as opposed to integrated with Google?
    What’s the push and pull there, particularly given that the enterprise go-to market
    and the support needs and all those things are very distinct from a many billion
    users consumer product?**'
  prefs: []
  type: TYPE_NORMAL
- en: '**TK:** We have different needs, we have shared values. So we have shared values
    on many things, treating our employees fairly, aspiring to excellence, making
    sure that our teams work effectively, empathetically, and collaboratively with
    the rest of Google. At the end of the day, all of our models are served on the
    infrastructure our team builds. When you use search, it runs on the infrastructure
    that our team delivers. When you use YouTube, it runs on the network that our
    team builds. So we work very well with the rest of Google.'
  prefs: []
  type: TYPE_NORMAL
- en: At the same time, the necessity to work in an enterprise context requires us
    to have some unique things. As an example, we have many functions that the rest
    of Google doesn’t have. They don’t need professional services, they don’t need
    certain kinds of commercial attorneys, they don’t need certain kinds of systems
    that we do. Even the sales team works differently than the way that our sales
    team has to do because we sell to technology departments while they sell to chief
    marketing officers, so there are differences in what we do.
  prefs: []
  type: TYPE_NORMAL
- en: At the same time, we’ve earned the respect of the rest of Google by the hard
    work we’ve done to win customers and grow the business. And we’re very proud.
  prefs: []
  type: TYPE_NORMAL
- en: '**And I think you should be. Is there a future where we look back in decades
    and we’re like, “Oh yeah, Google, the AI infrastructure company, they started
    as a search engine, believe it or not”? Is there a potential where what your building
    is actually in this new era what comes to matter most?**'
  prefs: []
  type: TYPE_NORMAL
- en: '**TK:** To us, five years ago, nobody gave us a shot, and to be frank, I was
    told it doesn’t matter at all. One way to look at it is, “Are we working on something
    that’s important?”, another way to look at it is, “It’s all upside”, and we took
    the latter. And today it matters, there’s no question about it. When we look at
    the investor community, when we look at the customer community, when we look at
    the relationships that Google has now, because we both serve the technology team
    and the marketing team in many companies, it definitely matters. Will it be the
    most important thing? Time will tell. I’ve learned to never make a forecast in
    technology.'
  prefs: []
  type: TYPE_NORMAL
- en: '**(laughing) I think you make lots of forecasts in your day-to-day job, but
    you’re not going to share them with me, which is totally fine. Thomas, thanks
    for coming back on. I thought it was a good keynote. There’s a bit about AI where
    we don’t have to discuss on here, it’s not your area, that proposes fundamental
    challenges for the consumer product from a business model perspective, from the
    answer perspective, but at the same time, it’s like a hand-in-glove fit with the
    cloud and enterprise and what you’re doing, and it seems to me that you seem to
    recognize and are capitalizing on the same thing.**'
  prefs: []
  type: TYPE_NORMAL
- en: '**TK:** Thank you so much for your time, Ben. Good to talk to you.'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: This Daily Update Interview is also available as a podcast. To receive it in
    your podcast player, [visit Stratechery](https://stratechery.passport.online/member).
  prefs: []
  type: TYPE_NORMAL
- en: The Daily Update is intended for a single recipient, but occasional forwarding
    is totally fine! If you would like to order multiple subscriptions for your team
    with a group discount (minimum 5), please contact me directly.
  prefs: []
  type: TYPE_NORMAL
- en: Thanks for being a supporter, and have a great day!
  prefs: []
  type: TYPE_NORMAL
- en: '*Related*'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
