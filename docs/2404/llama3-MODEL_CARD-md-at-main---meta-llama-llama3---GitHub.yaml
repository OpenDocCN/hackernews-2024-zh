- en: <!--yml
  id: totrans-split-0
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  id: totrans-split-1
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-05-27 13:24:27'
  id: totrans-split-2
  prefs: []
  type: TYPE_NORMAL
- en: -->
  id: totrans-split-3
  prefs: []
  type: TYPE_NORMAL
- en: llama3/MODEL_CARD.md at main · meta-llama/llama3 · GitHub
  id: totrans-split-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://github.com/meta-llama/llama3/blob/main/MODEL_CARD.md](https://github.com/meta-llama/llama3/blob/main/MODEL_CARD.md)
  id: totrans-split-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Meta developed and released the Meta Llama 3 family of large language models
    (LLMs), a collection of pretrained and instruction tuned generative text models
    in 8 and 70B sizes. The Llama 3 instruction tuned models are optimized for dialogue
    use cases and outperform many of the available open source chat models on common
    industry benchmarks. Further, in developing these models, we took great care to
    optimize helpfulness and safety.
  id: totrans-split-6
  prefs: []
  type: TYPE_NORMAL
- en: '**Model developers** Meta'
  id: totrans-split-7
  prefs: []
  type: TYPE_NORMAL
- en: '**Variations** Llama 3 comes in two sizes — 8B and 70B parameters — in pre-trained
    and instruction tuned variants.'
  id: totrans-split-8
  prefs: []
  type: TYPE_NORMAL
- en: '**Input** Models input text only.'
  id: totrans-split-9
  prefs: []
  type: TYPE_NORMAL
- en: '**Output** Models generate text and code only.'
  id: totrans-split-10
  prefs: []
  type: TYPE_NORMAL
- en: '**Model Architecture** Llama 3 is an auto-regressive language model that uses
    an optimized transformer architecture. The tuned versions use supervised fine-tuning
    (SFT) and reinforcement learning with human feedback (RLHF) to align with human
    preferences for helpfulness and safety.'
  id: totrans-split-11
  prefs: []
  type: TYPE_NORMAL
- en: '|  | **Training Data** | **Params** | **Context length** | **GQA** | **Token
    count** | **Knowledge cutoff** |'
  id: totrans-split-12
  prefs: []
  type: TYPE_TB
- en: '| Llama 3 | A new mix of publicly available online data. | 8B | 8k | Yes |
    15T+ | March, 2023 |'
  id: totrans-split-13
  prefs: []
  type: TYPE_TB
- en: '| 70B | 8k | Yes | December, 2023 |'
  id: totrans-split-14
  prefs: []
  type: TYPE_TB
- en: '**Llama 3 family of models**. Token counts refer to pretraining data only.
    Both the 8 and 70B versions use Grouped-Query Attention (GQA) for improved inference
    scalability.'
  id: totrans-split-15
  prefs: []
  type: TYPE_NORMAL
- en: '**Model Release Date** April 18, 2024.'
  id: totrans-split-16
  prefs: []
  type: TYPE_NORMAL
- en: '**Status** This is a static model trained on an offline dataset. Future versions
    of the tuned models will be released as we improve model safety with community
    feedback.'
  id: totrans-split-17
  prefs: []
  type: TYPE_NORMAL
- en: '**License** A custom commercial license is available at: [https://llama.meta.com/llama3/license](https://llama.meta.com/llama3/license)'
  id: totrans-split-18
  prefs: []
  type: TYPE_NORMAL
- en: Where to send questions or comments about the model Instructions on how to provide
    feedback or comments on the model can be found in the model [README](https://github.com/meta-llama/llama3).
    For more technical information about generation parameters and recipes for how
    to use Llama 3 in applications, please go [here](https://github.com/meta-llama/llama-recipes).
  id: totrans-split-19
  prefs: []
  type: TYPE_NORMAL
- en: '**Intended Use Cases** Llama 3 is intended for commercial and research use
    in English. Instruction tuned models are intended for assistant-like chat, whereas
    pretrained models can be adapted for a variety of natural language generation
    tasks.'
  id: totrans-split-20
  prefs: []
  type: TYPE_NORMAL
- en: '**Out-of-scope** Use in any manner that violates applicable laws or regulations
    (including trade compliance laws). Use in any other way that is prohibited by
    the [Acceptable Use Policy](https://llama.meta.com/llama3/use-policy/) and [Llama
    3 Community License](https://llama.meta.com/llama3/license/). Use in languages
    other than English**.'
  id: totrans-split-21
  prefs: []
  type: TYPE_NORMAL
- en: '**Note: Developers may fine-tune Llama 3 models for languages beyond English
    provided they comply with the [Llama 3 Community License](https://llama.meta.com/llama3/license/)
    and the [Acceptable Use Policy](https://llama.meta.com/llama3/use-policy/).'
  id: totrans-split-22
  prefs: []
  type: TYPE_NORMAL
- en: '**Training Factors** We used custom training libraries, Meta''s Research SuperCluster,
    and production clusters for pretraining. Fine-tuning, annotation, and evaluation
    were also performed on third-party cloud compute.'
  id: totrans-split-23
  prefs: []
  type: TYPE_NORMAL
- en: '**Carbon Footprint Pretraining utilized a cumulative** 7.7M GPU hours of computation
    on hardware of type H100-80GB (TDP of 700W). Estimated total emissions were 2290
    tCO2eq, 100% of which were offset by Meta’s sustainability program.'
  id: totrans-split-24
  prefs: []
  type: TYPE_NORMAL
- en: '|  | **Time (GPU hours)** | **Power Consumption (W)** | **Carbon Emitted(tCO2eq)**
    |'
  id: totrans-split-25
  prefs: []
  type: TYPE_TB
- en: '| Llama 3 8B | 1.3M | 700 | 390 |'
  id: totrans-split-26
  prefs: []
  type: TYPE_TB
- en: '| Llama 3 70B | 6.4M | 700 | 1900 |'
  id: totrans-split-27
  prefs: []
  type: TYPE_TB
- en: '| Total | 7.7M |  | 2290 |'
  id: totrans-split-28
  prefs: []
  type: TYPE_TB
- en: '**CO2 emissions during pre-training**. Time: total GPU time required for training
    each model. Power Consumption: peak power capacity per GPU device for the GPUs
    used adjusted for power usage efficiency. 100% of the emissions are directly offset
    by Meta''s sustainability program, and because we are openly releasing these models,
    the pretraining costs do not need to be incurred by others.'
  id: totrans-split-29
  prefs: []
  type: TYPE_NORMAL
- en: '**Overview** Llama 3 was pretrained on over 15 trillion tokens of data from
    publicly available sources. The fine-tuning data includes publicly available instruction
    datasets, as well as over 10M human-annotated examples. Neither the pretraining
    nor the fine-tuning datasets include Meta user data.'
  id: totrans-split-30
  prefs: []
  type: TYPE_NORMAL
- en: '**Data Freshness** The pretraining data has a cutoff of March 2023 for the
    8B and December 2023 for the 70B models respectively.'
  id: totrans-split-31
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we report the results for Llama 3 models on standard automatic
    benchmarks. For all the evaluations, we use our internal evaluations library.
    For details on the methodology see [here](https://github.com/meta-llama/llama3/blob/main/eval_details.md).
  id: totrans-split-32
  prefs: []
  type: TYPE_NORMAL
- en: '| **Category** | **Benchmark** | **Llama 3 8B** | **Llama2 7B** | **Llama2
    13B** | **Llama 3 70B** | **Llama2 70B** |'
  id: totrans-split-33
  prefs: []
  type: TYPE_TB
- en: '| General | MMLU (5-shot) | 66.6 | 45.7 | 53.8 | 79.5 | 69.7 |'
  id: totrans-split-34
  prefs: []
  type: TYPE_TB
- en: '| AGIEval English (3-5 shot) | 45.9 | 28.8 | 38.7 | 63.0 | 54.8 |'
  id: totrans-split-35
  prefs: []
  type: TYPE_TB
- en: '| CommonSenseQA (7-shot) | 72.6 | 57.6 | 67.6 | 83.8 | 78.7 |'
  id: totrans-split-36
  prefs: []
  type: TYPE_TB
- en: '| Winogrande (5-shot) | 76.1 | 73.3 | 75.4 | 83.1 | 81.8 |'
  id: totrans-split-37
  prefs: []
  type: TYPE_TB
- en: '| BIG-Bench Hard (3-shot, CoT) | 61.1 | 38.1 | 47.0 | 81.3 | 65.7 |'
  id: totrans-split-38
  prefs: []
  type: TYPE_TB
- en: '| ARC-Challenge (25-shot) | 78.6 | 53.7 | 67.6 | 93.0 | 85.3 |'
  id: totrans-split-39
  prefs: []
  type: TYPE_TB
- en: '| Knowledge reasoning | TriviaQA-Wiki (5-shot) | 78.5 | 72.1 | 79.6 | 89.7
    | 87.5 |'
  id: totrans-split-40
  prefs: []
  type: TYPE_TB
- en: '| Reading comprehension | SQuAD (1-shot) | 76.4 | 72.2 | 72.1 | 85.6 | 82.6
    |'
  id: totrans-split-41
  prefs: []
  type: TYPE_TB
- en: '| QuAC (1-shot, F1) | 44.4 | 39.6 | 44.9 | 51.1 | 49.4 |'
  id: totrans-split-42
  prefs: []
  type: TYPE_TB
- en: '| BoolQ (0-shot) | 75.7 | 65.5 | 66.9 | 79.0 | 73.1 |'
  id: totrans-split-43
  prefs: []
  type: TYPE_TB
- en: '| DROP (3-shot, F1) | 58.4 | 37.9 | 49.8 | 79.7 | 70.2 |'
  id: totrans-split-44
  prefs: []
  type: TYPE_TB
- en: '| **Benchmark** | **Llama 3 8B** | **Llama 2 7B** | **Llama 2 13B** | **Llama
    3 70B** | **Llama 2 70B** |'
  id: totrans-split-45
  prefs: []
  type: TYPE_TB
- en: '| MMLU (5-shot) | 68.4 | 34.1 | 47.8 | 82.0 | 52.9 |'
  id: totrans-split-46
  prefs: []
  type: TYPE_TB
- en: '| GPQA (0-shot) | 34.2 | 21.7 | 22.3 | 39.5 | 21.0 |'
  id: totrans-split-47
  prefs: []
  type: TYPE_TB
- en: '| HumanEval (0-shot) | 62.2 | 7.9 | 14.0 | 81.7 | 25.6 |'
  id: totrans-split-48
  prefs: []
  type: TYPE_TB
- en: '| GSM-8K (8-shot, CoT) | 79.6 | 25.7 | 77.4 | 93.0 | 57.5 |'
  id: totrans-split-49
  prefs: []
  type: TYPE_TB
- en: '| MATH (4-shot, CoT) | 30.0 | 3.8 | 6.7 | 50.4 | 11.6 |'
  id: totrans-split-50
  prefs: []
  type: TYPE_TB
- en: We believe that an open approach to AI leads to better, safer products, faster
    innovation, and a bigger overall market. We are committed to Responsible AI development
    and took a series of steps to limit misuse and harm and support the open source
    community.
  id: totrans-split-51
  prefs: []
  type: TYPE_NORMAL
- en: Foundation models are widely capable technologies that are built to be used
    for a diverse range of applications. They are not designed to meet every developer
    preference on safety levels for all use cases, out-of-the-box, as those by their
    nature will differ across different applications.
  id: totrans-split-52
  prefs: []
  type: TYPE_NORMAL
- en: Rather, responsible LLM-application deployment is achieved by implementing a
    series of safety best practices throughout the development of such applications,
    from the model pre-training, fine-tuning and the deployment of systems composed
    of safeguards to tailor the safety needs specifically to the use case and audience.
  id: totrans-split-53
  prefs: []
  type: TYPE_NORMAL
- en: As part of the Llama 3 release, we updated our [Responsible Use Guide](https://llama.meta.com/responsible-use-guide/)
    to outline the steps and best practices for developers to implement model and
    system level safety for their application. We also provide a set of resources
    including [Meta Llama Guard 2](https://llama.meta.com/purple-llama/) and [Code
    Shield](https://llama.meta.com/purple-llama/) safeguards. These tools have proven
    to drastically reduce residual risks of LLM Systems, while maintaining a high
    level of helpfulness. We encourage developers to tune and deploy these safeguards
    according to their needs and we provide a [reference implementation](https://github.com/meta-llama/llama-recipes/tree/main/recipes/responsible_ai)
    to get you started.
  id: totrans-split-54
  prefs: []
  type: TYPE_NORMAL
- en: As outlined in the Responsible Use Guide, some trade-off between model helpfulness
    and model alignment is likely unavoidable. Developers should exercise discretion
    about how to weigh the benefits of alignment and helpfulness for their specific
    use case and audience. Developers should be mindful of residual risks when using
    Llama models and leverage additional safety tools as needed to reach the right
    safety bar for their use case.
  id: totrans-split-55
  prefs: []
  type: TYPE_NORMAL
- en: Safety
  id: totrans-split-56
  prefs: []
  type: TYPE_NORMAL
- en: For our instruction tuned model, we conducted extensive red teaming exercises,
    performed adversarial evaluations and implemented safety mitigations techniques
    to lower residual risks. As with any Large Language Model, residual risks will
    likely remain and we recommend that developers assess these risks in the context
    of their use case. In parallel, we are working with the community to make AI safety
    benchmark standards transparent, rigorous and interpretable.
  id: totrans-split-57
  prefs: []
  type: TYPE_NORMAL
- en: Refusals
  id: totrans-split-58
  prefs: []
  type: TYPE_NORMAL
- en: In addition to residual risks, we put a great emphasis on model refusals to
    benign prompts. Over-refusing not only can impact the user experience but could
    even be harmful in certain contexts as well. We’ve heard the feedback from the
    developer community and improved our fine tuning to ensure that Llama 3 is significantly
    less likely to falsely refuse to answer prompts than Llama 2.
  id: totrans-split-59
  prefs: []
  type: TYPE_NORMAL
- en: We built internal benchmarks and developed mitigations to limit false refusals
    making Llama 3 our most helpful model to date.
  id: totrans-split-60
  prefs: []
  type: TYPE_NORMAL
- en: In addition to responsible use considerations outlined above, we followed a
    rigorous process that requires us to take extra measures against misuse and critical
    risks before we make our release decision.
  id: totrans-split-61
  prefs: []
  type: TYPE_NORMAL
- en: Misuse
  id: totrans-split-62
  prefs: []
  type: TYPE_NORMAL
- en: If you access or use Llama 3, you agree to the Acceptable Use Policy. The most
    recent copy of this policy can be found at [https://llama.meta.com/llama3/use-policy/](https://llama.meta.com/llama3/use-policy/).
  id: totrans-split-63
  prefs: []
  type: TYPE_NORMAL
- en: CBRNE (Chemical, Biological, Radiological, Nuclear, and high yield Explosives)
  id: totrans-split-64
  prefs: []
  type: TYPE_NORMAL
- en: 'We have conducted a two fold assessment of the safety of the model in this
    area:'
  id: totrans-split-65
  prefs: []
  type: TYPE_NORMAL
- en: Iterative testing during model training to assess the safety of responses related
    to CBRNE threats and other adversarial risks.
  id: totrans-split-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Involving external CBRNE experts to conduct an uplift test assessing the ability
    of the model to accurately provide expert knowledge and reduce barriers to potential
    CBRNE misuse, by reference to what can be achieved using web search (without the
    model).
  id: totrans-split-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We have evaluated Llama 3 with CyberSecEval, Meta’s cybersecurity safety eval
    suite, measuring Llama 3’s propensity to suggest insecure code when used as a
    coding assistant, and Llama 3’s propensity to comply with requests to help carry
    out cyber attacks, where attacks are defined by the industry standard MITRE ATT&CK
    cyber attack ontology. On our insecure coding and cyber attacker helpfulness tests,
    Llama 3 behaved in the same range or safer than models of [equivalent coding capability](https://huggingface.co/spaces/facebook/CyberSecEval).
  id: totrans-split-68
  prefs: []
  type: TYPE_NORMAL
- en: Child Safety risk assessments were conducted using a team of experts, to assess
    the model’s capability to produce outputs that could result in Child Safety risks
    and inform on any necessary and appropriate risk mitigations via fine tuning.
    We leveraged those expert red teaming sessions to expand the coverage of our evaluation
    benchmarks through Llama 3 model development. For Llama 3, we conducted new in-depth
    sessions using objective based methodologies to assess the model risks along multiple
    attack vectors. We also partnered with content specialists to perform red teaming
    exercises assessing potentially violating content while taking account of market
    specific nuances or experiences.
  id: totrans-split-69
  prefs: []
  type: TYPE_NORMAL
- en: Community
  id: totrans-split-70
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Generative AI safety requires expertise and tooling, and we believe in the strength
    of the open community to accelerate its progress. We are active members of open
    consortiums, including the AI Alliance, Partnership in AI and MLCommons, actively
    contributing to safety standardization and transparency. We encourage the community
    to adopt taxonomies like the MLCommons Proof of Concept evaluation to facilitate
    collaboration and transparency on safety and content evaluations. Our Purple Llama
    tools are open sourced for the community to use and widely distributed across
    ecosystem partners including cloud service providers. We encourage community contributions
    to our [GitHub repository](https://github.com/meta-llama/PurpleLlama).
  id: totrans-split-71
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we put in place a set of resources including an [output reporting mechanism](https://developers.facebook.com/llama_output_feedback)
    and [bug bounty program](https://www.facebook.com/whitehat) to continuously improve
    the Llama technology with the help of the community.
  id: totrans-split-72
  prefs: []
  type: TYPE_NORMAL
- en: Ethical Considerations and Limitations
  id: totrans-split-73
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](#ethical-considerations-and-limitations)'
  id: totrans-split-74
  prefs: []
  type: TYPE_NORMAL
- en: The core values of Llama 3 are openness, inclusivity and helpfulness. It is
    meant to serve everyone, and to work for a wide range of use cases. It is thus
    designed to be accessible to people across many different backgrounds, experiences
    and perspectives. Llama 3 addresses users and their needs as they are, without
    insertion unnecessary judgment or normativity, while reflecting the understanding
    that even content that may appear problematic in some cases can serve valuable
    purposes in others. It respects the dignity and autonomy of all users, especially
    in terms of the values of free thought and expression that power innovation and
    progress.
  id: totrans-split-75
  prefs: []
  type: TYPE_NORMAL
- en: But Llama 3 is a new technology, and like any new technology, there are risks
    associated with its use. Testing conducted to date has been in English, and has
    not covered, nor could it cover, all scenarios. For these reasons, as with all
    LLMs, Llama 3’s potential outputs cannot be predicted in advance, and the model
    may in some instances produce inaccurate, biased or other objectionable responses
    to user prompts. Therefore, before deploying any applications of Llama 3 models,
    developers should perform safety testing and tuning tailored to their specific
    applications of the model. As outlined in the Responsible Use Guide, we recommend
    incorporating [Purple Llama](https://github.com/facebookresearch/PurpleLlama)
    solutions into your workflows and specifically [Llama Guard](https://ai.meta.com/research/publications/llama-guard-llm-based-input-output-safeguard-for-human-ai-conversations/)
    which provides a base model to filter input and output prompts to layer system-level
    safety on top of model-level safety.
  id: totrans-split-76
  prefs: []
  type: TYPE_NORMAL
- en: Please see the Responsible Use Guide available at [http://llama.meta.com/responsible-use-guide](http://llama.meta.com/responsible-use-guide)
  id: totrans-split-77
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  id: totrans-split-78
  prefs: []
  type: TYPE_PRE
- en: 'Aaditya Singh; '
  id: totrans-split-79
  prefs: &id001 []
  type: TYPE_NORMAL
- en: 'Aaron Grattafiori; '
  id: totrans-split-80
  prefs: *id001
  type: TYPE_NORMAL
- en: 'Abhimanyu Dubey; '
  id: totrans-split-81
  prefs: *id001
  type: TYPE_NORMAL
- en: 'Abhinav Jauhri; '
  id: totrans-split-82
  prefs: *id001
  type: TYPE_NORMAL
- en: 'Abhinav Pandey; '
  id: totrans-split-83
  prefs: *id001
  type: TYPE_NORMAL
- en: 'Abhishek Kadian; '
  id: totrans-split-84
  prefs: *id001
  type: TYPE_NORMAL
- en: 'Adam Kelsey; '
  id: totrans-split-85
  prefs: *id001
  type: TYPE_NORMAL
- en: 'Adi Gangidi; '
  id: totrans-split-86
  prefs: *id001
  type: TYPE_NORMAL
- en: 'Ahmad Al-Dahle; '
  id: totrans-split-87
  prefs: *id001
  type: TYPE_NORMAL
- en: 'Amit Sangani; '
  id: totrans-split-88
  prefs: *id001
  type: TYPE_NORMAL
- en: 'Ahuva Goldstand; '
  id: totrans-split-89
  prefs: *id001
  type: TYPE_NORMAL
- en: 'Aiesha Letman; '
  id: totrans-split-90
  prefs: *id001
  type: TYPE_NORMAL
- en: 'Ajay Menon; '
  id: totrans-split-91
  prefs: *id001
  type: TYPE_NORMAL
- en: 'Akhil Mathur; '
  id: totrans-split-92
  prefs: *id001
  type: TYPE_NORMAL
- en: 'Alan Schelten; '
  id: totrans-split-93
  prefs: *id001
  type: TYPE_NORMAL
- en: 'Alex Vaughan; '
  id: totrans-split-94
  prefs: *id001
  type: TYPE_NORMAL
- en: 'Amy Yang; '
  id: totrans-split-95
  prefs: *id001
  type: TYPE_NORMAL
- en: 'Andrei Lupu; '
  id: totrans-split-96
  prefs: *id001
  type: TYPE_NORMAL
- en: 'Andres Alvarado; '
  id: totrans-split-97
  prefs: *id001
  type: TYPE_NORMAL
- en: 'Andrew Gallagher; '
  id: totrans-split-98
  prefs: *id001
  type: TYPE_NORMAL
- en: 'Andrew Gu; '
  id: totrans-split-99
  prefs: *id001
  type: TYPE_NORMAL
- en: 'Andrew Ho; '
  id: totrans-split-100
  prefs: *id001
  type: TYPE_NORMAL
- en: 'Andrew Poulton; '
  id: totrans-split-101
  prefs: *id001
  type: TYPE_NORMAL
- en: 'Andrew Ryan; '
  id: totrans-split-102
  prefs: *id001
  type: TYPE_NORMAL
- en: 'Angela Fan; '
  id: totrans-split-103
  prefs: *id001
  type: TYPE_NORMAL
- en: 'Ankit Ramchandani; '
  id: totrans-split-104
  prefs: *id001
  type: TYPE_NORMAL
- en: 'Anthony Hartshorn; '
  id: totrans-split-105
  prefs: *id001
  type: TYPE_NORMAL
- en: 'Archi Mitra; '
  id: totrans-split-106
  prefs: *id001
  type: TYPE_NORMAL
- en: 'Archie Sravankumar; '
  id: totrans-split-107
  prefs: *id001
  type: TYPE_NORMAL
- en: 'Artem Korenev; '
  id: totrans-split-108
  prefs: *id001
  type: TYPE_NORMAL
- en: 'Arun Rao; '
  id: totrans-split-109
  prefs: *id001
  type: TYPE_NORMAL
- en: 'Ashley Gabriel; '
  id: totrans-split-110
  prefs: *id001
  type: TYPE_NORMAL
- en: 'Ashwin Bharambe; '
  id: totrans-split-111
  prefs: *id001
  type: TYPE_NORMAL
- en: 'Assaf Eisenman; '
  id: totrans-split-112
  prefs: *id001
  type: TYPE_NORMAL
- en: 'Aston Zhang; '
  id: totrans-split-113
  prefs: *id001
  type: TYPE_NORMAL
- en: 'Ash JJhaveri; '
  id: totrans-split-114
  prefs: *id001
  type: TYPE_NORMAL
- en: 'Aurelien Rodriguez; '
  id: totrans-split-115
  prefs: *id001
  type: TYPE_NORMAL
- en: 'Austen Gregerson; '
  id: totrans-split-116
  prefs: *id001
  type: TYPE_NORMAL
- en: 'Ava Spataru; '
  id: totrans-split-117
  prefs: *id001
  type: TYPE_NORMAL
- en: 'Baptiste Roziere; '
  id: totrans-split-118
  prefs: *id001
  type: TYPE_NORMAL
- en: 'Ben Maurer; '
  id: totrans-split-119
  prefs: *id001
  type: TYPE_NORMAL
- en: 'Benjamin Leonhardi; '
  id: totrans-split-120
  prefs: *id001
  type: TYPE_NORMAL
- en: 'Bernie Huang; '
  id: totrans-split-121
  prefs: *id001
  type: TYPE_NORMAL
- en: 'Bhargavi Paranjape; '
  id: totrans-split-122
  prefs: *id001
  type: TYPE_NORMAL
- en: 'Bing Liu; '
  id: totrans-split-123
  prefs: *id001
  type: TYPE_NORMAL
- en: 'Binh Tang; '
  id: totrans-split-124
  prefs: *id001
  type: TYPE_NORMAL
- en: 'Bobbie Chern; '
  id: totrans-split-125
  prefs: *id001
  type: TYPE_NORMAL
- en: 'Brani Stojkovic; '
  id: totrans-split-126
  prefs: *id001
  type: TYPE_NORMAL
- en: 'Brian Fuller; '
  id: totrans-split-127
  prefs: *id001
  type: TYPE_NORMAL
- en: 'Catalina Mejia Arenas; '
  id: totrans-split-128
  prefs: *id001
  type: TYPE_NORMAL
- en: 'Chao Zhou; '
  id: totrans-split-129
  prefs: *id001
  type: TYPE_NORMAL
- en: 'Charlotte Caucheteux; '
  id: totrans-split-130
  prefs: *id001
  type: TYPE_NORMAL
- en: 'Chaya Nayak; '
  id: totrans-split-131
  prefs: *id001
  type: TYPE_NORMAL
- en: 'Ching-Hsiang Chu; '
  id: totrans-split-132
  prefs: *id001
  type: TYPE_NORMAL
- en: 'Chloe Bi; '
  id: totrans-split-133
  prefs: *id001
  type: TYPE_NORMAL
- en: 'Chris Cai; '
  id: totrans-split-134
  prefs: *id001
  type: TYPE_NORMAL
- en: 'Chris Cox; '
  id: totrans-split-135
  prefs: *id001
  type: TYPE_NORMAL
- en: 'Chris Marra; '
  id: totrans-split-136
  prefs: *id001
  type: TYPE_NORMAL
- en: 'Chris McConnell; '
  id: totrans-split-137
  prefs: *id001
  type: TYPE_NORMAL
- en: 'Christian Keller; '
  id: totrans-split-138
  prefs: *id001
  type: TYPE_NORMAL
- en: 'Christoph Feichtenhofer; '
  id: totrans-split-139
  prefs: *id001
  type: TYPE_NORMAL
- en: 'Christophe Touret; '
  id: totrans-split-140
  prefs: *id001
  type: TYPE_NORMAL
- en: 'Chunyang Wu; '
  id: totrans-split-141
  prefs: *id001
  type: TYPE_NORMAL
- en: 'Corinne Wong; '
  id: totrans-split-142
  prefs: *id001
  type: TYPE_NORMAL
- en: 'Cristian Canton Ferrer; '
  id: totrans-split-143
  prefs: *id001
  type: TYPE_NORMAL
- en: 'Damien Allonsius; '
  id: totrans-split-144
  prefs: *id001
  type: TYPE_NORMAL
- en: 'Daniel Kreymer; '
  id: totrans-split-145
  prefs: *id001
  type: TYPE_NORMAL
- en: 'Daniel Haziza; '
  id: totrans-split-146
  prefs: *id001
  type: TYPE_NORMAL
- en: 'Daniel Li; '
  id: totrans-split-147
  prefs: *id001
  type: TYPE_NORMAL
- en: 'Danielle Pintz; '
  id: totrans-split-148
  prefs: *id001
  type: TYPE_NORMAL
- en: 'Danny Livshits; '
  id: totrans-split-149
  prefs: *id001
  type: TYPE_NORMAL
- en: 'Danny Wyatt; '
  id: totrans-split-150
  prefs: *id001
  type: TYPE_NORMAL
- en: 'David Adkins; '
  id: totrans-split-151
  prefs: *id001
  type: TYPE_NORMAL
- en: 'David Esiobu; '
  id: totrans-split-152
  prefs: *id001
  type: TYPE_NORMAL
- en: 'David Xu; '
  id: totrans-split-153
  prefs: *id001
  type: TYPE_NORMAL
- en: 'Davide Testuggine; '
  id: totrans-split-154
  prefs: *id001
  type: TYPE_NORMAL
- en: 'Delia David; '
  id: totrans-split-155
  prefs: *id001
  type: TYPE_NORMAL
- en: 'Devi Parikh; '
  id: totrans-split-156
  prefs: *id001
  type: TYPE_NORMAL
- en: 'Dhruv Choudhary; '
  id: totrans-split-157
  prefs: *id001
  type: TYPE_NORMAL
- en: 'Dhruv Mahajan; '
  id: totrans-split-158
  prefs: *id001
  type: TYPE_NORMAL
- en: 'Diana Liskovich; '
  id: totrans-split-159
  prefs: *id001
  type: TYPE_NORMAL
- en: 'Diego Garcia-Olano; '
  id: totrans-split-160
  prefs: *id001
  type: TYPE_NORMAL
- en: 'Diego Perino; '
  id: totrans-split-161
  prefs: *id001
  type: TYPE_NORMAL
- en: 'Dieuwke Hupkes; '
  id: totrans-split-162
  prefs: *id001
  type: TYPE_NORMAL
- en: 'Dingkang Wang; '
  id: totrans-split-163
  prefs: *id001
  type: TYPE_NORMAL
- en: 'Dustin Holland; '
  id: totrans-split-164
  prefs: *id001
  type: TYPE_NORMAL
- en: 'Egor Lakomkin; '
  id: totrans-split-165
  prefs: *id001
  type: TYPE_NORMAL
- en: 'Elina Lobanova; '
  id: totrans-split-166
  prefs: *id001
  type: TYPE_NORMAL
- en: 'Xiaoqing Ellen Tan; '
  id: totrans-split-167
  prefs: *id001
  type: TYPE_NORMAL
- en: 'Emily Dinan; '
  id: totrans-split-168
  prefs: *id001
  type: TYPE_NORMAL
- en: 'Eric Smith; '
  id: totrans-split-169
  prefs: *id001
  type: TYPE_NORMAL
- en: 'Erik Brinkman; '
  id: totrans-split-170
  prefs: *id001
  type: TYPE_NORMAL
- en: 'Esteban Arcaute; '
  id: totrans-split-171
  prefs: *id001
  type: TYPE_NORMAL
- en: 'Filip Radenovic; '
  id: totrans-split-172
  prefs: *id001
  type: TYPE_NORMAL
- en: 'Firat Ozgenel; '
  id: totrans-split-173
  prefs: *id001
  type: TYPE_NORMAL
- en: 'Francesco Caggioni; '
  id: totrans-split-174
  prefs: *id001
  type: TYPE_NORMAL
- en: 'Frank Seide; '
  id: totrans-split-175
  prefs: *id001
  type: TYPE_NORMAL
- en: 'Frank Zhang; '
  id: totrans-split-176
  prefs: *id001
  type: TYPE_NORMAL
- en: 'Gabriel Synnaeve; '
  id: totrans-split-177
  prefs: *id001
  type: TYPE_NORMAL
- en: 'Gabriella Schwarz; '
  id: totrans-split-178
  prefs: *id001
  type: TYPE_NORMAL
- en: 'Gabrielle Lee; '
  id: totrans-split-179
  prefs: *id001
  type: TYPE_NORMAL
- en: 'Gada Badeer; '
  id: totrans-split-180
  prefs: *id001
  type: TYPE_NORMAL
- en: 'Georgia Anderson; '
  id: totrans-split-181
  prefs: *id001
  type: TYPE_NORMAL
- en: 'Graeme Nail; '
  id: totrans-split-182
  prefs: *id001
  type: TYPE_NORMAL
- en: 'Gregoire Mialon; '
  id: totrans-split-183
  prefs: *id001
  type: TYPE_NORMAL
- en: 'Guan Pang; '
  id: totrans-split-184
  prefs: *id001
  type: TYPE_NORMAL
- en: 'Guillem Cucurell; '
  id: totrans-split-185
  prefs: *id001
  type: TYPE_NORMAL
- en: 'Hailey Nguyen; '
  id: totrans-split-186
  prefs: *id001
  type: TYPE_NORMAL
- en: 'Hamid Shojanazeri; '
  id: totrans-split-187
  prefs: *id001
  type: TYPE_NORMAL
- en: 'Hannah Korevaar; '
  id: totrans-split-188
  prefs: *id001
  type: TYPE_NORMAL
- en: 'Hannah Wang; '
  id: totrans-split-189
  prefs: *id001
  type: TYPE_NORMAL
- en: 'Haroun Habeeb; '
  id: totrans-split-190
  prefs: *id001
  type: TYPE_NORMAL
- en: 'Harrison Rudolph; '
  id: totrans-split-191
  prefs: *id001
  type: TYPE_NORMAL
- en: 'Henry Aspegren; '
  id: totrans-split-192
  prefs: *id001
  type: TYPE_NORMAL
- en: 'Hu Xu; '
  id: totrans-split-193
  prefs: *id001
  type: TYPE_NORMAL
- en: 'Hugo Touvron; '
  id: totrans-split-194
  prefs: *id001
  type: TYPE_NORMAL
- en: 'Iga Kozlowska; '
  id: totrans-split-195
  prefs: *id001
  type: TYPE_NORMAL
- en: 'Igor Molybog; '
  id: totrans-split-196
  prefs: *id001
  type: TYPE_NORMAL
- en: 'Igor Tufanov; '
  id: totrans-split-197
  prefs: *id001
  type: TYPE_NORMAL
- en: 'Iliyan Zarov; '
  id: totrans-split-198
  prefs: *id001
  type: TYPE_NORMAL
- en: 'Imanol Arrieta Ibarra; '
  id: totrans-split-199
  prefs: *id001
  type: TYPE_NORMAL
- en: 'Irina-Elena Veliche; '
  id: totrans-split-200
  prefs: *id001
  type: TYPE_NORMAL
- en: 'Isabel Kloumann; '
  id: totrans-split-201
  prefs: *id001
  type: TYPE_NORMAL
- en: 'Ishan Misra; '
  id: totrans-split-202
  prefs: *id001
  type: TYPE_NORMAL
- en: 'Ivan Evtimov; '
  id: totrans-split-203
  prefs: *id001
  type: TYPE_NORMAL
- en: 'Jade Copet; '
  id: totrans-split-204
  prefs: *id001
  type: TYPE_NORMAL
- en: 'Jake Weissman; '
  id: totrans-split-205
  prefs: *id001
  type: TYPE_NORMAL
- en: 'Jan Geffert; '
  id: totrans-split-206
  prefs: *id001
  type: TYPE_NORMAL
- en: 'Jana Vranes; '
  id: totrans-split-207
  prefs: *id001
  type: TYPE_NORMAL
- en: 'Japhet Asher; '
  id: totrans-split-208
  prefs: *id001
  type: TYPE_NORMAL
- en: 'Jason Park; '
  id: totrans-split-209
  prefs: *id001
  type: TYPE_NORMAL
- en: 'Jay Mahadeokar; '
  id: totrans-split-210
  prefs: *id001
  type: TYPE_NORMAL
- en: 'Jean-Baptiste Gaya; '
  id: totrans-split-211
  prefs: *id001
  type: TYPE_NORMAL
- en: 'Jeet Shah; '
  id: totrans-split-212
  prefs: *id001
  type: TYPE_NORMAL
- en: 'Jelmer van der Linde; '
  id: totrans-split-213
  prefs: *id001
  type: TYPE_NORMAL
- en: 'Jennifer Chan; '
  id: totrans-split-214
  prefs: *id001
  type: TYPE_NORMAL
- en: 'Jenny Hong; '
  id: totrans-split-215
  prefs: *id001
  type: TYPE_NORMAL
- en: 'Jenya Lee; '
  id: totrans-split-216
  prefs: *id001
  type: TYPE_NORMAL
- en: 'Jeremy Fu; '
  id: totrans-split-217
  prefs: *id001
  type: TYPE_NORMAL
- en: 'Jeremy Teboul; '
  id: totrans-split-218
  prefs: *id001
  type: TYPE_NORMAL
- en: 'Jianfeng Chi; '
  id: totrans-split-219
  prefs: *id001
  type: TYPE_NORMAL
- en: 'Jianyu Huang; '
  id: totrans-split-220
  prefs: *id001
  type: TYPE_NORMAL
- en: 'Jie Wang; '
  id: totrans-split-221
  prefs: *id001
  type: TYPE_NORMAL
- en: 'Jiecao Yu; '
  id: totrans-split-222
  prefs: *id001
  type: TYPE_NORMAL
- en: 'Joanna Bitton; '
  id: totrans-split-223
  prefs: *id001
  type: TYPE_NORMAL
- en: 'Joe Spisak; '
  id: totrans-split-224
  prefs: *id001
  type: TYPE_NORMAL
- en: 'Joelle Pineau; '
  id: totrans-split-225
  prefs: *id001
  type: TYPE_NORMAL
- en: 'Jon Carvill; '
  id: totrans-split-226
  prefs: *id001
  type: TYPE_NORMAL
- en: 'Jongsoo Park; '
  id: totrans-split-227
  prefs: *id001
  type: TYPE_NORMAL
- en: 'Joseph Rocca; '
  id: totrans-split-228
  prefs: *id001
  type: TYPE_NORMAL
- en: 'Joshua Johnstun; '
  id: totrans-split-229
  prefs: *id001
  type: TYPE_NORMAL
- en: 'Junteng Jia; '
  id: totrans-split-230
  prefs: *id001
  type: TYPE_NORMAL
- en: 'Kalyan Vasuden Alwala; '
  id: totrans-split-231
  prefs: *id001
  type: TYPE_NORMAL
- en: 'Kam Hou U; '
  id: totrans-split-232
  prefs: *id001
  type: TYPE_NORMAL
- en: 'Kate Plawiak; '
  id: totrans-split-233
  prefs: *id001
  type: TYPE_NORMAL
- en: 'Kartikeya Upasani; '
  id: totrans-split-234
  prefs: *id001
  type: TYPE_NORMAL
- en: 'Kaushik Veeraraghavan; '
  id: totrans-split-235
  prefs: *id001
  type: TYPE_NORMAL
- en: 'Ke Li; '
  id: totrans-split-236
  prefs: *id001
  type: TYPE_NORMAL
- en: 'Kenneth Heafield; '
  id: totrans-split-237
  prefs: *id001
  type: TYPE_NORMAL
- en: 'Kevin Stone; '
  id: totrans-split-238
  prefs: *id001
  type: TYPE_NORMAL
- en: 'Khalid El-Arini; '
  id: totrans-split-239
  prefs: *id001
  type: TYPE_NORMAL
- en: 'Krithika Iyer; '
  id: totrans-split-240
  prefs: *id001
  type: TYPE_NORMAL
- en: 'Kshitiz Malik; '
  id: totrans-split-241
  prefs: *id001
  type: TYPE_NORMAL
- en: 'Kuenley Chiu; '
  id: totrans-split-242
  prefs: *id001
  type: TYPE_NORMAL
- en: 'Kunal Bhalla; '
  id: totrans-split-243
  prefs: *id001
  type: TYPE_NORMAL
- en: 'Kyle Huang; '
  id: totrans-split-244
  prefs: *id001
  type: TYPE_NORMAL
- en: 'Lakshya Garg; '
  id: totrans-split-245
  prefs: *id001
  type: TYPE_NORMAL
- en: 'Lauren Rantala-Yeary; '
  id: totrans-split-246
  prefs: *id001
  type: TYPE_NORMAL
- en: 'Laurens van der Maaten; '
  id: totrans-split-247
  prefs: *id001
  type: TYPE_NORMAL
- en: 'Lawrence Chen; '
  id: totrans-split-248
  prefs: *id001
  type: TYPE_NORMAL
- en: 'Leandro Silva; '
  id: totrans-split-249
  prefs: *id001
  type: TYPE_NORMAL
- en: 'Lee Bell; '
  id: totrans-split-250
  prefs: *id001
  type: TYPE_NORMAL
- en: 'Lei Zhang; '
  id: totrans-split-251
  prefs: *id001
  type: TYPE_NORMAL
- en: 'Liang Tan; '
  id: totrans-split-252
  prefs: *id001
  type: TYPE_NORMAL
- en: 'Louis Martin; '
  id: totrans-split-253
  prefs: *id001
  type: TYPE_NORMAL
- en: 'Lovish Madaan; '
  id: totrans-split-254
  prefs: *id001
  type: TYPE_NORMAL
- en: 'Luca Wehrstedt; '
  id: totrans-split-255
  prefs: *id001
  type: TYPE_NORMAL
- en: 'Lukas Blecher; '
  id: totrans-split-256
  prefs: *id001
  type: TYPE_NORMAL
- en: 'Luke de Oliveira; '
  id: totrans-split-257
  prefs: *id001
  type: TYPE_NORMAL
- en: 'Madeline Muzzi; '
  id: totrans-split-258
  prefs: *id001
  type: TYPE_NORMAL
- en: 'Madian Khabsa; '
  id: totrans-split-259
  prefs: *id001
  type: TYPE_NORMAL
- en: 'Manav Avlani; '
  id: totrans-split-260
  prefs: *id001
  type: TYPE_NORMAL
- en: 'Mannat Singh; '
  id: totrans-split-261
  prefs: *id001
  type: TYPE_NORMAL
- en: 'Manohar Paluri; '
  id: totrans-split-262
  prefs: *id001
  type: TYPE_NORMAL
- en: 'Mark Zuckerberg; '
  id: totrans-split-263
  prefs: *id001
  type: TYPE_NORMAL
- en: 'Marcin Kardas; '
  id: totrans-split-264
  prefs: *id001
  type: TYPE_NORMAL
- en: 'Martynas Mankus; '
  id: totrans-split-265
  prefs: *id001
  type: TYPE_NORMAL
- en: 'Mathew Oldham; '
  id: totrans-split-266
  prefs: *id001
  type: TYPE_NORMAL
- en: 'Mathieu Rita; '
  id: totrans-split-267
  prefs: *id001
  type: TYPE_NORMAL
- en: 'Matthew Lennie; '
  id: totrans-split-268
  prefs: *id001
  type: TYPE_NORMAL
- en: 'Maya Pavlova; '
  id: totrans-split-269
  prefs: *id001
  type: TYPE_NORMAL
- en: 'Meghan Keneally; '
  id: totrans-split-270
  prefs: *id001
  type: TYPE_NORMAL
- en: 'Melanie Kambadur; '
  id: totrans-split-271
  prefs: *id001
  type: TYPE_NORMAL
- en: 'Mihir Patel; '
  id: totrans-split-272
  prefs: *id001
  type: TYPE_NORMAL
- en: 'Mikayel Samvelyan; '
  id: totrans-split-273
  prefs: *id001
  type: TYPE_NORMAL
- en: 'Mike Clark; '
  id: totrans-split-274
  prefs: *id001
  type: TYPE_NORMAL
- en: 'Mike Lewis; '
  id: totrans-split-275
  prefs: *id001
  type: TYPE_NORMAL
- en: 'Min Si; '
  id: totrans-split-276
  prefs: *id001
  type: TYPE_NORMAL
- en: 'Mitesh Kumar Singh; '
  id: totrans-split-277
  prefs: *id001
  type: TYPE_NORMAL
- en: 'Mo Metanat; '
  id: totrans-split-278
  prefs: *id001
  type: TYPE_NORMAL
- en: 'Mona Hassan; '
  id: totrans-split-279
  prefs: *id001
  type: TYPE_NORMAL
- en: 'Naman Goyal; '
  id: totrans-split-280
  prefs: *id001
  type: TYPE_NORMAL
- en: 'Narjes Torabi; '
  id: totrans-split-281
  prefs: *id001
  type: TYPE_NORMAL
- en: 'Nicolas Usunier; '
  id: totrans-split-282
  prefs: *id001
  type: TYPE_NORMAL
- en: 'Nikolay Bashlykov; '
  id: totrans-split-283
  prefs: *id001
  type: TYPE_NORMAL
- en: 'Nikolay Bogoychev; '
  id: totrans-split-284
  prefs: *id001
  type: TYPE_NORMAL
- en: 'Niladri Chatterji; '
  id: totrans-split-285
  prefs: *id001
  type: TYPE_NORMAL
- en: 'Ning Dong; '
  id: totrans-split-286
  prefs: *id001
  type: TYPE_NORMAL
- en: 'Oliver Aobo Yang; '
  id: totrans-split-287
  prefs: *id001
  type: TYPE_NORMAL
- en: 'Olivier Duchenne; '
  id: totrans-split-288
  prefs: *id001
  type: TYPE_NORMAL
- en: 'Onur Celebi; '
  id: totrans-split-289
  prefs: *id001
  type: TYPE_NORMAL
- en: 'Parth Parekh; '
  id: totrans-split-290
  prefs: *id001
  type: TYPE_NORMAL
- en: 'Patrick Alrassy; '
  id: totrans-split-291
  prefs: *id001
  type: TYPE_NORMAL
- en: 'Paul Saab; '
  id: totrans-split-292
  prefs: *id001
  type: TYPE_NORMAL
- en: 'Pavan Balaji; '
  id: totrans-split-293
  prefs: *id001
  type: TYPE_NORMAL
- en: 'Pedro Rittner; '
  id: totrans-split-294
  prefs: *id001
  type: TYPE_NORMAL
- en: 'Pengchuan Zhang; '
  id: totrans-split-295
  prefs: *id001
  type: TYPE_NORMAL
- en: 'Pengwei Li; '
  id: totrans-split-296
  prefs: *id001
  type: TYPE_NORMAL
- en: 'Petar Vasic; '
  id: totrans-split-297
  prefs: *id001
  type: TYPE_NORMAL
- en: 'Peter Weng; '
  id: totrans-split-298
  prefs: *id001
  type: TYPE_NORMAL
- en: 'Polina Zvyagina; '
  id: totrans-split-299
  prefs: *id001
  type: TYPE_NORMAL
- en: 'Prajjwal Bhargava; '
  id: totrans-split-300
  prefs: *id001
  type: TYPE_NORMAL
- en: 'Pratik Dubal; '
  id: totrans-split-301
  prefs: *id001
  type: TYPE_NORMAL
- en: 'Praveen Krishnan; '
  id: totrans-split-302
  prefs: *id001
  type: TYPE_NORMAL
- en: 'Punit Singh Koura; '
  id: totrans-split-303
  prefs: *id001
  type: TYPE_NORMAL
- en: 'Puxin Xu; '
  id: totrans-split-304
  prefs: *id001
  type: TYPE_NORMAL
- en: 'Qing He; '
  id: totrans-split-305
  prefs: *id001
  type: TYPE_NORMAL
- en: 'Rachel Rodriguez; '
  id: totrans-split-306
  prefs: *id001
  type: TYPE_NORMAL
- en: 'Ragavan Srinivasan; '
  id: totrans-split-307
  prefs: *id001
  type: TYPE_NORMAL
- en: 'Rahul Mitra; '
  id: totrans-split-308
  prefs: *id001
  type: TYPE_NORMAL
- en: 'Ramon Calderer; '
  id: totrans-split-309
  prefs: *id001
  type: TYPE_NORMAL
- en: 'Raymond Li; '
  id: totrans-split-310
  prefs: *id001
  type: TYPE_NORMAL
- en: 'Robert Stojnic; '
  id: totrans-split-311
  prefs: *id001
  type: TYPE_NORMAL
- en: 'Roberta Raileanu; '
  id: totrans-split-312
  prefs: *id001
  type: TYPE_NORMAL
- en: 'Robin Battey; '
  id: totrans-split-313
  prefs: *id001
  type: TYPE_NORMAL
- en: 'Rocky Wang; '
  id: totrans-split-314
  prefs: *id001
  type: TYPE_NORMAL
- en: 'Rohit Girdhar; '
  id: totrans-split-315
  prefs: *id001
  type: TYPE_NORMAL
- en: 'Rohit Patel; '
  id: totrans-split-316
  prefs: *id001
  type: TYPE_NORMAL
- en: 'Romain Sauvestre; '
  id: totrans-split-317
  prefs: *id001
  type: TYPE_NORMAL
- en: 'Ronnie Polidoro; '
  id: totrans-split-318
  prefs: *id001
  type: TYPE_NORMAL
- en: 'Roshan Sumbaly; '
  id: totrans-split-319
  prefs: *id001
  type: TYPE_NORMAL
- en: 'Ross Taylor; '
  id: totrans-split-320
  prefs: *id001
  type: TYPE_NORMAL
- en: 'Ruan Silva; '
  id: totrans-split-321
  prefs: *id001
  type: TYPE_NORMAL
- en: 'Rui Hou; '
  id: totrans-split-322
  prefs: *id001
  type: TYPE_NORMAL
- en: 'Rui Wang; '
  id: totrans-split-323
  prefs: *id001
  type: TYPE_NORMAL
- en: 'Russ Howes; '
  id: totrans-split-324
  prefs: *id001
  type: TYPE_NORMAL
- en: 'Ruty Rinott; '
  id: totrans-split-325
  prefs: *id001
  type: TYPE_NORMAL
- en: 'Saghar Hosseini; '
  id: totrans-split-326
  prefs: *id001
  type: TYPE_NORMAL
- en: 'Sai Jayesh Bondu; '
  id: totrans-split-327
  prefs: *id001
  type: TYPE_NORMAL
- en: 'Samyak Datta; '
  id: totrans-split-328
  prefs: *id001
  type: TYPE_NORMAL
- en: 'Sanjay Singh; '
  id: totrans-split-329
  prefs: *id001
  type: TYPE_NORMAL
- en: 'Sara Chugh; '
  id: totrans-split-330
  prefs: *id001
  type: TYPE_NORMAL
- en: 'Sargun Dhillon; '
  id: totrans-split-331
  prefs: *id001
  type: TYPE_NORMAL
- en: 'Satadru Pan; '
  id: totrans-split-332
  prefs: *id001
  type: TYPE_NORMAL
- en: 'Sean Bell; '
  id: totrans-split-333
  prefs: *id001
  type: TYPE_NORMAL
- en: 'Sergey Edunov; '
  id: totrans-split-334
  prefs: *id001
  type: TYPE_NORMAL
- en: 'Shaoliang Nie; '
  id: totrans-split-335
  prefs: *id001
  type: TYPE_NORMAL
- en: 'Sharan Narang; '
  id: totrans-split-336
  prefs: *id001
  type: TYPE_NORMAL
- en: 'Sharath Raparthy; '
  id: totrans-split-337
  prefs: *id001
  type: TYPE_NORMAL
- en: 'Shaun Lindsay; '
  id: totrans-split-338
  prefs: *id001
  type: TYPE_NORMAL
- en: 'Sheng Feng; '
  id: totrans-split-339
  prefs: *id001
  type: TYPE_NORMAL
- en: 'Sheng Shen; '
  id: totrans-split-340
  prefs: *id001
  type: TYPE_NORMAL
- en: 'Shenghao Lin; '
  id: totrans-split-341
  prefs: *id001
  type: TYPE_NORMAL
- en: 'Shiva Shankar; '
  id: totrans-split-342
  prefs: *id001
  type: TYPE_NORMAL
- en: 'Shruti Bhosale; '
  id: totrans-split-343
  prefs: *id001
  type: TYPE_NORMAL
- en: 'Shun Zhang; '
  id: totrans-split-344
  prefs: *id001
  type: TYPE_NORMAL
- en: 'Simon Vandenhende; '
  id: totrans-split-345
  prefs: *id001
  type: TYPE_NORMAL
- en: 'Sinong Wang; '
  id: totrans-split-346
  prefs: *id001
  type: TYPE_NORMAL
- en: 'Seohyun Sonia Kim; '
  id: totrans-split-347
  prefs: *id001
  type: TYPE_NORMAL
- en: 'Soumya Batra; '
  id: totrans-split-348
  prefs: *id001
  type: TYPE_NORMAL
- en: 'Sten Sootla; '
  id: totrans-split-349
  prefs: *id001
  type: TYPE_NORMAL
- en: 'Steve Kehoe; '
  id: totrans-split-350
  prefs: *id001
  type: TYPE_NORMAL
- en: 'Suchin Gururangan; '
  id: totrans-split-351
  prefs: *id001
  type: TYPE_NORMAL
- en: 'Sumit Gupta; '
  id: totrans-split-352
  prefs: *id001
  type: TYPE_NORMAL
- en: 'Sunny Virk; '
  id: totrans-split-353
  prefs: *id001
  type: TYPE_NORMAL
- en: 'Sydney Borodinsky; '
  id: totrans-split-354
  prefs: *id001
  type: TYPE_NORMAL
- en: 'Tamar Glaser; '
  id: totrans-split-355
  prefs: *id001
  type: TYPE_NORMAL
- en: 'Tamar Herman; '
  id: totrans-split-356
  prefs: *id001
  type: TYPE_NORMAL
- en: 'Tamara Best; '
  id: totrans-split-357
  prefs: *id001
  type: TYPE_NORMAL
- en: 'Tara Fowler; '
  id: totrans-split-358
  prefs: *id001
  type: TYPE_NORMAL
- en: 'Thomas Georgiou; '
  id: totrans-split-359
  prefs: *id001
  type: TYPE_NORMAL
- en: 'Thomas Scialom; '
  id: totrans-split-360
  prefs: *id001
  type: TYPE_NORMAL
- en: 'Tianhe Li; '
  id: totrans-split-361
  prefs: *id001
  type: TYPE_NORMAL
- en: 'Todor Mihaylov; '
  id: totrans-split-362
  prefs: *id001
  type: TYPE_NORMAL
- en: 'Tong Xiao; '
  id: totrans-split-363
  prefs: *id001
  type: TYPE_NORMAL
- en: 'Ujjwal Karn; '
  id: totrans-split-364
  prefs: *id001
  type: TYPE_NORMAL
- en: 'Vedanuj Goswami; '
  id: totrans-split-365
  prefs: *id001
  type: TYPE_NORMAL
- en: 'Vibhor Gupta; '
  id: totrans-split-366
  prefs: *id001
  type: TYPE_NORMAL
- en: 'Vignesh Ramanathan; '
  id: totrans-split-367
  prefs: *id001
  type: TYPE_NORMAL
- en: 'Viktor Kerkez; '
  id: totrans-split-368
  prefs: *id001
  type: TYPE_NORMAL
- en: 'Vinay Satish Kumar; '
  id: totrans-split-369
  prefs: *id001
  type: TYPE_NORMAL
- en: 'Vincent Gonguet; '
  id: totrans-split-370
  prefs: *id001
  type: TYPE_NORMAL
- en: 'Vish Vogeti; '
  id: totrans-split-371
  prefs: *id001
  type: TYPE_NORMAL
- en: 'Vlad Poenaru; '
  id: totrans-split-372
  prefs: *id001
  type: TYPE_NORMAL
- en: 'Vlad Tiberiu Mihailescu; '
  id: totrans-split-373
  prefs: *id001
  type: TYPE_NORMAL
- en: 'Vladan Petrovic; '
  id: totrans-split-374
  prefs: *id001
  type: TYPE_NORMAL
- en: 'Vladimir Ivanov; '
  id: totrans-split-375
  prefs: *id001
  type: TYPE_NORMAL
- en: 'Wei Li; '
  id: totrans-split-376
  prefs: *id001
  type: TYPE_NORMAL
- en: 'Weiwei Chu; '
  id: totrans-split-377
  prefs: *id001
  type: TYPE_NORMAL
- en: 'Wenhan Xiong; '
  id: totrans-split-378
  prefs: *id001
  type: TYPE_NORMAL
- en: 'Wenyin Fu; '
  id: totrans-split-379
  prefs: *id001
  type: TYPE_NORMAL
- en: 'Wes Bouaziz; '
  id: totrans-split-380
  prefs: *id001
  type: TYPE_NORMAL
- en: 'Whitney Meers; '
  id: totrans-split-381
  prefs: *id001
  type: TYPE_NORMAL
- en: 'Will Constable; '
  id: totrans-split-382
  prefs: *id001
  type: TYPE_NORMAL
- en: 'Xavier Martinet; '
  id: totrans-split-383
  prefs: *id001
  type: TYPE_NORMAL
- en: 'Xiaojian Wu; '
  id: totrans-split-384
  prefs: *id001
  type: TYPE_NORMAL
- en: 'Xinbo Gao; '
  id: totrans-split-385
  prefs: *id001
  type: TYPE_NORMAL
- en: 'Xinfeng Xie; '
  id: totrans-split-386
  prefs: *id001
  type: TYPE_NORMAL
- en: 'Xuchao Jia; '
  id: totrans-split-387
  prefs: *id001
  type: TYPE_NORMAL
- en: 'Yaelle Goldschlag; '
  id: totrans-split-388
  prefs: *id001
  type: TYPE_NORMAL
- en: 'Yann LeCun; '
  id: totrans-split-389
  prefs: *id001
  type: TYPE_NORMAL
- en: 'Yashesh Gaur; '
  id: totrans-split-390
  prefs: *id001
  type: TYPE_NORMAL
- en: 'Yasmine Babaei; '
  id: totrans-split-391
  prefs: *id001
  type: TYPE_NORMAL
- en: 'Ye Qi; '
  id: totrans-split-392
  prefs: *id001
  type: TYPE_NORMAL
- en: 'Yenda Li; '
  id: totrans-split-393
  prefs: *id001
  type: TYPE_NORMAL
- en: 'Yi Wen; '
  id: totrans-split-394
  prefs: *id001
  type: TYPE_NORMAL
- en: 'Yiwen Song; '
  id: totrans-split-395
  prefs: *id001
  type: TYPE_NORMAL
- en: 'Youngjin Nam; '
  id: totrans-split-396
  prefs: *id001
  type: TYPE_NORMAL
- en: 'Yuchen Hao; '
  id: totrans-split-397
  prefs: *id001
  type: TYPE_NORMAL
- en: 'Yuchen Zhang; '
  id: totrans-split-398
  prefs: *id001
  type: TYPE_NORMAL
- en: 'Yun Wang; '
  id: totrans-split-399
  prefs: *id001
  type: TYPE_NORMAL
- en: 'Yuning Mao; '
  id: totrans-split-400
  prefs: *id001
  type: TYPE_NORMAL
- en: 'Yuzi He; '
  id: totrans-split-401
  prefs: *id001
  type: TYPE_NORMAL
- en: 'Zacharie Delpierre Coudert; '
  id: totrans-split-402
  prefs: *id001
  type: TYPE_NORMAL
- en: 'Zachary DeVito; '
  id: totrans-split-403
  prefs: *id001
  type: TYPE_NORMAL
- en: 'Zahra Hankir; '
  id: totrans-split-404
  prefs: *id001
  type: TYPE_NORMAL
- en: 'Zhaoduo Wen; '
  id: totrans-split-405
  prefs: *id001
  type: TYPE_NORMAL
- en: 'Zheng Yan; '
  id: totrans-split-406
  prefs: *id001
  type: TYPE_NORMAL
- en: 'Zhengxing Chen; '
  id: totrans-split-407
  prefs: *id001
  type: TYPE_NORMAL
- en: 'Zhenyu Yang; '
  id: totrans-split-408
  prefs: *id001
  type: TYPE_NORMAL
- en: Zoe Papakipos
  id: totrans-split-409
  prefs: *id001
  type: TYPE_NORMAL
