["```\nnavigator.mediaDevices.getUserMedia({ audio: true }).then((stream) => {\n  this.stream = stream;\n  const analyser = this.audioContext.createAnalyser();\n  const microphone = this.audioContext.createMediaStreamSource(this.stream);\n\n  microphone.connect(analyser);\n\n  analyser.fftSize = 512;\n  const bufferLength = analyser.frequencyBinCount;\n  const dataArray = new Uint8Array(bufferLength);\n\n  const checkAudio = () => {\n    analyser.getByteTimeDomainData(dataArray);\n\n    let sum = 0;\n    for (let i = 0; i < bufferLength; i++) {\n      sum += (dataArray[i] - 128) * (dataArray[i] - 128);\n    }\n    const volume = Math.sqrt(sum / bufferLength);\n\n    if (volume > VOLUME_THRESHOLD && !this.isRecording) {\n      this.startRecording();\n    } else if (this.isRecording()) {\n      this.stopRecording();\n    }\n\n    if (this.isMonitoring) {\n      requestAnimationFrame(checkAudio);\n    }\n  }\n\n  checkAudio();\n})\n\n```", "```\nnavigator.mediaDevices.getUserMedia({ audio: true }).then((stream) => {\n  this.stream = stream;\n  const analyser = this.audioContext.createAnalyser();\n  const microphone = this.audioContext.createMediaStreamSource(this.stream);\n\n  microphone.connect(analyser);\n\n  analyser.fftSize = 512;\n  const bufferLength = analyser.frequencyBinCount;\n  const dataArray = new Uint8Array(bufferLength);\n\n  this.silenceTimeout = null;\n\n  const checkAudio = () => {\n    analyser.getByteTimeDomainData(dataArray);\n\n    let sum = 0;\n    for (let i = 0; i < bufferLength; i++) {\n      sum += (dataArray[i] - 128) * (dataArray[i] - 128);\n    }\n    const volume = Math.sqrt(sum / bufferLength);\n\n    if (volume > VOLUME_THRESHOLD) {\n      if (!this.isRecording()) {\n        this.startRecording();\n      }\n\n      clearTimeout(this.silenceTimeout);\n\n      this.silenceTimeout = setTimeout(() => {\n        if (this.isRecording()) {\n          this.stopRecording();\n        }\n      }, SILENCE_TIMEOUT);\n    }\n\n    if (this.isMonitoring) {\n      requestAnimationFrame(checkAudio);\n    }\n  }\n\n  checkAudio();\n})\n\n```", "```\n  defp handle_progress(:audio, entry, socket) when entry.done? do\n    binary =\n      consume_uploaded_entry(socket, entry, fn %{path: path} ->\n        {:ok, File.read!(path)}\n      end)\n\n    audio = Nx.from_binary(binary, :f32)\n\n    socket =\n      start_async(socket, :transcription, fn ->\n        Nero.SpeechToText.transcribe(audio)\n      end)\n\n    {:noreply, socket}\n  end\n\n```", "```\ndefmodule Nero.SpeechToText do\n  @repo \"distil-whisper/distil-medium.en\"\n\n  def serving() do\n    {:ok, model_info} = Bumblebee.load_model({:hf, @repo})\n\n    {:ok, featurizer} = Bumblebee.load_featurizer({:hf, @repo})\n    {:ok, tokenizer} = Bumblebee.load_tokenizer({:hf, @repo})\n    {:ok, generation_config} = Bumblebee.load_generation_config({:hf, @repo})\n\n    Bumblebee.Audio.speech_to_text_whisper(model_info, featurizer, tokenizer, generation_config,\n      task: nil,\n      compile: [batch_size: 1],\n      defn_options: [compiler: EXLA]\n    )\n  end\n\n  def transcribe(audio) do\n    output = Nx.Serving.batched_run(__MODULE__, audio)\n    output.chunks |> Enum.map_join(& &1.text) |> String.trim()\n  end\nend\n\n```", "```\n  def handle_async(:transcription, {:ok, transcription}, socket) do\n    chat = socket.assigns.chat ++ [%{role: \"user\", content: transcription}]\n    response = Nero.Agent.respond(chat)\n\n    {:noreply,\n     socket\n     |> assign(chat: chat)\n     |> speak(response)}\n  end\n\n```", "```\n  def respond(chat) do\n    prompt = Nero.Prompts.response()\n\n    response_stream =\n      OpenAI.chat_completion(\n        model: \"gpt-3.5-turbo\",\n        messages: [%{role: \"system\", content: prompt} | chat],\n        max_tokens: 400,\n        stream: true\n      )\n\n    response_stream\n    |> Stream.map(&get_in(&1, [\"choices\", Access.at(0), \"delta\", \"content\"]))\n    |> Stream.reject(&is_nil/1)\n  end\n\n```", "```\n  defp speak(socket, text) do\n    start_async(socket, :speak, fn ->\n      Nero.TextToSpeech.stream(text)\n    end)\n  end\n\n```", "```\ndef handle_async(:speak, {:ok, response}, socket) do\n  chat = socket.assigns.chat ++ [%{role: \"assistant\", content: response}]\n  {:noreply, assign(socket, :chat, chat)}\nend\n\n```", "```\n  def hang_up?(chat) do\n    {:ok, %{hang_up: hang_up}} =\n      Instructor.chat_completion(\n        model: \"gpt-3.5-turbo\",\n        messages: [\n          %{\n            role: \"system\",\n            content:\n              \"Decide whether or not to hang up the phone given this transcript. You should hang up after the user says goodbye or that there's nothing else you can help them with. DO NOT HANG UP ON THE USER UNLESS THEY SAY GOODBYE.\"\n          }\n          | chat\n        ],\n        response_model: %{hang_up: :boolean}\n      )\n\n    hang_up\n  end\n\n```", "```\n  def handle_async(:speak, {:ok, response}, socket) do\n    chat = socket.assigns.chat ++ [%{role: \"assistant\", content: response}]\n\n    socket =\n      if Nero.Agent.hang_up?(chat) do\n        push_event(socket, \"hang_up\", %{})\n      else\n        socket\n      end\n\n    {:noreply, assign(socket, :chat, chat)}\n  end\n\n```", "```\n    this.handleEvent('hang_up', () => {\n      hook.pushEvent(\"toggle_conversation\");\n\n      if (this.audioContext) {\n        this.audioContext.close();\n        this.audioContext = null;\n      }\n\n      if (this.isMonitoring) {\n        this.stopMonitoring();\n      }\n    });\n\n```", "```\n  def handle_event(\"toggle_conversation\", _params, socket) do\n    socket =\n      if socket.assigns.conversing do\n        stop_conversation(socket)\n      else\n        start_conversation(socket)\n      end\n\n    {:noreply, socket}\n  end\n\n```", "```\n  defmodule Appointment do\n    use Ecto.Schema\n\n    embedded_schema do\n      field :booked, :boolean\n      field :date, :string\n      field :time, :string\n      field :name, :string\n      field :phone_number, :string\n      field :reason_for_visit, :string\n    end\n  end\n\n```", "```\n  def extract_appointment(chat) do\n    Instructor.chat_completion(\n      model: \"gpt-3.5-turbo\",\n      messages: [\n        %{\n          role: \"system\",\n          content:\n            \"Extract appointemnt information from the transcript. If info is missing, leave it blank. If it seems like no appointment was booked, mark booked as false and leave everything else blank. An appointment is not booked if there's no established date.\"\n        }\n        | chat\n      ],\n      response_model: Appointment\n    )\n  end\n\n```", "```\ndefp stop_conversation(socket) do\n  case Nero.Agent.extract_appointment(socket.assigns.chat) do\n    {:ok, %{booked: true} = appointment} ->\n      assign(socket,\n        message: \"You made an appointment! Details:\",\n        appointment: appointment,\n        conversing: false\n      )\n\n    _ ->\n      assign(socket,\n        message: \"Looks like you didn't actually book an appointment. Try again\",\n        conversing: false\n      )\n  end\nend\n\n```", "```\nconst audioOptions = {\n  sampleRate: SAMPLING_RATE,\n  echoCancellation: true,\n  noiseSuppression: true,\n  autoGainControl: true,\n  channelCount: 1,\n};\n\nnavigator.mediaDevices.getUserMedia({ audio: audioOptions }).then((stream) => {\n  ...\n}\n\n```", "```\nlet sum = 0;\nfor (let i = 0; i < bufferLength; i++) {\n  sum += (dataArray[i] - 128) * (dataArray[i] - 128);\n}\nconst volume = Math.sqrt(sum / bufferLength);\nconst smoothedVolume = SMOOTHING_ALPHA * volume + (1 - SMOOTHING_ALPHA) * lastSmoothedVolume;\n\nlastSmoothedVolume = smoothedVolume;\n\n```", "```\nthis.stream = stream;\nconst analyser = this.audioContext.createAnalyser();\nconst microphone = this.audioContext.createMediaStreamSource(this.stream);\n\nvar highPassFilter = this.audioContext.createBiquadFilter();\nhighPassFilter.type = 'highpass';\nhighPassFilter.frequency.value = FILTER_LOWER_BOUND;\n\nvar lowPassFilter = this.audioContext.createBiquadFilter();\nlowPassFilter.type = 'lowpass';\nlowPassFilter.frequency.value = FILTER_UPPER_BOUND;\n\nmicrophone.connect(highPassFilter);\nhighPassFilter.connect(lowPassFilter);\nlowPassFilter.connect(analyser);\n\n```", "```\nif (smoothedVolume > VOLUME_THRESHOLD) {\n  if (!this.isRecording()) {\n    // To handle interrupts, push an event to the LV which will\n    // then push an event to the TTS channel. Not sure how much\n    // these round trips will lag. Alternatively we could create\n    // a global audio context and stop that, but we would need\n    // a different way to push alignment info to the server\n    this.pushEvent(\"interrupt\");\n    this.startRecording();\n  }\n  ...\n}\n\n```", "```\n  def handle_event(\"interrupt\", _params, socket) do\n    NeroWeb.Endpoint.broadcast_from(\n      self(),\n      socket.assigns.audio_channel,\n      \"phx:audio-stop\",\n      %{}\n    )\n\n    {:noreply, socket}\n  end\n\n```", "```\n  this.channel.on(\"phx:audio-stop\", () => {\n    if (hook.audioContext.state === 'running') {\n      hook.audioContext.suspend().then(() => {\n        if (hook.source) {\n          hook.source.onended = null;\n          hook.source.stop();\n          hook.source = null;\n        }\n        hook.isPlaying = false;\n        hook.audioQueue = [];\n      });\n    }\n  });\n\n```", "```\n  defp nudge(socket) do\n    nudge_pid = Process.send_after(self(), :nudge, @nudge_ms)\n    assign(socket, :nudge_pid, nudge_pid)    \n  end\n\n```", "```\n  def handle_info(:nudge, socket) do\n    socket =\n      if socket.assigns.nudged? do\n        stop_conversation(socket)\n      else\n        socket\n        |> speak([\"Are \", \"you \", \"still \", \"there? \"])\n        |> assign(nudged?: true)\n      end\n\n    {:noreply, socket}\n  end\n\n```"]