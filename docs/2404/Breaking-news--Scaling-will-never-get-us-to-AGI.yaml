- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-05-27 13:00:56'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: 'Breaking news: Scaling will never get us to AGI'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://garymarcus.substack.com/p/breaking-news-scaling-will-never](https://garymarcus.substack.com/p/breaking-news-scaling-will-never)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Neural networks (at least in the configurations that have been dominant over
    the last three decades) have trouble generalizing beyond the multidimensional
    space that surrounds their training examples. That limits their ability to reason
    and plan reliably. It also drives their greediness with data, and even the ethical
    choices their developers have been making. There will never be enough data; there
    will always be outliers. This is why driverless cars are still just demos, and
    why LLMs will never be reliable.
  prefs: []
  type: TYPE_NORMAL
- en: I have said this so often in so many ways, going back to [1998](https://www.sciencedirect.com/science/article/pii/S0010028598906946),
    that today I am going to let someone else, Chomba Bupe, a sharp-thinking tech
    entrepreneur/computer vision researcher from Zambia, take a shot.
  prefs: []
  type: TYPE_NORMAL
- en: 'An important [new preprint](https://arxiv.org/abs/2404.04125) that just came
    out on data and scaling. Bupe explains it well:'
  prefs: []
  type: TYPE_NORMAL
- en: 'In short, diminishing returns are what we can expect. (Asterisk: current models
    aren’t literally look up tables; they can generalize to some degree, but not enough.
    As I explained in the 1998 paper and in The Algebraic Mind they can generalize
    within a space of training examples but face considerable trouble beyond that
    space.)'
  prefs: []
  type: TYPE_NORMAL
- en: Sooner or later, the exponential greed for data will exceed what is available.
  prefs: []
  type: TYPE_NORMAL
- en: To get to AGI, we need alternative approaches that can generalize better beyond
    the data on which they have been trained. End of story.
  prefs: []
  type: TYPE_NORMAL
- en: '***Gary Marcus*** **looks forward to watching what happens as people begin
    to take in the implications.**'
  prefs: []
  type: TYPE_NORMAL
