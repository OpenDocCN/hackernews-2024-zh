<!--yml

category: 未分类

date: 2024-05-27 13:08:31

-->

# 我是如何进入深度学习的 - Vikas Paruchuri

> 来源：[https://www.vikas.sh/post/how-i-got-into-deep-learning](https://www.vikas.sh/post/how-i-got-into-deep-learning)

我经营了一个教育公司，[Dataquest](https://www.dataquest.io/)，长达8年。去年，我开始有兴趣重新开始建设。深度学习一直很有趣，但我对它知之甚少。我开始解决这个问题。

从那时起，我训练了数十个[模型](https://huggingface.co/vikp)（其中几个是开源领域的最新技术），建立了[2个库](https://github.com/VikParuchuri/)，拥有5千多个Github星标，并最近接受了来自[answer.ai](http://www.answer.ai/)的邀请，这是由[Jeremy Howard](https://twitter.com/jeremyphoward)创立的研究实验室。

我说这些是为了大致勾画我的学习历程。在这篇文章中，我将更详细地讲述我如何学习深度学习。希望对你的学习之旅有所帮助。

我在学校里并没有学习这些内容。本科主修美国历史，[不少课程](https://www.vikas.sh/post/i-barely-graduated-college)都挂了。

我在2012年做了机器学习和Python工作，但是说服自己深度学习对我来说太复杂了。其中一个原因是我通过参加Kaggle竞赛来学习。Kaggle竞赛确实很适合快速学习，但可能会导致你在基础知识上留下空白，比如算法的数学原理。

当深度学习开始流行时，数学内容很多，我觉得自己永远无法理解它。当然，这是错误的，10年后我证明了这一点，但你对待问题的角度决定了一切。第一次我是从顶层开始接触深度学习，只是将模型粘贴在一起，而不了解它们的工作原理。最终我遇到了障碍，无法继续前行。

去年我学习深度学习时，我已经具备了一些有用的技能。首先是扎实的Python编程能力。尽管有些人试图推翻，Python仍然是AI的通用语言。如果你想进入AI领域，首先要[精通编程](https://twitter.com/gdb/status/1729893902814192096)。

无论我身处AI的哪个时代，数据清洗始终占据了我工作的超过70%。如果你在做纯粹的研究或处理玩具问题，可能可以避免与数据打交道，但否则数据处理技能是必不可少的。

还有一种稍微模糊的技能，我称之为实用主义。深度学习有很多兔子洞，从“最完美的基础模型是什么？”到“如果我在这里去掉sigmoid会怎样？”这些兔子洞中有些是有用的，但大多数会耗费大量时间。能够识别何时深入，何时采取快速/简单解决方案非常重要。

这一次，我决定从基础开始自底向上学习。我阅读了[深度学习书籍](https://www.deeplearningbook.org/)。这本书已经有些年头了，但仍然是一个非常好的资源。慢慢地阅读它。很多术语和数学内容可能会让你感到陌生 - 查阅它们。你可能需要草绘一些内容或者编写代码来理解它们 - 给自己足够的空间去做这些。如果数学内容让你感到陌生，一个很好的补充资源是[机器学习的数学](https://mml-book.github.io/book/mml-book.pdf)。虽然我还没有参加它们，但[fast.ai](https://www.fast.ai/)和[Karpathy的视频](https://www.youtube.com/@AndrejKarpathy)质量都很高。

尽管像CNN或RNN这样的架构在一个向全面使用transformers的世界中似乎已经过时，[CNN仍然被广泛使用](https://twitter.com/rasbt/status/1767561783382872194)，而像RNN这样的东西在[旧时代已然是新时代](https://arxiv.org/abs/2402.19427)。

完成书的前两部分后（第三部分可以跳过），你应该能够使用纯粹的numpy编写任何主要的神经网络架构（前向和反向传播）。

真正帮助你达到这一点的一件事是在学习过程中教授这些技能。我在阅读深度学习书籍的同时开始组织一个课程，[从零到GPT](https://github.com/VikParuchuri/zero_to_gpt)。教学是巩固概念的终极方式，我发现自己陷入了一个良好的学习循环：学习、查阅/草绘我不理解的内容，然后教授它们。

这本书会带你走进2015年左右的深度学习。阅读完这本书后，我阅读了2015-2022年间一些基础的深度学习论文，并在PyTorch中实现了它们。你可以使用[Google Colab](https://colab.research.google.com/)免费/廉价的GPU，并使用[Weights and Biases](https://wandb.ai/)来追踪你的训练运行。

一个非全面的列表是：

在此之后，你应该能够理解大多数关于深度学习模型架构的对话。

如今训练模型的最简单入口是对基础模型进行微调。[Huggingface transformers](https://github.com/huggingface/transformers)非常适合微调，因为它已经实现了许多模型，并使用PyTorch。

有一些Discord社区，比如[Nous Research](https://discord.gg/HhC3avAG)和[EleutherAI](https://discord.gg/BVwwHaeV)，人们在那里讨论最新的模型和论文。我建议你加入其中，了解当前的最新技术，并尝试一些微调。

进行微调的最简单方法是选择一个小模型（7B或更少的参数），并尝试使用LoRA进行微调。你可以使用Google Colab，或者像[Lambda Labs](https://lambdalabs.com/)这样的服务，如果需要更多的VRAM或多个GPU。

我想训练模型以编写更好的代码，所以我收集了数据集，并在来自StackOverflow和其他地方的数据上微调了几个不同的基础模型。这确实帮助我理解了模型架构、数据、计算和输出之间的联系。然而，微调是一个非常竞争激烈的领域，当前技术每天都在变化，要产生影响是很困难的。

当我在进行微调时，我意识到一些最高质量的数据是以教科书的形式存在并锁定在pdf中。我试图解决这个问题的一种方式是生成[合成数据](https://github.com/VikParuchuri/textbook_quality)。

另一种方法是从pdf中提取数据并将其转换为良好的训练数据（Markdown）。有一种叫做[nougat](https://arxiv.org/abs/2308.13418)的方法在许多情况下效果很好，但运行速度慢且成本高昂。我决定看看是否可以通过利用pdf中已有的数据（避免OCR），仅在需要时使用模型来构建更好的东西。我将几种不同的模型串联在一起，并在其中使用了启发式算法。[这种方法](https://github.com/VikParuchuri/marker)，marker，比nougat快10倍，适用于任何语言，并且通常更精确。

进行marker的工作让我想要解决更多的问题，我还训练了一个将方程式转换为LaTeX的[model](https://github.com/VikParuchuri/texify)，一个文本检测模型，一个[OCR模型](https://github.com/VikParuchuri/surya)，它与Google Cloud竞争力相当，并且一个布局模型。

对于所有这些模型，我采用了现有的架构，修改了层、损失和其他元素，然后生成/找到了合适的数据集。例如，对于OCR模型，我从[Donut架构](https://arxiv.org/abs/2111.15664)开始，加入了GQA、MoE层、UTF-16解码（任何字符1-2个令牌），并改变了一些模型形状。

由于OCR模型通常很小（参数少于300M），我能够在4x A6000上训练所有这些模型。如果我更高效一些，可能只需要2x A6000。

希望这为你阐明了三件事情：

+   理解基础原理对于训练优秀模型至关重要。

+   找到有趣的问题解决方案是利用你所构建的东西产生影响的最佳途径。

+   你不需要很多的GPU。

在AI中有许多小众领域，即使作为一个相对外行者也能产生重大影响。

正如你可能注意到的，我开源了我所有的AI项目。与影响相比，数据堆栈是AI领域中投资相对不足的一个领域。我坚信，你能够广泛分发高质量的训练数据，这样1-2家组织垄断好模型的风险就会降低。

开源还有一个曝光度良好的副作用。这让我想到了我的故事的最后一部分。

我本来考虑围绕我的开源工具建立一家企业。工作在我当时的视野中根本没有。但当Jeremy联系我关于answer.ai时，我觉得这是一个我必须抓住的机会。与才华横溢的人一起工作，产生积极影响并且学到很多东西，这样的机会实在难以放过。

我的开源工作直接导致了这次工作机会，既在显而易见的方式（它让我有了曝光），也在更微妙的方式（显著提高了我的技能）。希望当你学习时，你也能开源一些东西。

我怀疑我在answer.ai的工作会与我的开源工作非常相似。我将继续训练模型，改进数据堆栈，并且公开发布。

如果你正试图进入深度学习领域，希望这篇文章对你有所帮助。如果不是，请希望它能有些许娱乐性（你读到了结尾，所以它可能确实有趣，对吧？）。

至于我，我现在正在回到训练一些模型（目前正在观察其中的两个收敛）。
