- en: <!--yml
  id: totrans-split-0
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  id: totrans-split-1
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-05-27 13:06:33'
  id: totrans-split-2
  prefs: []
  type: TYPE_NORMAL
- en: -->
  id: totrans-split-3
  prefs: []
  type: TYPE_NORMAL
- en: Mixtral 8x22b latest-model benchmarks - Promptzone
  id: totrans-split-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://www.promptzone.com/promptzone/mixtral-8x22b-latest-model-benchmarks-4mh9](https://www.promptzone.com/promptzone/mixtral-8x22b-latest-model-benchmarks-4mh9)
  id: totrans-split-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Mistral AI, after OpenAI and Google, has quietly entered the race of Large
    Language Models by releasing one of its most potent models to date: the Mixtral
    8x22B.'
  id: totrans-split-6
  prefs: []
  type: TYPE_NORMAL
- en: '**Highlights:**'
  id: totrans-split-7
  prefs: []
  type: TYPE_NORMAL
- en: French startup Mistral AI has launched Mixtral 8x22B, its latest open-source
    LLM.
  id: totrans-split-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This model employs a sophisticated Mixture of Experts (MoE) architecture and
    has shown promising initial benchmarks compared to previous models like the Mixtral
    8x7B.
  id: totrans-split-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The model weights are available for download at Hugging Face, complete with
    installation instructions.
  id: totrans-split-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Why is Mixtral 8x22B So Powerful?**'
  id: totrans-split-11
  prefs: []
  type: TYPE_NORMAL
- en: The Mixtral 8x22B, utilizing the MoE architecture, boasts an impressive 176
    billion parameters and a context window of 65,000 tokens. This architecture allows
    for a sparse MoE strategy, providing access to various models each specialized
    in distinct areas, balancing performance and computational costs.
  id: totrans-split-12
  prefs: []
  type: TYPE_NORMAL
- en: '**Accessibility and Open-Source Commitment:**'
  id: totrans-split-13
  prefs: []
  type: TYPE_NORMAL
- en: Mistral AI continues to challenge proprietary models by adhering to open-source
    principles, making Mixtral 8x22B available for torrent download via Hugging Face.
    Detailed instructions are provided for running the model at different precisions
    to accommodate varying system capabilities.
  id: totrans-split-14
  prefs: []
  type: TYPE_NORMAL
- en: '**Market Position and Innovations:**'
  id: totrans-split-15
  prefs: []
  type: TYPE_NORMAL
- en: As the latest LLM in the Generative AI market, Mixtral 8x22B stands alongside
    recent releases like Databricks’ DBRX, OpenAI’s GPT-4 Turbo Vision, and Anthropic’s
    Claude 3\. Although designed primarily as an autocomplete model, distinct from
    chat or instruct models, it offers effective computing and performance for a broad
    range of tasks.
  id: totrans-split-16
  prefs: []
  type: TYPE_NORMAL
- en: '**Benchmarks and Comparisons:**'
  id: totrans-split-17
  prefs: []
  type: TYPE_NORMAL
- en: 'Despite the absence of official benchmarks, the Hugging Face community has
    conducted tests showing that Mixtral 8x22B closely competes with closed models
    from Google and OpenAI. It achieves notable scores in various benchmarks:'
  id: totrans-split-18
  prefs: []
  type: TYPE_NORMAL
- en: '**ARC-C Reasoning Abilities**: Scores 70.5, showcasing strong reasoning capabilities.'
  id: totrans-split-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Commonsense Reasoning**: Scores 88.9 on the HellaSwag benchmark, indicating
    robust commonsense reasoning skills.'
  id: totrans-split-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Natural Language Understanding**: Achieves a score of 77.3 in the MMLU benchmark,
    reflecting competitive NLP capabilities.'
  id: totrans-split-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Truthfulness**: Exhibits improvement in truthfulness, crucial for countering
    model-generated hallucinations.'
  id: totrans-split-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Mathematical Reasoning**: With a score of 76.5 in the GSM8K, it''s well-suited
    for basic mathematical problem-solving.'
  id: totrans-split-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Conclusion:**'
  id: totrans-split-24
  prefs: []
  type: TYPE_NORMAL
- en: Mistral AI's release of Mixtral 8x22B reflects a significant trend towards more
    transparent and cooperative AI development methods. The model’s potential for
    groundbreaking applications and research is generating considerable excitement
    within the AI community, promising to transform various technical fields globally.
  id: totrans-split-25
  prefs: []
  type: TYPE_NORMAL
