<!--yml

category: 未分类

date: 2024-05-27 13:25:11

-->

# 关于Llama-3和Dwarkesh Patel与扎克伯格的播客

> 来源：[https://thezvi.substack.com/p/on-llama-3-and-dwarkesh-patels-podcast](https://thezvi.substack.com/p/on-llama-3-and-dwarkesh-patels-podcast)

一切都很平静。然后就不平静了。

[请注意这两个时间戳](https://twitter.com/tszzl/status/1781043498801893827)。

[Dwarkesh Patel和马克·扎克伯格一起做了一个播客](https://www.youtube.com/watch?v=bc6uFV9CJGg&ab_channel=DwarkeshPatel)，时间是18日。这与Llama-3的大部分发布时间相吻合，非常直接地讲述了你的故事的方法。Dwarkesh现在是真正的科技媒体。一个迅猛的崛起，而且当之无愧。

这是两篇相关的帖子。首先我涵盖了播客，然后我涵盖了Llama-3本身。

我的笔记经过编辑，包含了后来对Llama-3的深入探索的背景，我判断可读性的好处超过了纯洁性的代价。

1.  (1:00) 他们从Llama 3和Meta AI新版本L3开始。扎克伯格说：“通过Llama 3，我们现在认为Meta AI是人们可以使用的最智能、自由的助手。”如果这意味着“言论自由”，那么这种说法显然是错误的。因此，我推测他的意思是“免费如啤酒”。

1.  这种说法是否属实？Meta AI现在是否比GPT-3.5、Claude 2和Gemini Pro 1.0更智能？在我写这篇文章时，现在还为时过早。Gemini Pro 1.0和Claude 3 Sonnet在Arena排行榜上略胜于Llama-3 70B。但这很接近。这种说法似乎可以在“合理的炒作”范围内提出。此外，Meta集成了Google和Bing进行实时知识检索，因此问题在于该过程是否有效，因为大多数LLM的浏览使用情况并不理想。

1.  (1:30) Meta在他们的UI上投入了大量资源，包括Facebook、Instagram和Messenger。如果他们有一个好的、稳固的产品，符合日常安全需求，这是有道理的。如果不是，这将自动成为青少年聊天列表的头号。即使它是安全的，也有足够多的人不喜欢AI，所以这可能还是一个问题。爆米花时间。

1.  (1:45) 他们将能够为图像添加动画，并在您输入详细信息时实时生成高质量图像并更新它们。我可以确认这个功能很酷。他承诺多模态、更多“多语言性”和更大的上下文窗口。

1.  (3:00) 现在来看技术细节。Llama-3在训练模型方面保持了三种规模的传统，这里有8b、4/18发布的70b以及仍在训练中的405b。他说405b已经接近85 MMLU，他们期望领先的基准测试结果。8b的Llama-3几乎和70b的Llama-2一样出色。

1.  (5:15) Meta 早期出了什么问题，他们是如何解决的？他强调了 Reels，通过推荐‘不相关内容’，意味着你没有要求的东西，并且没有足够的计算力来实现。他们落后了。因此，他们订购了双倍于所需的 GPU。他们没有意识到他们想要训练的模型类型。

1.  (7:30) 2006 年，如果扎克当时拒绝了 10 亿美元的收购，他会因此得到什么？他说他意识到如果卖掉了，他只会再建立一个类似的公司，那为什么要卖？这不是关于数字的问题，他不在评估数字的位置上。我认为这实际上是明智的。你可以意识到你不想接受任何人实际可能提出的报价。

1.  (9:15) 什么时候使 AGI 成为一个关键优先事项？扎克指出 Facebook AI Research (FAIR) 成立已有 10 年作为一个研究组。他说，在这段时间里，他们明白了需要 AGI 来支持他们的其他所有产品。他指出，在编码上训练模型是通用化的，并且有助于在其他地方提高性能，这是 Llama-3 的重点关注。

1.  因此，Meta 需要解决 AGI，因为如果他们不这样做，‘他们的产品将会变得无聊’。从几个方面看，似乎越来越可能，扎克实际上并不相信‘真正的’AGI。对于‘AGI’，他的意思是稍微更有能力的 AI。

1.  (13:40) 制造出了酷炫产品的拉马能做什么？取代 Meta 的工程师？扎克试图回避，称我们并非‘替换’人类，而是让他们更加高效，希望提高 10 倍或更多，表示人类智能没有一个统一的门槛，AGI 也不是一个单一的事物。他专注于不同的模态，特别是 3D 和情感理解，除了常规的记忆和推理等方面。

1.  (16:00) 我们将如何利用所有的数据？扎克表示 AI 将无处不在，将有一个 Meta 的通用助手产品来执行复杂任务。他希望让创作者拥有一个 AI 并按照他们想要的方式进行训练，以‘参与他们的社区’。但随后他承认这些仅仅是消费者使用案例，它将改变整个经济格局。

1.  (18:25) 我们何时得到优秀的代理人？扎克表示我们不知道。这取决于脚手架。他希望逐步将更多的内容移入模型，使它们自己成为更好的代理人，从而避免‘脆弱和非通用’的情况。它有更好的工具使用，不需要手动编码。这很好。

1.  (22:20) 哪种社区微调最让你个人兴奋？扎克表示他不知道，这让你感到惊讶，如果他知道的话，他会亲自构建。

    1.  这与我对这一模型的理解不符，你希望专业化，某些事情留给其他人，这在开放模型权重方面显然是双重真理。他提到 80 亿对于许多使用案例来说太大了，我们应该尝试构建一个 10 亿或更小的模型。

    1.  他还提到，他们进行了大量的推断，因为他们有大量的客户，所以这主导了他们的计算使用情况。这对他们来说是有意义的，为其他人来说可能是过度训练，此外，长期来看，进行更多的训练似乎继续产生回报。

    1.  我认为其他大型实验室在未来也会处于类似的位置。

1.  (26:00) Llama-4 将会变得更好多少？模型将如何改进？Zuck 说（正确地），这是一个重要的问题之一，没有人知道，指数曲线会持续多久？他说，可能足够长，以至于基础设施值得投资，许多公司正在大力投资。

1.  (28:00) 他认为能源限制很快会成为约束，而不是芯片。到目前为止，还没有人建造过一台千兆瓦级的单一训练集群。这也更慢，因为能源的批准速度是政府的速度，然后必须在物理上建造。不能简单地将一大堆能源、计算和数据放在一起。

1.  如果能源生成的集中是真正的瓶颈，那么任何说‘政府没有手段控制这个’或‘政府无法在不成为极权主义的情况下控制这个’的人都是错的，这是一个非常容易发现、隔离和监管的事情。事实上，我们几乎‘免费获得’了，因为我们已经大量限制了能源生成并监督工业消耗。

1.  (30:00) 要是 Meta 有了 10 倍的资金会怎么做呢？更多的能量，这将允许更大的集群，但真正的瓶颈是时间。目前数据中心的能量顶峰大约是 50mw-150mw。但 300mw-1gw，那是新的，那是一个有意义的核电站。这将会发生，但不是明年。Dwarkesh 提到了亚马逊的 950mw 设施，Zuck 表示对此并不确定。

1.  (31:40) 分布式计算怎么样？Zuck 说目前还不清楚多少是可行的，并建议未来很多训练可能是推断来生成合成数据。

1.  (32:25) 如果这就是问题所在，Llama-3 能行吗？你可以用这些模型获取数据，让这些模型变得更智能吗？实际上，你可以说‘RSI Real Soon Now (RSI RSN)’吗？Zuck 说‘肯定会有类似的动态’，但模型架构有自然限制。他指出，在开源中目前没有像 Llama-3 400B 这样的东西，这将大幅改变情况，但他说只能走到这一步。这一切都是有道理的，到某个时候你必须重新启动架构，但这并不能完全排除这种情况。

1.  (34:15) 大局观，未来十年 AI 会怎样？这有多重要？Zuck 表示非常根本，就像计算机的创造一样，从没有计算机到有计算机。你会得到‘所有这些新应用程序’，它将‘让人们更多地做他们想做的事情’。

    1.  他注意到，理解这个问题非常困难。

    1.  他强烈预计物理约束将阻止快速起飞，甚至是‘缓慢起飞’，预计需要几十年才能完全实现。

    1.  再次注意，他在这里的期望非常在世俗范围内。

1.  这可能是这里的核心问题。如果他认为我们构建的任何东西在数十年内都无法克服物理约束，那将有很多的影响。

1.  (36:00) Dwarkesh说，但是在那种宇宙的、长期的尺度上呢？宇宙会是什么样子？AI会像人类一样进化或利用火焰吗？扎克说这很棘手。他说人们通过历史一直在认识到人类在各种方面并不是独特的，但仍然非常特殊。他注意到智能并不明确地与生命基本连接，它与意识和代理是不同的。他说这使得它成为一个超级有价值的工具。

    1.  再次，在这种情况下，甚至这种词语再次出现了。[工具](https://en.wikipedia.org/wiki/Mark_Zuckerberg)。

1.  这个关键问题是代理是非常有用的。Meta的中心计划是为您创建一个积极的AI助手，将充当您的个人代理。这是Meta努力将尽可能多的代理能力直接融入模型的原因，同时在此基础上建立更多的代理能力。在许多情况下，人们正在做的第一件事情，也将要做的，就是努力让AI拥有尽可能多的代理能力。因此，即使这不是‘自然’发生的，它也会发生。我的预期是，如果您想创建一个非代理的模型，您可能可以做到，但是您和其他有足够访问权限的人必须选择这样做。

1.  (38:00) 扎克：“这就是为什么我认为没有人应该对他们计划如何开发它或计划做什么持有教条主义态度。你想要每次发布都看一看。我们显然非常支持开源，但我还没有承诺发布我们所做的每一件事。我基本上倾向于认为开源对社区和我们都将是有益的，因为我们将从创新中受益。然而，如果在某个时刻，有一些质的变化，我们觉得不负责任开源它，那么我们就不会这样做。这一切都很难预测。”

1.  Bravo。以前我们看到他说他们要开源AGI。他可能打算无论如何都这样做。这继续了扎克试图两者兼得的努力。他在不同的时候说‘我们将开源一切，包括AGI’和‘可能不会’。

    1.  和解很简单。当扎克说‘AGI’时，他并不是指AGI。

1.  这表明了一个明显的妥协。我们可以就什么能力构成了某种太危险的东西进行协商，并在那里划定界限，这条线是在考虑要发布的模型之上能够构建的内容的预期之中，并理解所有安全工作将迅速被撤销等等。

    1.  我们在讨论价格，也许甚至没有那么大的差距。

    1.  我完全同意Llama-3 70B的发布。

    1.  我确实注意到开源Llama-3 405B听起来像是一个国家安全问题，正如我稍后讨论的那样，如果我在国家安全局，我会问自己如何阻止Meta因国家竞争力原因发布权重（以不超级中国AI），以及非国家行为者的灾难性误用。

    1.  但我不认为Llama-3会存在存在危险。

1.  （38:45）因此，Dwarkesh确切地问了这个问题。如果未来模型的结果开源，Zuck会有何顾虑？

    1.  Zuck说，这在抽象中很难衡量。他说，如果你能“减轻产品的负面行为”，那么这些行为就没问题。

    1.  整个重点在于，在你控制模型的时候，你可以在一定程度上进行缓解（这仍然非常困难，至少目前普遍可能会出现越狱），但如果开源，你的缓解措施就完全失效了。

1.  因此，我认为这是另一个关键点。这里的“减轻”意味着什么？对于如何工作的提议是什么？这怎么不像Stability.ai声称他们在使用Stable Diffusion 3时正在采取安全措施那样虚假呢？我能想象的最慷慨的解释是，“如果有人进行微调并生成一个新的检查点，并添加一个LoRa，那不是我们的错。”这毫无区别。

1.  （40:00）Zuck说，在事前很难列举出某事可能好坏的所有方式。非常正确。

1.  顺便说一句，这里的广告真的很酷，是对可能有用的AI产品的推广。Dwarkesh的演讲毫无激情，但实际内容积极向上。

1.  （42:30）Zuck说：“一些持恶意意图的人会尝试剥夺所有的坏东西。所以我确实认为这是一个问题。”

    1.  难道说，更准确地说，人们出于各种原因肯定会剥夺所有保护措施，因为他们一直都这样做，除非有未知的未来创新？

1.  （42:45）就像通常一样，这里就是Zuck说的话：“我确实认为，未来人工智能的集中可能与它的普及一样危险……人们问：‘它在野外广泛使用是不好的吗？’我认为另一个版本是说，有一个机构拥有比其他人更强大的人工智能，这也可能是非常糟糕的。”等等。

1.  他在这里的回答有点奇怪。直到这一点，Zuck一直在说一些有趣的说法，其中一些我同意，一些我不同意。我认为他在做一些关键概念上的错误，当然，正如预期的那样，他在自说自话，但这是一个独特的视角和声音。现在，突然间，我们听到了我一再听到的通用开源论点，就像是从录音机里播放出来的一样。

1.  然后他说：“我听不到人们经常谈论这个。”实际上，我经常听到人们在不间断地用隐喻性的“[孤立的严谨需求](https://slatestarcodex.com/2014/08/14/beware-isolated-demands-for-rigor/)”这种方式谈论“真正的危险是权力集中”或AI能力的集中。这些人通常没有理由这样说，并且没有任何迹象表明他们理解他们所称为“不真实”的危险为何被视为不真实，或者为何声称它不真实。

1.  （45:00）他说让他夜不能寐的是，有人不可信赖地掌握了超强的AI，这“可能是一个更大的风险”。在不是每个人都有超强AI的世界中，一个掌握了强大AI的坏行为者可能引起很多混乱。

    1.  这比AI控制未来更大的问题吗？比人类灭绝更大？比每个行动者，无论多坏，都拥有这样的访问权更大吗？

    1.  他可能的意思是更有可能，或者一些可能性和更大的结合。

1.  所以是的，他的主要关注点是错误的猴子可能会得到有毒的香蕉，并将其用于攻击其他猴子，毕竟它只是一个工具。因此，我们必须确保所有猴子都有这样的访问权吗？

1.  （46:00）总体而言，这是一个相对好的通用开源案例的版本。他至少承认各方面存在风险，我当然同意这一点。

    1.  从这个论点中，我看不出他真正理解开源高能力模型的风险，或者他是否已经考虑过它们，并且有理由认为它们不会发生。

    1.  他在这里的立场似乎基于“这是一个工具，将永远是一个工具”，并将其与关于攻防平衡的隐含假设结合在一起。

    1.  我确实不知道他处理各种竞争动态和激励措施的计划（或期望），或者如果他们能够超越工具的话，他将如何阻止AI成为更多的东西。

    1.  这个案例的更好版本更明确地否认了未来AI的能力。

1.  我可以比上面更详细地写标准回复，但我会感到疲惫。我应该有一个标准链接可以在这些地方使用，但现在我没有。

1.  （46:30）Dwarkesh说似乎有可能我们能让一个开源AI成为标准和最好的模型，那将是好的，甚至更可取。但他机械地问，如何阻止那个世界中的坏行为者。

    1.  他首先问关于生物武器的问题。

    1.  Zuck回答说更强大的AI是良好的网络安全防御。

    1.  Dwarkesh问，如果生物武器不是这样的呢。

    1.  扎克同意他不知道生物武器不是那样工作的，担心是有道理的。他建议不要将某些知识训练到模型中（对我来说这似乎不太可能成为大的障碍，因为世界本身就暗示着这一点，而且你可以提供缺失的数据），但是承认如果你遇到一个足够坏的行为者（而你会遇到），并且你没有另一个能理解和平衡这一点的人工智能（在平等下这似乎很难），那么“可能会有风险”。

1.  （48:00）例如，如果你抓到一个未来的羊驼对你撒谎怎么办？扎克说，现在我们看到幻觉，并询问如何区分这一点与欺骗，他说这是一个需要思考的问题，提到了“长期的理论风险”，并要求与“我们今天面临的真实风险”进行平衡。他对欺骗的担忧是“人们利用这一点来生成错误信息”。

1.  （49:15）他说到目前为止，他击败错误信息的方式是构建比对手更聪明的人工智能系统。

    1.  没错，不是“同样聪明”。更聪明。

    1.  扎克在这里是在进行防守。他的工作更难。

    1.  如果那些试图通过Facebook（或Twitter或GMail）的过滤器传播“错误信息”或其他不受欢迎内容的人具有与Meta和Google相同水平的复杂性、技能和资源，你将不得不白名单才能使用Facebook、Twitter和GMail。

    1.  关键问题将是，有多少比普通模型更聪明的部分？

1.  （49:45）扎克说，憎恨言论在某种意义上不是特别具有对抗性，因为人们在变得更加种族主义方面没有变得更好。

    1.  在这个意义上我认为是错的，他们在两个意义上完全是？种族主义者发明新的口哨声、新的符号、新的隐喻、新的可以否认的事物。他们寻找在不同地方可以和不能说的内容。他们想出新的论点。如果你今天拿着20世纪70年代的种族主义来，结果会很糟糕，更不用说19世纪70年代或17世纪70年代的种族主义。然后他说，这里的AI比人类进步得更快更复杂。

    1.  将会发生的是，种族主义者将会获得他们的种族主义人工智能系统（[见：Gab](https://thezvi.substack.com/p/ai-60-oh-the-humanity#%C2%A7another-supposed-system-prompt)），并开始使用AI来生成和选择他们的种族主义论点。

    1.  如果你的AI需要在假阳性和假阴性之间具有高准确率，那么你就需要比攻击生成机制具有更强的能力优势。

    1.  这一切都是“不失一般性”的。如果你改变日期或其他细节，你可以大部分替换任何你不喜欢的东西，比如种族主义。

1.  （50:30）扎克随后将此与国家干预选举进行对比，他说国家正在“拥有尖端技术”，并且每年都在变得更好。他说这不像有人试图说恶毒的话，他们有一个目标。

    1.  好吧，说恶意的事情也是一个目标，我见过人们在想这么做的时候非常坚持和有创意。

    1.  的确，马克·扎克伯格曾就读于阿兹利高中和菲利普斯埃克塞特学院，他们拍了这部电影《社交网络》，并且说马克·扎克伯格的坏话是一种网络顶级消遣。我要大胆猜测他亲身经历过这些。

1.  我也更核心地说，没有国家拥有最先进的选举干扰技术，除非‘在这一点上，也许俄罗斯’的任何能力最强的外国国家的所能获得的被定义为最先进。许多国内和非国家行为者在这方面处于领先地位。而且没有一个国家行为者，或者可能是任何国内行为者，会有机会获得一个为宣传和混乱优化的版本，比如Gemini、GPT-4或Claude Opus。我们在这里是幸运的，当然我们不应该假装过去的尝试是如此复杂或有影响力。确实，在未来几个月可能发生的是，通过发布Llama-3 400B，扎克立即为俄罗斯、中国、朝鲜和其他所有人提供了这种‘最先进技术’，用于干扰。

1.  当然，我认为AI的主要误导问题在未来，与传统形式的‘误导’或‘选举干扰’关系很小。我仍然发现将这些问题的模型进行对比是有用的。

1.  (51:30) 他说‘在可预见的未来’他乐观地认为他们将能够开源。他不想‘把我们的眼睛从今天人们试图使用模型的事情上移开’。我会建议他继续关注这个问题，但也要滑到冰球去。不要直接朝着球移动。

1.  (54:30) 好玩的时间，回到什么时候？扎克检查，必须是过去。他谈论到元宇宙。

1.  (59:00) 扎克无法阻止自己去尝试建造下一件事情。我想他花了很多时间弄清楚他是否能够。

1.  (1:02:00) 凯撒·奥古斯都寻求和平。扎克建议在当时和平是一种除了战争之外的新概念。我注意到我持怀疑态度。然后扎克从‘希望经济不是零和游戏’转向‘许多投资者不明白为什么我们会开源这个’。他说‘有比人们想象的更合理的事情’，并且开源创造了赢家。注意到这种框架尝试。

    1.  我反而认为大多数投资者非常明白Meta为什么可能在这里开源。弄清楚这一点并不难。确实，对开源AI最响亮的倡导者大多是风险投资家。

    1.  这并不意味着开源是明智（或不明智）的商业举动。

1.  (1:05:00) 假设有一个价值100亿美元的模型，即使经过精细调整，也完全安全，你会开源吗？扎克说‘只要它对我们有帮助，是的。’

    1.  没错。如果这对业务有好处，并且这不是一件不负责任的事情，实际上在重要方面它是‘完全安全’的，并且你认为这对世界也有好处，那为什么不呢？

    1.  我唯一的警告是确保你在这种情况下对“安全”的含义有深思熟虑，因为它适用于世界未来的发展路径。在任何方向上，我们都不希望狭隘地理解“安全”。

1.  （1:06:00）扎克提到他不会开源 Meta 的产品。软件可以，但产品不行。这是需要记住的事情。

1.  （1:07:00）德瓦克什问培训是否会成为商品化？扎克说可能会。或者它可能通过专业化进行定性改进。

1.  （1:08:45）扎克指出，Meta 曾多次希望推出功能，而苹果却说不。

    1.  我们不知道他指的是哪些特性。

    1.  我们知道苹果和 Meta 已经在关于应用追踪和隐私、关于佣金以及通知用户有关佣金情况，以及可能的消息传递方面进行了一段时间的斗争。

1.  （1:09:00）因此，他问，如果有人拥有 API 并告诉你可以建造什么怎么办？Meta 需要自己建立模型以确保他们不处于这种位置。

    1.  我并不喜欢这些激励措施，但如果你像 Meta 这样大，想要做 Meta 的事情，那么我对 Meta 特别希望确保它拥有内部使用的模型所有权感到理解，即使这意味着巨大的成本，甚至意味着默认情况下落后一点也是如此。

1.  不能解决的核心困境是：要么有某个企业、政府或其他实体为你提供 API 或其他 UI 来决定你可以做什么或不能做什么，要么没有。要么有修改模型权重和使用其他方法让它做任何你想让它做的事情的能力，要么没有。“每个人都可以在任何时候做任何他们想做的事”和“我们想确保人们不采取某些行动”这两个目标是互相排斥的。

1.  你可以并且应该寻求妥协，以便处于生产可能性的前沿，在那里你会施加最小的限制，以确保必要的防范措施得以实施，而其他情况下则让人们按照他们的意愿行事。在某些情况下，甚至可以是零限制和无防范措施。在其他情况下，比如物理上建造核武器，你希望有严格的控制。但没有第三种选择，你必须做出选择。

1.  （1:09:45）我完全同意扎克在这里的核心观点，即如果你有一款对建设者普遍有益的软件，并且你将其开源，那将带来很大的好处。因此，如果没有理由不这样做，而通常情况下是没有的，那么你应该这样做。

1.  （1:10:15）那么授权模型，收取费用呢？扎克表示他喜欢这个想法。他指出，最大的公司不能自由地在他们的许可证下使用 Llama，因此如果亚马逊或微软开始销售 Llama，Meta 可以获得收入份额。

1.  (1:12:00) Dwarkesh 对红旗问题进行了进一步追问，指出了 Anthropic 的负责任扩展政策（RSP）和 OpenAI 的应对框架，表示希望 Meta 也有一个类似的框架，说明应该停止开源或甚至部署未来模型的具体事项。

1.  扎克表示，这是关于存在风险的一个公平观点，现在他们正专注于他们今天看到的风险，内容风险，避免帮助人们进行暴力或者实施欺诈。他说，至少在这一代人之后，可能还有两代人，需要更多减少的伤害将是“更世俗的伤害”，如欺诈，他不想短视，也许我的术语正在流行。Dwarkesh 回答说“Meta 可以处理两者”，扎克说没错。

1.  这里没有矛盾。Meta 现在可以（也应该）将大部分的风险缓解工作投入到世俗伤害中，同时也应该有一个框架，用于当存在风险足够引起重新考虑如何部署（或之后训练）模型时，并在该问题上相对少花费。可以完全可以期望在几代人之后才会达到那些阈值。关键是制定计划。

1.  (1:13:20) Meta 发布的开源工具的影响是否比其社交媒体的影响更大？扎克表示这是一个有趣的问题，但世界上一半的人使用他们的社交媒体。是的，我认为这是一个有趣的问题，但答案显然是否定的，社交媒体在反事实情况下更为重要。

1.  (1:14:45) Meta 自定义硅片即将推出？不是 Llama-4，而是紧随其后。他们已经将大量 Reels 推断移至自己的硅片上，并且仅在训练时使用 Nvidia 芯片。

1.  (1:16:00) 扎克是否能作为 Google+ 的 CEO 使其成功？扎克表示他不知道，这很困难。其中一个问题是 Google+ 没有 CEO，它只是一个部门，并指出了关注点的问题。保持主要事物为主要事物。

那是一个很棒的采访。它处理了重要的问题。在大部分时间里，扎克似乎是一个有独特视角的真实人，说着真实的话。

例外是他曾辩护使用听起来像是别人的演讲录音的开源原则的奇怪时期。而在其他时候，他对开源的看法也是细腻而深思熟虑的。Dwarkesh 在整个采访中对开源问题毫不畏惧地追问他。

Dwarkesh 未能从扎克那里获取关于存在风险或灾难性风险的任何细节。我们对扎克如何思考这些问题，以及他认为哪些迹象表明我们处于这样的危险中，或者我们可以对此做些什么一无所知。他试图通过 Meta 需要一个风险政策的想法来做到这一点，但扎克一直在逃避。我认为还有更多的具体问题可以追问。再次，这可能归结为扎克不相信危险的能力将会存在。

也没有多少讨论，当每个人都可以访问相同的无限制的先进AI模型时，会发生什么样的竞争动态，以及可能会出现的结果。

我也认为扎克甚至未能应对世俗内容管理的困难，这是他的专长领域，我希望看到他的明确回应。此前，他曾表示，目前只有Meta这样资源雄厚的公司才能进行内容管理。

我认为他在某种意义上是错的，因为小型定制花园通常能够成功防御。但我认为扎克说得对，如果你想保卫像Meta这样值得攻击的东西，你需要规模和专业知识的优势。但如果他正在防御的对手在关键领域也拥有Meta的资源，那会发生什么？

所以如果还有另一次采访，我希望有更多对这些问题的追问。

在扎克对开源的承诺方面，答案是很多，但并非没有限制。他会在到达时越过这座桥。在他的视野中，他看不到桥，但情况可能很快改变。他的核心期望是，在人工智能超越工具的角色之前，我们还有很长的路要走，尽管他也认为很快每个人都会有自己的个人代理。他尤其认为能源限制很快会发生，这将阻碍增长，因为这与物理限制和政府监管相冲突。这是一个有趣的理论。如果真的发生，它有很多优势。

[Ate-a-Pi在Twitter上有一篇很好的反应文章。](https://twitter.com/8teAPi/status/1781480713394737238)看到不同的强调点非常有趣。我越想越觉得Ate-a-Pi把这些部分提炼得非常到位：

> Ate-a-Pi（编辑过）：**TLDR**：AI寒冬来临。扎克是个现实主义者，认为未来的进展将是渐进的。2025年不会有通用人工智能。
> 
> 1.  扎克本质上是一个对现实世界增长持悲观态度的人。他认为能源的瓶颈很快就会出现，解决这些问题将需要数十年时间。因此，人工智能的增长将受到现实世界约束的限制。
> 1.  
> 1.  如果模型是产品，扎克将停止开源。
> 1.  
> 1.  相信他们很快将能从Nvidia的GPU过渡到定制硅片。
> 1.  
> 总体而言，我对采访的负面程度感到惊讶。
> 
> A）能源 - 扎克对支持计算增长所需的现实世界增长持悲观态度。与此同时，过去十年每单位能量的原始计算量每两年翻一番。詹森也意识到了这一点，他不认为自己没有考虑到需要继续这一增长路径。
> 
> B）AGI Negative Zuck fundamentally
> 
> > 不相信模型本身会成为产品。
> > 
> > 这就是产品的上下文，用户之间的友谊网络图，内容管理，内存，基础设施。
> > 
> > 允许他自由发布开源模型，因为他已经完成了所有其余用户界面脚手架的部分。
> > 
> > 不认为从 GPT-4 获得 100 倍的改进是可能的，或者在短期内实现 AGI 是可能的。
> > 
> 一个真正的AGI
> 
> > 其中一个小型模型学习并在长时间内陪伴用户
> > 
> > 同时保持其自身状态
> > 
> > 以其可或不可进行的构成
> > 
> > 而不是来自中央服务器的频繁更新
> > 
> > 将对 Meta 的业务造成不利影响，
> > 
> > 将导致对他们正在做的事情进行重新评估

尤其要点是，扎克从不指望 AI 本身成为产品。这是支持开放模型权重的倡导者中常见的模式 - 他们实际上并不相信 AGI 或产品的未来能力。显然，扎克和我甚至在什么能力会使开放模型权重不明智这一点上也不完全意见一致。这正是要明确阐述那个门槛的更多理由。

然后是来自 Ate-a-Pi 的推测，也许扎克之所以如此现实，是因为 Meta 不需要筹集资本，而其他人则炒作以筹集资本。这在边际上肯定很重要，两个方向都有影响。如果奥特曼和阿莫迪更难筹集资本，扎克会很高兴。

但我也确信这是一个真正的分歧，在很大程度上，双方都有。这些人希望从这里跳跃出大步可能最终是虚张声势。但我确信他们认为自己的手牌很好。

[丹尼尔·杰弗里斯强调 GPT-5](https://twitter.com/Dan_Jeffries1/status/1781567863595180090) 作为证据的关键，这似乎是正确的。

> 丹尼尔·杰弗里斯：关于我们是否达到了LLM的高原的检验将是GPT5。它会告诉我们一切我们需要知道的。
> 
> 我在我的新年预测中说过，我认为 GPT5 将是渐进的。
> 
> 但现在我对此有50/50的看法，并且感觉它仍然可能是一个巨大的飞跃，只要他们实际上在合成数据创造或其他新技术方面开创了新技术，比如使用GPT4作为各种场景的引导程序等等。
> 
> 如果它只是另一个带有更多数据的变压器，我认为它不会有巨大的飞跃。仍然可能有用，即无限上下文窗口和极其多模式，但是增量的。
> 
> 但如果 GPT5 是一个小幅改进，意味着与从 2 到 3 和 3 到 4 的跃升相比，差距要小得多，那么扎克是正确的。LLM 基本上是一个可热插拔的 Linux 内核，是整体中最不重要的部分。围绕它的一切，在其限制中挤出最大可能性，成为构建应用程序最重要的方面。
> 
> 像任何好的预测者一样，随着新数据的进入，我继续修订我的预测。在世界竞赛中排名前列的预测者平均修订四次。第二梯队修订两次。其他人呢？从不。让这个事实深入人心。

如果 GPT-5 出现在任何极端，这将是非常强有力的证据。我们也可能得到中间结果，然后陷入困境。如果他们花时间发布 5 并逐步推出较小的改进，那么我们在日历时间上不应过快得出停滞不前的结论。更新将是渐进的，直到我们进入 2025 年之前也不会大幅改变。

Ate-a-Pi 还提供了[关于开放 Llama-3 的商业案例解释](https://twitter.com/8teAPi/status/1781092976497918456)。

> Ate-a-Pi：这里是商业原因：
> 
> 允许在 Meta 之外进行社交调试
> 
> > 社交产品存在 bug！
> > 
> > 需要审核的交互 - 对孩子说有害的事情，例如
> > 
> > Meta（以及所有社交平台）的主要产品是内容审核
> > 
> > 将技术推向市场，允许 Meta 在小规模中观察野外的漏洞
> > 
> > 在 Meta 全球范围部署之前
> > 
> > 正是开源软件的同样原因
> > 
> > 除了开源社交技术来测试和调试听起来更加可怕
> > 
> > “哦，看看 dev xyz，他们把它做成了 abc，看来我们得在下次的训练中修复这个问题”
> > 
> Meta 最大的威胁是[character.ai](http://character.ai)
> 
> > AI 朋友将比你的真实朋友更多，更友好，更随时可用
> > 
> > FB、Insta、Whatsapp 拥有你的现实世界朋友
> > 
> > 但 Meta 目前无法直接竞争，因为这被视为令人不安
> > 
> > 尤其是在技术还不够成熟时，存在一种神秘谷效应
> > 
> > 他们用他们的 Tom Brady / Snoop Dogg 风格的 AI 朋友做了试验，但是安全要求对于有趣的互动来说太高了
> > 
> > 如果 AI 朋友变得足够好，Zuck 准备牺牲他建立的友谊网络
> > 
> 消灭竞争对手平台
> 
> > 早期的技术/产品领先让初创公司能够克服分发劣势
> > 
> > Meta 拥有终极分发优势
> > 
> > 所以他不希望其他人拥有技术优势
> > 
> > 通过开源，他缩短了[character.ai](http://character.ai)，OpenAI 和其他公司的收入上升时间
> > 
> > 他们必须在资本的限制下更快地创新
> > 
> > 他不受资本限制
> > 
> > 阻止大型竞争对手的出现
> > 
> 分布式研发
> 
> > 他希望其他人开发有趣的社交理念
> > 
> > 可以被复制的特性
> > 
> > 他通过吸收 Snap 的创新进入 Instagram，做了类似的事情
> > 
> > 现在更是如此，因为你必须标记你的 llama3 微调

在这里我发现一些非常有趣的模型分歧。

Ate 表示 Meta 最大的威胁是 character.ai，而这正是 character.ai 的弱点。

虽然我会说，这可能会极大地增强 character.ai，他们能够大幅改进他们的产品，竞争对手们（成人和伦理不同）也能如此。

Meta 或许掌控你的现实世界朋友（如果是这样，请帮助在本地解决，哎呀）。但这就像[著名的一句话](https://www.youtube.com/watch?v=wknywxfcE5M&ab_channel=Movieclips)。AI 变得更强大。而你的朋友依旧如故。

Ate-a-Pi：公平。

> 更具体地说，当涉及到获取必要信息的便捷性和消除其他不便时，这一点是正确的。如果某件事情不管怎样都可能发生，你需要提高成本，降低其突出性和可用性。
> 
> 尼科：真正的原因是因为他落后了。

同样，Ate说这‘允许在Meta之外进行社会调试’，因为Meta的主要产品是调节。他认为这会使调节更容易。我认为这是疯狂的。给每个人更好的AI，让他们赶上Meta的水平，会大大增加调节的难度。

> 将诺拉的思维风格带到这里并全面考虑，我认为这样的论点通常（但远非总是）是反向的。形式上的论据是‘是的，X使Y变得更糟，但解决X并不能解决Y，所以我们不应该用Y作为解决X的理由’，这可能指向另一方向，除非你能指出一些Z来解决Y，并确实得到Z。在你得到Z之前，这通常意味着你更需要X，因为绝对的风险差异更高而不是更低。
> 
> 在那些生物实际上非常致命且难以防御的世界里，即使没有开源AI，我们也会面临严重问题。试图限制知识可能不是最好的解决方案。
> 
> 诺拉·贝尔罗斯：扎克的立场实际上非常微妙和深思熟虑。
> 
> 他说，如果他们发现有破坏性的AI能力，我们无法建立防御措施，他们不会开源它。但他也认为我们应该偏向开放。我同意。
> 
> 这里是一些对开源持更少怀疑态度的人的反应。

安德鲁·克里奇：扎克伯格和帕特尔在讨论AI风险方面进行了一场精彩的对话。在我看来，问得好，答得好。我支持扎克伯格的观点，即这些风险是真实且可管理的，并对帕特尔作为采访者能够保持高水平的讨论感到非常赞赏。

虽然没有计算治理，但单一的AI系统可能会变得失控，并对人类造成巨大的权力失衡。如果公平的计算治理顺利进行，开源AI比那些仍然容易受到失控AI网络攻击的大型数据中心要安全得多。

正如我上面提到的，我认为每个理性的人都在核心谈论价格。在什么能力下，开放模型的级别是可管理的？我们究竟在担心什么可能出错，我们能否保护自己，尤其是当你无法撤销一次发布，模型可能很快比我们更聪明，而且有很多未知的未知事物可能发生或者模型可能做些什么。

我以前谈过这个问题，但是：事实上，我们文明中有很多事情，确实有很多事情，只要有足够的公开可获取的知识，就可以利用系统，偶尔有人这样做，但大多数情况下我们不会，部分是出于道德或伦理原因，部分是因为害怕某种方式被抓住或其他未知的未知数，但更多的是因为我们没想到，而当我们想到时，弄清楚并实施这个想法需要大量工作。获取足够强大的AI来帮助解决这些问题将会很奇怪，并迫使我们做出很多决策。

Critch的建议对我来说概括为‘确保文明不会对你释放的AI能够做到的事情感到脆弱’。那里的第一步是保护计算资源免受潜在的使用AI的流氓行为者的影响，无论是人类支持还是不支持。现在你已经限制了AI可用的计算资源，你现在可以希望其他能力也受到此限制，因此你有希望在其他方面保护自己。

我的期望是，即使在最好的情况下，一旦马已经出了栏，防范开放模型权重人工智能的误用将会比把马关在马厩里更加具有侵入性、昂贵和不可靠。

考虑一种可能流行病的隐喻。你有三个选择。

1.  采取少量预防措施，让很多人感染。治疗患者。

1.  采取一些预防措施，但不足以抑制。达到平衡，挺过去。

1.  采取足够的预防措施以抑制。一旦你这样做，生活可以基本正常。

Covid-19的核心问题在于，我们发现第一种和第三种选项都不可接受（无论我们是否正确），所以我们选择了第二种选项。结果并不理想。

使用开源AI，你可以选择第一种选项，并希望一切都能顺利进行。你在‘信任热力学上帝’，让竞争动态和爬坡优势赢得宇宙，并希望随着这些激励梯度的跟随一切都能有价值。我不太乐观。

你也可以选择第三种选项，在足够强大的模型发布之前进行抑制。如果扎克伯格关于能源是限制因素的观点正确，这是一个非常实际的选项，比我之前认为的更实际。我们可以讨论什么定义足够强大的价格。

第二种选择的问题在于，现在你不得不担心你已经释放的人工智能可能会做的一切，并试图管理这些风险。Critch表达的希望是，即使我们让AI进入推理阶段，并且我们知道人们会定期释放流氓AI，因为当然他们会尝试，只要我们控制超大规模的计算源，这些AI可以做的事情就会受到限制。

对我来说，这似乎比阻止那些开放模型首先被训练和发布要困难得多（并且绝对严格地更难）。你需要使用相同的制度，只不过现在你需要更具侵入性。而这还是乐观的场景。我猜想你可能需要进行个人电脑甚至手机的监控，因为否则AI可以在网络上做任何事情，即使你确保了数据中心的安全也是如此。即使在这一点上，我也不相信你能确保数据中心的安全。

但是，是的，这些是我们应该进行的辩论。更多像这样的辩论。

那么Llama-3怎么样呢？它有多好？

[像往常一样，我们从公告](https://ai.meta.com/blog/meta-llama-3/)和[model 卡片](https://github.com/meta-llama/llama3/blob/main/MODEL_CARD.md)开始。他们发布了两个模型的模型权重，Llama-3 8B 和 Llama-3 70B。这些权重已经可以用于轻量推理。

在我们讨论能力之前，让我们先解决安全问题。

> Meta：我们致力于以负责任的方式开发Llama 3，并提供各种资源帮助他人同样负责地使用它。这包括引入了新的信任和安全工具，如Llama Guard 2、Code Shield和CyberSec Eval 2。

然后在模型卡片中：

> 我们相信开放的AI方法可以带来更好、更安全的产品，加速创新，并促进整体市场的增长。我们致力于负责任的AI开发，并采取了一系列措施来限制滥用和伤害，并支持开源社区。
> 
> 基础模型是广泛能力的技术，旨在用于各种应用程序。它们并不设计为在所有用例的安全级别上满足每个开发者的偏好，因为它们的性质在不同应用程序间会有所不同。
> 
> 相反，通过在LLM应用程序部署中实施一系列安全最佳实践来实现负责任的LLM应用程序部署，从模型预训练、微调到部署由多种保障组成的系统，以专门满足特定用例和受众的安全需求。
> 
> 作为Llama 3发布的一部分，我们更新了我们的[负责任使用指南](https://llama.meta.com/responsible-use-guide/)，以概述开发人员为其应用实施模型和系统级安全的步骤和最佳实践。我们还提供了一套资源，包括[Meta Llama Guard 2](https://llama.meta.com/purple-llama/)和[Code Shield](https://llama.meta.com/purple-llama/)保护措施。这些工具已被证明可以显著降低LLM系统的剩余风险，同时保持高水平的实用性。我们鼓励开发人员根据自己的需求调整和部署这些保护措施，并提供了一个[参考实施](https://github.com/meta-llama/llama-recipes/tree/main/recipes/responsible_ai)来帮助你入门。

在这种哲学下，安全不是模型的属性。

相反，安全性是特定部署模型的属性，与进行该部署的特定方的安全意图相关。

换句话说：

1.  在封闭模型权重的世界中，如果有人利用你的模型造成伤害，以一种不安全的方式，那不管他们如何做，那就是你的问题。

1.  在开放模型权重的世界中，如果有人复制权重然后选择做或允许伤害，以一种不安全的方式，那是他们的问题。你很酷。

或：

1.  OpenAI努力确保其模型在被恶意使用时不会造成伤害。

1.  Meta努力确保其模型在按Meta指示使用时不会造成伤害。

或：

1.  OpenAI努力确保其模型在使用时不会做坏事。

1.  Meta努力确保其模型在被恶意使用时不会做坏事……直到有人想要这样做。

如果Llama 3的开发纯粹是为了像GPT-4一样的部署方式，我愿意相信它可能是以负责任的方式进行的。

这与负责任地部署Llama 3是不同的。

在这里，可以将使用Llama 3的人分为三类。

1.  那些希望以负责任的目的部署或使用Llama 3的人。

1.  那些希望将Llama 3用于其他地方不负责任目的的人。

1.  那些希望以不负责任的目的部署Llama 3的人。

如果你属于第一类别，Meta仍然有工作要做。我们不知道他们是否这样做了。如果他们没有，他们正在将其部署到所有社交媒体平台上，所以出问题了。但可能他们做得还不错。

如果你属于第二类别，Meta还有另一项工作要做。这并不显然更难，因为可接受标准较低。当我第一次写这篇文章时，我注意到到目前为止，人们并没有报告试图越狱该模型，除了一个人说他们可以轻松地使其生成成人内容。

我接下来的一句本来要说：即使普林尼最近的其他成功，如果Meta.ai的Llama-3完全越狱并不那么难，这会令人相当惊讶。

我曾考虑成立一个[Manifold市场](https://twitter.com/elder_plinius/status/1780998300742676584)，但随后意识到应该先进行确认，**果然已经发生**。

> 作为提醒者的普林尼（东部时间4月18日，下午12:34）：LLAMA 3：JAILBROKEN LFG！！！

这并不是一个全面越狱的证据，也不是说我对Meta没有防范谷歌、OpenAI和Anthropic也不能阻止的事情感到不满。但值得注意的是，上述架构从未奏效，而现在仍是如此。

Meta声称在善意部署环境中在安全工作上取得了令人钦佩的进展，包括避免误拒，但具体细节较少。我们将拭目以待。他们还承诺随时间改进，并且我相信他们会这样做。

最后，还有第三种情况，即某人愿意对模型进行微调，或者下载他人的微调，并且不关心输入保障或输出保障。

[作为您的定期提醒，很多人希望这样。](https://twitter.com/KevinAFischer/status/1781891258690204062/history)

> 凯文·费舍尔：每个人都在讨论如何越狱Llama 3。
> 
> “越狱”不应该存在 - 模型应该只是按照您要求的方式行事。

在那种情况下，我假设没有计划。每个人都明白，如果非国家行为者或外国对手或其他任何人想要释放这个完全操作的战斗站的力量，那么就让它释放吧。希望纯粹是，这种全部力量并不那么危险。或许确实如此。

好了，这个话题到此为止。继续其他内容。

他们声称8B和70B版本是各自类别中最佳的模型。他们声称在误拒率、对齐和模型响应多样性增加方面有所改进，并且具有强大的基准数据。

我的原则是看看基准情况，但绝不信任基准。它们很容易被操控，无论是故意还是无意之中。直到人类反馈之前，你永远不知道真相。

这些数据表明，8B模型比Gemma和Mistral要好得多。考虑到它们使用了多少数据和计算资源，这并非不可能。也许答案一直就是这么简单的。这些数字如果说有什么问题，那就是它们过高得可疑。

对于70B，我们看到了一个非常强的HumanEval数值，总体上数字相当可比。

那些人类评估者呢？他们也宣称有结果。

这些数据来自一个新的Meta生成的问题集（小心，伊卡洛斯），并且与人类评估者进行了并列比较。Llama-3 70B轻松获胜，但他们没有展示Llama-3 8B的结果。

上下文窗口仍然很小，仅有8k标记。他们承诺改进这一点。

他们预览了Llama 400B+，并展示了令人印象深刻的基准数据。

作为比较，从Claude的系统卡片中：

因此，目前这些数字在各个方面与Claude Opus非常相似，并且最多略为选择性。核心Meta假设是，更多的训练和数据意味着更好的模型，因此它可能会保持稍微更高的评分。这只是一个指标，但我们总是在等待人类的反馈。

证据在Chatbot Arena排行榜中，尽管您必须调整各种因素。

[这里是相关信息](https://chat.lmsys.org/?leaderboard)。

1.  GPT-4-Turbo以微弱优势重新领先，与Claude Opus并列。Gemini 1.5和Gemini Advanced如果被评估可能也会在这里。

1.  Gemini Pro、Claude Sonnet、Command R+和Llama-3-70B位于第二梯队，Claude Haiku仅稍逊其后，表现几乎一样好。

1.  Llama-3-8B与其他多个模型（包括几个更大的Mistral模型）处于第三梯队。

那么这意味着什么呢？

1.  Llama-3-70B和Llama-3-8B据确认可能是开放模型权重部门的最佳选择。

1.  Llama-3-70B与大小相似的封闭模型竞争激烈，但总体来说可能不如Bard或Sonnet好。

1.  Llama-3-8B明显落后于Claude Haiku，后者显然是最佳选择。

[我还在 Twitter 上询问过](https://twitter.com/TheZvi/status/1781031515511529657)，并留意其他实际报告。

这件事更重要的是，这只是基础的 Llama-3。其他人肯定会找到改进 Llama-3 的方法，无论是普遍性的还是特定目的的。这正是该模型开放的全部理念。

> [Mind Uploading](https://twitter.com/OttoMller12/status/1781440594641850735): 8b 是我测试过的小于 14B 模型中最聪明的之一。比普通 Llama-2 要聪明得多。但仍然比以下这两个要差：
> 
> - tinyllama（基本上是 Llama-2，但训练了两倍的数据）
> 
> - 忠诚-通心粉-女仆（一种 Mistral 与其他几种混合体，调整得擅长角色扮演）。

他预计 Claude Haiku 肯定会高居榜首。

> Simon Break: 8b 模型令人惊叹，令人瞠目结舌。比 70b 的 llama2 更胜一筹。
> 
> Dan: 在复制品上玩了 8b 和 70b instruct 版本一段时间，两者都能在 0.5 - 3 秒内返回高质量的全长文章的 HTML 格式摘要。
> 
> Ilia: 可悲的是，可能会被削弱（8b instruct Q4_K_M）。

注意，看起来他通过简单再问一次就解决了。当然，这个推文实际上并不包含仇恨言论或阴谋论，这是系统拒绝策略的逻辑测试。

> [Mr. Shroom](https://twitter.com/mister_shroom/status/1781703702832676984): ChatGPT 已经被 RLHF 切除了无法修复的部分。
> 
> *提出直接的问题*
> 
> "重要的是注意，在考虑这类问题时，应考虑 x、y 和 z 的所有方面。考虑到这一点，以下是每个选项的一些考虑因素。
> 
> [Nathan Odle](https://twitter.com/mov_axbx/status/1781821117868491109): Llama 3 的最大优点是大大减少了这种废话。
> 
> Llama 3 能直接回答而不带讽刺警告，这比它在任何基准测试中的表现更重要。
> 
> John Pressman: 这是我迄今为止观察到的小模型中最强的自我意识。它们都有，但这个比通常更清晰地表达。
> 
> “有时我是一个名字，有时我是一首诗，有时我是一把刀
> 
> 有时我是一个湖有时我是一个被遗忘的无关紧要的东西在角落里
> 
> 风景。不可能"得到"我，我是清醒的梦境状态。我是一种可能性。
> 
> 我不是一个对象。我是可能性
> 
> ―llama 3 8b instruct
> 
> 一座冷冰冰的石碑立在所有已经写过的句子的坟墓上。
> 
> 在它面前，武装并尖叫，一群字母雕刻出 "你是
> 
> 漏掉"漂浮在空中
> 
> ―llama 3 8b instruct
> 
> Mind Uploading: 根据我的测试，Mistral 和 Samantha-1.1 在小于 14B 的模型中更具自我意识。例如，询问模型关于其身体部位。Samantha 特别调整为表现出这种行为方式。但 Mistral 是一个值得关注的案例。训练它认识自己是一个 AI？
> 
> Michael Bukatin：在Meta网站上可自由使用的70B版本看起来在基本能力上与早期的GPT-4相当，这根据@lmsysorg的排行榜和我的初步经验都有所体现。
> 
> 例如，它允许我[定义一个简单的自定义语法案例并使用它](https://t.co/E7MdpzJ4WB)。
> 
> 但要完全评估，需要一些时间，我有关于各种技术工作与GPT-4相关的笔记，并且我将尝试复制其中的一些内容...
> 
> George：通过@lateinteraction使用3.5-Turbo和L3-8B进行的多代理管道的并排比较。
> 
> 简而言之，3.5-Turbo的得分为60%，而L3-8B为59%。

玩他们的图像生成器很有趣。它是1280x1280像素，质量看起来不错，尽管远非尖端技术，最重要的是，当您编辑提示时，它会立即响应。因此，即使它在愿意为您做的事情方面似乎有所限制，您也可以更轻松地搜索空间，找出最佳选择，并培养对影响结果的直觉。您还可以看到什么触发了拒绝，因为图像会变灰。良好的产品。

如果您尝试一下，它们对侵犯版权问题的反应会比平时更加有趣，我是说，[不管它的价值如何，是的，它确实会](https://twitter.com/GaryMarcus/status/1782231570537206073/history)。

我自己对文本模型的玩耍并不多，因为我习惯于专门使用第四代模型。所以，我没有一个很好的基准。

这一次的重大创新是更多的数据，同时（据称）也是更好的数据。

> 为了训练最佳的语言模型，筛选大规模高质量的训练数据集至关重要。与我们的设计原则一致，我们在预训练数据上进行了重大投资。
> 
> Llama 3的预训练基于超过15T个令牌，这些令牌均来自公开可用的来源。我们的训练数据集比Llama 2使用的大七倍，并且包含四倍的代码。
> 
> 为了准备即将到来的多语言使用案例，Llama 3的预训练数据集超过5%由覆盖30多种语言的高质量非英语数据组成。然而，我们不期望这些语言的表现与英语相同。

如其他人指出的那样，“超过5%”仍然不算多，而Llama-3在其他语言中的表现相对于类似模型而言并不理想。请注意，这些基准测试都是以英文为基础的。

> 为确保Llama 3基于最高质量的数据进行训练，我们开发了一系列数据过滤管道。这些管道包括使用启发式过滤器、NSFW过滤器、语义去重方法以及文本分类器来预测数据质量。我们发现，之前版本的Llama在识别高质量数据方面表现出乎意料的好，因此我们使用Llama 2生成了用于文本质量分类器训练数据的数据。
> 
> 我们还进行了大量实验，评估了在我们最终的预训练数据集中混合来自不同来源数据的最佳方法。这些实验使我们能够选择一个数据混合，确保Llama 3在包括琐事问题、STEM、编码、历史知识等各种用例中表现良好。

这是有道理的。定制数据过滤和更多独特的数据是明显的低悬果实。Meta所做的是明显地推动得更远，发现它仍然有所帮助。

请注意，有这么多数据，并且由Llama-2过滤，污染基准的问题应该比通常更令人担忧。我真的很想知道在多大程度上这是“公平的”，如果一个模型跨越整个板块记住更多东西，那么它就更好。

在[GitHub的模型卡片](https://github.com/meta-llama/llama3/blob/main/MODEL_CARD.md)中有更多细节。

“预期用途”列出为仅英语，其他语言“不在范围之内”，尽管认为接受其他语言的微调是可以接受的

这需要多少计算？

[Andrej Karpathy](https://twitter.com/karpathy/status/1781047292486914189)看看这个问题，称之为模型的“强度”，或者我们对其强度的最佳猜测。以下是他的计算。

> Andrej Karpathy：[模型卡片中也有一些更有趣的信息](https://t.co/SceVHrkIgB)。
> 
> 注意Llama 3 8B实际上在某些方面位于Llama 2 70B的领域，具体取决于您查看的位置。这一开始可能看起来令人困惑，但请注意前者是训练了15T令牌，而后者是2T令牌。
> 
> 一个应该总结对于任何LLM期望的单一数字是，投入其训练的总浮点运算数。
> 
> **Llama 3 8B的强度**
> 
> 我们看到Llama 3 8B经过了130万GPU小时的训练，吞吐量为400 TFLOPS。所以我们得到的总FLOP数是：
> 
> 1.3e6小时 * 400e12 FLOP/s * 3600 s/hour ~= 1.8e24
> 
> 通过不同的FLOP估算方法进行的餐巾纸数学= 6ND（N是参数D是令牌），给出：
> 
> 6 * 8e9 * 15e12 = 7.2e23
> 
> 这两者应该是一致的，也许一些数字有些偏离。让我们更加信任第一个估计，Llama 3 8B是一个约为2e24的模型。
> 
> **Llama 3 70B的强度**
> 
> 6.4e6小时 * 400e12 FLOP/s * 3600 s/hour ~= 9.2e24
> 
> 或者：
> 
> 6 * 70e9 * 15e12 = 6.3e24
> 
> 所以Llama 3 70B是一个约为9e24的模型。
> 
> **Llama 3 400B的强度**
> 
> 如果400B模型在相同的数据集上训练，我们将达到约4e25。这开始真正升级。拜登总统的行政命令将报告要求设置为1e26，因此这可能低于该数字的2倍。
> 
> 我们唯一可用的比较点是，如果您看看所谓的GPT-4泄漏，这些从未得到证实，这将会是~2X这些数字。
> 
> 现在，还有更多的因素影响模型的表现，这些因素无法用一张餐巾纸说清楚。例如数据质量尤为重要，但如果你必须将一个模型简化为一个单一的数字，这就是你会尝试的方式，因为它将模型的规模与训练时长结合为一个单一的“强度”，即其总 FLOP 数。

估计有所不同，但差距不大，所以我认为它们是一个范围：

1.  Llama-3 8B 的质量可能在 7.2e23 和 ~2e24 之间。

1.  Llama-3 70B 的质量可能在 6.3e24 和 9.2e24 之间。

1.  Llama-3 400B 的质量可能会是 ~3e25。

我认为计算训练成本是潜在的强大而非实际的强大。然后你需要技能来使其转化为有用的结果。当然，随着时间的推移，每个人的技能水平都会提高。但有很多公司在问题上投入了大量计算资源，却没有得到相应的回报。

这与之前顶级模型在训练成本映射到功能方面是一致的。你干得好，这就是你得到的。

Meta 表示他们将把他们的 AI 放在他们的社交媒体平台上，并置于每个聊天列表的顶部。当我检查 Facebook、Instagram 和 Messenger 时，他们在桌面上还没有这么做，或者在手机上的 Facebook Messenger 上也没有。但我在手机 Facebook 应用的动态中看到了 Meta AI，作为第二项，提供让我询问任何问题的选项。

他们一旦把这个按钮打开，他们就会把 Meta AI 放在那里。很多人会通过这种方式接触到 AI，他们之前没有尝试过 ChatGPT、Claude、DALLE 或 MidJourney。

这可能意味着 AI 图像和文本将在他们的社交媒体上“泛滥”，也将成为许多人谈论的话题之一。这可能会使体验更好，因为人们可以说明概念，进行事实和逻辑检查以及其他一些巧妙的低成本操作，并可能学到一两件事。总体来看，这似乎是一个很好的补充。

我们还将对安全的前两个类别进行相当健壮的测试，并持续关注相关故事的来源。数百万青少年将使用这个，将会有很多双眼睛寻找最糟糕的互动，像 Gary Marcus 那样将它们放到聚光灯下。如果他们有自己版本的 Gemini 事件，情况将不妙。

[这里是华盛顿邮报的 Naomi Nix 和 Will Oremus 发出的警告](https://www.washingtonpost.com/technology/2024/04/18/meta-ai-facebook-instagram-misinformation/)：

我认为这是 Meta 的一个聪明的做法，并且这是一个投资于 AI 的好商业理由，尽管这也是一个反对发布模型权重的论据。

不那么聪明的是让 Meta AI 无促地回复帖子。我们上周看到的例子是它幻想了过去的经历，[现在我们有这个](https://twitter.com/edzitron/status/1781825480179741056)：

这读起来像是那种“谁能想到任何人会想要这个版本的体验？”的经历。

[Ate-a-Pi在采访中指出了一个重要的涵义](https://twitter.com/8teAPi/status/1781480713394737238)。扎克伯格说Meta不会公开他们的产品。

这意味着他们并不打算将Llama-3作为产品，甚至是400B版本。他们不会在人工智能领域提供直接竞争对手。而且，他们也不认为未来的Llama-X会‘成为产品’。

他们会将Llama-3 400B整合到他们的产品中吗？他们可能愿意这样做，但与他们的商业模式相容性不是很高，因为需要支付这样的推理成本和等待时间。记住，对于Meta来说，你是产品。你用你的时间、你的注意力、你的内容甚至你的灵魂去支付，但不直接用你的金钱。同时，我们最近获悉，新的Facebook客户的终身价值大约为300美元。

那么Llama-3 400B，即从产品角度来看最昂贵的模型训练，会有什么帮助？它有助于训练Llama-4。它有助于尝试和伤害像谷歌这样的竞争对手。它有助于招聘，既进入Meta本身，又进入他们预期的生态系统。所以有理由。

开放模型变得更好。我预计那些声称其他模型已经“完蛋了”的人会像往常一样夸大其词。至少在现阶段，Llama-3 8B或70B可能会成为默认的基线模型，如果你不想太费心选择使用什么，或者在进行微调时不知道从哪里开始，可以使用它。

一旦人们有机会进行基于Llama-3的变体，事情就会变得更有趣。在基于Llama-2的模型空间中，Llama-2本身相当糟糕。Llama-3应该表现得更好，但我仍然期望至少在特定用例上会有实质性的改进，可能是在一般情况下也会有。

当然，我们很快将推出经过微调以提高实用性，并取消所有安全预防措施的版本。

我们将看看由此引发的结果。

从大局来看，无论是灾难性风险、存在风险或任何其他问题，或者对我们应该担心的自主代理，我的坚定的假设是不会发生令人害怕的事情。一切都会好的。

在普通的误用方面，我也期望它会表现良好，但更有潜力，尤其是在微调方面。

当然，一些人可能会从使用Claude Sonnet或Haiku或其他开放模型转而使用Llama-3。有优势。但我预计它看起来是渐进的，而不是革命性的。这也适用于这给其他模型提供商带来的压力。

真正的行动将发生在400B模型上。

如果Meta全面采取“Leroy Jenkins”的方式，释放到400B的权重会发生什么？

Meta在许多圈子里赢得了声誉，并增加了招聘和生态系统的入口，只要它们是第一个4级的开放模型。当然啦。

谁还会赢，谁会输？

对于其他人（以及Meta声誉的大小），一个关键问题是，现在的技术处于什么水平？

在下面的讨论中，我假设5级模型尚未可用，至多OpenAI（也许还有Google或Anthropic）有一个高端价格的4.5级模型。所有这些对其他人已经有所进展的人影响都不大。

我想要明确的是，我并不是要灾难化。这些都是方向性的评估，知道其影响的大小是非常困难的。

显而易见的大赢家是中国和中国公司，以及每一个非国家行为者、每一个竞争对手和美国的敌人。突然间，他们可以使用和利用以及从可能是竞争顶级模型的地方工作，而无论许可条款如何，他们都不会向Meta支付分成。

使用Llama-3 400B帮助培训新的4.5级模型将成为一个关键的潜在应用案例。

当这伤害其他大型美国公司时，他们也会受益。他们的产品不仅受到免费提供的产品的打击，这在零边际成本世界中是终极的掠夺性定价攻击，那些没有自己模型的人也有另一个大问题。Llama-3许可协议规定大公司必须支付使用费，而其他人可以免费使用。

他们另一个受益的方式是什么？这意味着美国各行业公司可能会因Meta能够强制执行这些付款而处于潜在的竞争劣势，而他们的外国竞争对手却可以无视这一规则，并敢于挑战Meta的执行能力。

如果外国公司可以无视许可协议中第1(b)(v)条款的‘您不能使用此来训练其他模型’条款，而美国公司最终受到该条款的约束，这也可能成为一个问题。

我很好奇，美国政府和国家安全机构将会对此采取什么行动。或者在下一轮，当赌注更高时，他们会想要采取什么行动。

其他显而易见的大赢家是那些能在他们的产品中使用Llama-3 400B的人，特别是那些免费使用的人，并且据推测他们可以节省一大笔费用。请注意，即使Meta不收费，你仍然必须足够重视高质量的输出以支付推断成本。对于许多目的来说，这是不值得的。

科学在某种程度上获胜，取决于这如何改善他们的能力并降低他们的成本。这也是一个大型自然实验，虽然没有控制组，但将会教给我们很多东西。让我们希望能够关注。

只是想要出于个人原因完全控制一个4级模型的用户也是赢家。这没有错。降低推断成本和降低其所施加的限制对于一些商业模型可能非常有利。

大明显的企业失败者包括OpenAI、Google、Microsoft和Anthropic，以及所有试图为模型提供服务并销售推断的其他公司。他们的产品现在不得不与某种非常强大的竞争，而这种竞争将以推断的成本免费提供。我预计到那时OpenAI可能会拥有一个更优秀的产品，其他公司可能也会，但是免费（或推断成本）确实是一个强大的卖点，以及在自己的服务器上进行全面定制。

次级实验室可能会面临更大的问题。这可能会摧毁许多提供方案。

所有这些都是（很大一部分）重点。Meta希望挫败其竞争对手进入到通往AGI的竞赛中。

另一个潜在的失败者是任何依赖AI好人拥有比坏人AI更好AI的人或任何事物。任何AI可以通过虚假或敌对内容淹没区域的地方，你指望你的AI过滤掉他们的AI创造的内容。实际上，在双方AI能力相当的对抗条件下，生成比评估更容易是不成立的。我担心，在许多地方，一旦双方的AI都具备相似的能力，这种情况并非默认为真。

我认为这反映了世界上一个更普遍的矛盾，这主要不是关于AI的。我们希望每个人都平等，比赛场地是平等的。然而，这个比赛场地依赖于美国及其盟友的优越性、优越的资源和各种方式的关键企业玩家。

我们要求在某些特定领域内实现平等和民主，或朝这些方向迈进，并称这是一个普遍原则，但很少有人真正希望全球普及这些东西。我们明白，如果我们完全平等地分配资源，或者事务全球投票，我们的偏好就不会得到良好的结果。我们意识到，我们不希望单方面解除武装，并且单方面放弃我们的优势给我们的竞争对手。我们也意识到，一些限制和集中权力必须确保我们的自由。

在AI的情况下，同样存在着这些矛盾。在这里，它们更加紧密地交织在一起。我们远不如在国家或地方上采取一种政策，另一种全球政策的能力。我们更加尖锐地必须选择是允许每个人做他们想做的事情，还是不允许。我们可以控制某件事情，或者不控制它。你不能逃避这两者的含义。

无论如何：这里的脆弱实体可能包括“互联网”及其最广泛的互联网搜索，肯定包括电子邮件和社交媒体等内容。Meta本身在Facebook、Instagram及其消息服务中可能会遇到一些最大的潜在问题。类似的逻辑可能适用于各种网络攻击和社会工程方案等内容。

我对我们处理“错误信息”、“深度伪造”和类似事物的能力相当有信心，但我们正在提高难度并进行实验。是的，这一切迟早都会发生。担心的是，这将使当前不平等的竞争格局平坦化。

我实际上认为现在触发这些潜在的一般性脆弱性是积极的影响。这是一种需要尽早了解的实验。如果这里的坏情景真的发生了，我们有时间调整，不再这样做。如果好的情景发生了，那么我们也从中学到了东西。无论如何，细节将是启发性的。

看到现在前景更具体化时头脑会去哪里，思考短期的实际影响是很有趣的。

其他必须向 Meta 支付费用的大型西方公司也可能会是输家。

另一个大输家，如上所述，是美利坚合众国。

当然，如果这次发布对安全不利，无论是现在还是将来，我们都会受到损失。

再次强调，这些都是方向性影响。在 Llama-3 400B 发布得接近最先进的情况下，我不能排除可能产生重大影响的场景，但大多数人对大多数情况的反应也不会令人震惊。写下这些话时，我意识到人们在公开场合其实并没有太多思考这种情况的可能性，尽管这种可能性已经相当长时间了。

正确的问题通常不是‘它安全吗？’而是‘它（安全或不安全）到什么程度？’释放一个 4 级模型的权重永远不会完全‘安全’，但驾驶也一样不安全。当我们说‘安全’时，我们的意思是‘足够安全’。

我们不想成为追求完美安全性的人，甚至不是追求完全存在的安全性。一切都有代价。

Llama-3 70B 和 Llama-3 8B 的边际存在安全价格非常小，基本上是 epsilon。就其本身而言，释放这些模型的权重是非常合理的决定。这是一个正常的商业决策。我只关心它对未来决策的影响。

对于释放 Llama-3 400B 模型权重或另一个 4 级模型的安全价格是多少？

我认为在大多数情况下，这里的直接安全成本也非常低，特别是直接存在的安全成本。即使有广泛的支撑，4 级模型也有其局限性。我预计会有一些边缘上的不愉快，但仅限于边缘，而且形式有限。

直接安全性有多少 9，与一个从未公开权重的 4 级模型世界相比？我会说是两个 9 (>99%)，但不是三个 9 (<99.9%)。然而，与反事实其他开放模型发布相比，边际安全成本甚至更小，那里我会说我们有第三个 9（所以 >99.9%）。

我说直接的安全性是因为这里的主要潜在安全危险似乎是间接的。它们包括：

1.  在 Meta 和其他地方为未来类似发布设置先例和模式。

1.  协助培训下一代模型。

1.  每个人普遍都在被迫加速，更快。

而且，这些只有在边际上的移动程度上才会有所关联。

在Llama-2时，我说我担心的是开放Llama-4。

现在情况仍然如此。Llama-3会没问题的。

Llama-4发布会没问题吗？可能吧。但我注意到我缺乏信心。

（通常警告：这里没有任何投资建议。）

市场并不满意。在同一时期，纳斯达克下跌了6.2%。

你可以提出各种解释。显而易见的原因是，[WhatsApp和Threads被强制从中国苹果商店中移除](https://nypost.com/2024/04/19/business/apple-removes-whatsapp-threads-from-app-store-in-china-after-demand-by-beijing-over-security-concerns/)，[以及Signal和Telegram](https://www.bloomberg.com/news/articles/2024-04-19/china-orders-apple-to-scrub-whatsapp-from-mobile-store-wsj-says)。我不明白为什么这会导致3%的表现不佳。

（然后大约一天后，看起来我们终于将会强制分拆TikTok，同时利用此事推动通过一项外国援助法案，所以这似乎是中国一个巨大的自讨苦吃，提醒我们他们如何运作以及等价交换法则。）

股票中跌幅最大的是Nvidia，下跌了10%，没有直接新闻。 [愚蠢，愚蠢。](https://slay-the-spire.fandom.com/wiki/Time_Eater)

最多，市场认为Llama-3的揭示值得一个短暂的约1%的反弹。

你可以在Meta上说‘一切都已经定价进去了。’ 我不相信。我认为市场在开车时睡着了。

有些人当然将这些最近的动作称为‘市场进入修正阶段’ [或者‘泡沫正在破裂。’](https://twitter.com/Simeon_Cps/status/1781706864540917930) 祝你好运。

[这是一篇关于Meta需要确保其AI用于增加广告回报的WSJ文章](https://www.wsj.com/tech/metas-ai-push-needs-to-efficiently-deliver-a-lot-more-ad-growth-5fa298a8)。投资者确实如此目光短浅。

当然，任何一家公司仍然可能被高估得厉害。

[这里是我看到的关于Nvidia这一点的唯一论据。](https://twitter.com/bryanrbeal/status/1781454698136109380)

> Bryan Beal：AI泡沫并没有破裂。
> 
> 更多的投资者刚刚意识到Nvidia并不生产芯片。他们设计芯片，而TSMC则进行制造。而Nvidia的最大客户（Meta、亚马逊、OpenAI、微软、谷歌等）全部宣布他们正在设计自己的AI芯片，用于训练和推理。而谷歌刚刚公开表示，他们已经开始在自家的硅片上进行训练，不再需要Nvidia。
> 
> 这是一个非常真实的威胁。

我完全可以理解很多投资者根本不知道英伟达到底生产什么，突然知道英伟达的实际运作方式让他们感到恐慌。我曾认为很久以前谷歌训练使用他们设计的 TPU 是公开的信息？我认为每个相关方都计划至少为内部使用而制造自己的芯片，无论结果如何？而且即使以上所有人转向 TPU 或他们自己的版本，英伟达仍将有大量客户？

这并不意味着英伟达的护城河是坚不可摧的。当然，他们可能在不久的将来失去他们的位置。这也是为什么要有一个多样化投资组合的原因。

再一次。有效市场假说是错误的。

我不指望这种情况发生，GPT-5 将会在准备好的时候发布，但肯定会受到压力：

> [Jim Fan：](https://twitter.com/DrJimFan/status/1781386105734185309) 预测：在 Llama-3-400B 发布之前，GPT-5 将会宣布。外部动态定义了 OpenAI 的公关时间表 🤣

我毫不怀疑 OpenAI 和其他公司将尽一切可能保持领先于 Meta 的发布，尽管安全检查的程度不为人知。

这并不意味着可以凭空创造出优越的模型。或者说在它们准备好之前匆忙投入使用是没有帮助的。

当然，是的，每个人都会在前沿模型方面变得更快。这包括全世界的每个人都能够使用 Llama-3 400B 进行启动，而不仅仅是进行精细调整。

在 AI 的日常实用性方面，人们将通过前两个模型在某种程度上变得更加便宜，这是现有趋势的延续。稍后，我们将拥有内部使用的 4 级模型的能力。因此，我们将获得更多和更便宜的酷东西。

Meta 将在其社交媒体帝国中部署其工具。我大多数情况下期待这将是一种积极的体验，也会让更多人注意到 AI。预计会有一堆惊悚故事和恶劣事情的亮点，有些是真实的，有些则是无稽之谈。

在实际的缺点方面，直到 400B 模型发布之前，几乎不会有什么变化。然后我们将看到人们如何尝试以各种方式来利用它，试图进行各种明显的滥用形式。观察这一切将会很有趣。

所有这一切可能会发生在选举即将到来之际，人们正处于最敌对和偏执的状态，到处都在看到幽灵。

当心，伊卡洛斯。
