- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-05-27 13:30:29'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: apple/OpenELM · Hugging Face
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://huggingface.co/apple/OpenELM](https://huggingface.co/apple/OpenELM)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[](#openelm-an-efficient-language-model-family-with-open-training-and-inference-framework)OpenELM:
    An Efficient Language Model Family with Open Training and Inference Framework'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '*Sachin Mehta, Mohammad Hossein Sekhavat, Qingqing Cao, Maxwell Horton, Yanzi
    Jin, Chenfan Sun, Iman Mirzadeh, Mahyar Najibi, Dmitry Belenko, Peter Zatloukal,
    Mohammad Rastegari*'
  prefs: []
  type: TYPE_NORMAL
- en: We introduce **OpenELM**, a family of **Open** **E**fficient **L**anguage **M**odels.
    OpenELM uses a layer-wise scaling strategy to efficiently allocate parameters
    within each layer of the transformer model, leading to enhanced accuracy. We pretrained
    OpenELM models using the [CoreNet](https://github.com/apple/corenet) library.
    We release both pretrained and instruction tuned models with 270M, 450M, 1.1B
    and 3B parameters.
  prefs: []
  type: TYPE_NORMAL
- en: Our pre-training dataset contains RefinedWeb, deduplicated PILE, a subset of
    RedPajama, and a subset of Dolma v1.6, totaling approximately 1.8 trillion tokens.
    Please check license agreements and terms of these datasets before using them.
  prefs: []
  type: TYPE_NORMAL
- en: 'See the list below for the details of each model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '[](#usage)Usage'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We have provided an example function to generate output from OpenELM models
    loaded via [HuggingFace Hub](https://huggingface.co/docs/hub/) in `generate_openelm.py`.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can try the model by running the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Please refer to [this link](https://huggingface.co/docs/hub/security-tokens)
    to obtain your hugging face access token.
  prefs: []
  type: TYPE_NORMAL
- en: 'Additional arguments to the hugging face generate function can be passed via
    `generate_kwargs`. As an example, to speedup the inference, you can try [lookup
    token speculative generation](https://huggingface.co/docs/transformers/generation_strategies)
    by passing the `prompt_lookup_num_tokens` argument as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Alternatively, try model-wise speculative generation with an [assistive model](https://huggingface.co/blog/assisted-generation)
    by passing a smaller model through the `assistant_model` argument, for example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '[](#main-results)Main Results'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](#zero-shot)Zero-Shot'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[](#llm360)LLM360'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[](#openllm-leaderboard)OpenLLM Leaderboard'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '| **Model Size** | **ARC-c** | **CrowS-Pairs** | **HellaSwag** | **MMLU** |
    **PIQA** | **RACE** | **TruthfulQA** | **WinoGrande** | **Average** |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| [OpenELM-270M](https://huggingface.co/apple/OpenELM-270M) | 27.65 | **66.79**
    | 47.15 | 25.72 | 69.75 | 30.91 | **39.24** | **53.83** | 45.13 |'
  prefs: []
  type: TYPE_TB
- en: '| [OpenELM-270M-Instruct](https://huggingface.co/apple/OpenELM-270M-Instruct)
    | **32.51** | 66.01 | **51.58** | **26.70** | **70.78** | 33.78 | 38.72 | 53.20
    | **46.66** |'
  prefs: []
  type: TYPE_TB
- en: '| [OpenELM-450M](https://huggingface.co/apple/OpenELM-450M) | 30.20 | **68.63**
    | 53.86 | **26.01** | 72.31 | 33.11 | 40.18 | 57.22 | 47.69 |'
  prefs: []
  type: TYPE_TB
- en: '| [OpenELM-450M-Instruct](https://huggingface.co/apple/OpenELM-450M-Instruct)
    | **33.53** | 67.44 | **59.31** | 25.41 | **72.63** | **36.84** | **40.48** |
    **58.33** | **49.25** |'
  prefs: []
  type: TYPE_TB
- en: '| [OpenELM-1_1B](https://huggingface.co/apple/OpenELM-1_1B) | 36.69 | **71.74**
    | 65.71 | **27.05** | **75.57** | 36.46 | 36.98 | 63.22 | 51.68 |'
  prefs: []
  type: TYPE_TB
- en: '| [OpenELM-1_1B-Instruct](https://huggingface.co/apple/OpenELM-1_1B-Instruct)
    | **41.55** | 71.02 | **71.83** | 25.65 | 75.03 | **39.43** | **45.95** | **64.72**
    | **54.40** |'
  prefs: []
  type: TYPE_TB
- en: '| [OpenELM-3B](https://huggingface.co/apple/OpenELM-3B) | 42.24 | **73.29**
    | 73.28 | **26.76** | 78.24 | **38.76** | 34.98 | 67.25 | 54.35 |'
  prefs: []
  type: TYPE_TB
- en: '| [OpenELM-3B-Instruct](https://huggingface.co/apple/OpenELM-3B-Instruct) |
    **47.70** | 72.33 | **76.87** | 24.80 | **79.00** | 38.47 | **38.76** | **67.96**
    | **55.73** |'
  prefs: []
  type: TYPE_TB
- en: See the technical report for more results and comparison.
  prefs: []
  type: TYPE_NORMAL
- en: '[](#evaluation)Evaluation'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](#setup)Setup'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Install the following dependencies:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '[](#evaluate-openelm)Evaluate OpenELM'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: '[](#bias-risks-and-limitations)Bias, Risks, and Limitations'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The release of OpenELM models aims to empower and enrich the open research community
    by providing access to state-of-the-art language models. Trained on publicly available
    datasets, these models are made available without any safety guarantees. Consequently,
    there exists the possibility of these models producing outputs that are inaccurate,
    harmful, biased, or objectionable in response to user prompts. Thus, it is imperative
    for users and developers to undertake thorough safety testing and implement appropriate
    filtering mechanisms tailored to their specific requirements.
  prefs: []
  type: TYPE_NORMAL
- en: '[](#citation)Citation'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'If you find our work useful, please cite:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
