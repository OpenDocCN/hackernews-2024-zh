- en: <!--yml
  id: totrans-split-0
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  id: totrans-split-1
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-05-27 12:48:37'
  id: totrans-split-2
  prefs: []
  type: TYPE_NORMAL
- en: -->
  id: totrans-split-3
  prefs: []
  type: TYPE_NORMAL
- en: There were no ancient computers and it's fine
  id: totrans-split-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://lcamtuf.substack.com/p/there-were-no-ancient-computers-and](https://lcamtuf.substack.com/p/there-were-no-ancient-computers-and)
  id: totrans-split-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Last August, in an article about the [history of the calculator](https://lcamtuf.substack.com/p/a-brief-history-of-counting-stuff),
    I opened with the following quip:'
  id: totrans-split-6
  prefs: []
  type: TYPE_NORMAL
- en: '*"I find it difficult to talk about the history of the computer. The actual
    record is dreadfully short: almost nothing of consequence happened before the
    year 1935\. We keep looking for a better story, but we inevitably end up grasping
    at straws.*'
  id: totrans-split-7
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '*Just look at what we’ve done so far. The “father of the computer” is no longer
    Konrad Zuse (Z1, 1938) or John Mauchly (ENIAC, 1943). Somehow, we pivoted to Charles
    Babbage — a 19th century polymath who never constructed such a device, and had
    no luck inspiring others to try. Not content with this injustice, we also turned
    “computing” into a meaningless word. On Wikipedia, the timeline of computer hardware
    includes mechanical clocks, dolls, weaving looms, and a miniature chariot from
    910 BCE. It’s historical synthesis run amok."*'
  id: totrans-split-8
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'My point was simple: if a cuckoo clock or a differential gear qualifies as
    a “proto-computer”, what *doesn’t* meet the bar? Almost any man-made tool performs
    some sort of a calculation. Let’s take a crowbar: it’s an instrument for multiplying
    force by a preset amount. Sure, it might not be Turing-complete, but it’s getting
    there!'
  id: totrans-split-9
  prefs: []
  type: TYPE_NORMAL
- en: I jest, but I wasn’t arguing for semantic purity just for the sake of it. My
    concern was that when we paint with overly broad strokes, much of the important
    detail is lost. Consider that Charles Babbage — the seemingly undisputed father
    of modern computing — had zero actual impact on the development of the computer;
    it can be quite illuminating to ponder why. Heck, even Alan Turing and John von
    Neumann — the two intellectual titans who laid the theoretical groundwork for
    computer science — probably weren’t the ones who made the early computers “click.”
  id: totrans-split-10
  prefs: []
  type: TYPE_NORMAL
- en: 'Perhaps my beef is with the terminology itself: the word “computer” is imprecise
    and its meaning has evolved over time. That said, in contemporary usage, there’s
    no doubt that it means something more than a differential gear, an abacus, or
    a slide rule.'
  id: totrans-split-11
  prefs: []
  type: TYPE_NORMAL
- en: '*A computer? Thales Model A, circa 1912\. From author’s collection.*'
  id: totrans-split-12
  prefs: []
  type: TYPE_NORMAL
- en: 'Yet, absent a clear definition, many historians apply the term to just about
    any calculating device they see. In this view, we’ve been using computers for
    millennia. The designs ranged from simple mechanical registers (such as the abacus),
    to automated lookup tables (slide rule), to clockwork contraptions such as the
    four-operation calculator pictured above. Move over, Charles Babbage: the general
    operating principle behind that last device has been known at least since the
    1600s — although it wasn’t until the late 19th century that we learned how to
    mass-produce such mechanisms.'
  id: totrans-split-13
  prefs: []
  type: TYPE_NORMAL
- en: Again, it’s not that I wish to pointlessly debate semantics; my problem with
    this approach is that it obscures the nature of the revolution that happened after
    1935\. A better definition of a computer would include not just the words “designed
    for calculation” but also “programmable.” In other words, the device should consist
    not only of an arithmetic logic unit (ALU), but also a sequencing mechanism that
    activates different subsystems — and passes data around — according to a flexible,
    user-specified plan.
  id: totrans-split-14
  prefs: []
  type: TYPE_NORMAL
- en: By that standard, all the non-programmable calculating devices — from notched
    sticks to the Atanasoff-Berry “computer” of 1942 — ought to be disqualified. The
    first true computers would probably be the creations of Konrad Zuse, followed
    by Howard Aiken’s Harvard Mark I; both of these designs cropped up in the late
    1930s.
  id: totrans-split-15
  prefs: []
  type: TYPE_NORMAL
- en: 'Some computer science nerds might also be tempted to add “Turing-complete”
    to the list of requirements. By that criterion, the ENIAC — constructed by John
    Mauchly and J. Presper Eckert in 1945 — gets to claim primacy. That said, it’s
    a wonky metric: many devices that are Turing-complete aren’t practical computers.
    Conversely, Konrad Zuse’s Z3 — although not 100% Turing-complete in the conventional
    sense — was unmistakably the real deal.'
  id: totrans-split-16
  prefs: []
  type: TYPE_NORMAL
- en: '*Opcode handling in Zuse’s Z3\. From the Konrad Zuse Internet Archive.*'
  id: totrans-split-17
  prefs: []
  type: TYPE_NORMAL
- en: The popular answer to this question is rather hand-wavy. You learn about Charles
    Babbage as the inventor of the programmable computer, and then Alan Turing and
    John von Neumann as the duo who teamed up (?) to make Mr. Babbage’s dream come
    true. Such pervasive is the cult of the three personalities that even in the Wikipedia
    article on Harvard Mark I, Mr. Babbage and Mr. von Neumann are name-dropped before
    any mention Mr. Aiken, the actual inventor of the device.
  id: totrans-split-18
  prefs: []
  type: TYPE_NORMAL
- en: Of course, this story doesn’t quite add up. Charles Babbage was a successful
    designer of advanced and elegant mechanical calculators, but such devices existed
    long before his time. So did commercially-successful programmable mechanisms,
    notably including the punch-card-operated weaving loom designed by Joseph Marie
    Jacquard in 1804.
  id: totrans-split-19
  prefs: []
  type: TYPE_NORMAL
- en: The bottleneck wasn’t conceptual; it was technical. We knew how to do calculation
    and we knew how to do the sequencing of operations; the challenge was the effortless
    propagation of data from one portion of the device to another. This was nearly
    impossible to do with sufficient flexibility, reliability, and scalability in
    the domain of sprockets, cams, and pawls. It’s the reason why Mr. Babbage’s Analytical
    Engine proved to be nothing but a pipe dream — and why even today, we don’t see
    copies of his grand design popping up left and right.
  id: totrans-split-20
  prefs: []
  type: TYPE_NORMAL
- en: It follows that some of the most important names in early computing might not
    be Charles Babbage, Alan Turing, or John von Neumann — but the people who came
    up with early forms of electronic memory and thus made it simple to store and
    propagate data within increasingly complex calculating devices. In particular,
    the inventors of the humble yet enduring flip-flop circuit — William Eccles and
    Frank Wilfred Jordan — deserve far more recognition than they have ever gotten
    for their work.
  id: totrans-split-21
  prefs: []
  type: TYPE_NORMAL
- en: 'Well, that’s a good question! Perhaps it’s this: in a world where the era of
    programmable computers begins with Zuse’s Z1 or Harvard Mark I, there’s still
    plenty of interesting stories to tell. In a world where it all goes back to the
    wheel and the pointed stick, the actual challenges and the ingenuity that went
    into overcoming the issues is easy to miss.'
  id: totrans-split-22
  prefs: []
  type: TYPE_NORMAL
- en: 'Of course, I get it: we want to be inspired. We imagine Charles Babbage as
    a misunderstood Victorian scientist in the mold of Nikola Tesla. We lament the
    fate of Alan Turing, a WWII hero betrayed by his country. We look up to Ada Lovelace,
    a role model for women engineers around the globe. These are good stories; we
    should keep telling them. In comparison, Konrad Zuse’s Nazi-era biopic might not
    be a shoo-in.'
  id: totrans-split-23
  prefs: []
  type: TYPE_NORMAL
- en: '*A microscope photo of Ampex high-density core memory, 1970\. By author.*'
  id: totrans-split-24
  prefs: []
  type: TYPE_NORMAL
- en: '*For a another pretty solid if provocative take on the history of computing,
    check out [Rob Graham’s “deprogrammer”](https://cybersect.substack.com/p/a-computer-history-deprogrammer).
    For more of my articles about electronics, [click here](https://lcamtuf.coredump.cx/offsite.shtml).*'
  id: totrans-split-25
  prefs: []
  type: TYPE_NORMAL
