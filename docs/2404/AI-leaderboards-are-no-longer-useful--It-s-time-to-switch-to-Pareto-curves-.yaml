- en: <!--yml
  id: totrans-split-0
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  id: totrans-split-1
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-05-27 13:40:45'
  id: totrans-split-2
  prefs: []
  type: TYPE_NORMAL
- en: -->
  id: totrans-split-3
  prefs: []
  type: TYPE_NORMAL
- en: AI leaderboards are no longer useful. It's time to switch to Pareto curves.
  id: totrans-split-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://www.aisnakeoil.com/p/ai-leaderboards-are-no-longer-useful](https://www.aisnakeoil.com/p/ai-leaderboards-are-no-longer-useful)
  id: totrans-split-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '*By Sayash Kapoor, [Benedikt Stroebl](https://citp.princeton.edu/citp-people/benedikt-strobl/),
    Arvind Narayanan*'
  id: totrans-split-6
  prefs: []
  type: TYPE_NORMAL
- en: Which is the most accurate AI system for generating code? Surprisingly, there
    isn’t currently a good way to answer questions like these.
  id: totrans-split-7
  prefs: []
  type: TYPE_NORMAL
- en: Based on [HumanEval](https://paperswithcode.com/sota/code-generation-on-humaneval),
    a widely used benchmark for code generation, the most accurate publicly available
    system is [LDB](https://arxiv.org/abs/2402.16906) (short for LLM debugger). But
    there’s a catch. The most accurate generative AI systems, including LDB, tend
    to be agents, which repeatedly invoke language models like GPT-4\. That means
    they can be orders of magnitude more costly to run than the models themselves
    (which are already pretty costly). If we eke out a 2% accuracy improvement for
    100x the cost, is that really better?
  id: totrans-split-8
  prefs: []
  type: TYPE_NORMAL
- en: 'In this post, we argue that:'
  id: totrans-split-9
  prefs: []
  type: TYPE_NORMAL
- en: AI agent accuracy measurements that don’t control for cost aren’t useful.
  id: totrans-split-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pareto curves can help visualize the accuracy-cost tradeoff.
  id: totrans-split-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Current state-of-the-art agent architectures are complex and costly but no more
    accurate than extremely simple baseline agents that cost 50x less in some cases.
  id: totrans-split-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Proxies for cost such as parameter count are misleading if the goal is to identify
    the best system for a given task. We should directly measure dollar costs instead.
  id: totrans-split-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Published agent evaluations are difficult to reproduce because of a lack of
    standardization and questionable, undocumented evaluation methods in some cases.
  id: totrans-split-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: LLMs are stochastic. Simply calling a model [many](https://arxiv.org/abs/2402.05120)  [times](https://arxiv.org/pdf/2403.02419.pdf)
    and outputting the most common answer can increase accuracy.
  id: totrans-split-15
  prefs: []
  type: TYPE_NORMAL
- en: On some tasks, there is seemingly no limit to the amount of inference compute
    that can improve accuracy. Google Deepmind's [AlphaCode](https://arxiv.org/pdf/2203.07814.pdf),
    which improved accuracy on automated coding evaluations, showed that this trend
    holds even when calling LLMs millions of times.
  id: totrans-split-16
  prefs: []
  type: TYPE_NORMAL
- en: '*The accuracy of [AlphaCode](https://arxiv.org/pdf/2203.07814.pdf) on coding
    tasks continues to improve even after making a million calls to the underlying
    model (the different curves represent varying parameter counts). Accuracy is measured
    by how often one of the top 10 answers generated by the model is correct.*'
  id: totrans-split-17
  prefs: []
  type: TYPE_NORMAL
- en: 'A useful evaluation of agents must therefore ask: What did it cost? If we don’t
    do cost-controlled comparisons, it will encourage researchers to develop extremely
    costly agents just to claim they topped the leaderboard.'
  id: totrans-split-18
  prefs: []
  type: TYPE_NORMAL
- en: In fact, when we evaluate agents that have been proposed in the last year for
    solving coding tasks, we find that visualizing the tradeoff between cost and accuracy
    yields surprising insights.
  id: totrans-split-19
  prefs: []
  type: TYPE_NORMAL
- en: 'We re-evaluated the accuracy of three agents that have been claimed to occupy
    top spots on the HumanEval leaderboard: [LDB](https://arxiv.org/pdf/2402.16906v3.pdf),
    [LATS](https://arxiv.org/pdf/2310.04406v2.pdf), and [Reflexion](https://arxiv.org/pdf/2303.11366.pdf).
    We also evaluated the cost and time requirements of running these agents.'
  id: totrans-split-20
  prefs: []
  type: TYPE_NORMAL
- en: These agents rely on running the code generated by the model, and if it fails
    the test cases provided with the problem description, they try to debug the code,
    look at alternative paths in the code generation process, or "reflect" on why
    the model's outputs were incorrect before generating another solution.
  id: totrans-split-21
  prefs: []
  type: TYPE_NORMAL
- en: 'In addition, we calculated the accuracy, cost, and running time of  a few simple
    baselines:'
  id: totrans-split-22
  prefs: []
  type: TYPE_NORMAL
- en: '**GPT-3.5** and **GPT-4** models (zero shot; no agent architecture)'
  id: totrans-split-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Retry:** We repeatedly invoke a model with the temperature set to zero, up
    to five times, if it fails the test cases provided with the problem description.
    Retrying makes sense because LLMs [aren’t deterministic](https://community.openai.com/t/run-same-query-many-times-different-results/140588#:~:text=Apr%202023-,OpenAI%20models%20are%20non%2Ddeterministic%2C%20meaning%20that%20identical%20inputs%20can%20yield,amount%20of%20variability%20may%20remain%20due%20to%20GPU%20floating%20point%20math.,-Solution)
    even at temperature zero.'
  id: totrans-split-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Warming:** This is the same as the retry strategy, but we gradually increase
    the temperature of the underlying model with each run, from 0 to 0.5\. This increases
    the stochasticity of the model and, we hope, increases the likelihood that at
    least one of the retries will succeed.'
  id: totrans-split-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Escalation:** We start with a cheap model (Llama-3 8B) and escalate to more
    expensive models (GPT-3.5, Llama-3 70B, GPT-4) if we encounter a test case failure.'
  id: totrans-split-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Surprisingly, we are not aware of any papers that compare their proposed agent
    architectures with any of the latter three simple baselines.
  id: totrans-split-27
  prefs: []
  type: TYPE_NORMAL
- en: 'Our most striking result is that **agent architectures for HumanEval do not
    outperform our simpler baselines despite costing more**. In fact, agents differ
    drastically in terms of cost: for substantially similar accuracy, the cost can
    differ by almost two orders of magnitude! Yet, the cost of running these agents
    isn''t a top-line metric reported in any of these papers.'
  id: totrans-split-28
  prefs: []
  type: TYPE_NORMAL
- en: '*Our simple baselines offer Pareto improvements over existing agent architectures. We
    run each agent five times and report the mean accuracy and the mean total cost
    on the 164 HumanEval problems. Where results for LDB have two models/agents in
    parenthesis, they indicate the language model or agent used to generate the code,
    followed by the language model used to debug the code. Where they have just one,
    they indicate that the same model was used to both generate the code and debug
    it. Note that the y-axis is shown from 0.7 to 1; figures with the full axis (0
    to 1) and error bars, robustness checks, and other details about our empirical
    results are included in the [appendix](https://www.cs.princeton.edu/~sayashk/papers/ai-leaderboards-appendix.pdf). *'
  id: totrans-split-29
  prefs: []
  type: TYPE_NORMAL
- en: There is no significant accuracy difference between the warming strategy and
    the best-performing agent architecture. Yet, Reflexion and LDB cost over 50% more
    than the warming strategy, and LATS over 50 times more (all these costs are entirely
    or predominantly from calls to GPT-4, so these ratios will be stable even if model
    costs change).  Meanwhile, the escalation strategy strictly improves accuracy
    while costing less than half of LDB (GPT-3.5) at current inference prices.
  id: totrans-split-30
  prefs: []
  type: TYPE_NORMAL
- en: 'Our results point to another underlying problem: papers making claims about
    the usefulness of agents have so far failed to test if simple agent baselines
    can lead to similar accuracy. This has led to widespread beliefs among AI researchers
    that complex ideas like planning, reflection, and debugging are responsible for
    accuracy gains. In fact, Lipton and Steinhardt noted a trend in the AI literature
    of failing to identify the sources of empirical gains [back in 2018](https://arxiv.org/abs/1807.03341).'
  id: totrans-split-31
  prefs: []
  type: TYPE_NORMAL
- en: Based on our findings, the question of whether debugging, reflection, and other
    such “System 2” approaches are useful for code generation remains open. It is
    possible that they will be useful on harder programming tasks than those represented
    in HumanEval. For now, the over-optimism about System 2 approaches is exacerbated
    by a lack of reproducibility and standardization that we report below.
  id: totrans-split-32
  prefs: []
  type: TYPE_NORMAL
- en: 'At first glance, reporting dollar costs is jarring. It breaks many properties
    of benchmarking that we take for granted: that measurements don’t change over
    time (whereas costs tend to come down) and that different models compete on a
    level playing field (whereas some developers may [benefit](https://papers.nips.cc/paper_files/paper/2023/file/d1a14493e5f84d6c6129414f0cd1a7c6-Paper-Conference.pdf)
    from economies of scale, leading to lower inference costs). Because of this, researchers
    usually pick a different axis for the Pareto curve, such as parameter count.'
  id: totrans-split-33
  prefs: []
  type: TYPE_NORMAL
- en: The downsides of reporting costs are real, but we describe below how they can
    be mitigated. More importantly, we think using attributes like parameter count
    as a proxy for cost is a mistake and doesn’t solve the problem it’s intended to
    solve. To understand why, we need to introduce a conceptual distinction.
  id: totrans-split-34
  prefs: []
  type: TYPE_NORMAL
- en: AI evaluations serve at least two distinct purposes. Model developers and AI
    researchers use them to identify which changes to the training data and architecture
    improve accuracy. We call this [model evaluation](https://arxiv.org/pdf/2211.09110).
    And downstream developers, such as programmers who use AI to build consumer-facing
    products, use evaluations to decide which AI systems to use in their products.
    We call this [downstream evaluation](https://www.arthur.ai/product/bench).
  id: totrans-split-35
  prefs: []
  type: TYPE_NORMAL
- en: The difference between model evaluation and downstream evaluation is underappreciated.
    This has led to much confusion about how to factor in the cost of running AI.
  id: totrans-split-36
  prefs: []
  type: TYPE_NORMAL
- en: 'Model evaluation is a scientific question of interest to researchers. So it
    makes sense to stay away from dollar costs for the aforementioned reasons. Instead,
    controlling for compute is a reasonable approach: if we normalize the amount of
    compute used to train a model, we can then understand if factors like architectural
    changes or changes in the data composition are responsible for improvements, as
    opposed to more compute. Notably, Nathan Lambert [argues](https://www.interconnects.ai/p/compute-efficient-open-llms)
    that many of the accuracy gains in the last year (such as Meta''s Llama 2) are
    simply consequences of using more compute.'
  id: totrans-split-37
  prefs: []
  type: TYPE_NORMAL
- en: On the other hand, downstream evaluation is an engineering question that helps
    inform a procurement decision. Here, cost is the actual construct of interest.
    The downsides of cost measurement aren’t downsides at all; they are exactly what’s
    needed. Inference costs do come down over time, and that greatly matters to downstream
    developers. It is unnecessary and counterproductive for the evaluation to stay
    frozen in time.
  id: totrans-split-38
  prefs: []
  type: TYPE_NORMAL
- en: In this context, proxies for cost (such as the number of active parameters or
    amount of compute used) are misleading. For example, Mistral [released](https://mistral.ai/news/mixtral-8x22b/)
    the figure below alongside their latest model, Mixtral 8x22B, to explain why developers
    should choose it over competitors.
  id: totrans-split-39
  prefs: []
  type: TYPE_NORMAL
- en: '*Substituting active parameters as a proxy for cost is misleading. Source:
    [Mistral](https://mistral.ai/news/mixtral-8x22b/).*'
  id: totrans-split-40
  prefs: []
  type: TYPE_NORMAL
- en: In this figure, the number of active parameters is a poor proxy for cost. On
    [Anyscale](https://docs.endpoints.anyscale.com/pricing/), Mixtral 8x7B costs twice
    as much as Llama 2 13B, yet Mistral's figure shows it costs about the same, because
    they only consider the number of active parameters. Of course, downstream developers
    don't care about the number of active parameters when they're using an API. They
    simply care about the dollar cost relative to accuracy. Mistral chose “active
    parameters” as a proxy, presumably because it makes their models look better than
    dense models such as Meta’s Llama and Cohere’s Command R+. If we start using proxies
    for cost, every model developer can pick a proxy that makes their model look good.
  id: totrans-split-41
  prefs: []
  type: TYPE_NORMAL
- en: Some hurdles to cost evaluation remain. Different providers can charge different
    amounts for [the](https://docs.endpoints.anyscale.com/pricing/)  [same](https://www.together.ai/pricing)  [model](https://replicate.com/pricing),
    the cost of an API call might change [overnight](https://openai.com/blog/new-models-and-developer-products-announced-at-devday),
    and cost might vary based on model developer decisions, such as whether [bulk
    API](https://twitter.com/OpenAIDevs/status/1779922566091522492?ref_src=twsrc%5Etfw%7Ctwcamp%5Etweetembed%7Ctwterm%5E1779922566091522492%7Ctwgr%5E100b538f87ce81b828a23634fa12d127c126b5ba%7Ctwcon%5Es1_c10&ref_url=https%3A%2F%2Fwww.theverge.com%2F2024%2F4%2F15%2F24131401%2Fopenai-will-give-you-a-50-percent-discount-for-off-peak-gpt-use)
    calls are charged differently. These downsides can be partly addressed by making
    the evaluation results customizable using mechanisms to adjust the cost of running
    models, i.e., providing users the option to adjust the cost of input and output
    tokens for their provider of choice to recalculate the tradeoff between cost and
    accuracy. In turn, downstream evaluations of agents should include input/output
    token counts in addition to dollar costs, so that anyone looking at the evaluation
    in the future can instantly recalculate the cost using current prices.
  id: totrans-split-42
  prefs: []
  type: TYPE_NORMAL
- en: But ultimately, despite the hurdles, good measurement requires modeling the
    underlying construct of interest. For downstream evaluations, that underlying
    construct is cost. All other proxies are lacking.
  id: totrans-split-43
  prefs: []
  type: TYPE_NORMAL
- en: In the course of our evaluation, we found many shortcomings in the reproducibility
    and standardization of agent evaluations.
  id: totrans-split-44
  prefs: []
  type: TYPE_NORMAL
- en: '**We were unable to reproduce the results of the LATS and LDB agents on HumanEval.**
    In particular, across all 5 runs for LDB (Reflexion, GPT-3.5), the maximum accuracy
    was 91.5%, much lower than the 95.1% reported in the paper. The maximum accuracy
    of LATS across all five runs was similarly lower, at 91.5% instead of 94.4%.'
  id: totrans-split-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Similarly, the accuracy for the baseline GPT-4 model reported in the LDB paper
    is drastically lower than our reproduction of the paper's code (75.0% vs. a mean
    of 89.6% across five runs). In fact, according to the paper, the GPT-3.5 and GPT-4
    models perform very similarly (73.9% vs. 75.0%). Weak baselines could give a false
    sense of the amount of improvement attributable to the agent architecture.
  id: totrans-split-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The LATS agent was evaluated on only a [subset](https://github.com/andyz245/LanguageAgentTreeSearch/blob/554886901183a9908183d2cb104c3088c493650a/programming/mcts.py#L138C1-L138C29)
    of the test cases provided in the HumanEval benchmark. This exaggerated their
    accuracy numbers, since the code for a particular HumanEval problem might be incorrect,
    but if it passes only a portion of the test cases for that problem, it could still
    be marked as correct. In our analysis, this was responsible for a 3% difference
    in accuracy (mean across five runs), which explains a substantial part of the
    difference between the accuracy we found and the one reported in the paper. In
    addition, many details about the implementation, such as hyperparameter values,
    were not reported in the paper or GitHub repository (see [appendix](https://www.cs.princeton.edu/~sayashk/papers/ai-leaderboards-appendix.pdf)
    for details).
  id: totrans-split-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To the best of our knowledge, this post is the first time the four agents with
    the highest accuracy—Retry, Warming, LDB (GPT-4), and LDB (GPT-4 + Reflexion)—have
    been tested on HumanEval.
  id: totrans-split-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Reflexion, LDB, and LATS all use different subsets of HumanEval.** Three
    (out of 164) coding problems in the original version of HumanEval lack example
    tests. Since these agents require example tests to debug or rerun their solutions,
    Reflexion removes the three problems that don''t have example tests. LATS removes
    these three problems, plus another problem, for unreported reasons. LDB adds example
    tests for the three problems that are missing in the original benchmark. **None
    of the three papers reports this.** The paper introducing LATS claims (incorrectly):
    "We use all 164 problems for our experiments." In our analysis, we conducted all
    evaluations on the version of the benchmark provided by LDB, since it contains
    example tests for all problems.'
  id: totrans-split-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The LDB paper claims to use GPT-3.5 for code generation using Reflexion: "For
    Reflexion, we select the version based on GPT-3.5 and utilize the corresponding
    generated programs published in the official Github repository." However, the
    [generated program](https://github.com/noahshinn/reflexion/blob/d15acda1c81d464d9a81648d7f29fb951e326c70/programming_runs/root/reflexion_humaneval_py_pass_at_1/reflexion_humaneval_py_pass_at_1.jsonl)
    they used from the Reflexion repository relies on GPT-4 for code generation, not
    GPT-3.5.'
  id: totrans-split-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'These shortcomings in the empirical results have also led to errors of interpretation
    in broader discussions around the accuracy of AI agents. For example, a recent
    [post](https://www.deeplearning.ai/the-batch/issue-241/) by Andrew Ng claimed
    that agents that use GPT-3.5 can outperform GPT-4\. In particular, he claimed:'
  id: totrans-split-51
  prefs: []
  type: TYPE_NORMAL
- en: '*[For HumanEval,] GPT-3.5 (zero shot) was 48.1% correct. GPT-4 (zero shot)
    does better at 67.0%. However, the improvement from GPT-3.5 to GPT-4 is dwarfed
    by incorporating an iterative agent workflow. Indeed, wrapped in an agent loop,
    GPT-3.5 achieves up to 95.1%. *'
  id: totrans-split-52
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: While this claim received a lot of attention, it is incorrect. The claim ("GPT-3.5
    wrapped in an agent workflow achieves 95.1% accuracy") seems to be about the LDB
    agent. The [Papers With Code leaderboard](https://paperswithcode.com/sota/code-generation-on-humaneval)
    for HumanEval makes the same claim. However, as we discussed above, for LDB, GPT-3.5
    is only used to find bugs. The code is generated using GPT-4 (or the Reflexion
    agent that uses GPT-4), not GPT-3.5\. Unfortunately, the error in the paper has
    led to much overoptimism about agents in the broader AI community.
  id: totrans-split-53
  prefs: []
  type: TYPE_NORMAL
- en: 'Ng''s post also makes the familiar error of repeating results from papers without
    verifying them or accounting for changes in prompts and model versions. For example,
    the zero-shot accuracy numbers of GPT-3.5 (48.1%) and GPT-4 (67.0%) seem to be
    copied from the [GPT-4 technical report](https://arxiv.org/abs/2303.08774) from
    March 2023\. However, the models have been updated many times since release. Indeed,
    in our comparison, we find that the base models perform much better compared to
    the claimed figures in Ng''s post when we use them with the prompts provided with
    the LDB paper (GPT-3.5: 73.9%, GPT-4: 89.6%). As a result, the post drastically
    overestimates the improvement attributable to agent architectures.'
  id: totrans-split-54
  prefs: []
  type: TYPE_NORMAL
- en: Evaluation frameworks like Stanford's [HELM](https://crfm.stanford.edu/helm/lite/latest/)
    and EleutherAI's [LM Evaluation Harness](https://www.eleuther.ai/projects/large-language-model-evaluation)
    attempt to fix similar shortcomings for model evaluations, by providing standardized
    evaluation results. We are working on solutions to make agent evaluations standardized
    and reproducible, especially from the perspective of downstream evaluation of
    agents.
  id: totrans-split-55
  prefs: []
  type: TYPE_NORMAL
- en: Finally, downstream developers should keep in mind that HumanEval or any other
    standardized benchmark is nothing more than a rough proxy for the specific tasks
    that arise in a particular downstream application. To understand how agents will
    perform in practice, it is necessary to evaluate them on a [custom dataset](https://arxiv.org/abs/2404.12272)
    from the domain of interest — or even better, A/B test different agents in the
    production environment.
  id: totrans-split-56
  prefs: []
  type: TYPE_NORMAL
- en: '[Zaharia et al.](https://bair.berkeley.edu/blog/2024/02/18/compound-ai-systems/)
    observe that state-of-the-art accuracy on AI benchmarks is often attained by composite
    systems. If the adoption of agents continues, visualizing cost and accuracy as
    a Pareto curve would become even more necessary.'
  id: totrans-split-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Santhanam et al.](https://arxiv.org/pdf/2212.01340.pdf) point out the importance
    of evaluating cost alongside accuracy for information retrieval benchmarks.'
  id: totrans-split-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Ozrmazabal et al.](https://arxiv.org/html/2404.12387v1) highlight the [accuracy
    vs. cost per output token tradeoffs](https://arxiv.org/html/2404.12387v1#:~:text=Figure%201%3A,different%20LLM%20APIs.)
    for various models (but not agents) on MMLU. While the cost of output tokens might
    not be a good indicator of the overall cost, given the varying input token costs
    as well as output lengths for different models, it is better than not reporting
    the tradeoffs at all.'
  id: totrans-split-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The [Berkeley Function Calling leaderboard](https://gorilla.cs.berkeley.edu/leaderboard.html)
    includes various metrics for language model evaluations of function calling, including
    cost and latency.
  id: totrans-split-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Xie et al.](https://arxiv.org/pdf/2404.07972.pdf) develop OSWorld, a benchmark
    for evaluating agents in computer environments. In their [GitHub repository](https://github.com/xlang-ai/OSWorld)
    (though not in the paper), they give a rough cost estimate for running various
    multimodal agents on their benchmark.'
  id: totrans-split-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Unsurprisingly, the main impetus for cost vs. accuracy tradeoffs has come from
    the [downstream developers](https://x.com/swyx/status/1772799201023557697) who
    [use AI](https://x.com/jaredpalmer/status/1783899239140986884).
  id: totrans-split-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In a previous talk, we discussed [three major pitfalls](https://www.cs.princeton.edu/~arvindn/talks/evaluating_llms_minefield/)
    in LLM evaluation: prompt sensitivity, construct validity, and contamination.
    The current research is largely orthogonal: prompt sensitivity isn’t a concern
    for agent evaluation (as agents are allowed to define their own prompts); downstream
    developers can address contamination and construct validity by evaluating on custom
    datasets.'
  id: totrans-split-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The code for reproducing our analysis is available [here](https://github.com/benediktstroebl/agent-eval).
    The [appendix](https://www.cs.princeton.edu/~sayashk/papers/ai-leaderboards-appendix.pdf)
    includes more details about our setup and results.
  id: totrans-split-64
  prefs: []
  type: TYPE_NORMAL
- en: We thank Rishi Bommasani, Rumman Chowdhury, Percy Liang, Shayne Longpre, Yifan
    Mai, Nitya Nadgir, Matt Salganik, Hailey Schoelkopf, Zachary Siegel, and Venia
    Veselovsky for discussions and inputs that informed our analysis. We acknowledge
    Cunxiang Wang and Ruoxi Ning for their prompt responses to our questions about
    the NovelQA benchmark.
  id: totrans-split-65
  prefs: []
  type: TYPE_NORMAL
- en: We are grateful to the authors of the papers we engage with in this post for
    their quick responses and for sharing their code, which makes such reproduction
    analysis possible in the first place. In particular, we are grateful to Zilong
    Wang (LDB), Andy Zhou (LATS), and Karthik Narasimhan (Reflexion), who gave us
    feedback in response to an earlier draft of this blog post.
  id: totrans-split-66
  prefs: []
  type: TYPE_NORMAL
