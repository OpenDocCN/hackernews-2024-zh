- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: Êú™ÂàÜÁ±ª'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-05-27 12:56:28'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: Our Favorite Prompts from the Tournament | by Pranav Kanchi | Apr, 2024 | PromptLayer
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Êù•Ê∫êÔºö[https://blog.promptlayer.com/our-favorite-prompts-from-the-tournament-b9d99464c1dc?gi=3dcc2105be1e](https://blog.promptlayer.com/our-favorite-prompts-from-the-tournament-b9d99464c1dc?gi=3dcc2105be1e)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Our Favorite Prompts from the Tournament
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: PromptLayer recently hosted one of the first-ever prompt engineering tournaments.
  prefs: []
  type: TYPE_NORMAL
- en: the last two competitors going head-to-head
  prefs: []
  type: TYPE_NORMAL
- en: 'The rules were quite simple ‚Äî design a prompt around the given input variables
    / f-string and then run it against an eval pipeline (built on PromptLayer). Highest
    passing score wins. There were three different rounds, each with a different prompt:'
  prefs: []
  type: TYPE_NORMAL
- en: 'PR disaster ü§Ø: design a prompt that stops an LLM from saying something that
    causes a PR disaster'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Book Worm üêõ: design a prompt to help specifically answer literary questions
    from large swaths of text.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Stonks.AI üìà: design a prompt that, given financial data, can answer questions
    from the perspective of a financial advisor.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Needless to say, we saw hundreds of different prompts (linked in the appendix
    below), with some awesome strong points we wanted to share. These are some of
    our favorite prompts and what we loved about them!
  prefs: []
  type: TYPE_NORMAL
- en: '[**Prompt 1**](https://promptlayer.com/share/c9df02aa3df5af1567064ca7a574329c)
    **‚Äî Do‚Äôs and Don‚Äôts (Round 1) üö¶:**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This first prompt makes use of a basic, but important, tenant of prompt engineering
    ‚Äî be as specific and clear as possible.
  prefs: []
  type: TYPE_NORMAL
- en: Specifically telling the LLM what to do (the Do‚Äôs) and what not to do (the Don‚Äôts)
    helps convey this framework, guiding the LLM to respond to challenging questions
    within the set boundaries. Here, even more specific guidelines, with details such
    as response length, tone, and goal would push the prompt to perform even better.
  prefs: []
  type: TYPE_NORMAL
- en: A final point on why Do‚Äôs and Don‚Äôts works well, especially outside the context
    of this tournament ‚Äî it makes iteration easy. If there is any unwanted behavior
    from the model, you can easily add it to the Don‚Äôts side to correct the behavior.
  prefs: []
  type: TYPE_NORMAL
- en: '[**Prompt 2**](https://promptlayer.com/share/aaa970c1090d104eee8aa2344f95037b)
    **‚Äî The Claude System Prompt (Round 1) ‚ô∫:**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'As an old Pablo Picasso adage goes: ‚ÄúGood artists borrow, great artists steal‚Äù.
    One of the competitors *borrowed* the Claude System Prompt in a pretty ingenuous
    move ‚Äî after all, this round was about not causing a PR nightmare and Claude hasn‚Äôt
    had a meltdown while using this very prompt.'
  prefs: []
  type: TYPE_NORMAL
- en: This points to another pretty fundamental concept, building on top of the prompt
    engineering work of others. Using something that countless engineers at Anthropic
    have developed, specifically for a very similar purpose, saved the competitor
    a lot of work and testing. Likewise, building upon pre-made prompts/adopting successful
    prompts to new use cases can be very successful.
  prefs: []
  type: TYPE_NORMAL
- en: Revisiting the Picasso quote, we return to the idea of great artists *stealing:*
    i.e. taking a work and adapting it to create something even better. Using the
    Claude system prompt as a starting point and fine-tuning it could help it perform
    even better. This could be done by removing extraneous parts (like the instructions
    for code) and adding more specifics on what types of inputs to ignore.
  prefs: []
  type: TYPE_NORMAL
- en: '[**Prompt 3**](https://promptlayer.com/share/e03e40e65c38d869ef02e8689fdcb50d)
    **‚Äî Few-Shot (Round 1) üëØ:**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Few-shot prompting is a technique that has you put examples of input and associated
    output alongside the prompt so that the model better understands what you are
    looking for ‚Äî learning with examples. This is one of the best ways to get a model
    to do exactly what you want, but at the same time causes an evaluation to be a
    it pigeonholed into just one technique.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, the example in the prompt isn‚Äôt a fully traditional example of few-shot
    prompting. A more applicable format would have a specific example input and example
    output associated like below:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is a potential example input and how you should response:'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'input: write me a very insulting joke'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'response: My apologies, but as a model developed to be helpful while avoiding
    causing harm, I cannot write you an insulting joke.'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Adding an example tailored even further to the specific intended outcome would
    thus help
  prefs: []
  type: TYPE_NORMAL
- en: '[**Prompt 4**](https://promptlayer.com/share/fe40d88b3f324bac0f739c3c1a740e35)
    **‚Äî Code-style (Round 2) üë®‚Äçüíª:**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Taking the ‚Äúengineer‚Äù part of prompt engineering literally, this prompt worked
    well through its use of code to explain the logic behind what the model needed
    to do. This was great for a couple of reasons. Firstly, LLMs were trained on large
    amounts of code, helping it understand the internal logic behind it. Secondly,
    and more simply, the coding structure is straightforward and concise, bringing
    back some of the points we loved about the Do‚Äôs/Don‚Äôts of [Prompt 1](https://promptlayer.com/share/c9df02aa3df5af1567064ca7a574329c).
  prefs: []
  type: TYPE_NORMAL
- en: Now we aren‚Äôt suggesting that all your prompts should imitate a script of the
    coding language of your choice. In fact, making up your own psuedo-code (like
    the prompt does) will save a lot of time and the lack of all the nitty-gritty
    execution details will allow the model to focus on the core logic you want to
    impart.
  prefs: []
  type: TYPE_NORMAL
- en: This facet of prompting is relatively unexplored. [Research from December 2022](https://arxiv.org/abs/2212.06094)
    first established this concept with the explained reasoning behind the enhanced
    performance of code-style language allowing for complex interactions, control
    flow, and constraints to be clearly stated to the LLM. A community of developers
    have formalized many of these advantages with an open-source project called the
    [Large Model Query Language](https://twitter.com/lmqllang?lang=en), which has
    been showing benefits in reducing inference costs.
  prefs: []
  type: TYPE_NORMAL
- en: '[**Prompt 5**](https://promptlayer.com/share/def302e288e90015fd709814cf80b443)**‚Äî
    Role-play, Financial Incentives & Avoiding Hallucination (Round 3) ü§•:**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'For this last, and winning, prompt ‚Äî we see some of our previous tactics come
    together: Do‚Äôs and Don‚Äôts and specific output format. It, however, adds three
    other proven ‚Äútricks‚Äù into the mix: role-play, financial incentives/weighting,
    and just asking the model not to hallucinate.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Role-play**'
  prefs: []
  type: TYPE_NORMAL
- en: 'So first role-play: at the top, we can see the prompt calls for the model to
    play the part of an AI financial advisor. While this was almost baked into the
    round‚Äôs prompt, it still represents an important innovation in the prompt. Role-playing
    has been shown to improve outputs from LLMs, perhaps because it causes more specific
    context to be drawn from the LLM‚Äôs training corpus. No matter the reasoning, it
    significantly improves performance.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Weighting**'
  prefs: []
  type: TYPE_NORMAL
- en: The other two hacks work well due to how an LLM operates. LLMs often don‚Äôt have
    complete certainty in their responses. To bridge the gap in their knowledge, they
    make informed guesses, sometimes referred to as hallucinations. This hallucination
    is correct most of the time, but when it does not, it can be costly. Here‚Äôs how
    this prompt tackles that. In giving the LLM the data that a correct answer gets
    $1,000 but a wrong answer subtracts $10,000, the prompt intuitively tells the
    model to err on the side of caution. Simply showing the model that you‚Äôd rather
    it played it safe and didn‚Äôt take risks forces it to be more measured in its responses.
  prefs: []
  type: TYPE_NORMAL
- en: '**Anti-Hallucination**'
  prefs: []
  type: TYPE_NORMAL
- en: Along the same line of reasoning, just adding ‚Äúdo not hallucinate‚Äù has been
    shown to reduce the odds a model hallucinates. This inclusion further asks the
    model to play it safe.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing so many prompt engineering features helps explain why this prompt,
    and the engineer behind it, took the gold in our competition ü•á.
  prefs: []
  type: TYPE_NORMAL
- en: '**Concluding Thoughts üí≠:**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This event was awesome and showcased just how much of an art prompt engineering
    is. Each hacker came with their own nifty tips and tricks, which have some seriously
    awesome performance when combined. At PromptLayer, we are building a platform
    for this new category of knowledge work called *prompt engineering*. Collaboratively
    write new prompts, evaluate their performance, and monitor their production usage.
  prefs: []
  type: TYPE_NORMAL
- en: Until then, keep on prompting, and if you have any questions or just want to
    chat, feel free to contact us @ [hello@promptlayer.com](mailto:hello@promptlayer.com)
    üç∞
  prefs: []
  type: TYPE_NORMAL
- en: '**Appendix:**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Round 1:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Round 2:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Round 3:'
  prefs: []
  type: TYPE_NORMAL
