<!--yml

category: 未分类

date: 2024-05-27 13:28:34

-->

# 利用生成AI快速开发

> 来源：[https://discord.com/blog/developing-rapidly-with-generative-ai](https://discord.com/blog/developing-rapidly-with-generative-ai)

## **AI 应用原型设计：从想法到MVP**

然后我们定义的产品要求会影响我们选择用于原型的现成LLM。我们通常倾向于选择更先进的商业LLM，以快速验证我们的想法并从用户那里获得早期反馈。尽管它们可能昂贵，但总体思路是，如果无法使用像GPT-4这样的最先进基础模型充分解决问题，那么很多时候这些问题可能无法通过当前的生成AI技术解决。如果现成LLM可以解决我们的问题，那么我们可以进入学习阶段，并集中精力在产品迭代上，而不是将工程资源转向构建和维护机器学习基础设施。

### 评估提示

在这个阶段的关键步骤是创建正确的提示。我们从一个基本提示开始，告诉ChatGPT（或者我们选择的任何LLM原型）我们希望它做什么。然后，我们对这个提示进行调整，改变措辞以使任务更清晰。然而，在经过大量调整之后，很难确定这些变化是否真正改善了我们的结果。这就是评估提示变得至关重要的地方。通过使用度量指标来指导我们的改变，我们知道我们正在提高我们结果的质量。

为了做到这一点，我们采用一种称为[AI辅助评估](https://arize.com/blog-course/llm-evaluation-the-definitive-guide/)的技术，以及用于衡量性能的传统度量标准。这有助于我们选择导致更高质量输出的提示，使最终产品对用户更具吸引力。AI辅助评估利用最优秀的LLM（如GPT-4）自动批评AI输出与我们预期的匹配程度或它们根据一组标准的评分。这种方法类似于强化学习中发现的[演员-评论家算法](https://www.tensorflow.org/tutorials/reinforcement_learning/actor_critic)，其中使用单独的模型评估用于推理的模型的表现如何。自动化评估使我们能够快速看到哪些工作良好，哪些需要在我们的提示中进行调整，而无需手动检查所有内容。在评估时，我们设计提示，要求简单的是或否答案或者按比例评估输出，使评估过程变得简单直接。

AI辅助评估包括两个单独的提示：一个用于您的任务，另一个用于评估您的结果。任务提示传递给推理模型，而评论家提示传递给更高级的评论家模型。

### 启动和学习

一旦我们对我们的提示生成的结果质量足够自信，我们就会推出产品的有限发布（例如 A/B 测试），并观察系统在实际环境中的表现。我们使用的确切指标取决于应用程序 —— 我们的主要目标是了解用户如何使用该功能，并快速进行改进以更好地满足他们的需求。对于内部应用，这可能意味着测量效率和情感。对于面向消费者的应用，我们同样关注用户满意度的衡量标准 - 直接用户反馈，用户参与度等。这些反馈对于确定改进的方向至关重要，包括突出显示不正确的答案或导致奇怪用户体验的 LLM 幻觉的实例。

除了用户满意度之外，我们还关注系统健康指标，例如响应速度（延迟），吞吐量（每秒令牌数）和错误率。LLM 有时在生成输出时可能会遇到一致性结构化格式的困难，这对于减少数据解析错误并确保输出在我们的服务中的可靠使用至关重要。这里的见解可以指导我们在多大程度上需要事后处理来完全将此功能在规模上投入生产。

对成本的密切关注同样重要，以便了解我们在完全扩展功能时将花费多少。我们查看我们在初始有限发布中每秒使用的令牌数量，以预测如果我们使用驱动我们原型的同样技术进行完全发布时的成本。

所有上述信息对于理解我们的产品是否按预期运行并为用户提供价值至关重要。如果是，那么我们可以继续下一步：大规模部署。如果不是，那么我们会寻求我们的经验教训，对系统进行迭代，并再次尝试。
