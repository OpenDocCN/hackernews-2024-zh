- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-05-27 13:15:19'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: AI cloud startup TensorWave bets AMD can beat Nvidia • The Register
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://www.theregister.com/2024/04/16/amd_tensorwave_mi300x/](https://www.theregister.com/2024/04/16/amd_tensorwave_mi300x/)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Specialist cloud operators skilled at running hot and power-hungry GPUs and
    other AI infrastructure are emerging, and while some of these players like CoreWeave,
    Lambda, or Voltage Park — have built their clusters using tens of thousands of
    Nvidia GPUs, others are turning to AMD instead.
  prefs: []
  type: TYPE_NORMAL
- en: An example of the latter is bit barn startup TensorWave which earlier this month
    began racking up systems powered by AMD's Instinct MI300X ,which it plans to lease
    the chips at a fraction of the cost charged to access Nvidia accelerators.
  prefs: []
  type: TYPE_NORMAL
- en: TensorWave co-founder Jeff Tatarchuk believes AMD's latest accelerators have
    many fine qualities. For starters, you can actually buy them. TensorWave has secured
    a large allocation of the parts.
  prefs: []
  type: TYPE_NORMAL
- en: By the end of 2024, TensorWave aims to have 20,000 MI300X accelerators deployed
    across two facilities, and plans to bring additional liquid-cooled systems online
    next year.
  prefs: []
  type: TYPE_NORMAL
- en: AMD's latest AI silicon is also faster than Nvidia's much coveted H100\. "Just
    in raw specs, the MI300x dominates H100," Tatarchuk said.
  prefs: []
  type: TYPE_NORMAL
- en: Launched at AMD’s Advancing AI event in December, the MI300X is the chip design
    firm’s most advanced accelerator to date. The [750W chip](https://www.theregister.com/2023/12/06/amd_mi300_gpu/)
    uses a combination of advanced packaging to stitch together 12 chiplets — 20 if
    you count the HBM3 modules — into a single GPU that's claimed to be 32 percent
    faster than Nvidia's H100.
  prefs: []
  type: TYPE_NORMAL
- en: In addition to higher floating point performance, the chip also boasts a larger
    192GB of HBM3 memory capable of delivering 5.3TB/s of bandwidth versus the 80GB
    and 3.35TB/s claimed by the H100.
  prefs: []
  type: TYPE_NORMAL
- en: As we've seen from Nvidia's H200 – a version of the H100 boosted by the inclusion
    of HBM3e - memory bandwidth is a [major contributor](https://www.theregister.com/2023/12/21/nvidia_amd_benchmarks/)
    to AI performance, particularly in inferencing on large language models.
  prefs: []
  type: TYPE_NORMAL
- en: Much like Nvidia's HGX and Intel's OAM designs, standard configurations of AMD's
    latest GPU require eight accelerators per node.
  prefs: []
  type: TYPE_NORMAL
- en: That’s the configuration the folks at TensorWave are busy racking and stacking.
  prefs: []
  type: TYPE_NORMAL
- en: '"We have hundreds going in now and thousands going in the months to come,"
    Tatarchuk said.'
  prefs: []
  type: TYPE_NORMAL
- en: Racking them up
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In a photo [posted](https://twitter.com/TensorWaveCloud/status/1775018324373479654)
    to social media, the TensorWave crew showed what appeared to be three 8U Supermicro
    AS-8125GS-TNMR2 [systems](https://www.supermicro.com/en/products/system/gpu/8u/as-8125gs-tnmr2)
    racked up. This led us to question whether TensorWave's racks were power or thermally
    limited after all, it's not unusual for these systems to pull in excess of 10kW
    when fully loaded.
  prefs: []
  type: TYPE_NORMAL
- en: It turns out that the folks at TensorWave hadn't finished installing the machines
    and that the firm is targeting four nodes with a total capacity of around 40kW
    per rack. These systems will be cooled using rear door heat exchangers (RDHx).
    As we've [discussed](https://www.theregister.com/2023/12/13/equinix_liquid_cooling/)
    in the past, these are rack-sized radiators through which cool water flows. As
    hot air exits a conventional server, it passes through the radiator which cools
    it to acceptable levels.
  prefs: []
  type: TYPE_NORMAL
- en: This cooling tech has become a hot commodity among datacenter operators looking
    to support denser GPU clusters and led to some supply chain challenges, TensorWave
    COO Piotr Tomasik said.
  prefs: []
  type: TYPE_NORMAL
- en: '"There''s a lot of capacity issues, even in the ancillary equipment around
    data centers right now," he said, specifically referencing RDHx as a pain point.
    "We''ve been successful thus far and we were very bullish on our ability to deploy
    them."'
  prefs: []
  type: TYPE_NORMAL
- en: Longer term, however, TensorWave has its sights set on direct-to-chip cooling
    which can be hard to deploy in datacenters that weren’t designed to house GPUs,
    Tomasik said. "We're excited to deploy direct to chip cooling in the second half
    of the year. We think that that's going to be a lot better and easier with density."
  prefs: []
  type: TYPE_NORMAL
- en: Performance anxiety
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Another challenge is confidence in AMD's performance. According to Tatarchuk,
    while there's a lot of enthusiasm around AMD offering an alternative to Nvidia,
    customers are not certain they will enjoy the same performance. "There's also
    a lot of 'We're not 100 percent sure if it's going to be as great as what we're
    currently used to on Nvidia',” he said.
  prefs: []
  type: TYPE_NORMAL
- en: In the interest of getting systems up and running as quickly as possible, TensorWave
    will launch its MI300X nodes using RDMA over Converged Ethernet (RoCE). These
    bare metal systems will be available for fixed lease periods, apparently for as
    little as $1/hr/GPU.
  prefs: []
  type: TYPE_NORMAL
- en: Scaling up
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Over time, the outfit aims to introduce a more cloud-like orchestration layer
    for provisioning resources. Implementing GigaIO's PCIe 5.0-based FabreX technology
    to stitch together up to 5,750 GPUs in a single domain with more than a petabyte
    of high bandwidth memory is also on the agenda.
  prefs: []
  type: TYPE_NORMAL
- en: These so-called TensorNODEs are based on GigaIO's SuperNODE architecture it
    [showed off](https://www.nextplatform.com/2023/08/15/crafting-a-dgx-alike-ai-server-out-of-amd-gpus-and-pci-switches/)
    last year, which used a pair of PCIe switch appliances to connect up to 32 AMD
    MI210 GPUs together. In theory, this should allow a single CPU head node to address
    far more than the eight accelerators typically seen in GPU nodes today.
  prefs: []
  type: TYPE_NORMAL
- en: This approach differs from Nvidia’s preferred design, which uses NVLink to stitch
    together multiple superchips into one big GPU. While NVLink is considerably faster
    topping out at 1.8TB/s of bandwidth in its [latest iteration](https://www.theregister.com/2024/03/21/nvidia_dgx_gb200_nvk72/)
    compared to just 128GB/s on PCIe 5.0, it only supports configurations up to 576
    GPUs.
  prefs: []
  type: TYPE_NORMAL
- en: TensorWave will fund its bit barn build by using its GPUs as collateral for
    a large round of debt financing, an approach used by other datacenter operators.
    Just last week, Lambda [revealed](https://www.theregister.com/2024/04/05/lambda_500m_loan/)
    it'd secured a $500 million loan to fund the deployment of "tens of thousands"
    of Nvidia's fastest accelerators.
  prefs: []
  type: TYPE_NORMAL
- en: Meanwhile, CoreWeave, one of the largest providers of GPUs for rent, was able
    to [secure](https://www.prnewswire.com/news-releases/coreweave-secures-2-3-billion-debt-financing-facility-led-by-magnetar-capital-and-blackstone-to-meet-surging-demand-and-ongoing-expansion-of-specialized-cloud-infrastructure-to-power-ai-301892706.html)
    a massive $2.3 billion loan to expand its datacenter footprint.
  prefs: []
  type: TYPE_NORMAL
- en: '"You would, you should expect us to have the same sort of announcement here
    later this year," Tomasik said. ®'
  prefs: []
  type: TYPE_NORMAL
