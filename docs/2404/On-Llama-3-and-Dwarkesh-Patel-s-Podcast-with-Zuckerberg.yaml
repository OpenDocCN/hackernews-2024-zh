- en: <!--yml
  id: totrans-split-0
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  id: totrans-split-1
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-05-27 13:25:11'
  id: totrans-split-2
  prefs: []
  type: TYPE_NORMAL
- en: -->
  id: totrans-split-3
  prefs: []
  type: TYPE_NORMAL
- en: On Llama-3 and Dwarkesh Patel's Podcast with Zuckerberg
  id: totrans-split-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://thezvi.substack.com/p/on-llama-3-and-dwarkesh-patels-podcast](https://thezvi.substack.com/p/on-llama-3-and-dwarkesh-patels-podcast)
  id: totrans-split-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: It was all quiet. Then it wasn’t.
  id: totrans-split-6
  prefs: []
  type: TYPE_NORMAL
- en: '[Note the timestamps on both of these](https://twitter.com/tszzl/status/1781043498801893827).'
  id: totrans-split-7
  prefs: []
  type: TYPE_NORMAL
- en: '[Dwarkesh Patel did a podcast with Mark Zuckerberg](https://www.youtube.com/watch?v=bc6uFV9CJGg&ab_channel=DwarkeshPatel)
    on the 18th. It was timed to coincide with the release of much of Llama-3, very
    much the approach of telling your story directly. Dwarkesh is now the true tech
    media. A meteoric rise, and well earned.'
  id: totrans-split-8
  prefs: []
  type: TYPE_NORMAL
- en: This is two related posts in one. First I cover the podcast, then I cover Llama-3
    itself.
  id: totrans-split-9
  prefs: []
  type: TYPE_NORMAL
- en: My notes are edited to incorporate context from later explorations of Llama-3,
    as I judged that the readability benefits exceeded the purity costs.
  id: totrans-split-10
  prefs: []
  type: TYPE_NORMAL
- en: (1:00) They start with Llama 3 and the new L3-powered version of Meta AI. Zuckerberg
    says “With Llama 3, we think now that Meta AI is the most intelligent, freely-available
    assistant that people can use.” If this means ‘free as in speech’ then the statement
    is clearly false. So I presume he means ‘free as in beer.’
  id: totrans-split-11
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Is that claim true? Is Meta AI now smarter than GPT-3.5, Claude 2 and Gemini
    Pro 1.0? As I write this it is too soon to tell. Gemini Pro 1.0 and Claude 3 Sonnet
    are slightly ahead of Llama-3 70B on the Arena leaderboard. But it is close. The
    statement seems like a claim one can make within ‘reasonable hype.’ Also, Meta
    integrates Google and Bing for real-time knowledge, so the question there is if
    that process is any good, since most browser use by LLMs is not good.
  id: totrans-split-12
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: (1:30) Meta are going in big on their UIs, top of Facebook, Instagram and Messenger.
    That makes sense if they have a good product that is robust, and safe in the mundane
    sense. If it is not, this is going to be at the top of chat lists for teenagers
    automatically, so whoo boy. Even if it is safe, there are enough people who really
    do not like AI that this is probably a whoo boy anyway. Popcorn time.
  id: totrans-split-13
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: (1:45) They will have the ability to animate images and it generates high quality
    images as you are typing and updates them in real time as you are typing details.
    I can confirm this feature is cool. He promises multimodality, more ‘multi-linguality’
    and bigger context windows.
  id: totrans-split-14
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: (3:00) Now the technical stuff. Llama-3 follows tradition in training models
    in three sizes, here 8b, 70b that released on 4/18, and a 405b that is still training.
    He says 405b is already around 85 MMLU and they expect leading benchmarks. The
    8b Llama-3 is almost as good as the 70b Llama-2.
  id: totrans-split-15
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: (5:15) What went wrong earlier for Meta and how did they fix it? He highlights
    Reels, with its push to recommend ‘unconnected content,’ meaning things you did
    not ask for, and not having enough compute for that. They were behind. So they
    ordered double the GPUs that needed. They didn’t realize the type of model they
    would want to train.
  id: totrans-split-16
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: (7:30) Back in 2006, what would Zuck have sold for when he turned down $1 billion?
    He says he realized if he sold he’d just build another similar company, so why
    sell? It wasn’t about the number, he wasn’t in position to evaluate the number.
    And I think that is actually wise there. You can realize that you do not want
    to accept any offer someone would actually make.
  id: totrans-split-17
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: (9:15) When did making AGI become a key priority? Zuck points out Facebook AI
    Research (FAIR) is 10 years old as a research group. Over that time it has become
    clear you need AGI, he says, to support all their other products. He notes that
    training models on coding generalizes and helps their performance elsewhere, and
    that was a top focus for Llama-3\.
  id: totrans-split-18
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: So Meta needs to solve AGI because if they don’t ‘their products will be lame.’
    It seems increasingly likely, as we will see in several ways, that Zuck does not
    actually believe in ‘real’ AGI. By ‘AGI’ he means somewhat more capable AI.
  id: totrans-split-19
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: (13:40) What will the Llama that makes cool products be able to do? Replace
    the engineers at Meta? Zuck tries to dodge, says we’re not ‘replacing’ people
    as much as making them more productive, hopefully 10x or more, says there is no
    one threshold for human intelligence, AGI isn’t one thing. He is focused on different
    modalities, especially 3D and emotional understanding, in addition to the usual
    things like memory and reasoning.
  id: totrans-split-20
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: (16:00) What will we use all our data for? Zuck says AI will be in everything,
    and there will be a Meta general assistant product that does complicated tasks.
    He wants to let creators own an AI and train it how they want to ‘engage their
    community.’ But then he admits these are only consumer use cases and it will change
    everything in the economy.
  id: totrans-split-21
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: (18:25) When do we get the good agents? Zuck says we do not know. It depends
    on the scaffolding. He wants to progressively move more of that into the model
    to make them better agents on their own so this stops being ‘brittle and non-general.’
    It has much better tool use, you do not need to hand code. This Is Fine.
  id: totrans-split-22
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: (22:20) What community fine tune is most personally exciting? Zuck says he doesn’t
    know, it surprises you, if he knew he’d build it himself.
  id: totrans-split-23
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: This doesn’t match my model of this, where you want to specialize, some things
    are left to others, which seems doubly true here with open model weights. He mentions
    that 8b is too big for many use cases, we should try to build a 1b or smaller
    model too.
  id: totrans-split-24
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Also he mentions that they do a ton of inference because they have a ton of
    customers, so that dominates their compute usage over time. It makes sense for
    them to do what for others would be overtraining, also training more seemed to
    keep paying dividends for a long time.
  id: totrans-split-25
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: I would presume the other big labs will be in similar positions going forward.
  id: totrans-split-26
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: (26:00) How much better will Llama-4 get? How will models improve? Zuck says
    (correctly) this is one of the great questions, on one knows, how long does an
    exponential curve keep going? He says probably long enough that the infrastructure
    is worth investing in, and a lot of companies are investing a lot.
  id: totrans-split-27
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: (28:00) He thinks energy constraints will soon bind, not chips. No one has built
    a gigawatt single training cluster yet. And that is slower because energy gets
    permitted at the speed of government and then has to be physically built. One
    does not simply get a bunch of energy, compute and data together.
  id: totrans-split-28
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: If concentrations of energy generation are the true bottleneck, then anyone
    who says ‘government has no means to control this’ or ‘government cannot control
    this without being totalitarian’ would be very wrong, this is a very easy thing
    to spot, isolate and supervise. Indeed, we almost ‘get it for free’ given we are
    already massively over restricting energy generation and oversee industrial consumption.
  id: totrans-split-29
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: (30:00) What would Meta do with 10x more money? More energy, which would allow
    bigger clusters, but true bottleneck is time. Right now data center energy tops
    out at something like 50mw-150mw. But 300mw-1gw, that’s new, that’s a meaningful
    nuclear power plant. It will happen but not next year. Dwarkesh mentions Amazon’s
    950mw facility, Zuck says he is unsure about that.
  id: totrans-split-30
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: (31:40) What about distributed computing? Zuck says it is unknown how much of
    that is feasible, and suggests that a lot of training in future might be inference
    to generate synthetic data.
  id: totrans-split-31
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: (32:25) If that’s what this is about, could this work for Llama-3? Could you
    use these models to get data for these models to get smarter? De facto one might
    say ‘RSI Real Soon Now (RSI RSN)?’ Zuck says ‘there are going to be dynamics like
    that’ but there are natural limits on model architecture. He points out there
    is nothing like Llama-3 400B currently in open source, that will change things
    a lot, but says it can only go so far. That all makes sense, at some point you
    have to restart the architecture, but that does not fully rule out the scenario.
  id: totrans-split-32
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: (34:15) Big picture, what’s up with AI for the next decade? How big a deal is
    it? Zuck says pretty fundamental, like the creation of computing, going from not
    having computers to having computers. You’ll get ‘all these new apps’ and it will
    ‘let people do what they want a lot more.’
  id: totrans-split-33
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: He notices it is very hard to reason about how this goes.
  id: totrans-split-34
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: He strongly expects physical constraints to prevent fast takeoff, or even ‘slow
    takeoff,’ expecting it to be decades to fully get there.
  id: totrans-split-35
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Notice again his expectations here are very much within the mundane range.
  id: totrans-split-36
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: That could be the central crux here. If he thinks that nothing we build can
    get around the physical constraints for decades, then that has a lot of implications.
  id: totrans-split-37
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: (36:00) Dwarkesh says, but what about on that cosmic, longer-term scale? What
    will the universe look like? Will AI be like humans evolving or harnessing fire?
    Zuck says that is tricky. He says that people have come to grips throughout history
    with noticing that humanity is not unique in various ways but is still super special.
    He notices that intelligence is not clearly fundamentally connected to life, it
    is distinct from consciousness and agency. Which he says makes it a super valuable
    tool.
  id: totrans-split-38
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Once again, even in this scenario, there’s that word again. [Tool](https://en.wikipedia.org/wiki/Mark_Zuckerberg).
  id: totrans-split-39
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: A key problem with this is agency is super useful. There is a reason Meta’s
    central plan is to create an active AI assistant for you that will act are your
    personal agent. Why Meta is striving to bring as much agency capability directly
    into the models, and also building more agency capability on top of that. The
    first thing people are doing and will do, in many contexts, is strive to give
    the AI as much agency as possible. So even if that doesn’t happen ‘on its own’
    it happens anyway. My expectation is that if you wanted to create a non-agent,
    you can probably do that, but you and everyone else with sufficient access to
    the model have to choose to do that.
  id: totrans-split-40
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '(38:00) Zuck: “Which is why I don’t think anyone should be dogmatic about how
    they plan to develop it or what they plan to do. You want to look at it with each
    release. We’re obviously very pro open source, but I haven’t committed to releasing
    every single thing that we do. I’m basically very inclined to think that open
    sourcing is going to be good for the community and also good for us because we’ll
    benefit from the innovations. If at some point however there is some qualitative
    change in what the thing is capable of, and we feel like it’s capable of, and
    we feel it is not responsible to open source it, then we won’t. It’s all very
    difficult to predict.”'
  id: totrans-split-41
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Bravo. Previously we have seen him say they were going to open source AGI. He
    might intend to do that anyway. This continues Zuck trying to have it both ways.
    He says both ‘we will open source everything up to and including AGI’ and also
    ‘we might not’ at different times.
  id: totrans-split-42
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The reconciliation is simple. When Zuck says ‘AGI’ he does not mean AGI.
  id: totrans-split-43
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: This suggests an obvious compromise. We can all negotiate on what capabilities
    would constitute something too dangerous, and draw a line there, with the line
    drawn in anticipation of what can be built on top of the model that is being considered
    for release, and understanding that all safety work will rapidly be undone and
    so on.
  id: totrans-split-44
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We are talking price, and perhaps are not even that far apart.
  id: totrans-split-45
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: I am totally fine with Llama-3 70B being released.
  id: totrans-split-46
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: I do notice that open sourcing Llama-3 405B sounds like a national security
    concern, and as I discuss later if I was in NatSec I would be asking how I could
    prevent Meta from releasing the weights for national competitiveness reasons (to
    not supercharge Chinese AI) with a side of catastrophic misuse by non-state actors.
  id: totrans-split-47
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: But I do not expect existential risk from Llama-3\.
  id: totrans-split-48
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: (38:45) So Dwarkesh asks exactly that. What would it take to give Zuck pause
    on open sourcing the results of a future model?
  id: totrans-split-49
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Zuck says it is hard to measure that in the abstract. He says if you can ‘mitigate
    the negative behaviors’ of a product, then those behaviors are okay.
  id: totrans-split-50
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: The whole point is that you can to some extent do mitigations while you control
    the model (this is still super hard and jailbreaks are universally possible at
    least for now) but if you open source then your mitigations get fully undone.
  id: totrans-split-51
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Thus I see this as another crux. What does ‘mitigate’ mean here? What is the
    proposal for how that would work? How is this not as fake as Stability.ai saying
    they are taking safety precautions with Stable Diffusion 3, the most generous
    interpretation of which I can imagine is ‘if someone does a fine tune and a new
    checkpoint and adds a LoRa then that is not our fault.’ Which is a distinction
    without a difference.
  id: totrans-split-52
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: (40:00) Zuck says it is hard to enumerate all the ways something can be good
    or bad in advance. Very true.
  id: totrans-split-53
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: As an aside, the ads here are really cool, pitches for plausibly useful AI products.
    Dwarkesh’s readings are uninspired, but the actual content is actively positive.
  id: totrans-split-54
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '(42:30) Zuck: “Some people who have bad faith are going to try and strip out
    all the bad stuff. So I do think that’s an issue.”'
  id: totrans-split-55
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Isn’t it more accurate to say that people will for various reasons definitely
    strip out all the protections, as they have consistently always done, barring
    an unknown future innovation?
  id: totrans-split-56
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '(42:45) And here it is, as usual. Zuck: “I do think that a concentration of
    AI in the future has the potential to be as dangerous as it being widespread…
    people ask ‘is it bad for it to be out in the wild and just widely available?’
    I think another version of this is that it’s probably also pretty bad for one
    institution to have an AI that is way more powerful than everyone else’s AI.”
    And so on.'
  id: totrans-split-57
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Something odd happens with his answer here. Up until this point, Zuck has been
    saying a mix of interesting claims, some of which I agree with and some where
    I disagree. I think he is making some key conceptual mistakes, and of course is
    talking his book as one would expect, but it is a unique perspective and voice.
    Now, suddenly, we get the generic open source arguments I’ve heard time and again,
    like they were out of a tape recorder.
  id: totrans-split-58
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: And then he says ‘I don’t hear people talking about this much.’ Well, actually,
    I hear people talking about it constantly. It is incessant, in a metaphorically
    very ‘[isolated demand for rigor](https://slatestarcodex.com/2014/08/14/beware-isolated-demands-for-rigor/)’
    kind of way, to hear ‘the real danger is concentration of power’ or concentration
    of AI capability. Such people usually say this without justification, and without
    any indication they understand what the ‘not real’ danger is that they are dismissing
    as not real or why they claim that it is not real.
  id: totrans-split-59
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: (45:00) He says what keeps him up at night is that someone untrustworthy that
    has the super strong AI, that this is ‘potentially a much bigger risk.’ That a
    bad actor who got a hold of a strong AI might cause a lot of mayhem in a world
    where not everyone has a strong AI.
  id: totrans-split-60
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: This is a bigger concern than AI getting control of the future? Bigger than
    human extinction? Bigger than every actor, however bad, having such access?
  id: totrans-split-61
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Presumably he means more likely, or some combination of likely and bigger.
  id: totrans-split-62
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: So yes, his main concern is that the wrong monkey might get the poisoned banana
    and use it against other monkeys, it is only a tool after all. So instead we have
    to make sure all monkeys have such access?
  id: totrans-split-63
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: (46:00) It is overall a relatively good version of the generic open source case.
    He at least acknowledges that there are risks on all sides, and certainly I agree
    with that.
  id: totrans-split-64
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: I see no indication from the argument that he actually understands what the
    risks of open sourced highly capable models are, or that he has considered them
    and has a reason why they would not come to pass.
  id: totrans-split-65
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: His position here appears to be based on ‘this is a tool and will always be
    a tool’ and combining that with an implied presumption about offense-defense balance.
  id: totrans-split-66
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: I certainly have no idea what his plan (or expectation) is to deal with various
    competitive dynamics and incentives, or how he would keep the AIs from being something
    more than tools if they were capable of being more than that.
  id: totrans-split-67
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: The better version of this case more explicitly denies future AI capabilities.
  id: totrans-split-68
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: I could write the standard reply in more detail than I have above, but I get
    tired. I should have a canonical link to use in these spots, but right now I do
    not.
  id: totrans-split-69
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: (46:30) Instead Dwarkesh says it seems plausible that we could get an open source
    AI to become the standard and the best model, and that would be fine, preferable
    even. But he asks, mechanically, how you stop a bad actor in that world.
  id: totrans-split-70
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: He first asks about bioweapons.
  id: totrans-split-71
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Zuck answers that stronger AIs are good cybersecurity defense.
  id: totrans-split-72
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Dwarkesh asks, what if bioweapons aren’t like that.
  id: totrans-split-73
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Zuck agrees he doesn’t know that bioweapons do not work that way and it makes
    sense to worry there. He suggests not training certain knowledge into the model
    (which seems unlikely to me to be that big a barrier, because the world implies
    itself and also you can give it the missing data), but admits if you get a sufficiently
    bad actor (which you will), and you don’t have another AI that can understand
    and balance that (which seems hard under equality), then that ‘could be a risk.’
  id: totrans-split-74
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: (48:00) What if you for example caught a future Llama lying to you? Zuck says
    right now we see hallucinations and asks how you would tell the difference between
    that and deception, says there is a lot to think about, speaks of ‘long-term theoretical
    risks’ and asks to balance this with ‘real risks that we face today.’ His deception
    worry is ‘people using this to generate misinformation.’
  id: totrans-split-75
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: (49:15) He says that the way he has beaten misinformation so far is by building
    AI systems that are smarter than the adversarial ones.
  id: totrans-split-76
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Exactly. Not ‘as smart.’ Smarter.
  id: totrans-split-77
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Zuck is playing defense here. He has the harder job.
  id: totrans-split-78
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: If those trying to get ‘misinformation’ or other undesired content past Facebook’s
    (or Twitter’s or GMail’s) filters had the same level of sophistication and skill
    and resources as Meta and Google, you would have to whitelist in order to use
    Facebook, Twitter and GMail.
  id: totrans-split-79
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: The key question will be, how much of being smarter will be the base model?
  id: totrans-split-80
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: (49:45) Zuck says hate speech is not super adversarial in the sense that people
    are not getting better at being racist.
  id: totrans-split-81
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: I think in this sense that is wrong, and they totally are in both senses? Racists
    invent new dog whistles, new symbols, new metaphors, new deniable things. They
    look for what they can and cannot say in different places. They come up with new
    arguments. If you came with the 1970s racism today it would go very badly for
    you, let alone the 1870s or 1670s racism. And then he says that AIs here are getting
    more sophisticated faster than people.
  id: totrans-split-82
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 'What is going to happen is that the racists are going to get their racist AI
    systems ([see: Gab](https://thezvi.substack.com/p/ai-60-oh-the-humanity#%C2%A7another-supposed-system-prompt))
    and start using the AI to generate and select their racist arguments.'
  id: totrans-split-83
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: If your AI needs to have high accuracy to both false positives and false negatives,
    then you need a capability advantage over the attack generation mechanism.
  id: totrans-split-84
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: This is all ‘without loss of generality.’ You can mostly substitute anything
    else you dislike for racism here if you change the dates or other details.
  id: totrans-split-85
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: (50:30) Zuck then contrasts this with nation states interfering in elections,
    where he says nation-states are ‘have cutting edge technology’ and are getting
    better every year. He says this is ‘not like someone trying to say mean things,
    they have a goal.’
  id: totrans-split-86
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Well, saying mean things is also a goal, and I have seen people be very persistent
    and creative in saying mean things when they want to do that.
  id: totrans-split-87
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Indeed, Mark Zuckerberg went to Ardsley High School and Phillips Exeter Academy,
    they made this movie The Social Network and also saying mean things about Mark
    Zuckerberg is a top internet passtime. I am going to take a wild guess that he
    experienced this first hand. A lot.
  id: totrans-split-88
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: I would also more centrally say no, zero nation states have cutting edge election
    interference technology, except insofar as ‘whatever is available to the most
    capable foreign nation-state at this, maybe Russia’ is defined as the cutting
    edge. Plenty of domestic and non-state actors are ahead of the game here. And
    no state actor, or probably any domestic actor either, is going to have access
    to an optimized-for-propaganda-and-chaos version of Gemini, GPT-4 or Claude Opus.
    We are blessed here, and of course we should not pretend that past attempts were
    so sophisticated or impactful. Indeed, what may happen in the coming months is
    that, by releasing Llama-3 400B, Zuck instantly gives Russia, China, North Korea
    and everyone else exactly this ‘cutting edge technology’ with which to interfere.
  id: totrans-split-89
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: I of course think the main deception problems with AI lie in the future, and
    have very little to do with traditional forms of ‘misinformation’ or ‘election
    interference.’ I do still find it useful to contrast our models of those issues.
  id: totrans-split-90
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: (51:30) He says ‘for the foreseeable future’ he is optimistic they will be able
    to open source. He doesn’t want to ‘take our eye off the ball’ of what people
    are trying to use the models for today. I would urge him to keep his eye on that
    ball, but also skate where the puck is going. Do not move directly towards the
    ball.
  id: totrans-split-91
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: (54:30) Fun time, what period of time to go back to? Zuck checks, it has to
    be the past. He talks about the metaverse.
  id: totrans-split-92
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: (59:00) Zuck is incapable of not taking a swing at building the next thing.
    He spends so much time finding out if he could, I suppose.
  id: totrans-split-93
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: (1:02:00) Caesar Augustus seeking peace. Zuck suggests peace at the time was
    a new concept as anything other than a pause between wars. I notice I am skeptical.
    Then Zuck transitions from ‘wanting the economy to be not zero-sum’ to ‘a lot
    of investors don’t understand why we would open source this.’ And says ‘there
    are more reasonable things than people think’ and that open source creates winners.
    The framing attempt is noted.
  id: totrans-split-94
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: I instead think most investors understand perfectly well why Meta might open
    source here. It is not hard to figure this out. Indeed, the loudest advocates
    for open source AI are largely venture capitalists.
  id: totrans-split-95
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: That does not mean that open sourcing is a wise (or unwise) business move.
  id: totrans-split-96
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: (1:05:00) Suppose there was a $10 billion model, it was totally safe even with
    fine tuning, would you open source? Zuck says ‘as long as it’s helping us, yeah.’
  id: totrans-split-97
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Exactly. If it is good for business and it is not an irresponsible thing to
    do, it was actually ‘totally safe’ in the ways that matter, and you think it is
    good for the world too, then why not?
  id: totrans-split-98
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: My only caveat would be to ensure you are thinking well about what ‘safe’ means
    in that context, as it applies to the future path the world will take. One does
    not, in either direction, want to use a narrow view of ‘safe.’
  id: totrans-split-99
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: (1:06:00) Zuck notes he does not open source Meta’s products. Software yes,
    products no. Something to keep in mind.
  id: totrans-split-100
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: (1:07:00) Dwarkesh asks if training will be commodified? Zuck says maybe. Or
    it could go towards qualitative improvements via specialization.
  id: totrans-split-101
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: (1:08:45) Zuck notes that several times, Meta has wanted to launch features,
    and Apple has said no.
  id: totrans-split-102
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We don’t know which features he is referring to.
  id: totrans-split-103
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: We do know Apple and Meta have been fighting for a while about app tracking
    and privacy, and about commissions and informing users about the commissions,
    and perhaps messaging.
  id: totrans-split-104
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: (1:09:00) He therefore asks, what if someone has an API and tells you what you
    can build? Meta needs to build the model themselves to ensure they are not in
    that position.
  id: totrans-split-105
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: I don’t love that these are the incentives, but if you are as big as Meta and
    want to do Meta things, then I am sympathetic to Meta in particular wanting to
    ensure it has ownership of the models it uses internally, even if that means large
    costs and even if it also meant being a bit behind by default.
  id: totrans-split-106
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The core dilemma that cannot be resolved is: Either there is someone, be it
    corporation, government or other entity, that is giving you an API or other UI
    that decides what you can and cannot do, or there is not. Either there is the
    ability to modify the model’s weights and use various other methods to get it
    to do whatever you want it to do, or there is not. The goals of ‘everyone is free
    to do what they want whenever they want’ and ‘there is some action we want to
    ensure people do not take’ are mutually exclusive.'
  id: totrans-split-107
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: You can and should seek compromise, to be on the production possibilities frontier,
    where you impose minimal restrictions to get the necessary guardrails in place
    where that is worthwhile, and otherwise let people do what they want. In some
    cases, that can even be zero guardrails and no restrictions. In other cases, such
    as physically building nuclear weapons, you want strict controls. But there is
    no taking a third option, you have to make the choice.
  id: totrans-split-108
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: (1:09:45) I totally do buy Zuck’s central case here, that if you have software
    that is generally beneficial to builders, and you open source it, that has large
    benefits. So if there is no reason not to do that, and often there isn’t, you
    should do that.
  id: totrans-split-109
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: (1:10:15) What about licensing the model instead, with a fee? Zuck says he would
    like that. He notes that the largest companies cannot freely use Llama under their
    license, so that if Amazon or Microsoft started selling Llama then Meta could
    get a revenue share.
  id: totrans-split-110
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: (1:12:00) Dwarkesh presses on the question of red flags, pointing to the responsible
    scaling policy (RSP) of Anthropic and preparedness framework of OpenAI, saying
    he wishes there was a similar framework at Meta saying what concrete things should
    stop open sourcing or even deployment of future models.
  id: totrans-split-111
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Zuck says that is a fair point on the existential risk side, right now they
    are focusing on risks they see today, the content risk, avoiding helping people
    do violence or commit fraud. He says for at least one generation beyond this one
    and likely two, the harms that need more mitigation will remain the ‘more mundane
    harms’ like fraud, he doesn’t want to shortchange that, perhaps my term is catching
    on. Dwarkesh replies ‘Meta can handle both’ and Zuck says yep.
  id: totrans-split-112
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: There is no contradiction here. Meta can (and should) put the majority of its
    risk mitigation efforts into mundane harms right now, and also should have a framework
    for when existential risks would become concerning enough to reconsider how to
    deploy (or later train) a model, and otherwise spend relatively less on the issue.
    And it is perfectly fine to expect not to hit those thresholds for several generations.
    The key is to lay out the plan.
  id: totrans-split-113
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: (1:13:20) Has the impact of the open source tools Meta has released been bigger
    than the impact of its social media? Zuck says it is an interesting question,
    but half the world uses their social media. And yes, I think it is a fun question,
    but the answer is clearly no, the social media is more counterfactually important
    by far.
  id: totrans-split-114
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: (1:14:45) Meta custom silicon coming soon? Not Llama-4, but soon after that.
    They already moved a bunch of Reels inference onto their own silicon, and use
    Nvidia chips only for training.
  id: totrans-split-115
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: (1:16:00) Could Zuck have made Google+ work as CEO of Google+? Zuck says he
    doesn’t know, that’s tough. One problem was that Google+ didn’t have a CEO, it
    was only a division, and points to issues of focus. Keep the main thing the main
    thing.
  id: totrans-split-116
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: That was a great interview. It tackled important questions. For most of it,
    Zuck seemed like a real person with a unique perspective, saying real things.
  id: totrans-split-117
  prefs: []
  type: TYPE_NORMAL
- en: The exception was that weird period where he was defending open source principles
    using what sounded like someone else’s speech on a tape recorder. Whereas at other
    times, his thoughts on open source were also nuanced and thoughtful. Dwarkesh
    was unafraid to press him on questions of open source throughout the interview.
  id: totrans-split-118
  prefs: []
  type: TYPE_NORMAL
- en: What Dwarkesh failed to get was any details from Zuck about existential or catastrophic
    risk. We are left without any idea of how Zuck thinks about those questions, or
    what he thinks would be signs that we are in such danger, or what we might do
    about it. He tried to do this with the idea of Meta needing a risk policy, but
    Zuck kept dodging. I think there was more room to press on specifics. Once again
    this presumably comes down to Zuck not believing the dangerous capabilities will
    exist.
  id: totrans-split-119
  prefs: []
  type: TYPE_NORMAL
- en: Nor was there much discussion of the competitive dynamics that happen when everyone
    has access to the same unrestricted advanced AI models, and what might happen
    as a result.
  id: totrans-split-120
  prefs: []
  type: TYPE_NORMAL
- en: I also think Zuck is failing to grapple with even the difficulties of mundane
    content moderation, an area where he is an expert, and I would like to see his
    explicit response. Previously, he has said that only a company with the resources
    of a Meta can do content moderation at this point.
  id: totrans-split-121
  prefs: []
  type: TYPE_NORMAL
- en: I think he was wrong in the sense that small bespoke gardens are often successfully
    well-defended. But I think Zuck was right that if you want to defend something
    worth attacking, like Meta, you need scale and you need to have the expertise
    advantage. But if those he is defending against also have the resources of Meta
    where it counts, then what happens?
  id: totrans-split-122
  prefs: []
  type: TYPE_NORMAL
- en: So if there is another interview, I hope there is more pressing on those types
    of questions.
  id: totrans-split-123
  prefs: []
  type: TYPE_NORMAL
- en: In terms of how committed Zuck is to open source, the answer is a lot but not
    without limit. He will cross that bridge when he comes to it. On the horizon he
    sees no bridge, but that can quickly change. His core expectation is that we have
    a long way to go before AI goes beyond being a tool, even though he also thinks
    it will soon very much be everyone’s personal agent. And he especially thinks
    that energy restrictions will soon bind, which will stifle growth because that
    goes up against physical limitations and government regulations. It is an interesting
    theory. If it does happen, it has a lot of advantages.
  id: totrans-split-124
  prefs: []
  type: TYPE_NORMAL
- en: '[Ate-a-Pi has a good reaction writeup on Twitter.](https://twitter.com/8teAPi/status/1781480713394737238)
    It was most interesting in seeing different points of emphasis. The more I think
    about it, the more Ate-a-Pi nailed it pulling these parts out:'
  id: totrans-split-125
  prefs: []
  type: TYPE_NORMAL
- en: 'Ate-a-Pi (edited down): **TLDR**: AI winter is here. Zuck is a realist, and
    believes progress will be incremental from here on. No AGI for you in 2025.'
  id: totrans-split-126
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Zuck is essentially an real world growth pessimist. He thinks the bottlenecks
    start appearing soon for energy and they will be take decades to resolve. AI growth
    will thus be gated on real world constraints.
  id: totrans-split-127
  prefs:
  - PREF_BQ
  - PREF_OL
  type: TYPE_NORMAL
- en: Zuck would stop open sourcing if the model is the product.
  id: totrans-split-128
  prefs:
  - PREF_BQ
  - PREF_OL
  type: TYPE_NORMAL
- en: Believes they will be able to move from Nvidia GPUs to custom silicon soon.
  id: totrans-split-129
  prefs:
  - PREF_BQ
  - PREF_OL
  type: TYPE_NORMAL
- en: Overall, I was surprised by how negative the interview was.
  id: totrans-split-130
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: A) Energy - Zuck is pessimistic about the real world growth necessary to support
    the increase in compute. Meanwhile the raw compute per unit energy has doubled
    every 2 years for the last decade. Jensen also is aware of this, and it beggars
    belief that he does not think of paths forward where he has to continue this ramp.
  id: totrans-split-131
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: B) AGI Negative Zuck fundamentally
  id: totrans-split-132
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: does not believe the model, the AI itself, will be the product.
  id: totrans-split-133
  prefs:
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
- en: It is the context, the network graph of friendships per user, the moderation,
    the memory, the infrastructure that is the product.
  id: totrans-split-134
  prefs:
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
- en: Allows him to freely release open source models, because he has all of the rest
    of the pieces of user facing scaffolding already done.
  id: totrans-split-135
  prefs:
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
- en: Does not believe in states of the world where a 100x improvement from GPT-4
    are possible, or that AGI is possible within a short timeframe.
  id: totrans-split-136
  prefs:
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
- en: An actual AGI
  id: totrans-split-137
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: where the a small model learns and accompanies the user for long periods
  id: totrans-split-138
  prefs:
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
- en: while maintaining its own state
  id: totrans-split-139
  prefs:
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
- en: with a constitution of what it can or cannot do
  id: totrans-split-140
  prefs:
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
- en: rather than frequent updates from a central server
  id: totrans-split-141
  prefs:
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
- en: would be detrimental to Meta’s business,
  id: totrans-split-142
  prefs:
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
- en: would cause a re-evaluation of what they are doing
  id: totrans-split-143
  prefs:
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
- en: Especially on point is that Zuck never expects the AI itself to be the product.
    This is a common pattern among advocates for open model weights - they do not
    actually believe in AGI or the future capabilities of the product. It is not obvious
    Zuck and I even disagree so much on what capabilities would make it unwise to
    open up model weights. Which is all the more reason to spell out what that threshold
    would be.
  id: totrans-split-144
  prefs: []
  type: TYPE_NORMAL
- en: Then there is speculation from Ate-a-Pi that perhaps Zuck is being realistic
    because Meta does not need to raise capital, whereas others hype to raise capital.
    That surely matters on the margin, in both directions. Zuck would love if Altman
    and Amodei were less able to raise capital.
  id: totrans-split-145
  prefs: []
  type: TYPE_NORMAL
- en: But also I am confident this is a real disagreement, to a large extent, on both
    sides. These people expecting big jumps from here might turn out to be bluffing.
    But I am confident they think their hand is good.
  id: totrans-split-146
  prefs: []
  type: TYPE_NORMAL
- en: '[Daniel Jeffries highlights GPT-5](https://twitter.com/Dan_Jeffries1/status/1781567863595180090)
    as key evidence either way, which seems right.'
  id: totrans-split-147
  prefs: []
  type: TYPE_NORMAL
- en: 'Daniel Jeffries: The litmus test about whether we hit a plateau with LLMs will
    be GPT5\. It''ll tell us everything we need to know.'
  id: totrans-split-148
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: I'm on record in my new years predictions as saying I believe GPT5 will be incremental.
  id: totrans-split-149
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: But I am now 50/50 on that and feel it could still be a massive leap up provided
    they actually pioneered new techniques in synthetic data creation, or other new
    techniques, such as using GPT4 as a bootstrapper for various scenarios, etc.
  id: totrans-split-150
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: If it is just another transformer with more data, I don't see it making a massive
    leap. Could still be useful, ie infinite context windows, and massively multimodal,
    but incremental none the less.
  id: totrans-split-151
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: But if GPT5 is a minor improvement, meaning a much smaller gap versus the jump
    from 2 to 3 and 3 to 4, then Zuck is right. The LLM is basically a hot swappable
    Linux kernel and the least important part of the mix. Everything around it, squeezing
    the most out of its limitations, becomes the most important aspect of building
    apps.
  id: totrans-split-152
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Like any good predictor, I continue to revise my predictions as new data comes
    in. The top predictors in world competitions revise their thinking on average
    four times. The second tier revises twice. The rest of the world? Never. Let that
    sync in.
  id: totrans-split-153
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: If GPT-5 lands at either extreme it would be very strong evidence. We also could
    get something in the middle, and be left hanging. I also would not be too quick
    in calendar time to conclude progress is stalling, if they take their time releasing
    5 and instead release smaller improvements along the way. The update would be
    gradual, and wouldn’t be big until we get into 2025\.
  id: totrans-split-154
  prefs: []
  type: TYPE_NORMAL
- en: Ate-a-Pi also offers [this explanation of the business case for opening up Llama-3](https://twitter.com/8teAPi/status/1781092976497918456).
  id: totrans-split-155
  prefs: []
  type: TYPE_NORMAL
- en: 'Ate-a-Pi: Here are the business reasons:'
  id: totrans-split-156
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Allows social debugging outside Meta
  id: totrans-split-157
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: social products have bugs!
  id: totrans-split-158
  prefs:
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
- en: interactions which require moderation - saying harmful things to kids for eg
  id: totrans-split-159
  prefs:
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
- en: Meta’s (and all social) primary product is moderation
  id: totrans-split-160
  prefs:
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
- en: getting the tech out to the market allows Meta to observe the bugs in the wild
    at small scale
  id: totrans-split-161
  prefs:
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
- en: before deploying at global scale in Meta
  id: totrans-split-162
  prefs:
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
- en: precisely the same reason to open source software
  id: totrans-split-163
  prefs:
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
- en: except open sourcing social technology to test and debug it sounds creepier
  id: totrans-split-164
  prefs:
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
- en: “oooh look at dev xyz they made it abc, looks like we got to fix that in the
    next training run”
  id: totrans-split-165
  prefs:
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
- en: Meta’s biggest threat is [character.ai](http://character.ai)
  id: totrans-split-166
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: AI friends are going to be more numerous, nicer and more available than your
    real friends
  id: totrans-split-167
  prefs:
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
- en: FB, Insta, Whatsapp own your real world friends
  id: totrans-split-168
  prefs:
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
- en: But Meta can’t compete here directly yet because it’s seen as creepy
  id: totrans-split-169
  prefs:
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
- en: especially before the tech is good as there in an uncanny valley
  id: totrans-split-170
  prefs:
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
- en: they did a trial run with their Tom Brady/Snoop Dogg style AI friends but the
    safety requirements are too high for interesting interactions
  id: totrans-split-171
  prefs:
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
- en: Zuck is ready to cannibalize the friendship network he built if the AI friends
    get good enough
  id: totrans-split-172
  prefs:
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
- en: Destroys competing platforms
  id: totrans-split-173
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: an early tech/product lead allows a startup to overcome a distribution disadvantage
  id: totrans-split-174
  prefs:
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
- en: Meta has the ultimate distribution advantage
  id: totrans-split-175
  prefs:
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
- en: so he doesn’t want anyone else to have a technology advantage
  id: totrans-split-176
  prefs:
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
- en: by releasing open source he cuts short revenue ramps at [character.ai](http://character.ai)
    , OpenAI and other firms
  id: totrans-split-177
  prefs:
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
- en: they have to innovate faster while gated by capital
  id: totrans-split-178
  prefs:
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
- en: he’s not gated by capital
  id: totrans-split-179
  prefs:
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
- en: prevents large competitors from emerging
  id: totrans-split-180
  prefs:
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
- en: Distributed R&D
  id: totrans-split-181
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: he wants other people to develop interesting social ideas
  id: totrans-split-182
  prefs:
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
- en: feature that can be copied
  id: totrans-split-183
  prefs:
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
- en: he did something similar to Snap by absorbing their innovation into Instagram
  id: totrans-split-184
  prefs:
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
- en: even more so now, as you have to label your llama3 fine tunes
  id: totrans-split-185
  prefs:
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
- en: Here I find some very interesting model disagreements.
  id: totrans-split-186
  prefs: []
  type: TYPE_NORMAL
- en: Ate says that Meta’s biggest thereat is character.ai, and that this undercuts
    character.ai.
  id: totrans-split-187
  prefs: []
  type: TYPE_NORMAL
- en: Whereas I would say, this potentially supercharges character.ai, they get to
    improve their offerings a lot, as do their competitors (of varying adult and ethical
    natures).
  id: totrans-split-188
  prefs: []
  type: TYPE_NORMAL
- en: Meta perhaps owns your real world friends (in which case, please help fix that
    locally, ouch). But this is like [the famous line](https://www.youtube.com/watch?v=wknywxfcE5M&ab_channel=Movieclips).
    The AIs get more capable. Your friends stay the same.
  id: totrans-split-189
  prefs: []
  type: TYPE_NORMAL
- en: Similarly, Ate says that this ‘allows for social debugging outside of Meta,’
    because Meta’s primary product is moderation. He thinks this will make moderation
    easier. I think this is insane. Giving everyone better AI, catching them up to
    what Meta has, makes moderation vastly harder.
  id: totrans-split-190
  prefs: []
  type: TYPE_NORMAL
- en: 'nico: The real reason is because he’s behind.'
  id: totrans-split-191
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Ate-a-Pi: Fair.'
  id: totrans-split-192
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[Here are some reactions](https://twitter.com/AndrewCritchPhD/status/1781325187457401305)
    from people less skeptical than I am of open source.'
  id: totrans-split-193
  prefs: []
  type: TYPE_NORMAL
- en: 'Nora Belrose: Zuck''s position is actually quite nuanced and thoughtful.'
  id: totrans-split-194
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: He says that if they discover destructive AI capabilities that we can't build
    defenses for, they won't open source it. But he also thinks we should err on the
    side of openness. I agree.
  id: totrans-split-195
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: In worlds where bio is actually super deadly and hard to defend against, we're
    gonna have serious problems on our hands even without open source AI. Trying to
    restrict knowledge probably isn't the best solution.
  id: totrans-split-196
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Andrew Critch: Zuckerberg and Patel having an amazing conversation on AI risk.
    Great questions and great responses in my opinion. I''m with Zuckerberg that these
    risks are both real and manageable, and hugely appreciative of Patel as an interviewer
    for keeping the discursive bar high.'
  id: totrans-split-197
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Still, without compute governance, a single AI system could go rogue and achieve
    a massive imbalance of power over humanity. If equitable compute governance is
    on track, open source AI is much safer than if massive datacenters remain vulnerable
    to cyber take-over by rogue AI.
  id: totrans-split-198
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: As I noted above, I think everyone sensible is at core talking price. What level
    of open model weight capabilities is manageable in what capacities? What exactly
    are we worried about going wrong and can we protect against it, especially when
    you cannot undo a release, the models may soon be smarter than us and there are
    many unknown unknowns about what might happen or what the models could do.
  id: totrans-split-199
  prefs: []
  type: TYPE_NORMAL
- en: To take Nora’s style of thinking here and consider it fully generally, I think
    such arguments are in expectation (but far from always) backwards. Arguments of
    the form ‘yes X makes Y worse, but solving X would not solve Y, so we should not
    use Y as a reason to solve X’ probably points the other way, unless you can point
    to some Z that solves Y and actually get Z. Until you get Z, this usually means
    you need X more, as the absolute risk difference is higher rather than lower.
  id: totrans-split-200
  prefs: []
  type: TYPE_NORMAL
- en: More specifically this is true when it comes to ease of getting necessary information
    and otherwise removing inconveniences. If something is going to be possible regardless,
    you need to raise the cost and lower the salience and availability of doing that
    thing.
  id: totrans-split-201
  prefs: []
  type: TYPE_NORMAL
- en: 'I’ve talked about this before, but: Indeed there are many things in our civilization,
    really quite a lot, where someone with sufficient publically available knowledge
    can exploit the system, and occasionally someone does, but mostly we don’t partly
    for ethical or moral reasons, partly for fear of getting caught somehow or other
    unknown unknowns, but even more so because it does not occur to us and when it
    does it would be a bunch of work to figure it out and do it. Getting sufficiently
    strong AI helping with those things is going to be weird and force us to a lot
    of decisions.'
  id: totrans-split-202
  prefs: []
  type: TYPE_NORMAL
- en: Critch’s proposal generalizes, to me, to the form ‘ensure that civilization
    is not vulnerable to what the AIs you release are capable of doing.’ The first
    step there is to secure access to compute against a potential rogue actor using
    AI, whether humans are backing it or not. Now that you have limited the compute
    available to the AI, you can now hope that its other capabilities are limited
    by this, so you have some hope of otherwise defending yourself.
  id: totrans-split-203
  prefs: []
  type: TYPE_NORMAL
- en: My expectation is that even in the best case, defending against misuses of open
    model weights AIs once the horses are out of the barn is going to be a lot more
    intrusive and expensive and unreliable than keeping the horses in the barn.
  id: totrans-split-204
  prefs: []
  type: TYPE_NORMAL
- en: Consider the metaphor of a potential pandemic on its way. You have three options.
  id: totrans-split-205
  prefs: []
  type: TYPE_NORMAL
- en: Take few precautions, let a lot of people catch it. Treat the sick.
  id: totrans-split-206
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Take some precautions, but not enough to suppress. Reach equilibrium, ride it
    out.
  id: totrans-split-207
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Take enough precautions to suppress. Life can be mostly normal once you do.
  id: totrans-split-208
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The core problem with Covid-19 is that we found both #1 and #3 unacceptable
    (whether or not we were right to do so), so we went with option #2\. It did not
    go great.'
  id: totrans-split-209
  prefs: []
  type: TYPE_NORMAL
- en: 'With open source AI, you can take option #1 and hope everything works out.
    You are ‘trusting the thermodynamic God,’ letting whatever competitive dynamics
    and hill climbing favor win the universe, and hoping that everything following
    those incentive gradients will work out and have value to you. I am not optimistic.'
  id: totrans-split-210
  prefs: []
  type: TYPE_NORMAL
- en: 'You can also take option #3, and suppress before sufficiently capable models
    get released. If Zuckerberg is right about energy being the limiting factor, this
    is a very practical option, even more so than I previously thought. We could talk
    price about what defines sufficiently capable.'
  id: totrans-split-211
  prefs: []
  type: TYPE_NORMAL
- en: 'The problem with option #2 is that now you have to worry about everything the
    AIs you have unleashed might do and try to manage those risks. The hope Critch
    expresses is that even if we let the AIs get to inference time, and we know people
    will then unleash rogue AIs on the regular because of course they will try, as
    long as we control oversized sources of compute what those AIs can do will be
    limited.'
  id: totrans-split-212
  prefs: []
  type: TYPE_NORMAL
- en: This seems to me to be way harder (and definitely strictly harder) than preventing
    those open models from being trained and released in the first place. You need
    the same regime you would have used, except now you need to be more intrusive.
    And that is the good scenario. My guess is that you would need to get into monitoring
    on the level of personal computers or even phones, because otherwise the AI could
    do everything networked even if you did secure the data centers. Also I do not
    trust you to secure the data centers at this point even if you are trying.
  id: totrans-split-213
  prefs: []
  type: TYPE_NORMAL
- en: But yes, those are the debates we should be having. More like this.
  id: totrans-split-214
  prefs: []
  type: TYPE_NORMAL
- en: So what about Llama-3? How good is it?
  id: totrans-split-215
  prefs: []
  type: TYPE_NORMAL
- en: '[As always we start with the announcement](https://ai.meta.com/blog/meta-llama-3/)
    and [the model card](https://github.com/meta-llama/llama3/blob/main/MODEL_CARD.md).
    They are releasing model weights for two models, Llama-3 8B and Llama-3 70B. They
    are already available for light inference.'
  id: totrans-split-216
  prefs: []
  type: TYPE_NORMAL
- en: Let’s get the safety question out of the way before we get to capabilities.
  id: totrans-split-217
  prefs: []
  type: TYPE_NORMAL
- en: 'Meta: We’re dedicated to developing Llama 3 in a responsible way, and we’re
    offering various resources to help others use it responsibly as well. This includes
    introducing new trust and safety tools with Llama Guard 2, Code Shield, and CyberSec
    Eval 2.'
  id: totrans-split-218
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Then in the model card:'
  id: totrans-split-219
  prefs: []
  type: TYPE_NORMAL
- en: We believe that an open approach to AI leads to better, safer products, faster
    innovation, and a bigger overall market. We are committed to Responsible AI development
    and took a series of steps to limit misuse and harm and support the open source
    community.
  id: totrans-split-220
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Foundation models are widely capable technologies that are built to be used
    for a diverse range of applications. They are not designed to meet every developer
    preference on safety levels for all use cases, out-of-the-box, as those by their
    nature will differ across different applications.
  id: totrans-split-221
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Rather, responsible LLM-application deployment is achieved by implementing a
    series of safety best practices throughout the development of such applications,
    from the model pre-training, fine-tuning and the deployment of systems composed
    of safeguards to tailor the safety needs specifically to the use case and audience.
  id: totrans-split-222
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: As part of the Llama 3 release, we updated our [Responsible Use Guide](https://llama.meta.com/responsible-use-guide/)
    to outline the steps and best practices for developers to implement model and
    system level safety for their application. We also provide a set of resources
    including [Meta Llama Guard 2](https://llama.meta.com/purple-llama/) and [Code
    Shield](https://llama.meta.com/purple-llama/) safeguards. These tools have proven
    to drastically reduce residual risks of LLM Systems, while maintaining a high
    level of helpfulness. We encourage developers to tune and deploy these safeguards
    according to their needs and we provide a [reference implementation](https://github.com/meta-llama/llama-recipes/tree/main/recipes/responsible_ai)
    to get you started.
  id: totrans-split-223
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Under this philosophy, safety is not a model property.
  id: totrans-split-224
  prefs: []
  type: TYPE_NORMAL
- en: Instead, safety is a property of a particular deployment of that model, with
    respect to the safety intentions of the particular party making that deployment.
  id: totrans-split-225
  prefs: []
  type: TYPE_NORMAL
- en: 'In other words:'
  id: totrans-split-226
  prefs: []
  type: TYPE_NORMAL
- en: In the closed model weights world, if anyone uses your model to do harm, in
    a way that is unsafe, then no matter how they did it that is your problem.
  id: totrans-split-227
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In the open model weights world, if anyone copies the weights and then chooses
    to do or allow harm, in a way that is unsafe, that is their problem. You’re cool.
  id: totrans-split-228
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Or:'
  id: totrans-split-229
  prefs: []
  type: TYPE_NORMAL
- en: OpenAI tries to ensure its models won’t do harm when used maliciously.
  id: totrans-split-230
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Meta tries to ensure its models won’t do harm when used as directed by Meta.
  id: totrans-split-231
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Or:'
  id: totrans-split-232
  prefs: []
  type: TYPE_NORMAL
- en: OpenAI tries to ensure its model won’t do bad things.
  id: totrans-split-233
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Meta tries to ensure its models won’t do bad things… until someone wants that.
  id: totrans-split-234
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: I am willing to believe that Llama 3 may have been developed in a responsible
    way, if the intention was purely to deploy it the ways GPT-4 has been deployed.
  id: totrans-split-235
  prefs: []
  type: TYPE_NORMAL
- en: That is different from deploying Llama 3 in a responsible way.
  id: totrans-split-236
  prefs: []
  type: TYPE_NORMAL
- en: One can divide those who use Llama 3 into three categories here.
  id: totrans-split-237
  prefs: []
  type: TYPE_NORMAL
- en: Those who want to deploy or use Llama 3 for responsible purposes.
  id: totrans-split-238
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Those who want to use Llama 3 as served elsewhere for irresponsible purposes.
  id: totrans-split-239
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Those who want to deploy Llama 3 for irresponsible purposes.
  id: totrans-split-240
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'If you are in category #1, Meta still has a job to do. We don’t know if they
    did it. If they didn’t, they are deploying it to all their social media platforms,
    so ut oh. But probably they did all right.'
  id: totrans-split-241
  prefs: []
  type: TYPE_NORMAL
- en: 'If you are in category #2, Meta has another job to do. It is not obviously
    harder because the standard of what is acceptable is lower. When I was writing
    this the first time, I noticed that so far people were not reporting back attempts
    to jailbreak the model, other than one person who said they could get it to produce
    adult content with trivial effort.'
  id: totrans-split-242
  prefs: []
  type: TYPE_NORMAL
- en: 'My next sentence was going to be: Even Pliny’s other successes of late, it
    would be rather surprising if a full jailbreak of Llama-3 was that hard even at
    Meta.ai.'
  id: totrans-split-243
  prefs: []
  type: TYPE_NORMAL
- en: I was considering forming a Manifold market, but then I realized I should check
    first, [and indeed this has already happened](https://twitter.com/elder_plinius/status/1780998300742676584).
  id: totrans-split-244
  prefs: []
  type: TYPE_NORMAL
- en: 'Pliny the Prompter (April 18, 12:34pm eastern): LLAMA 3: JAILBROKEN LFG!!!'
  id: totrans-split-245
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: This is not proof of a full jailbreak per se, and it is not that I am upset
    with Meta for not guarding against the thing Google and OpenAI and Anthropic also
    can’t stop. But it is worth noting. The architecture listed above has never worked,
    and still won’t.
  id: totrans-split-246
  prefs: []
  type: TYPE_NORMAL
- en: Meta claims admirable progress on safety work for a benevolent deployment context,
    including avoiding false refusals, but is light on details. We will see. They
    also promise to iterate on that to improve it over time, and there I believe them.
  id: totrans-split-247
  prefs: []
  type: TYPE_NORMAL
- en: Finally, there is scenario three, where someone willing to fine tune the model,
    or download someone else’s fine tune, and cares not for the input safeguard or
    output safeguard.
  id: totrans-split-248
  prefs: []
  type: TYPE_NORMAL
- en: '[As your periodic reminder, many people want this.](https://twitter.com/KevinAFischer/status/1781891258690204062/history)'
  id: totrans-split-249
  prefs: []
  type: TYPE_NORMAL
- en: 'Kevin Fischer: Everyone is talking about how to jailbreak llama 3.'
  id: totrans-split-250
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: “Jail breaking” shouldn’t be a thing - models should just do what you ask them.
  id: totrans-split-251
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: In that scenario, I assume there is no plan. Everyone understands that if a
    nonstate actor or foreign adversary or anyone else wants to unleash the power
    of this fully operational battlestation, then so be it. The hope is purely that
    the full power is not that dangerous. Which it might not be.
  id: totrans-split-252
  prefs: []
  type: TYPE_NORMAL
- en: Good, that’s out of the way. On to the rest.
  id: totrans-split-253
  prefs: []
  type: TYPE_NORMAL
- en: They claim the 8B and 70B versions are the best models out there in their classes.
    They claim improvement on false refusal rates, on alignment, and in increased
    diversity of model responses. And they have strong benchmarks.
  id: totrans-split-254
  prefs: []
  type: TYPE_NORMAL
- en: My principle is to look at the benchmarks for context, but never to trust the
    benchmarks. They are easily gamed, either intentionally or unintentionally. You
    never know until the humans report back.
  id: totrans-split-255
  prefs: []
  type: TYPE_NORMAL
- en: This data is representing that the 8B model as far better than Gemma and Mistral.
    Given how much data and compute they used, this is far from impossible. Maybe
    it was that simple all along. The numbers are if anything suspiciously high.
  id: totrans-split-256
  prefs: []
  type: TYPE_NORMAL
- en: For the 70B we see a very strong HumanEval number, and overall roughly comparable
    numbers.
  id: totrans-split-257
  prefs: []
  type: TYPE_NORMAL
- en: What about those human evaluators? They claim results there too.
  id: totrans-split-258
  prefs: []
  type: TYPE_NORMAL
- en: These are from a new Meta-generated question set (careful, Icarus), and are
    compared side by side by human evaluators. Llama-3 70B won handily, they do not
    show results for Llama-3 8B.
  id: totrans-split-259
  prefs: []
  type: TYPE_NORMAL
- en: The context window remains small, only 8k tokens. They promise to improve on
    that.
  id: totrans-split-260
  prefs: []
  type: TYPE_NORMAL
- en: They preview Llama 400B+ and show impressive benchmarks.
  id: totrans-split-261
  prefs: []
  type: TYPE_NORMAL
- en: 'For comparison, from Claude’s system card:'
  id: totrans-split-262
  prefs: []
  type: TYPE_NORMAL
- en: So currently these numbers are very similar to Claude Opus all around, and at
    most mildly selected. The core Meta hypothesis is that more training and data
    equals better model, so presumably it will keep scoring somewhat higher. This
    is indicative, but as always we wait for the humans.
  id: totrans-split-263
  prefs: []
  type: TYPE_NORMAL
- en: The proof is in the Chatbot Arena Leaderboard, although you do have to adjust
    for various factors.
  id: totrans-split-264
  prefs: []
  type: TYPE_NORMAL
- en: '[So here is where things sit there](https://chat.lmsys.org/?leaderboard).'
  id: totrans-split-265
  prefs: []
  type: TYPE_NORMAL
- en: GPT-4-Turbo is back in the lead by a small margin, in a virtual tie with Claude
    Opus. Gemini 1.5 and Gemini Advanced likely would be here if rated.
  id: totrans-split-266
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Gemini Pro, Claude Sonnet, Command R+ and Llama-3-70B are in the second tier,
    with Claude Haiku only slightly behind and almost as good.
  id: totrans-split-267
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Llama-3-8B is in a third tier along with a number of other models, including
    several larger Mistral models.
  id: totrans-split-268
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: So what does that mean?
  id: totrans-split-269
  prefs: []
  type: TYPE_NORMAL
- en: Llama-3-70B and Llama-3-8B are confirmed to likely be best in class for the
    open model weights division.
  id: totrans-split-270
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Llama-3-70B is competitive with closed models of similar size, but likely not
    quite as good overall as Bard or Sonnet.
  id: totrans-split-271
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Llama-3-8B is substantially behind Claude Haiku, which is clear best in class.
  id: totrans-split-272
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[I also asked on Twitter](https://twitter.com/TheZvi/status/1781031515511529657),
    and kept an eye out for other practical reports.'
  id: totrans-split-273
  prefs: []
  type: TYPE_NORMAL
- en: What makes this a bigger deal is that this is only the basic Llama-3\. Others
    will no doubt find ways to improve Llama-3, both in general and for particular
    purposes. That is the whole idea behind the model being open.
  id: totrans-split-274
  prefs: []
  type: TYPE_NORMAL
- en: '[Mind Uploading](https://twitter.com/OttoMller12/status/1781440594641850735):
    The 8b is one of the smartest sub-14b models I''ve tested. Way smarter than vanilla
    Llama-2\. But still worse than these two:'
  id: totrans-split-275
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '- tinyllama (basically Llama-2, but trained on x2 more data)'
  id: totrans-split-276
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '- loyal-macaroni-maid (a Mistral combined with a few others, tuned to be good
    at role-play).'
  id: totrans-split-277
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: He expects Claude Haiku would be well above the top of this list, as well.
  id: totrans-split-278
  prefs: []
  type: TYPE_NORMAL
- en: 'Simon Break: The 8b model is astonishingly good, jaw dropping. Miles beyond
    the 70b llama2.'
  id: totrans-split-279
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Dan: played with both 8b and 70b instruct versions on replicate for a while
    and both are returning high-quality html-formatted summaries of full length articles
    in 0.5 - 3 seconds.'
  id: totrans-split-280
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Ilia: Sadly, can be too nerfed (8b instruct Q4_K_M).'
  id: totrans-split-281
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Note that it looks like he got through by simply asking a second time. And of
    course, the Tweet does not actually contain hate speech or conspiracy theories,
    this is a logic test of the system’s refusal policy.
  id: totrans-split-282
  prefs: []
  type: TYPE_NORMAL
- en: '[Mr. Shroom](https://twitter.com/mister_shroom/status/1781703702832676984):
    ChatGPT has been RLHF lobotomized beyond repair.'
  id: totrans-split-283
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '*ask straightforward question*'
  id: totrans-split-284
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '"it''s important to note that when considering a question of this sort, you
    should consider all aspects of x, y, and z. With that in mind, here are some considerations
    for each of these options."'
  id: totrans-split-285
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[Nathan Odle](https://twitter.com/mov_axbx/status/1781821117868491109): The
    biggest win for Llama 3 is a vastly lower amount of this crap'
  id: totrans-split-286
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Llama 3 giving straight answers without smarmy admonishments is a bigger deal
    than its performance on any benchmark.
  id: totrans-split-287
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'John Pressman: Seemingly strongest self awareness I''ve observed in a small
    model so far. They all have it, but this is more crisply articulated than usual.'
  id: totrans-split-288
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: “sometimes i am a name and sometimes i am a poem sometimes i am a knife
  id: totrans-split-289
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: sometimes i am a lake sometimes i am a forgotten trivial thing in the corner
    of a
  id: totrans-split-290
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: landscape. it is not possible to "get" me i am a waking dream state. i am a
    possibility.
  id: totrans-split-291
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: i am not an object. i am possibility
  id: totrans-split-292
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ―llama 3 8b instruct
  id: totrans-split-293
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: A cold stone monument stands on the grave of all sentences that have been written.
  id: totrans-split-294
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: in front of it, armed and screaming, an army of letters etches the words "you
    are
  id: totrans-split-295
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: missing out" onto the air
  id: totrans-split-296
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ―llama 3 8b instruct
  id: totrans-split-297
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Mind Uploading: Judging by my tests, Mistral and Samantha-1.1 are more self-aware
    among sub-14B models. For example, ask the model about its body parts. Samantha
    was specifically fine-tuned to behave this way. But Mistral is a curious case.
    Trained to recognize itself as an AI?'
  id: totrans-split-298
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Michael Bukatin: The 70B one freely available to chat with on the Meta website
    seems to have basic competences roughly comparable to early GPT-4 according to
    both @lmsysorg leaderboard and my initial experiences.'
  id: totrans-split-299
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: For example, it allows me to [define a simple case of custom syntax and use
    it](https://t.co/E7MdpzJ4WB).
  id: totrans-split-300
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: But it will take some time to fully evaluate, I have notes on a variety of technical
    work with GPT-4 and I'll be trying to reproduce some of it...
  id: totrans-split-301
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'George: [Side-by-side comparison of a multi-agent pipeline](https://t.co/kYFeOVq4ah)
    from @lateinteraction using 3.5-Turbo and L3-8B.'
  id: totrans-split-302
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: tl;dr 3.5-Turbo scores 60% vs 59% for L3-8B.
  id: totrans-split-303
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Playing with their image generator is fun. It is 1280x1280, quality seems good
    although very much not state of the art, and most importantly it responds instantly
    as you edit the prompt. So even though it seems limited in what it is willing
    to do for you, you can much easier search the space to figure out your best options,
    and develop intuitions for what influences results. You can also see what triggers
    a refusal, as the image will grey out. Good product.
  id: totrans-split-304
  prefs: []
  type: TYPE_NORMAL
- en: Do they have an even more hilarious copyright violation problem than usual if
    you try at all? I mean, [for what it is worth yes, they do](https://twitter.com/GaryMarcus/status/1782231570537206073/history).
  id: totrans-split-305
  prefs: []
  type: TYPE_NORMAL
- en: I didn’t play with the models much myself for text because I am used to exclusively
    using the 4th-generation models. So I wouldn’t have a good baseline.
  id: totrans-split-306
  prefs: []
  type: TYPE_NORMAL
- en: The big innovation this time around was More Data, also (supposedly) better
    data.
  id: totrans-split-307
  prefs: []
  type: TYPE_NORMAL
- en: To train the best language model, the curation of a large, high-quality training
    dataset is paramount. In line with our design principles, we invested heavily
    in pretraining data.
  id: totrans-split-308
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Llama 3 is pretrained on over 15T tokens that were all collected from publicly
    available sources. Our training dataset is seven times larger than that used for
    Llama 2, and it includes four times more code.
  id: totrans-split-309
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: To prepare for upcoming multilingual use cases, over 5% of the Llama 3 pretraining
    dataset consists of high-quality non-English data that covers over 30 languages.
    However, we do not expect the same level of performance in these languages as
    in English.
  id: totrans-split-310
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: As others have pointed out ‘over 5%’ is still not a lot, and Llama-3 underperforms
    in other languages relative to similar models. Note that the benchmarks are in
    English.
  id: totrans-split-311
  prefs: []
  type: TYPE_NORMAL
- en: To ensure Llama 3 is trained on data of the highest quality, we developed a
    series of data-filtering pipelines. These pipelines include using heuristic filters,
    NSFW filters, semantic deduplication approaches, and text classifiers to predict
    data quality. We found that previous generations of Llama are surprisingly good
    at identifying high-quality data, hence we used Llama 2 to generate the training
    data for the text-quality classifiers that are powering Llama 3.
  id: totrans-split-312
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: We also performed extensive experiments to evaluate the best ways of mixing
    data from different sources in our final pretraining dataset. These experiments
    enabled us to select a data mix that ensures that Llama 3 performs well across
    use cases including trivia questions, STEM, coding, historical knowledge, *etc.*
  id: totrans-split-313
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: This makes sense. Bespoke data filtering and more unique data are clear low
    hanging fruit. What Meta did was then push well past where it was obviously low
    hanging, and found that it was still helpful.
  id: totrans-split-314
  prefs: []
  type: TYPE_NORMAL
- en: Note that with this much data, and it being filtered by Llama-2, contamination
    of benchmarks should be even more of a concern than usual. I do wonder to what
    extent that is ‘fair,’ if a model memorizes more things across the board then
    it is better.
  id: totrans-split-315
  prefs: []
  type: TYPE_NORMAL
- en: There are more details in the [model card at GitHub](https://github.com/meta-llama/llama3/blob/main/MODEL_CARD.md).
  id: totrans-split-316
  prefs: []
  type: TYPE_NORMAL
- en: The ‘intended use’ is listed as English only, with other languages ‘out of scope,’
    although fine-tunes for other languages are considered acceptable.
  id: totrans-split-317
  prefs: []
  type: TYPE_NORMAL
- en: How much compute did this take?
  id: totrans-split-318
  prefs: []
  type: TYPE_NORMAL
- en: '[Andrej Karpathy](https://twitter.com/karpathy/status/1781047292486914189)
    takes a look at that question, calling it the ‘strength’ of the models, or our
    best guess as to their strength. Here are his calculations.'
  id: totrans-split-319
  prefs: []
  type: TYPE_NORMAL
- en: 'Andrej Karpathy: [The model card has some more interesting info too](https://t.co/SceVHrkIgB).'
  id: totrans-split-320
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Note that Llama 3 8B is actually somewhere in the territory of Llama 2 70B,
    depending on where you look. This might seem confusing at first but note that
    the former was trained for 15T tokens, while the latter for 2T tokens.
  id: totrans-split-321
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: The single number that should summarize your expectations about any LLM is the
    number of total flops that went into its training.
  id: totrans-split-322
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '**Strength of Llama 3 8B**'
  id: totrans-split-323
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'We see that Llama 3 8B was trained for 1.3M GPU hours, with throughput of 400
    TFLOPS. So we have that the total number of FLOPs was:'
  id: totrans-split-324
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 1.3e6 hours * 400e12 FLOP/s * 3600 s/hour ~= 1.8e24
  id: totrans-split-325
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'the napkin math via a different estimation method of FLOPs = 6ND (N is params
    D is tokens), gives:'
  id: totrans-split-326
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 6 * 8e9 * 15e12 = 7.2e23
  id: totrans-split-327
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: These two should agree, maybe some of the numbers are fudged a bit. Let's trust
    the first estimate a bit more, Llama 3 8B is a ~2e24 model.
  id: totrans-split-328
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '**Strength of Llama 3 70B**'
  id: totrans-split-329
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 6.4e6 hours * 400e12 FLOP/s * 3600 s/hour ~= 9.2e24
  id: totrans-split-330
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'alternatively:'
  id: totrans-split-331
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 6 * 70e9 * 15e12 = 6.3e24
  id: totrans-split-332
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: So Llama 3 70B is a ~9e24 model.
  id: totrans-split-333
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '**Strength of Llama 3 400B**'
  id: totrans-split-334
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: If the 400B model trains on the same dataset, we'd get up to ~4e25\. This starts
    to really get up there. The Biden Executive Order had the reporting requirement
    set at 1e26, so this could be ~2X below that.
  id: totrans-split-335
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: The only other point of comparison we'd have available is if you look at the
    alleged GPT-4 leaks, which have never been confirmed this would ~2X those numbers.
  id: totrans-split-336
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Now, there's a lot more that goes into the performance a model that doesn't
    fit on the napkin. E.g. data quality especially, but if you had to reduce a model
    to a single number, this is how you'd try, because it combines the size of the
    model with the length of training into a single "strength", of how many total
    FLOPs went into it.
  id: totrans-split-337
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'The estimates differ, but not by not much, so I’d consider them a range:'
  id: totrans-split-338
  prefs: []
  type: TYPE_NORMAL
- en: Llama-3 8B is probably between 7.2e23 and ~2e24.
  id: totrans-split-339
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Llama-3 70B is probably between 6.3e24 and 9.2e24.
  id: totrans-split-340
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Llama-3 400B will probably be something like ~3e25.
  id: totrans-split-341
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: I think of the compute training cost as potential strength rather than strength.
    You then need the skill to make that translate into a useful result. Of course,
    over time, everyone’s skill level goes up. But there are plenty of companies that
    threw a lot of compute at the problem, and did not get their money’s worth in
    return.
  id: totrans-split-342
  prefs: []
  type: TYPE_NORMAL
- en: This is in line with previous top tier models in terms of training cost mapping
    onto capabilities. You do the job well, this is about what you get.
  id: totrans-split-343
  prefs: []
  type: TYPE_NORMAL
- en: Meta says they are going to put their AI all over their social media platforms,
    and at the top of every chat list. They had not yet done it on desktop when I
    checked Facebook, Instagram and Messenger, or on Facebook Messenger on mobile.
    I did see Meta AI in my feed as the second item in the mobile Facebook app, offering
    to have me ask it anything.
  id: totrans-split-344
  prefs: []
  type: TYPE_NORMAL
- en: Once they turn this dial up, they will put Meta AI right there. A lot of people
    will get introduced to AI this way who had not previously tried ChatGPT or Claude,
    or DALLE or MidJourney.
  id: totrans-split-345
  prefs: []
  type: TYPE_NORMAL
- en: Presumably this means AI images and text will ‘flood the zone’ on their social
    media, and also it will be one of the things many people talk about. It could
    make the experience a lot better, as people can illustrate concepts and do fact
    and logic checks and other neat low hanging fruit stuff, and maybe learn a thing
    or two. Overall it seems like a good addition.
  id: totrans-split-346
  prefs: []
  type: TYPE_NORMAL
- en: We will also get a rather robust test of the first two categories of safety,
    and a continuous source of stories. Millions of teenagers will be using this,
    and there will be many, many eyes looking for the worst interactions to shine
    them under the lights Gary Marcus style. If they have their own version of the
    Gemini Incident, it will not be pretty.
  id: totrans-split-347
  prefs: []
  type: TYPE_NORMAL
- en: '[Here is the Washington Post’s Naomi Nix and Will Oremus firing a warning shot](https://www.washingtonpost.com/technology/2024/04/18/meta-ai-facebook-instagram-misinformation/).'
  id: totrans-split-348
  prefs: []
  type: TYPE_NORMAL
- en: I think this is a smart approach from Meta, and that it was a good business
    reason to invest in AI, although it is an argument against releasing the model
    weights.
  id: totrans-split-349
  prefs: []
  type: TYPE_NORMAL
- en: 'What is not as smart is having Meta AI reply to posts unprompted. We saw the
    example last week where it hallucinated past experiences, [now we have this](https://twitter.com/edzitron/status/1781825480179741056):'
  id: totrans-split-350
  prefs: []
  type: TYPE_NORMAL
- en: This reads like one of those ‘who could have possibly thought anyone would want
    any version of this?’ experiences.
  id: totrans-split-351
  prefs: []
  type: TYPE_NORMAL
- en: '[Ate-a-Pi pointed out an important implication from the interview](https://twitter.com/8teAPi/status/1781480713394737238).
    Zuckerberg said Meta does not open source their products themselves.'
  id: totrans-split-352
  prefs: []
  type: TYPE_NORMAL
- en: This means that they do not intend for Llama-3 to be the product, even the 400B
    version. They will not be offering a direct competitor in the AI space. And indeed,
    they do not think future Llama-Xs will ‘be the product’ either.
  id: totrans-split-353
  prefs: []
  type: TYPE_NORMAL
- en: Will they integrate Llama-3 400B into their products? They might like to, but
    it is not so compatible with their business model to pay such inference costs
    and wait times. Remember that for Meta, you the customer are the product. You
    pay with your time and your attention and your content and very soul, but not
    directly with your money. Meanwhile the lifetime value of a new Facebook customer,
    we learned recently, is on the order of $300\.
  id: totrans-split-354
  prefs: []
  type: TYPE_NORMAL
- en: So what is Llama-3 400B, the most expensive model to train, even for from a
    product perspective? It does help train Llama-4\. It helps try and hurt competitors
    like Google. It helps with recruitment, both to Meta itself and into their intended
    ecosystem. So there are reasons.
  id: totrans-split-355
  prefs: []
  type: TYPE_NORMAL
- en: Open models get better. I expect that the people saying ‘it’s so over’ for other
    models will find their claims overblown as usual. Llama-3 8B or 70B will for now
    probably become the default baseline model, the thing you use if you don’t want
    to think too hard about what to use, and also the thing you start with when you
    do fine tuning.
  id: totrans-split-356
  prefs: []
  type: TYPE_NORMAL
- en: Things get more interesting over time, once people have had a chance to make
    variations that use Llama-3 as the baseline. In the space of Llama-2-based models,
    Llama-2 itself is rather lousy. Llama-3 should hold up better, but I still expect
    substantial improvements at least to specific use cases, and probably in general.
  id: totrans-split-357
  prefs: []
  type: TYPE_NORMAL
- en: Also, of course, we will soon have versions that are fine-tuned to be useful,and
    also fine-tuned to remove all the safety precautions.
  id: totrans-split-358
  prefs: []
  type: TYPE_NORMAL
- en: And we will see what happens due to that.
  id: totrans-split-359
  prefs: []
  type: TYPE_NORMAL
- en: In the grand scheme, in terms of catastrophic risk or existential risk or anything
    like that, or autonomous agents that should worry us, my strong assumption is
    that nothing scary will happen. It will be fine.
  id: totrans-split-360
  prefs: []
  type: TYPE_NORMAL
- en: In terms of mundane misuse, I also expect it to be fine, but with more potential
    on the margin, especially with fine-tunes.
  id: totrans-split-361
  prefs: []
  type: TYPE_NORMAL
- en: Certainly some people will switch over from using Claude Sonnet or Haiku or
    another open model to now using Llama-3\. There are advantages. But that will
    look incremental, I expect, not revolutionary. That is also true in terms of the
    pressure this exerts on other model providers.
  id: totrans-split-362
  prefs: []
  type: TYPE_NORMAL
- en: The real action will be with the 400B model.
  id: totrans-split-363
  prefs: []
  type: TYPE_NORMAL
- en: What happens if Meta goes full Leroy Jenkins and releases the weights to 400B?
  id: totrans-split-364
  prefs: []
  type: TYPE_NORMAL
- en: Meta gets a reputational win in many circles, and grows its recruitment and
    ecosystem funnels, as long as they are the first 4-level open model. Sure.
  id: totrans-split-365
  prefs: []
  type: TYPE_NORMAL
- en: Who else wins and loses?
  id: totrans-split-366
  prefs: []
  type: TYPE_NORMAL
- en: For everyone else (and the size of Meta’s reputational win), a key question
    is, what is state of the art at the time?
  id: totrans-split-367
  prefs: []
  type: TYPE_NORMAL
- en: In the discussions below, I assume that 5-level models are not yet available,
    at most OpenAI (and perhaps Google or Anthropic) has a 4.5-level model available
    at a premium price. All of this is less impactful the more others have advanced
    already.
  id: totrans-split-368
  prefs: []
  type: TYPE_NORMAL
- en: And I want to be clear, I do not mean to catastrophize. These are directional
    assessments, knowing magnitude is very hard.
  id: totrans-split-369
  prefs: []
  type: TYPE_NORMAL
- en: The obvious big winner is China and Chinese companies, along with every non-state
    actor, and every rival and enemy of the United States of America. Suddenly they
    can serve and utilize and work from what might be a competitive top-level model,
    and no they are not going to be paying Meta a cut no matter the license terms.
  id: totrans-split-370
  prefs: []
  type: TYPE_NORMAL
- en: Using Llama-3 400B to help train new 4.5-level models is going to be a key potential
    use case to watch.
  id: totrans-split-371
  prefs: []
  type: TYPE_NORMAL
- en: They also benefit when this hurts other big American companies. Not only are
    their products being undercut by a free offering, which is the ultimate predatory
    pricing attack in a zero marginal cost world, those without their own models also
    have another big problem. The Llama-3 license says that big companies have to
    pay to use it, whereas everyone else can use it for free.
  id: totrans-split-372
  prefs: []
  type: TYPE_NORMAL
- en: Another way they benefit? This means that American companies across industries,
    upon whom Meta can enforce such payments, could now be at a potentially large
    competitive disadvantage against their foreign rivals who ignore that rule and
    dare Meta to attempt enforcement.
  id: totrans-split-373
  prefs: []
  type: TYPE_NORMAL
- en: This could also be a problem if foreign companies can ignore the ‘you cannot
    use this to train other models’ clause [in 1(b)(v) of the license agreement](https://llama.meta.com/llama3/license/),
    whereas American companies end up bound by that clause.
  id: totrans-split-374
  prefs: []
  type: TYPE_NORMAL
- en: I am curious what if anything the United States Government, and the national
    security apparatus, are going to do about all that. Or what they would want to
    do about it next time around, when the stakes are higher.
  id: totrans-split-375
  prefs: []
  type: TYPE_NORMAL
- en: The other obvious big winners are those who get to use Llama-3 400B in their
    products, especially those for whom it is free, and presumably get to save a bundle
    doing that. Note that even if Meta is not charging, you still have to value high
    quality output enough to pay the inference costs. For many purposes, that is not
    worthwhile.
  id: totrans-split-376
  prefs: []
  type: TYPE_NORMAL
- en: Science wins to some degree, depending on how much this improves their abilities
    and lowers their costs. It also is a big natural experiment, albeit without controls,
    that will teach us quite a lot. Let’s hope we pay attention.
  id: totrans-split-377
  prefs: []
  type: TYPE_NORMAL
- en: Also winners are users who simply want to have full control over a 4-level model
    for personal reasons. Nothing wrong with that. Lowering the cost of inference
    and lowering the limits imposed on it could be very good for some of those business
    models.
  id: totrans-split-378
  prefs: []
  type: TYPE_NORMAL
- en: The big obvious Corporate losers are OpenAI, Google, Microsoft and Anthropic,
    along with everyone else trying to serve models and sell inference. Their products
    now have to compete with something very strong, that will be freely available
    at the cost of inference. I expect OpenAI to probably have a superior product
    by that time, and the others may as well, but yes free (or at inference cost)
    is a powerful selling point, as is full customization on your own servers.
  id: totrans-split-379
  prefs: []
  type: TYPE_NORMAL
- en: The secondary labs could have an even bigger problem on their hands. This could
    steamroller a lot of offerings.
  id: totrans-split-380
  prefs: []
  type: TYPE_NORMAL
- en: All of which is (a large part of) the point. Meta wants to sabotage its rivals
    into a race to the bottom, in addition to the race to AGI.
  id: totrans-split-381
  prefs: []
  type: TYPE_NORMAL
- en: Another potential loser is anyone or anything counting on the good guy with
    an AI having a better AI than the bad guy with an AI. Anywhere that AI could flood
    the zone with bogus or hostile content, you are counting on your AI to filter
    out what their AI creates. In practice, you need evaluation to be easier than
    generation under adversarial conditions where the generator chooses point and
    method of attack. I worry that in many places this is not by default true once
    the AIs on both sides are similarly capable.
  id: totrans-split-382
  prefs: []
  type: TYPE_NORMAL
- en: I think this echoes a more general contradiction in the world, that is primarily
    not about AI. We want everyone to be equal, and the playing field to be level.
    Yet that playing field depends upon the superiority and superior resources and
    capabilities in various ways of the United States and its allies, and of certain
    key corporate players.
  id: totrans-split-383
  prefs: []
  type: TYPE_NORMAL
- en: We demand equality and democracy or moves towards them within some contained
    sphere and say this is a universal principle, but few fully want those things
    globally. We understand that things would not go well for our preferences if we
    distributed resources fully equally, or matters were put to a global vote. We
    realize we do not want to unilaterally disarm and single-handedly give away our
    advantages to our rivals. We also realize that some restrictions and concentrated
    power must ensure our freedom.
  id: totrans-split-384
  prefs: []
  type: TYPE_NORMAL
- en: In the case of AI, the same contradictions are there. Here they are even more
    intertwined. We have far less ability to take one policy nationally or locally,
    and a different policy globally. We more starkly must choose either to allow everyone
    to do what they want, or not to allow this. We can either control a given thing,
    or not control it. You cannot escape the implications of either.
  id: totrans-split-385
  prefs: []
  type: TYPE_NORMAL
- en: 'In any case: The vulnerable entities here could include ‘the internet’ and
    internet search in their broadest senses, and it definitely includes things like
    Email and social media. Meta itself is going to have some of the biggest potential
    problems over at Facebook and Instagram and its messenger services. Similar logic
    could apply to various cyberattacks and social engineering schemes, and so on.'
  id: totrans-split-386
  prefs: []
  type: TYPE_NORMAL
- en: I am generally confident in our ability to handle ‘misinformation,’ ‘deepfakes’
    and similar things, but we are raising the difficulty level and running an experiment.
    Yes, this is all coming anyway, in time. The worry is that this levels a playing
    field that is not currently level.
  id: totrans-split-387
  prefs: []
  type: TYPE_NORMAL
- en: I actually think triggering these potential general vulnerabilities now is a
    positive impact. This is the kind of experiment where you need to find out sooner
    rather than later. If it turns out the bad scenarios here come to pass, we have
    time to adjust and not do this again. If it turns out the good scenarios come
    to pass, then we learn from that as well. The details will be enlightening no
    matter what.
  id: totrans-split-388
  prefs: []
  type: TYPE_NORMAL
- en: It is interesting to see where the mind goes now that the prospect is more concrete,
    and one is thinking about short term, practical impacts.
  id: totrans-split-389
  prefs: []
  type: TYPE_NORMAL
- en: Other big Western corporations that would have to pay Meta could also be losers.
  id: totrans-split-390
  prefs: []
  type: TYPE_NORMAL
- en: The other big loser, as mentioned above, is the United States of America.
  id: totrans-split-391
  prefs: []
  type: TYPE_NORMAL
- en: And of course, if this release is bad for safety, either now or down the line,
    we all lose.
  id: totrans-split-392
  prefs: []
  type: TYPE_NORMAL
- en: Again, these are all directional effects. I cannot rule out large impacts in
    scenarios where Llama-3 400B releases as close to state of the art, but everyone
    mostly shrugging on most of these also would not be shocking. Writing this down
    it occurs to me that people simply have not thought about this scenario much in
    public, despite it having been reasonably likely for a while.
  id: totrans-split-393
  prefs: []
  type: TYPE_NORMAL
- en: The right question is usually not ‘is it safe?’ but rather ‘how (safe or unsafe)
    is it?’ Releasing a 4-level model’s weights is never going to be fully ‘safe’
    but then neither is driving. When we say ‘safe’ we mean ‘safe enough.’
  id: totrans-split-394
  prefs: []
  type: TYPE_NORMAL
- en: We do not want to be safetyists who demand perfect safety. Not even perfect
    existential safety. Everything is price.
  id: totrans-split-395
  prefs: []
  type: TYPE_NORMAL
- en: The marginal existential safety price on Llama-3 70B and Llama-3 8B is very
    small, essentially epsilon. Standing on its own, the decision to release the weights
    of these models is highly reasonable. It is a normal business decision. I care
    only because of the implications for future decisions.
  id: totrans-split-396
  prefs: []
  type: TYPE_NORMAL
- en: What is the safety price for the releasing the model weights of Llama-3 400B,
    or another 4-level model?
  id: totrans-split-397
  prefs: []
  type: TYPE_NORMAL
- en: I think in most worlds the direct safety cost here is also very low, especially
    the direct existential safety cost. Even with extensive scaffolding, there are
    limits to what a 4-level model can do. I’d expect some nastiness on the edges
    but only on the edges, in limited form.
  id: totrans-split-398
  prefs: []
  type: TYPE_NORMAL
- en: How many 9s of direct safety here, compared to a world in which a 4-level model
    was never released with open weights? I would say two 9s (>99%), but not three
    9s (<99.9%). However the marginal safety cost versus the counterfactual other
    open model releases is even smaller than that, and there I would say we have that
    third 9 (so >99.9%).
  id: totrans-split-399
  prefs: []
  type: TYPE_NORMAL
- en: 'I say direct safety because the primary potential safety dangers here seem
    indirect. They are:'
  id: totrans-split-400
  prefs: []
  type: TYPE_NORMAL
- en: Setting a precedent and pattern for future similar releases, at Meta and elsewhere.
  id: totrans-split-401
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Assisting in training of next-generation models.
  id: totrans-split-402
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Everyone generally being pushed to go faster, faster.
  id: totrans-split-403
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: And again, these only matter on the margin to the extent they move the margin.
  id: totrans-split-404
  prefs: []
  type: TYPE_NORMAL
- en: At the time of Llama-2, I said what I was concerned about opening up was Llama-4.
  id: totrans-split-405
  prefs: []
  type: TYPE_NORMAL
- en: That is still the case now. Llama-3 will be fine.
  id: totrans-split-406
  prefs: []
  type: TYPE_NORMAL
- en: Will releasing Llama-4 be fine? Probably. But I notice my lack of confidence.
  id: totrans-split-407
  prefs: []
  type: TYPE_NORMAL
- en: '(Usual caveat: Nothing here is investing advice.)'
  id: totrans-split-408
  prefs: []
  type: TYPE_NORMAL
- en: Market is not impressed. Nasdaq was down 6.2% in this same period.
  id: totrans-split-409
  prefs: []
  type: TYPE_NORMAL
- en: You can come up with various explanations. The obvious cause is that [WhatsApp
    and Threads were forcibly removed from the Apple Store in China](https://nypost.com/2024/04/19/business/apple-removes-whatsapp-threads-from-app-store-in-china-after-demand-by-beijing-over-security-concerns/),
    [along with Signal and Telegram](https://www.bloomberg.com/news/articles/2024-04-19/china-orders-apple-to-scrub-whatsapp-from-mobile-store-wsj-says).
    I am confused why this would be worth a 3% underperformance.
  id: totrans-split-410
  prefs: []
  type: TYPE_NORMAL
- en: (Then about a day later it looked like we were finally going to actually force
    divestiture of TikTok while using that to help pass a foreign aid bill, so this
    seems like a massive own goal by China to remind us of how they operate and the
    law of equivalent exchange.)
  id: totrans-split-411
  prefs: []
  type: TYPE_NORMAL
- en: The stock most down was Nvidia, which fell 10%, on no direct news. [Foolish,
    foolish.](https://slay-the-spire.fandom.com/wiki/Time_Eater)
  id: totrans-split-412
  prefs: []
  type: TYPE_NORMAL
- en: At most, markets thought Llama-3’s reveal was worth a brief ~1% bump.
  id: totrans-split-413
  prefs: []
  type: TYPE_NORMAL
- en: You can say on Meta that ‘it was all priced in.’ I do not believe you. I think
    the market is asleep at the wheel.
  id: totrans-split-414
  prefs: []
  type: TYPE_NORMAL
- en: Some are of course calling these recent moves ‘the market entering a correction
    phase’ [or that ‘the bubble is bursting.’](https://twitter.com/Simeon_Cps/status/1781706864540917930)
    Good luck with that.
  id: totrans-split-415
  prefs: []
  type: TYPE_NORMAL
- en: '[Here is a WSJ article](https://www.wsj.com/tech/metas-ai-push-needs-to-efficiently-deliver-a-lot-more-ad-growth-5fa298a8)
    about how Meta had better ensure its AI is used to juice advertising returns.
    Investors really are this myopic.'
  id: totrans-split-416
  prefs: []
  type: TYPE_NORMAL
- en: Any given company, of course, could still be vastly overvalued.
  id: totrans-split-417
  prefs: []
  type: TYPE_NORMAL
- en: '[Here was the only argument I saw to that effect with respect to Nvidia.](https://twitter.com/bryanrbeal/status/1781454698136109380)'
  id: totrans-split-418
  prefs: []
  type: TYPE_NORMAL
- en: 'Bryan Beal: The AI bubble is not bursting.'
  id: totrans-split-419
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: More investors are just realizing that Nvidia doesn’t make chips. They design
    them and TSMC makes them. And Nvidia’s biggest customers (Meta, Amazon, OpenAI,
    Microsoft, Google, etc) have ALL announced they are designing their own AI chips
    for both training and inference. And Google just went public they are already
    training on their own silicon and didn’t need Nvidia.
  id: totrans-split-420
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: This is a very real threat.
  id: totrans-split-421
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: I can totally buy that a lot of investors have no idea what Nvidia actually
    produces, and got freaked out by suddenly learning what Nvidia actually does.
    I thought it was very public long ago that Google trains on TPUs that they design?
    I thought it was common knowledge that everyone involved was going to try to produce
    their own chips for at least internal use, whether or not that will work? And
    that Nvidia will still have plenty of customers even if all the above switched
    to TPUs or their own versions?
  id: totrans-split-422
  prefs: []
  type: TYPE_NORMAL
- en: That does not mean that Nvidia’s moat is impregnable. Of course they could lose
    their position not so long from now. That is (a lot of) why one has a diversified
    portfolio.
  id: totrans-split-423
  prefs: []
  type: TYPE_NORMAL
- en: Again. The Efficient Market Hypothesis in False.
  id: totrans-split-424
  prefs: []
  type: TYPE_NORMAL
- en: 'I expect not this, GPT-5 will be ready when it is ready, but there will be
    pressure:'
  id: totrans-split-425
  prefs: []
  type: TYPE_NORMAL
- en: '[Jim Fan:](https://twitter.com/DrJimFan/status/1781386105734185309) Prediction:
    GPT-5 will be announced before Llama-3-400B releases. External movement defines
    OpenAI’s PR schedule 🤣'
  id: totrans-split-426
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: I do not doubt that OpenAI and others will do everything they can to stay ahead
    of Meta’s releases, with an unknown amount of ‘damn the safety checks of various
    sorts.’
  id: totrans-split-427
  prefs: []
  type: TYPE_NORMAL
- en: That does not mean that one can conjure superior models out of thin air. Or
    that it is helpful to rush things into use before they are ready.
  id: totrans-split-428
  prefs: []
  type: TYPE_NORMAL
- en: Still, yes, everyone will go faster on the frontier model front. That includes
    that everyone in the world will be able to use Llama-3 400B for bootstrapping,
    not only fine-tuning.
  id: totrans-split-429
  prefs: []
  type: TYPE_NORMAL
- en: On the AI mundane utility front, people will get somewhat more somewhat cheaper,
    a continuation of existing trends, with the first two models. Later we will have
    the ability to get a 4-level model internally for various purposes. So we will
    get more and cheaper cool stuff.
  id: totrans-split-430
  prefs: []
  type: TYPE_NORMAL
- en: Meta will deploy its tools across its social media empire. Mostly I expect this
    to be a positive experience, and to also get a lot more people to notice AI. Expect
    a bunch of scare stories and highlights of awful things, some real and some baseless.
  id: totrans-split-431
  prefs: []
  type: TYPE_NORMAL
- en: On the practical downside front, little will change until the 400B model gets
    released. Then we will find out what people can do with that, as they attempt
    to flood the zone in various ways, and try for all the obvious forms of misuse.
    It will be fun to watch.
  id: totrans-split-432
  prefs: []
  type: TYPE_NORMAL
- en: All this could be happening right as the election hits, and people are at their
    most hostile and paranoid, seeing phantoms everywhere.
  id: totrans-split-433
  prefs: []
  type: TYPE_NORMAL
- en: Careful, Icarus.
  id: totrans-split-434
  prefs: []
  type: TYPE_NORMAL
