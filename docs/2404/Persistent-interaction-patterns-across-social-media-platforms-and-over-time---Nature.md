<!--yml
category: 未分类
date: 2024-05-27 13:03:25
-->

# Persistent interaction patterns across social media platforms and over time | Nature

> 来源：[https://www.nature.com/articles/s41586-024-07229-y](https://www.nature.com/articles/s41586-024-07229-y)

### Data collection

In our study, data collection from various social media platforms was strategically designed to encompass various topics, ensuring maximal heterogeneity in the discussion themes. For each platform, where feasible, we focus on gathering posts related to diverse areas such as politics, news, environment and vaccinations. This approach aims to capture a broad spectrum of discourse, providing a comprehensive view of conversation dynamics across different content categories.

#### Facebook

We use datasets from previous studies that covered discussions about vaccines^([50](/articles/s41586-024-07229-y#ref-CR50 "Schmidt, A. L., Zollo, F., Scala, A., Betsch, C. & Quattrociocchi, W. Polarization of the vaccination debate on Facebook. Vaccine 36, 3606–3612 (2018).")), news^([51](/articles/s41586-024-07229-y#ref-CR51 "Schmidt, A. L. et al. Anatomy of news consumption on Facebook. Proc. Natl Acad. Sci. USA 114, 3035–3039 (2017).")) and brexit^([52](/articles/s41586-024-07229-y#ref-CR52 "Del Vicario, M., Zollo, F., Caldarelli, G., Scala, A. & Quattrociocchi, W. Mapping social dynamics on Facebook: the brexit debate. Soc. Netw. 50, 6–16 (2017).")). For the vaccines topic, the resulting dataset contains around 2 million comments retrieved from public groups and pages in a period that ranges from 2 January 2010 to 17 July 2017\. For the news topic, we selected a list of pages from the Europe Media Monitor that reported the news in English. As a result, the obtained dataset contains around 362 million comments between 9 September 2009 and 18 August 2016\. Furthermore, we collect a total of about 4.5 billion likes that the users put on posts and comments concerning these pages. Finally, for the brexit topic, the dataset contains around 460,000 comments from 31 December 2015 to 29 July 2016.

#### Gab

We collect data from the Pushshift.io archive ([https://files.pushshift.io/gab/](https://files.pushshift.io/gab/)) concerning discussions taking place from 10 August 2016, when the platform was launched, to 29 October 2018, when Gab went temporarily offline due to the Pittsburgh shooting^([53](/articles/s41586-024-07229-y#ref-CR53 "Hunnicutt, T. & Dave, P. Gab.com goes offline after Pittsburgh synagogue shooting. Reuters,                    www.reuters.com/article/uk-pennsylvania-shooting-gab-idUKKCN1N20QN                                     (29 October 2018).")). As a result, we collect a total of around 14 million comments.

#### Reddit

Data were collected from the Pushshift.io archive ([https://pushshift.io/](https://pushshift.io/)) for the period ranging from 1 January 2018 to 31 December 2022\. For each topic, whenever possible, we manually identified and selected subreddits that best represented the targeted topics. As a result of this operation, we obtained about 800,000 comments from the r/conspiracy subreddit for the conspiracy topic. For the vaccines topic, we collected about 70,000 comments from the r/VaccineDebate subreddit, focusing on the COVID-19 vaccine debate. We collected around 400,000 comments from the r/News subreddit for the news topic. We collected about 70,000 comments from the r/environment subreddit for the climate change topic. Finally, we collected around 550,000 comments from the r/science subreddit for the science topic.

#### Telegram

We created a list of 14 channels, associating each with one of the topics considered in the study. For each channel, we manually collected messages and their related comments. As a result, from the four channels associated with the news topic (news notiziae, news ultimora, news edizionestraordinaria, news covidultimora), we obtained around 724,000 comments from posts between 9 April 2018 and 20 December 2022\. For the politics topic, instead, the corresponding two channels (politics besttimeline, politics polmemes) produced a total of around 490,000 comments between 4 August 2017 and 19 December 2022\. Finally, the eight channels assigned to the conspiracy topic (conspiracy bennyjhonson, conspiracy tommyrobinsonnews, conspiracy britainsfirst, conspiracy loomeredofficial, conspiracy thetrumpistgroup, conspiracy trumpjr, conspiracy pauljwatson, conspiracy iononmivaccino) produced a total of about 1.4 million comments between 30 August 2019 and 20 December 2022.

#### Twitter

We used a list of datasets from previous studies that includes discussions about vaccines^([54](/articles/s41586-024-07229-y#ref-CR54 "Valensise, C. M. et al. Lack of evidence for correlation between COVID-19 infodemic and vaccine acceptance. Preprint at                    arxiv.org/abs/2107.07946                                     (2021).")), climate change^([49](/articles/s41586-024-07229-y#ref-CR49 "Falkenberg, M. et al. Growing polarization around climate change on social media. Nat. Clim. Change 12, 1114–1121 (2022).")) and news^([55](/articles/s41586-024-07229-y#ref-CR55 "Quattrociocchi, A., Etta, G., Avalle, M., Cinelli, M. & Quattrociocchi, W. in Social Informatics (eds Hopfgartner, F. et al.) 245–256 (Springer, 2022).")) topics. For the vaccines topic, we collected around 50 million comments from 23 January 2010 to 25 January 2023\. For the news topic, we extend the dataset used previously^([55](/articles/s41586-024-07229-y#ref-CR55 "Quattrociocchi, A., Etta, G., Avalle, M., Cinelli, M. & Quattrociocchi, W. in Social Informatics (eds Hopfgartner, F. et al.) 245–256 (Springer, 2022).")) by collecting all threads composed of less than 20 comments, obtaining a total of about 9.5 million comments for a period ranging from 1 January 2020 to 29 November 2022\. Finally, for the climate change topic, we collected around 9.7 million comments between 1 January 2020 and 10 January 2023.

#### Usenet

We collected data for the Usenet discussion system by querying the Usenet Archive ([https://archive.org/details/usenet?tab=about](https://archive.org/details/usenet?tab=about)). We selected a list of topics considered adequate to contain a large, broad and heterogeneous number of discussions involving active and populated newsgroups. As a result of this selection, we selected conspiracy, politics, news and talk as topic candidates for our analysis. For the conspiracy topic, we collected around 280,000 comments between 1 September 1994 and 30 December 2005 from the alt.conspiracy newsgroup. For the politics topics, we collected around 2.6 million comments between 29 June 1992 and 31 December 2005 from the alt.politics newsgroup. For the news topic, we collected about 620,000 comments between 5 December 1992 and 31 December 2005 from the alt.news newsgroup. Finally, for the talk topic, we collected all of the conversations from the homonym newsgroup on a period that ranges from 13 February 1989 to 31 December 2005 for around 2.1 million contents.

#### Voat

We used a dataset presented previously^([56](/articles/s41586-024-07229-y#ref-CR56 "Mekacher, A. & Papasavva, A. “I can’t keep it up” a dataset from the defunct voat.co news aggregator. In Proc. International AAAI Conference on Web and Social Media Vol. 16, 1302–1311 (AAAI, 2022).")) that covers the entire lifetime of the platform, from 9 January 2018 to 25 December 2020, including a total of around 16.2 million posts and comments shared by around 113,000 users in about 7,100 subverses (the equivalent of a subreddit for Voat). Similarly to previous platforms, we associated the topics to specific subverses. As a result of this operation, for the conspiracy topic, we collected about 1 million comments from the greatawakening subverse between 9 January 2018 and 25 December 2020\. For the politics topic, we collected around 1 million comments from the politics subverse between 16 June 2014 and 25 December 2020\. Finally, for the news topic, we collected about 1.4 million comments from the news subverse between 21 November 2013 and 25 December 2020.

#### YouTube

We used a dataset proposed in previous studies that collected conversations about the climate change topic^([49](/articles/s41586-024-07229-y#ref-CR49 "Falkenberg, M. et al. Growing polarization around climate change on social media. Nat. Clim. Change 12, 1114–1121 (2022).")), which is extended, coherently with previous platforms, by including conversations about vaccines and news topics. The data collection process for YouTube is performed using the YouTube Data API ([https://developers.google.com/youtube/v3](https://developers.google.com/youtube/v3)). For the climate change topic, we collected around 840,000 comments between 16 March 2014 and 28 February 2022\. For the vaccines topic, we collected conversations between 31 January 2020 and 24 October 2021 containing keywords about COVID-19 vaccines, namely Sinopharm, CanSino, Janssen, Johnson&Johnson, Novavax, CureVac, Pfizer, BioNTech, AstraZeneca and Moderna. As a result of this operation, we gathered a total of around 2.6 million comments to videos. Finally, for the news topic, we collected about 20 million comments between 13 February 2006 and 8 February 2022, including videos and comments from a list of news outlets, limited to the UK and provided by Newsguard (see the ‘Polarization and user leaning attribution’ section).

### Content moderation policies

Content moderation policies are guidelines that online platforms use to monitor the content that users post on their sites. Platforms have different goals and audiences, and their moderation policies may vary greatly, with some placing more emphasis on free expression and others prioritizing safety and community guidelines.

Facebook and YouTube have strict moderation policies prohibiting hate speech, violence and harassment^([57](/articles/s41586-024-07229-y#ref-CR57 "Facebook Community Standards, transparency.fb.com/policies/community-standards/hate-speech/ (Facebook, 2023).")). To address harmful content, Facebook follows a ‘remove, reduce, inform’ strategy and uses a combination of human reviewers and artificial intelligence to enforce its policies^([58](/articles/s41586-024-07229-y#ref-CR58 "Rosen, G. & Lyons, T. Remove, reduce, inform: new steps to manage problematic content. Meta, about.fb.com/news/2019/04/remove-reduce-inform-new-steps/ (10 April 2019).")). Similarly, YouTube has a similar set of community guidelines regarding hate speech policy, covering a wide range of behaviours such as vulgar language^([59](/articles/s41586-024-07229-y#ref-CR59 "Vulgar Language Policy, support.google.com/youtube/answer/10072685? (YouTube, 2023).")), harassment^([60](/articles/s41586-024-07229-y#ref-CR60 "Harassment & Cyberbullying Policies, support.google.com/youtube/answer/2802268 (YouTube, 2023).")) and, in general, does not allow the presence of hate speech and violence against individuals or groups based on various attributes^([61](/articles/s41586-024-07229-y#ref-CR61 "Hate Speech Policy, support.google.com/youtube/answer/2801939 (YouTube, 2023).")). To ensure that these guidelines are respected, the platform uses a mix of artificial intelligence algorithms and human reviewers^([62](/articles/s41586-024-07229-y#ref-CR62 "How Does YouTube Enforce Its Community Guidelines?,                    www.youtube.com/intl/enus/howyoutubeworks/policies/community-guidelines/enforcing-community-guidelines                                     (YouTube, 2023).")).

Twitter also has a comprehensive content moderation policy and specific rules against hateful conduct^([63](/articles/s41586-024-07229-y#ref-CR63 "The Twitter Rules, help.twitter.com/en/rules-and-policies/twitter-rules (Twitter, 2023)."),[64](/articles/s41586-024-07229-y#ref-CR64 "Hateful Conduct, help.twitter.com/en/rules-and-policies/hateful-conduct-policy (Twitter, 2023).")). They use automation^([65](/articles/s41586-024-07229-y#ref-CR65 "Gorwa, R., Binns, R. & Katzenbach, C. Algorithmic content moderation: technical and political challenges in the automation of platform governance. Big Data Soc. 7, 2053951719897945 (2020).")) and human review in the moderation process^([66](/articles/s41586-024-07229-y#ref-CR66 "Our Range of Enforcement Options, help.twitter.com/en/rules-and-policies/enforcement-options (Twitter, 2023).")). At the date of submission, Twitter’s content policies have remained unchanged since Elon Musk’s takeover, except that they ceased enforcing their COVID-19 misleading information policy on 23 November 2022\. Their policy enforcement has faced criticism for inconsistency^([67](/articles/s41586-024-07229-y#ref-CR67 "Elliott, V. & Stokel-Walker, C. Twitter’s moderation system is in tatters. WIRED (17 November 2022).")).

Reddit falls somewhere in between regarding how strict its moderation policy is. Reddit’s content policy has eight rules, including prohibiting violence, harassment and promoting hate based on identity or vulnerability^([68](/articles/s41586-024-07229-y#ref-CR68 "Reddit Content Policy,                    www.redditinc.com/policies/content-policy                                     (Reddit, 2023)."),[69](/articles/s41586-024-07229-y#ref-CR69 "Promoting Hate Based on Identity or Vulnerability,                    www.reddithelp.com/hc/en-us/articles/360045715951                                     (Reddit, 2023).")). Reddit relies heavily on user reports and volunteer moderators. Thus, it could be considered more lenient than Facebook, YouTube and Twitter regarding enforcing rules. In October 2022, Reddit announced that they intend to update their enforcement practices to apply automation in content moderation^([70](/articles/s41586-024-07229-y#ref-CR70 "Malik, A. Reddit acqui-hires team from ML content moderation startup Oterlu. TechCrunch, tcrn.ch/3yeS2Kd (4 October 2022).")).

By contrast, Telegram, Gab and Voat take a more hands-off approach with fewer restrictions on content. Telegram has ambiguity in its guidelines, which arises from broad or subjective terms and can lead to different interpretations^([71](/articles/s41586-024-07229-y#ref-CR71 "Terms of Service, telegram.org/tos (Telegram, 2023).")). Although they mentioned they may use automated algorithms to analyse messages, Telegram relies mainly on users to report a range of content, such as violence, child abuse, spam, illegal drugs, personal details and pornography^([72](/articles/s41586-024-07229-y#ref-CR72 "Durov, P. The rules of @telegram prohibit calls for violence and hate speech. We rely on our users to report public content that violates this rule. Twitter, twitter.com/durov/status/917076707055751168?lang=en (8 October 2017).")). According to Telegram’s privacy policy, reported content may be checked by moderators and, if it is confirmed to violate their terms, temporary or permanent restrictions may be imposed on the account^([73](/articles/s41586-024-07229-y#ref-CR73 "Telegram Privacy Policy, telegram.org/privacy (Telegram, 2023).")). Gab’s Terms of Service allow all speech protected under the First Amendment to the US Constitution, and unlawful content is removed. They state that they do not review material before it is posted on their website and cannot guarantee prompt removal of illegal content after it has been posted^([74](/articles/s41586-024-07229-y#ref-CR74 "Terms of Service, gab.com/about/tos (Gab, 2023).")). Voat was once known as a ‘free-speech’ alternative to Reddit and allowed content even if it may be considered offensive or controversial^([56](/articles/s41586-024-07229-y#ref-CR56 "Mekacher, A. & Papasavva, A. “I can’t keep it up” a dataset from the defunct voat.co news aggregator. In Proc. International AAAI Conference on Web and Social Media Vol. 16, 1302–1311 (AAAI, 2022).")).

Usenet is a decentralized online discussion system created in 1979\. Owing to its decentralized nature, Usenet has been difficult to moderate effectively, and it has a reputation for being a place where controversial and even illegal content can be posted without consequence. Each individual group on Usenet can have its own moderators, who are responsible for monitoring and enforcing their group’s rules, and there is no single set of rules that applies to the entire platform^([75](/articles/s41586-024-07229-y#ref-CR75 "Salzenberg, C. & Spafford, G. What is Usenet?, www0.mi.infn.it/∼calcolo/Wis usenet.html (1995).")).

### Logarithmic binning and conversation size

Owing to the heavy-tailed distributions of conversation length (Extended Data Fig. [1](/articles/s41586-024-07229-y#Fig5)), to plot the figures and perform the analyses, we used logarithmic binning. Thus, according to its length, each thread of each dataset is assigned to 1 out of 21 bins. To ensure a minimal number of points in each bin, we iteratively change the left bound of the last bin so that it contains at least *N* = 50 elements (we set *N* = 100 in the case of Facebook news, due to its larger size). Specifically, considering threads ordered in increasing length, the size of the largest thread is changed to that of the second last largest one, and the binning is recalculated accordingly until the last bin contains at least *N* points.

For visualization purposes, we provide a normalization of the logarithmic binning outcome that consists of mapping discrete points into coordinates of the *x* axis such that the bins correspond to {0, 0.05, 0.1, ..., 0.95, 1}.

To perform the part of the analysis, we select conversations belonging to the [0.7, 1] interval of the normalized logarithmic binning of thread length. This interval ensures that the conversations are sufficiently long and that we have a substantial number of threads. Participation and toxicity trends are obtained by applying to such conversations a linear binning of 21 elements to a chronologically ordered sequence of comments, that is, threads. A breakdown of the resulting datasets is provided in Supplementary Table [2](/articles/s41586-024-07229-y#MOESM1).

Finally, to assess the equality of the growth rates of participation values in toxic and non-toxic threads (see the ‘Conversation evolution and toxicity’ section), we implemented the following linear regression model:

$${\rm{p}}{\rm{a}}{\rm{r}}{\rm{t}}{\rm{i}}{\rm{c}}{\rm{i}}{\rm{p}}{\rm{a}}{\rm{t}}{\rm{i}}{\rm{o}}{\rm{n}}={\beta }_{0}+{\beta }_{1}\cdot {\rm{b}}{\rm{i}}{\rm{n}}+{\beta }_{2}\cdot \,({\rm{b}}{\rm{i}}{\rm{n}}\cdot {\rm{i}}{\rm{s}}{\rm{T}}{\rm{o}}{\rm{x}}{\rm{i}}{\rm{c}}),$$

where the term *β*[2] accounts for the effect that being a toxic conversation has on the growth of participation. Our results show that *β*[2] is not significantly different from 0 in most original and validation datasets (Supplementary Tables [8](/articles/s41586-024-07229-y#MOESM1) and [11](/articles/s41586-024-07229-y#MOESM1))

### Toxicity detection and validation of the models used

The problem of detecting toxicity is highly debated, to the point that there is currently no agreement on the very definition of toxic speech^([64](/articles/s41586-024-07229-y#ref-CR64 "Hateful Conduct, help.twitter.com/en/rules-and-policies/hateful-conduct-policy (Twitter, 2023)."),[76](/articles/s41586-024-07229-y#ref-CR76 "Castelle, M. The linguistic ideologies of deep abusive language classification. In Proc. 2nd Workshop on Abusive Language Online (ALW2) (eds Fišer, D. et al.) 160–170, aclanthology.org/W18-5120 (Association for Computational Linguistics, 2018).")). A toxic comment can be regarded as one that includes obscene or derogatory language^([32](/articles/s41586-024-07229-y#ref-CR32 "Davidson, T., Warmsley, D., Macy, M. & Weber, I. Automated hate speech detection and the problem of offensive language. In Proc. International AAAI Conference on Web and Social Media 11 (Association for the Advancement of Artificial Intelligence, 2017).")), that uses harsh, abusive language and personal attacks^([33](/articles/s41586-024-07229-y#ref-CR33 "Kolhatkar, V. et al. The SFU opinion and comments corpus: a corpus for the analysis of online news comments. Corpus Pragmat. 4, 155–190 (2020).")), or contains extremism, violence and harassment^([11](/articles/s41586-024-07229-y#ref-CR11 "Sheth, A., Shalin, V. L. & Kursuncu, U. Defining and detecting toxicity on social media: context and knowledge are key. Neurocomputing 490, 312–318 (2022).")), just to give a few examples. Even though toxic speech should, in principle, be distinguished from hate speech, which is commonly more related to targeted attacks that denigrate a person or a group on the basis of attributes such as race, religion, gender, sex, sexual orientation and so on^([77](/articles/s41586-024-07229-y#ref-CR77 "Tontodimamma, A., Nissi, E. & Sarra, A. E. A. Thirty years of research into hate speech: topics of interest and their evolution. Scientometrics 126, 157–179 (2021).")), it sometimes may also be used as an umbrella term^([78](/articles/s41586-024-07229-y#ref-CR78 "Sap, M. et al. Annotators with attitudes: how annotator beliefs and identities bias toxic language detection. In Proc. 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (eds. Carpuat, M. et al.) 5884–5906 (Association for Computational Linguistics, 2022)."),[79](/articles/s41586-024-07229-y#ref-CR79 "Pavlopoulos, J., Sorensen, J., Dixon, L., Thain, N. & Androutsopoulos, I. Toxicity detection: does context really matter? In Proc. 58th Annual Meeting of the Association for Computational Linguistics (eds Jurafsky, D. et al.) 4296–4305 (Association for Computational Linguistics, 2020).")). This lack of agreement directly reflects the challenging and inherent subjective nature of the concept of toxicity. The complexity of the topic makes it particularly difficult to assess the reliability of natural language processing models for automatic toxicity detection despite the impressive improvements in the field. Modern natural language processing models, such as Perspective API, are deep learning models that leverage word-embedding techniques to build representations of words as vectors in a high-dimensional space, in which a metric distance should reflect the conceptual distance among words, therefore providing linguistic context. A primary concern regarding toxicity detection models is their limited ability to contextualize conversations^([11](/articles/s41586-024-07229-y#ref-CR11 "Sheth, A., Shalin, V. L. & Kursuncu, U. Defining and detecting toxicity on social media: context and knowledge are key. Neurocomputing 490, 312–318 (2022)."),[80](/articles/s41586-024-07229-y#ref-CR80 "Yin, W. & Zubiaga, A. Hidden behind the obvious: misleading keywords and implicitly abusive language on social media. Online Soc. Netw. Media 30, 100210 (2022).")). These models often struggle to incorporate factors beyond the text itself, such as the participant’s personal characteristics, motivations, relationships, group memberships and the overall tone of the discussion^([11](/articles/s41586-024-07229-y#ref-CR11 "Sheth, A., Shalin, V. L. & Kursuncu, U. Defining and detecting toxicity on social media: context and knowledge are key. Neurocomputing 490, 312–318 (2022).")). Consequently, what is considered to be toxic content can vary significantly among different groups, such as ethnicities or age groups^([81](/articles/s41586-024-07229-y#ref-CR81 "Sap, M., Card, D., Gabriel, S., Choi, Y. & Smith, N. A. The risk of racial bias in hate speech detection. In Proc. 57th Annual Meeting of the Association for Computational Linguistics (eds Kohonen, A. et al.) 1668–1678 (Association for Computational Linguistics, 2019).")), leading to potential biases. These biases may stem from the annotators’ backgrounds and the datasets used for training, which might not adequately represent cultural heterogeneity. Moreover, subtle forms of toxic content, like indirect allusions, memes and inside jokes targeted at specific groups, can be particularly challenging to detect. Word embeddings equip current classifiers with a rich linguistic context, enhancing their ability to recognize a wide range of patterns characteristic of toxic expression. However, the requirements for understanding the broader context of a conversation, such as personal characteristics, motivations and group dynamics, remain beyond the scope of automatic detection models. We acknowledge these inherent limitations in our approach. Nonetheless, reliance on automatic detection models is essential for large-scale analyses of online toxicity like the one conducted in this study. We specifically resort to the Perspective API for this task, as it represents state-of-the-art automatic toxicity detection, offering a balance between linguistic nuance and scalable analysis capabilities. To define an appropriate classification threshold, we draw from the existing literature^([64](/articles/s41586-024-07229-y#ref-CR64 "Hateful Conduct, help.twitter.com/en/rules-and-policies/hateful-conduct-policy (Twitter, 2023).")), which uses 0.6 as the threshold for considering a comment to be toxic. This threshold can also be considered a reasonable one as, according to the developer guidelines offered by Perspective, it would indicate that the majority of the sample of readers, namely 6 out of 10, would perceive that comment as toxic. Due to the limitations mentioned above (for a criticism of Perspective API, see ref. ^([82](/articles/s41586-024-07229-y#ref-CR82 "Rosenblatt, L., Piedras, L. & Wilkins, J. Critical perspectives: a benchmark revealing pitfalls in PerspectiveAPI. In Proc. Second Workshop on NLP for Positive Impact (NLP4PI) (eds Biester, L. et al.) 15–24 (Association for Computational Linguistics, 2022)."))), we validate our results by performing a comparative analysis using two other toxicity detectors: Detoxify ([https://github.com/unitaryai/detoxify](https://github.com/unitaryai/detoxify)), which is similar to Perspective, and IMSYPP, a classifier developed for a European Project on hate speech^([16](/articles/s41586-024-07229-y#ref-CR16 "Cinelli, M. et al. Dynamics of online hate and misinformation. Sci. Rep. 11, 22083 (2021).")) ([https://huggingface.co/IMSyPP](https://huggingface.co/IMSyPP)). In Supplementary Table [14](/articles/s41586-024-07229-y#MOESM1), the percentages of agreement among the three models in classifying 100,000 comments taken randomly from each of our datasets are reported. For Detoxify we used the same binary toxicity threshold (0.6) as used with Perspective. Although IMSYPP operates on a distinct definition of toxicity as outlined previously^([16](/articles/s41586-024-07229-y#ref-CR16 "Cinelli, M. et al. Dynamics of online hate and misinformation. Sci. Rep. 11, 22083 (2021).")), our comparative analysis shows a general agreement in the results. This alignment, despite the differences in underlying definitions and methodologies, underscores the robustness of our findings across various toxicity detection frameworks. Moreover, we perform the core analyses of this study using all classifiers on a further, vast and heterogeneous dataset. As shown in Supplementary Figs. [1](/articles/s41586-024-07229-y#MOESM1) and [2](/articles/s41586-024-07229-y#MOESM1), the results regarding toxicity increase with conversation size and user participation and toxicity are quantitatively very similar. Furthermore, we verify the stability of our findings under different toxicity thresholds. Although the main analyses in this paper use the threshold value recommended by the Perspective API, set at 0.6, to minimize false positives, our results remain consistent even when applying a less conservative threshold of 0.5\. This is demonstrated in Extended Data Fig. [5](/articles/s41586-024-07229-y#Fig9), confirming the robustness of our observations across varying toxicity levels. For this study, we used the API support for languages prevalent in the European and American continents, including English, Spanish, French, Portuguese, German, Italian, Dutch, Polish, Swedish and Russian. Detoxify also offers multilingual support. However, IMSYPP is limited to English and Italian text, a factor considered in our comparative analysis.

### Polarization and user leaning attribution

Our approach to measuring controversy in a conversation is based on estimating the degree of political partisanship among the participants. This measure is closely related to the political science concept of political polarization. Political polarization is the process by which political attitudes diverge from moderate positions and gravitate towards ideological extremes, as described previously^([83](/articles/s41586-024-07229-y#ref-CR83 "DiMaggio, P., Evans, J. & Bryson, B. Have American’s social attitudes become more polarized? Am. J. Sociol. 102, 690–755 (1996).")). By quantifying the level of partisanship within discussions, we aim to provide insights into the extent and nature of polarization in online debates. In this context, it is important to distinguish between ‘ideological polarization’ and ‘affective polarization’. Ideological polarization refers to divisions based on political viewpoints. By contrast, affective polarization is characterized by positive emotions towards members of one’s group and hostility towards those of opposing groups^([84](/articles/s41586-024-07229-y#ref-CR84 "Fiorina, M. P. & Abrams, S. J. Political polarization in the American public. Annu. Rev. Polit. Sci. 11, 563–588 (2008)."),[85](/articles/s41586-024-07229-y#ref-CR85 "Iyengar, S., Gaurav, S. & Lelkes, Y. Affect, not ideology: a social identity perspective on polarization. Publ. Opin. Q. 76, 405–431 (2012).")). Here we focus specifically on ideological polarization. The subsequent description of our procedure for attributing user political leanings will further clarify this focus. On online social media, the individual leaning of a user toward a topic can be inferred through the content produced or the endorsement shown toward specific content. In this study, we consider the endorsement of users to news outlets of which the political leaning has been evaluated by trustworthy external sources. Although not without limitations—which we address below—this is a standard approach that has been used in several studies, and has become a common and established practice in the field of social media analysis due to its practicality and effectiveness in providing a broad understanding of political dynamics on these online platforms^([1](/articles/s41586-024-07229-y#ref-CR1 "Cinelli, M., Morales, G. D. F., Galeazzi, A., Quattrociocchi, W. & Starnini, M. The echo chamber effect on social media. Proc. Natl Acad. Sci. USA 118, e2023301118 (2021)."),[43](/articles/s41586-024-07229-y#ref-CR43 "Garimella, K., Morales, G. D. F., Gionis, A. & Mathioudakis, M. Quantifying controversy on social media. ACM Trans. Soc. Comput. 1, 3 (2018)."),[86](#ref-CR86 "Cota, W., Ferreira, S. & Pastor-Satorras, R. E. A. Quantifying echo chamber effects in information spreading over political communication networks. EPJ Data Sci. 8, 38 (2019)."),[87](#ref-CR87 "Bessi, A. et al. Users polarization on Facebook and Youtube. PLoS ONE 11, e0159641 (2016)."),[88](/articles/s41586-024-07229-y#ref-CR88 "Bessi, A. et al. Science vs conspiracy: collective narratives in the age of misinformation. PLoS ONE 10, e0118093 (2015).")). We label news outlets with a political score based on the information reported by Media Bias/Fact Check (MBFC) ([https://mediabiasfactcheck.com](https://mediabiasfactcheck.com)), integrating with the equivalent information from Newsguard ([https://www.newsguardtech.com/](https://www.newsguardtech.com/)). MBFC is an independent fact-checking organization that rates news outlets on the basis of the reliability and the political bias of the content that they produce and share. Similarly, Newsguard is a tool created by an international team of journalists that provides news outlet trust and political bias scores. Following standard methods used in the literature^([1](/articles/s41586-024-07229-y#ref-CR1 "Cinelli, M., Morales, G. D. F., Galeazzi, A., Quattrociocchi, W. & Starnini, M. The echo chamber effect on social media. Proc. Natl Acad. Sci. USA 118, e2023301118 (2021)."),[43](/articles/s41586-024-07229-y#ref-CR43 "Garimella, K., Morales, G. D. F., Gionis, A. & Mathioudakis, M. Quantifying controversy on social media. ACM Trans. Soc. Comput. 1, 3 (2018).")), we calculated the individual leaning of a user *l* ∈ [−1, 1] as the average of the leaning scores *l*[*c*] ∈ [−1, 1] attributed to each of the content it produced/shared, where *l*[*c*] results from a mapping of the news organizations political scores provided by MBFC and Newsguard, respectively: [left, centre-left, centre, centre-right, right] to [−1, − 0.5, 0, 0.5, 1], and [far left, left, right, far right] to [−1, −0.5, 0.5, 1]). Our datasets have different structures, so we have to evaluate user leanings in different ways. For Facebook News, we assign a leaning score to users who posted a like at least three times and commented at least three times under news outlet pages that have a political score. For Twitter News, a leaning is assigned to users who posted at least 15 comments under scored news outlet pages. For Twitter Vaccines and Gab, we consider users who shared content produced by scored news outlet pages at least three times. A limitation of our approach is that engaging with politically aligned content does not always imply agreement; users may interact with opposing viewpoints for critical discussion. However, research indicates that users predominantly share content aligning with their own views, especially in politically charged contexts^([87](/articles/s41586-024-07229-y#ref-CR87 "Bessi, A. et al. Users polarization on Facebook and Youtube. PLoS ONE 11, e0159641 (2016)."),[89](/articles/s41586-024-07229-y#ref-CR89 "Himelboim, I., McCreery, S. & Smith, M. Birds of a feather tweet together: integrating network and content analyses to examine cross-ideology exposure on Twitter. J. Comput. Med. Commun. 18, 40–60 (2013)."),[90](/articles/s41586-024-07229-y#ref-CR90 "An, J., Quercia, D. & Crowcroft, J. Partisan sharing: Facebook evidence and societal consequences. In Proc. Second ACM Conference on Online Social Networks, COSN′14 13–24 (Association for Computing Machinery, 2014).")). Moreover, our method captures users who actively express their political leanings, omitting the ‘passive’ ones. This is due to the lack of available data on users who do not explicitly state their opinions. Nevertheless, analysing active users offers valuable insights into the discourse of those most engaged and influential on social media platforms.

### Burst analysis

We used the Kleinberg burst detection algorithm^([46](/articles/s41586-024-07229-y#ref-CR46 "Kleinberg, J. Bursty and hierarchical structure in streams. Data Min. Knowl. Discov. 7, 373–397 (2003).")) (see the ‘Controversy and toxicity’ section) to all conversations with at least 50 comments in a dataset. In our analysis, we randomly sample up to 5,000 conversations, each containing a specific number of comments. To ensure the reliability of our data, we exclude conversations with an excessive number of double timestamps—defined as more than 10 consecutive or over 100 within the first 24 h. This criterion helps to mitigate the influence of bots, which could distort the patterns of human activity. Furthermore, we focus on the first 24 h of each thread to analyse streams of comments during their peak activity period. Consequently, Usenet was excluded from our study. The unique usage characteristics of Usenet render such a time-constrained analysis inappropriate, as its activity patterns do not align with those of the other platforms under consideration. By reconstructing the density profile of the comment stream, the algorithm divides the entire stream’s interval into subintervals on the basis of their level of intensity. Labelled as discrete positive values, higher levels of burstiness represent higher activity segments. To avoid considering flat-density phases, threads with a maximum burst level equal to 2 are excluded from this analysis. To assess whether a higher intensity of comments results in a higher comment toxicity, we perform a Mann–Whitney *U*-test^([91](/articles/s41586-024-07229-y#ref-CR91 "Mann, H. B. & Whitney, D. R. On a test of whether one of two random variables is stochastically larger than the other. Ann. Math. Stat. 18, 50–60 (1947).")) with Bonferroni correction for multiple testing between the distributions of the fraction of toxic comments *t*[*i*] in three intensity phases: during the peak of engagement and at the highest levels before and after. Extended Data Table [4](/articles/s41586-024-07229-y#Tab5) shows the corrected *P* values of each test, at a 0.99 confidence level, with H1 indicated in the column header. An example of the distribution of the frequency of toxic comments in threads at the three phases of a conversation considered (pre-peak, peak and post-peak) is reported in Fig. [4c](/articles/s41586-024-07229-y#Fig4).

### Toxicity detection on Usenet

As discussed in the section on toxicity detection and the Perspective API above, automatic detectors derive their understanding of toxicity from the annotated datasets that they are trained on. The Perspective API is predominantly trained on recent texts, and its human labellers conform to contemporary cultural norms. Thus, although our dataset dates back to no more than the early 1990s, we provide a discussion on the viability of the application of Perspective API to Usenet and validation analysis. Contemporary society, especially in Western contexts, is more sensitive to issues of toxicity, including gender, race and sexual orientation, compared with a few decades ago. This means that some comments identified as toxic today, including those from older platforms like Usenet, might not have been considered as such in the past. However, this discrepancy does not significantly affect our analysis, which is centred on current standards of toxicity. On the other hand, changes in linguistic features may have some repercussions: there may be words and locutions that were frequently used in the 1990s that instead appear sparsely in today’s language, making Perspective potentially less effective in classifying short texts that contain them. We therefore proceeded to evaluate the impact that such a possible scenario could have on our results. In light of the above considerations, we consider texts labelled as toxic as correctly classified; instead, we assume that there is a fixed probability *p* that a comment may be incorrectly labelled as non-toxic. Consequently, we randomly designate a proportion *p* of non-toxic comments, relabel them as toxic and compute the toxicity versus conversation size trend (Fig. [2](/articles/s41586-024-07229-y#Fig2)) on the altered dataset across various *p*. Specifically, for each value, we simulate 500 different trends, collecting their regression slopes to obtain a null distribution for them. To assess if the probability of error could lead to significant differences in the observed trend, we compute the fraction *f* of slopes lying outside the interval (−|*s*|,|*s*|), where *s* is the slope of the observed trend. We report the result in Supplementary Table [9](/articles/s41586-024-07229-y#MOESM1) for different values of *p*. In agreement with our previous analysis, we assume that the slope differs significantly from the ones obtained from randomized data if *f* is less than 0.05.

We observed that only the Usenet Talk dataset shows sensitivity to small error probabilities, and the others do not show a significant difference. Consequently, our results indicate that Perspective API is suitable for application to Usenet data in our analyses, notwithstanding the potential linguistic and cultural shifts that might affect the classifier’s reliability with older texts.

### Toxicity of short conversations

Our study focuses on the relationship between user participation and the toxicity of conversations, particularly in engaged or prolonged discussions. A potential concern is that concentrating on longer threads overlooks conversations that terminate quickly due to early toxicity, therefore potentially biasing our analysis. To address this, we analysed shorter conversations, comprising 6 to 20 comments, in each dataset. In particular, we computed the distributions of toxicity scores of the first and last three comments in each thread. This approach helps to ensure that our analysis accounts for a range of conversation lengths and patterns of toxicity development, providing a more comprehensive understanding of the dynamics at play. As shown in Supplementary Fig. [3](/articles/s41586-024-07229-y#MOESM1), for each dataset, the distributions of the toxicity scores display high similarity, meaning that, in short conversations, the last comments are not significantly more toxic than the initial ones, indicating that the potential effects mentioned above do not undermine our conclusions. Regarding our analysis of longer threads, we notice here that the participation quantity can give rise to similar trends in various cases. For example, high participation can be achieved because many users take part in the conversation, but also with small groups of users in which everyone is equally contributing over time. Or, in very large discussions, the contributions of individual outliers may remain hidden. By measuring participation, these and other borderline cases may not be distinct from the statistically highly likely discussion dynamics but, ultimately, this lack of discriminatory power does not have any implications on our findings nor on the validity of the conclusions that we draw.

### Reporting summary

Further information on research design is available in the [Nature Portfolio Reporting Summary](/articles/s41586-024-07229-y#MOESM2) linked to this article.