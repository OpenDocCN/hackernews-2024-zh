<!--yml

category: 未分类

date: 2024-05-27 13:00:56

-->

# 突发新闻：扩展永远无法让我们达到通用人工智能

> 来源：[https://garymarcus.substack.com/p/breaking-news-scaling-will-never](https://garymarcus.substack.com/p/breaking-news-scaling-will-never)

神经网络（至少是过去三十年中主导的配置）在超出其训练示例周围的多维空间进行泛化时存在困难。这限制了它们进行可靠推理和规划的能力。这也促使它们对数据的贪婪，甚至影响到它们开发者的伦理选择。永远不会有足够的数据；永远会有异常值。这就是为什么无人驾驶汽车仍然只是演示，以及为什么大型语言模型永远不会可靠的原因。

我已经在很多场合多次如此说过，回溯到[1998](https://www.sciencedirect.com/science/article/pii/S0010028598906946)，今天我要让来自赞比亚的敏锐思维技术企业家/计算机视觉研究员 Chomba Bupe 来试试看。

一个重要的[新预印本](https://arxiv.org/abs/2404.04125)关于数据和扩展刚刚发布。Bupe 很好地解释了：

简而言之，我们可以期待边际效益递减。（星号：当前模型并非字面上的查找表；它们可以在某种程度上泛化，但不够。正如我在1998年的论文和《代数心灵》中所解释的，它们可以在训练示例空间内泛化，但在那个空间之外遇到了相当大的困难。）

迟早，对数据的指数级需求将超过可获得的量。

要达到通用人工智能，我们需要能够在它们被训练的数据之外更好地泛化的替代方法。故事的结尾。

***Gary Marcus*** **期待着看看人们开始接受这些含义时会发生什么。**
