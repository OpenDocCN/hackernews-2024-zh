- en: <!--yml
  id: totrans-split-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-split-1
  prefs: []
  type: TYPE_NORMAL
  zh: 类别：未分类
- en: 'date: 2024-05-27 13:20:20'
  id: totrans-split-2
  prefs: []
  type: TYPE_NORMAL
  zh: 'date: 2024-05-27 13:20:20'
- en: -->
  id: totrans-split-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: Intel unveils largest-ever AI 'neuromorphic computer' that mimics the human
    brain | Live Science
  id: totrans-split-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 英特尔推出最大规模的AI类脑计算机，模仿人类大脑 | Live Science
- en: 来源：[https://www.livescience.com/technology/computing/intel-unveils-largest-ever-ai-neuromorphic-computer-that-mimics-the-human-brain](https://www.livescience.com/technology/computing/intel-unveils-largest-ever-ai-neuromorphic-computer-that-mimics-the-human-brain)
  id: totrans-split-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://www.livescience.com/technology/computing/intel-unveils-largest-ever-ai-neuromorphic-computer-that-mimics-the-human-brain](https://www.livescience.com/technology/computing/intel-unveils-largest-ever-ai-neuromorphic-computer-that-mimics-the-human-brain)
- en: Scientists at Intel have built the world's largest neuromorphic computer, or
    one designed and structured to mimic [the human brain](https://www.livescience.com/29365-human-brain.html).
    The company hopes it will support future artificial intelligence (AI) research.
  id: totrans-split-6
  prefs: []
  type: TYPE_NORMAL
  zh: 英特尔的科学家们建造了世界上最大的类脑计算机，或一种旨在模仿**人类大脑**的设备（参见 <https://www.livescience.com/29365-human-brain.html>）。该公司希望这款设备能支持未来的AI研究。
- en: The machine, dubbed "Hala Point," can perform AI workloads 50 times faster and
    use 100 times less energy than conventional computing systems that use central
    processing units (CPUs) and graphics processing units (GPUs), Intel representatives
    said in a [statement](https://www.intel.com/content/www/us/en/newsroom/news/intel-builds-worlds-largest-neuromorphic-system.html#gs.84tuhb).
    These figures are based on findings uploaded March 18 to the preprint server [IEEE
    Explore,](https://ieeexplore.ieee.org/document/10448003) which have not been peer-reviewed.
  id: totrans-split-7
  prefs: []
  type: TYPE_NORMAL
  zh: 称为 "Hala Point" 的这台机器相比于使用中央处理器（CPU）和图形处理器（GPU）的传统计算系统，其执行AI工作负载的速度提高50倍，能源使用效率提高100倍（英特尔代表在
    <https://www.intel.com/content/www/us/en/newsroom/news/intel-builds-worlds-largest-neuromorphic-system.html#gs.84tuhb>
    的声明中提及）。这些数据显示，基于3月18日上传至预印论文服务器IEEE Explore（<https://ieeexplore.ieee.org/document/10448003>）上还未经过同行评审研究得出的数据。
- en: Hala Point will initially be deployed at Sandia National Laboratories in New
    Mexico, where scientists will use it to tackle problems in device physics, computing
    architecture and computer science.
  id: totrans-split-8
  prefs: []
  type: TYPE_NORMAL
  zh: Hala Point 初始部署于美国新墨西哥州的桑迪亚国家实验室，科学家在那里使用它来解决设备物理学、计算架构和计算机科学领域的问题。
- en: '**Related:** [**China develops new light-based chiplet that could power artificial
    general intelligence — where AI is smarter than humans**](https://www.livescience.com/technology/electronics/china-develops-light-based-chiplet-power-agi-artificial-general-intelligence)'
  id: totrans-split-9
  prefs: []
  type: TYPE_NORMAL
  zh: '**相关链接**：[中国开发新光基芯片组件，有可能推动人工智能——这里的人工智能比人类更聪明](https://www.livescience.com/technology/electronics/china-develops-light-based-chiplet-power-agi-artificial-general-intelligence)'
- en: Powered by 1,152 of Intel's new Loihi 2 processors — a neuromorphic research
    chip — this large-scale system comprises 1.15 billion artificial neurons and 128
    billion artificial synapses distributed over 140,544 processing cores.
  id: totrans-split-10
  prefs: []
  type: TYPE_NORMAL
  zh: 这款大型系统由 1,152 枚英特尔新的 Loihi 2 处理器（类脑研究芯片）提供动力，包含 115 亿个人工神经元和 1,280 亿个人工突触，分布在
    140,544 个处理核心上。
- en: It can make 20 quadrillion operations per second — or 20 petaops. Neuromorphic
    computers process data differently from supercomputers, so it's hard to compare
    them. But Trinity, the [38th most powerful](https://www.top500.org/lists/top500/list/2023/11/)
    supercomputer in the world boasts approximately 20 petaFLOPS of power — where
    a FLOP is a floating-point operation per second. The [world's most powerful supercomputer](https://www.livescience.com/technology/computing/top-7-most-powerful-supercomputers-in-the-world-right-now)
    is Frontier, which boasts a performance of 1.2 exaFLOPS, or 1,194 petaFLOPS.
  id: totrans-split-11
  prefs: []
  type: TYPE_NORMAL
  zh: 这台机器每秒能够执行 20 万亿次操作，或者说 20 petaops。类脑电脑以不同于超级计算机的方式处理数据，因此很难将它们相互对比。不过，全球第38强大的超级计算机“Trinity”拥有大约20
    petaFLOPS的计算能力（每秒浮点运算量）。全球最强大的超级计算机“前线（Frontier）”则拥有1.2 exaFLOPS的性能，相当于1,194 petaFLOPS。
- en: How neuromorphic computing works
  id: totrans-split-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何理解类脑计算的工作原理
- en: Neuromorphic computing differs from conventional computing because of its architecture,
    [Prasanna Date](https://www.researchgate.net/profile/Prasanna-Date), a computer
    scientist with the Oak Ridge National Laboratory (ORNL), wrote on [ResearchGate](https://www.researchgate.net/figure/Comparison-of-the-von-Neumann-architecture-with-the-neuromorphic-architecture-These_fig1_358255092).
    These types of computers use neural networks to build the machine.
  id: totrans-split-13
  prefs: []
  type: TYPE_NORMAL
  zh: 神经形态计算与传统计算不同，因为其体系结构由[Prasanna Date](https://www.researchgate.net/profile/Prasanna-Date)，美国奥克里奇国家实验室（ORNL）的计算机科学家，在[ResearchGate](https://www.researchgate.net/figure/Comparison-of-the-von-Neumann-architecture-with-the-neuromorphic-architecture-These_fig1_358255092)上写道。这些类型的计算机使用神经网络构建机器。
- en: Get the world’s most fascinating discoveries delivered straight to your inbox.
  id: totrans-split-14
  prefs: []
  type: TYPE_NORMAL
  zh: 获取世界上最引人入胜的发现，直接送到您的收件箱。
- en: In classical computing, binary bits of 1s and 0s flow into hardware like the
    CPU, GPU or memory before processing calculations in sequence and spitting out
    a binary output.
  id: totrans-split-15
  prefs: []
  type: TYPE_NORMAL
  zh: 在经典计算中，二进制的1和0位流入硬件，如CPU、GPU或内存，在处理计算前按顺序进行处理，并输出二进制输出。
- en: '(Image credit: Walden Kirsch/Intel Corporation)'
  id: totrans-split-16
  prefs: []
  type: TYPE_NORMAL
  zh: （图片来源：瓦尔登·基尔什/英特尔公司）
- en: In neuromorphic computing, however, a "spike input" — a set of [discrete electrical
    signals](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC9313413/#:~:text=In%20SNNs%2C%20such%20as%20in,and%20work%20in%20continuous%20time.)
    — is fed into the spiking neural networks (SNNs), represented by the processors.
    Where software-based neural networks are a collection of machine learning algorithms
    arranged to mimic the human brain, SNNs are a physical embodiment of how that
    information is transmitted. It allows for parallel processing and spike outputs
    are measured following calculations.
  id: totrans-split-17
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，在神经形态计算中，“尖峰输入”——一组[离散电信号](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC9313413/#:~:text=In%20SNNs%2C%20such%20as%20in,and%20work%20in%20continuous%20time.)——被输入到尖峰神经网络（SNNs）中，这些网络由处理器表示。在软件基础的神经网络中，是一组排列成仿真人脑的机器学习算法，而SNNs是这些信息传递方式的物理体现。它允许并行处理，并根据计算后的尖峰输出进行测量。
- en: Like the brain, Hala Point and the Loihi 2 processors use these SNNs, where
    different nodes are connected and information is processed at different layers,
    similar to neurons in the brain. The chips also integrate memory and computing
    power in one place. In conventional computers, processing power and memory are
    separated; this creates a bottleneck as data must physically travel between these
    components. Both of these enable parallel processing and reduce power consumption.
  id: totrans-split-18
  prefs: []
  type: TYPE_NORMAL
  zh: 类似于大脑，Hala Point和Loihi 2处理器使用这些SNNs，其中不同的节点相连，信息在不同层次进行处理，类似于大脑中的神经元。芯片还将内存和计算能力集成在一个地方。在传统计算机中，处理能力和内存是分离的；这会导致数据必须在这些组件之间物理传输，从而形成瓶颈。这两者都实现了并行处理并减少了功耗。
- en: Why neuromorphic computing could be an AI game-changer
  id: totrans-split-19
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 为什么神经形态计算可能是人工智能的游戏改变者
- en: Early results also show that Hala Point achieved a high energy efficiency reading
    for AI workloads of 15 trillion operations per watt (TOPS/W). Most conventional
    neural processing units (NPUs) and other AI systems achieve well under [10 TOPS/W](https://basicmi.github.io/AI-Chip/).
  id: totrans-split-20
  prefs: []
  type: TYPE_NORMAL
  zh: 早期结果还显示，Hala Point在AI工作负载的能效读数达到了每瓦15万亿次操作（TOPS/W）。大多数传统的神经处理单元（NPU）和其他AI系统的能效远低于[10
    TOPS/W](https://basicmi.github.io/AI-Chip/)。
- en: Neuromorphic computing is still a developing field, with few other machines
    like Hala Point in deployment, if any. Researchers with the International Centre
    for Neuromorphic Systems (ICNS) at Western Sydney University in Australia, however,
    [announced plans to deploy a similar machine](https://www.westernsydney.edu.au/newscentre/news_centre/more_news_stories/world_first_supercomputer_capable_of_brain-scale_simulation_being_built_at_western_sydney_university)
    in December 2023.
  id: totrans-split-21
  prefs: []
  type: TYPE_NORMAL
  zh: 神经形态计算仍然是一个发展中的领域，如果有的话，像Hala Point这样的机器很少部署。然而，澳大利亚西悉尼大学国际神经形态系统中心（ICNS）的研究人员[宣布计划在2023年12月部署类似的机器](https://www.westernsydney.edu.au/newscentre/news_centre/more_news_stories/world_first_supercomputer_capable_of_brain-scale_simulation_being_built_at_western_sydney_university)。
- en: Their computer, called "DeepSouth," emulates large networks of spiking neurons
    at 228 trillion synaptic operations per second, the ICNS researchers said in the
    statement, which they said was equivalent to the rate of operations of the human
    brain.
  id: totrans-split-22
  prefs: []
  type: TYPE_NORMAL
  zh: 他们的计算机“DeepSouth”以每秒228万亿次突触操作模拟大规模的尖峰神经网络，ICNS研究人员在声明中说道，这相当于人类大脑操作的速率。
- en: Hala Point meanwhile is a "starting point," a research prototype that will eventually
    feed into future systems that could be deployed commercially, according to Intel
    representatives.
  id: totrans-split-23
  prefs: []
  type: TYPE_NORMAL
  zh: 据英特尔代表称，Hala Point与此同时是一个“起点”，一个研究原型，最终将被纳入未来可以商业化部署的系统。
- en: These future neuromorphic computers might even lead to large language models
    (LLMs) like ChatGPT learning continuously from new data, which would reduce the
    massive training burden inherent in current AI deployments.
  id: totrans-split-24
  prefs: []
  type: TYPE_NORMAL
  zh: 这些未来的神经形态计算机甚至可能导致大型语言模型（LLMs），例如ChatGPT，持续从新数据中学习，这将减少当前人工智能部署中固有的巨大训练负担。
