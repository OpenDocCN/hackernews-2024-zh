- en: <!--yml
  id: totrans-split-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-split-1
  prefs: []
  type: TYPE_NORMAL
  zh: 'category: 未分类'
- en: 'date: 2024-05-27 12:54:12'
  id: totrans-split-2
  prefs: []
  type: TYPE_NORMAL
  zh: 'date: 2024-05-27 12:54:12'
- en: -->
  id: totrans-split-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: 'Yi-34B, Llama 2, and common practices in LLM training: a fact check of the
    New York Times | EleutherAI Blog'
  id: totrans-split-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Yi-34B、Llama 2以及LLM训练中的常见实践：对《纽约时报》的事实检验 | EleutherAI博客
- en: 来源：[https://blog.eleuther.ai/nyt-yi-34b-response/](https://blog.eleuther.ai/nyt-yi-34b-response/)
  id: totrans-split-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://blog.eleuther.ai/nyt-yi-34b-response/](https://blog.eleuther.ai/nyt-yi-34b-response/)
- en: 'On February 21 2024, the New York Times published [“China’s Rush to Dominate
    A.I. Comes With a Twist: It Depends on U.S. Technology.”](https://www.nytimes.com/2024/02/21/technology/china-united-states-artificial-intelligence.html)
    The authors claim that Yi-34B, a recent large language model by the Chinese startup
    01.AI, is fundamentally indebted to Meta’s Llama 2:'
  id: totrans-split-6
  prefs: []
  type: TYPE_NORMAL
  zh: 在2024年2月21日，《纽约时报》发表了[“中国加快主导人工智能的步伐：伴随着一个扭曲：它依赖于美国技术。”](https://www.nytimes.com/2024/02/21/technology/china-united-states-artificial-intelligence.html)
    作者声称，中国初创公司01.AI的最新大型语言模型Yi-34B在本质上依赖于Meta的Llama 2：
- en: 'There was just one twist: Some of the technology in 01.AI’s system [came](https://archive.is/o/krRqo/https://huggingface.co/01-ai/Yi-34B/discussions/11)
    from Llama. Mr. Lee’s start-up then built on Meta’s technology, training its system
    with new data to make it more powerful.'
  id: totrans-split-7
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 有一个扭曲之处：01.AI系统中的一些技术来自Llama。李先生的创业公司随后利用Meta的技术，用新数据训练其系统，使其更加强大。
- en: This assessment is based on a misreading of the [cited Hugging Face issue](https://huggingface.co/01-ai/Yi-34B/discussions/11).
    While we make no claims about the overall state of US-China AI competition, we
    want to explain why what 01.AI did is unremarkable and well within the bounds
    of common machine learning practices.
  id: totrans-split-8
  prefs: []
  type: TYPE_NORMAL
  zh: 这一评估基于对[cited Hugging Face issue](https://huggingface.co/01-ai/Yi-34B/discussions/11)的误读。尽管我们对中美人工智能竞争的整体状态没有任何主张，但我们想解释为什么01.AI所做的事情并不特别，并且完全在常见的机器学习实践范围内。
- en: In short, all modern large language models (LLMs) are made from the same algorithmic
    building blocks. The architectural differences between Llama 2 and the original
    2017 Transformer were not invented by Meta, and are all public owing to open access
    publishing being the norm in computer science. So, even though Yi-34B adopts Llama
    2's architecture, Meta's model did not give 01.AI access to any previously inaccessible
    innovation.
  id: totrans-split-9
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，所有现代大型语言模型（LLMs）都是由相同的算法构建模块制成的。Llama 2与原始的2017年Transformer之间的架构差异并非由Meta发明，并且所有这些都是公开的，因为计算机科学中的开放获取出版是常态。因此，尽管Yi-34B采用了Llama
    2的架构，Meta的模型并没有使01.AI获得任何先前无法访问的创新。
- en: In November 2023, a Hugging Face user asked for two components of Yi-34B to
    be renamed so that the model would be compatible with third-party codebases developed
    for Llama 2 by default. Training data is the main reason LLMs differ from one
    another, and in that regard Meta disclosed no useful details; 01.AI developed
    and described their own English-Chinese dataset. The similarities between Yi-34B
    and Llama 2 do not support the argument that Chinese AI firms simply rely on American
    open models, because all LLMs have extremely similar architecture. Moreover, we
    note that most of the architectural innovations in Llama 2 came from implementing
    algorithms that were already known and described in research literature.
  id: totrans-split-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在2023年11月，一位Hugging Face用户要求将Yi-34B的两个组件重命名，以使该模型默认兼容Llama 2的第三方代码库。训练数据是LLM之间差异的主要原因，而在这方面，Meta没有披露任何有用的细节；01.AI开发并描述了他们自己的英中数据集。Yi-34B与Llama
    2之间的相似性并不支持中国人工智能公司仅仅依赖于美国开放模型的论点，因为所有LLM都具有极其相似的架构。此外，我们注意到Llama 2中的大部分架构创新来自于实现已知并在研究文献中描述的算法。
- en: An overview of the Hugging Face discussion for non-coders[#](#an-overview-of-the-hugging-face-discussion-for-non-coders)
  id: totrans-split-11
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '[Hugging Face讨论的非编程人员概述](#an-overview-of-the-hugging-face-discussion-for-non-coders)'
- en: Like in other pieces of software, language model developers assign names to
    software components in order to reference them later. For example, the first layer
    of a neural network is often named `layer_one`. Importantly, the actual name has
    no semantic content. It's just a symbol to refer to a particular piece of the
    software. However, these names are very important for interoperability between
    developers. If two people train neural networks but one calls the first layer
    `layer_one` and the other calls the first layer `layer_1`, then it can be very
    inconvenient for a third-party piece of code to interact with both of them, because
    the third-party code needs to be able to refer to the components *by name*.
  id: totrans-split-12
  prefs: []
  type: TYPE_NORMAL
  zh: 就像其他软件中一样，语言模型开发者为软件组件指定名称，以便以后引用。例如，神经网络的第一层通常被命名为 `layer_one`。重要的是，实际名称没有语义内容，它只是一个符号，用于指代软件的特定部分。然而，这些名称对于开发者之间的互操作性非常重要。如果两个人训练神经网络，但一个称第一层为
    `layer_one`，另一个称其为 `layer_1`，那么第三方代码要同时与它们交互将非常不方便，因为第三方代码需要能够按名称引用组件。
- en: When 01.AI uploaded Yi-34B to the Hugging Face Hub, they used a different naming
    convention than Meta. This meant that third party code designed to interact with
    Llama 2 didn't work with 01.AI’s model. This can be fixed by each third party
    developer tweaking their code, but across all third party developers this would
    cumulatively result in a lot of work. The purpose of the HF issue was to request
    01.AI rename the components to avoid this overhead.
  id: totrans-split-13
  prefs: []
  type: TYPE_NORMAL
  zh: 当 01.AI 将 Yi-34B 上传到 Hugging Face Hub 时，他们使用了与 Meta 不同的命名约定。这意味着旨在与 Llama 2
    交互的第三方代码无法与 01.AI 的模型配合使用。每个第三方开发者可以通过调整其代码来解决此问题，但在所有第三方开发者中，这将累计产生大量工作量。HF 提出问题的目的是要求
    01.AI 重命名这些组件，以避免这种额外工作量。
- en: These kinds of compatibility issues are common in open source releases, and
    are not evidence of nefarious intent nor of 01.AI relying on Llama to train their
    model. EleutherAI has experienced it with many of our early model releases, and
    now we do compatibility checks as part of our standard pre-release review.
  id: totrans-split-14
  prefs: []
  type: TYPE_NORMAL
  zh: 在开源发布中，这类兼容性问题很常见，但并不表明 01.AI 有恶意意图或依赖 Llama 来训练他们的模型。EleutherAI 在我们的早期模型发布中也遇到过这种情况，现在我们已将兼容性检查作为标准的预发布审查的一部分。
- en: Nevertheless, a number of [media](https://technode.com/2023/11/15/kai-fu-lees-ai-large-language-model-yi-used-metas-llama-architecture-without-namechecking-its-source/)
    [outlets](https://www.scmp.com/tech/tech-trends/article/3241680/chinese-tech-unicorn-01ai-admits-oversight-changing-name-ai-model-built-meta-platforms-llama-system)
    blew the Hugging Face issue out of proportion, claiming in November that 01.AI
    had willfully concealed the connection between its leading model and Llama 2\.
    These hyperbolic claims received little attention in the machine learning community.
  id: totrans-split-15
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，一些[媒体](https://technode.com/2023/11/15/kai-fu-lees-ai-large-language-model-yi-used-metas-llama-architecture-without-namechecking-its-source/)在11月份夸大了
    Hugging Face 的问题，声称 01.AI 故意隐瞒了其主导模型与 Llama 2 的关联。这些夸张的说法在机器学习社区中并未引起多少关注。
- en: How all LLMs are similar[#](#how-all-llms-are-similar)
  id: totrans-split-16
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 所有 LLMs 的相似性[#](#how-all-llms-are-similar)
- en: At a high level, every existing large language model is currently trained using
    the same basic techniques and methodologies. The developer starts with a large
    dataset of text on the web, separated out into individual word-like components
    called "tokens." Then, a deep learning architecture (almost always a basic Transformer,
    as introduced by Attention Is All You Need [(Vaswani et al. 2017)](https://arxiv.org/abs/1706.03762)
    is trained to fit to the dataset by optimizing a loss function causing the neural
    network to predict the most likely next token that comes next in the dataset.
    This process is referred to as “pretraining.”
  id: totrans-split-17
  prefs: []
  type: TYPE_NORMAL
  zh: 在高层次上，目前每个现有的大型语言模型都是使用相同的基本技术和方法进行训练的。开发者从网络上的大量文本数据集开始，将其分成称为 "tokens" 的单个类似词的组件。然后，通过训练一个深度学习架构（几乎总是基于注意力机制的
    Transformer，如由 Vaswani 等人于 2017 年提出的）来适应数据集，通过优化损失函数来预测数据集中接下来最有可能出现的下一个 token。这个过程被称为
    "预训练"。
- en: This basic recipe, and the building blocks used in it, have not fundamentally
    changed since the Transformer was introduced by Google Brain in 2017, and slightly
    tweaked to today’s left-to-right language models by OpenAI in [GPT-1](https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf)
    and [GPT-2](https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf).
  id: totrans-split-18
  prefs: []
  type: TYPE_NORMAL
  zh: 自从Google Brain在2017年引入Transformer以来，这种基本的配方和其中使用的构建模块基本上没有根本性的变化，稍作调整后成为今天的由OpenAI在[GPT-1](https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf)和[GPT-2](https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf)中采用的从左到右语言模型。
- en: 'For example, the neural network architecture of Meta’s Llama 2 model series
    only adopts a few changes that differentiate it from the original Transformer
    in 2017, or the Transformer as used in GPT-2 or [GPT-3](https://arxiv.org/abs/2005.14165).
    These are the following:'
  id: totrans-split-19
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，Meta的Llama 2系列模型的神经网络架构只是在2017年原始Transformer或GPT-2或[GPT-3](https://arxiv.org/abs/2005.14165)中采用了几个区别的变化：
- en: '- [The SwiGLU Activation Function](https://arxiv.org/abs/2002.05202) is a minor
    change to the transformer architecture that was observed to provide a slight quality
    improvement. It was introduced by Noam Shazeer at Google Brain in 2020, and subsequently
    adopted by Google’s PaLM models. It is now **de facto** used by most new language
    models, although not universally.'
  id: totrans-split-20
  prefs: []
  type: TYPE_NORMAL
  zh: '[SwiGLU激活函数](https://arxiv.org/abs/2002.05202)是对变压器架构的轻微改动，据观察能提供轻微的质量改进。它是由Google
    Brain的Noam Shazeer于2020年引入的，并随后被Google的PaLM模型采用。现在**事实上**被大多数新语言模型使用，尽管不是普遍。'
- en: '- [Rotary Positional Embeddings (RoPE)](https://arxiv.org/abs/2104.09864) are
    a method introduced by a group of Chinese researchers from Zhuiyi Technology Co.,
    Ltd. in 2021, that allows language models to more cleanly keep track of the relative
    “positions” in a piece of text of two different tokens. The technique was subsequently
    popularized among the Western research community in part [by EleutherAI](%5Bhttps://blog.eleuther.ai/rotary-embeddings/%5D(https://blog.eleuther.ai/rotary-embeddings/))
    and, due to its simplicity and several useful quality-of-life enhancements it
    provides, has become the standard for the vast majority of current LLMs.'
  id: totrans-split-21
  prefs: []
  type: TYPE_NORMAL
  zh: '[旋转位置嵌入（RoPE）](https://arxiv.org/abs/2104.09864)是由2021年由珠宜科技公司的一群中国研究人员引入的方法，允许语言模型更清晰地跟踪文本中两个不同标记的相对“位置”。这一技术后来在西方研究社区中部分由[EleutherAI](https://blog.eleuther.ai/rotary-embeddings/)推广，并因其简单性和提供的几个实用的生活质量改进而成为当前绝大多数LLM的标准。'
- en: '- A [“Pre-norm” architecture](%5Bhttps://arxiv.org/abs/2002.04745%5D(https://arxiv.org/abs/2002.04745))
    is a re-ordering of the transformer model’s operations which makes it mathematically
    cleaner to reliably train. This correction was proposed by a number of groups,
    including a collaboration between Chinese academics and Microsoft Research Asia.
    As an interesting historical aside, this adjustment was discovered and used by
    Vaswani et al. (2017) in the original transformer paper, but was reported inconsistently.'
  id: totrans-split-22
  prefs: []
  type: TYPE_NORMAL
  zh: '[“Pre-norm”架构](https://arxiv.org/abs/2002.04745)是变压器模型操作的重新排序，使得它在数学上更加清洁可靠地训练。这一修正由包括中国学术界和微软亚洲研究院的合作提出。作为一个有趣的历史插曲，这一调整最初是由Vaswani等人（2017年）在原始变压器论文中发现并使用的，但报告不一致。'
- en: '- [Multi-query Attention](https://arxiv.org/abs/1911.02150) and [Grouped-Query
    Attention](https://arxiv.org/abs/2305.13245) were both introduced by Google. Multi-Query
    Attention (MQA) is an improvement to the efficiency of running many inputs through
    a language model simultaneously for downstream use. It was first presented by
    Noam Shazeer of Google Brain in 2019, and was adopted by [Google’s PaLM model](https://arxiv.org/abs/2204.02311)
    at scale. Grouped-Query Attention (GQA) was proposed relatively recently, in 2023,
    by Google Research, as an extension of Multi-Query Attention which retained its
    benefits. Grouped-Query Attention’s adoption in Llama 2 has helped it become a
    new baseline for many recent language model architectures.'
  id: totrans-split-23
  prefs: []
  type: TYPE_NORMAL
  zh: '- [多查询注意力](https://arxiv.org/abs/1911.02150)和[分组查询注意力](https://arxiv.org/abs/2305.13245)都是由Google提出的。多查询注意力（MQA）是一种提高同时运行多个输入到语言模型中以供下游使用效率的改进技术。它最早由Google
    Brain的Noam Shazeer在2019年提出，并被[Google的PaLM模型](https://arxiv.org/abs/2204.02311)大规模采用。分组查询注意力（GQA）是相对较新的提议，于2023年由Google
    Research提出，作为多查询注意力的扩展，并保留了其优势。分组查询注意力在Llama 2中的应用已帮助其成为许多最近语言模型架构的新基准。'
- en: While it does not reduce Meta’s achievements in training Llama 2 at all, it
    is important to note that none of these advances were invented by Meta, and many
    have existed for several years. These techniques had become accepted as de facto
    best practices at the time Llama 2 was created and released.
  id: totrans-split-24
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管这并不完全削弱Meta在训练Llama 2方面的成就，但重要的是要注意，这些进展都不是由Meta发明的，并且其中许多技术已经存在了几年。这些技术在Llama
    2创建和发布时已被视为事实上的最佳实践。
- en: We hope that this illustrates the commonalities between LLMs today, many tracing
    back several years to the original Transformer paper and first transformer-based
    models.
  id: totrans-split-25
  prefs: []
  type: TYPE_NORMAL
  zh: 我们希望这说明了今天的LLM之间的共同点，其中许多追溯到几年前的原始Transformer论文和第一个基于Transformer的模型。
- en: So what makes Yi-34B different from Llama 2?[#](#so-what-makes-yi-34b-different-from-llama-2)
  id: totrans-split-26
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 那么Yi-34B与Llama 2有何不同？[#](#so-what-makes-yi-34b-different-from-llama-2)
- en: Although the Llama 2 models’ weights, and therefore their architectural details,
    are available (this is necessary for the models to be run by other developers
    or users in their code), it is useful to look to the components of Llama 2’s creation
    that were **not** disclosed as an indication of the true differentiators.
  id: totrans-split-27
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然Llama 2模型的权重以及它们的架构细节是可用的（这对其他开发人员或用户在其代码中运行模型是必要的），但有必要看看Llama 2创建过程中未披露的组成部分，这表明了真正的差异化因素。
- en: 'Llama 2’s [technical report](https://arxiv.org/abs/2307.09288) says only the
    following about the exact contents of their dataset:'
  id: totrans-split-28
  prefs: []
  type: TYPE_NORMAL
  zh: Llama 2的[技术报告](https://arxiv.org/abs/2307.09288)仅对其数据集的确切内容做了以下说明：
- en: Our training corpus includes a new mix of data from publicly available sources,
    which does not include data from Meta’s products or services. We made an effort
    to remove data from certain sites known to contain a high volume of personal information
    about private individuals.
  id: totrans-split-29
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 我们的训练语料库包括来自公开来源的新数据混合，不包括 Meta 的产品或服务数据。我们努力删除了来自某些已知包含大量私人信息的网站的数据。
- en: No further details about data sources are given. This is because, since all
    current language models mostly use similar architecture, Meta’s key competitive
    advantage comes from their training dataset.
  id: totrans-split-30
  prefs: []
  type: TYPE_NORMAL
  zh: 关于数据来源的进一步细节未提供。这是因为，由于当前的语言模型大多使用类似的架构，Meta 的关键竞争优势来自他们的训练数据集。
- en: 'Additionally, while Meta provides a [file](https://github.com/meta-llama/llama/blob/main/llama/model.py)
    defining the Llama architecture that can be used to load their models, they also
    do **not** release their infrastructure or codebase which was used to train the
    Llama models from scratch. To actually train one’s own language model requires
    assembling one’s own dataset and building this training infrastructure on one’s
    own computing clusters, which further requires an engineering team with expertise
    in high performance computing (HPC). Releasing the Llama 2 model weights does
    not enable other teams to develop a competitor for free any more than they already
    could, especially considering that advances like SwiGLU or GQA were already publicly
    known through open access academic publishing. In addition, Llama 2 also gave
    Meta concrete benefits: the Hugging Face discussion positions Llama as the current
    standard for open-weight LLM software, such that not using its architecture and
    naming convention is a disadvantage.'
  id: totrans-split-31
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，尽管Meta提供了一个[文件](https://github.com/meta-llama/llama/blob/main/llama/model.py)来定义Llama架构，可以用来加载他们的模型，但他们也**没有**公开用于从头开始训练Llama模型的基础设施或代码库。要实际训练自己的语言模型，需要组建自己的数据集并在自己的计算集群上构建这种训练基础设施，这进一步需要具备高性能计算（HPC）专业知识的工程团队。释放Llama
    2模型权重并不会使其他团队能够免费开发竞争对手，特别是考虑到像SwiGLU或GQA这样的进展已经通过开放获取的学术出版物公开知晓。此外，Llama 2还为Meta带来了明显的好处：Hugging
    Face的讨论将Llama定位为开放权重LLM软件的当前标准，因此不使用其架构和命名约定将是一个不利因素。
- en: Two weeks after the New York Times article was published, 01.AI released the
    [Yi technical report](https://arxiv.org/abs/2403.04652), which mentions these
    factors. Perhaps in response to negative media coverage, they explicitly state
    that a “standard” architecture is sufficient, and that far more effort must be
    spent on curating and processing a high-quality dataset. For Yi-34B, this included
    work to ensure performance in Chinese, and they had to build out this data processing
    pipeline as well as model pretraining infrastructure from scratch. 01.AI thus
    had to solve the same problems as those faced by other language model training
    companies, and is not “built on” Llama 2 any more than it is built on other language
    models, including models that were never released, such as Google’s PaLM.
  id: totrans-split-32
  prefs: []
  type: TYPE_NORMAL
  zh: 在《纽约时报》文章发布两周后，01.AI发布了[Yi技术报告](https://arxiv.org/abs/2403.04652)，提到了这些因素。或许是作为对负面媒体报道的回应，他们明确表示“标准”架构是足够的，而要花费更多精力来筛选和处理高质量的数据集。对于Yi-34B来说，这包括确保在中文表现良好，并且他们不得不从头开始构建这个数据处理流水线以及模型预训练基础设施。因此，01.AI必须解决与其他语言模型训练公司面临的相同问题，并不像“建立在”Llama
    2之上，而是建立在其他从未发布的语言模型（例如谷歌的PaLM）之上。
- en: Conclusion[#](#conclusion)
  id: totrans-split-33
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 结论[#](#conclusion)
- en: We hope this post illustrates how Yi-34B fits into larger trends in language
    model training. We want to emphasize that the NYT writers were not negligent and
    that the error is completely understandable, as interpreting this [Hugging Face
    issue](https://huggingface.co/01-ai/Yi-34B/discussions/11) clearly requires a
    substantial amount of context about model development and the current open AI
    ecosystem. Journalists cannot be expected to have this level of machine learning
    know-how. Our goal here is to educate, because we believe public discussion of
    AI needs to be firmly grounded in facts. In this case, 01.AI’s founder was absolutely
    correct when he stated that the company followed standard industry practices.
  id: totrans-split-34
  prefs: []
  type: TYPE_NORMAL
  zh: 我们希望这篇文章能说明**Yi-34B**如何融入语言模型训练的更大趋势。我们想强调《纽约时报》的作者们并不是疏忽大意，错误完全可以理解，因为要解释这个[Hugging
    Face问题](https://huggingface.co/01-ai/Yi-34B/discussions/11)，显然需要大量关于模型开发和当前开放AI生态系统的背景知识。我们不能指望记者们具备这种机器学习水平。我们在这里的目标是教育，因为我们认为公众对AI的讨论必须以事实为基础。在这种情况下，01.AI公司的创始人在指出公司遵循了标准行业实践时是完全正确的。
