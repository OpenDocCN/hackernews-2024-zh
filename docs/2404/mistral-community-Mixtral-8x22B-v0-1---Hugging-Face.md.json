["```\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\nmodel_id = \"mistral-community/Mixtral-8x22B-v0.1\"\ntokenizer = AutoTokenizer.from_pretrained(model_id)\n\nmodel = AutoModelForCausalLM.from_pretrained(model_id)\n\ntext = \"Hello my name is\"\ninputs = tokenizer(text, return_tensors=\"pt\")\n\noutputs = model.generate(**inputs, max_new_tokens=20)\nprint(tokenizer.decode(outputs[0], skip_special_tokens=True)) \n```", "```\n+ import torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\nmodel_id = \"mistral-community/Mixtral-8x22B-v0.1\"\ntokenizer = AutoTokenizer.from_pretrained(model_id)\n\n+ model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.float16).to(0)\n\ntext = \"Hello my name is\"\n+ inputs = tokenizer(text, return_tensors=\"pt\").to(0)\n\noutputs = model.generate(**inputs, max_new_tokens=20)\nprint(tokenizer.decode(outputs[0], skip_special_tokens=True)) \n```", "```\n+ import torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\nmodel_id = \"mistral-community/Mixtral-8x22B-v0.1\"\ntokenizer = AutoTokenizer.from_pretrained(model_id)\n\n+ model = AutoModelForCausalLM.from_pretrained(model_id, load_in_4bit=True)\n\ntext = \"Hello my name is\"\n+ inputs = tokenizer(text, return_tensors=\"pt\").to(0)\n\noutputs = model.generate(**inputs, max_new_tokens=20)\nprint(tokenizer.decode(outputs[0], skip_special_tokens=True)) \n```", "```\n+ import torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\nmodel_id = \"mistral-community/Mixtral-8x22B-v0.1\"\ntokenizer = AutoTokenizer.from_pretrained(model_id)\n\n+ model = AutoModelForCausalLM.from_pretrained(model_id, use_flash_attention_2=True)\n\ntext = \"Hello my name is\"\n+ inputs = tokenizer(text, return_tensors=\"pt\").to(0)\n\noutputs = model.generate(**inputs, max_new_tokens=20)\nprint(tokenizer.decode(outputs[0], skip_special_tokens=True)) \n```"]