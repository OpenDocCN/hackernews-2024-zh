- en: <!--yml
  id: totrans-split-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-split-1
  prefs: []
  type: TYPE_NORMAL
  zh: 'category: 未分类'
- en: 'date: 2024-05-27 13:30:00'
  id: totrans-split-2
  prefs: []
  type: TYPE_NORMAL
  zh: 'date: 2024-05-27 13:30:00'
- en: -->
  id: totrans-split-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: NVIDIA to Acquire GPU Orchestration Software Provider Run:ai | NVIDIA Blog
  id: totrans-split-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: NVIDIA将收购GPU编排软件提供商Run:ai | NVIDIA Blog
- en: 来源：[https://blogs.nvidia.com/blog/runai/](https://blogs.nvidia.com/blog/runai/)
  id: totrans-split-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://blogs.nvidia.com/blog/runai/](https://blogs.nvidia.com/blog/runai/)
- en: To help customers make more efficient use of their AI computing resources, NVIDIA
    today announced it has entered into a definitive agreement to acquire Run:ai,
    a Kubernetes-based workload management and orchestration software provider.
  id: totrans-split-6
  prefs: []
  type: TYPE_NORMAL
  zh: 为了帮助客户更有效地利用他们的AI计算资源，NVIDIA今天宣布已达成协议收购Run:ai，这是一个基于Kubernetes的工作负载管理和编排软件提供商。
- en: Customer AI deployments are becoming increasingly complex, with workloads distributed
    across cloud, edge and on-premises data center infrastructure.
  id: totrans-split-7
  prefs: []
  type: TYPE_NORMAL
  zh: 客户AI部署越来越复杂，工作负载分布在云端、边缘和本地数据中心基础设施之间。
- en: Managing and orchestrating generative AI, recommender systems, search engines
    and other workloads requires sophisticated scheduling to optimize performance
    at the system level and on the underlying infrastructure.
  id: totrans-split-8
  prefs: []
  type: TYPE_NORMAL
  zh: 管理和编排生成式AI、推荐系统、搜索引擎和其他工作负载需要复杂的调度以优化系统级别和基础架构上的性能。
- en: Run:ai enables enterprise customers to manage and optimize their compute infrastructure,
    whether on premises, in the cloud or in hybrid environments.
  id: totrans-split-9
  prefs: []
  type: TYPE_NORMAL
  zh: Run:ai使企业客户能够管理和优化其计算基础设施，无论是在本地、云端还是混合环境中。
- en: The company has built an open platform on [Kubernetes](https://www.nvidia.com/en-us/glossary/kubernetes/),
    the orchestration layer for modern AI and cloud infrastructure. It supports all
    popular Kubernetes variants and integrates with third-party AI tools and frameworks.
  id: totrans-split-10
  prefs: []
  type: TYPE_NORMAL
  zh: 公司在[Kubernetes](https://www.nvidia.com/en-us/glossary/kubernetes/)上建立了一个开放平台，这是现代人工智能和云基础设施的编排层。它支持所有流行的Kubernetes变体，并与第三方AI工具和框架集成。
- en: Run:ai customers include some of the world’s largest enterprises across multiple
    industries, which use the Run:ai platform to manage data-center-scale GPU clusters.
  id: totrans-split-11
  prefs: []
  type: TYPE_NORMAL
  zh: Run:ai的客户包括全球多个行业中一些最大的企业，这些企业使用Run:ai平台管理数据中心规模的GPU集群。
- en: “Run:ai has been a close collaborator with NVIDIA since 2020 and we share a
    passion for helping our customers make the most of their infrastructure,” said
    Omri Geller, Run:ai cofounder and CEO. “We’re thrilled to join NVIDIA and look
    forward to continuing our journey together.”
  id: totrans-split-12
  prefs: []
  type: TYPE_NORMAL
  zh: “自2020年以来，Run:ai与NVIDIA一直是密切合作伙伴，我们共同致力于帮助客户充分利用他们的基础设施，”Run:ai联合创始人兼CEO Omri
    Geller表示，“我们很高兴能够加入NVIDIA，并期待继续共同前行。”
- en: 'The Run:ai platform provides AI developers and their teams:'
  id: totrans-split-13
  prefs: []
  type: TYPE_NORMAL
  zh: Run:ai平台为AI开发者及其团队提供：
- en: A centralized interface to manage shared compute infrastructure, enabling easier
    and faster access for complex AI workloads.
  id: totrans-split-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个集中式界面，管理共享计算基础设施，为复杂的AI工作负载提供更便捷和更快速的访问。
- en: Functionality to add users, curate them under teams, provide access to cluster
    resources, control over quotas, priorities and pools, and monitor and report on
    resource use.
  id: totrans-split-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实现用户添加功能，将其归类到团队下，提供对集群资源的访问权限、配额、优先级和池的控制，并监控和报告资源使用情况。
- en: The ability to pool GPUs and share computing power — from [fractions of GPUs](https://www.nvidia.com/en-us/technologies/multi-instance-gpu/#:~:text=Multi%2DInstance%20GPU%20(MIG)%20expands%20the%20performance%20and%20value,%2C%20cache%2C%20and%20compute%20cores.)
    to multiple GPUs or multiple nodes of GPUs running on different clusters — for
    separate tasks.
  id: totrans-split-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可以汇集GPU并共享计算能力 —— 从[GPU的分数](https://www.nvidia.com/en-us/technologies/multi-instance-gpu/#:~:text=Multi%2DInstance%20GPU%20(MIG)%20expands%20the%20performance%20and%20value,%2C%20cache%2C%20and%20compute%20cores.)到多个GPU或在不同集群上运行的多个节点的GPU，用于不同的任务。
- en: Efficient GPU cluster resource utilization, enabling customers to gain more
    from their compute investments.
  id: totrans-split-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 高效的GPU集群资源利用，帮助客户更好地利用其计算投资。
- en: NVIDIA will continue to offer Run:ai’s products under the same business model
    for the immediate future. And NVIDIA will continue to invest in the Run:ai product
    roadmap, including enabling on [NVIDIA DGX Cloud](http://www.nvidia.com/dgx-cloud),
    an AI platform co-engineered with leading clouds for enterprise developers, offering
    an integrated, full-stack service optimized for generative AI.
  id: totrans-split-18
  prefs: []
  type: TYPE_NORMAL
  zh: NVIDIA将继续在未来维持Run:ai的产品在同一商业模式下提供。NVIDIA将继续投资于Run:ai的产品路线图，包括在[NVIDIA DGX Cloud](http://www.nvidia.com/dgx-cloud)上实现，这是与领先云平台共同工程的AI平台，提供为生成式AI优化的集成全栈服务。
- en: NVIDIA HGX, DGX and DGX Cloud customers will gain access to Run:ai’s capabilities
    for their AI workloads, particularly for large language model deployments. Run:ai’s
    solutions are already integrated with [NVIDIA DGX](https://www.nvidia.com/en-us/data-center/dgx-platform/),
    [NVIDIA DGX SuperPOD](https://www.nvidia.com/en-us/data-center/dgx-superpod/),
    [NVIDIA Base Command](https://www.nvidia.com/en-us/data-center/base-command/),
    [NGC](https://www.nvidia.com/en-us/gpu-cloud/) containers, and [NVIDIA AI Enterprise](https://www.nvidia.com/en-us/data-center/products/ai-enterprise/)
    software, among other products.
  id: totrans-split-19
  prefs: []
  type: TYPE_NORMAL
  zh: NVIDIA HGX、DGX 和 DGX Cloud 客户将获得对 Run:ai 的能力的访问，特别是针对大型语言模型部署的AI工作负载。 Run:ai
    的解决方案已经与 [NVIDIA DGX](https://www.nvidia.com/en-us/data-center/dgx-platform/)、[NVIDIA
    DGX SuperPOD](https://www.nvidia.com/en-us/data-center/dgx-superpod/)、[NVIDIA
    Base Command](https://www.nvidia.com/en-us/data-center/base-command/)、[NGC](https://www.nvidia.com/en-us/gpu-cloud/)
    容器和 [NVIDIA AI Enterprise](https://www.nvidia.com/en-us/data-center/products/ai-enterprise/)
    软件等产品集成。
- en: NVIDIA’s accelerated computing platform and Run:ai’s platform will continue
    to support a broad ecosystem of third-party solutions, giving customers choice
    and flexibility.
  id: totrans-split-20
  prefs: []
  type: TYPE_NORMAL
  zh: NVIDIA 的加速计算平台和 Run:ai 的平台将继续支持广泛的第三方解决方案生态系统，为客户提供选择和灵活性。
- en: Together with Run:ai, NVIDIA will enable customers to have a single fabric that
    accesses GPU solutions anywhere. Customers can expect to benefit from better GPU
    utilization, improved management of GPU infrastructure and greater flexibility
    from the open architecture.
  id: totrans-split-21
  prefs: []
  type: TYPE_NORMAL
  zh: 与 Run:ai 合作，NVIDIA 将使客户能够在任何地方访问 GPU 解决方案的单一平台。 客户可以期待从开放架构中获得更好的 GPU 利用率、改善的
    GPU 基础设施管理以及更大的灵活性。
