- en: <!--yml
  id: totrans-split-0
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  id: totrans-split-1
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-05-27 13:30:00'
  id: totrans-split-2
  prefs: []
  type: TYPE_NORMAL
- en: -->
  id: totrans-split-3
  prefs: []
  type: TYPE_NORMAL
- en: NVIDIA to Acquire GPU Orchestration Software Provider Run:ai | NVIDIA Blog
  id: totrans-split-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://blogs.nvidia.com/blog/runai/](https://blogs.nvidia.com/blog/runai/)
  id: totrans-split-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: To help customers make more efficient use of their AI computing resources, NVIDIA
    today announced it has entered into a definitive agreement to acquire Run:ai,
    a Kubernetes-based workload management and orchestration software provider.
  id: totrans-split-6
  prefs: []
  type: TYPE_NORMAL
- en: Customer AI deployments are becoming increasingly complex, with workloads distributed
    across cloud, edge and on-premises data center infrastructure.
  id: totrans-split-7
  prefs: []
  type: TYPE_NORMAL
- en: Managing and orchestrating generative AI, recommender systems, search engines
    and other workloads requires sophisticated scheduling to optimize performance
    at the system level and on the underlying infrastructure.
  id: totrans-split-8
  prefs: []
  type: TYPE_NORMAL
- en: Run:ai enables enterprise customers to manage and optimize their compute infrastructure,
    whether on premises, in the cloud or in hybrid environments.
  id: totrans-split-9
  prefs: []
  type: TYPE_NORMAL
- en: The company has built an open platform on [Kubernetes](https://www.nvidia.com/en-us/glossary/kubernetes/),
    the orchestration layer for modern AI and cloud infrastructure. It supports all
    popular Kubernetes variants and integrates with third-party AI tools and frameworks.
  id: totrans-split-10
  prefs: []
  type: TYPE_NORMAL
- en: Run:ai customers include some of the world’s largest enterprises across multiple
    industries, which use the Run:ai platform to manage data-center-scale GPU clusters.
  id: totrans-split-11
  prefs: []
  type: TYPE_NORMAL
- en: “Run:ai has been a close collaborator with NVIDIA since 2020 and we share a
    passion for helping our customers make the most of their infrastructure,” said
    Omri Geller, Run:ai cofounder and CEO. “We’re thrilled to join NVIDIA and look
    forward to continuing our journey together.”
  id: totrans-split-12
  prefs: []
  type: TYPE_NORMAL
- en: 'The Run:ai platform provides AI developers and their teams:'
  id: totrans-split-13
  prefs: []
  type: TYPE_NORMAL
- en: A centralized interface to manage shared compute infrastructure, enabling easier
    and faster access for complex AI workloads.
  id: totrans-split-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Functionality to add users, curate them under teams, provide access to cluster
    resources, control over quotas, priorities and pools, and monitor and report on
    resource use.
  id: totrans-split-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The ability to pool GPUs and share computing power — from [fractions of GPUs](https://www.nvidia.com/en-us/technologies/multi-instance-gpu/#:~:text=Multi%2DInstance%20GPU%20(MIG)%20expands%20the%20performance%20and%20value,%2C%20cache%2C%20and%20compute%20cores.)
    to multiple GPUs or multiple nodes of GPUs running on different clusters — for
    separate tasks.
  id: totrans-split-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Efficient GPU cluster resource utilization, enabling customers to gain more
    from their compute investments.
  id: totrans-split-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: NVIDIA will continue to offer Run:ai’s products under the same business model
    for the immediate future. And NVIDIA will continue to invest in the Run:ai product
    roadmap, including enabling on [NVIDIA DGX Cloud](http://www.nvidia.com/dgx-cloud),
    an AI platform co-engineered with leading clouds for enterprise developers, offering
    an integrated, full-stack service optimized for generative AI.
  id: totrans-split-18
  prefs: []
  type: TYPE_NORMAL
- en: NVIDIA HGX, DGX and DGX Cloud customers will gain access to Run:ai’s capabilities
    for their AI workloads, particularly for large language model deployments. Run:ai’s
    solutions are already integrated with [NVIDIA DGX](https://www.nvidia.com/en-us/data-center/dgx-platform/),
    [NVIDIA DGX SuperPOD](https://www.nvidia.com/en-us/data-center/dgx-superpod/),
    [NVIDIA Base Command](https://www.nvidia.com/en-us/data-center/base-command/),
    [NGC](https://www.nvidia.com/en-us/gpu-cloud/) containers, and [NVIDIA AI Enterprise](https://www.nvidia.com/en-us/data-center/products/ai-enterprise/)
    software, among other products.
  id: totrans-split-19
  prefs: []
  type: TYPE_NORMAL
  zh: NVIDIA HGX、DGX 和 DGX Cloud 客户将获得对 Run:ai 的能力的访问，特别是针对大型语言模型部署的AI工作负载。 Run:ai
    的解决方案已经与 [NVIDIA DGX](https://www.nvidia.com/en-us/data-center/dgx-platform/)、[NVIDIA
    DGX SuperPOD](https://www.nvidia.com/en-us/data-center/dgx-superpod/)、[NVIDIA
    Base Command](https://www.nvidia.com/en-us/data-center/base-command/)、[NGC](https://www.nvidia.com/en-us/gpu-cloud/)
    容器和 [NVIDIA AI Enterprise](https://www.nvidia.com/en-us/data-center/products/ai-enterprise/)
    软件等产品集成。
- en: NVIDIA’s accelerated computing platform and Run:ai’s platform will continue
    to support a broad ecosystem of third-party solutions, giving customers choice
    and flexibility.
  id: totrans-split-20
  prefs: []
  type: TYPE_NORMAL
  zh: NVIDIA 的加速计算平台和 Run:ai 的平台将继续支持广泛的第三方解决方案生态系统，为客户提供选择和灵活性。
- en: Together with Run:ai, NVIDIA will enable customers to have a single fabric that
    accesses GPU solutions anywhere. Customers can expect to benefit from better GPU
    utilization, improved management of GPU infrastructure and greater flexibility
    from the open architecture.
  id: totrans-split-21
  prefs: []
  type: TYPE_NORMAL
  zh: 与 Run:ai 合作，NVIDIA 将使客户能够在任何地方访问 GPU 解决方案的单一平台。 客户可以期待从开放架构中获得更好的 GPU 利用率、改善的
    GPU 基础设施管理以及更大的灵活性。
