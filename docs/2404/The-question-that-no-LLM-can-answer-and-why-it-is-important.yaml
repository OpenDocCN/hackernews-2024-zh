- en: <!--yml
  id: totrans-split-0
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  id: totrans-split-1
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-05-27 13:30:43'
  id: totrans-split-2
  prefs: []
  type: TYPE_NORMAL
- en: -->
  id: totrans-split-3
  prefs: []
  type: TYPE_NORMAL
- en: The question that no LLM can answer and why it is important
  id: totrans-split-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://www.mindprison.cc/p/the-question-that-no-llm-can-answer](https://www.mindprison.cc/p/the-question-that-no-llm-can-answer)
  id: totrans-split-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '***[Notes From the Desk](https://www.mindprison.cc/s/notes-from-the-desk)**
    are periodic posts that summarize recent topics of interest or other brief notable
    commentary that might otherwise be a tweet or note.*'
  id: totrans-split-6
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  id: totrans-split-7
  prefs: []
  type: TYPE_NORMAL
- en: “Which episode of Gilligan’s Island was about mind reading?”
  id: totrans-split-8
  prefs: []
  type: TYPE_NORMAL
- en: No LLM properly answers this question unless it uses a web search. But how can
    a LLM not know the answer if it was trained on essentially the entire internet
    of data and certainly most likely all the data in IMDB? The correct answer is
    “[Seer Gilligan](https://www.imdb.com/title/tt1230843/)”, one of the most popular
    episodes.
  id: totrans-split-9
  prefs: []
  type: TYPE_NORMAL
- en: 'The following are the responses from multiple top models:'
  id: totrans-split-10
  prefs: []
  type: TYPE_NORMAL
- en: '**Opus**'
  id: totrans-split-11
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: I apologize, but there is no episode of Gilligan's Island that specifically
    focuses on mind reading.
  id: totrans-split-12
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '* * *'
  id: totrans-split-13
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '**GPT**  **4**'
  id: totrans-split-14
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: The episode of "Gilligan's Island" that involves mind reading is titled "Ring
    Around Gilligan". It is the sixth episode of the third season.
  id: totrans-split-15
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '* * *'
  id: totrans-split-16
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '**Gemini 1.5**'
  id: totrans-split-17
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: The episode of Gilligan's Island that features mind reading is Season 2, Episode
    12 - "The Mind Reader".
  id: totrans-split-18
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '* * *'
  id: totrans-split-19
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '**Llama 3**'
  id: totrans-split-20
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: After conducting a thorough search and reviewing episode guides, I found that
    there is no episode of Gilligan's Island that specifically focuses on mind reading.
  id: totrans-split-21
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '* * *'
  id: totrans-split-22
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '**Llama 3 -** ***asked to list and name episodes***'
  id: totrans-split-23
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '"The Postman Cometh" (March 13, 1965)'
  id: totrans-split-24
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '"**Seer Gilligan**" (Wait, no! I apologize, but there is no episode titled
    "Seer Gilligan")'
  id: totrans-split-25
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '"Love Me, Love My Chicken" (March 20, 1965)'
  id: totrans-split-26
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Please note that there is no episode titled "**Seer Gilligan**". If you have
    any other questions or need help with a specific episode, feel free to ask!
  id: totrans-split-27
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: All four models fail to give the correct answer. Two of the models hallucinate
    an answer and the remaining two deny such an episode exists. Interestingly, probing
    Llama 3 a bit more reveals it is aware of the episode; however, it still denies
    its existence. It seems to know there are 98 episodes, but the majority of titles
    it lists are made up.
  id: totrans-split-28
  prefs: []
  type: TYPE_NORMAL
- en: In the case of Llama 3, we can probe some of the training dataset [using Infini-gram](https://huggingface.co/spaces/liujch1998/infini-gram)
    and verify that the episode does exist in the corpus along with text describing
    the episode.
  id: totrans-split-29
  prefs: []
  type: TYPE_NORMAL
- en: We also see another interesting data and training phenomenon revealed when LLMs
    are asked to provide a number between 1-100\. They all converge to 42!
  id: totrans-split-30
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  id: totrans-split-31
  prefs: []
  type: TYPE_NORMAL
- en: Pointed out by [Information is Beautiful](https://twitter.com/infobeautiful/status/1778059112250589561),
    a very interesting distribution forms when AI is asked to pick a number between
    1 and 100\. There is a heavy weighting toward picking the number ‘42’. Likely,
    this is the [Hitchhiker’s Guide to the Galaxy](https://en.wikipedia.org/wiki/Phrases_from_The_Hitchhiker%27s_Guide_to_the_Galaxy#The_Answer_to_the_Ultimate_Question_of_Life,_the_Universe,_and_Everything_is_42)
    effect. The number 42 is overrepresented or weighted in some way through training,
    resulting in a higher propensity for the LLM to choose 42.
  id: totrans-split-32
  prefs: []
  type: TYPE_NORMAL
- en: The implications are that LLMs do not perform reasoning over data in the way
    that most people conceive or desire.
  id: totrans-split-33
  prefs: []
  type: TYPE_NORMAL
- en: There is no self-reflection of its information; it does not know what it knows
    and what it does not. The line between hallucination and truth is simply a probability
    factored by the prevalence of training data and post-training processes like fine-tuning.
    Reliability will always be nothing more than a probability built on top of this
    architecture.
  id: totrans-split-34
  prefs: []
  type: TYPE_NORMAL
- en: As such, it becomes unsuitable as a machine to find rare hidden truths or valuable
    neglected information. It will always simply converge toward popular narrative
    or data. At best, it can provide new permutations of views of existing well-known
    concepts, but it can not invent new concepts or reveal concepts rarely spoken
    about.
  id: totrans-split-35
  prefs: []
  type: TYPE_NORMAL
- en: “You can't cache reality in some compressed lookup table. If a particular outcome
    was never in the training data, the model will perform a random guess which is
    quite limiting.”
  id: totrans-split-36
  prefs: []
  type: TYPE_NORMAL
- en: — [Chomba Bupe](https://twitter.com/ChombaBupe/status/1781831176367312981)
  id: totrans-split-37
  prefs: []
  type: TYPE_NORMAL
- en: Furthermore, it can never be a system for absolute dependability. Mission-critical
    systems that require deterministic, provably correct behavior are not something
    applicable to LLM automation or control. The problem is that LLMs are impressively
    convincing when they are wrong, which may lead to ill-advised adoption. What business
    wants to balance the books with a hallucinating calculator?
  id: totrans-split-38
  prefs: []
  type: TYPE_NORMAL
- en: '**Implications**:'
  id: totrans-split-39
  prefs: []
  type: TYPE_NORMAL
- en: Results are probabilities defined more by data prevalence than logic or reason.
  id: totrans-split-40
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: It is indiscernible to what degree a LLM is reliable on a given question.
  id: totrans-split-41
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Not useful to find undiscovered truths or neglected but brilliant ideas.
  id: totrans-split-42
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Inability to theorize new concepts or discoveries.
  id: totrans-split-43
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: It is substantially ironic that LLMs are failing at the primary use cases that
    are attracting [billions of investment](https://www.mindprison.cc/p/stargate-the-100-billion-hail-mary-agi),
    but are rather proficient at the use cases we do not desire, such as [destruction
    of privacy and liberty](https://www.mindprison.cc/p/ai-end-of-privacy-end-of-sanity),
    a [post-truth society](https://www.mindprison.cc/p/ai-accelerates-post-truth-civilization),
    [social manipulation](https://www.mindprison.cc/p/ai-instructed-brainwashing-effectively),
    the [severance of human connection](https://www.mindprison.cc/p/ai-is-transforming-us-into-npcs),
    [fountains of noise](https://www.mindprison.cc/p/ai-automation-to-bury-civilization),
    the [devaluation of meaning](https://www.mindprison.cc/p/ai-art-challenges-meaning-in-a-world),
    and a plethora of other [societal issues](https://www.mindprison.cc/p/ai-and-the-end-to-all-things).
  id: totrans-split-44
  prefs: []
  type: TYPE_NORMAL
  zh: 是有些讽刺的是，LLM（大语言模型）正失败于吸引[数十亿美元的投资](https://www.mindprison.cc/p/stargate-the-100-billion-hail-mary-agi)的主要用例，而相反，在我们不希望的用例上表现出色，比如[隐私和自由的破坏](https://www.mindprison.cc/p/ai-end-of-privacy-end-of-sanity)，[后真相社会](https://www.mindprison.cc/p/ai-accelerates-post-truth-civilization)，[社会操控](https://www.mindprison.cc/p/ai-instructed-brainwashing-effectively)，[人类联系的断裂](https://www.mindprison.cc/p/ai-is-transforming-us-into-npcs)，[噪音的喷泉](https://www.mindprison.cc/p/ai-automation-to-bury-civilization)，[意义贬值](https://www.mindprison.cc/p/ai-art-challenges-meaning-in-a-world)，以及众多[社会问题](https://www.mindprison.cc/p/ai-and-the-end-to-all-things)。
- en: '* * *'
  id: totrans-split-45
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Unlike much of the internet now, there is a human mind behind all the content
    created here at Mind Prison. I typically spend hours to days on articles including
    creating the illustrations for each. I hope if you find them valuable and you
    still appreciate the creations from the organic hardware within someone’s head
    that you will consider subscribing. Thank you!
  id: totrans-split-46
  prefs: []
  type: TYPE_NORMAL
  zh: 与现在的互联网不同，Mind Prison 的所有内容都是由人类心智创造的。我通常花数小时到数天撰写文章，并为每篇文章制作插图。如果您觉得这些内容有价值，并且仍然欣赏来自某人有机硬件内的创作，希望您考虑订阅。谢谢！
- en: '* * *'
  id: totrans-split-47
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: '*No compass through the dark exists without hope of reaching the other side
    and the belief that it matters …*'
  id: totrans-split-48
  prefs: []
  type: TYPE_NORMAL
  zh: '*没有指南针能在没有希望到达另一边，并且相信这很重要的黑暗中存在……*'
- en: Mind Prison is a reader-supported publication. You can also assist by sharing.
  id: totrans-split-49
  prefs: []
  type: TYPE_NORMAL
  zh: Mind Prison 是一本由读者支持的出版物。您也可以通过分享来提供帮助。
- en: '[Share](https://www.mindprison.cc/p/the-question-that-no-llm-can-answer?utm_source=substack&utm_medium=email&utm_content=share&action=share)'
  id: totrans-split-50
  prefs: []
  type: TYPE_NORMAL
  zh: '[分享](https://www.mindprison.cc/p/the-question-that-no-llm-can-answer?utm_source=substack&utm_medium=email&utm_content=share&action=share)'
