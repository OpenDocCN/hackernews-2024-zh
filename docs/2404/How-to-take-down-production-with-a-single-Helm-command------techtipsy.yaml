- en: <!--yml
  id: totrans-split-0
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  id: totrans-split-1
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-05-27 12:55:13'
  id: totrans-split-2
  prefs: []
  type: TYPE_NORMAL
- en: -->
  id: totrans-split-3
  prefs: []
  type: TYPE_NORMAL
- en: 'How to take down production with a single Helm command :: ./techtipsy'
  id: totrans-split-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://ounapuu.ee/posts/2024/04/04/helm-rollbljat/](https://ounapuu.ee/posts/2024/04/04/helm-rollbljat/)
  id: totrans-split-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'You’re Cletus Kubernetus: a software developer, and a proud Fedora Linux user.'
  id: totrans-split-6
  prefs: []
  type: TYPE_NORMAL
- en: You know Kubernetes, especially after the time you migrated some services to
    it.
  id: totrans-split-7
  prefs: []
  type: TYPE_NORMAL
- en: '[Everything is calm.](https://www.youtube.com/watch?v=ia8Q51ouA_s&pp=ygUGa3JhemFt)'
  id: totrans-split-8
  prefs: []
  type: TYPE_NORMAL
- en: Your pods are running. Your service is up. Business as usual.
  id: totrans-split-9
  prefs: []
  type: TYPE_NORMAL
- en: You release some minor changes to production. Everything is still working. Great!
  id: totrans-split-10
  prefs: []
  type: TYPE_NORMAL
- en: But then you receive a message from a colleague. Oh no, something has gone wrong
    with a particular piece of functionality!
  id: totrans-split-11
  prefs: []
  type: TYPE_NORMAL
- en: No worries. You’re using Helm. You can roll this change back safely. You ask
    your colleague. “Oh yeah, `helm rollback` should work.”
  id: totrans-split-12
  prefs: []
  type: TYPE_NORMAL
- en: '`helm rollback` it is.'
  id: totrans-split-13
  prefs: []
  type: TYPE_NORMAL
- en: Cool, cool, new pod is starting up. Seems like it is indeed working.
  id: totrans-split-14
  prefs: []
  type: TYPE_NORMAL
- en: '**Wait, where did all the pods go?**'
  id: totrans-split-15
  prefs: []
  type: TYPE_NORMAL
- en: After a hectic troubleshooting session with the team, you redeploy the service
    and start investigating. A colleague uses the staging environment to do a `helm
    rollback` and it works as expected, the previous version of the service is successfully
    deployed.
  id: totrans-split-16
  prefs: []
  type: TYPE_NORMAL
- en: You investigate logs. The `helm rollback` call worked as expected, and then
    it began deleting every entity related to the deployment. Pods, secrets, ingresses,
    *everything* related to the service was gone, and your name was present on each
    deletion.
  id: totrans-split-17
  prefs: []
  type: TYPE_NORMAL
- en: The troubleshooting was on standby for a few days since you had no further leads
    and had to get other work done. But you couldn’t really move on from this issue
    mentally, could you?
  id: totrans-split-18
  prefs: []
  type: TYPE_NORMAL
- en: One day you continue the investigation by opening the Helm GitHub repository,
    looking at the open issues and throwing in some keywords that might be relevant,
    such as “rollback”.
  id: totrans-split-19
  prefs: []
  type: TYPE_NORMAL
- en: '[What the fuck.](https://github.com/helm/helm/issues/12681)'
  id: totrans-split-20
  prefs: []
  type: TYPE_NORMAL
- en: It wasn’t an issue with Helm, or the way you ran it. Apparently the version
    of Helm packaged in Fedora Linux included a patch that introduced this issue.
    You then use the staging environment to reproduce the issue. Everything was gone,
    again, but this time in a safer environment.
  id: totrans-split-21
  prefs: []
  type: TYPE_NORMAL
- en: You promptly run `dnf remove -y helm`.
  id: totrans-split-22
  prefs: []
  type: TYPE_NORMAL
- en: After this and the [xz backdoor](https://openwall.com/lists/oss-security/2024/03/29/4),
    the idea of living in the countryside and learning beekeeping doesn’t sound *that*
    bad, does it?
  id: totrans-split-23
  prefs: []
  type: TYPE_NORMAL
