<!--yml

category: 未分类

date: 2024-05-27 13:35:32

-->

# LLMs永远做不到什么？ - 罗希特·克里希南

> 来源：[https://www.strangeloopcanon.com/p/what-can-llms-never-do](https://www.strangeloopcanon.com/p/what-can-llms-never-do)

在过去几年里，每次我们找出LLMs不能解决的问题时，它们都表现得非常出色。但即使它们表现得非常出色，它们仍然不能回答看似简单的问题，原因不明。

所以，在过去的几周里，我一直试图找出LLMs的失败模式。这始于我发现的一些探索。承认有些奇怪，但我觉得很有趣。AI的失败可以教会我们更多，远胜于成功。

起始点更大了，需要对LLM所最终将要做的许多工作进行任务评估 [的必要性](https://www.strangeloopcanon.com/p/evaluations-are-all-we-need)。但后来我开始问自己，如何找出它推理能力的极限，以便我们可以信任它的学习能力。

> 正如我多次所写，语言模型很难，它们的推理能力很难脱离其训练内容。因此，我希望找到一种方法来测试其逐步推理和回答问题的能力。
> 
> 我从我能想到的最简单版本开始，满足标准的版本：即它是否能够连续创建3x3、4x4和5x5大小的字谜。为什么这个？因为评估应该是a)易于创建，且b)易于评估，同时仍然很难做到！

结果表明，所有现代大型语言模型在这一点上都失败了。包括重量级的Opus和GPT-4。这些都是非凡的模型，能够回答关于经济学和量子力学的深奥问题，帮助你编码、绘画、制作音乐或视频，创建整个应用程序，甚至在高水平下下棋。但它们却不能玩数独。

或者说，LLMs受到了 [逆转诅咒](https://arxiv.org/abs/2309.12288) 的影响。

> 如果一个模型训练的句子形式为“A是B”，它不会自动推广到反向“B是A”。这就是逆转诅咒。例如，如果一个模型训练了“瓦伦蒂娜·列尔什科娃是第一位太空旅行的女性”，它不会自动回答问题“谁是第一位太空旅行的女性？”。而且，正确答案（“瓦伦蒂娜·列尔什科娃”）的可能性不会高于随机姓名。

换句话说，这些模型并不擅长概括理解人与人之间的关系。顺便说一句，目前最优秀的前沿模型 [依然不行](https://chat.openai.com/share/d6ee0c3f-4536-48ac-b540-e0cabcabaf51)。

让我们再做一个例子。也许问题出在一些奇怪的训练数据分布上。我们只是还没有展示足够的例子给它们看。那么，如果我们拿一些高度确定性的东西来做呢？我决定尝试教变压器预测元胞自动机。这似乎是一件有趣的事情。我以为只需要花2小时，但已经过去了2周。这里没有翻译问题，但仍然失败了！

好吧。那么这可能是为什么呢？这就是我想要尝试弄清楚的事情。这里至少有两个不同的问题：1）有些问题LLMs根本无法做到，因为它们的训练数据中没有这些信息，它们也没有被训练去做，2）由于它们的构建方式，LLMs无法做到的问题。几乎每件事都让我们想起问题二，尽管问题一也很常见。

我的论点是，某种方式模型存在目标漂移，因为它们被迫一次一个记号地前进，它们永远无法真正超越提示中的上下文，并且不知道应该将注意力集中在哪里。这也是为什么你可以通过说“[像这样的事情](https://arxiv.org/pdf/2404.08676.pdf)”，来越狱它们。

在LLMs中，正如在人类中一样，上下文是稀缺的。

太长不读，在我们深入探讨之前。

1.  LLMs是概率模型，模仿计算，有时与实际计算非常接近。

1.  当我们训练更大的模型时，它们将学习数据中更多的隐含关联，这将有助于更好的推断。请注意，它们学到的关联可能并不总是清晰地映射到*我们*的观念。

1.  推断总是一次单独的通过。LLMs无法停下来，收集世界状态，推理，重新访问旧答案或预测未来答案，除非该过程也在训练数据中详细说明。即使包括以前的提示和回应，这仍然意味着下一个推断从头开始是另一次单独的通过。

1.  这就产生了一个问题，那就是难免会出现一种‘目标漂移’的形式，推理变得不太可靠。（这也是为什么一些提示注入的形式有效，因为它扭曲了注意力机制。）这种‘目标漂移’意味着代理程序或按顺序执行的任务变得不太可靠。它‘忘记’了应该关注的地方，因为它的注意力既不选择性也不动态。

1.  LLMs 不能动态地重置它们自己的上下文。例如，图灵机使用磁带来存储记忆，而变压器则使用其内部状态（通过自注意力管理）来跟踪中间计算。这意味着有很多类型的计算变压器做不好。

1.  这可以部分通过像[思维链](https://arxiv.org/pdf/2310.07923.pdf)这样的方法或者使用其他LLMs来[审查和纠正输出](https://github.com/marquisdepolis/ReflectGPT)来解决，从本质上找到使推理保持在正确轨道上的方法。因此，通过足够的智能提示和逐步迭代，LLMs ***可以*** 被训练得能够引导几乎任何训练数据中的内容。随着模型的改进，每次推断也会变得更好，这将提高可靠性并使代理程序更加强大。

1.  通过大量的努力，我们将最终得到一个[链接的GPT系统](https://www.amazon.com/Building-God-Demystifying-Decision-Makers-ebook/dp/B0CJ9F327M)，内部迭代多次，持续错误检查和校正以及外部化的内存，作为功能组件。但是，即使我们通过多个领域的蛮力方法逼近AGI，它仍然无法真正超越其训练数据的广泛性。但这仍然是神奇的。

让我们跳进去。

这个很惊人。[LLMs不能做Wordle](https://github.com/marquisdepolis/LOOP-Evals)。或数独，或字谜，最简单的形式的纵横字谜。

显然这很奇怪，因为这些不是难题。任何一年级学生都可以尝试一下，但即使是最好的LLMs也不能做到。

第一个假设可能是缺乏训练数据。但在这种情况下会是这样吗？当然不会，因为规则肯定已经在数据中了。并不是Wordle在当前LLMs的训练数据集中无法避免。

另一个假设是由于标记化问题。但这也不能是真的。即使你给它多次机会并提供先前的答案，它仍然很难思考到正确的解决方案。在字母之间加上空格，依然没有运气。

即使你给它以前的答案和上下文以及问题，它通常只是重新开始整个回答序列，而不是编辑某些单元格[3,4]中的内容。

实际上，这是因为它的本质上每一步似乎需要不同层次的迭代计算，没有一个模型似乎能够做到。在某些方面这是有道理的，因为自回归模型一次只能进行一次前向传递，这意味着它最多只能使用它现有的令牌存储库和输出作为一个草稿本来继续大声思考，但是它很快就会失去追踪。

这里看起来的结论是，每一步都需要记忆和计算，这是当前变压器模型在层数和注意力头数方面无法解决的问题，即使你谈论的是像所谓的万亿令牌GPT 4这样的极大模型。

具有讽刺意味的是它无法弄清楚注意力放在哪里。因为当前的注意力方式是静态的，并同时处理序列的所有部分，而不是使用多个启发式方法来更有选择性地重置动态上下文，尝试反事实。

这是因为它所测量的注意力并不真正是我们进行的多线程分层分析的方式？或者更确切地说，它可能是，但它所做的概率评估并未将其上下文转化为任何个别问题。

在进行这个Wordle [评估实验](https://github.com/marquisdepolis/LOOP-Evals)时，我再次阅读了沃尔夫勒姆，并开始思考康威生命游戏，我想知道我们是否能教会变压器成功地学会模拟运行这些自动机几代的输出。

为什么呢？好吧，因为如果这样行得通，那么我们可以看看变压器是否可以充当准图灵完备的计算机，这意味着我们可以尝试在一个变压器上“堆叠”另一个，然后将多个元胞自动机连接在一起。我被Nerd Sniped了。

我的朋友乔恩·埃文斯称LLMs是柏拉图的洞穴中的生命形式。我们向它们投射我们世界的影子，它们试图推断出现实中发生的事情。它们在这方面非常擅长！但康威生命游戏不是一个影子，它是实际的信息。

而它们仍然失败了！

所以我决定微调一个GPT模型，看看我能否训练它完成这个任务。我尝试了更简单的版本，比如第28规则，看哪，它学会了！

它似乎也学会了复杂的规则，比如规则110或90（110号是著名的图灵完备，90号创建了相当漂亮的谢尔宾斯基三角形）。顺便说一句，在微调中只能使用二进制，不能有任何词语（比如“初始状态”或“最终状态”）。

所以我想，成功了，我们教会了它。

但是。

它只学到了它所展示的内容。如果你改变输入网格的大小，它就失败了。比如，我用32个输入单元调整了它，但如果我将问题扩展到更大的输入（如64或96的倍数），它就失败了。它不能泛化。它不理解。

现在，如果你使用更大的调整或更大的模型，可能可以使它学会，但问题是，为什么这种相对简单的过程，一个孩子都能算出来，对于这样一个庞大的模型却是遥不可及的呢？答案是，它试图在一次运行中预测所有的输出，凭直觉运行，无法回溯或检查更广泛的逻辑。这也意味着它并没有学习到实际支撑输出的5或8条规则。

并且它仍然无法学习康威生命游戏，即使是一个简单的8x8网格。

如果学习一个小的基本元胞自动机需要数万亿的参数和大量的例子，并且需要极其小心的提示后跟大量的迭代，那么这告诉我们它**不能**学到什么？

这也显示了同样的问题。它无法预测中间状态，然后从那一点开始工作，因为它试图完全通过预测来学习下一个状态。如果有足够的权重和层，它可能能够在某种程度上模仿这样的递归函数运行的外观，但它实际上无法模仿它。

像之前的Wordle一样，尝试的常规答案是通过思维链或重复的LLM调用来进行这个过程。

就像Wordle一样，除非你将整个输入分解，强制仅以标记为单位输出，否则它仍然会出错。因为注意力不可避免地会漂移，这只有在高度精确的情况下才有效。

现在你可能能够接受下一个最伟大的LLM，显示它的注意力不会漂移，尽管我们必须检查它的错误，看看失败是相似的形式还是不同的。

请稍等片刻。在这一点上，我认为我应该能够在这里教授基础知识，因为你可以生成无限的数据，只要你持续训练，直到得到你想要的结果。所以我决定编写一个小模型来预测这些。

下面是实际的网格 - 左边是CA，右边是Transformer的输出。看看你能否分辨出它们。

所以... 结果是它无法被训练来预测结果。我也搞不清楚为什么。尽管这些都是玩具变压器，但它们仍然可以处理我试图让它们学习的各种方程，甚至可以稍微概括一下。

我序列化了生命游戏的输入以便更容易查看，第二行是细胞自动机的输出（正确的输出），变压器的输出是第三行。它们是不同的。

所以我尝试了更小的网格，各种超参数优化，厨房水槽，但仍然不行。

随后我想也许问题在于它需要更多关于物理布局的信息。所以我添加了卷积网络层来帮助，并且改变了位置嵌入，明确分别指定了X轴和Y轴。但还是不行。

然后我真的感到气馁，试图教它一个非常简单的方程，希望我不是完全无能。

（实际上一开始甚至这都完全没用，我陷入了绝望之中，但最后一搏，简单地添加起始和停止标记，一切都起作用了。变压器很奇怪。）

缩放并不完美，但是它几乎没有任何头或层，并且max_iter是1000，显然它正在接近。

所以我想这个想法很明显，它需要学习许多状态并记住历史，这意味着我需要以某种方式添加这种能力。所以我甚至尝试改变解码器，在输出之后添加另一个输入，这相当于添加了另一个RNN（递归神经网络）层，或者说给它记住我们之前做过的步骤的记忆。

但不幸的是，依然不行。

即使你回到元胞自动机，从[基础元胞自动机](https://mathworld.wolfram.com/ElementaryCellularAutomaton.html)开始，事情也无法解决。而且那是一维的，甚至有一些非常简单的规则，比如 0，并不仅仅是图灵完备的规则，比如 110。

不。

或者当它学会在一堆问题上正确回答时，这是否意味着它学会了底层规则，或者是某种规则的仿制品，以至于它在我们给定的分布内模仿输出，并且可能以错误的方式获得结果？

它不仅仅是玩具模型或 GPT 3.5，它在更大的LLM中也展示了相同的问题，比如 GPT 4 或 Claude 或 Gemini，至少在聊天模式下。

无论是经过精细调整还是专门训练，LLMs似乎都不能玩康威生命游戏。

（如果有人能解决这个问题，我会非常感兴趣。或者即使他们能解释为什么问题存在。）

好吧，回到LLM。

好吧，解决这些问题的一种方法是我们尽可能多地将我们的智能纳入这些系统的设计中，这样最终输出就越有可能模拟所需的转换。

我们可以逐个尝试教授每个单独的难题，并希望它们能够转移推理，但我们如何知道它是否会甚至是否学到了泛化？直到最近，即使是加法和乘法[对这些模型来说也很困难](https://arxiv.org/abs/2201.02177)。

上周，Higher Order Comp 的创始人兼优秀的软件工程师 Victor Taelin 在网上声称“GPT永远不会解决A::B问题”。他的例子表明基于变压器的模型无法学习其训练集之外的真正新问题，或进行长期推理。

引用 [Taelin](https://twitter.com/VictorTaelin/status/1776248021858111542?t=QDsAaXNmYp9_peE-dIgA4w&s=19) 的话：

> 强大的GPT（如GPT-4或Opus）基本上是在其权重内演化出一个“电路设计师”。但是作为计算模型的注意力的严格性不允许这种演化电路足够灵活。这有点像AGI试图在其内部生长，但由于施加的计算和通信限制而无法做到。请记住，人类大脑一直在进行突触可塑性。存在一种更灵活的架构，如果在更小的规模上进行训练，可能会导致AGI；但我们尚不知道这种架构。

他对此提出了10000美元的赏金，并且[当天就有人认领了](https://x.com/VictorTaelin/status/1777049193489572064)。

显然，LLMs可以学习。

但最终我们需要这个模型能够告诉我们它学到的底层规则是什么，这是我们唯一能知道它是否学到了泛化的方式。

或者在这里，我通过Lewis看到了基本元胞自动机的最佳解决方案，他让[Claude Opus](https://x.com/ctjlewis/status/1780649750620180489)做了多代模拟。你也可以让它们运行康威生命游戏的每一步，尽管它们[有时会有些错误](https://arxiv.org/pdf/2303.14310.pdf)。

重点不在于它们在个别案例中答对或答错，而是它们答错的过程是不可逆的。即，由于它们没有全局上下文，除非再次运行以找出错误，否则在过程中不能这样做。它不能像我们那样，走到网格的一半然后重新检查，因为“看起来有问题”。或者只填写网格的相关部分然后再填写其余部分。或者我们解决问题的其他任何方式。

无论LLM看起来像什么，我们应该推测它与我们所认为的不太相似。

到目前为止，我们建造的最好的模型没有理由在一个儿童游戏“简单的重复交互”或“选择约束”中失败，这些似乎是LLM应该能够轻松做到的事情。但是它们确实会失败。经常。

如果它不能玩Wordle，那它能玩什么呢？

它[可以回答](https://arxiv.org/pdf/2303.12712.pdf)困难的数学问题，处理竞争性经济推理，费米估算或者甚至在没有显式训练的语言中解决物理问题。它可以解决像“*我开飞机从露营地出发，直奔东方精确飞行24901英里，却发现自己回到了营地。我发现帐篷里有一只老虎在吃我的食物！这只老虎是什么物种？*”

（答案只能是孟加拉或苏门答腊，因为24901是赤道的长度。）

他们还可以[下棋](https://arxiv.org/abs/2402.04494)。

但我们得到的答案非常依赖于我们提示的方式。

> 虽然这并不意味着GPT-4只是记忆常用的数学句子，并执行简单的模式匹配来决定使用哪一个（例如，交替名称/数字等通常不会影响GPT-4的答案质量），但我们确实看到问题的措辞变化可能会改变模型展示的知识。

**最好的说法可能是，LLM展示了令人难以置信的直觉，但智力有限。** 它可以在一次直觉过程中回答几乎任何可以回答的问题。在足够的训练数据和足够的迭代次数下，它可以达到一种推理智能的外观。

添加一个RNN类型的链接似乎在某种程度上有所改进，尽管在玩具模型中远远不足以解决问题，这表明了这个方向的迹象。但这并不足以解决问题。

换句话说，存在**“目标漂移”**，随着步骤增加，整个系统开始做错事情。随着语境增加，即使考虑了先前对话的历史，大型语言模型也难以确定焦点和实际目标。对于许多问题来说，注意力并不足够精确。

更接近的答案是神经网络一旦添加了外部内存，可以学习各种不规则的模式（[once you add an external memory](https://arxiv.org/abs/2207.02098)）。

> 我们的结果显示，在我们的任务子集中，循环神经网络（RNNs）和Transformer在非正则任务上无法泛化，长短期记忆网络（LSTMs）可以解决正则和反语言任务，只有增加结构化记忆（如堆栈或内存带）的网络才能成功泛化上下文无关和上下文敏感的任务。

这证据表明问题是某种类型的“目标漂移”确实存在。

从思维链提示开始，使用便签、将中间思想写在纸上并检索，它们都是通过解决问题来减少目标漂移的例子。它们有些有效，但仍受到原罪的限制。

因此，依赖于所有先前输入的状态的输出，特别是如果每个步骤都需要计算，对于当前基于Transformer的模型来说过于复杂和过长。

这也是它们目前不太可靠的原因。这就像智能版的宇宙射线导致的位翻转，但那里你可以轻易检查（最多3次），而在这里每个推理调用都需要时间和金钱。

尽管较大的模型在思维链更长的情况下不断改进以回答这类问题，它们在推理链的任意点上持续显示错误，这些错误似乎几乎独立于它们其他的所谓能力。

这就是自回归诅咒。如在最近的Dwarkesh播客中[Sholto所说的](https://www.dwarkeshpatel.com/p/sholto-douglas-trenton-bricken)：

> 我认为这并不是智能体尚未起飞的原因。我认为更多是关于[可靠性的9个标准](https://en.wikipedia.org/wiki/High_availability#%22Nines%22)以及模型实际成功执行任务的问题。如果你无法以足够高的概率连续链式任务，那么你将得不到看起来像代理的东西。这就是为什么像代理这样的东西可能更像一个阶跃函数。

基本上，即使在解决同一任务的过程中，随着步骤数的增加，也会出现错误。为什么会这样？我其实不知道，因为*感觉上不应该发生*。但确实发生了。

主要的扩展优势是这种类型错误的水平会下降吗？这可能是的，GPT-4的幻觉和错误比3.5版本少。随着规模的扩大，我们是不是只是得到了更有能力的模型，或者我们只是学会了如何通过规模扩展来减少幻觉？

但是，即使像GPT-4或Opus这样的东西甚至在玩Wordle游戏时都失败了，即使Devin能解决它，构建一个1000倍Devin是否真的是正确答案？

考试的问题是：如果存在一类问题，一个小学生可以轻松解决，但万亿令牌、价值数十亿美元的复杂模型无法解决，这对我们认知本质有何启示？

更大的问题在于，如果我们说的一切都是正确的，那么几乎可以通过定义，我们无法接近一个推理机。原因在于AGI中的G是难点，它可以轻松地超越其分布。尽管这不可能发生，[我们可以接近创造一个人工科学家](https://www.amazon.com/Building-God-Demystifying-Decision-Makers-ebook/dp/B0CJ9F327M)，它将有助于推动科学进步。

我们拥有的更接近于巴别塔图书馆的一个切片，我们不仅可以阅读已经写好的书籍，还可以阅读接近已经写好的书籍的书籍，信息存在于这些书籍之间的间隙中。

但它也是区分库恩范式的绝佳示例。人类在判断规模影响时很差劲，[一个链接](https://www.strangeloopcanon.com/p/llms-have-special-intelligence-not)。

> 它们接受的训练信息比一个人在一生中希望看到的信息还要多。假设一个人可以每分钟阅读300个单词，每天阅读8小时，他们一生可能会读30000到50000本书。大多数人可能最多只能读到这些书籍的一个微薄子集，至多1%。这充其量是1 GB的数据。
> 
> 另一方面，LLM不仅仅吸收了互联网上的一切，还有许多其他内容，跨越所有领域和学科的数百亿字。GPT-3是在45 TB的数据上训练的。按每本书2MB计算，这大约是2250万本书。

如果有人读了200万本书，它会是什么样子，并不是一个我们能给出直接或者甚至是指数推断答案的问题。如果一个简单的模式识别器读了200万本书，它能做到什么程度也是一个问题，我们并没有一个简单的答案。问题在于LLM学习训练数据中的模式和隐含规则，但不容易将其明确化。除非LLM有办法知道哪些模式匹配与哪些方程相关，否则它无法学会推广。这就是为什么我们仍然有逆转诅咒。

LLM是否像一个实体，或者像一个神经元，或者像新皮质的一部分，这些在某些时候都是有用的比喻，但没有一个完全捕捉到我们从它们那里看到的行为。

能够学习模式的模型的有趣部分在于，它学习了我们可能没有明确纳入数据集的模式。它开始学习语言，但在这个过程中，它还发现了数据中存在的多个链接，使得它能够将冯·诺伊曼与查尔斯·狄更斯联系起来，并输出一个足够逼真的模拟品，我们可能会做到。

即使假设数据集编码了其内在的整个人类复杂性，存在于较小数据集中的这类模式的数量也会迅速超过模型的大小。这几乎是数学上的必然性。

与我们之前测试过的细胞自动机问题类似，目前尚不清楚它是否真正学会了这种方法或者其可靠性如何。因为他们的错误更能显示出他们不知道的东西，而不是成功。

关于更大的神经网络的另一个观点是，它们不仅会从数据中学习，还会学会学习。它显然做到了这一点，这就是为什么你可以提供一些例子，让它处理训练集中从未见过的问题。但它们使用的方法似乎不够泛化，绝对不能像学习在哪里集中注意力那样泛化。

学会学习并非对我们来说是一个单一的全局算法。对某些事物效果更好，对其他事物则更差。它在不同类型的问题上以不同的方式工作。所有这些都必须写入相同数量的参数中，以便通过这些权重进行计算，可以回答关于木偶的问题，并告诉我将毁掉弦理论的下一个最伟大的物理发现。

如果序列中的符号以一种方式相互作用，其中一个符号的存在或位置会影响下一个符号的信息内容，那么数据集的整体Shannon熵可能会高于仅通过查看单个符号所建议的内容，这将使像康威生命游戏这样状态相关的事物变得非常困难。

这也是为什么即使在生命游戏数据集上进行了精调，即使GPT也似乎无法真正学习到模式，而是学会了足够回答问题。一种特定形式的Goodhart现象。

（在括号中要求一个捉弄性的问题，以定义任何一个这些问题中的单个问题，以便你可以对其进行测试，并运行它对一个llm的笨蛋问题，这也是一个愚蠢的举动，当你考虑到定义任何一个这些问题实际上是科学研究大纲的一半世纪或更长时间。

这也意味着，类似于当前的理论，**向llm模型添加更多递归当然会使它们变得更好**。但只要你能记住原始目标和迄今为止的路径，你应该能够逐步解决更复杂的规划问题。

虽然目前还不清楚为什么它不可靠。与 3.5 相比，GPT 4 更可靠，但我不知道这是因为我们在训练这些模型方面取得了更大进展，还是因为扩展规模增加了可靠性并减少了幻觉。

这项技术的梦想用例是代理人，能够为我们完成整个任务的自主实体。确实，对于许多任务来说，[更多的代理人就是你所需要的](https://twitter.com/sebkrier/status/1776665719477858619?t=vmCwylJtdXgALir8BQYBCA&s=19)。如果这对某些任务有所改善，是否意味着如果你有足够数量的代理人，它对所有任务的效果会更好？这是有可能的，但目前不太可能。

有了像 [Devin, from Cognition Labs](https://www.cognition-labs.com/introducing-devin) 这样的选择，我们看到了它潜力有多强大。来自 [实际用例](https://www.notion.so/What-can-LLMs-not-do-Why-can-t-LLMs-play-Conway-s-Game-Of-Life-de30ffdcd5ec4a09b922fb3530df21bd?pvs=21)：

> 有了 Devin，我们已经：
> 
> +   将 Swift 代码发布到 Apple App Store
> +   
> +   编写了 Elixir/Liveview 多人应用
> +   
> +   在以下项目中移植了整个项目：
> +   
> +   从 0 开始启动了完整的 MERN 全栈项目
> +   
> +   自主创建了 PR，完整记录了文档
> +   
> 我不知道我刚提到的技术的一半。我只是作为半技术监督员参与工作，偶尔检查并复制错误消息并提供饼干。事实上，我感觉自己像是一个工程/产品经理，只是在检查 5 名同时工作的工程师。 （我现在在路上，稍后会发送截图。）
> 
> 它完美吗？当然不是。它速度慢，可能非常昂贵，受到 24 小时窗口的限制，设计方面很差，而且在 Git 操作方面表现出乎意料的糟糕。

这种行为是否能够在未来几年内扩展到相当比例的工作岗位？我认为没有理由不行。你可能需要 [逐个职位来分析](https://github.com/marquisdepolis/galen-evals)，而且这些将是不容易扩展的专业模型，而不一定是一种模型能够解决所有问题。

开源版本已经向我们透露了 [秘密酱](https://x.com/jyangballin/status/1775114448513958134) 的部分内容，这是仔细审查信息抵达底层模型的顺序，信息量达到多少以及在给定（如前所述）限制条件下创建它们可以繁荣的环境的关键。

因此，这里的解决方案是，GPT 不能单独解决像生命游戏这样的问题并不重要，甚至当它思考步骤时也不重要，重要的是它可以编写程序来解决问题。这意味着，如果我们能训练它识别那些编写每个程序都有意义的情况，它将接近通用人工智能。

（这是我持有的观点。）

此外，至少对于较小的模型来说，权重内部存在学习内容的竞争。空间有限，这是我在 [DeepSeek 论文](https://arxiv.org/html/2403.05525v2) 中看到的最好评论之一。

> 尽管如此，DeepSeek-VL-7B显示了数学（GSM8K）方面的某种程度下降，这表明尽管努力促进视觉和语言模态之间的和谐，它们之间仍然存在竞争关系。这可能归因于模型容量有限（7B），更大的模型可能会显著缓解这一问题。

所以，这就是我们学到的。

1.  存在某些类型的问题，今天的LLMs无法解决，尤其是那些依赖于先前状态或预测未来状态的较长推理步骤。玩Wordle或预测CA都是这种情况的例子。

1.  借助更大的LLMs，我们可以[教会它推理](https://github.com/VictorTaelin/ab_challenge_eval/blob/main/users/futuristfrog/prompt.txt)，在一定程度上，通过为其提供关于问题的逐步信息和多个示例来跟随。然而，这抽象了实际问题，并将思考答案的方式置于提示中。

1.  通过a)更好的提示，b)中间访问内存和计算工具，它会变得更好。但是它将无法像我们对待人类那样使用那个词来达到可泛化的感知能力。**只要我们向LLM提供了信息，凭借合适的提示，它可能会回答出来。**

1.  因此，正确使用这些模型的**巨大**一部分是根据手头任务正确提示它们。这可能需要精心构造长序列的正确和错误答案来解决计算问题，以引导模型适当地回复，同时设定外部的保护栏。

1.  因为“注意力”容易发生目标偏移，所以在没有显著的外部支持的情况下，这真的很难保证可靠性。LLMs犯的错误***比它们的成功更有启发性***。

我认为要实现AGI，要达到足够的泛化水平，我们需要基本的架构改进。扩展现有模型并添加新的架构如Jamba等将使它们更有效、工作更快、更可靠。但它们并没有解决泛化或“目标偏移”缺乏的根本问题。

即使添加专门的代理进行“提示工程”并添加17个GPT来彼此交流，也不完全能达到我们的目标，尽管通过足够多的修补，结果在我们关心的领域可能是无法区分的。当象棋引擎首次出现时，早期AI的日子里，它们拥有有限的处理能力和几乎没有真正有用的搜索或评估功能。因此，你必须依赖于修补，比如硬编码的开局或残局，用于更好的搜索的迭代加深，alpha-beta剪枝等等。最终它们通过增量改进克服了这些问题，正如我们在LLMs中所做的那样。

我倾向于的一个想法是在不同层次的层级中有多个规划代理，它们能够指导其他具有自己子代理的专业代理等等，所有这些代理相互连接，一旦可靠性稍微提高。

我们可能能够添加推理、迭代、添加持久和随机访问记忆的模块，甚至提供对物理世界的理解。在这一点上，我们似乎应该像从动物那里获得一样，从LLMs获得“足够规模化的统计数据与智能相仿”，但我们会吗？它也可能最终成为一个极具说服力的统计模型，模仿我们需要的内容，同时在分布之外失败。

这也是为什么我称LLMs为[模糊处理器](https://www.strangeloopcanon.com/p/beyond-google-to-assistants-with)。这也是为什么像“LLM是什么样”的问题最终会导致循环对话的结束。

绝对不能将这些中的任何一点视为今天拥有的不是奇迹。仅仅因为我认为苦涩的教训不会一直推广到通用人工智能，并不意味着我们已经拥有的成果不是非凡的。

我完全相信LLMs确实从它们看到的数据中“学习”。它们不是简单的压缩器，也不是鹦鹉。它们能够连接来自训练集不同部分或提示的细微数据，并提供智能响应。

如果汤姆·纳格愿意的话，他可能会问LLM是什么样的问题。蝙蝠作为哺乳动物，比LLMs更接近我们，如果它们的内部对我们来说是一片模糊，那么我们有什么机会去理解新模型的内部功能呢？或者相反，因为对于LLMs，我们可以自由地检查每一个权重和电路，那么我们可能会对我们使用的这些模型有怎样的洞察呢？

这就是为什么我正式愿意接受这一现实。**在训练数据分布内，足够规模化的统计学几乎无法区分是否具有智能**。虽然不是对所有事情都有效，也不足以完成所有事情，但这并不是幻影。这就是为什么从测试中得到的错误比成功更有用于诊断的原因。

如果LLMs是一个[任意到任意的机器](https://www.strangeloopcanon.com/p/generative-ai-or-the-anything-from)，那么我们应该能够让它做大多数事情。最终，通过大量的敦促和刺激。也许不能启发它像巴赫或冯·诺伊曼那样的天才，但是可以进行更为普通但同样重要的创新和发现。并且我们可以在不需要它具有意识或道德人格的情况下完成这些。如果我们能够自动化或加速库恩所写关于范式内跨越的跃迁，那么我们就能自由地在不同范式之间跃迁。
