- en: <!--yml
  id: totrans-split-0
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  id: totrans-split-1
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-05-27 13:29:48'
  id: totrans-split-2
  prefs: []
  type: TYPE_NORMAL
- en: -->
  id: totrans-split-3
  prefs: []
  type: TYPE_NORMAL
- en: How we manage individual user concurrencies with Nginx and Lua
  id: totrans-split-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://www.browserless.io/blog/managing-concurrencies-with-nginx-and-lua](https://www.browserless.io/blog/managing-concurrencies-with-nginx-and-lua)
  id: totrans-split-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '**In this article we’ll look at the challenges of managing individual concurrency
    limits, hosting thousands of browsers for our users.**'
  id: totrans-split-6
  prefs: []
  type: TYPE_NORMAL
- en: ‍
  id: totrans-split-7
  prefs: []
  type: TYPE_NORMAL
- en: There’s countless articles about straightforward load balancing. Even we have
    one about using [NGINX for scaling Puppeteer](https://www.browserless.io/blog/horizontally-scaling-chrome).
  id: totrans-split-8
  prefs: []
  type: TYPE_NORMAL
- en: '*“It’s the same way you scale anything, containerize it and put a load balancer
    in front. Boring.”* - u/express-set, reddit'
  id: totrans-split-9
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: So, I wanted to share details about our complex custom load balancers. This
    is a behind the scenes info about how we dynamically limit concurrent connections
    per user. It’s particularly important for us since browsers are notoriously [messy
    to host](https://www.browserless.io/blog/advanced-issues-when-managing-chrome-on-aws),
    and our thousands of users can change to plans with different concurrency limits
    at any time.
  id: totrans-split-10
  prefs: []
  type: TYPE_NORMAL
- en: To address this challenge effectively, we must be able to adjust and update
    connection limits on-the-fly to accommodate different customer plans and usage
    patterns.
  id: totrans-split-11
  prefs: []
  type: TYPE_NORMAL
- en: First, some context about what we’re load balancing
  id: totrans-split-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This article is about my work managing concurrencies for Browserless.
  id: totrans-split-13
  prefs: []
  type: TYPE_NORMAL
- en: Our service provides a pool of hosted Chrome, Firefox and WebKit browsers. We
    have thousands of users, each with limits of at least 25 browsers depending on
    their plan. These are primarily spread between our two main server locations of
    London and San Francisco.
  id: totrans-split-14
  prefs: []
  type: TYPE_NORMAL
- en: These browsers are for using with Puppeteer, Playwright or via one of our APIs.
    That means they start up, running a script for some amount of seconds or minutes,
    then close.
  id: totrans-split-15
  prefs: []
  type: TYPE_NORMAL
- en: At the extreme end, some of our users will open up a hundred browsers, do a
    large batch of web scraping, then do nothing for a week. Meanwhile, we have users
    fairly regularly starting browsers 24/7 for tasks such as generating PDF exports
    of dashboards.
  id: totrans-split-16
  prefs: []
  type: TYPE_NORMAL
- en: So, it’s a rapidly fluctuating environment with a range of usage requirements.
  id: totrans-split-17
  prefs: []
  type: TYPE_NORMAL
- en: '**Plan of Attack**'
  id: totrans-split-18
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Breaking down the challenge into a few steps went as follows:'
  id: totrans-split-19
  prefs: []
  type: TYPE_NORMAL
- en: '**Identifying the Optimal Location -** Recognizing that all traffic passes
    through the load balancer, I determined it to be the ideal point for implementing
    connection limits.'
  id: totrans-split-20
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Limitation of Embedded Nginx Module -** Initial exploration revealed that
    the embedded [Nginx module for limiting connections](https://nginx.org/en/docs/http/ngx_http_limit_conn_module.html)
    lacked the capability to adjust limits dynamically, prompting us to explore alternative
    solutions.'
  id: totrans-split-21
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Leveraging OpenResty and Lua Scripting -** I opted to leverage the versatility
    of OpenResty and Lua scripting to overcome the limitations of the embedded Nginx
    module. This decision was driven by the flexibility and extensibility offered
    by Lua scripting.'
  id: totrans-split-22
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Utilizing `lua-resty-limit-trafic` Library -** I discovered the [lua-resty-limit-trafic](https://github.com/openresty/lua-resty-limit-traffic)
    library during my research, an official solution for limiting traffic. This library
    offers dedicated modules for controlling both request frequency and concurrent
    connections, aligning perfectly with our requirements.'
  id: totrans-split-23
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Implementation
  id: totrans-split-24
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'I developed a custom limiter module based on the `lua-resty-limit-trafic` library,
    comprising three main functions:'
  id: totrans-split-25
  prefs: []
  type: TYPE_NORMAL
- en: '**GetAllowedConcurrency -** This function retrieves the concurrency limit for
    a given customer token. It fetches the limit either from a database or a local
    cache. In cases where the limit cannot be retrieved, a default limit is applied.'
  id: totrans-split-26
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**LimitRequestsByToken -** Utilizing the lua-resty-limit-trafic library, this
    function increments the number of connections associated with a specific token.
    It checks if the limit is exceeded and throws an error accordingly. The limit
    is dynamically set based on the customer''s concurrency limit.'
  id: totrans-split-27
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: ‍**LeavingConnection -** Upon the closure of a connection, this function decreases
    the number of active connections associated with the token.
  id: totrans-split-28
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The following code snippet showcases the implementation of these functions,
    demonstrating how the lua-resty-limit-trafic library is utilized to manage connection
    limits dynamically.
  id: totrans-split-29
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  id: totrans-split-30
  prefs: []
  type: TYPE_PRE
- en: To integrate the limiter module into the Nginx configuration, we import it into
    the default.conf file. The module is invoked within the `access_by_lua_block`
    and `log_by_lua_block` directives, leveraging the Authorization header as the
    identifier for each customer token.
  id: totrans-split-31
  prefs: []
  type: TYPE_NORMAL
- en: It's imperative to create a `lua_shared_dict` to store the number of existing
    connections per token.
  id: totrans-split-32
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  id: totrans-split-33
  prefs: []
  type: TYPE_PRE
- en: Additionally, to avoid `module not found` errors, the `lua_package_path` directive
    must be configured in the nginx.conf file to specify the location of the limiter
    module.
  id: totrans-split-34
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  id: totrans-split-35
  prefs: []
  type: TYPE_PRE
- en: At this point we are limiting connections as planned. However, prior to implementing
    this solution, we encountered some challenges that needed to be addressed.
  id: totrans-split-36
  prefs: []
  type: TYPE_NORMAL
- en: Issues Faced
  id: totrans-split-37
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: When using the limiter, we encountered escalating CPU and memory consumption
    over time, indicating the presence of a memory leak. This posed a significant
    challenge as it necessitated periodic service restarts to prevent crashes, prompting
    an exhaustive investigation to identify and resolve the underlying issue.
  id: totrans-split-38
  prefs: []
  type: TYPE_NORMAL
- en: I began troubleshooting by disconnecting non-essential components. First, I
    removed the database connection, but unfortunately, it didn't resolve the issue.
    Next, I removed the local cache, yet the problem persisted.
  id: totrans-split-39
  prefs: []
  type: TYPE_NORMAL
- en: As a last resort, I decided to test removing the import `local limits = require
    "limiter"` and consolidating all code into a single file. Surprisingly, this resolved
    the issue.
  id: totrans-split-40
  prefs: []
  type: TYPE_NORMAL
- en: I finally identified the problem. Prior to editing the nginx.conf file, I was
    using the [package.path](https://www.lua.org/manual/5.1/manual.html#pdf-package.path)
    within the Lua script to import my custom Lua folder.
  id: totrans-split-41
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  id: totrans-split-42
  prefs: []
  type: TYPE_PRE
- en: 'An important oversight occurred: each time the script ran, the package.path
    increased because I was concatenating my folder name to the same value in every
    request. Consequently, after a few requests, inspecting the value of package.path
    revealed an accumulation of folder names.'
  id: totrans-split-43
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  id: totrans-split-44
  prefs: []
  type: TYPE_PRE
- en: Initially, it seemed that concatenating folder names to package.path was the
    correct approach, as suggested by the Lua official documentation. However, this
    method caused the mentioned problems in our specific use case. Eventually, I referred
    to the [lua-nginx-module](https://github.com/openresty/lua-nginx-module#lua_package_path)
    documentation to find the correct way to import modules, and the problem was resolved
    by adding the `lua_package_path` directive to the nginx.conf file.
  id: totrans-split-45
  prefs: []
  type: TYPE_NORMAL
- en: Below is a graphical representation illustrating the consumption of resources
    before and after implementing the fix.
  id: totrans-split-46
  prefs: []
  type: TYPE_NORMAL
- en: Additionally, to enhance performance and minimize database queries, we incorporated
    a local cache with a Time-to-Live (TTL) mechanism to store customer limits. This
    optimization strategy effectively reduced latency and alleviated the strain on
    the database.
  id: totrans-split-47
  prefs: []
  type: TYPE_NORMAL
- en: '**How It Turned Out**'
  id: totrans-split-48
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the end, the new limiter worked great. It now effectively manages concurrent
    connections per customer to keep the service healthy and stable, while preventing
    service abuses and maintaining healthy worker processes.
  id: totrans-split-49
  prefs: []
  type: TYPE_NORMAL
- en: If you would like to see it in action, go ahead and grab a [Browserless trial](https://www.browserless.io/pricing)
    and have a play with using the concurrent browsers.
  id: totrans-split-50
  prefs: []
  type: TYPE_NORMAL
