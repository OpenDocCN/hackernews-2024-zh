- en: <!--yml
  id: totrans-split-0
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  id: totrans-split-1
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-05-27 13:20:06'
  id: totrans-split-2
  prefs: []
  type: TYPE_NORMAL
- en: -->
  id: totrans-split-3
  prefs: []
  type: TYPE_NORMAL
- en: First Impressions of Early-Access GPT-4 Fine-Tuning | Supersimple
  id: totrans-split-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://www.supersimple.io/blog/gpt-4-fine-tuning-early-access](https://www.supersimple.io/blog/gpt-4-fine-tuning-early-access)
  id: totrans-split-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: A few weeks ago we finally got access to the GPT-4 fine-tuning API (in limited
    early access), and were super excited to check out how well it works. We’d been
    a user of OpenAI’s fine-tuned models since fine-tuning the original GPT-3 davinci
    model first became available.
  id: totrans-split-6
  prefs: []
  type: TYPE_NORMAL
- en: 'Unsurprisingly, a fine-tuned GPT-4 greatly outperforms fine-tuned GPT-3.5 (more
    than 50% improvement for our use case!). We’ll go deeper into how we use these
    models below, but let’s start by comparing several of the OpenAI-based models
    we use at Supersimple:'
  id: totrans-split-7
  prefs: []
  type: TYPE_NORMAL
- en: Fine tuned (FT) GPT-3 Davinci model, which we used at the beginning
  id: totrans-split-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: GPT-3.5 and GPT-4 base models
  id: totrans-split-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: GPT-3.5 and GPT-4, fine-tuned models using a custom proprietary data set
  id: totrans-split-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Relative model performance based on our test set; FT signifies a fine-tuned
    model
  id: totrans-split-11
  prefs: []
  type: TYPE_NORMAL
- en: The models in question were fine-tuned for a domain-specific use case of asking
    data questions in natural language. The LLMs are tasked with putting together
    the appropriate reports and underlying queries for the user’s question.
  id: totrans-split-12
  prefs: []
  type: TYPE_NORMAL
- en: These evaluations were performed using a proprietary test data set we use at
    Supersimple; Davinci’s (GPT-3) performance is taken as a baseline. To give more
    insight into where we see these models working well, let’s give a bit more context
    on what we actually do. We’ll also include more comparative data, including cost
    and latency, below!
  id: totrans-split-13
  prefs: []
  type: TYPE_NORMAL
- en: Context & our LLM usage at Supersimple
  id: totrans-split-14
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Supersimple is a data analytics platform that allows users to dive deep into
    their data, to answer complex ad-hoc questions in seconds and to uncover deep
    data insights.
  id: totrans-split-15
  prefs: []
  type: TYPE_NORMAL
- en: Our users can ask natural language (plain English) questions about their data,
    and get answers in the form of tables and visualizations. The AI explains its
    work using a few no-code steps, and the user can always continue to go even deeper
    – either using natural language or a few clicks on our data platform.
  id: totrans-split-16
  prefs: []
  type: TYPE_NORMAL
- en: We use LLMs to answer our users’ natural language questions, with the goal of
    providing a great starting point for further deep dives into the data. The LLM
    takes the user's question, relevant parts of their [semantic data model](https://www.supersimple.io/blog/semantic-layers),
    existing reports and dashboards, and generates an exploration that answers the
    question with the most context possible.
  id: totrans-split-17
  prefs: []
  type: TYPE_NORMAL
- en: ‍
  id: totrans-split-18
  prefs: []
  type: TYPE_NORMAL
- en: This feature is also particularly useful as an onboarding tool, which showcases
    the capabilities of our platform and teaches users to use our data exploration
    UI. For long-time users, plain English is at times still the fastest way to answer
    questions, but at times it’s just as quick for them to just click a few buttons.
  id: totrans-split-19
  prefs: []
  type: TYPE_NORMAL
- en: Implementation
  id: totrans-split-20
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Technically speaking, the LLM outputs a custom high-level markup language (DSL)
    that we designed for our specific use case, that’s token-efficient and well-aligned
    with pre-trained language models. This DSL gets compiled into JSON, that we in
    turn compile into one or more database queries to be executed. Finally, we render
    an interactive data exploration to the user based on the report structure dictated
    by the DSL.
  id: totrans-split-21
  prefs: []
  type: TYPE_NORMAL
  zh: 从技术上讲，LLM输出一种我们为特定用例设计的自定义高级标记语言（DSL），这种DSL在token效率和与预训练语言模型的良好对齐方面表现良好。这个DSL编译成JSON，然后我们将其编译成一个或多个要执行的数据库查询。最后，根据DSL指定的报告结构，我们呈现一个交互式数据探索给用户。
- en: 'It''s important to highlight that this task is quite different from the traditional
    text-to-SQL problem for a number of reasons:'
  id: totrans-split-22
  prefs: []
  type: TYPE_NORMAL
  zh: 强调一下，这个任务与传统的文本转SQL问题有很多不同之处：
- en: The output is a no-code exploration, which is transparent and explainable to
    the user. This means that the model directly interacts with our data platform.
    We believe that generating & showing code or SQL is a terrible way to prove/explain
    reports to users – even if those users are technical, but especially if they aren’t.
  id: totrans-split-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 输出是一个无代码探索，对用户透明且可解释。这意味着模型直接与我们的数据平台交互。我们认为生成和展示代码或SQL是向用户证明/解释报告的可怕方式——即使这些用户是技术人员，尤其是如果他们不是。
- en: The resulting report/exploration is easily editable in the UI, and is meant
    to serve as a starting point for further deep dives. When exploring data, you
    won’t want to think about refactoring SQL queries.
  id: totrans-split-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 结果报告/探索在UI中易于编辑，旨在作为进一步深入探索的起点。在探索数据时，您不希望考虑重构SQL查询。
- en: The complex output is broken into individual blocks that represent logical steps
    in the reasoning process.
  id: totrans-split-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 复杂的输出被分成代表推理过程中逻辑步骤的单个块。
- en: We offload the complexity of constructing correct SQL queries with many joins,
    CTEs and subqueries to the platform’s “query engine”, which deterministically
    compiles the semantic steps output by the model into valid and performant SQL.
    This means the LLM doesn’t need to worry about an entire class of problems, allowing
    it to perform better overall.
  id: totrans-split-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们将构建正确的SQL查询的复杂性（包括许多连接、CTE和子查询）转移到平台的“查询引擎”，该引擎将模型输出的语义步骤确定性地编译成有效且高效的SQL。这意味着LLM不需要担心整个问题类别，从而可以在整体上表现更好。
- en: When generating output, the model takes into account existing dashboards and
    user-defined concepts.
  id: totrans-split-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在生成输出时，模型会考虑现有的仪表板和用户定义的概念。
- en: Due to the complexity of the task and the need for a custom output format (a
    DSL that the AI uses to effectively construct app UI state), we found that fine-tuned
    models tend to perform significantly better than the base ones.
  id: totrans-split-28
  prefs: []
  type: TYPE_NORMAL
  zh: 由于任务的复杂性和需要自定义的输出格式（我们设计的用于有效构建应用程序UI状态的DSL），我们发现微调模型的性能通常比基础模型显著更好。
- en: 'To reiterate: we never generate SQL using LLMs – our LLMs only interact with
    our app directly.'
  id: totrans-split-29
  prefs: []
  type: TYPE_NORMAL
  zh: 重申一下：我们从不使用LLM生成SQL——我们的LLM只直接与我们的应用程序交互。
- en: '**Fine-tuning and benchmarking**'
  id: totrans-split-30
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**微调和基准测试**'
- en: For fine-tuning, we use a proprietary dataset that contains tens of millions
    of tokens worth of question-answering examples, with combinations of data models,
    questions and the perfect output reports to answer these questions.
  id: totrans-split-31
  prefs: []
  type: TYPE_NORMAL
  zh: 对于微调，我们使用了一个专有数据集，其中包含数千万个token的问答示例，包括数据模型的组合，问题和完美的输出报告以回答这些问题。
- en: GPT-3.5 (gpt-3.5-turbo-0125) and GPT-4 (gpt-4-0613, the only fine-tunable GPT-4
    model available at the moment) were fine-tuned for 3 epochs.
  id: totrans-split-32
  prefs: []
  type: TYPE_NORMAL
  zh: GPT-3.5（gpt-3.5-turbo-0125）和GPT-4（gpt-4-0613，目前唯一可微调的GPT-4模型）都进行了3轮的微调。
- en: For both of the base models here (gpt-3.5-turbo-0125 and gpt-4-0125-preview),
    we used the same prompt, containing 8-shot examples. Performance of all models
    except davinci was evaluated in the beginning of March 2024 while the performance
    of davinci was last evaluated back in August 2023.
  id: totrans-split-33
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这里的两个基础模型（gpt-3.5-turbo-0125和gpt-4-0125-preview），我们使用了相同的提示，包含8次示例。除了davinci之外的所有模型的性能在2024年3月初进行了评估，而davinci的性能则是在2023年8月进行了最后评估。
- en: The performance comparison is shown in the next table. For our test set, fine-tuned
    (FT) GPT-4 **outperformed GPT-3.5 by 56%**, a significant, albeit slightly smaller
    improvement than the jump from Davinci FT to GPT-3.5 FT (96%).
  id: totrans-split-34
  prefs: []
  type: TYPE_NORMAL
  zh: 性能比较显示在下表中。对于我们的测试集，微调的GPT-4 **性能比GPT-3.5提高了56%**，这是一个显著的改进，尽管略小于从Davinci微调到GPT-3.5微调（96%）的跃升。
- en: Fine-tuned GPT-4 is slightly slower than fine-tuned GPT-3.5 (21.6 tokens/s vs
    31 tokens/s).
  id: totrans-split-35
  prefs: []
  type: TYPE_NORMAL
  zh: 微调的GPT-4比微调的GPT-3.5稍慢（21.6 tokens/s对31 tokens/s）。
- en: '*Note: the original version of this article showed significantly worse latency
    stats for GPT-4: our first benchmarks only achieved 5.8 tokens/s, but OpenAI has
    since greatly improved the service''s stability and speed.*'
  id: totrans-split-36
  prefs: []
  type: TYPE_NORMAL
  zh: '*注：本文的原始版本显示了GPT-4的延迟统计数据明显更差：我们的第一组基准测试仅实现了5.8个token/s，但OpenAI已经大大改进了服务的稳定性和速度。*'
- en: While cost is significantly higher for GPT-4 (15x higher for inference, 11x
    higher for training, compared to GPT-3.5), it might not be an important factor
    for you depending on your use case.
  id: totrans-split-37
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然GPT-4的成本显著更高（推断高15倍，训练高11倍，相比GPT-3.5），但根据您的使用情况，这可能不是一个重要因素。
- en: 'We benchmarked models on our in-house test set of 100 diverse questions. Examples
    of questions and model performance are shown in the table below:'
  id: totrans-split-38
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在我们的内部测试集上对模型进行了基准测试，该测试集包含100个多样化的问题。下表显示了问题和模型性能的示例：
- en: '| Question | Davinci FT | GPT-3.5 | GPT-4 | GPT-3.5 FT | GPT-4 FT |'
  id: totrans-split-39
  prefs: []
  type: TYPE_TB
  zh: '| 问题 | Davinci FT | GPT-3.5 | GPT-4 | GPT-3.5 FT | GPT-4 FT |'
- en: '| User with id 342122 | ✅ | ✅ | ✅ | ✅ | ✅ |'
  id: totrans-split-40
  prefs: []
  type: TYPE_TB
  zh: '| 用户ID为342122的用户 | ✅ | ✅ | ✅ | ✅ | ✅ |'
- en: '| Companies that are using the analytics dashboard? | ✅ | ❌ | ✅ | ✅ | ✅ |'
  id: totrans-split-41
  prefs: []
  type: TYPE_TB
  zh: '| 使用分析仪表板的公司？ | ✅ | ❌ | ✅ | ✅ | ✅ |'
- en: '| What''s our ARR and how is it changing over time? | ❌ | ❌ | ❌ | ✅ | ✅ |'
  id: totrans-split-42
  prefs: []
  type: TYPE_TB
  zh: '| 我们的ARR是多少，以及随时间的变化如何？ | ❌ | ❌ | ❌ | ✅ | ✅ |'
- en: '| Companies whose subscriptions are expiring this month | ❌ | ❌ | ❌ | ✅ | ✅
    |'
  id: totrans-split-43
  prefs: []
  type: TYPE_TB
  zh: '| 本月订阅即将到期的公司 | ❌ | ❌ | ❌ | ✅ | ✅ |'
- en: '| QoQ change in feature usage on account level | ❌ | ❌ | ❌ | ❌ | ✅ |'
  id: totrans-split-44
  prefs: []
  type: TYPE_TB
  zh: '| 账户级别特征使用情况的季度季度变化 | ❌ | ❌ | ❌ | ❌ | ✅ |'
- en: '| Churn trends this year vs last year | ❌ | ❌ | ❌ | ❌ | ✅ |'
  id: totrans-split-45
  prefs: []
  type: TYPE_TB
  zh: '| 今年与去年的流失趋势 | ❌ | ❌ | ❌ | ❌ | ✅ |'
- en: '| What are the main blockers in our onboarding funnel? | ❌ | ❌ | ❌ | ❌ | ❌
    |'
  id: totrans-split-46
  prefs: []
  type: TYPE_TB
  zh: '| 我们的入职漏斗中的主要障碍是什么？ | ❌ | ❌ | ❌ | ❌ | ❌ |'
- en: ‍
  id: totrans-split-47
  prefs: []
  type: TYPE_NORMAL
  zh: ‍
- en: Despite the performance enhancements, models continue to struggle with broad
    and open-ended queries when trying to answer with a single completion.
  id: totrans-split-48
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管性能有所提升，但模型在试图用单一完成来回答广泛和开放的查询时仍然存在困难。
- en: Worryingly, there is an observable trend of diminishing returns from fine-tuning.
    While fine-tuned Davinci showed marked improvement over its base model, fine-tuned
    GPT-3.5 offered lesser gains, and the progress achieved by fine-tuning GPT-4 was
    even smaller.
  id: totrans-split-49
  prefs: []
  type: TYPE_NORMAL
  zh: 令人担忧的是，从微调中出现了递减回报的可观趋势。虽然微调的Davinci相比其基础模型显示了显著改进，微调的GPT-3.5提供的收益较少，并且通过微调GPT-4获得的进展更小。
- en: To address these limitations, in production, we seldom rely on a single model
    call. Instead, we employ a mix of various specialized models, prompts, and heuristics
    to improve both accuracy and response time, achieving better performance than
    any individual model.
  id: totrans-split-50
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决这些限制，在生产中，我们很少依赖单一模型调用。相反，我们采用各种专门的模型、提示和启发式方法来提高准确性和响应时间，从而实现比任何单一模型更好的性能。
- en: '**Conclusion**'
  id: totrans-split-51
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**结论**'
- en: Having used fine-tuned LLMs in production from the early days of GPT-3-davinci
    being publicly available, we were extremely excited to get to try out fine-tuning
    GPT-4\. On the one hand, we were blown away by the performance gains over the
    previously-best GPT-3.5-FT.
  id: totrans-split-52
  prefs: []
  type: TYPE_NORMAL
  zh: 从GPT-3-davinci公开可用的早期，我们就开始在生产中使用微调的LLMs，因此我们非常兴奋地试用了微调GPT-4。一方面，我们对其在性能上超越之前最好的GPT-3.5-FT感到惊讶。
- en: On the other hand, we actually expected this, as the gap between the base versions
    of GPT-3.5 and GPT-4 was already immense.
  id: totrans-split-53
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，我们确实预期到这一点，因为GPT-3.5和GPT-4基础版本之间的差距已经是巨大的。
- en: Fine-tuned GPT-4 significantly outperforms both fine-tuned GPT-3.5 and vanilla
    GPT-4 on our test set. We saw comparable improvements to those that we saw when
    first switching from GPT-3-FT to GPT-3.5-FT last year.
  id: totrans-split-54
  prefs: []
  type: TYPE_NORMAL
  zh: 经过微调的GPT-4在我们的测试集上明显优于经过微调的GPT-3.5和原始GPT-4。我们看到了与去年从GPT-3-FT转换到GPT-3.5-FT时相似的改进。
- en: The main issue at the moment with the largest models is higher latency. As our
    goal is to make human interactions with data seamless and effortless, we can't
    have users waiting behind an AI for more than a few seconds.
  id: totrans-split-55
  prefs: []
  type: TYPE_NORMAL
  zh: 目前最大模型面临的主要问题是较高的延迟。由于我们的目标是使人与数据的交互变得流畅和轻松，我们不能让用户在人工智能后面等待超过几秒钟。
- en: Because of the higher latency (and cost), we only use GPT-4-FT for a certain
    subset of questions and for some of the most critical reasoning steps. For the
    rest, we use various other models, including GPT-3.5-FT.
  id: totrans-split-56
  prefs: []
  type: TYPE_NORMAL
  zh: 由于较高的延迟（和成本），我们仅在某些问题的特定子集以及某些最关键的推理步骤中使用GPT-4-FT。对于其余部分，我们使用包括GPT-3.5-FT在内的各种其他模型。
- en: 'With current state-of-the-art models, we believe that:'
  id: totrans-split-57
  prefs: []
  type: TYPE_NORMAL
  zh: 使用当前最先进的模型，我们认为：
- en: A single model with a single completion (or API call if using hosted LLMs) is
    not sufficient for many real-world applications that require non-trivial levels
    of reasoning capabilities
  id: totrans-split-58
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一个单一模型及其单一完成（或者使用托管的LLMs进行API调用）对于许多需要非平凡推理能力的真实应用程序是不够的。
- en: In a work context, it’s critical for any AI to accurately explain its work to
    users (people don’t even believe other humans before seeing proof!)
  id: totrans-split-59
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在工作环境中，任何AI都需要准确地向用户解释其工作（人们在看到证据之前甚至不相信其他人！）
- en: Read more about our [natural language question-answering AI](https://www.supersimple.io/platform/natural-language-ai)
    on our website, or get started with our next-generation business intelligence
    platform today.
  id: totrans-split-60
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的网站上阅读更多关于我们的[自然语言问答AI](https://www.supersimple.io/platform/natural-language-ai)，或者立即开始使用我们的下一代商业智能平台。
