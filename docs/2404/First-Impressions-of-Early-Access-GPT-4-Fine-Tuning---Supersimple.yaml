- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-05-27 13:20:06'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: First Impressions of Early-Access GPT-4 Fine-Tuning | Supersimple
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://www.supersimple.io/blog/gpt-4-fine-tuning-early-access](https://www.supersimple.io/blog/gpt-4-fine-tuning-early-access)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: A few weeks ago we finally got access to the GPT-4 fine-tuning API (in limited
    early access), and were super excited to check out how well it works. We’d been
    a user of OpenAI’s fine-tuned models since fine-tuning the original GPT-3 davinci
    model first became available.
  prefs: []
  type: TYPE_NORMAL
- en: 'Unsurprisingly, a fine-tuned GPT-4 greatly outperforms fine-tuned GPT-3.5 (more
    than 50% improvement for our use case!). We’ll go deeper into how we use these
    models below, but let’s start by comparing several of the OpenAI-based models
    we use at Supersimple:'
  prefs: []
  type: TYPE_NORMAL
- en: Fine tuned (FT) GPT-3 Davinci model, which we used at the beginning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: GPT-3.5 and GPT-4 base models
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: GPT-3.5 and GPT-4, fine-tuned models using a custom proprietary data set
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Relative model performance based on our test set; FT signifies a fine-tuned
    model
  prefs: []
  type: TYPE_NORMAL
- en: The models in question were fine-tuned for a domain-specific use case of asking
    data questions in natural language. The LLMs are tasked with putting together
    the appropriate reports and underlying queries for the user’s question.
  prefs: []
  type: TYPE_NORMAL
- en: These evaluations were performed using a proprietary test data set we use at
    Supersimple; Davinci’s (GPT-3) performance is taken as a baseline. To give more
    insight into where we see these models working well, let’s give a bit more context
    on what we actually do. We’ll also include more comparative data, including cost
    and latency, below!
  prefs: []
  type: TYPE_NORMAL
- en: Context & our LLM usage at Supersimple
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Supersimple is a data analytics platform that allows users to dive deep into
    their data, to answer complex ad-hoc questions in seconds and to uncover deep
    data insights.
  prefs: []
  type: TYPE_NORMAL
- en: Our users can ask natural language (plain English) questions about their data,
    and get answers in the form of tables and visualizations. The AI explains its
    work using a few no-code steps, and the user can always continue to go even deeper
    – either using natural language or a few clicks on our data platform.
  prefs: []
  type: TYPE_NORMAL
- en: We use LLMs to answer our users’ natural language questions, with the goal of
    providing a great starting point for further deep dives into the data. The LLM
    takes the user's question, relevant parts of their [semantic data model](https://www.supersimple.io/blog/semantic-layers),
    existing reports and dashboards, and generates an exploration that answers the
    question with the most context possible.
  prefs: []
  type: TYPE_NORMAL
- en: ‍
  prefs: []
  type: TYPE_NORMAL
- en: This feature is also particularly useful as an onboarding tool, which showcases
    the capabilities of our platform and teaches users to use our data exploration
    UI. For long-time users, plain English is at times still the fastest way to answer
    questions, but at times it’s just as quick for them to just click a few buttons.
  prefs: []
  type: TYPE_NORMAL
- en: Implementation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Technically speaking, the LLM outputs a custom high-level markup language (DSL)
    that we designed for our specific use case, that’s token-efficient and well-aligned
    with pre-trained language models. This DSL gets compiled into JSON, that we in
    turn compile into one or more database queries to be executed. Finally, we render
    an interactive data exploration to the user based on the report structure dictated
    by the DSL.
  prefs: []
  type: TYPE_NORMAL
- en: 'It''s important to highlight that this task is quite different from the traditional
    text-to-SQL problem for a number of reasons:'
  prefs: []
  type: TYPE_NORMAL
- en: The output is a no-code exploration, which is transparent and explainable to
    the user. This means that the model directly interacts with our data platform.
    We believe that generating & showing code or SQL is a terrible way to prove/explain
    reports to users – even if those users are technical, but especially if they aren’t.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The resulting report/exploration is easily editable in the UI, and is meant
    to serve as a starting point for further deep dives. When exploring data, you
    won’t want to think about refactoring SQL queries.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The complex output is broken into individual blocks that represent logical steps
    in the reasoning process.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We offload the complexity of constructing correct SQL queries with many joins,
    CTEs and subqueries to the platform’s “query engine”, which deterministically
    compiles the semantic steps output by the model into valid and performant SQL.
    This means the LLM doesn’t need to worry about an entire class of problems, allowing
    it to perform better overall.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When generating output, the model takes into account existing dashboards and
    user-defined concepts.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Due to the complexity of the task and the need for a custom output format (a
    DSL that the AI uses to effectively construct app UI state), we found that fine-tuned
    models tend to perform significantly better than the base ones.
  prefs: []
  type: TYPE_NORMAL
- en: 'To reiterate: we never generate SQL using LLMs – our LLMs only interact with
    our app directly.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Fine-tuning and benchmarking**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: For fine-tuning, we use a proprietary dataset that contains tens of millions
    of tokens worth of question-answering examples, with combinations of data models,
    questions and the perfect output reports to answer these questions.
  prefs: []
  type: TYPE_NORMAL
- en: GPT-3.5 (gpt-3.5-turbo-0125) and GPT-4 (gpt-4-0613, the only fine-tunable GPT-4
    model available at the moment) were fine-tuned for 3 epochs.
  prefs: []
  type: TYPE_NORMAL
- en: For both of the base models here (gpt-3.5-turbo-0125 and gpt-4-0125-preview),
    we used the same prompt, containing 8-shot examples. Performance of all models
    except davinci was evaluated in the beginning of March 2024 while the performance
    of davinci was last evaluated back in August 2023.
  prefs: []
  type: TYPE_NORMAL
- en: The performance comparison is shown in the next table. For our test set, fine-tuned
    (FT) GPT-4 **outperformed GPT-3.5 by 56%**, a significant, albeit slightly smaller
    improvement than the jump from Davinci FT to GPT-3.5 FT (96%).
  prefs: []
  type: TYPE_NORMAL
- en: Fine-tuned GPT-4 is slightly slower than fine-tuned GPT-3.5 (21.6 tokens/s vs
    31 tokens/s).
  prefs: []
  type: TYPE_NORMAL
- en: '*Note: the original version of this article showed significantly worse latency
    stats for GPT-4: our first benchmarks only achieved 5.8 tokens/s, but OpenAI has
    since greatly improved the service''s stability and speed.*'
  prefs: []
  type: TYPE_NORMAL
- en: While cost is significantly higher for GPT-4 (15x higher for inference, 11x
    higher for training, compared to GPT-3.5), it might not be an important factor
    for you depending on your use case.
  prefs: []
  type: TYPE_NORMAL
- en: 'We benchmarked models on our in-house test set of 100 diverse questions. Examples
    of questions and model performance are shown in the table below:'
  prefs: []
  type: TYPE_NORMAL
- en: '| Question | Davinci FT | GPT-3.5 | GPT-4 | GPT-3.5 FT | GPT-4 FT |'
  prefs: []
  type: TYPE_TB
- en: '| User with id 342122 | ✅ | ✅ | ✅ | ✅ | ✅ |'
  prefs: []
  type: TYPE_TB
- en: '| Companies that are using the analytics dashboard? | ✅ | ❌ | ✅ | ✅ | ✅ |'
  prefs: []
  type: TYPE_TB
- en: '| What''s our ARR and how is it changing over time? | ❌ | ❌ | ❌ | ✅ | ✅ |'
  prefs: []
  type: TYPE_TB
- en: '| Companies whose subscriptions are expiring this month | ❌ | ❌ | ❌ | ✅ | ✅
    |'
  prefs: []
  type: TYPE_TB
- en: '| QoQ change in feature usage on account level | ❌ | ❌ | ❌ | ❌ | ✅ |'
  prefs: []
  type: TYPE_TB
- en: '| Churn trends this year vs last year | ❌ | ❌ | ❌ | ❌ | ✅ |'
  prefs: []
  type: TYPE_TB
- en: '| What are the main blockers in our onboarding funnel? | ❌ | ❌ | ❌ | ❌ | ❌
    |'
  prefs: []
  type: TYPE_TB
- en: ‍
  prefs: []
  type: TYPE_NORMAL
- en: Despite the performance enhancements, models continue to struggle with broad
    and open-ended queries when trying to answer with a single completion.
  prefs: []
  type: TYPE_NORMAL
- en: Worryingly, there is an observable trend of diminishing returns from fine-tuning.
    While fine-tuned Davinci showed marked improvement over its base model, fine-tuned
    GPT-3.5 offered lesser gains, and the progress achieved by fine-tuning GPT-4 was
    even smaller.
  prefs: []
  type: TYPE_NORMAL
- en: To address these limitations, in production, we seldom rely on a single model
    call. Instead, we employ a mix of various specialized models, prompts, and heuristics
    to improve both accuracy and response time, achieving better performance than
    any individual model.
  prefs: []
  type: TYPE_NORMAL
- en: '**Conclusion**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Having used fine-tuned LLMs in production from the early days of GPT-3-davinci
    being publicly available, we were extremely excited to get to try out fine-tuning
    GPT-4\. On the one hand, we were blown away by the performance gains over the
    previously-best GPT-3.5-FT.
  prefs: []
  type: TYPE_NORMAL
- en: On the other hand, we actually expected this, as the gap between the base versions
    of GPT-3.5 and GPT-4 was already immense.
  prefs: []
  type: TYPE_NORMAL
- en: Fine-tuned GPT-4 significantly outperforms both fine-tuned GPT-3.5 and vanilla
    GPT-4 on our test set. We saw comparable improvements to those that we saw when
    first switching from GPT-3-FT to GPT-3.5-FT last year.
  prefs: []
  type: TYPE_NORMAL
- en: The main issue at the moment with the largest models is higher latency. As our
    goal is to make human interactions with data seamless and effortless, we can't
    have users waiting behind an AI for more than a few seconds.
  prefs: []
  type: TYPE_NORMAL
- en: Because of the higher latency (and cost), we only use GPT-4-FT for a certain
    subset of questions and for some of the most critical reasoning steps. For the
    rest, we use various other models, including GPT-3.5-FT.
  prefs: []
  type: TYPE_NORMAL
- en: 'With current state-of-the-art models, we believe that:'
  prefs: []
  type: TYPE_NORMAL
- en: A single model with a single completion (or API call if using hosted LLMs) is
    not sufficient for many real-world applications that require non-trivial levels
    of reasoning capabilities
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In a work context, it’s critical for any AI to accurately explain its work to
    users (people don’t even believe other humans before seeing proof!)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Read more about our [natural language question-answering AI](https://www.supersimple.io/platform/natural-language-ai)
    on our website, or get started with our next-generation business intelligence
    platform today.
  prefs: []
  type: TYPE_NORMAL
