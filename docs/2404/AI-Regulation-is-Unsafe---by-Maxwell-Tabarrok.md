<!--yml

类别：未分类

日期：2024年05月27日 13:25:11

-->

# AI监管不安全 - 马克西姆·塔巴洛克

> 来源：[https://www.maximum-progress.com/p/ai-regulation-is-unsafe](https://www.maximum-progress.com/p/ai-regulation-is-unsafe)

AI安全问题和呼吁政府对技术进行管控之间存在高度相关性，但它们本不应如此。

AI风险主要有两种形式：滥用和错位。滥用风险源于人们以危险的方式使用AI作为工具。错位风险是指如果AI以损害人类利益为代价采取自己的行动。

政府不适合管理任何一种风险。滥用监管就像监管其他任何技术一样。政府可能制定合理规则，但[遗漏偏倚](https://en.wikipedia.org/wiki/Omission_bias)和以保护小而组织有序的群体为代价而产生大量代价高昂的规则也会发生。错位规管在任何政府的Overton窗口之外。政府没有强烈的动机去关心长期、全球的成本或收益，而他们*确实*有强烈的动机推动AI的发展以符合他们自己的目的。

注意到AI公司将世界置于风险之中并不足以支持政府更多地介入这一技术。政府介入可能会加剧AI最危险的部分，同时限制上行空间。

政府并非社会福利最大化者。政府的行动是成千上万个个人福利最大化者行动的综合，这些人宽松地对齐并受到限制。总的来说，政府对[近视](https://www.maximum-progress.com/p/spatial-vs-temporal-externalities)、与其他政府的激烈竞争，以及对小而组织有序的群体的负和转移等方面都有强烈的动机。这加剧了存在风险并限制了潜在的上行空间。

存在风险的绝大部分成本发生在任何单一政府的边界之外，并且超出了任何现任决策者的任期，因此我们应该预计政府会忽视它们。

我们看到政府对其他长期或全球外部性问题（如债务和气候变化）的反应符合这种预期。世界各国政府乐于对未来几代人造成数万亿美元的[直接成本和重大违约风险](https://en.wikipedia.org//wiki/National_debt_of_the_United_States#Risks_and_debates)，因为这些未来代人的成本和利益在下次选举中影响甚微。同样，政府[花费数十亿资助化石燃料](https://ourworldindata.org/grapher/fossil-fuel-subsidies?country=Northern+America+%28UN%29~CHN~Europe+%28UN%29)生产，并忽视全球变暖的潜在解决方案，如碳税或地球工程，因为[气候变化的长期或领域外成本和利益未进入其优化函数](https://www.maximum-progress.com/p/spatial-vs-temporal-externalities)。

AI风险并无二致。政府乐意以国家的短期利益来换取全球的长期风险。它们最显著的做法是通过军事竞争。政府对私人AI开发的管制不会阻止它们竞相将AI整合到军事中。[自主无人机战争](https://www.forbes.com/sites/davidhambling/2023/10/17/ukraines-ai-drones-seek-and-attack-russian-forces-without-human-oversight/?sh=70fde60b66da)已在乌克兰和以色列发生。美国军方与[Palantir](https://en.wikipedia.org/wiki/Palantir_Technologies#Products)和[Andruil](https://en.wikipedia.org/wiki/Anduril_Industries#Products)签有合同，利用AI增强军事战略或驱动武器系统。政府将希望利用AI进行[预测性警务、宣传](https://www.rfa.org/english/news/china/surveillance-06052023142155.html)和其他形式的人口控制。

核技术的案例具有启发性。尽管政府严格管制了这项技术，但它们仍互相竞逐并利用技术制造人类有史以来最[存在危险的武器](https://en.wikipedia.org/wiki/Thermonuclear_weapon)。同时，它们打压了民用应用。现在，我们生活在至少一方拥有核武器的主要地缘政治热点且核能产业比停滞还糟的世界中。

政府的军事野心意味着它们的管制将保留AI最危险的误用风险。它们还将推动AI前沿，并训练更大的模型，因此我们仍将面临错位风险。如果政府在AI安全技术上不那么感兴趣或熟练，这些风险可能会加剧。政府对AI开发的控制可能会总体上减缓AI的进步。但接受这一前提并不足以邀请管制，因为政府的控制将导致最反乌托邦使用AI的*相对*加速。

短期内，政府主要关注保护那些组织良好的团体免受人工智能影响的影响。例如，[版权持有者](https://www.axios.com/2024/01/02/copyright-law-violation-artificial-intelligence-courts)，[驾驶员联盟](https://www2.gov.bc.ca/gov/content/transportation/driving-and-cycling/road-safety-rules-and-consequences/self-drive)，和[其他专业](https://www.bmj.com/company/newsroom/doctors-and-public-health-experts-join-calls-for-halt-to-ai-rd-until-its-regulated/)游说团体。以下是去年国会人工智能听证会的摘要，来自兹维·莫沃绍维茨。

> [参议员们](https://www.lesswrong.com/posts/5nDxmAvZ9w5CPa9gR/ai-12-the-quest-for-sane-regulations#Deepfaketown_and_Botpocalypse_Soon)对政客们关心的类型非常关注。克洛布彻问到如何保护本地新闻媒体的版税。布莱克本问到如何保护加思·布鲁克斯的版税。对版权侵犯，尤其是在音频模型中未经适当许可使用数据进行训练的担忧很大。格雷厄姆专注于第230条，尽管多次提醒它不适用，豪利也稍作提及。

这种类型的监管比不对齐的杀戮机器人风险小，但它确实限制了技术的潜在上行空间。

私人发展AI的激励远非完美。仍然存在大量外部性和竞争动态，可能会推动进展过快。但是，仅仅识别这个问题并不足以证明政府介入的必要性。我们需要有理由相信，政府可以可靠地改善私人组织面临的激励。政府对短视、军事竞争和寻租的强大激励使得找到这样的理由变得困难。

政府和追求利润的公司的默认激励是不完善的。但人工智能安全倡导的整点就在于改变这些激励，或者说说服决策者在这些激励下行动，所以你可以认为政府是不完善的，但仍然支持呼吁AI监管。这样做的问题在于，即使在政府中非常成功的倡导也可能被转向相反而灾难性的影响。

考虑一下[山姆·阿尔特曼在去年五月的国会证词](https://www.youtube.com/watch?v=fP5YdyjTfG0)。没有人被任何事情说服，只有对AI恐惧在他们自己的宠物项目中的力量。这里是一个典型的引用：

> 参议员布卢门瑟尔对山姆·阿尔特曼说：我认为你确实说过，我要引用一下，“超人类机器智能的发展可能是持续存在的最大威胁。”你可能考虑到了对工作的影响。这确实是我长期以来最大的噩梦。

AI安全游说的潜力的合理上限可以看作是20世纪70年代的环保运动。它是非常有效的。他们的倡导导致了一系列法律的出台，包括国家环境政策法（NEPA），这些法律是有史以来最全面和最有力的法规之一。这些法律并不明确地服务于某些现有的政府激励。事实上，它们比任何其他东西更严格地监管联邦政府，并经常[阻碍其行动](https://en.wikipedia.org/wiki/Tennessee_Valley_Authority_v._Hill#Majority_opinion)。环保运动的文化和政治倡导对法律产生了重大的反事实影响，这些法律至今仍具有巨大的影响力。

尽管取得了这样的成功，但这已变得苦涩，因为这些法律的巨大影响现在成为减少碳排放的巨大障碍。NEPA对石油和天然气有豁免条款，但对太阳能或风力发电场没有。对高速公路有豁免条款，但对高速铁路没有。遵循NEPA官僚程序主义的成本使[Terraform Industries](https://terraformindustries.com/)受到的损害比Shell还大。标准的政府激励措施使利益集中于大型可辨识群体，将成本扩散到更大的群体和未来，政治意愿和环保运动的机构力量被重定向到一些环境破坏最严重、经济成本最高的法律之中。

AI安全倡导者不应期望做得比这更好，特别是因为他们的许多建议是基于[像NEPA许可建设项目一样允许AI模型](https://twitter.com/AdamThierer/status/1772987264290709987)。

相信AI可能存在的灭绝风险并不意味着政府应该对其发展具有更大的影响力。政府的激励使其与减少灭绝风险的目标不一致。他们不会因其边界外或任期限制之外的成本或收益而受到奖励或惩罚，而这几乎是灭绝风险重要性的全部所在。政府对于快速发展能增强他们在竞争对手身上的权力的军事技术感到满足。他们还会因为为组织有序、可辨识的群体提供即时好处而受到奖励，即使这些奖励会对更大或更遥远的人群造成巨大损失。这些激励加剧了AI的最严重误用和不一致风险，并限制了潜在的经济上行空间。
