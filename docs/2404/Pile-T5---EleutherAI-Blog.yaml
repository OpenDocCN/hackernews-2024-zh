- en: <!--yml
  id: totrans-split-0
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  id: totrans-split-1
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-05-27 13:13:01'
  id: totrans-split-2
  prefs: []
  type: TYPE_NORMAL
- en: -->
  id: totrans-split-3
  prefs: []
  type: TYPE_NORMAL
- en: Pile-T5 | EleutherAI Blog
  id: totrans-split-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://blog.eleuther.ai/pile-t5/](https://blog.eleuther.ai/pile-t5/)
  id: totrans-split-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'The T5 model (Raffel et al, 2019) is widely used in the NLP community. Its
    base model has been downloaded from Hugging Face millions of times, leaving no
    doubt that these models are a favorite of the community. However, T5''s tokenizer
    omits important code-related tokens and subsequent pretraining datasets have been
    released with higher quality filtering and more diverse domains. In this blog
    post, we introduce a new version of T5 intended to address those weaknesses: **Pile-T5**,
    trained on the Pile (Gao et al, 2020) and using the LLaMA tokenizer (Touvron et
    al, 2023).'
  id: totrans-split-6
  prefs: []
  type: TYPE_NORMAL
- en: Model Description[#](#model-description)
  id: totrans-split-7
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Our alternative version replaces the pretraining dataset with the Pile and switches
    the original T5 tokenizer for the LLaMA tokenizer. Pile-T5 was trained to 2 million
    steps or 2 trillion tokens in total - twice what the original T5 model was trained
    for. We train with the original span corruption method and observe improvements
    for finetuning on downstream tasks applicable to users. We find that our models
    substantially outperform the most widely used T5 models (called T5-v1.1) even
    in token-matched settings. In particular, Pile-T5 performs much better on code
    tasks. Our released models were trained on the same hyperparameters as the original
    T5, utilizing [T5x](https://github.com/google-research/t5x). We release our experiment
    scripts [here](https://github.com/EleutherAI/improved-t5).
  id: totrans-split-8
  prefs: []
  type: TYPE_NORMAL
- en: These models are accessible from EleutherAI's [Hugging Face page](https://huggingface.co/collections/EleutherAI/pile-t5-65a76a0d0022dd270b385a66).
    A notable difference from the original T5 is the use of the transformer implementation
    from [umT5](https://huggingface.co/docs/transformers/model_doc/umt5) (Chung, Constant,
    Garcia et al, 2023) due to the use of the scalable implementation in T5x. Inspired
    by Pythia (Biderman and Schoelkopf et al 2023), we release [intermediate checkpoints](https://huggingface.co/collections/EleutherAI/pile-t5-65a76a0d0022dd270b385a66)
    that span every 10,000 steps with the goal of empowering researchers who wish
    to study the evolution of our models over time. The `main` branch for these models
    in their respective Hugging Face page contains the 2 million step version, and
    the the partially trained checkpoints can be found in the other branches. In addition,
    we release the T5x versions of the checkpoints [here](https://huggingface.co/collections/EleutherAI/pile-t5-t5x-checkpoints-660aaab3e8c24412c5f69a6a).
  id: totrans-split-9
  prefs: []
  type: TYPE_NORMAL
- en: Going Beyond 1 Trillion Tokens[#](#going-beyond-1-trillion-tokens)
  id: totrans-split-10
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The Pile-T5 models were evaluated on SuperGLUE, CodeXGLUE, as well as MMLU and
    Bigbench Hard. The Pile-T5 models were compared with [T5v1.1](https://github.com/google-research/text-to-text-transfer-transformer/blob/main/released_checkpoints.md)
    where both were finetuned over the same amount of tokens. We also compare Pile-T5
    models against the Flan-T5 models for MMLU and BBH as a loose comparison. All
    evaluations were done with the [LM-Evaluation Harness](https://github.com/EleutherAI/lm-evaluation-harness)
    (Gao et al, 2023) to report model performance over the benchmarks presented here.
    We release the finetuned checkpoints for [Base](https://huggingface.co/collections/lintang/pile-t5-base-finetuned-models-65d307353ecda975d828ebb8),
    [Large](https://huggingface.co/collections/lintang/pile-t5-large-finetuned-models-65d307ec0545ab7c1153b804),
    [XL](https://huggingface.co/collections/lintang/pile-t5-xl-finetuned-models-65d30bbcf5a15aa421099b4e),
    and [XXL](https://huggingface.co/collections/lintang/pile-t5-xxl-finetuned-models-65a76bbc4908b2676c6b8a94).
  id: totrans-split-11
  prefs: []
  type: TYPE_NORMAL
  zh: Pile-T5 模型在 SuperGLUE、CodeXGLUE、以及 MMLU 和 Bigbench Hard 上进行了评估。Pile-T5 模型与 [T5v1.1](https://github.com/google-research/text-to-text-transfer-transformer/blob/main/released_checkpoints.md)
    进行了比较，两者均在相同数量的 tokens 上进行了微调。我们还将 Pile-T5 模型与 Flan-T5 模型在 MMLU 和 BBH 上进行了宽泛比较。所有评估都是使用
    [LM-Evaluation Harness](https://github.com/EleutherAI/lm-evaluation-harness)（Gao
    et al, 2023）来报告模型在这里提出的基准测试中的性能。我们发布了 [Base](https://huggingface.co/collections/lintang/pile-t5-base-finetuned-models-65d307353ecda975d828ebb8)、[Large](https://huggingface.co/collections/lintang/pile-t5-large-finetuned-models-65d307ec0545ab7c1153b804)、[XL](https://huggingface.co/collections/lintang/pile-t5-xl-finetuned-models-65d30bbcf5a15aa421099b4e)
    和 [XXL](https://huggingface.co/collections/lintang/pile-t5-xxl-finetuned-models-65a76bbc4908b2676c6b8a94)
    的微调检查点。
- en: Performance on SuperGLUE[#](#performance-on-superglue)
  id: totrans-split-12
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 在 SuperGLUE 上的表现[#](#performance-on-superglue)
- en: To assess performance on SuperGLUE, we finetune the Pile-T5 (both the 1 trillion
    token version and the final 2 trillion token version) and T5v1.1 models with a
    batch size of 128 for 263K steps, matching the original T5 paper. With all models
    except for Large, we observe a substantial performance increase. Note that Pile-T5
    (1T) already outperforms T5-v1.1 and that performance is further increased by
    further training.
  id: totrans-split-13
  prefs: []
  type: TYPE_NORMAL
  zh: 为了评估在 SuperGLUE 上的性能，我们使用批量大小为 128 对 Pile-T5（包括万亿令牌版本和最终的两万亿令牌版本）和 T5v1.1 模型进行了263K步的微调，与原始
    T5 论文保持一致。除了 Large 模型外，我们观察到所有模型都有显著的性能提升。请注意，Pile-T5 (1T) 已经优于 T5-v1.1，并且通过进一步的训练进一步提高了性能。
- en: '| Size | Variant | Average | boolq | cb |  | copa | multirc |  | record |  |
    rte | wic | wsc |'
  id: totrans-split-14
  prefs: []
  type: TYPE_TB
  zh: '| 大小 | 变体 | 平均 | boolq | cb |  | copa | multirc |  | record |  | rte | wic
    | wsc |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | ---
    | --- |'
  id: totrans-split-15
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | ---
    | --- |'
- en: '|  |  |  | acc | f1 | acc | acc | f1 | em | em | f1 | acc | acc | acc |'
  id: totrans-split-16
  prefs: []
  type: TYPE_TB
  zh: '|  |  |  | 准确率 | F1 分数 | 准确率 | 准确率 | F1 分数 | 精确匹配 | 精确匹配 | F1 分数 | 准确率 | 准确率
    | 准确率 |'
- en: '| Base | T5-v1.1 | 71.33 | 79.36 | 83.63 | 87.5 | 63 | 73.45 | 33.26 | 69.7
    | 68.75 | 78.34 | 65.83 | **75.96** |'
  id: totrans-split-17
  prefs: []
  type: TYPE_TB
  zh: '| Base | T5-v1.1 | 71.33 | 79.36 | 83.63 | 87.5 | 63 | 73.45 | 33.26 | 69.7
    | 68.75 | 78.34 | 65.83 | **75.96** |'
- en: '|  | Pile-T5 (1T) | 74.85 | 81.46 | 93.69 | **94.64** | 65 | **77.75** | **40.50**
    | 76.97 | **76.49** | 80.86 | **67.39** | 74.03 |'
  id: totrans-split-18
  prefs: []
  type: TYPE_TB
  zh: '|  | Pile-T5 (1T) | 74.85 | 81.46 | 93.69 | **94.64** | 65 | **77.75** | **40.50**
    | 76.97 | **76.49** | 80.86 | **67.39** | 74.03 |'
- en: '|  | **Pile-T5** | **76.13** | **82.45** | **96.07** | **94.64** | **72** |
    **77.74** | 39.56 | **77.64** | **76.88** | **83.03** | **67.24** | 73.08 |'
  id: totrans-split-19
  prefs: []
  type: TYPE_TB
  zh: '|  | **Pile-T5** | **76.13** | **82.45** | **96.07** | **94.64** | **72** |
    **77.74** | 39.56 | **77.64** | **76.88** | **83.03** | **67.24** | 73.08 |'
- en: '| Large | **T5-v1.1** | **81.11** | **85.96** | **93.21** | **96.43** | **82**
    | 81.71 | 48.37 | 82.18 | 81.71 | **85.92** | **71.47** | **81.73** |'
  id: totrans-split-20
  prefs: []
  type: TYPE_TB
  zh: '| Large | **T5-v1.1** | **81.11** | **85.96** | **93.21** | **96.43** | **82**
    | 81.71 | 48.37 | 82.18 | 81.71 | **85.92** | **71.47** | **81.73** |'
- en: '|  | Pile-T5 (1T) | 79.18 | 83.70 | 91.85 | 94.64 | 79 | 82.36 | 47.85 | 82.72
    | 82.14 | 83.03 | 65.2 | **81.73** |'
  id: totrans-split-21
  prefs: []
  type: TYPE_TB
  zh: '|  | Pile-T5 (1T) | 79.18 | 83.70 | 91.85 | 94.64 | 79 | 82.36 | 47.85 | 82.72
    | 82.14 | 83.03 | 65.2 | **81.73** |'
- en: '|  | Pile-T5 | 79.67 | 85.71 | 88.96 | 94.64 | 74 | **82.60** | **50.47** |
    **84.1** | **83.70** | 85.19 | 68.49 | **81.73** |'
  id: totrans-split-22
  prefs: []
  type: TYPE_TB
  zh: '|  | Pile-T5 | 79.67 | 85.71 | 88.96 | 94.64 | 74 | **82.60** | **50.47** |
    **84.1** | **83.70** | 85.19 | 68.49 | **81.73** |'
- en: '| XL | T5-v1.1 | 81.76 | 86.79 | 81.18 | 91.07 | 84 | 84.03 | 52.89 | 83.92
    | 83.5 | 90.25 | 73.04 | 81.73 |'
  id: totrans-split-23
  prefs: []
  type: TYPE_TB
  zh: '| XL | T5-v1.1 | 81.76 | 86.79 | 81.18 | 91.07 | 84 | 84.03 | 52.89 | 83.92
    | 83.5 | 90.25 | 73.04 | 81.73 |'
- en: '|  | Pile-T5 (1T) | 86.09 | 89.76 | 90.6 | 94.64 | **96** | 88.17 | 63.90 |
    91.58 | 91.36 | **93.50** | 72.73 | 86.54 |'
  id: totrans-split-24
  prefs: []
  type: TYPE_TB
  zh: '|  | Pile-T5 (1T) | 86.09 | 89.76 | 90.6 | 94.64 | **96** | 88.17 | 63.90 |
    91.58 | 91.36 | **93.50** | 72.73 | 86.54 |'
- en: '|  | **Pile-T5** | **89.00** | **90.4** | **93.1** | **96.43** | **96** | **88.63**
    | **65.16** | **92.21** | **91.96** | 92.78 | **75.24** | **96.15** |'
  id: totrans-split-25
  prefs: []
  type: TYPE_TB
  zh: '|  | **Pile-T5** | **89.00** | **90.4** | **93.1** | **96.43** | **96** | **88.63**
    | **65.16** | **92.21** | **91.96** | 92.78 | **75.24** | **96.15** |'
- en: '| XXL | T5-v1.1 | 82.43 | 88.29 | 93.61 | 94.64 | 86 | 75.22 | 51.00 | 84.67
    | 84.55 | 89.17 | 72.41 | 81.73 |'
  id: totrans-split-26
  prefs: []
  type: TYPE_TB
  zh: '| XXL | T5-v1.1 | 82.43 | 88.29 | 93.61 | 94.64 | 86 | 75.22 | 51.00 | 84.67
    | 84.55 | 89.17 | 72.41 | 81.73 |'
- en: '|  | Pile-T5 (1T) | 87.11 | 90.46 | 94.3 | 96.43 | 93 | 80.81 | 56.77 | 91.36
    | 91.18 | 92.42 | 70.38 | 95.19 |'
  id: totrans-split-27
  prefs: []
  type: TYPE_TB
  zh: '|  | Pile-T5 (1T) | 87.11 | 90.46 | 94.3 | 96.43 | 93 | 80.81 | 56.77 | 91.36
    | 91.18 | 92.42 | 70.38 | 95.19 |'
- en: '|  | **Pile-T5** | **90.08** | **90.98** | **98.68** | **98.21** | **95** |
    **89.28** | **67.68** | **93.04** | **92.7** | **93.5** | **75.24** | **96.15**
    |'
  id: totrans-split-28
  prefs: []
  type: TYPE_TB
  zh: '|  | **Pile-T5** | **90.08** | **90.98** | **98.68** | **98.21** | **95** |
    **89.28** | **67.68** | **93.04** | **92.7** | **93.5** | **75.24** | **96.15**
    |'
- en: Performance on CodeXGLUE[#](#performance-on-codexglue)
  id: totrans-split-29
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: CodeXGLUE性能[#](#performance-on-codexglue)
- en: As one of our major goals is to improve the ability of the models to understand
    code, we ran evaluations on the Code-to-Text subtask of CodeXGLUE (Su et al, 2021).
    All models were finetuned on each programming language variant for 10 epochs with
    the same method as the [original repository](https://github.com/microsoft/CodeXGLUE/tree/main/Code-Text/code-to-text).
  id: totrans-split-30
  prefs: []
  type: TYPE_NORMAL
  zh: 作为我们的主要目标之一是提高模型理解代码的能力，我们在CodeXGLUE的Code-to-Text子任务上进行了评估（Su等人，2021）。所有模型在每种编程语言变体上进行了10个时期的微调，使用与[原始存储库](https://github.com/microsoft/CodeXGLUE/tree/main/Code-Text/code-to-text)相同的方法。
- en: '| Size | Version | Average | Python | PHP | Go | Java | JavaScript | Ruby |'
  id: totrans-split-31
  prefs: []
  type: TYPE_TB
  zh: '| Size | Version | Average | Python | PHP | Go | Java | JavaScript | Ruby |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-split-32
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- | --- |'
- en: '| Base | T5-v1.1 | 14.34 | 15.55 | 21.72 | 14.71 | 14.89 | 9.25 | 9.90 |'
  id: totrans-split-33
  prefs: []
  type: TYPE_TB
  zh: '| Base | T5-v1.1 | 14.34 | 15.55 | 21.72 | 14.71 | 14.89 | 9.25 | 9.90 |'
- en: '|  | Pile-T5 (1T) | 15.90 | 17.20 | 22.90 | **16.75** | 16.24 | 11.23 | 11.10
    |'
  id: totrans-split-34
  prefs: []
  type: TYPE_TB
  zh: '|  | Pile-T5 (1T) | 15.90 | 17.20 | 22.90 | **16.75** | 16.24 | 11.23 | 11.10
    |'
- en: '|  | **Pile-T5** | **16.37** | **17.78** | **23.12** | **16.70** | **16.68**
    | **11.89** | **12.06** |'
  id: totrans-split-35
  prefs: []
  type: TYPE_TB
  zh: '|  | **Pile-T5** | **16.37** | **17.78** | **23.12** | **16.70** | **16.68**
    | **11.89** | **12.06** |'
- en: '| Large | T5-v1.1 | 11.53 | 12.18 | 14.17 | 12.37 | 12.30 | 8.85 | 9.32 |'
  id: totrans-split-36
  prefs: []
  type: TYPE_TB
  zh: '| Large | T5-v1.1 | 11.53 | 12.18 | 14.17 | 12.37 | 12.30 | 8.85 | 9.32 |'
- en: '|  | Pile-T5 (1T) | 15.74 | 17.09 | 22.80 | **17.16** | 16.33 | 10.75 | 10.31
    |'
  id: totrans-split-37
  prefs: []
  type: TYPE_TB
  zh: '|  | Pile-T5 (1T) | 15.74 | 17.09 | 22.80 | **17.16** | 16.33 | 10.75 | 10.31
    |'
- en: '|  | **Pile-T5** | **16.28** | **17.72** | **22.95** | 17.07 | **16.41** |
    **12.05** | **11.45** |'
  id: totrans-split-38
  prefs: []
  type: TYPE_TB
  zh: '|  | **Pile-T5** | **16.28** | **17.72** | **22.95** | 17.07 | **16.41** |
    **12.05** | **11.45** |'
- en: '| XL | T5-v1.1 | 16.17 | 17.36 | 21.91 | 16.69 | 17.74 | 11.08 | 12.25 |'
  id: totrans-split-39
  prefs: []
  type: TYPE_TB
  zh: '| XL | T5-v1.1 | 16.17 | 17.36 | 21.91 | 16.69 | 17.74 | 11.08 | 12.25 |'
- en: '|  | Pile-T5 (1T) | 18.01 | 18.61 | 23.75 | 19.04 | 18.43 | 14.27 | 13.93 |'
  id: totrans-split-40
  prefs: []
  type: TYPE_TB
  zh: '|  | Pile-T5 (1T) | 18.01 | 18.61 | 23.75 | 19.04 | 18.43 | 14.27 | 13.93 |'
- en: '|  | **Pile-T5** | **18.68** | **19.25** | **24.37** | **19.42** | **19.15**
    | **15.1** | **14.81** |'
  id: totrans-split-41
  prefs: []
  type: TYPE_TB
  zh: '|  | **Pile-T5** | **18.68** | **19.25** | **24.37** | **19.42** | **19.15**
    | **15.1** | **14.81** |'
- en: '| XXL | T5-v1.1 | 17.67 | 17.89 | 23.21 | 18.54 | 19.17 | 13.85 | 13.33 |'
  id: totrans-split-42
  prefs: []
  type: TYPE_TB
  zh: '| XXL | T5-v1.1 | 17.67 | 17.89 | 23.21 | 18.54 | 19.17 | 13.85 | 13.33 |'
- en: '|  | Pile-T5 (1T) | 18.55 | 19.53 | 24.11 | 19.27 | 18.52 | **15.11** | 14.75
    |'
  id: totrans-split-43
  prefs: []
  type: TYPE_TB
  zh: '|  | Pile-T5 (1T) | 18.55 | 19.53 | 24.11 | 19.27 | 18.52 | **15.11** | 14.75
    |'
- en: '|  | **Pile-T5** | **18.72** | **19.27** | **24.49** | **19.60** | **18.96**
    | **15.10** | **14.92** |'
  id: totrans-split-44
  prefs: []
  type: TYPE_TB
  zh: '|  | **Pile-T5** | **18.72** | **19.27** | **24.49** | **19.60** | **18.96**
    | **15.10** | **14.92** |'
- en: As a result of both the Pile including code-based data and the LLaMA tokenizer
    including characters frequently used in code, we observe a sharp improvement in
    performance. Note that even though Pile-T5-Large performs worse than T5-v1.1 in
    general, it substantially outperforms it on these coding benchmarks. This appears
    to be primarily driven by the very poor performance of T5-v1.1-Large, which substantially
    underperforms T5-v1.1-base! By contrast, Pile-T5-Large performs similarly to Pile-T5-base.
  id: totrans-split-45
  prefs: []
  type: TYPE_NORMAL
  zh: 由于Pile包含基于代码的数据和LLaMA标记器包含在代码中经常使用的字符，我们观察到性能显著提高。请注意，尽管Pile-T5-Large在一般情况下表现不如T5-v1.1，但在这些编程基准测试中，它的表现明显优于T5-v1.1-base！相比之下，Pile-T5-Large的表现与Pile-T5-base相似。
- en: Using Flan Instruction Tuning[#](#using-flan-instruction-tuning)
  id: totrans-split-46
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用Flan指导调优[#](#using-flan-instruction-tuning)
- en: We continue by finetuning Pile-T5 models on Flan (Chung, Hou, Longpre et all,
    2022) with the same training hyperparameters and evaluating on MMLU (Hendrycks
    et al, 2021) and BigBench Hard (Suzgun et al, 2022).
  id: totrans-split-47
  prefs: []
  type: TYPE_NORMAL
  zh: 我们继续通过在Flan上对Pile-T5模型进行微调（Chung, Hou, Longpre等人，2022），使用相同的训练超参数，并在MMLU（Hendrycks等人，2021）和BigBench
    Hard（Suzgun等人，2022）上进行评估。
- en: When compared to the Flan-T5 model, we found that Pile-T5 falls short by a small
    but meaningful amount. After following up with the authors, we learned that not
    all of the finetuning data used to produce Flan-T5 was publicly released, which
    may account for the difference in performance.
  id: totrans-split-48
  prefs: []
  type: TYPE_NORMAL
- en: For a fairer comparison, we also finetuned T5-v1.1 checkpoints with the same
    procedure and data used on the Pile-T5 models. We specifically use the 2 trillion
    token version of Pile-T5 so that the comparison with T5-v1.1 reflects both the
    increased scale of the training data and the change in data and tokenizer.
  id: totrans-split-49
  prefs: []
  type: TYPE_NORMAL
- en: Performance on Held-In[#](#performance-on-held-in)
  id: totrans-split-50
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We observe competitive performance over held-in tasks (tasks that were included
    in the Flan Instruction Tuning dataset) with a dip in performance in the Large
    variant, similar to the behaviour observed with SuperGLUE.
  id: totrans-split-51
  prefs: []
  type: TYPE_NORMAL
- en: '| Size | Version | Average | ANLI R1 | ANLI R2 | ANLI R3 | Arc Easy | Arc Challenge
    | BoolQ | RTE |'
  id: totrans-split-52
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-split-53
  prefs: []
  type: TYPE_TB
- en: '| Base | T5-v1.1 | **46.50** | **39.90** | 34.96 | 37.33 | **38.12** | 28.23
    | 70.26 | **76.73** |'
  id: totrans-split-54
  prefs: []
  type: TYPE_TB
- en: '|  | Pile-T5 | 46.37 | 39.32 | **35.28** | **37.53** | 36.61 | **30.67** |
    **71.87** | 73.28 |'
  id: totrans-split-55
  prefs: []
  type: TYPE_TB
- en: '| Large | T5-v1.1 | **54.90** | **52.46** | **39.67** | **42.53** | **50.60**
    | **39.99** | **78.56** | **80.50** |'
  id: totrans-split-56
  prefs: []
  type: TYPE_TB
- en: '|  | Pile-T5 | 36.97 | 33.00 | 33.03 | 32.98 | 29.27 | 21.21 | 56.36 | 52.95
    |'
  id: totrans-split-57
  prefs: []
  type: TYPE_TB
- en: '| XL | T5-v1.1 | 56.40 | 53.82 | 40.22 | 41.01 | 56.31 | 39.08 | 80.66 | **83.71**
    |'
  id: totrans-split-58
  prefs: []
  type: TYPE_TB
- en: '|  | Pile-T5 | **64.41** | **64.36** | **48.02** | **49.18** | **66.56** |
    **58.28** | **85.16** | 79.30 |'
  id: totrans-split-59
  prefs: []
  type: TYPE_TB
- en: '| XXL | T5-v1.1 | **69.99** | **71.63** | 55.81 | **57.41** | **75.56** | **62.30**
    | 86.53 | 80.71 |'
  id: totrans-split-60
  prefs: []
  type: TYPE_TB
- en: '|  | Pile-T5 | 69.21 | 71.16 | **55.92** | 55.19 | 70.85 | 59.82 | **87.55**
    | **83.96** |'
  id: totrans-split-61
  prefs: []
  type: TYPE_TB
- en: Performance on MMLU[#](#performance-on-mmlu)
  id: totrans-split-62
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The models are evaluated with two different versions of the prompt: the original
    prompt (Hendrycks et al, 2021) and the Flan prompt (Chung, Hou, Longpre et all,
    2022).'
  id: totrans-split-63
  prefs: []
  type: TYPE_NORMAL
- en: 'MMLU Prompt:'
  id: totrans-split-64
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  id: totrans-split-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Flan Prompt:'
  id: totrans-split-66
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  id: totrans-split-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: We observed performance gains when using Pile-T5\. For MMLU, both the highest
    log-likelihood and generative generation were used for evaluation. We observed
    that log-likelihood evaluation primarily benefitted zero-shot prompting; greedy
    generation often struggled with outputting a well-structured response, including
    generating complete answers instead of a single letter that would be rejected
    by the strict evaluator.
  id: totrans-split-68
  prefs: []
  type: TYPE_NORMAL
- en: Performance on greedy generation is improved by the use of five-shot prompting,
    which provide the models with examples of the correct response format. It should
    be noted that performance can vary significantly depending on the prompt format.
    Averaging across all variations show that Pile-T5 improves upon v1.1 and is competitive
    against Flan-T5 variants.
  id: totrans-split-69
  prefs: []
  type: TYPE_NORMAL
- en: '| Size | Variant | Average | Highest Log-likelihood |  |  |  | Greedy Generation
    |  |  |  |'
  id: totrans-split-70
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-split-71
  prefs: []
  type: TYPE_TB
- en: '|  |  |  | Original Prompt |  | Flan Prompt |  | Original Prompt |  | Flan
    Prompt |  |'
  id: totrans-split-72
  prefs: []
  type: TYPE_TB
  zh: '|  |  |  | 原始提示 |  | Flan提示 |  | 原始提示 |  | Flan提示 |  |'
- en: '|  |  |  | 0-Shot | 5-Shot | 0-Shot | 5-Shot | 0-Shot | 5-Shot | 0-Shot | 5-Shot
    |'
  id: totrans-split-73
  prefs: []
  type: TYPE_TB
  zh: '|  |  |  | 0-样本 | 5-样本 | 0-样本 | 5-样本 | 0-样本 | 5-样本 | 0-样本 | 5-样本 |'
- en: '| XL | Flan-T5 | **42.45** | **47.37** | **49.17** | **47.83** | **49.43**
    | 6.63 | **48.8** | **40.98** | **49.39** |'
  id: totrans-split-74
  prefs: []
  type: TYPE_TB
  zh: '| XL | Flan-T5 | **42.45** | **47.37** | **49.17** | **47.83** | **49.43**
    | 6.63 | **48.8** | **40.98** | **49.39** |'
- en: '|  | T5-v1.1 | 36.58 | 38.59 | 39.52 | 40.64 | 39.79 | **25.95** | 38.84 |
    29.66 | 39.67 |'
  id: totrans-split-75
  prefs: []
  type: TYPE_TB
  zh: '|  | T5-v1.1 | 36.58 | 38.59 | 39.52 | 40.64 | 39.79 | **25.95** | 38.84 |
    29.66 | 39.67 |'
- en: '|  | Pile-T5 | 40.82 | 46.04 | 48.71 | 47.13 | 48.18 | 3.61 | 48.58 | 35.53
    | 48.77 |'
  id: totrans-split-76
  prefs: []
  type: TYPE_TB
  zh: '|  | Pile-T5 | 40.82 | 46.04 | 48.71 | 47.13 | 48.18 | 3.61 | 48.58 | 35.53
    | 48.77 |'
- en: '| XXL | Flan-T5 | 46.94 | **51.47** | **54.28** | **53.31** | **53.85** | 2.69
    | **53.93** | **52.15** | **53.85** |'
  id: totrans-split-77
  prefs: []
  type: TYPE_TB
  zh: '| XXL | Flan-T5 | 46.94 | **51.47** | **54.28** | **53.31** | **53.85** | 2.69
    | **53.93** | **52.15** | **53.85** |'
- en: '|  | T5-v1.1 | 45.76 | 51.03 | 51.15 | 46.72 | 50.77 | 31.00 | 50.72 | 33.90
    | 50.78 |'
  id: totrans-split-78
  prefs: []
  type: TYPE_TB
  zh: '|  | T5-v1.1 | 45.76 | 51.03 | 51.15 | 46.72 | 50.77 | 31.00 | 50.72 | 33.90
    | 50.78 |'
- en: '|  | Pile-T5 | **48.27** | 50.88 | 53.35 | 52.22 | 53.06 | **35.8** | 53.13
    | 33.85 | **53.84** |'
  id: totrans-split-79
  prefs: []
  type: TYPE_TB
  zh: '|  | Pile-T5 | **48.27** | 50.88 | 53.35 | 52.22 | 53.06 | **35.8** | 53.13
    | 33.85 | **53.84** |'
- en: Performance on BigBench Hard (BBH)[#](#performance-on-bigbench-hard-bbh)
  id: totrans-split-80
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: BigBench Hard (BBH)的表现[#](#performance-on-bigbench-hard-bbh)
- en: Pile-T5 performs substantially better than T5v1.1 on BBH on both few-shot and
    zero-shot settings, and is competitive with Flan-T5.
  id: totrans-split-81
  prefs: []
  type: TYPE_NORMAL
  zh: Pile-T5在BBH的零样本和少样本设置上表现明显优于T5v1.1，并且与Flan-T5竞争力强。
- en: '| Size | Variant | Greedy Generation |  |'
  id: totrans-split-82
  prefs: []
  type: TYPE_TB
  zh: '| 大小 | 变体 | 贪婪生成 |  |'
- en: '| --- | --- | --- | --- |'
  id: totrans-split-83
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '|  |  | Zero-Shot | Few-Shot |'
  id: totrans-split-84
  prefs: []
  type: TYPE_TB
  zh: '|  |  | 零样本 | 少样本 |'
- en: '| XL | Flan-T5 | 24.71 | 40.36 |'
  id: totrans-split-85
  prefs: []
  type: TYPE_TB
  zh: '| XL | Flan-T5 | 24.71 | 40.36 |'
- en: '|  | T5-v1.1 | 28.67 | 33.06 |'
  id: totrans-split-86
  prefs: []
  type: TYPE_TB
  zh: '|  | T5-v1.1 | 28.67 | 33.06 |'
- en: '|  | Pile-T5 | **29.98** | **41.49** |'
  id: totrans-split-87
  prefs: []
  type: TYPE_TB
  zh: '|  | Pile-T5 | **29.98** | **41.49** |'
- en: '| XXL | Flan-T5 | **43.06** | 44.72 |'
  id: totrans-split-88
  prefs: []
  type: TYPE_TB
  zh: '| XXL | Flan-T5 | **43.06** | 44.72 |'
- en: '|  | T5-v1.1 | 35.14 | 39.84 |'
  id: totrans-split-89
  prefs: []
  type: TYPE_TB
  zh: '|  | T5-v1.1 | 35.14 | 39.84 |'
- en: '|  | Pile-T5 | 41.61 | **46.71** |'
  id: totrans-split-90
  prefs: []
  type: TYPE_TB
  zh: '|  | Pile-T5 | 41.61 | **46.71** |'
- en: Conclusion[#](#conclusion)
  id: totrans-split-91
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 结论[#](#conclusion)
- en: We observe improvements on finetuned benchmarks such as SuperGLUE, CodeXGLUE,
    MMLU and BBH. Pile-T5 outperforms T5v1.1, with the caveat that Pile-T5 finetuned
    on the Flan mixture is still outpeformed by Flan-T5\. We conclude that Pile-T5
    would be well-suited for future multitask finetuning and other tasks that benefit
    from the encoder-decoder architecture. As Pile-T5 Large underperforms in benchmarks,
    including SuperGLUE and Flan Held-In, we believe that there may have been a bug
    during training and advise caution in its use. Finally, we hope that the release
    of the intermediate checkpoints will be of benefit to the research community in
    interpretability and other endeavours.
  id: totrans-split-92
  prefs: []
  type: TYPE_NORMAL
  zh: 我们观察到在微调基准上的改进，例如SuperGLUE、CodeXGLUE、MMLU和BBH。Pile-T5在超过T5v1.1上表现出色，尽管Pile-T5在Flan混合微调上仍然被Flan-T5超越。我们得出结论，Pile-T5非常适合未来的多任务微调和其他受益于编码器-解码器架构的任务。由于Pile-T5
    Large在SuperGLUE和Flan Held-In等基准测试中表现不佳，我们认为在训练过程中可能存在错误，并建议在使用时谨慎。最后，我们希望发布中间检查点对研究社区在可解释性和其他努力方面有所帮助。
- en: Acknowledgments[#](#acknowledgments)
  id: totrans-split-93
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 致谢[#](#acknowledgments)
- en: We are grateful to Stability AI for providing the compute required to train
    these models, and to the TRC Program for providing compute to finetune some of
    the models. Thanks to Stella Biderman and @philpax for making adjustments to the
    blog post.
  id: totrans-split-94
  prefs: []
  type: TYPE_NORMAL
  zh: 我们感谢Stability AI提供的计算资源来训练这些模型，以及TRC计划提供的计算资源来微调部分模型。感谢Stella Biderman和@philpax调整博客文章。
- en: Citation[#](#citation)
  id: totrans-split-95
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 引用[#](#citation)
- en: '[PRE2]'
  id: totrans-split-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: References[#](#references)
  id: totrans-split-97
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献[#](#references)
- en: 'Biderman, Stella, Hailey Schoelkopf, Quentin Anthony, Herbie Bradley, Kyle
    O’Brien, Eric Hallahan, Mohammad Aflah Khan, et al. *Pythia: A Suite for Analyzing
    Large Language Models Across Training and Scaling*. arXiv [Cs.CL], 2023\. arXiv.
    [http://arxiv.org/abs/2304.01373](http://arxiv.org/abs/2304.01373).'
  id: totrans-split-98
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '斯特拉·比德曼, 海莉·舒尔科普夫, 昆汀·安东尼, 赫比·布拉德利, 凯尔·奥布莱恩, 埃里克·哈拉汉, 穆罕默德·阿夫拉·汗, 等人。*Pythia:
    用于分析大型语言模型在训练和扩展过程中的套件*. arXiv [Cs.CL], 2023\. arXiv. [http://arxiv.org/abs/2304.01373](http://arxiv.org/abs/2304.01373).'
- en: 'Chung, Hyung Won, Noah Constant, Xavier Garcia, Adam Roberts, Yi Tay, Sharan
    Narang, and Orhan Firat. *UniMax: Fairer and More Effective Language Sampling
    for Large-Scale Multilingual Pretraining*. arXiv [Cs.CL], 2023\. arXiv. [http://arxiv.org/abs/2304.09151](http://arxiv.org/abs/2304.09151).'
  id: totrans-split-99
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '钟贤元, 诺亚·康斯坦, 阿维尔·加西亚, 亚当·罗伯茨, 易·泰, 沙兰·纳兰, 和奥尔汉·菲拉特。*UniMax: 更公平更有效的大规模多语言预训练语言采样*.
    arXiv [Cs.CL], 2023\. arXiv. [http://arxiv.org/abs/2304.09151](http://arxiv.org/abs/2304.09151).'
- en: Chung, Hyung Won, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus,
    Yunxuan Li, et al. *Scaling Instruction-Finetuned Language Models*. arXiv [Cs.LG],
    2022\. arXiv. [http://arxiv.org/abs/2210.11416](http://arxiv.org/abs/2210.11416).
  id: totrans-split-100
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Chung, Hyung Won, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus,
    Yunxuan Li, 等人。*Scaling Instruction-Finetuned Language Models*. arXiv [Cs.LG],
    2022\. arXiv. [http://arxiv.org/abs/2210.11416](http://arxiv.org/abs/2210.11416).
- en: 'Gao, Leo, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles
    Foster, Jason Phang, et al. ‘The Pile: An 800GB Dataset of Diverse Text for Language
    Modeling’. arXiv [Cs.CL], 2020\. arXiv. [http://arxiv.org/abs/2101.00027](http://arxiv.org/abs/2101.00027).'
  id: totrans-split-101
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 'Gao, Leo, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles
    Foster, Jason Phang, 等人。‘The Pile: An 800GB Dataset of Diverse Text for Language
    Modeling’. arXiv [Cs.CL], 2020\. arXiv. [http://arxiv.org/abs/2101.00027](http://arxiv.org/abs/2101.00027).'
- en: Gao, Leo, Jonathan Tow, Baber Abbasi, Stella Biderman, Sid Black, Anthony DiPofi,
    Charles Foster, et al. ‘A Framework for Few-Shot Language Model Evaluation’. Zenodo,
    12 2023\. [https://doi.org/10.5281/zenodo.10256836](https://doi.org/10.5281/zenodo.10256836).
  id: totrans-split-102
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Gao, Leo, Jonathan Tow, Baber Abbasi, Stella Biderman, Sid Black, Anthony DiPofi,
    Charles Foster, 等人。‘A Framework for Few-Shot Language Model Evaluation’. Zenodo,
    12 2023\. [https://doi.org/10.5281/zenodo.10256836](https://doi.org/10.5281/zenodo.10256836).
- en: Hendrycks, Dan, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn
    Song, and Jacob Steinhardt. ‘Measuring Massive Multitask Language Understanding’.
    arXiv [Cs.CY], 2021\. arXiv. [http://arxiv.org/abs/2009.03300](http://arxiv.org/abs/2009.03300).
  id: totrans-split-103
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Hendrycks, Dan, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn
    Song, 以及 Jacob Steinhardt。‘Measuring Massive Multitask Language Understanding’.
    arXiv [Cs.CY], 2021\. arXiv. [http://arxiv.org/abs/2009.03300](http://arxiv.org/abs/2009.03300).
- en: 'Longpre, Shayne, Le Hou, Tu Vu, Albert Webson, Hyung Won Chung, Yi Tay, Denny
    Zhou, et al. *The Flan Collection: Designing Data and Methods for Effective Instruction
    Tuning*. arXiv [Cs.AI], 2023\. arXiv. [http://arxiv.org/abs/2301.13688](http://arxiv.org/abs/2301.13688).'
  id: totrans-split-104
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 'Longpre, Shayne, Le Hou, Tu Vu, Albert Webson, Hyung Won Chung, Yi Tay, Denny
    Zhou, 等人。*The Flan Collection: Designing Data and Methods for Effective Instruction
    Tuning*. arXiv [Cs.AI], 2023\. arXiv. [http://arxiv.org/abs/2301.13688](http://arxiv.org/abs/2301.13688).'
- en: 'Lu, Shuai, Daya Guo, Shuo Ren, Junjie Huang, Alexey Svyatkovskiy, Ambrosio
    Blanco, Colin Clement, et al. ‘CodeXGLUE: A Machine Learning Benchmark Dataset
    for Code Understanding and Generation’. arXiv [Cs.SE], 2021\. arXiv. [http://arxiv.org/abs/2102.04664](http://arxiv.org/abs/2102.04664).'
  id: totrans-split-105
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 'Lu, Shuai, Daya Guo, Shuo Ren, Junjie Huang, Alexey Svyatkovskiy, Ambrosio
    Blanco, Colin Clement, 等人。‘CodeXGLUE: A Machine Learning Benchmark Dataset for
    Code Understanding and Generation’. arXiv [Cs.SE], 2021\. arXiv. [http://arxiv.org/abs/2102.04664](http://arxiv.org/abs/2102.04664).'
- en: Suzgun, Mirac, Nathan Scales, Nathanael Schärli, Sebastian Gehrmann, Yi Tay,
    Hyung Won Chung, Aakanksha Chowdhery, et al. ‘Challenging BIG-Bench Tasks and
    Whether Chain-of-Thought Can Solve Them’. arXiv [Cs.CL], 2022\. arXiv. [http://arxiv.org/abs/2210.09261](http://arxiv.org/abs/2210.09261).
  id: totrans-split-106
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Suzgun, Mirac, Nathan Scales, Nathanael Schärli, Sebastian Gehrmann, Yi Tay,
    Hyung Won Chung, Aakanksha Chowdhery, 等人。‘Challenging BIG-Bench Tasks and Whether
    Chain-of-Thought Can Solve Them’. arXiv [Cs.CL], 2022\. arXiv. [http://arxiv.org/abs/2210.09261](http://arxiv.org/abs/2210.09261).
- en: 'Touvron, Hugo, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne
    Lachaux, Timothée Lacroix, Baptiste Rozière, et al. ‘LLaMA: Open and Efficient
    Foundation Language Models’. arXiv [Cs.CL], 2023\. arXiv. [http://arxiv.org/abs/2302.13971](http://arxiv.org/abs/2302.13971).'
  id: totrans-split-107
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 'Touvron, Hugo, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne
    Lachaux, Timothée Lacroix, Baptiste Rozière, 等人。‘LLaMA: Open and Efficient Foundation
    Language Models’. arXiv [Cs.CL], 2023\. arXiv. [http://arxiv.org/abs/2302.13971](http://arxiv.org/abs/2302.13971).'
