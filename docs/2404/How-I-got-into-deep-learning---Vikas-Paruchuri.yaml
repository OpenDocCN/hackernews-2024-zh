- en: <!--yml
  id: totrans-split-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-split-1
  prefs: []
  type: TYPE_NORMAL
  zh: 'category: 未分类'
- en: 'date: 2024-05-27 13:08:31'
  id: totrans-split-2
  prefs: []
  type: TYPE_NORMAL
  zh: 'date: 2024-05-27 13:08:31'
- en: -->
  id: totrans-split-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: How I got into deep learning - Vikas Paruchuri
  id: totrans-split-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 我是如何进入深度学习的 - Vikas Paruchuri
- en: 来源：[https://www.vikas.sh/post/how-i-got-into-deep-learning](https://www.vikas.sh/post/how-i-got-into-deep-learning)
  id: totrans-split-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://www.vikas.sh/post/how-i-got-into-deep-learning](https://www.vikas.sh/post/how-i-got-into-deep-learning)
- en: I ran an education company, [Dataquest](https://www.dataquest.io/), for 8 years.
    Last year, I got the itch to start building again. Deep learning was always interesting
    to me, but I knew very little about it. I set out to fix that problem.
  id: totrans-split-6
  prefs: []
  type: TYPE_NORMAL
  zh: 我经营了一个教育公司，[Dataquest](https://www.dataquest.io/)，长达8年。去年，我开始有兴趣重新开始建设。深度学习一直很有趣，但我对它知之甚少。我开始解决这个问题。
- en: Since then, I’ve trained dozens of [models](https://huggingface.co/vikp) (several
    state of the art for open source), built [2 libraries](https://github.com/VikParuchuri/)
    that have 5k+ Github stars, and recently accepted an offer from [answer.ai](http://www.answer.ai/),
    a research lab started by [Jeremy Howard](https://twitter.com/jeremyphoward).
  id: totrans-split-7
  prefs: []
  type: TYPE_NORMAL
  zh: 从那时起，我训练了数十个[模型](https://huggingface.co/vikp)（其中几个是开源领域的最新技术），建立了[2个库](https://github.com/VikParuchuri/)，拥有5千多个Github星标，并最近接受了来自[answer.ai](http://www.answer.ai/)的邀请，这是由[Jeremy
    Howard](https://twitter.com/jeremyphoward)创立的研究实验室。
- en: I say this to establish the very rough outline of my learning journey. In this
    post, I’m going to cover more detail about how I learned deep learning. Hopefully
    it helps on your journey.
  id: totrans-split-8
  prefs: []
  type: TYPE_NORMAL
  zh: 我说这些是为了大致勾画我的学习历程。在这篇文章中，我将更详细地讲述我如何学习深度学习。希望对你的学习之旅有所帮助。
- en: I didn’t learn this stuff in school. I majored in American History for undergrad,
    and [failed quite a few classes](https://www.vikas.sh/post/i-barely-graduated-college).
  id: totrans-split-9
  prefs: []
  type: TYPE_NORMAL
  zh: 我在学校里并没有学习这些内容。本科主修美国历史，[不少课程](https://www.vikas.sh/post/i-barely-graduated-college)都挂了。
- en: I did machine learning and Python work in 2012, but convinced myself that deep
    learning was too complicated for me. One reason for this was because I learned
    by doing Kaggle competitions. Kaggle competitions are amazing for learning quickly,
    but can leave you with gaps in the fundamentals - like how algorithms work mathematically.
  id: totrans-split-10
  prefs: []
  type: TYPE_NORMAL
  zh: 我在2012年做了机器学习和Python工作，但是说服自己深度学习对我来说太复杂了。其中一个原因是我通过参加Kaggle竞赛来学习。Kaggle竞赛确实很适合快速学习，但可能会导致你在基础知识上留下空白，比如算法的数学原理。
- en: When deep learning started to become popular, it was very math-heavy, and I
    felt like I’d never be able to understand it. Of course, this was false, as I
    proved to myself 10 years later, but the angle at which you approach something
    makes all the difference. I approached deep learning top-down the first time,
    by gluing models together without understanding how they worked. I eventually
    hit a wall, and couldn’t get past it.
  id: totrans-split-11
  prefs: []
  type: TYPE_NORMAL
  zh: 当深度学习开始流行时，数学内容很多，我觉得自己永远无法理解它。当然，这是错误的，10年后我证明了这一点，但你对待问题的角度决定了一切。第一次我是从顶层开始接触深度学习，只是将模型粘贴在一起，而不了解它们的工作原理。最终我遇到了障碍，无法继续前行。
- en: When I studied deep learning last year, I already had useful skills. The first
    was strong Python programming ability. Despite efforts to the contrary, Python
    is still the universal language of AI. If you want to get into AI, start by [getting
    really good at programming](https://twitter.com/gdb/status/1729893902814192096).
  id: totrans-split-12
  prefs: []
  type: TYPE_NORMAL
  zh: 去年我学习深度学习时，我已经具备了一些有用的技能。首先是扎实的Python编程能力。尽管有些人试图推翻，Python仍然是AI的通用语言。如果你想进入AI领域，首先要[精通编程](https://twitter.com/gdb/status/1729893902814192096)。
- en: No matter what era of AI I’ve been in, data cleaning has been >70% of my work.
    It’s possible if you’re doing pure research or working on toy problems you can
    avoid working with data, but otherwise data skills are essential.
  id: totrans-split-13
  prefs: []
  type: TYPE_NORMAL
  zh: 无论我身处AI的哪个时代，数据清洗始终占据了我工作的超过70%。如果你在做纯粹的研究或处理玩具问题，可能可以避免与数据打交道，但否则数据处理技能是必不可少的。
- en: There’s a slightly more nebulous skill I’ll call pragmatism. Deep learning has
    a lot of rabbit holes - ranging from “what’s the perfect base model?”, to “what
    if I get rid of the sigmoid here?” Some of these rabbit holes are useful, but
    most of them will eat a lot of time. Being able to recognize when to go deep,
    and when to just do the fast/easy solution is important.
  id: totrans-split-14
  prefs: []
  type: TYPE_NORMAL
  zh: 还有一种稍微模糊的技能，我称之为实用主义。深度学习有很多兔子洞，从“最完美的基础模型是什么？”到“如果我在这里去掉sigmoid会怎样？”这些兔子洞中有些是有用的，但大多数会耗费大量时间。能够识别何时深入，何时采取快速/简单解决方案非常重要。
- en: This time, I decided to learn bottom-up, fundamentals first. I read [The Deep
    Learning Book](https://www.deeplearningbook.org/). It’s a few years old, but still
    a fantastic resource. Read it slowly. A lot of the terminology and math will be
    unfamiliar - look them up. You may need to sketch some things out or code them
    to get them - give yourself the space to do that. If the math is unfamiliar, a
    good complementary resource is [Math for Machine Learning](https://mml-book.github.io/book/mml-book.pdf).
    Although I haven’t taken them, [fast.ai](https://www.fast.ai/) and the [Karpathy
    videos](https://www.youtube.com/@AndrejKarpathy) are high quality.
  id: totrans-split-15
  prefs: []
  type: TYPE_NORMAL
  zh: 这一次，我决定从基础开始自底向上学习。我阅读了[深度学习书籍](https://www.deeplearningbook.org/)。这本书已经有些年头了，但仍然是一个非常好的资源。慢慢地阅读它。很多术语和数学内容可能会让你感到陌生
    - 查阅它们。你可能需要草绘一些内容或者编写代码来理解它们 - 给自己足够的空间去做这些。如果数学内容让你感到陌生，一个很好的补充资源是[机器学习的数学](https://mml-book.github.io/book/mml-book.pdf)。虽然我还没有参加它们，但[fast.ai](https://www.fast.ai/)和[Karpathy的视频](https://www.youtube.com/@AndrejKarpathy)质量都很高。
- en: Even though architectures like CNN or RNN might seem out of date in a world
    that is moving towards transformers for everything, [CNNs are still widely used](https://twitter.com/rasbt/status/1767561783382872194),
    and everything [old is new again](https://arxiv.org/abs/2402.19427) with RNNs.
  id: totrans-split-16
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管像CNN或RNN这样的架构在一个向全面使用transformers的世界中似乎已经过时，[CNN仍然被广泛使用](https://twitter.com/rasbt/status/1767561783382872194)，而像RNN这样的东西在[旧时代已然是新时代](https://arxiv.org/abs/2402.19427)。
- en: When you’re done with the first 2 parts of the book (you can skip part 3), you
    should be at a point where you can code up any of the main neural networks architectures
    in plain numpy (forward and backward passes).
  id: totrans-split-17
  prefs: []
  type: TYPE_NORMAL
  zh: 完成书的前两部分后（第三部分可以跳过），你应该能够使用纯粹的numpy编写任何主要的神经网络架构（前向和反向传播）。
- en: One thing that will really help you get to that point is teaching the skills
    while you learn them. I started putting together a course, [Zero to GPT](https://github.com/VikParuchuri/zero_to_gpt),
    as I read the deep learning book. Teaching is the ultimate way to solidify concepts
    in your head, and I found myself falling into a nice cycle of learning, looking
    up/sketching what I didn’t understand, then teaching it.
  id: totrans-split-18
  prefs: []
  type: TYPE_NORMAL
  zh: 真正帮助你达到这一点的一件事是在学习过程中教授这些技能。我在阅读深度学习书籍的同时开始组织一个课程，[从零到GPT](https://github.com/VikParuchuri/zero_to_gpt)。教学是巩固概念的终极方式，我发现自己陷入了一个良好的学习循环：学习、查阅/草绘我不理解的内容，然后教授它们。
- en: The book will take you up to 2015-era deep learning. After reading the book,
    I read some of the foundational deep learning papers from the 2015-2022 era and
    implemented them in PyTorch. You can use [Google Colab](https://colab.research.google.com/)
    for free/cheap GPUs, and [Weights and Biases](https://wandb.ai/) to track your
    training runs.
  id: totrans-split-19
  prefs: []
  type: TYPE_NORMAL
  zh: 这本书会带你走进2015年左右的深度学习。阅读完这本书后，我阅读了2015-2022年间一些基础的深度学习论文，并在PyTorch中实现了它们。你可以使用[Google
    Colab](https://colab.research.google.com/)免费/廉价的GPU，并使用[Weights and Biases](https://wandb.ai/)来追踪你的训练运行。
- en: 'A noncomprehensive list is:'
  id: totrans-split-20
  prefs: []
  type: TYPE_NORMAL
  zh: 一个非全面的列表是：
- en: After this, you should be able to understand most conversations people have
    about deep learning model architectures.
  id: totrans-split-21
  prefs: []
  type: TYPE_NORMAL
  zh: 在此之后，你应该能够理解大多数关于深度学习模型架构的对话。
- en: The easiest entrypoint for training models these days is finetuning a base model.
    [Huggingface transformers](https://github.com/huggingface/transformers) is great
    for finetuning because it implements a lot of models already, and uses PyTorch.
  id: totrans-split-22
  prefs: []
  type: TYPE_NORMAL
  zh: 如今训练模型的最简单入口是对基础模型进行微调。[Huggingface transformers](https://github.com/huggingface/transformers)非常适合微调，因为它已经实现了许多模型，并使用PyTorch。
- en: There are Discord communities, like [Nous Research](https://discord.gg/HhC3avAG)
    and [EleutherAI](https://discord.gg/BVwwHaeV) where people discuss the latest
    models and papers. I’d recommend joining them, seeing what’s state of the art
    at the moment, and trying some finetuning.
  id: totrans-split-23
  prefs: []
  type: TYPE_NORMAL
  zh: 有一些Discord社区，比如[Nous Research](https://discord.gg/HhC3avAG)和[EleutherAI](https://discord.gg/BVwwHaeV)，人们在那里讨论最新的模型和论文。我建议你加入其中，了解当前的最新技术，并尝试一些微调。
- en: The easiest way to finetune is to pick a small model (7B or fewer params), and
    try finetuning with LoRA. You can use Google Colab, or something like [Lambda
    Labs](https://lambdalabs.com/) if you need more VRAM or multiple GPUs.
  id: totrans-split-24
  prefs: []
  type: TYPE_NORMAL
  zh: 进行微调的最简单方法是选择一个小模型（7B或更少的参数），并尝试使用LoRA进行微调。你可以使用Google Colab，或者像[Lambda Labs](https://lambdalabs.com/)这样的服务，如果需要更多的VRAM或多个GPU。
- en: I wanted to train models to code better, so I put together datasets and finetuned
    a few different base models on data from StackOverflow and other places. It really
    helped me understand the linkage between model architecture, data, compute, and
    output. However, finetuning is a very crowded space, and it’s hard to make an
    impact when the state of the art changes every day.
  id: totrans-split-25
  prefs: []
  type: TYPE_NORMAL
  zh: 我想训练模型以编写更好的代码，所以我收集了数据集，并在来自StackOverflow和其他地方的数据上微调了几个不同的基础模型。这确实帮助我理解了模型架构、数据、计算和输出之间的联系。然而，微调是一个非常竞争激烈的领域，当前技术每天都在变化，要产生影响是很困难的。
- en: As I was working on finetuning, I realized that some of the highest quality
    data was in textbook form, and locked away in pdfs. One way I tried to solve this
    was to generate [synthetic data](https://github.com/VikParuchuri/textbook_quality).
  id: totrans-split-26
  prefs: []
  type: TYPE_NORMAL
  zh: 当我在进行微调时，我意识到一些最高质量的数据是以教科书的形式存在并锁定在pdf中。我试图解决这个问题的一种方式是生成[合成数据](https://github.com/VikParuchuri/textbook_quality)。
- en: Another way was to extract the data from pdfs and turn it into good training
    data (markdown). There was an approach called [nougat](https://arxiv.org/abs/2308.13418)
    that worked well in many cases, but was slow and expensive to run. I decided to
    see if I could build something better by leveraging the data already in the pdf
    (avoiding OCR), and only using models when needed. I chained together several
    different models, along with heuristics in between. [This approach](https://github.com/VikParuchuri/marker),
    marker, is 10x faster than nougat, works with any language, and is usually more
    accurate.
  id: totrans-split-27
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种方法是从pdf中提取数据并将其转换为良好的训练数据（Markdown）。有一种叫做[nougat](https://arxiv.org/abs/2308.13418)的方法在许多情况下效果很好，但运行速度慢且成本高昂。我决定看看是否可以通过利用pdf中已有的数据（避免OCR），仅在需要时使用模型来构建更好的东西。我将几种不同的模型串联在一起，并在其中使用了启发式算法。[这种方法](https://github.com/VikParuchuri/marker)，marker，比nougat快10倍，适用于任何语言，并且通常更精确。
- en: Working on marker led me to want to solve several more problems, and I’ve also
    trained an equation to LaTeX [model](https://github.com/VikParuchuri/texify),
    a text detection model, an [OCR model](https://github.com/VikParuchuri/surya)
    that’s competitive with Google Cloud, and a layout model.
  id: totrans-split-28
  prefs: []
  type: TYPE_NORMAL
  zh: 进行marker的工作让我想要解决更多的问题，我还训练了一个将方程式转换为LaTeX的[model](https://github.com/VikParuchuri/texify)，一个文本检测模型，一个[OCR模型](https://github.com/VikParuchuri/surya)，它与Google
    Cloud竞争力相当，并且一个布局模型。
- en: For all of these models, I took existing architectures, changed the layers,
    loss, and other elements, then generated/found the right datasets. For example,
    for the OCR model, I started with the [Donut architecture](https://arxiv.org/abs/2111.15664),
    added GQA, an MoE layer, UTF-16 decoding (1-2 tokens for any character), and changed
    some of the model shapes.
  id: totrans-split-29
  prefs: []
  type: TYPE_NORMAL
  zh: 对于所有这些模型，我采用了现有的架构，修改了层、损失和其他元素，然后生成/找到了合适的数据集。例如，对于OCR模型，我从[Donut架构](https://arxiv.org/abs/2111.15664)开始，加入了GQA、MoE层、UTF-16解码（任何字符1-2个令牌），并改变了一些模型形状。
- en: Since OCR models are typically small (less than 300M params), I was able to
    train all of these models on 4x A6000s. I probably could have gotten away with
    2x A6000s if I was a bit more efficient.
  id: totrans-split-30
  prefs: []
  type: TYPE_NORMAL
  zh: 由于OCR模型通常很小（参数少于300M），我能够在4x A6000上训练所有这些模型。如果我更高效一些，可能只需要2x A6000。
- en: 'Hopefully this illustrates 3 things for you:'
  id: totrans-split-31
  prefs: []
  type: TYPE_NORMAL
  zh: 希望这为你阐明了三件事情：
- en: Understanding the fundamentals is important to training good models
  id: totrans-split-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解基础原理对于训练优秀模型至关重要。
- en: Finding interesting problems to solve is the best way to make an impact with
    what you build
  id: totrans-split-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 找到有趣的问题解决方案是利用你所构建的东西产生影响的最佳途径。
- en: You don’t need a lot of GPUs
  id: totrans-split-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你不需要很多的GPU。
- en: There are many niches in AI where you can make a big impact, even as a relative
    outsider.
  id: totrans-split-35
  prefs: []
  type: TYPE_NORMAL
  zh: 在AI中有许多小众领域，即使作为一个相对外行者也能产生重大影响。
- en: As you may have noticed, I open source all of my AI projects. The data stack
    is a very underinvested area of AI relative to impact. I feel strongly that the
    more widely you can distribute high quality training data, the lower the risk
    of 1-2 organizations having a monopoly on good models.
  id: totrans-split-36
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你可能注意到的，我开源了我所有的AI项目。与影响相比，数据堆栈是AI领域中投资相对不足的一个领域。我坚信，你能够广泛分发高质量的训练数据，这样1-2家组织垄断好模型的风险就会降低。
- en: Open source also has a side effect of being a good way to get exposure. Which
    leads me to the last part of my story.
  id: totrans-split-37
  prefs: []
  type: TYPE_NORMAL
  zh: 开源还有一个曝光度良好的副作用。这让我想到了我的故事的最后一部分。
- en: I was thinking about building a business around my open source tools. Working
    somewhere wasn’t on my radar at all. But when Jeremy reached out about answer.ai,
    I felt like it was an opportunity I had to take. The chance to work with talented
    people, make a positive impact, and learn a lot is hard to pass up.
  id: totrans-split-38
  prefs: []
  type: TYPE_NORMAL
  zh: 我本来考虑围绕我的开源工具建立一家企业。工作在我当时的视野中根本没有。但当Jeremy联系我关于answer.ai时，我觉得这是一个我必须抓住的机会。与才华横溢的人一起工作，产生积极影响并且学到很多东西，这样的机会实在难以放过。
- en: My open source work directly led to the job opportunity, both in the obvious
    way (it gave me exposure), and in a subtler way (it significantly improved my
    skills). Hopefully, you’ll open source something as you learn, too.
  id: totrans-split-39
  prefs: []
  type: TYPE_NORMAL
  zh: 我的开源工作直接导致了这次工作机会，既在显而易见的方式（它让我有了曝光），也在更微妙的方式（显著提高了我的技能）。希望当你学习时，你也能开源一些东西。
- en: I suspect my work at answer.ai will look very similar to my open source work.
    I’ll keep training models, improving the data stack, and releasing publicly.
  id: totrans-split-40
  prefs: []
  type: TYPE_NORMAL
  zh: 我怀疑我在answer.ai的工作会与我的开源工作非常相似。我将继续训练模型，改进数据堆栈，并且公开发布。
- en: If you’re trying to break into deep learning, I hope this post was useful. If
    not, I hope it was somewhat entertaining (you made it to the end, so it probably
    was, right?).
  id: totrans-split-41
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你正试图进入深度学习领域，希望这篇文章对你有所帮助。如果不是，请希望它能有些许娱乐性（你读到了结尾，所以它可能确实有趣，对吧？）。
- en: As for me, I’m going back to training some models (watching 2 of them converge
    right now).
  id: totrans-split-42
  prefs: []
  type: TYPE_NORMAL
  zh: 至于我，我现在正在回到训练一些模型（目前正在观察其中的两个收敛）。
