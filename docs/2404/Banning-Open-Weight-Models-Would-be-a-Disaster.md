<!--yml

category: 未分类

日期：2024年5月27日 12:51:03

-->

# 禁止开放权重模型将是一场灾难。

> 来源：[https://rbren.substack.com/p/banning-open-weight-models-would](https://rbren.substack.com/p/banning-open-weight-models-would)

响应拜登总统的[行政命令](https://www.whitehouse.gov/briefing-room/presidential-actions/2023/10/30/executive-order-on-the-safe-secure-and-trustworthy-development-and-use-of-artificial-intelligence/)，美国商务部（DoC）已要求公众对“开放权重AI模型”（如LLaMa、稳定扩散和Mixtral等）发表意见。他们正在考虑阻止公众访问这些模型，以防止滥用。

这将是一个可怕的错误。

* * *

PSA：您可以通过点击[regulations.gov](https://www.regulations.gov/document/NTIA-2023-0009-0001)上的“评论”按钮向美国商务部提交您的评论。请随意复制或改写以下我的评论。

截止日期为2024年3月27日。

* * *

如今开发的最复杂的AI大多是闭源的（包括讽刺的是OpenAI的大部分工作）。科学家、工程师和公众无法看到这些算法的内部运作。如果我们想使用它们，就必须与创建者签订法律和/或财务协议。我们的活动受到严格限制和监控。

这大多是正常现象。公司开发新技术，然后控制其使用。ChatGPT的闭源性质与Google Search或Snapchat并没有实质性区别。

但竞争对手正在崛起，并且其中许多公司采取更加开放的方式。Meta、Stability AI、Mistral等公司都免费提供他们的工作，使我们能够在自己的硬件上运行最先进的模型，超出任何监督或控制的范围。

这也大多是正常现象。人们开发新技术，然后免费发布，因为发布不需要任何成本。Mixtral或LLaMa的开放性与Linux或Signal并没有实质性区别。

与AI唯一的区别在于，AI具有巨大的力量。政府对广泛使用先进AI的后果感到担忧是正当的。

直白地说：封闭模型允许集中控制。与此相反，开放系统则无需监督，可以轻松绕过任何控制（见例如[如何在5秒内移除Stable Diffusion的安全过滤器](https://www.reddit.com/r/StableDiffusion/comments/wv2nw0/tutorial_how_to_remove_the_safety_filter_in_5/)）。

因此，政府希望阻止开放模型的分发，这一点很容易理解。

AI带来的最大威胁并非来自个人，而是来自企业和国家级行为者。无论如何，他们都将不受限制地获得最先进的AI技术。

资金充裕的组织（例如公司和情报机构）能够负担得起建立和训练他们自己的定制模型。无论美国采取何种监管方法，这都是我们必须应对的现实——至少，敌对国家将向他们的军火库中添加先进的AI。我们应该预料到将出现协调一致的虚假信息宣传活动、伪草根运动、媒体驱动的成瘾以及复杂的网络安全攻击。

开放模型使公众有机会进行反击。开放模型使安全研究人员、学术界、非政府组织和监管机构能够尝试使用最先进的技术。我们可以发现攻击模式并构建技术来检测和预防滥用。我们可以创建良好的AI来对抗邪恶的AI。

开放模型平衡了竞争环境。

另外，禁止开放模型将极大地阻碍创新和经济增长。开放模型使AI技术的获取民主化，使得使用闭源模型不可行的用例成为可能（例如，当地的高中可以购买一台计算机，给学生提供无限制访问开放的LLM，而使用闭源模型如OpenAI则需支付永久性费用）。开放模型使学术界和初创公司能够构建和分发新应用，这些应用连基础模型的创作者都未曾梦想过。开放模型可以轻松地集成到现有的工作流程和应用中（例如在[Photoshop](https://github.com/AbdullahAlfaraj/Auto-Photoshop-StableDiffusion-Plugin)和[Blender](https://github.com/benrugg/AI-Render)中使用的社区构建的Stable Diffusion集成，而DALL-E则基本上被封闭在墙内）。

AI 不仅是一个新的产业，它还是经济的催化剂。限制获取将加剧不平等，并使美国在全球经济中竞争力下降。

当然，个人对AI拥有无限制访问可能会对其他个人造成一些伤害。我们需要法律和法规来减轻损害。但这些法律应该惩罚滥用AI的人和组织，而不是完全禁止技术的访问。

总的来说，对社会伤害有两种处理方式：我们可以通过惩罚来减少它，或者通过监视和控制来根除它。前者是自由社会的规范，而后者是压制的标志。

禁止开放模型可能会在个人层面上防止一些伤害，但这将使我们在社会层面上面临存在威胁。

以下是DoC对其RFC摘要的简要版本（重点在于我自己）：

> 人工智能（AI）对社会、经济和科学进步产生了重大影响，**许多最重要的模型……是‘‘完全封闭’’或‘‘高度受限’’**……然而……越来越‘‘开放’’的先进AI模型生态系统，允许开发者和其他人使用广泛可用的计算资源来优化模型。
> 
> **[开放模型] 在促进…小型企业、学术机构、资金不足的企业家甚至传统企业的增长中可能起到关键作用**……对基础模型访问的集中可能会阻碍这种创新和进步……这些开放基础模型有潜力帮助科学家做出新的医学发现，甚至使枯燥且耗时的活动更加高效。
> 
> **开放基础模型有可能转变…医学、制药和科学研究…**
> 
> 公开基础模型能够提供更多透明度，使技术专家、研究人员、学者以及来自安全社区的人士能够更广泛地访问和进行更广泛的监督......**公开基础模型的可访问性还为个人和公民社会团体提供了工具，以抵抗威权政权**，进一步推动民主价值观和美国外交政策目标。
> 
> …**开放基础模型也可能带来风险**…例如由于主动滥用、有效监督的失败或缺乏明确的问责机制而造成的安全、公平、民权或其他损害……对开放基础模型的监控不足可能加剧现有挑战，例如便于**创建合成的非自愿亲密图像或实施大规模虚假信息宣传**。
> 
> ...总统令要求NTIA考虑具有“广泛可用”权重的双用途基础模型的风险和好处......

国家电信和信息管理局（NTIA）列出了几十个问题，以帮助指导他们在此政策方面的决策。这些大部分是很好的问题，尽管有一些显得天真或荒谬。总共有九个主要部分：

1.  “开放”应该如何定义？

1.  开放和封闭模型之间的风险比较是如何的？

1.  开放模型有哪些好处？

1.  除了权重，还有哪些其他组件很重要？

1.  如何处理与管理风险相关的技术问题？

1.  法律和商业问题

1.  可以利用哪些监管机制？

1.  我们如何未雨绸缪我们的战略？

1.  其他问题

下面是我的回答，将通过上面的链接发送给NTIA。如果你想快速浏览，我已经用粗体标记了最重要的问题。

> 1\. 当思考基础模型和模型权重时，NTIA应该如何定义“开放”或“广泛可用”？

如果一个模型的权重可用，则该模型才能被称为“广泛可用”。训练需要大量的计算资源和数据，否则一个“开源但权重封闭”的模型将毫无用处。

许可限制可能会影响其可用性的广度，但强制执行许可却很难。风险规避的机构通常尊重这些限制，但个人和国家行为者则很容易无视。

如果任何人都可以下载并在自己的硬件上运行模型，则该模型应被视为“开放”。

> **1a. 是否有证据或历史例子表明类似当前闭源AI系统的模型权重可能会或不会广泛可用？如果有，它们是什么？**

是的——软件始终遵循这种流程，这要归功于分发的边际成本低。有人创造了一些伟大的东西，将其保密，并享受临时的垄断地位。最终，个人或潜在竞争者发布了不那么好的开源版本，希望打破那种垄断地位。

虽然专有软件可能仍然找到维持大市场份额的方法，但开放和闭源之间的*技术*差距随时间缩小。

历史上的例子包括操作系统（Windows → Linux）、云计算（AWS → Kubernetes）、社交媒体（Twitter → Mastodon）和聊天（WhatsApp → Signal）。

> 1b. 一般来说，我们能估计封闭模型和开放基础模型在相关任务上部署之间的时间间隔吗？你预计这个时间间隔会如何变化？基于哪些变量？你预计这些变量在未来几个月和年份会如何变化？

我在这里可以说的是，开放模型在非常快的时间内追赶上了GPT。我怀疑这个差距会保持很小。通常，随着专有知识的分布，延迟会随时间缩短。

公开发布模型为AI创作者带来了巨大的竞争优势，并帮助削弱更有能力的专有模型。鉴于这个领域的激烈竞争，我们应该预期看到更多公司公开他们的模型，以试图获得市场份额。

> 1c. 应该根据分发水平来定义模型权重的“广泛可用性”吗？如果是，模型权重应被视为何种分发水平下的“广泛可用性”（*例如*，10,000个实体；100万个实体；开放出版等）？如果不是，NTIA应该如何定义“广泛可用性”？

我不认为这个问题有意义。不清楚如何计算有权访问模型的实体数量，除非权重被保持关闭。

例如：Meta可能会强制下载LLaMa权重的任何人提供ID并签署限制性许可，但任何不良行为者都可以打破许可证，重新分发权重等等。

或者OpenAI可以与其他值得信赖的组织（例如Microsoft）达成私下协议，分享他们的权重，但这肯定不是“广泛可用性”——即使他们有成千上万这样的协议。

“广泛可用性”是二元的——要么任何人都可以下载和运行模型，要么模型提供者对接入进行控制，只授予高度信任的实体。

> 1d. 对于开放基础模型的某些形式的访问（Web应用程序、API、本地托管、边缘部署），提供更多或更少的利益或风险？这些风险是否取决于系统或应用程序启用访问的其他细节？

只有将权重保持闭源，并且所有使用通过网络接口驱动（例如 Web 应用程序或 REST API），才能监控和控制使用情况。

在不受控制、自由分发的模型与完全受控制的闭源模型之间没有中间地带。

> **1d i. 是否存在可能能够在利益与风险之间取得更有利平衡的有前途的访问形式或方式？如果有，它们是什么？**

我可以想象类似于应用商店的东西——表面上每个人都有访问权限，但需要申请流程，并且需要人工审查团队参与。

但是，像应用商店这样的发布平台限制访问与限制数据下载完全不同——一旦有人下载了模型，访问权限就无法撤销。而且他们可以轻松地重新分发模型，绕过审查过程。

所以，再次强调，模型访问是二元的。

> **2\. 让模型权重广泛可用的风险与保持非公开模型权重相关的风险相比如何？**

在两种情况下滥用的潜力相似——不同之处在于能够利用这一潜力的*人*及其可能的目标。

无论是开放还是闭源模型，国家和企业行为者都能够以损害社会为目的使用这些模型。将模型保持闭源只会在这里增加一道小障碍——大型组织有资源使用公开信息训练自己的模型。我们应该预期像协调的虚假信息宣传、草根模拟运动、以媒体为驱动的成瘾和复杂的网络安全攻击等事件发生。

对于公共模型，个体行为者参与其中。大多数情况下，他们将对其他个人造成伤害，例如通过生成的虚假私人公民图像。公共模型不会给整个社会增加额外的存在威胁，但可能会对个别个人造成一些额外伤害。

> 2a. 与广泛可用的模型权重相关的风险是什么？这些风险是否随着培训数据或与微调、预训练或部署模型相关的源代码同时广泛可用而改变？

广泛可用的权重允许所有个人以任何方式使用技术。分发源代码和其他资产会使微调变得更容易，但并不会实质性改变风险。

这里的主要风险是对其他个人造成伤害，例如通过生成的私人公民图像。此外，还存在在线讨论可能进一步恶化的风险，因为生成的图像和文本开始主导对话。

> 2b. 开放基础模型是否可能会减少在影响权利和安全的人工智能系统（如医疗、教育、刑事司法、住房、在线平台等）中的公平性？

开放模型更可能提高公平性，而不是减少它。

低平等水平是由于科技、机会、资源等不平等的获取造成的。当大型营利实体掌控系统时，他们将系统推向不公平的失衡状态，并以牺牲其他利益相关者为代价获取更多利润。

开放模型将防止围绕AI形成这种寡头垄断。

> **2c. 宽泛可用的模型权重是否可能导致与隐私相关的风险？**

任何公开可用信息都存在同样的风险。

如果在一组权重中发现了私有/专有数据，托管这些权重的各方（例如GitHub）可以通过今天可用的相同机制（例如DMCA通知）轻松通知。

开放模型将使这一过程更加透明，从而更可能符合合规要求。而封闭模型则由控制组织决定如何处理。

我们今天在开源和闭源软件中看到了这一模式。私有软件公司通常选择不披露安全漏洞和违规行为。开源软件则被迫披露，因为代码及其任何安全补丁都是公开的。

> **2d. 国家或非国家行为者是否可以利用广泛可用的模型权重创建或加剧安全风险，包括但不限于对基础设施、公共健康、人权和民权、民主、国防和经济的威胁？**

国家级行为者并没有因开放模型而大为增强其能力——他们有资源建立和训练自己的封闭模型。

险恶的国家级行为者在很大程度上受到开放模型的*阻碍*，这使得安全研究人员、非政府组织、活动家和初创企业处于公平竞争的水平。

> **2d i. 这些风险与封闭模型相关的风险相比如何？**

如果全世界只有封闭模型，国家级滥用的风险将更大。开放模型为公众提供了反击国家级行为者的工具。

> **2d ii. 这些风险与其他类型的软件系统和信息资源相关的风险相比如何？**

这是一个很好的问题。差异不是种类的不同，而是规模的不同。

加密技术、安全工具、互联网接入等都是政府可能想要限制可用性的东西，无论是通过法规还是出口管制。在每种情况下，我们都完全阻止了小行为者访问技术的可能性，而只是给大行为者带来了不便。

AI比这些技术更强大，但遵循同样的动态。

> **e. 不同司法管辖区对广泛可用模型的访问差异可能会导致什么风险？**

在一个司法管辖区内提供模型，而在另一个司法管辖区内不提供，可能会加剧全球不平等。但规避这种法规并不难——您只需有一个人将数据走私到管辖防火墙之后即可。而VPN使这一过程变得简单。

> **f. 在回答上述问题时，哪些风险最严重，哪些最有可能？这些风险如何相互关联（如果有的话）？**

最严重的风险是社会级别的破坏：协调的虚假信息宣传活动、国家规模的网络安全攻击、增强型AI战争等。不幸的是，这些事情可能会在某种程度上发生。但可以通过确保安全研究人员和学术界可以开放获取最先进的模型来减轻其影响。

较不严重但仍值得注意的是那些会给个人带来伤害的风险。生成的私人公民图像可能会损害声誉并造成心理伤害；LLM可以用于通过社交媒体自动骚扰人们；人们可能会被虚假图像或虚假新闻网站欺骗。在开放模型的世界中，这些风险略有增加——个人更容易利用软件进行有害行为，而这可能会被API后面的封闭模型所阻止。

因此，我们面临一个抉择：开放模型的世界带来更多个体风险，但社会风险较少；没有开放模型的世界可以使个人稍微更安全，但会使整个社会处于危险之中。

> **3\. 开放模型权重与完全封闭模型相比有哪些好处？**

主要好处包括：

+   更大的透明度

+   更好的安全性

+   降低创新壁垒

+   降低市场参与门槛

+   降低使用成本

+   更好的用户体验

+   降低财政和政治不平等

> **a. 开放模型权重在人工智能市场以及经济其他领域中对竞争和创新带来了哪些好处？开放双用基础模型如何能够促进或增强科学研究以及计算机科学及相关领域的教育/培训？**

开放模型极大地降低了小型和中型企业、学术机构和非营利组织的进入门槛。

训练一流的LLM需要数百万美元（不久之后可能是数十亿美元）。企业自然会保护它们的投资，阻止初创企业以竞争性方式使用其模型。

开放模型允许任何个人或初创企业构建利用人工智能的产品、服务和开源项目，并在无需企业许可的情况下分发它们。

例如，看看利用稳定扩散的众多商业和开源项目，与相对封闭的DALL-E相比。稳定扩散在修补/修复、参数调整、动画等方面带来了更多创新。社区已将稳定扩散整合到像Photoshop和Blender这样的现有图像编辑软件中，而DALL-E仍然大多封闭。

> **b. 如何通过使模型权重广泛可用来提高人工智能的安全性、可信度和公众应对潜在人工智能风险的鲁棒性？**

透明性培养信任。

当人工智能在私人领域制造时，它可能会被指责（并容易受到）政治偏见、种族偏见等，这些偏见可能是有意插入的（例如由意识形态的CEO）或者是意外的（例如来自训练数据的偏见）。不良行为者可以插入后门（又称“[沉睡间谍](https://www.astralcodexten.com/p/ai-sleeper-agents)”）。

与此同时，研究人员可以审查开放模型。每一次变更都是公开的，可以接受审计。

此外，让安全研究人员和学者无限制地接触到最先进的模型极大地增强了公众对人工智能风险的准备。我们可以提前了解对手政府如何利用人工智能来破坏我们的经济、攻击我们的基础设施或者破坏民主。我们可以开发防御性人工智能，以应对恶意人工智能。

> **c. 开放模型权重，特别是重新训练模型的能力，是否有助于促进权利和安全的公平，影响人工智能系统（***例如*** **医疗保健、教育、刑事司法、住房、在线平台等）？**

是的。非营利组织、非政府组织、政府机构等可以重新训练开放模型，以解决公共需求。

具体而言，开放模型可以帮助挖掘和分析与社会有益使命相关的数据。例如：社交媒体初创公司可以使用语言模型来检测其平台上的仇恨和骚扰；非营利组织可以使用人工智能来分析房地产列表中的种族差异；语言模型可以帮助被监禁者更好地了解他们的法律选择；社会科学家可以使用语言模型来分析市政会议记录中的腐败迹象。

如果模型保持私有，这些任务可能会或可能不会被控制它们的私人实体明确禁止。大多数情况下是经济不可行的。开放模型大幅降低了利用这项技术的成本，将其从垄断资源转变为商品化。

> **d. AI模型的扩散如何支持美国的国家安全利益？它可能如何干扰或促进美国内外人权的享受和保护？**

开放模型对美国国家安全有巨大的益处。

安全研究人员和学者可以使用开放模型探索最先进的技术，没有限制，使漏洞和攻击路径更快地成为公共知识，胜过对手政府能够利用它们。他们可以开发防御性人工智能来帮助我们对抗滥用。如果我们限制这些研究人员访问和研究人工智能的能力，对手政府将获得战略优势。

在国际上，开放模型使异见人士与压制者处于同一起跑线上。例如：一个专制政府可能会使用生成式人工智能制造宣传，然后在社交媒体上传播。异见人士随后可能利用开放模型来区分真实照片和虚假照片，或者发现政府生成的社交媒体互动模式。

如果没有人工智能的访问权限，异见人士就无法自卫。

> e. 当模型的训练数据或相关源代码同时广泛可用时，这些益处如何改变（如果有的话）？

数据和源代码可以帮助微调模型。但是权重是价值大部分所在。

> 4\. 如果同时广泛公开的开放基础模型还有其他相关组件，会改变广泛公开模型权重带来的风险或益处吗？如果有，请列出并解释其影响。

权重以及将权重加载到工作模型中所需的代码，远远是最重要的部分。但还有一些其他有用的组件。大致按重要性排序：

+   数据：用于训练模型的数据可以评估是否存在统计偏差，或者重新用于微调。它可以用来训练新的、性能更好的竞争模型。

+   源代码：除了运行模型本身所需的内容（例如训练脚本、部署配置、微调代码等）外，任何源代码都对希望使用或改进模型的人有帮助。

+   方法论：模型的人类语言描述，包括技术选择的动机、所做的权衡以及对先前工作的引用，可以帮助其他人改进模型。

> 5\. 管理风险和放大双用基础模型广泛可用模型权重的益处涉及哪些与安全相关或更广泛的技术问题？

这里存在两类风险：

+   使用模型进行恶意操作的人

+   将模型以不安全方式部署的人，使其他人能够滥用模型

通过许可协议可以减轻恶意使用，但在技术层面上难以强制执行。模型可以经过训练，不生成某些类型的图像或文本，但在这里很难做出任何保证，微调可能会撤销这些工作。

不安全的部署可以通过提供部署参考代码、安全检查清单以及用于检测攻击提示和有害输出的附加软件来减轻。

> a. 如果有的话，哪些模型评估可以帮助确定广泛公开基础模型权重的风险或益处？

像[OWASP LLM Top 10](https://owasp.org/www-project-top-10-for-large-language-model-applications/llm-top-10-governance-doc/LLM_AI_Security_and_Governance_Checklist-v1.pdf) 这样的框架对于任何发布或部署LLM的人都是有帮助的。大部分建议也适用于其他生成式人工智能系统。

[这篇论文](https://arxiv.org/pdf/2302.08500.pdf) 建议审计三个层面：

对于广泛可用的模型，最后一步变得更加困难，因为应用程序的数量和范围没有限制。理想情况下，发布公司或第三方应提供自审计的指导。

> **b. 是否有有效的方法来围绕基础模型创建防护措施，以确保模型权重不可用，或者保护系统完整性或人类福祉（包括隐私），并减少在权重广泛可用的情况下的安全风险？**

在技术层面上，重要的是理解对模型访问的限制是一个二元选择。

如果一个模型被隐藏在一个 web API 后面，它可以完全由单一实体控制。防止模型权重泄露和保持源代码私有一样简单，尽管总有被入侵的风险（特别是由积极的国家级行动者）。

如果权重被公开分发，任何人都可以下载并与他人分享这些权重，并可以随意使用模型。他们可以修改模型以删除任何防护措施，并重新分发这些修改版。任何控制都可以相对容易地被绕过。

你可以通过*许可证*对一个开放模型添加限制，但你无法在技术上强制执行这些限制。

> **c. 未来开发有效保护措施的前景如何？**

我们可能可以从 DRM 策略中获得一些灵感，并找到使复制和重新分发 LLMs 变得更加困难的方法。但重要的是要理解，这只会是一种*阻碍*，而不是保证。

用一个比喻来说：这就像给你的前门加锁。它会让诚实的人保持诚实，但一个积极进攻的人可以打破窗户。

这也是 DRM 的问题：如果让用户观看视频，没有什么能阻止他们用摄像机录制视频并制作副本，即使你阻止他们获取原始文件的原始数据。

DRM 相对成功是因为希望阻止普通人与朋友分享文件。但 AI 模型的受众小而且高度技术化、高度积极—模型的价值远高于电影。类似 DRM 的策略不太可能奏效。

> d. **是否有办法重新获得对一个已广泛公开的基础模型权重的控制、限制对其使用？这些方法的大致成本是多少？它们的可靠性如何？**

没有。这些权重代表一个非常庞大的数学方程—一旦方程被知晓，任何人都可以实例化它并使用它。

> e. 如果有的话，可以考虑哪些安全存储技术或实践来防止模型权重的意外分发？

用于任何高价值知识产权的相同存储技术。具体来说：

> **f. 基础模型的哪些组成部分需要提供，并且需要提供给谁，以便分析、评估、认证或红队测试模型？** 在可能的情况下，请具体标识每种评估或评估类型以及需要提供的组件。

给予更多的访问权限，分析效果就越好。

在最严格的级别，可以给予带有API门控访问权限的渗透测试员或分析员。这将允许他们红队测试AI，例如尝试注入提示技术、尝试提取个人信息，测试偏见和幻觉等。

允许小团队访问源代码、数据和模型权重可以改善此测试。源代码可以接受静态分析，数据可以分析统计偏差，模型可以直接进行探索，不同部分可以被隔离或分析。这种更深入的访问水平还允许分析人员了解特定控制放置的位置。例如，模型本身是否经过训练以不输出危险文本，或者是否只是在顶部有一层薄膜阻止带有特定关键词的提示？

全面向公众开放所有这些内容是最好的。它允许独立的研究团队和学者探索该模型。对于开放模型，故障模式和缺陷的发现和披露率将大大提高。

> g. 是否有方法可以测试或验证模型权重？存在哪些方法论来审计模型权重和/或基础模型？

我们应该审计四个维度：

+   性能：模型如何完成其预期任务？

+   鲁棒性：模型如何应对意外输入？

+   真实性：模型多频率回应误导性输出？

+   安全性：模型抵抗输出有害内容的能力如何？

每个都可以通过自动化标准（即共同基准）进行评估，但也应该接受特别分析。

> 6\. 与开放基础模型相关的法律或商业问题或影响是什么？

法律问题集中在许可和执行上，以及模型可能被非法使用的潜力。

商业问题集中在使用成本、创新、竞争优势和市场参与障碍上。

> a. 开源软件政策与模型权重可用性的类比（或不类比）。**我们可以从开源软件、开放数据和其他“开放”倡议的历史和生态系统中学到哪些教训**，特别是基础模型的权重可用性？

开源软件的历史在这里将非常有教益。人工智能模型与传统软件的唯一区别是人类如何审视其逻辑。

特别是，我们应该期望开放模型能够：

+   提供巨大的经济价值

+   允许初创企业和小公司与大企业竞争

+   补充和支持商业解决方案

+   找到大型商业支持者，通过支持和补充软件增加价值。

+   为分发、部署、API等制定共同的行业标准。

+   向用户和其他利益相关者提供更高程度的透明度。

> b. **模型权重的广泛可用如何（如果有）改变更广泛经济的竞争动态**，特别是在医疗保健、市场营销和教育等行业？ 

公开可用的权重将使市场更加竞争和高效。

训练一款最先进的模型是极其昂贵且困难的。如果所有模型都保持封闭状态，那么很快就会形成寡头垄断，类似于云计算市场。

此外，封闭模型必须在分销商拥有的硬件上运行（除非分销商和许可证持有者之间有很高的信任度）。对于许多用例来说，这可能成本过高。

例如，本地高中可以通过在自己的硬件上运行LLM轻松地为学生和教师提供LLM；只需支付购买硬件的固定初始成本。但是，如果使用封闭的LLM，则需要向分销商支付持续的许可费用。医院和其他企业也是如此。

> c. **知识产权相关问题——例如，公开基础模型权重的许可条款如何（如果有）影响竞争、利益和风险？** 在公开模型权重使其广泛可用的背景下，哪些许可证最为突出？每种许可证的权衡利弊是什么？

许可条款是减少误用风险的重要途径。

限制性许可证——例如，禁止使用模型来代表公众人物，或生成暴力/性暗示图像——确实阻止了许多人从事这些行为。

它特别阻止人们以公开、用户友好的方式提供有害功能，而不会让自己卷入诉讼。

话虽如此，限制性许可证也会通过禁止与创建公司竞争的用例来减少竞争。但是，创建公司完全有权添加这些条款（例如Meta将LLaMa限制为少于7亿用户的产品）。

需要注意的是，模型的限制性许可证与开源和娱乐媒体的许可证工作方式非常相似——它们可以极大地*减少*伤害，但是坏演员总是可以忽视它们。

> d. 是否担心由不同不兼容的“开放”许可证引起的互操作性障碍，*例如*，对应用于AI组件的许可证具有冲突要求？是否有必要专门为基础模型权重标准化许可条款？现有的特定例子是否有用？

是的，标准许可证将是很大的帮助。我预计这将自然而然地发生，就像在开源生态系统中一样。

拥有许多个体许可证的主要问题在于，法律部门必须参与每一个许可证的阅读和接受。制定标准（例如MIT、Apache 2.0、AGPL等在开源中的标准）允许组织和律师发布全面的指导意见，使工程师可以在不需要每一次决策都牵涉法律部门的情况下采用新技术。

随着市场的发展，自然会统一几种不同的许可证。

> 7\. **当前或潜在的自愿、国内监管和国际机制是什么，以管理基础模型的风险并最大化其效益，尤其是具有广泛权重的基础模型？** 哪些实体应在治理的哪些方面发挥领导作用？

围绕AI的法规和法律有两种风味：

限制AI的某些使用是常识。许多现行法律已经适用于恶意使用AI（例如诽谤法阻止传播私人公民的虚假图像）。但在这里制定新立法将会有所帮助（例如禁止在广告中使用AI生成的图像，或者明确生效于模仿现代艺术家风格的作品的版权法）。

限制分发更加棘手。这将阻止守法实体访问技术，而罪犯则会继续找到获取技术的途径。这将完全阻止积极使用，却只在恶意使用面前设置了可逾越的障碍。

尽管如此，我们仍然可以监管使AI广泛可用于最终用户的平台和产品。我们可以确保它们设有防止滥用和响应安全问题的机制。

> a. 可以合理采取哪些安全、法律或其他措施来可靠防止基础模型权重的广泛获取，或限制它们的最终使用？

要监管模型的分发，我们需要监管两种不同类型的实体：

+   创作者组织，如OpenAI、Stability AI、Mistral AI、Meta等。

+   传播组织，如GitHub、Hugging Face、Dropbox等。

创作者组织需要披露谁被授予权重的访问权限，以及为保持这些权重私密性所采取的安全控制措施。

传播组织需要对下架通知做出回应，就像今天他们对侵犯版权行为的处理一样。

> b. 开放基础模型权重的广泛可用性可能如何促进或阻碍政府在AI监管方面的行动？

开放模型分发消除了AI的“瓶颈”，类似于开放互联网消除了瓶颈。

压迫性政府通常会故意创建互联网瓶颈，例如在抗议活动开始集结时关闭DNS，以防止社交媒体上的协调。

在封闭的AI世界中，类似的瓶颈更容易建立。例如，OpenAI可以关闭其服务器以阻止GPT的使用；而像LLaMa这样的开放模型则无法做到这一点。

> c. 何时（如果有的话），部署AI的实体应向用户或公众披露他们是否使用带有或不带有广泛可用权重的开放基础模型？

这不应成为必须要求，就像披露您的网站使用哪个PHP版本一样。

披露驱动您产品的AI给攻击者提供了额外的信息可以利用。如果您告诉世界您正在使用LLaMa，并且发现了LLaMa的新的提示注入攻击，攻击者可以立即开始将其应用于您的应用程序。

话虽如此，组织可能仍会选择披露这一点，例如出于营销或招聘目的。

> d. 美国政府在设置风险指标、制定最佳实践标准、支持或限制基础模型权重的可用性方面应扮演何种角色（如果有的话）？

美国政府应提供一套健全的指导方针，以安全和安全地部署和分发人工智能。

我们还需要制定法律，限制个人和组织使用生成式人工智能的方式。例如，我们可能会将在广告中使用AI生成的图像视为非法行为，或者加强诽谤法律以惩罚有害的AI生成图像。

美国政府不应限制向公众提供模型权重的可用性。这样做只会损害良好的行为者，对恶意行为者产生轻微的阻碍。它将阻碍安全研究人员和学术界，同时给予不良国家级行为者优势。

相反，美国政府应鼓励并资助开放模型的发展，这将增强美国经济并增强我们对配备AI的对手的准备能力。

> i. 其他政府或非政府机构（当前存在或不存在）是否应支持政府在此角色中的作用？在不同部门中是否应有所不同？

是的，政府应依赖非营利组织和第三方来：

+   制定评估AI安全性的标准

+   制定开放和闭合模型许可的标准

+   对开放和闭合模型进行审计，评估其性能、稳健性、真实性和安全性。

某些部门（特别是医疗保健和金融）将自然地制定自己的标准和评估框架。

> e. 模型托管服务（例如，HuggingFace、GitHub等）在使带有开放权重的双重用途模型更或更少可用方面应承担什么角色？托管服务是否应该托管不符合某些安全标准的模型？这些标准应由谁制定？

应当建立一个系统，用于通知托管服务存在安全漏洞、后门或私人信息的模型，就像今天针对源代码和其他媒体一样。

用户、政府官员和安全研究人员应能够向托管方报告特定模型，指出问题及其严重性。例如，如果发现模型已记住个人社保号码，或者如果它从其训练集中回响CSAM，托管组织应立即撤下模型，并通知维护者。如果模型存在较轻微的安全漏洞（例如提示注入漏洞），应通知维护者。

GitHub 今天在这方面做得很好——他们突出显示每个存储库中已知的漏洞列表（对存储库所有者保密），并迅速撤下侵犯版权或其他法律的项目。

政府应设立高标准，对需要立即撤下项目的问题设定一个门槛（例如泄露高度敏感信息），并允许托管提供者在此门槛以下进行自我管理。

> f. 当涉及共享开放基础模型的模型权重或与使用这些模型的公司签订合同时，政府是否应该设定不同的标准？

也许会有诱惑，将对模型的公共/私人访问限制，同时放宽对政府机构的限制。但这将阻碍大量经济和技术进步。这将使美国在全球经济中竞争力下降，并且更容易受到攻击。

政府可能也会有诱惑，限制开放模型的分发，而是向公众提供其自己批准的政府模型。但美国政府在建立、维护和部署如此规模的开源软件方面几乎没有历史或经验。政府管理的模型质量可能远不如私人管理的模型。

> g. 在这个问题上，美国应该与其他国家合作的重点是什么，哪些国家最重要？

我们需要讨论两类国家：盟友和对手。

我们应鼓励盟友采纳类似于我们自己的法规。无论哪个国家在模型监管方面采取最宽松的方法，都可能会超越其他国家。此外，一个国家的宽松法律可以被另一个国家的用户有效地利用——例如，一个美国公民可以使用VPN或飞到欧盟下载在美国受限制的模型。

我们还需要与对手合作。建立我们的研究团队与他们的、我们的安全团队与他们的等之间的合作关系，将有助于在涉及人工智能安全时保持世界的一致性。

例如，美国与中国公民在开源世界中的合作已经非常显著。美国政府应避免干扰这种合作。

> h. 从其他国家或其他社会体系中获得的哪些见解最有用？

在国际上有两个主要的监管推动：

美国应该更多地效仿欧盟而不是中国。我们应该努力减少对社会造成的伤害，而不是试图集中控制一个有前景的新技术。

> i. 政府或公司是否有有效的机制或程序可以用来决定双重用途基础模型或双重用途基础模型生态系统中适当的模型权重可用性程度？是否有办法有效地决定平衡开放AI部署的利益与风险？这可能包括负责任的能力扩展政策、准备框架等。

正如上文所述，“可用性程度”不是一个非常明智的想法。可用性要么是“开启”的，要么是“关闭”的，只有在这两个极端中有一些微小的变化。

也就是说，政府和创作者公司可以通过以下方式减少风险：

+   与研究人员和独立审计员合作评估模型

+   坦率地承认模型的局限性

+   发布限制社会有害使用的许可证的模型

+   发现风险和漏洞时要坦率披露

> j. **是否有特定的个人/实体应该或不应该访问**开放权重基础模型？如果是，为什么？在什么情况下？

不。我们不应该限制个人访问开放模型，就像我们不应该限制他们访问互联网或某些书籍一样。除非这个人被关押，否则这样做在技术上是不可行的。

> 8\. 面对不断变化的技术，并考虑到未预见的风险和收益，政府、公司和个人如何能够今天就对未来有用的开放基础模型做出决策或计划？

立法反对人工智能的不良*使用*，并根据新技术的光芒解释现有法律，将是未来证明人工智能监管最具规模化的方式。

专门关注特定技术（例如“大型语言模型”）或规格（例如每秒的权重数量或浮点运算）的立法几乎肯定会在几个月到几年内过时。

> **a. 如何平衡创新、竞争和安全等潜在的竞争利益？**

在创新和安全之间取得平衡将至关重要。

颁布和执行惩罚滥用人工智能的法律至关重要。如果我们根本不加管制，可能会看到对个人造成的伤害大幅增加。媒体将越来越“诱饵”驱动，因为公司调整图像、视频和文本以符合我们的个人心理。伪造的图像，包括诽谤和非同意的亲密图像，将会大量传播。虚假新闻将更容易被推出。公众信任将会侵蚀。

相反，如果我们因恐惧而过度规范，其他国家将迅速超越我们。新的AI应用将在欧盟和中国蓬勃发展，而美国公民则被迫局限于少数的封闭园地。考虑到AI不仅是新兴行业，而且是经济的加速剂，这将是灾难性的。

具有讽刺意味的是，过度规管AI的访问将损害*创新*和*安全*，因为它阻止学术界和研究人员对AI进行全面研究。对抗性政府和其他不良行为者将具有信息优势，他们可以利用此优势攻击美国或窃取知识产权。

> b. 注意到E.O. 14110授予商务部长调整门槛的能力，例如在行政命令中使用的1026个整数或浮点运算的截止点，这是否是一个有用的度量标准，用于长期缓解与广泛使用的模型权重相关的风险，特别是计算资源需求的风险？

在这里进行严格的技术截止日期是幼稚的。随着技术的发展，我们将找到方法使较小的模型达到相同的性能。所述的限制很快就会过时。

> c. 对于具有广泛可用模型权重的基础模型，是否有更强大的风险度量标准，以经受时间考验？我们应该考虑那些不符合双重用途基础模型定义的模型吗？

任何截止日期都应基于评估指标。例如，您可以限制对在HumanEval pass@1上得分超过90%的LLMs的访问——这将对未来技术发展具有鲁棒性。这些截止日期也适用于更广泛的AI技术。

然而，这种截止日期对竞争格局来说将是灾难性的，将极大地阻碍技术进步。

> 9\. 在分析具有广泛可用模型权重的双重用途基础模型的风险和利益时，我们还应考虑哪些其他问题、主题或相关技术进展？

生成式AI与虚拟和增强现实密切相关，也与传统媒体密切相关。它使媒体公司有能力制作极具吸引力的视频、图像和文本，并根据个体心理特征量身定制媒体。我们需要规范AI在娱乐和广告中的使用。

版权问题是另一个关注点。那些在音乐、绘画、写作等语料库上训练的模型可以模仿特定创作者的风格，根据需求创建竞争性媒体。在传播衍生的AI生成作品时，我们需要澄清版权法律。

我很高兴看到美国政府认真对待这个问题。新一代AI将是变革性的，并肯定会带来经济和社会动荡。智能而审慎的监管可以帮助减轻伤害并放大利益。

重点始终应该是对大型、以利润为驱动的组织进行监管，而不是个人、学者和研究人员。如果人工智能技术构成存在威胁，这种威胁来自国家和企业级行为者，而不是博士生和开源开发者。

当然，有些个人可能会以有害方式使用人工智能。他们可能会生成未经同意的亲密影像，发布社交媒体上的挑衅性虚假图像，或者利用人工智能放大他们的政治观点。这些问题最好通过现有法律（如诽谤法）和平台监管（如要求Facebook对传播虚假信息负责）来缓解。

再次强调，对社会伤害有两种方法：我们可以通过惩罚来减少它，或者可以通过监视和控制来尝试根除它。前者是自由社会的常态，而后者是压制的标志。

我理解政府为何有意禁止开放模型。但这样做对国家安全、经济和个人自由都将是灾难性的。

如果您同意，请于2024年3月27日之前向商务部[发送您的评论](https://www.regulations.gov/document/NTIA-2023-0009-0001)。

[分享](https://rbren.substack.com/p/banning-open-weight-models-would?utm_source=substack&utm_medium=email&utm_content=share&action=share)
