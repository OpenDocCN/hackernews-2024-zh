<!--yml

category: 未分类

date: 2024-05-27 13:12:06

-->

# Quanta Magazine

> 来源：[https://www.quantamagazine.org/how-do-machines-grok-data-20240412/](https://www.quantamagazine.org/how-do-machines-grok-data-20240412/)

随着网络的训练，它往往会学习更复杂的函数，训练数据中预期输出与实际输出之间的差异开始减少。更好的是，这种差异，即损失，对于测试数据也开始减少，测试数据是未在训练中使用的新数据。但是在某个时刻，模型开始过拟合，虽然训练数据上的损失持续下降，但测试数据上的损失开始上升。因此，通常研究人员会在这时停止训练网络。

这是当 OpenAI 的团队开始探索神经网络如何进行数学运算时的普遍智慧。他们使用了一个小型[transformer](https://www.quantamagazine.org/will-transformers-take-over-artificial-intelligence-20220310/) —— 这种网络架构最近已经彻底改变了大型语言模型的方式 —— 来进行不同类型的模块化算术运算，在这些运算中，你使用的数字集会循环回到起点。例如，在模 12 中，你可以把它想象成一个时钟：11 + 2 = 1。团队向网络展示了如何将两个数字 *a* 和 *b* 相加得到一个输出 *c*，在模 97 中（相当于一个有 97 个数字的时钟面）。然后，他们在未曾见过的 *a* 和 *b* 的组合上测试了 transformer，以查看它是否能够正确预测 *c*。

正如预期的那样，当网络进入过拟合阶段时，训练数据上的损失接近零（它开始记住它所见到的内容），而测试数据上的损失开始上升。它没有泛化。“然后有一天，我们走运了，” 团队负责人 Alethea Power 在[2022 年九月的一个会议上说道](https://youtu.be/gYGWFjMf9JA?t=1236)，“我说的走运，意思是健忘。”

负责训练网络的团队成员去度假时忘记停止训练。当这个版本的网络继续训练时，它突然在未见过的数据上变得精确。自动测试向团队的其他成员展示了这种意外的准确性，他们很快意识到网络找到了安排数字 *a* 和 *b* 的巧妙方式。在内部，网络以某种高维空间表示这些数字，但当研究人员将这些数字投射到二维空间并进行映射时，这些数字形成了一个圆形。

这是令人惊讶的。团队从未告诉模型它在进行模 97 的数学运算，甚至什么是模运算 —— 他们只是向它展示了算术的例子。模型似乎偶然发现了一些更深层次的分析解决方案 —— 一个可以推广到所有 *a* 和 *b* 组合的方程式，甚至超出了训练数据。网络已经理解，测试数据的准确率飙升到了100%。“这很奇怪，” Power 在听众面前说道。

团队使用不同任务和不同网络验证了结果。这一发现经得住考验。

## **关于时钟和比萨**

但网络找到的方程是什么呢？OpenAI 的论文没有说，但结果引起了南达的注意。“神经网络的一个核心谜团和让人恼火的事情是，它们擅长自己的工作，但默认情况下，我们不知道它们是如何工作的，”南达说道，他的工作重点是逆向工程训练好的网络，以找出它学到了什么算法。

南达对OpenAI的发现深感兴趣，决定拆解一个已经理解的神经网络。他设计了一个更简单的OpenAI神经网络版本，以便他可以仔细检查模型学习模数算术时的参数。他看到了相同的行为：过度拟合导致泛化和测试精度的突然提高。他的网络也在圆圈里安排数字。尽管需要一些努力，但南达最终弄清楚了原因。

虽然它在圆圈上表示数字，但这个网络并不像幼儿园小朋友看时钟一样简单地数数：它进行了一些复杂的数学操作。通过研究网络参数的值，[南达和同事们揭示](https://arxiv.org/abs/2301.05217)，它是通过对它们执行“离散傅里叶变换”来添加时钟数字 —— 使用三角函数如正弦和余弦来变换数字，然后使用三角恒等式来操作这些值以得到解。至少，这是他特定的网络正在做的事情。

当麻省理工学院的一个团队[跟进](https://arxiv.org/abs/2306.17844)南达的工作时，他们表明，理解神经网络不总是发现这种“时钟”算法。有时，网络反而会找到研究人员称之为“比萨”算法。这种方法想象一个被切成片并按顺序编号的比萨饼。要添加两个数字，想象从比萨的中心到所讨论数字的箭头，然后计算通过第一对箭头形成的角度的中分线。这条线经过比萨的某个片的中心：片的编号就是两个数字的和。这些操作也可以用三角函数和代数操作来写出，理论上与时钟方法一样准确。
