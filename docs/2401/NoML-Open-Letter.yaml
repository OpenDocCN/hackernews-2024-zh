- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-05-27 15:08:44'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: NoML Open Letter
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://noml.info/](https://noml.info/)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: NoML Open Letter
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A specification for those who want content searchable on search engines, but
    not used for machine learning.
  prefs: []
  type: TYPE_NORMAL
- en: Publishers need improved ways to indicate how they want content to be used in
    search and machine learning. Using robots.txt does not cover all use cases, and
    so a complementary approach is needed as proposed here. It is one which can be
    applied to individual webpages as desired, and can be preserved as such in datasets
    of web content.
  prefs: []
  type: TYPE_NORMAL
- en: '[Sign the open letter via Github,](https://github.com/Mojeek/noml-open-letter/issues/new?assignees=PrivacyDingus&labels=&projects=&template=sign.md&title=SIGN%3A+NAME)
    **or if you''d prefer to**, you can [sign the letter via email.](mailto:josh@mojeek.com?subject=Sign%20the%20open%20letter&body=Please%20provide%20your%20name,%20a%20URL%20if%20you%20would%20like%20your%20name%20to%20be%20hyperlinked%20somewhere,%20and%20an%20affiliation%20(company,%20organisation%20etc.)%20if%20relevant.%20If%20you%20are%20signing%20the%20letter%20as%20a%20company%20or%20organisation,%20consider%20attaching%20a%20logo)'
  prefs: []
  type: TYPE_NORMAL
- en: NoML Proposal
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Four cases
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Blocking a user agent in robots.txt (such as the search spider BingBot) for
    ML/AI products may also block potential visibility in associated search results.
    So if you don’t want your content to be used in products which leverage machine
    learning, you may also lose the benefits of being listed in search results. This
    is an unacceptable situation where content creators and publishers may be required
    to make a choice between “all or nothing”. We therefore need an approach which
    addresses four basic use cases as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **1.** Do use content for search Do use content for machine learning | **2.**
    Do use content for search **Do not** use content for machine learning |'
  prefs: []
  type: TYPE_TB
- en: '| **3\. Do not** use content for search Do use content for machine learning
    | **4\. Do not** use content for search **Do not** use content for machine learning
    |'
  prefs: []
  type: TYPE_TB
- en: Several suggestions for how to address this issue have been put forward. Here
    we propose something different, simpler and we think fairer. It is a simple adaptation
    of how meta and X-Robots tags are already used as we explain below in detail.
  prefs: []
  type: TYPE_NORMAL
- en: Explanation and Examples
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Since there are many companies scraping/crawling webpages in order to collect
    data, and often without identifying themselves, a way which addresses more than
    just search engine crawler bots and using robots.txt is needed. The proposal here
    is to add a new ‘noml’ value to the already-existing [meta and X-Robots tag](https://www.semrush.com/blog/robots-meta/).
    Meta robots tags are used already for search engine crawlers, so companies like
    Google that crawl for their search engine already follow requests made in this
    way. For example the `noindex` and `nofollow` values are used to instruct search
    engines on how to handle a webpage.
  prefs: []
  type: TYPE_NORMAL
- en: Also meta data is stored in Common Crawl whose crawled data is a very common,
    and often the biggest, source of data used in machine learning and to train AI
    models. The `noindex` attribute is used to tell search engines not to include
    a page in their search results, even though they can crawl the page. This tag
    is used when a webpage is not intended to be indexed by search engines. The `nofollow`
    attribute is used to tell search engines not to follow the links on a webpage.
    This attribute is used when a webpage that contains links should not be considered
    as endorsements or recommendations. Similarly the `noml` could be used to instruct
    any service (e.g. a search engine or AI builder) that any data from that page
    should not be used in machine learning.
  prefs: []
  type: TYPE_NORMAL
- en: 'This can be simply expressed for HTML pages using:'
  prefs: []
  type: TYPE_NORMAL
- en: '`<meta name="robots" content="noml">`'
  prefs: []
  type: TYPE_NORMAL
- en: 'and for non-HTML using:'
  prefs: []
  type: TYPE_NORMAL
- en: '`X-Robots-Tag: noml`'
  prefs: []
  type: TYPE_NORMAL
- en: 'Just as for the meta tag, where the name “robots” refers to all user-agent
    tokens, you can also identify individual user-agents. For example, you can currently
    request that Google does not include a page in the search results:'
  prefs: []
  type: TYPE_NORMAL
- en: '`<meta name="googlebot" content="noindex">`'
  prefs: []
  type: TYPE_NORMAL
- en: 'and similarly you could request that Google **does include a page in their
    search** index, but **does not use the data for machine learning** with:'
  prefs: []
  type: TYPE_NORMAL
- en: '`<meta name="googlebot" content="noml">`'
  prefs: []
  type: TYPE_NORMAL
- en: 'and request that Microsoft **does not include** a page in their Bing search
    index and **does not use** the page for training, with:'
  prefs: []
  type: TYPE_NORMAL
- en: '`<meta name="bingbot" content="noindex, noml">`'
  prefs: []
  type: TYPE_NORMAL
- en: 'you might also request that OpenAI, for example, does not use the page for
    machine learning with:'
  prefs: []
  type: TYPE_NORMAL
- en: '`<meta name="gptbot" content="noml">`'
  prefs: []
  type: TYPE_NORMAL
- en: 'however in this case, since OpenAI is not operating a search engine, you could
    (also) block them from crawling in the [robots.txt](https://en.wikipedia.org/wiki/Robots.txt)
    file using:'
  prefs: []
  type: TYPE_NORMAL
- en: '`User-agent: gptbot`'
  prefs: []
  type: TYPE_NORMAL
- en: '`Disallow: /`'
  prefs: []
  type: TYPE_NORMAL
- en: 'Further examples of are shown below, for HTML and non-HTML:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | Include page in search | Follow links | Use in machine learning |'
  prefs: []
  type: TYPE_TB
- en: '| `<meta name="robots">`'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: '`X-Robots-Tag:` | ✓ | ✓ | ✓ |'
  prefs: []
  type: TYPE_NORMAL
- en: '| `<meta name="robots" content="noindex">`'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: '`X-Robots-Tag: noindex` | ✕ | ✓ | ✓ |'
  prefs: []
  type: TYPE_NORMAL
- en: '| `<meta name="robots" content="nofollow">`'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: '`X-Robots-Tag: nofollow` | ✓ | ✕ | ✓ |'
  prefs: []
  type: TYPE_NORMAL
- en: '| `<meta name="robots" content="noml">`'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: '`X-Robots-Tag: noml` | ✓ | ✓ | ✕ |'
  prefs: []
  type: TYPE_NORMAL
- en: '| `<meta name="robots" content="noindex, nofollow">`'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: '`X-Robots-Tag: noindex, nofollow` | ✕ | ✕ | ✓ |'
  prefs: []
  type: TYPE_NORMAL
- en: '| `<meta name="robots" content="noindex, nofollow, noml">`'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: '`X-Robots-Tag: noindex, nofollow, noml` | ✕ | ✕ | ✕ |'
  prefs: []
  type: TYPE_NORMAL
- en: Search Engine API Usage
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Crawler based search engines such as Mojeek, Bing and Google, also provide their
    results to other search engines and services via APIs. We propose that these search
    engines, as Mojeek would do, include the ‘noml’ directive within their API response.
    Search API providers should make it part of their terms of API service that results
    labelled as such are not used for machine learning purposes by API end users.
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This proposed solution achieves the desired goals, by simply adding one additional
    value to already existing methods, and without necessarily requiring more user-agents
    to be used.
  prefs: []
  type: TYPE_NORMAL
- en: With this proposal creators and publishers can indicate separately whether they
    want content to be findable in search engines and/or used for machine learning.
  prefs: []
  type: TYPE_NORMAL
- en: Share the open letter
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Dear AI companies, Crawlers, Search Engines, ML Projects, Scrapers etc.
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We, the undersigned, support the adoption of this proposal, which enables creators,
    publishers, and all other content contributors on the web to indicate whether
    their content can be utilized for machine learning training and applications.
  prefs: []
  type: TYPE_NORMAL
- en: Signatures from organisations and companies
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Signatures from individuals
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
