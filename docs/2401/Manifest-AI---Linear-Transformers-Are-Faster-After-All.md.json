["```\ndef LT_attention(Q, K, V):\n \"\"\"\n Shapes of inputs are\n Q: [t, d]  K: [t, d]  V: [t, d]\n Shapes of outputs are\n Y: [t, d]\n \"\"\"\n t, d = Q.shape\n Y_list = []\n for i in range(t):           # loop cost: O(t^2 d)\n Y_i = zeros(d)\n Q_i = Q[i]\n for j in range(i):       # loop cost: O(id)\n A_ij = inner(K[j], Q_i)  # cost: O(d)\n Y_i += A_ij * V[j]   # cost: O(d)\n Y_list.append(Y_i)\n return stack(Y_list)\n```", "```\ndef LT_state(Q, K, V):\n \"\"\"\n Shapes of inputs are\n Q: [t, d]  K: [t, d]  V: [t, d]\n Shapes of outputs are\n Y: [t, d]\n \"\"\"\n t, d = Q.shape\n S_i = zeros(d, d) # shape [d,d]\n Y_list = []\n for i in range(t):        # loop cost: O(t d^2)\n S_i += outer(K[i], V[i]) # cost: O(d^2)\n Y_i = S_i @ Q[i]      # cost: O(d^2)\n Y_list.append(Y_i)\n return stack(Y_list)\n```", "```\ndef LT_attention_parallel_no_flash(Q, K, V):\n \"\"\"\n Shapes of inputs are\n Q: [t, d]  K: [t, d]  V: [t, d]\n Shapes of outputs are\n Y: [t, d]\n \"\"\"\n t = Q.shape[0]\n M = causal_mask(t)\n A_raw = Q @ K.T  # cost O(t^2 d)\n A = A_raw * M    # cost O(t^2)\n Y = A @ V        # cost O(t^2 d)\n return Y\n```", "```\ndef LT_state_parallel(Q, K, V):\n \"\"\"\n Shapes of inputs are\n Q: [t, d]  K: [t, d]  V: [t, d]\n Shapes of outputs are\n Y: [t, d]\n \"\"\"\n P = V[:,:,None] @ K[:,None,:]  # cost: O(t d^2)\n S = cumsum(P, axis=0)          # cost: O(log_2(t) t d^2)\n Y = S @ Q[:,:,None]            # cost: O(t d^2)\n return Y[:,:0]\n```", "```\ndef LT_attention_with_initial_state(S, Q, K, V):\n \"\"\"\n Shapes of inputs are\n S: [d, d]  Q: [c, d]  K: [c, d]  V: [c, d]\n Shapes of outputs are\n Y: [c, d]\n \"\"\"\n Y_state = Q @ S                               # cost O(c d^2)\n Y_attention = LT_attention_parallel(Q, K, V)  # cost O(c^2 d)\n Y = Y_state + Y_attention                     # cost O(cd)\n return Y\n\ndef LT_chunked(Q, K, V, c):\n \"\"\"\n Shapes of inputs are\n Q: [t, d]  K: [t, d]  V: [t, d], c: int\n Shapes of outputs are\n Y: [t, d]\n \"\"\"\n t, d = Q.shape\n assert t % c == 0\n Q_, K_, V_ = [arr.reshape(t//c, c, d)\n `               for arr in [Q,K,V]]\n P_ = K_.transpose([0,2,1]) @ V_  # cost O(t d^2)\n S_ = cumsum(P_, axis=0) - P_     # cost O(log_2(t/c)(t/c)d^2)\n Y_ = vmap(LT_attention_with_initial_state, axis=0)(\n S_, Q_, K_, V_)      # cost O(td^2 + tcd)\n return Y_.reshape(t, d)\n```", "```\n@misc{buckman2024,\n  author = {Buckman, Jacob and Gelada, Carles},\n  publisher = {Manifest AI},\n  title = {Linear {Transformers} {Are} {Faster} {After} {All}},\n  date = {2024-01-05},\n  langid = {en}\n} \n```"]