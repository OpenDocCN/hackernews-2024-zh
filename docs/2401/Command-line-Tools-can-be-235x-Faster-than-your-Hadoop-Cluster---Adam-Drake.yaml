- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 类别：未分类
- en: 'date: 2024-05-27 15:12:48'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-05-27 15:12:48
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: Command-line Tools can be 235x Faster than your Hadoop Cluster - Adam Drake
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 命令行工具可能比你的 Hadoop 集群快 235 倍 - Adam Drake
- en: 来源：[https://adamdrake.com/command-line-tools-can-be-235x-faster-than-your-hadoop-cluster.html](https://adamdrake.com/command-line-tools-can-be-235x-faster-than-your-hadoop-cluster.html)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://adamdrake.com/command-line-tools-can-be-235x-faster-than-your-hadoop-cluster.html](https://adamdrake.com/command-line-tools-can-be-235x-faster-than-your-hadoop-cluster.html)
- en: Command-line Tools can be 235x Faster than your Hadoop Cluster
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 命令行工具可能比你的 Hadoop 集群快 235 倍
- en: As I was browsing the web and catching up on some sites I visit periodically,
    I found a cool article from [Tom Hayden](https://tomhayden3.com/2013/12/27/chess-mr-job/)
    about using [Amazon Elastic Map Reduce](https://aws.amazon.com/elasticmapreduce/)
    (EMR) and [mrjob](https://github.com/Yelp/mrjob) in order to compute some statistics
    on win/loss ratios for chess games he downloaded from the [millionbase archive](https://www.top-5000.nl/pgn.htm),
    and generally have fun with EMR. Since the data volume was only about 1.75GB containing
    around 2 million chess games, I was skeptical of using Hadoop for the task, but
    I can understand his goal of learning and having fun with mrjob and EMR. Since
    the problem is basically just to look at the result lines of each file and aggregate
    the different results, it seems ideally suited to stream processing with shell
    commands. I tried this out, and for the same amount of data I was able to use
    my laptop to get the results in about 12 seconds (processing speed of about 270MB/sec),
    while the Hadoop processing took about 26 minutes (processing speed of about 1.14MB/sec).
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 当我浏览网页并定期访问一些站点时，我发现了 [Tom Hayden](https://tomhayden3.com/2013/12/27/chess-mr-job/)
    的一篇有趣的文章，他讲述了如何使用 [Amazon Elastic Map Reduce](https://aws.amazon.com/elasticmapreduce/)
    (EMR) 和 [mrjob](https://github.com/Yelp/mrjob) 计算他从 [millionbase archive](https://www.top-5000.nl/pgn.htm)
    下载的国际象棋比赛数据的胜负比率，并在 EMR 上玩得开心。由于数据量只有约 1.75GB，包含约 200 万场国际象棋比赛，我对使用 Hadoop 进行任务处理持怀疑态度，但我能理解他学习和在
    mrjob 和 EMR 上玩耍的目标。由于问题基本上只是查看每个文件的结果行并汇总不同的结果，因此使用 shell 命令进行流处理似乎是理想的选择。我尝试了这个方法，对于相同数量的数据，我能够使用我的笔记本电脑在约
    12 秒内获得结果（处理速度约为 270MB/秒），而 Hadoop 处理大约需要 26 分钟（处理速度约为 1.14MB/秒）。
- en: After reporting that the time required to process the data with 7 c1.medium
    machine in the cluster took 26 minutes, Tom remarks
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在报告了在集群中使用 7 台 c1.medium 机器处理数据所需的时间为 26 分钟后，Tom 补充道
- en: This is probably better than it would take to run serially on my machine but
    probably not as good as if I did some kind of clever multi-threaded application
    locally.
  id: totrans-9
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 这可能比在我的机器上串行运行要好，但可能不如如果我在本地使用某种聪明的多线程应用程序好。
- en: This is absolutely correct, although even serial processing may beat 26 minutes.
    Although Tom was doing the project for fun, often people use Hadoop and other
    so-called *Big Data (tm)* tools for real-world processing and analysis jobs that
    can be done faster with simpler tools and different techniques.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 这绝对是正确的，尽管甚至串行处理可能会比 26 分钟要快。尽管 Tom 是出于娱乐而进行这个项目，但人们经常使用 Hadoop 和其他所谓的 *Big
    Data (tm)* 工具进行可以使用更简单的工具和不同技术更快地完成的真实世界处理和分析工作。
- en: One especially under-used approach for data processing is using standard shell
    tools and commands. The benefits of this approach can be massive, since creating
    a data pipeline out of shell commands means that all the processing steps can
    be done in parallel. This is basically like having your own [Storm](https://storm-project.net/)
    cluster on your local machine. Even the concepts of Spouts, Bolts, and Sinks transfer
    to shell pipes and the commands between them. You can pretty easily construct
    a stream processing pipeline with basic commands that will have extremely good
    performance compared to many modern *Big Data (tm)* tools.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 数据处理的一个特别少用的方法是使用标准的 shell 工具和命令。这种方法的好处是巨大的，因为使用 shell 命令创建数据管道意味着所有处理步骤都可以并行进行。这基本上就像在本地机器上拥有自己的
    [Storm](https://storm-project.net/) 集群。甚至 Spouts、Bolts 和 Sinks 的概念也可以转移到 shell
    管道和它们之间的命令中。你可以相当容易地用基本命令构建一个流处理管道，它将与许多现代 *Big Data (tm)* 工具相比具有极好的性能。
- en: An additional point is the batch versus streaming analysis approach. Tom mentions
    in the beginning of the piece that after loading 10000 games and doing the analysis
    locally, that he gets a bit short on memory. This is because all game data is
    loaded into RAM for the analysis. However, considering the problem for a bit,
    it can be easily solved with streaming analysis that requires basically no memory
    at all. The resulting stream processing pipeline we will create will be over 235
    times faster than the Hadoop implementation and use virtually no memory.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个要点是批处理与流处理分析方法的比较。Tom 在文章开头提到，加载了 10000 场比赛并在本地进行分析后，他的内存有点不够用了。这是因为所有的游戏数据都被加载到
    RAM 中进行分析。然而，仔细考虑了一下这个问题，它可以很容易地通过流式分析来解决，这几乎不需要任何内存。我们将创建的结果流处理流水线将比 Hadoop 实现快
    235 倍，并且几乎不使用任何内存。
- en: The first step in the pipeline is to get the data out of the PGN files. Since
    I had no idea what kind of format this was, I checked it out on [Wikipedia](https://en.wikipedia.org/wiki/Portable_Game_Notation).
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，在流水线中的第一步是从 PGN 文件中获取数据。由于我不知道这是什么格式，我在[Wikipedia](https://en.wikipedia.org/wiki/Portable_Game_Notation)上查看了一下。
- en: '[PRE0]'
  id: totrans-14
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: We are only interested in the results of the game, which only have 3 real outcomes.
    The 1-0 case means that white won, the 0-1 case means that black won, and the
    1/2-1/2 case means the game was a draw. There is also a *-* case meaning the game
    is ongoing or cannot be scored, but we ignore that for our purposes.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 我们只对游戏的结果感兴趣，这只有 3 种真实结果。1-0 表示白棋赢，0-1 表示黑棋赢，而 1/2-1/2 表示游戏是平局。还有一个 *-* 表示游戏正在进行中或无法计分，但我们对此不感兴趣。
- en: The first thing to do is get a lot of game data. This proved more difficult
    than I thought it would be, but after some looking around online I found a git
    repository on GitHub from [rozim](https://github.com/rozim/ChessData) that had
    plenty of games. I used this to compile a set of 3.46GB of data, which is about
    twice what Tom used in his test. The next step is to get all that data into our
    pipeline.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 首先要做的是获取大量的游戏数据。这比我想象的要困难得多，但在网上找了一些之后，我发现了来自[rozim](https://github.com/rozim/ChessData)的
    GitHub 上的一个 git 仓库，里面有大量的游戏数据。我用这个来编译了一组大小为 3.46GB 的数据，大约是 Tom 在他的测试中所用数据的两倍。下一步是将所有这些数据放入我们的流水线中。
- en: '*If you are following along and timing your processing, don’t forget to clear
    your OS page cache as otherwise you won’t get valid processing times.*'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: '*如果你在跟踪并计时你的处理过程，请不要忘记清除操作系统的页面缓存，否则你将无法获得有效的处理时间。*'
- en: Shell commands are great for data processing pipelines because you get parallelism
    for free. For proof, try a simple example in your terminal.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: Shell 命令非常适合数据处理流水线，因为你可以免费获得并行性。要证明，请在你的终端中尝试一个简单的例子。
- en: '[PRE1]'
  id: totrans-19
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Intuitively it may seem that the above will sleep for 3 seconds and then print
    `Hello world` but in fact both steps are done at the same time. This basic fact
    is what can offer such great speedups for simple non-IO-bound processing systems
    capable of running on a single machine.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 直观地看，上述代码会休眠 3 秒，然后打印 `Hello world`，但实际上两个步骤是同时进行的。这个基本事实是为什么对于简单的非 IO 限制的处理系统能够在单个机器上运行时提供如此大的加速。
- en: Before starting the analysis pipeline, it is good to get a reference for how
    fast it could be and for this we can simply dump the data to `/dev/null`.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 在开始分析流水线之前，最好先了解一下其处理速度可能有多快，为此我们可以将数据简单地转储到 `/dev/null`。
- en: In this case, it takes about 13 seconds to go through the 3.46GB, which is about
    272MB/sec. This would be a kind of upper-bound on how quickly data could be processed
    on this system due to IO constraints.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，处理 3.46GB 大约需要 13 秒，大约是 272MB/sec。由于 IO 约束，这将是系统上处理数据的速度的一种上限。
- en: Now we can start on the analysis pipeline, the first step of which is using
    `cat` to generate the stream of data.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以开始分析流水线了，其第一步是使用 `cat` 生成数据流。
- en: Since only the result lines in the files are interesting, we can simply scan
    through all the data files, and pick out the lines containing ‘Results’ with `grep`.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 由于文件中只有结果行是有趣的，我们可以简单地扫描所有数据文件，并使用 `grep` 选取包含 ‘Results’ 的行。
- en: '[PRE2]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: This will give us only the `Result` lines from the files. Now if we want, we
    can simply use the `sort` and `uniq` commands in order to get a list of all the
    unique items in the file along with their counts.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 这将只给我们带来文件中的 `Result` 行。现在如果我们想的话，我们可以简单地使用 `sort` 和 `uniq` 命令来获取文件中所有唯一项目的列表以及它们的计数。
- en: '[PRE3]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: This is a very straightforward analysis pipeline, and gives us the results in
    about 70 seconds. While we can certainly do better, assuming linear scaling this
    would have taken the Hadoop cluster approximately 52 minutes to process.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个非常简单的分析管道，并在大约70秒内给我们结果。虽然我们肯定可以做得更好，但是假设线性扩展，这将需要Hadoop集群大约52分钟来处理。
- en: In order to reduce the speed further, we can take out the `sort | uniq` steps
    from the pipeline, and replace them with AWK, which is a wonderful tool/language
    for event-based data processing.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 为了进一步提高速度，我们可以从管道中去除`sort | uniq`步骤，并用AWK替换它们，这是一个非常适用于事件驱动数据处理的精彩工具/语言。
- en: '[PRE4]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: This will take each result record, split it on the hyphen, and take the character
    immediately to the left, which will be a 0 in the case of a win for black, a 1
    in the case of a win for white, or a 2 in the case of a draw. Note that `$0` is
    a built-in variable that represents the entire record.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 这将获取每个结果记录，将其拆分为连字符，并取左侧的字符，这将在黑色赢得的情况下为0，在白色赢得的情况下为1，在平局的情况下为2。注意，`$0`是一个代表整个记录的内置变量。
- en: This reduces the running time to approximately 65 seconds, and since we’re processing
    twice as much data this is a speedup of around 47 times.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 这将运行时间减少到大约65秒，因为我们处理的数据量增加了一倍，所以这是大约47倍的加速。
- en: So even at this point we already have a speedup of around 47 with a naive local
    solution. Additionally, the memory usage is effectively zero since the only data
    stored is the actual counts, and incrementing 3 integers is almost free in memory
    space terms. However, looking at `htop` while this is running shows that `grep`
    is currently the bottleneck with full usage of a single CPU core.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，即使在这一点上，我们已经通过一个简单的本地解决方案实现了大约47倍的加速。此外，由于唯一存储的数据是实际计数，而且在内存空间方面增加3个整数几乎是免费的，因此内存使用量实际上为零。然而，查看`htop`运行时，可以看到`grep`目前是瓶颈，完全使用了一个CPU核心。
- en: This problem of unused cores can be fixed with the wonderful `xargs` command,
    which will allow us to parallelize the `grep`. Since `xargs` expects input in
    a certain way, it is safer and easier to use `find` with the `-print0` argument
    in order to make sure that each file name being passed to `xargs` is null-terminated.
    The corresponding `-0` tells `xargs` to expected null-terminated input. Additionally,
    the `-n` how many inputs to give each process and the `-P` indicates the number
    of processes to run in parallel. Also important to be aware of is that such a
    parallel pipeline doesn’t guarantee delivery order, but this isn’t a problem if
    you are used to dealing with distributed processing systems. The `-F` for `grep`
    indicates that we are only matching on fixed strings and not doing any fancy regex,
    and can offer a small speedup, which I did not notice in my testing.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 这个未使用的核心问题可以通过精彩的`xargs`命令解决，它将允许我们并行化`grep`。由于`xargs`期望以某种方式输入，因此最好使用`find`与`-print0`参数，以确保传递给`xargs`的每个文件名都是空终止的。相应的`-0`告诉`xargs`期望空终止输入。此外，`-n`表示每个进程给多少个输入，`-P`表示要并行运行的进程数。还需要注意的是，这样的并行管道不能保证交付顺序，但如果您习惯于处理分布式处理系统，则这不是问题。`grep`的`-F`表示我们仅匹配固定字符串而不执行任何花哨的正则表达式，并且可能会提供小的加速，我在测试中没有注意到。
- en: '[PRE5]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: This results in a run time of about 38 seconds, which is an additional 40% or
    so reduction in processing time from parallelizing the `grep` step in our pipeline.
    This gets us up to approximately 77 times faster than the Hadoop implementation.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 这导致运行时间约为38秒，这比在我们的管道中并行化`grep`步骤后的处理时间额外减少了大约40%左右。这使我们的速度大约比Hadoop实现快77倍。
- en: Although we have improved the performance dramatically by parallelizing the
    `grep` step in our pipeline, we can actually remove this entirely by having `awk`
    filter the input records (lines in this case) and only operate on those containing
    the string “Result”.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管我们通过在我们的管道中并行化`grep`步骤大大改善了性能，但实际上我们可以通过让`awk`过滤输入记录（在这种情况下是行），并且仅对包含字符串“Result”的记录进行操作，从而完全删除它。
- en: '[PRE6]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: You may think that would be the correct solution, but this will output the results
    of **each** file individually, when we want to aggregate them all together. The
    resulting correct implementation is conceptually very similar to what the MapReduce
    implementation would be.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能认为这将是正确的解决方案，但这将输出**每个**文件的结果，而我们想要将它们全部聚合在一起。结果的正确实现在概念上与MapReduce实现非常相似。
- en: '[PRE7]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: By adding the second awk step at the end, we obtain the aggregated game information
    as desired.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 通过在最后添加第二个awk步骤，我们就可以得到所需的汇总游戏信息。
- en: This further improves the speed dramatically, achieving a running time of about
    18 seconds, or about 174 times faster than the Hadoop implementation.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 进一步显著提高了速度，达到了约18秒的运行时间，约为Hadoop实现的174倍。
- en: However, we can make it a bit faster still by using [mawk](https://invisible-island.net/mawk/mawk.html),
    which is often a drop-in replacement for `gawk` and can offer better performance.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 不过，我们可以通过使用[mawk](https://invisible-island.net/mawk/mawk.html)使其运行速度更快一些，它经常可以作为`gawk`的替代品，并提供更好的性能。
- en: '[PRE8]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: This `find | xargs mawk | mawk` pipeline gets us down to a runtime of about
    12 seconds, or about 270MB/sec, which is around 235 times faster than the Hadoop
    implementation.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 这个`find | xargs mawk | mawk`管道使我们的运行时间缩短到约12秒，约为270MB/sec，比Hadoop实现快约235倍。
- en: Hopefully this has illustrated some points about using and abusing tools like
    Hadoop for data processing tasks that can better be accomplished on a single machine
    with simple shell commands and tools. If you have a huge amount of data or really
    need distributed processing, then tools like Hadoop may be required, but more
    often than not these days I see Hadoop used where a traditional relational database
    or other solutions would be far better in terms of performance, cost of implementation,
    and ongoing maintenance.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 希望这些例子能够说明一些关于在数据处理任务中使用和滥用Hadoop等工具的观点，这些任务在单台计算机上使用简单的shell命令和工具就可以更好地完成。如果你有大量数据或确实需要分布式处理，那么可能需要使用Hadoop等工具，但现在我经常看到Hadoop被用在传统的关系型数据库或其他解决方案在性能、实施成本和持续维护方面更好的地方。
- en: <data class="p-bridgy-omit-link" value="false"></data>
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: <data class="p-bridgy-omit-link" value="false"></data>
