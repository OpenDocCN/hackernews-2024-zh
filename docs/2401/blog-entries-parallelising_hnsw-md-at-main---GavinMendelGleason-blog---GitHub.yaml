- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-05-27 14:26:26'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: blog/entries/parallelising_hnsw.md at main · GavinMendelGleason/blog · GitHub
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://github.com/GavinMendelGleason/blog/blob/main/entries/parallelising_hnsw.md](https://github.com/GavinMendelGleason/blog/blob/main/entries/parallelising_hnsw.md)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: HNSW (Hierarchical Navigable Small World graphs) were introduced in an excellent
    and very readable paper [Yu A. Malkov, D. A. Yashunin](https://arxiv.org/abs/1603.09320)
    which has given rise to a large number of implementations which are at the core
    of many popular vector databases.
  prefs: []
  type: TYPE_NORMAL
- en: The HNSW is a method of searching for vectors in a dataset which are *close*
    to a given query vector. The basic idea of an HNSW is to make a series of proximity
    graphs, organized in a stack which allows us to zoom-in as we approach ever closer
    neighborhoods.
  prefs: []
  type: TYPE_NORMAL
- en: The top layer of the HNSW has relatively few elements, and we can quickly find
    our best match in this layer greedily, and then we drill down to the next layer
    down. Each layer down is an order of magnitude larger, but contains all of the
    points from above. In this we can we zoom in on an even closer alternative, and
    drill down another layer.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, when we reach the bottom, we search around a bit for candidates in
    our neighborhood and end up with an priority queue of candidates ordered by distance.
  prefs: []
  type: TYPE_NORMAL
- en: The HNSW approach is essentially probabilistic. We try to find candidates to
    match by choosing randomly, in the hopes that being pretty well connected via
    super-nodes, we will not end up getting too far from matches in our local minimum.
  prefs: []
  type: TYPE_NORMAL
- en: This makes the structure very appealing from the point of view of incremental
    indexing. We can simply roll a die to see which layer we are first inserted into,
    with a distribution putting everything in the lowest layer, and increasingly seldomly
    putting it in ever higher layers in a log-like way. This should, hopefully, be
    somewhat self-balancing as new vectors come into the system we promote them to
    supernodes of the tree for routing in a random manner.
  prefs: []
  type: TYPE_NORMAL
- en: This approach of creating tiered proximity graphs radically cuts down on the
    computation which is necessary to do full distance calculations over a set as
    new elements are added. Vector distance computations over large vectors, such
    as exist in embeddings, can become prohibitavely expensive very quickly when the
    number of vectors is large.
  prefs: []
  type: TYPE_NORMAL
- en: Parallelising the approach
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](#parallelising-the-approach)'
  prefs: []
  type: TYPE_NORMAL
- en: The algorithm outlined in the Malkov, Yashunin paper is however not very ameanable
    to parallelisation. It relies on the previously constructed index to search its
    way to good neighbors. There is a lot of opportunity for contention over resources
    when parallelising it naively (as the authors have found).
  prefs: []
  type: TYPE_NORMAL
- en: However, if we have a large batch job of vectors to add simultaneously, there
    are very clear methods of building the index faster using parallelism. This can
    play well with the above incremental approach. In addition we can think of a mixed-incremental
    batch job, which is somewhere intermediate between the two approaches.
  prefs: []
  type: TYPE_NORMAL
- en: Constructing One Layer at a Time
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[](#constructing-one-layer-at-a-time)'
  prefs: []
  type: TYPE_NORMAL
- en: The simplest, batch parallel approach assuming we are creating an index *ex
    nihilo* constructs an HNSW *one layer at a time*.
  prefs: []
  type: TYPE_NORMAL
- en: We do this by first choosing a number of layers based on the input indexing
    data *N*. This can be done with a calculation that takes the log of N at a base
    M related to the number of elements in a neighborhood.
  prefs: []
  type: TYPE_NORMAL
- en: Next we segment our input into overlapping slices, each one including a prefix
    of the data of length `M^i` for each layer `i` starting with `i=0`. At each layer
    we use our single layer generation.
  prefs: []
  type: TYPE_NORMAL
- en: This approach of layer at a time generation is much more amenable to parallelism,
    especially as the layer sizes increase. However, it is also more sensitive to
    the order in which the data is ingested. For this reason it is necessary to ensure
    a randomisation of the input batch or you can end up with a very poorly constructed
    layer.
  prefs: []
  type: TYPE_NORMAL
- en: 'The (rusty) pseudocode might be written as:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'This single layer generation consists of the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: In parallel, iterate over all vectors in this layer, looking for their closest
    matches in the layer above.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Partition the set according to these super-neighborhoods. If we are the first
    layer to be constructed, this is simply one partition.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Go through each partition node by node, select candidate neighbors from this
    partition, and the next best partition, and the next best after that, with exponentially
    decaying probability. Truncate this to the neighborhood size (M). This will build
    up our initial neighborhood set of distances for this node.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Go through every node and make neighborhoods bidirectional (unless evicted as
    our distance exceeds the worst case within a neighborhood max size.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: For each node, in parallel, write the neighborhoods into the neighborhood vector.
    This can be done in parallel since each neighborhood is independent and of fixed
    size.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Unfortunately the above algorithm can leave some things unreachable. We found
    that on small data sets, the recall was quite high, often exceeding .999, however
    once we reach the 10s of millions, the algorithm can leave vectors orphaned, and
    recall can drop as below 0.90 (depending on the size of the search queue).
  prefs: []
  type: TYPE_NORMAL
- en: 'Luckily there is a strategy for dealing with the stragglers, using the following
    algorithm:'
  prefs: []
  type: TYPE_NORMAL
- en: Starting from layer zero, and proceeding upwards, find all points which are
    disconnected or distant from a super-node.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Segment these into neigbhorhoods and choose a representative super-node for
    promotion, which we add to the next layer up.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Repeat for the layer above.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Using this strategy ensures that all vectors are routable through *some* path,
    though it's possible that greedy routing will lead to some loss of recall despite.
  prefs: []
  type: TYPE_NORMAL
- en: The search mechanism works exactly as with the HNSW, except that we don't need
    all of the layers to be constructed yet. We simply stop searching when we've gotten
    to the last layer that has been constructed. This allows us to construct neighborhoods
    by using the pre-existing routing of the layers above.
  prefs: []
  type: TYPE_NORMAL
- en: Incrementally Extending The Index
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[](#incrementally-extending-the-index)'
  prefs: []
  type: TYPE_NORMAL
- en: Building *ex nihilo* is not the only way you want to build an index in practice.
    One of the big advantages to the HNSW algorithm is its incremental nature. In
    fact, we can just reuse the original algorithm to extend the structure and it
    works perfectly well.
  prefs: []
  type: TYPE_NORMAL
- en: However, there is another common mode of operation, which is a batch update
    - where we have inserts, updates and deletes.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can create a sort of half-way house between the two algorithms, with either
    of the two as degenerate cases as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Sample from an exponential distribution defined by the neighborhood size M,
    N times, where N is the number of new vectors. Create a count of these bins and
    use it as the prefix counts per layer of the vector.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Create any new layers (that are higher than previous) using the prefixes (if
    such exist) with the generate_layer algorithm above.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Use a modified version of the generate_layer algorithm as above performing all
    steps for only the new nodes.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Append the new nodes and new neighborhoods to the end of the existing layer.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: HNSW creates a very nice indexing structure, but the method of construction
    does not need to be a trickle down approach. It is quite possible to perform the
    steps in batches which enable much greater parallelism to be exploited.
  prefs: []
  type: TYPE_NORMAL
