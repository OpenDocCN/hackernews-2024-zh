- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: -\>yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 分类：未分类
- en: 'date: 2024-05-27 14:56:02'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-05-27 14:56:02
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -\>
- en: Delta Lake vs. Parquet Comparison | Delta Lake
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Delta Lake vs. Parquet Comparison | Delta Lake
- en: 来源：[https://delta.io/blog/delta-lake-vs-parquet-comparison/](https://delta.io/blog/delta-lake-vs-parquet-comparison/)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://delta.io/blog/delta-lake-vs-parquet-comparison/](https://delta.io/blog/delta-lake-vs-parquet-comparison/)
- en: This post explains the differences between Delta Lake and Parquet tables and
    why Delta Lakes are almost always a better option for real-world use cases. Delta
    Lake has all the benefits of Parquet tables and many other critical features for
    data practitioners. That’s why using a Delta Lake instead of a Parquet table is
    almost always advantageous.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 本文介绍了 Delta Lake 和 Parquet 表格之间的区别，以及为什么 Delta Lake 几乎总是实际用例中更好的选择。Delta Lake
    具有 Parquet 表格的所有优点以及许多其他对数据从业者至关重要的功能。这就是为什么使用 Delta Lake 而不是 Parquet 表格几乎总是有利的原因。
- en: Parquet tables are OK when data is in a single file but are hard to manage and
    unnecessarily slow when data is in many files. Delta Lake makes it easy to manage
    data in many Parquet files.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 当数据在单个文件中时，Parquet 表格是可以的，但是当数据在多个文件中时，它们很难管理并且不必要地慢。Delta Lake 可以轻松地管理多个 Parquet
    文件中的数据。
- en: Let’s compare the basic structure of a Parquet table and a Delta table to understand
    Delta Lake's advantages better.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们比较一下 Parquet 表格和 Delta 表格的基本结构，以更好地理解 Delta Lake 的优势。
- en: Essential characteristics of Parquet files
  id: totrans-9
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Parquet 文件的基本特征
- en: 'Parquet is an immutable, binary, columnar file format with several advantages
    compared to a row-based format like CSV. Here are the core advantages of Parquet
    files compared to CSV:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: Parquet 是一种不可变的、二进制的、列式文件格式，与 CSV 等基于行的格式相比具有几个优点。以下是 Parquet 文件与 CSV 文件相比的核心优势：
- en: The columnar nature of Parquet files allows query engines to cherry-pick individual
    columns. For row-based file formats, query engines must read all the columns,
    even those irrelevant to the query.
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Parquet 文件的列式特性允许查询引擎挑选单独的列。对于基于行的文件格式，查询引擎必须读取所有列，即使对查询无关的列也是如此。
- en: Parquet files contain schema information in the metadata, so the query engine
    doesn’t need to infer the schema / the user doesn’t need to manually specify the
    schema when reading the data.
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Parquet 文件在元数据中包含模式信息，因此查询引擎不需要推断模式，/ 用户在读取数据时不需要手动指定模式。
- en: Columnar file formats like Parquet files are more compressible than row-based
    file formats.
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 与基于行的文件格式相比，列式文件格式如 Parquet 文件更易于压缩。
- en: Parquet files store data in row groups. Each row group has min/max statistics
    for each column. Parquet allows query engines to skip over entire row groups for
    specific queries, which can be a huge performance gain when reading data.
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Parquet 文件将数据存储在行组中。每个行组都有每列的最小/最大统计信息。Parquet 允许查询引擎针对特定查询跳过整个行组，这在读取数据时可能会带来巨大的性能提升。
- en: Parquet files are immutable, discouraging the antipattern of manually updating
    source data.
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Parquet 文件是不可变的，阻止了手动更新源数据的反模式。
- en: See [this video on five reasons Parquet is better than CSV](https://www.youtube.com/watch?v=9LYYOdIwQXg)
    to learn more.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 查看 [这个视频以了解 Parquet 优于 CSV 的五个原因](https://www.youtube.com/watch?v=9LYYOdIwQXg)。
- en: You can save small datasets in a single Parquet file without usability issues.
    A single Parquet file for a small dataset generally provides users with a much
    better data analysis experience than a CSV file.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以将小型数据集保存在单个 Parquet 文件中，而不会出现可用性问题。通常，对于小数据集，单个 Parquet 文件提供给用户的数据分析体验要比
    CSV 文件好得多。
- en: However, data practitioners often have large datasets split across multiple
    Parquet files. Managing multiple Parquet files isn’t great.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，数据从业者经常将大型数据集拆分成多个 Parquet 文件。管理多个 Parquet 文件并不理想。
- en: The challenge of storing data in multiple Parquet files
  id: totrans-19
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在多个 Parquet 文件中存储数据的挑战
- en: A Parquet table consists of files in a data store. Here’s what a bunch of Parquet
    files look like on disk.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: Parquet 表格由数据存储中的文件组成。以下是磁盘上一堆 Parquet 文件的样子。
- en: '[Copy](#)'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '[复制](#)'
- en: '[PRE0]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Many of the usability benefits of a single Parquet file extend to Parquet data
    lakes - it’s easy to do column pruning on one Parquet file or many Parquet files.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 单个 Parquet 文件的许多可用性优势也适用于 Parquet 数据湖 - 在一个 Parquet 文件或多个 Parquet 文件上进行列修剪非常容易。
- en: 'Here are some of the challenges of working with Parquet tables:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 Parquet 表格的一些挑战包括：
- en: No ACID transactions for Parquet data lakes
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Parquet 数据湖没有 ACID 事务
- en: It is not easy to delete rows from Parquet tables
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从 Parquet 表格中删除行不容易
- en: No DML transactions
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 没有 DML 事务
- en: There is no change data feed
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 没有变更数据反馈
- en: Slow file listing overhead
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 缓慢的文件列表检索开销
- en: Expensive footer reads to gather statistics for file skipping
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 获取用于跳过文件的统计信息的昂贵页脚读取
- en: There is no way to rename, reorder, or drop columns without rewriting the whole
    table
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 没有办法重命名、重新排序或删除列而不重写整个表。
- en: And many more
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 还有很多
- en: Delta Lake makes managing Parquet tables easier and faster. Delta Lake is also
    optimized to prevent you from corrupting your data table. Let’s look at how Delta
    Lakes are structured to understand how it provides these features.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: Delta Lake 使管理 Parquet 表变得更容易和更快。Delta Lake 也经过优化，可以防止数据表损坏。让我们看看 Delta Lake
    是如何构造的，以了解它是如何提供这些功能的。
- en: The basic structure of a Delta Lake
  id: totrans-34
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Delta Lake 的基本结构
- en: Delta Lake stores metadata in a transaction log and table data in Parquet files.
    Here are the contents of a Delta table.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: Delta Lake 在事务日志中存储元数据，表数据存储在 Parquet 文件中。以下是 Delta 表的内容。
- en: '[Copy](#)'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '[复制](#)'
- en: '[PRE1]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Here’s a visual representation of a Delta table:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 这是 Delta 表的可视化表示：
- en: You can see the full Delta Lake specification by looking at the [protocol](https://github.com/delta-io/delta/blob/master/PROTOCOL.md).
    Let’s look at how Delta Lakes make file listing operations faster.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以通过查看[协议](https://github.com/delta-io/delta/blob/master/PROTOCOL.md)来查看完整的
    Delta Lake 规范。让我们看看 Delta Lake 如何使文件列表操作更快。
- en: 'Delta Lake vs. Parquet: file listing'
  id: totrans-40
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Delta Lake vs. Parquet：文件列表
- en: 'When you want to read a Parquet lake, you must perform a file listing operation
    and then read all the data. You can’t read the data till you’ve listed all the
    files. See the following illustration:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 当你想要读取 Parquet 数据湖时，你必须执行文件列表操作，然后读取所有数据。在列出所有文件之前，你无法读取数据。请参见以下示例：
- en: On a UNIX filesystem, listing files isn’t too expensive. File listing operations
    are slower for data in the cloud. Cloud-based file systems are key/value object
    stores, which aren’t similar to UNIX-like filesystems. Key value stores are slow
    at listing files.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 在 UNIX 文件系统上，列出文件并不太昂贵。对于云中的数据，文件列表操作速度较慢。基于云的文件系统是键/值对象存储，与类 UNIX 文件系统不相似。键值存储在列出文件时速度较慢。
- en: Delta Lakes store the paths to Parquet files in the transaction log to avoid
    performing an expensive file listing. Delta Lake doesn’t need to list all Parquet
    files in the cloud object store to fetch their paths. It can just look up the
    file paths in the transaction log.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: Delta Lake 将 Parquet 文件的路径存储在事务日志中，以避免执行昂贵的文件列表。Delta Lake 不需要列出云对象存储中的所有 Parquet
    文件来获取它们的路径。它只需在事务日志中查找文件路径。
- en: Cloud object stores are bad at listing files that are nested in directories.
    Files stored with Hive-style partitioning in cloud-based systems can require file
    listing operations that take minutes or hours to compute.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 云对象存储不擅长列出嵌套在目录中的文件。在基于云的系统中使用 Hive 风格分区存储的文件可能需要花费几分钟甚至几小时来计算文件列表操作。
- en: It’s better to rely on the transaction log to get the paths to files in a table
    instead of performing a file listing operation.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 最好依赖于事务日志来获取表中文件的路径，而不是执行文件列表操作。
- en: 'Delta Lake vs. Parquet: small file problem'
  id: totrans-46
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Delta Lake vs. Parquet：小文件问题
- en: Big data systems that are incrementally updated can create a lot of small files.
    The small file problem is particularly pronounced when incremental updates happen
    frequently and for Hive partitioned datasets.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 增量更新的大数据系统可能会生成大量小文件。当增量更新频繁发生且针对 Hive 分区数据集时，小文件问题尤为突出。
- en: Data processing engines don’t perform well when reading datasets with many small
    files. You typically want files that are between 64 MB and 1 GB. You don’t want
    tiny 1 KB files that require excessive I/O overhead.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 当读取具有许多小文件的数据集时，数据处理引擎性能表现不佳。通常希望文件大小在 64 MB 到 1 GB 之间。你不希望有 1 KB 的微小文件，这些文件需要过多的
    I/O 开销。
- en: Data practitioners will commonly want to compact the small files into larger
    files with a process referred to as “small file compaction” or “bin-packing”.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 数据从业者通常希望将小文件合并成较大的文件，这个过程称为“小文件合并”或“装箱”。
- en: Suppose you have a dataset with 10,000 small files that are slow to query. You
    can compact these 10,000 small files into a dataset with 100 right-sized files.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 假设你有一个包含 10,000 个查询速度缓慢的小文件的数据集。你可以将这 10,000 个小文件合并成一个包含 100 个合适大小文件的数据集。
- en: If you’re working with a plain vanilla Parquet data lake, you need to write
    the small file compaction code yourself. With Delta Lake, you can simply run the
    `OPTIMIZE` command, and Delta Lake will handle the small file compaction for you.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你正在使用普通的 Parquet 数据湖，你需要自己编写小文件合并代码。使用 Delta Lake，你只需简单运行`OPTIMIZE`命令，Delta
    Lake 将为你处理小文件合并。
- en: 'ETL pipelines often process new files. With a plain vanilla Parquet lake, there
    are two types of new files: new data and old data that’s compacted into bigger
    files. You don’t want downstream systems to reprocess old data already processed.
    Delta Lake has a `data_change=False` flag that lets downstream systems distinguish
    between new data and new files that are just compacted versions of existing data.
    Delta Lake is much better for production ETL pipelines.'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: ETL 管道通常会处理新文件。在普通的 Parquet 数据湖中，有两种类型的新文件：新数据和已压缩成较大文件的旧数据。您不希望下游系统重新处理已处理过的旧数据。Delta
    Lake 有一个 `data_change=False` 标志，可以让下游系统区分新数据和只是现有数据的压缩版本的新文件。Delta Lake 对于生产 ETL
    管道来说更加优秀。
- en: Delta Lake makes small file compaction easier than a plain vanilla Parquet table.
    See [this blog post on small file compaction with OPTIMIZE](https://delta.io/blog/2023-01-25-delta-lake-small-file-compaction-optimize/)
    to learn more.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: Delta Lake 使小文件压缩比普通的 Parquet 表更容易。查看[此博客文章关于使用 OPTIMIZE 进行小文件压缩](https://delta.io/blog/2023-01-25-delta-lake-small-file-compaction-optimize/)以了解更多信息。
- en: 'Delta Lake vs. Parquet: ACID transactions'
  id: totrans-54
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Delta Lake 对比 Parquet：ACID 事务
- en: Databases support transactions, which prevent a host of data errors compared
    to data systems that don’t support transactions.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 数据库支持事务，与不支持事务的数据系统相比，可以防止一系列数据错误。
- en: Parquet tables don’t support transactions, so they are easy to corrupt. Suppose
    you’re appending a large amount of data to an existing Parquet lake, and your
    cluster dies in the middle of the write operation. Then, you’ll have several partially
    written Parquet files in your table.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: Parquet 表不支持事务，因此它们很容易损坏。假设您正在向现有 Parquet 数据湖追加大量数据，并且在写操作中间集群失败。那么，您的表中将有几个部分写入的
    Parquet 文件。
- en: The partially written files will break any subsequent read operations. The compute
    engine will try to read in the corrupt files and error out. You’ll need to manually
    identify all the corrupted files and delete them to fix your lake. A corrupt table
    typically breaks a lot of data systems in an organization and requires an urgent
    hotfix - not fun.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 部分写入的文件会破坏后续的读取操作。计算引擎将尝试读取损坏的文件并报错。您需要手动识别所有损坏的文件并删除它们以修复数据湖。一个损坏的表通常会破坏组织中的许多数据系统，并需要紧急修复
    - 非常不愉快。
- en: Delta Lake supports transactions, so you’ll never corrupt a Delta Lake by a
    write operation that errors out midway through. If a cluster dies when writing
    to a Delta table, the Delta Lake will simply ignore the partially written files,
    and subsequent reads won’t break. Transactions also have a lot of other benefits,
    and this is just one example.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: Delta Lake 支持事务，因此您永远不会因为写操作在中途出错而损坏 Delta Lake。如果在向 Delta 表写入数据时集群失败，Delta
    Lake 将简单地忽略部分写入的文件，后续的读取操作不会中断。事务还有许多其他好处，这只是一个例子。
- en: 'Delta Lake vs. Parquet: column pruning'
  id: totrans-59
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Delta Lake 对比 Parquet：列剪枝
- en: Queries run faster if you can send less data to the computation cluster. Column-based
    file formats allow you to cherry-pick specific columns from a table, whereas row-based
    file formats require sending all the columns to the cluster.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 如果能够将更少的数据发送到计算集群，查询运行速度就会更快。基于列的文件格式允许您从表中挑选特定的列，而基于行的文件格式则需要将所有列发送到集群。
- en: Delta Lake and Parquet are columnar, so you can cherry-pick specific columns
    from a data set via column pruning (aka column projection). Column pruning isn’t
    an advantage for Delta Lake compared to Parquet because they support this feature.
    Delta Lake stores data in Parquet files under the hood.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: Delta Lake 和 Parquet 都是列式存储的，因此您可以通过列剪枝（也称为列投影）从数据集中挑选特定的列。与 Parquet 相比，列剪枝对
    Delta Lake 并不是一个优势，因为它们都支持此功能。Delta Lake 在底层使用 Parquet 文件存储数据。
- en: However, column pruning isn’t possible with data stored in a row-based file
    format like CSV or JSON, so this is a significant performance benefit for Delta
    Lake compared to a row-based file format.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，对于存储在类似 CSV 或 JSON 这样的基于行的文件格式中的数据，无法进行列剪枝，因此与基于行的文件格式相比，这对 Delta Lake 是一个重大的性能优势。
- en: 'Delta Lake vs. Parquet: file skipping'
  id: totrans-63
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Delta Lake 对比 Parquet：文件跳过
- en: Delta tables store metadata information about the underlying Parquet files in
    the transaction log. It’s quick to read the transaction log of a Delta table and
    figure out what files can be skipped.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: Delta 表在事务日志中存储关于底层 Parquet 文件的元数据信息。读取 Delta 表的事务日志并找出可以跳过的文件是非常快速的。
- en: Parquet files store metadata for row groups in the footer, but fetching all
    the footers and building the file-level metadata for the entire table is slow.
    It requires a file-listing operation, and we’ve already discussed how file-listing
    can be slow.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: Parquet 文件在页脚中存储行组的元数据，但获取所有页脚并构建整个表的文件级别元数据速度较慢。它需要一个文件列表操作，而我们已经讨论过文件列表操作可能很慢。
- en: Parquet doesn’t support file-level skipping, but row-group filtering is possible.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: Parquet 不支持文件级别的跳过，但可以进行行组过滤。
- en: 'Delta Lake vs. Parquet: predicate pushdown filtering'
  id: totrans-67
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Delta Lake 与 Parquet：谓词推送过滤
- en: Parquet files have metadata statistics in the footer that can be leveraged by
    data processing engines to run queries more efficiently.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: Parquet 文件在页脚中包含的元数据统计信息可以被数据处理引擎利用，以更有效地运行查询。
- en: Sending less data to a computation cluster is a great way to make a query run
    faster. You can accomplish this by sending fewer columns or rows of data to the
    engine.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 发送较少的数据到计算集群是使查询运行更快的一个好方法。您可以通过发送更少的列或行数据到引擎来实现这一点。
- en: The Parquet file footer also contains min/max statistics for each column in
    the file (the min/max statistics are technically tracked for each row group, but
    let’s keep it simple). Depending on the query, you can skip an entire row group.
    For example, suppose you’re running a filtering operation and would like to find
    all values where `col4=65`. If there is a Parquet file with a max `col4` value
    of 34, you know that file doesn’t have any relevant data for your query. You can
    skip it entirely.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: Parquet 文件页脚还包含文件中每列的最小/最大统计信息（技术上，最小/最大统计信息是针对每个行组跟踪的，但让我们保持简单）。根据查询的方式，您可以跳过整个行组。例如，假设您正在运行一个过滤操作，并且想要找到所有
    `col4=65` 的值。如果有一个 Parquet 文件，其中 `col4` 的最大值为 34，您就知道该文件不包含您查询所需的任何相关数据。您可以完全跳过它。
- en: The effectiveness of data skipping depends on how many files you can skip with
    your query. But this tactic can provide 10x - 100x speed gains, or more - it’s
    essential.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 数据跳过的效果取决于您的查询可以跳过多少个文件。但这种策略可以提供 10 倍至 100 倍甚至更多的速度提升 - 这是至关重要的。
- en: When you’re reading a single Parquet file, having the metadata in the Parquet
    file footer is fine. If you have 10,000 files, you don’t want to have to read
    in all the file footers, gather the statistics for the overall lake, and then
    run the query. That’s way too much overhead.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 当您读取单个 Parquet 文件时，将元数据放在 Parquet 文件页脚中是可以接受的。如果您有 10,000 个文件，您不希望必须读取所有文件的页脚，收集整个湖泊的统计信息，然后再运行查询。那将是太多的开销。
- en: Delta Lake stores the metadata statistics in the transaction log, so the query
    engine doesn’t need to read all the individual files and gather the statistics
    before running a query. It’s way more efficient to fetch the statistics from the
    transaction log.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: Delta Lake 将元数据统计信息存储在事务日志中，因此查询引擎在运行查询之前不需要读取所有单独的文件并收集统计信息。从事务日志中获取统计信息要更有效率。
- en: Delta Lake Z Order indexing
  id: totrans-74
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Delta Lake 的 Z Order 索引
- en: Skipping is much more efficient when the data is Z Ordered. More data can be
    skipped when similar data is co-located.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 当数据被 Z Ordered 时，跳过操作效率更高。当相似的数据位于相邻位置时，可以跳过更多的数据。
- en: The Data & AI Summit talk on [Why Delta Lake is the Best Storage Format for
    pandas analyses](https://www.youtube.com/watch?v=A8bvJlG6phk) shows how Z Ordering
    data in a Delta table can significantly decrease the runtime of a query.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: '[为什么 Delta Lake 是 pandas 分析的最佳存储格式](https://www.youtube.com/watch?v=A8bvJlG6phk)
    的 Data & AI Summit 演讲展示了如何在 Delta 表中对数据进行 Z Ordering 可以显著减少查询的运行时间。'
- en: It’s easy to Z Order the data in a Delta table. It’s not easy to Z Order the
    data in a Parquet table. See the blog post on [Delta Lake Z Order](https://delta.io/blog/2023-06-03-delta-lake-z-order/)
    to learn more.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Delta 表中对数据进行 Z Ordering 是很容易的。在 Parquet 表中对数据进行 Z Ordering 则不容易。请参阅有关 [Delta
    Lake Z Order](https://delta.io/blog/2023-06-03-delta-lake-z-order/) 的博客文章以了解更多信息。
- en: 'Delta Lake vs. Parquet: renaming columns'
  id: totrans-78
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Delta Lake 与 Parquet：重命名列
- en: Parquet files are immutable, so you can’t modify the file to update the column
    name. If you want to change the column name, read it into a DataFrame, change
    the name, and then rewrite the entire file. Renaming a column can be an expensive
    computation.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: Parquet 文件是不可变的，因此无法修改文件以更新列名。如果要更改列名，请将其读入 DataFrame，更改名称，然后重新写入整个文件。重命名列可能是一项昂贵的计算。
- en: Delta Lake abstracted the concept of physical column names and logical column
    names. The physical column name is the actual column name in the Parquet file.
    The logical column name is the column name humans use when referencing the column.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: Delta Lake 抽象了物理列名和逻辑列名的概念。物理列名是 Parquet 文件中的实际列名。逻辑列名是人类在引用列时使用的列名。
- en: Delta Lake lets users quickly rename columns by changing the logical column
    name, a pure-metadata operation. It’s just a simple entry in the Delta transaction
    log.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: Delta Lake 允许用户通过更改逻辑列名快速重命名列，这是一个纯元数据操作。这只是 Delta 事务日志中的一个简单条目。
- en: There isn’t a quick way to update the column name of a Parquet table. You need
    to read all the data, rename the column, and then rewrite all the data. This is
    slow for big datasets.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 并没有快速的方法来更新 Parquet 表的列名。您需要读取所有数据，重命名列，然后重新编写所有数据。对于大数据集来说，这是一个缓慢的过程。
- en: 'Delta Lake vs. Parquet: dropping columns'
  id: totrans-83
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Delta Lake vs. Parquet：删除列
- en: Delta Lake also allows you to drop a column quickly. You can add an entry to
    the Delta transaction log and instruct Delta to ignore columns on future operations
    - it’s a pure metadata operation.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: Delta Lake 还允许您快速删除列。您可以向 Delta 事务日志添加一个条目，并指示 Delta 在未来操作中忽略列 - 这是一个纯元数据操作。
- en: Parquet tables require that you read all the data, drop the column with a query
    engine, and then rewrite all the data. It’s an extensive computation for a relatively
    small operation.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: Parquet 表要求您读取所有数据，通过查询引擎删除列，然后重新编写所有数据。这对于一个相对较小的操作来说是一次广泛的计算。
- en: See this blog post for more information on [how to drop columns from Delta tables](https://delta.io/blog/2022-08-29-delta-lake-drop-column/).
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 有关如何从 Delta 表中删除列的更多信息，请参阅[此博客文章](https://delta.io/blog/2022-08-29-delta-lake-drop-column/)。
- en: 'Delta Lake vs. Parquet: schema enforcement'
  id: totrans-87
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Delta Lake vs. Parquet：模式强制执行
- en: You usually want to allow appending DataFrames with a schema that matches the
    existing table and to reject appends of DataFrames with schemas that don’t match.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 通常情况下，您希望允许追加具有与现有表匹配的模式的 DataFrame，并拒绝具有不匹配模式的 DataFrame 的追加。
- en: With Parquet tables, you need to code this schema enforcement manually. You
    can append DataFrames with any schema to a Parquet table by default (unless they’re
    registered with a metastore and schema enforcement is provided via the metastore).
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 Parquet 表，您需要手动编写此模式强制执行。默认情况下，您可以将任何模式的 DataFrame 追加到 Parquet 表中（除非它们已在元数据存储中注册，并通过元数据存储提供模式强制执行）。
- en: Delta Lakes have built-in schema enforcement, which saves you from costly errors
    that can corrupt your Delta Lake. See [this post](https://delta.io/blog/2022-11-16-delta-lake-schema-enforcement/)
    for more information about schema enforcement.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: Delta Lake 具有内置的模式强制执行，可防止可能损坏 Delta Lake 的昂贵错误。有关模式强制执行的更多信息，请参阅[此文章](https://delta.io/blog/2022-11-16-delta-lake-schema-enforcement/)。
- en: You can also bypass schema enforcement in Delta tables and change the schema
    of a table over time.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 您还可以在 Delta 表中绕过模式强制执行，并随时间更改表的模式。
- en: 'Delta Lake vs. Parquet: schema evolution'
  id: totrans-92
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Delta Lake vs. Parquet：模式演化
- en: Sometimes, you’d like to add additional columns to your Delta Lake. Perhaps
    you rely on a data vendor, and they’ve added a new column to your data feed. You’d
    prefer not to rewrite all the existing data with a blank column so that you can
    add a new column to your table. You’d like a little schema flexibility. You’d
    just like to write the new data with the additional column and keep all the existing
    data as is.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 有时，您可能想要向 Delta Lake 添加额外的列。也许您依赖于数据供应商，他们已经向您的数据源添加了新列。您不希望以空列重写所有现有数据，以便可以向表中添加新列。您希望有一些模式灵活性。您只想写入具有附加列的新数据，并保留所有现有数据不变。
- en: Delta Lake allows for schema evolution so you can seamlessly add new columns
    to your dataset without running big computations. It’s another convenience feature
    that commonly comes in handy for real-world data applications. See [this blog
    post](https://www.databricks.com/blog/2019/09/24/diving-into-delta-lake-schema-enforcement-evolution.html)
    for more information about schema evolution.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: Delta Lake 允许模式演化，因此您可以无缝地向数据集添加新列，而无需运行大型计算。这是另一个通常在实际数据应用程序中非常有用的便利功能。有关模式演化的更多信息，请参阅[这篇博文](https://www.databricks.com/blog/2019/09/24/diving-into-delta-lake-schema-enforcement-evolution.html)。
- en: Suppose you append a DataFrame to a Parquet table with a mismatched schema.
    In that case, you must remember to set a specific option every time you read the
    table to ensure accurate results. Query engines usually take shortcuts when determining
    the schema of a Parquet table. They look at the schema of one file and just assume
    that all the other files have the same schema.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 假设您将DataFrame追加到具有不匹配架构的Parquet表中。在这种情况下，您必须记住每次读取表时设置特定选项，以确保准确的结果。查询引擎通常在确定Parquet表的架构时采取捷径。它们查看一个文件的架构，然后假定所有其他文件具有相同的架构。
- en: The engine can consults the schema of all the files in a Parquet table when
    determining the schema of the overall table when you manually set a flag. Checking
    the schema of all the files is more computationally expensive, so it isn’t set
    by default. Delta Lake schema evolution is better than what’s offered by Parquet.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 当手动设置标志时，引擎可以查看Parquet表中所有文件的架构，以确定整个表的架构。检查所有文件的架构会更加耗费计算资源，因此默认情况下不会设置。Delta
    Lake的架构演变比Parquet提供的更好。
- en: 'Delta Lake vs. Parquet: check constraints'
  id: totrans-97
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Delta Lake vs. Parquet：检查约束
- en: You can also apply custom SQL checks to columns to ensure data appended to a
    table is a specified form.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 你也可以对列应用自定义SQL检查，以确保追加到表中的数据具有指定的形式。
- en: Simply checking the schema of a string column might not be enough. You may also
    want to ensure that the string matches a certain regular expression pattern and
    that a column does not contain `NULL` values.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 仅仅检查字符串列的架构可能还不够。您可能还希望确保字符串与某种正则表达式模式匹配，并且列不包含`NULL`值。
- en: Parquet tables don’t support check constraints like Delta Lake does. See this
    blog post on [Delta Lake Constraints and Checks](https://delta.io/blog/2022-11-21-delta-lake-contraints-check/)
    to learn more.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: Parquet表不支持像Delta Lake那样的检查约束。了解更多，请参阅[Delta Lake Constraints and Checks](https://delta.io/blog/2022-11-21-delta-lake-contraints-check/)的博客文章。
- en: 'Delta Lake vs. Parquet: versioned data'
  id: totrans-101
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Delta Lake vs. Parquet：版本化数据
- en: Delta tables can have many versions, and users can easily “time travel” between
    the different versions. Versioned data comes in handy for regulatory requirements,
    audit purposes, experimentation, and rolling back mistakes.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: Delta表可以有许多版本，用户可以轻松地在不同版本之间“时间旅行”。版本化数据在监管要求、审计目的、实验和撤消错误时非常有用。
- en: Versioned data also impacts how engines execute certain transactions. For example,
    when you “overwrite” a Delta table, you don’t physically remove files from storage.
    You simply mark the existing files as deleted, but don’t actually delete them.
    This is referred to as a “logical delete”.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 版本化数据还会影响引擎执行某些事务的方式。例如，当你“覆盖”Delta表时，你不会从存储中物理删除文件。你只是将现有文件标记为已删除，但实际上并不删除它们。这被称为“逻辑删除”。
- en: Parquet tables don’t support versioned data. When you remove data from a Parquet
    table, you actually delete it from storage, which is referred to as a “physical
    deletes”.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: Parquet表不支持版本化数据。当您从Parquet表中删除数据时，您实际上是从存储中删除它，这被称为“物理删除”。
- en: Logical data operations are better because they are safer and allow for mistakes
    to be reversed. If you overwrite a Parquet table, it is an irreversible error
    (unless there is a separate mechanism backing up the data). It’s easy to undo
    an overwrite tranaction in a Delta table.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 逻辑数据操作更好，因为它们更安全，允许撤消错误。如果您覆盖了Parquet表，则是不可逆的错误（除非有单独的机制备份数据）。在Delta表中撤消覆盖事务很容易。
- en: See this blog post on [Why PySpark append and overwrite operations are safer
    in Delta Lake than Parquet tables](https://delta.io/blog/2022-11-01-pyspark-save-mode-append-overwrite-error/)
    to learn more.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 了解更多，请参阅[Why PySpark append and overwrite operations are safer in Delta Lake
    than Parquet tables](https://delta.io/blog/2022-11-01-pyspark-save-mode-append-overwrite-error/)的博客文章。
- en: 'Delta Lake vs. Parquet: time travel'
  id: totrans-107
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Delta Lake vs. Parquet：时间旅行
- en: Versioned data also allows you to easily switch between different versions of
    your Delta Lake, which is referred to as time travel.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 版本化数据还允许您轻松切换到您的Delta Lake的不同版本之间，这被称为时间旅行。
- en: Time travel is useful in a variety of situations, as described in detail in
    the [Delta Lake Time Travel](https://delta.io/blog/2023-02-01-delta-lake-time-travel/)
    post. Parquet tables don’t support time travel.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 时间旅行在各种情况下都很有用，详细描述请参阅[Delta Lake Time Travel](https://delta.io/blog/2023-02-01-delta-lake-time-travel/)的文章。Parquet表不支持时间旅行。
- en: Delta Lake needs to keep some versions of the data around to support time travel,
    which adds an unnecessary storage cost if you don’t need historical data versions.
    Delta Lake makes it easy for you to optionally delete these legacy files.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: Delta Lake 需要保留一些数据版本以支持时间旅行，如果您不需要历史数据版本，则会增加不必要的存储成本。Delta Lake 为您可选地删除这些遗留文件提供了便利。
- en: Delta Lake vacuum command
  id: totrans-111
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Delta Lake `vacuum`命令
- en: You can delete legacy files with the Delta Lake vacuum command. For example,
    you can set the retention period to 30 days and run a vacuum command, which will
    allow you to delete all the unnecessary data that’s older than 30 days old.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以使用 Delta Lake 的`vacuum`命令删除旧的遗留文件。例如，您可以将保留期设置为 30 天，然后运行`vacuum`命令，这将允许您删除所有超过
    30 天的不必要数据。
- en: It won’t delete all of the data older than 30 days old, of course. If there
    is data still required in the current version of the Delta Lake that’s “old,”
    it won’t get deleted.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，它不会删除所有超过 30 天的旧数据。如果当前版本的 Delta Lake 中仍需要“旧”数据，则不会删除它。
- en: Once you run a vacuum command, you can’t roll back to earlier versions of the
    Delta Lake. You can’t time travel back to a Delta Lake version from 60 days ago
    if you had set the retention period to 7 days and executed a vacuum command.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦运行了`vacuum`命令，就无法回滚到 Delta Lake 的早期版本。例如，如果您将保留期设置为 7 天并执行了`vacuum`命令，那么无法在
    60 天前的 Delta Lake 版本中进行时间旅行。
- en: See this blog post for more information on [the Delta Lake VACUUM command](https://delta.io/blog/2023-01-03-delta-lake-vacuum-command/).
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 有关 Delta Lake `vacuum`命令的更多信息，请参阅这篇博客文章[the Delta Lake VACUUM command](https://delta.io/blog/2023-01-03-delta-lake-vacuum-command/)。
- en: Delta Lake rollback
  id: totrans-116
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Delta Lake 回滚
- en: Delta Lake also makes it easy to reset your entire lake to an earlier version.
    Let’s say you inserted some data on a Wednesday and realized it was incorrect.
    You can easily roll back the entire Delta Lake to the state on Tuesday, effectively
    undoing all the mistakes you made on Wednesday.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: Delta Lake 还可以轻松将整个 lake 回退到较早版本。假设您在星期三插入了一些数据，后来发现数据有误。您可以轻松将整个 Delta Lake
    回滚到星期二的状态，从而撤消所有在星期三所做的错误。
- en: You can’t roll back the Delta Lake to a version that’s farther back than the
    retention period if you’ve already run a vacuum command. That’s why you need to
    be careful before vacuuming your Delta Lake.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您已经运行了`vacuum`命令，就不能将 Delta Lake 回滚到比保留期更早的版本。因此，在清理 Delta Lake 之前务必小心。
- en: This blog post on [How to Rollback a Delta Lake Table to a Previous Version
    with Restore](https://delta.io/blog/2022-10-03-rollback-delta-lake-restore/).
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 有关如何使用还原来回退 Delta Lake 表到先前版本的博客文章，请参阅[How to Rollback a Delta Lake Table to
    a Previous Version with Restore](https://delta.io/blog/2022-10-03-rollback-delta-lake-restore/)。
- en: 'Delta Lake vs. Parquet: deleting rows'
  id: totrans-120
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Delta Lake vs. Parquet：删除行
- en: You may want the ability to delete rows from your table, especially to comply
    with regulatory requirements like GDPR. Delta Lake makes it easy to perform a
    minimal delete operation, whereas it’s not easy to delete rows from a Parquet
    lake.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 您可能希望能够从表中删除行，尤其是为了符合诸如 GDPR 等法规要求。Delta Lake 可以轻松执行最小化删除操作，而从 Parquet lake
    删除行则不容易。
- en: Suppose you have a user who would like their account deleted and all their data
    removed from your systems. You have some of their data stored in your table. Your
    table has 50,000 files, and this particular customer has data in 10 of those files.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 假设您有一个用户希望删除其帐户并从系统中删除所有数据。您的表中存储了部分他们的数据。您的表有 50,000 个文件，而这位特定客户的数据存在于其中的 10
    个文件中。
- en: Delta Lake makes it easy to run a delete command and will efficiently rewrite
    the ten impacted files without the customer data. Delta Lake also makes it easy
    to write a file that flags the rows that are deleted (deletion vectors), which
    makes this operation run even faster.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: Delta Lake 可以轻松运行删除命令，并有效地重写受影响的十个文件而不包含客户数据。Delta Lake 还可以轻松编写标记已删除行的文件（删除向量），从而使此操作运行得更快。
- en: If you have a Parquet table, the only convenient operation is to read all the
    data, filter out the data for that particular user, and then rewrite the entire
    table. That will take a long time!
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您有一个 Parquet 表，唯一方便的操作就是读取所有数据，过滤出特定用户的数据，然后重新写入整个表。这将花费很长时间！
- en: Manually identifying the 10 files that contain the user data and rewriting those
    specific files is tedious and error prone. It’s exactly the type of task that
    you’d like to delegate to your Lakehouse storage system rather than performing
    yourself.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 手动识别包含用户数据的 10 个文件并重写这些特定文件是繁琐且容易出错的。这正是您更愿意将其委托给您的 Lakehouse 存储系统而非自己执行的任务类型。
- en: Check out the blog post on [How to Delete Rows from a Delta Lake table](https://delta.io/blog/2022-12-07-delete-rows-from-delta-lake-table/)
    to learn more. Also make sure to check out the [Delta Lake Deletion Vectors](https://delta.io/blog/2023-07-05-deletion-vectors/)
    blog post to learn about how deletion operations can run much faster.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 查看关于[如何从 Delta Lake 表中删除行](https://delta.io/blog/2022-12-07-delete-rows-from-delta-lake-table/)的博客文章以了解更多信息。同时，确保查看关于[Delta
    Lake 删除向量](https://delta.io/blog/2023-07-05-deletion-vectors/)的博客文章，以了解删除操作可以更快地运行的原因。
- en: 'Delta Lake vs. Parquet: merge transactions'
  id: totrans-127
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Delta Lake 对比 Parquet：合并事务
- en: Delta Lake provides a powerful merge command that allows you to update rows,
    perform upserts, build slowly changing dimension tables, and more.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: Delta Lake 提供了一个强大的合并命令，允许您更新行、执行插入更新、构建慢变化维度表等。
- en: Delta Lake makes it easy to perform merge commands and efficiently updates the
    minimal number of files under the hood, similar to the efficient implementation
    of the delete command.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: Delta Lake 使执行合并命令并在幕后高效更新最小数量的文件变得简单，类似于高效实现删除命令。
- en: If you work with a Parquet table, you don’t have any access to a merge command.
    You need to implement all the low level merge details yourself, which is challenging
    and time consuming.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你使用 Parquet 表，就无法使用合并命令。你需要自己实现所有底层的合并细节，这是具有挑战性且耗时的。
- en: See the [Delta Lake Merge blog post](https://delta.io/blog/2023-02-14-delta-lake-merge/)
    and how it makes the data manipulation language operations (`INSERT`, `UPDATE`,
    and `DELETE`) easier.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 参阅[Delta Lake 合并博客文章](https://delta.io/blog/2023-02-14-delta-lake-merge/)以及它如何使数据操作语言操作（`INSERT`、`UPDATE`
    和 `DELETE`）更容易。
- en: Other advantages of Delta Lake tables
  id: totrans-132
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Delta Lake 表的其他优点
- en: 'Delta Lake has many other advantages over Parquet tables that aren’t discussed
    in this article, but you can check out these posts to learn more:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: Delta Lake 具有许多其他优点，本文未讨论，但您可以查阅这些文章以了解更多信息：
- en: Conclusion
  id: totrans-134
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 结论
- en: This article has shown you how Delta Lakes are generally better than Parquet
    tables. Delta Lakes make it easy to perform common data operations like dropping
    columns, renaming columns, deleting rows, and DML operations. Delta Lakes also
    support transactions and schema enforcement, so it’s much less likely you’ll corrupt
    your table. Delta Lake abstract the file metadata to a transaction log and support
    Z Ordering, so you can run queries faster.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 本文向您展示了 Delta Lake 通常比 Parquet 表更好。Delta Lake 使执行常见的数据操作变得简单，如删除列、重命名列、删除行和
    DML 操作。Delta Lake 还支持事务和模式强制执行，因此您很少会损坏表格。Delta Lake 将文件元数据抽象到一个事务日志中，并支持 Z Ordering，因此您可以更快地运行查询。
- en: Parquet lakes are still useful when you’re interfacing with systems that don’t
    support Delta Lake. You may need to convert a Delta Lake to a Parquet lake if
    a downstream system is unable to read the Delta Lake format. Delta tables store
    data in Parquet files, so it’s easy to convert from a Delta table to a Parquet
    table. Reading Delta tables with other systems is a nuanced topic, but many Delta
    Lake connectors have been built, so it’s unlikely that you cannot read a Delta
    talbe with your query engine of choice.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 当您与不支持 Delta Lake 的系统进行接口时，Parquet 表仍然是有用的。如果下游系统无法读取 Delta Lake 格式，则可能需要将 Delta
    Lake 转换为 Parquet 表。Delta 表将数据存储在 Parquet 文件中，因此从 Delta 表转换为 Parquet 表很容易。使用其他系统读取
    Delta 表是一个微妙的话题，但已构建了许多 Delta Lake 连接器，因此您很可能能够使用所选查询引擎读取 Delta 表。
- en: See [this blog post](https://delta.io/blog/2022-08-02-delta-2-0-the-foundation-of-your-data-lake-is-open/)
    to learn more about the growing Delta Lake connector ecosystem.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 查看[此博客文章](https://delta.io/blog/2022-08-02-delta-2-0-the-foundation-of-your-data-lake-is-open/)以了解更多关于不断增长的
    Delta Lake 连接器生态系统的信息。
