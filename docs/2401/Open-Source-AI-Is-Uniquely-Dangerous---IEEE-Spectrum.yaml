- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: '分类: 未分类'
- en: 'date: 2024-05-27 14:45:06'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '日期: 2024-05-27 14:45:06'
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: Open-Source AI Is Uniquely Dangerous - IEEE Spectrum
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 开源人工智能具有独特的危险性 - IEEE Spectrum
- en: 来源：[https://spectrum.ieee.org/open-source-ai-2666932122](https://spectrum.ieee.org/open-source-ai-2666932122)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://spectrum.ieee.org/open-source-ai-2666932122](https://spectrum.ieee.org/open-source-ai-2666932122)
- en: '*This is a guest post. *For the other side of the argument about open-source
    AI, see the recent guest post “[Open Source AI Is Good for Us](https://spectrum.ieee.org/open-source-ai-good).”**'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: '*这是一篇客座文章。*关于开源人工智能的另一方面的论述，请参阅最近的客座文章“[开源人工智能对我们有益](https://spectrum.ieee.org/open-source-ai-good)。”**'
- en: '*When people think of AI applications these days, they likely think of “closed-source”
    AI applications like OpenAI’s [ChatGPT](https://chat.openai.com/)—where the system’s
    software is securely held by its maker and a limited set of vetted partners. Everyday
    users interact with these systems through a Web interface like a chatbot, and
    business users can access an application programming interface (API) which allows
    them to embed the AI system in their own applications or workflows. Crucially,
    these uses allow the company that owns the model to provide access to it as a
    service, while keeping the underlying software secure. Less well understood by
    the public is the rapid and uncontrolled release of powerful unsecured (sometimes
    called open-source) AI systems.*'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '*当今人们谈到人工智能应用时，他们可能会想到像OpenAI的[ChatGPT](https://chat.openai.com/)这样的“封闭源代码”人工智能应用——在这些系统中，软件由其制造商和一组有限的经过验证的合作伙伴安全保管。普通用户通过网页界面与这些系统交互，例如聊天机器人，而企业用户可以访问应用程序编程接口（API），允许他们将人工智能系统嵌入到自己的应用程序或工作流程中。关键是，这些用途允许拥有模型的公司将其作为服务提供给用户，同时保持底层软件的安全性。公众不太了解的是，强大的未安全保护（有时称为开源）人工智能系统的迅速和不受控制的发布。*'
- en: '*A good first step in understanding the threats posed by unsecured AI is to
    ask secured AI systems like [ChatGPT](https://spectrum.ieee.org/tag/chatgpt),
    Bard, or Claude to misbehave.*'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '*了解未安全保护的人工智能带来的威胁的一个好的第一步是要求像[ChatGPT](https://spectrum.ieee.org/tag/chatgpt)，Bard或Claude这样的安全人工智能系统表现不端。*'
- en: '*[OpenAI](https://openai.com/)’s brand name adds to the confusion. While the
    company was originally founded to produce open-source AI systems, its leaders
    determined in 2019 that it was [too dangerous](https://www.wired.com/story/ai-text-generator-too-dangerous-to-make-public/)
    to continue releasing its GPT systems’ source code and model weights (the numerical
    representations of relationships between the nodes in its artificial neural network)
    to the public. [OpenAI](https://spectrum.ieee.org/tag/openai) worried because
    these text-generating AI systems can be used to generate massive amounts of well-written
    but misleading or [toxic](https://spectrum.ieee.org/open-ais-powerful-text-generating-tool-is-ready-for-business)
    content.*'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '*[OpenAI](https://openai.com/)的品牌名称加剧了混淆。虽然该公司最初成立的目的是生产开源人工智能系统，但其领导人在2019年确定，继续向公众发布其GPT系统的源代码和模型权重（人工神经网络中节点之间关系的数字表示）太危险了。[OpenAI](https://spectrum.ieee.org/tag/openai)之所以担心，是因为这些文本生成的人工智能系统可以用来生成大量写得很好但具有误导性或[有毒](https://spectrum.ieee.org/open-ais-powerful-text-generating-tool-is-ready-for-business)内容。*'
- en: '*Companies including [Meta](https://www.meta.com/) (my former employer) have
    moved in the opposite direction, choosing to release powerful unsecured AI systems
    in the name of [democratizing](https://about.fb.com/news/2023/07/llama-2/) access
    to AI. Other examples of companies releasing unsecured AI systems include [Stability
    AI](https://stability.ai/), [Hugging Face](https://huggingface.co/), [Mistral](https://mistral.ai/),
    [EleutherAI](https://www.eleuther.ai/), and the [Technology Innovation Institute](https://www.tii.ae/).
    These companies and like-minded advocacy groups have made limited progress in
    [obtaining exemptions](https://ec.europa.eu/commission/presscorner/detail/en/QANDA_21_1683)
    for some unsecured models in the European Union’s [AI Act](https://artificialintelligenceact.eu/),
    which is designed to reduce the risks of powerful AI systems. They may push for
    similar exemptions in the United States via the public comment period recently
    [set forth in](https://docs.google.com/document/d/1u-MUpA7TLO4rnrhE2rceMSjqZK2vN9ltJJ38Uh5uka4/edit#heading=h.t02rblknwe5)
    the White House’s [AI Executive Order](https://spectrum.ieee.org/biden-ai-executive-order).*'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '*包括[Meta](https://www.meta.com/)（我的前雇主）在内的公司已经朝着相反的方向发展，选择以[民主化](https://about.fb.com/news/2023/07/llama-2/)AI的获取为名发布功能强大的不安全人工智能系统。其他发布不安全人工智能系统的公司的例子包括[Stability
    AI](https://stability.ai/)，[Hugging Face](https://huggingface.co/)，[Mistral](https://mistral.ai/)，[EleutherAI](https://www.eleuther.ai/)和[Technology
    Innovation Institute](https://www.tii.ae/)。这些公司和志同道合的倡导团体在欧盟的[AI法](https://artificialintelligenceact.eu/)中取得了一些不安全模型的[豁免](https://ec.europa.eu/commission/presscorner/detail/en/QANDA_21_1683)，该法旨在减少功能强大的人工智能系统的风险。他们可能会通过最近在白宫发布的[AI行政命令](https://spectrum.ieee.org/biden-ai-executive-order)中设定的公开评论期来推动在美国取得类似的豁免。*'
- en: '*I think the [open-source](https://spectrum.ieee.org/tag/open-source) movement
    has an important role in AI. With a technology that brings so many new capabilities,
    it’s important that no single entity acts as a gatekeeper to the technology’s
    use. However, as things stand today, unsecured AI poses an enormous risk that
    we are not yet able to contain.*'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '*我认为[开源](https://spectrum.ieee.org/tag/open-source)运动在人工智能领域起着重要作用。有了一项带来如此多新功能的技术，确保没有单一实体作为技术使用的门卫是很重要的。然而，就目前情况而言，不安全的人工智能带来了我们尚无法控制的巨大风险。*'
- en: '*Understanding the Threat of Unsecured AI*'
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '*了解不安全人工智能的威胁*'
- en: '*A good first step in understanding the threats posed by unsecured AI is to
    ask secured AI systems like ChatGPT, [Bard](https://bard.google.com/), or [Claude](https://claude.ai/)
    to misbehave. You could ask them to design a more deadly coronavirus, provide
    instructions for making a bomb, make naked pictures of your favorite actor, or
    write a series of inflammatory text messages designed to make voters in swing
    states more angry about immigration. You will likely receive polite refusals to
    all such requests because [they violate](https://openai.com/policies/usage-policies)
    the [usage policies](https://policies.google.com/terms/generative-ai/use-policy)
    of [these AI systems](https://console.anthropic.com/legal/aup). Yes, it is possible
    to “jailbreak” these [AI systems](https://arxiv.org/abs/2305.13860) and get them
    to misbehave, but as these vulnerabilities are discovered, they can be fixed.*'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: '*了解不安全人工智能带来的威胁的一个好的第一步是要求像ChatGPT、[Bard](https://bard.google.com/)或者[Claude](https://claude.ai/)这样的安全人工智能系统去做坏事。你可以要求它们设计一个更致命的冠状病毒，提供制造炸弹的指南，制作你最喜欢的演员的裸照，或者写一系列旨在让摇摆州选民更愤怒于移民问题的煽动性短信。你可能会收到礼貌的拒绝，因为这些请求违反了[这些人工智能系统](https://openai.com/policies/usage-policies)的[使用政策](https://policies.google.com/terms/generative-ai/use-policy)。是的，可能会“越狱”这些[人工智能系统](https://arxiv.org/abs/2305.13860)并让它们做坏事，但是一旦发现这些漏洞，就可以修复它们。*'
- en: '*Enter the unsecured models. Most famous is Meta’s [Llama 2](https://ai.meta.com/llama/).
    It was released by [Meta](https://spectrum.ieee.org/tag/meta) with a 27-page “[Responsible
    Use Guide](https://ai.meta.com/static-resource/responsible-use-guide/),” which
    was promptly ignored by the creators of “[Llama 2 Uncensored](https://huggingface.co/jarradh/llama2_70b_chat_uncensored),”
    a derivative model with safety features stripped away, and hosted for free download
    on the Hugging Face AI repository. Once someone releases an “uncensored” version
    of an unsecured AI system, the original maker of the system is largely powerless
    to do anything about it.*'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: '*进入未保护的模型。最有名的是Meta的[Llama 2](https://ai.meta.com/llama/)。它是由[Meta](https://spectrum.ieee.org/tag/meta)发布的，附带有一个27页的“[负责任使用指南](https://ai.meta.com/static-resource/responsible-use-guide/)”，但很快被[Llama
    2未经审查版](https://huggingface.co/jarradh/llama2_70b_chat_uncensored)的创建者忽略，这是一个剥去安全功能的衍生模型，并且在Hugging
    Face人工智能存储库上免费提供下载。一旦有人发布了未经审查的未保护人工智能系统的“未经审查”版本，系统的原始制造商基本上无能为力。*'
- en: '*As things stand today, unsecured AI poses an enormous risk that we are not
    yet able to contain.*'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: '*就目前情况而言，未保护的人工智能构成了我们尚无法控制的巨大风险。*'
- en: '*The threat posed by unsecured AI systems lies in the ease of misuse. They
    are particularly dangerous in the hands of sophisticated threat actors, who could
    easily download the original versions of these AI systems and disable their safety
    features, then make their own custom versions and abuse them for a wide variety
    of tasks. Some of the abuses of unsecured AI systems also involve taking advantage
    of vulnerable distribution channels, such as social media and messaging platforms.
    These platforms cannot yet accurately detect AI-generated content at scale and
    can be used to distribute massive amounts of personalized misinformation and,
    of course, [scams](https://theconversation.com/ai-scam-calls-imitating-familiar-voices-are-a-growing-problem-heres-how-they-work-208221).
    This could have catastrophic effects on the information ecosystem, and on [elections](https://spectrum.ieee.org/deepfakes-election)
    in particular. Highly damaging [nonconsensual deepfake pornography](https://www.wired.com/story/deepfake-porn-is-out-of-control/)
    is yet another domain where unsecured AI can have deep negative consequences.*'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: '*未保护的人工智能系统带来的威胁在于容易被误用。它们在精密威胁行为者手中尤其危险，后者可以轻松下载这些人工智能系统的原始版本并禁用其安全功能，然后制作自己的定制版本并滥用它们进行各种任务。一些未保护的人工智能系统的滥用还涉及利用脆弱的分发渠道，例如社交媒体和消息平台。这些平台目前还无法准确地大规模检测人工智能生成的内容，并且可以用来传播大量个性化的虚假信息和当然也有[诈骗](https://theconversation.com/ai-scam-calls-imitating-familiar-voices-are-a-growing-problem-heres-how-they-work-208221)。这可能对信息生态系统，尤其是对[选举](https://spectrum.ieee.org/deepfakes-election)产生灾难性影响。高度破坏性的[未经同意的深度伪造色情内容](https://www.wired.com/story/deepfake-porn-is-out-of-control/)是另一个未保护人工智能可能产生深远负面影响的领域。*'
- en: '*Unsecured AI also has the potential to facilitate production of dangerous
    materials, such as [biological and chemical weapons](https://www.axios.com/2023/06/16/pandemic-bioterror-ai-chatgpt-bioattacks).
    The White House Executive Order references chemical, biological, radiological,
    and nuclear ([CBRN](https://docs.google.com/document/d/1u-MUpA7TLO4rnrhE2rceMSjqZK2vN9ltJJ38Uh5uka4/edit#heading=h.6fkzizejib9o))
    risks, and [multiple bills](https://www.markey.senate.gov/news/press-releases/sens-markey-budd-announce-legislation-to-assess-health-security-risks-of-ai)
    are now under consideration by the U.S. Congress to address these threats.*'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: '*未经保护的人工智能还有可能促进危险材料的生产，例如[生物和化学武器](https://www.axios.com/2023/06/16/pandemic-bioterror-ai-chatgpt-bioattacks)。白宫的行政命令提到了化学、生物、放射性和核（[CBRN](https://docs.google.com/document/d/1u-MUpA7TLO4rnrhE2rceMSjqZK2vN9ltJJ38Uh5uka4/edit#heading=h.6fkzizejib9o)）风险，并且[多项法案](https://www.markey.senate.gov/news/press-releases/sens-markey-budd-announce-legislation-to-assess-health-security-risks-of-ai)现在正在美国国会考虑，以应对这些威胁。*'
- en: '*Recommendations for AI Regulations*'
  id: totrans-18
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '*人工智能监管建议*'
- en: '*We don’t need to specifically regulate unsecured AI—nearly all of the regulations
    that have been publicly discussed apply to secured AI systems as well. The only
    difference is that it’s much easier for developers of secured AI systems to comply
    with these regulations because of the inherent properties of secured and unsecured
    AI. The entities that operate secured AI systems can actively monitor for abuses
    or failures of their systems (including bias and the production of dangerous or
    offensive content) and release regular updates that make their systems more fair
    and safe.*'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不需要专门监管不安全的人工智能——几乎所有公开讨论过的法规都适用于安全的人工智能系统。唯一的区别在于，安全和不安全的人工智能系统的开发者更容易遵守这些法规，因为安全和不安全的人工智能系统的固有属性不同。操作安全人工智能系统的实体可以积极监控其系统的滥用或故障（包括偏见和生成危险或冒犯性内容），并发布定期更新，使其系统更加公平和安全。
- en: '*“I think how we regulate open-source AI is THE most important unresolved issue
    in the immediate term.”'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: “我认为我们如何监管开源人工智能是当务之急中最重要但未解决的问题。”
- en: '**—Gary Marcus, New York University***'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '**—加里·马库斯，纽约大学**'
- en: '*Almost all the regulations recommended below generalize to all AI systems.
    Implementing these regulations would make companies think twice before releasing
    unsecured AI systems that are ripe for abuse.*'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 几乎所有下面推荐的法规都适用于所有人工智能系统。实施这些法规将使公司在发布易于滥用的未安全化人工智能系统之前三思而行。
- en: '***Regulatory Action for AI Systems***'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: '**人工智能系统的监管行动**'
- en: '***Pause all new releases** of unsecured AI systems until developers have met
    the requirements below, and in ways that ensure that safety features cannot be
    easily removed by bad actors.*'
  id: totrans-24
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**暂停所有未安全化人工智能系统的新发布**，直到开发者满足以下要求，并确保安全功能不易被不良行为者轻易移除。'
- en: '***Establish registration and licensing** (both retroactive and ongoing) of
    all AI systems above a certain capability threshold.*'
  id: totrans-25
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**建立所有超过一定能力门槛的人工智能系统的注册和许可证**（包括追溯和持续）。'
- en: '***Create liability** for “reasonably foreseeable misuse” and negligence: Developers
    of AI systems should be legally liable for harms caused to both individuals and
    to society.*'
  id: totrans-26
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**对“合理预见的滥用”和疏忽创建责任**：人工智能系统的开发者应对对个人和社会造成的伤害承担法律责任。'
- en: '***Establish risk assessment, mitigation, and independent audit** procedures
    for AI systems crossing the threshold mentioned above.*'
  id: totrans-27
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**建立风险评估、缓解和独立审计**程序，用于超过上述门槛的人工智能系统。'
- en: '***Require watermarking and provenance** best practices so that AI-generated
    content is clearly labeled and authentic content has metadata that lets users
    understand its provenance.*'
  id: totrans-28
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**要求水印和出处**最佳实践，以便人工智能生成的内容明确标记，并且真实内容具有元数据，让用户了解其来源。'
- en: '***Require transparency of training data** and prohibit training systems on
    personally identifiable information, content designed to generate hateful content,
    and content related to biological and chemical weapons.*'
  id: totrans-29
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**要求透明的训练数据**，并禁止在个人可识别信息、设计用于生成仇恨内容的内容以及与生物和化学武器相关的内容上进行训练。'
- en: '***Require and fund independent researcher access**, giving vetted researchers
    and civil society organizations predeployment access to generative AI systems
    for research and testing.*'
  id: totrans-30
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**要求并资助独立研究者访问**，为经过审核的研究人员和公民社会组织提供对生成式人工智能系统的预部署访问，以进行研究和测试。'
- en: '***Require “know your customer” procedures**, similar to those [used by financial
    institutions](https://www.swift.com/your-needs/financial-crime-cyber-security/know-your-customer-kyc/meaning-kyc),
    for sales of powerful hardware and cloud services designed for AI use; restrict
    sales in the same way that weapons sales would be restricted.*'
  id: totrans-31
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**要求“了解您的客户”程序**，类似于[金融机构使用的](https://www.swift.com/your-needs/financial-crime-cyber-security/know-your-customer-kyc/meaning-kyc)用于销售专为人工智能使用而设计的强大硬件和云服务的程序；以限制销售方式，就像武器销售将被限制一样。'
- en: '***Mandatory incident disclosure**: When developers learn of vulnerabilities
    or failures in their AI systems, they must be [legally required to report](https://csrc.nist.gov/pubs/sp/800/61/r2/final)
    this to a designated government authority.*'
  id: totrans-32
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**强制性事件披露**：当开发人员了解到其人工智能系统存在漏洞或故障时，他们必须向指定的政府机构[法定要求报告](https://csrc.nist.gov/pubs/sp/800/61/r2/final)。'
- en: '***Regulatory Action for Distribution Channels and Attack Surfaces***'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: '**分销渠道和攻击面的监管行动**'
- en: '***Require content credential implementation** for social media, giving companies
    a deadline to implement the [Content Credentials labeling standard](https://contentcredentials.org/)
    from C2PA.*'
  id: totrans-34
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '***要求社交媒体实施内容凭证**，并给予公司截止日期以实施由 C2PA 提供的[内容凭证标准](https://contentcredentials.org/)。*'
- en: '***Automate digital signatures** so people can rapidly verify their human-generated
    content.*'
  id: totrans-35
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '***自动化数字签名**，使人们能够迅速验证其人类生成的内容。*'
- en: '***Limit the reach of AI-generated content**: Accounts that haven’t been verified
    as distributors of human-generated content could have certain features disabled,
    including viral distribution of their content.*'
  id: totrans-36
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '***限制 AI 生成内容的影响范围**：未经验证为人类生成内容分发者的帐户可能会被禁用某些功能，包括其内容的病毒式传播。*'
- en: '***Reduce chemical, biological, radiological, and nuclear risks** by educating
    all suppliers of custom nucleic acids or other potentially dangerous substances
    about best practices.*'
  id: totrans-37
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '***通过向所有定制核酸或其他潜在危险物质的供应商提供最佳实践教育，降低化学、生物、放射性和核风险**。*'
- en: '***Government Action***'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '***政府行动***'
- en: '***Establish a nimble regulatory body** that can act and enforce quickly and
    update certain enforcement criteria. This entity would have the power to approve
    or reject risk assessments, mitigations, and audit results and have the authority
    to block model deployment.*'
  id: totrans-39
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '***建立灵活的监管机构**，该机构能够迅速行动并执行，并更新某些执行标准。该实体将有权批准或拒绝风险评估、减轻措施和审计结果，并有权阻止模型的部署。*'
- en: '***Support fact-checking organizations** and civil-society groups (including
    the “[trusted flaggers](https://ec.europa.eu/commission/presscorner/detail/en/QANDA_20_2348)”
    defined by the [EU Digital Services Act](https://digital-strategy.ec.europa.eu/en/policies/digital-services-act-package))
    and require generative AI companies to work directly with these groups.*'
  id: totrans-40
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '***支持事实核查组织**和民间社会团体（包括由[欧盟数字服务法](https://digital-strategy.ec.europa.eu/en/policies/digital-services-act-package)定义的“受信任标志者”以及[受信任标志者](https://ec.europa.eu/commission/presscorner/detail/en/QANDA_20_2348)）并要求生成性
    AI 公司直接与这些组织合作。*'
- en: '***Cooperate internationally** with the goal of eventually creating an international
    [treaty](https://www.cigionline.org/articles/voluntary-curbs-arent-enough-ai-risk-requires-a-binding-international-treaty/)
    or new international agency to prevent companies from circumventing these regulations.
    The recent [Bletchley Declaration](https://www.gov.uk/government/publications/ai-safety-summit-2023-the-bletchley-declaration/the-bletchley-declaration-by-countries-attending-the-ai-safety-summit-1-2-november-2023)
    was signed by 28 countries, including the home countries of all of the world’s
    leading AI companies (United States, China, United Kingdom, United Arab Emirates,
    France, and Germany); this declaration stated shared values and carved out a path
    for additional meetings.*'
  id: totrans-41
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '***国际合作**，旨在最终创建一个国际[条约](https://www.cigionline.org/articles/voluntary-curbs-arent-enough-ai-risk-requires-a-binding-international-treaty/)或新的国际机构，以防止公司规避这些监管措施。最近，由
    28 个国家签署的[Bletchley 宣言](https://www.gov.uk/government/publications/ai-safety-summit-2023-the-bletchley-declaration/the-bletchley-declaration-by-countries-attending-the-ai-safety-summit-1-2-november-2023)，包括世界领先
    AI 公司的总部所在国（美国、中国、英国、阿拉伯联合酋长国、法国和德国）；此宣言声明了共同的价值观，并为额外会议开辟了道路。*'
- en: '***Democratize AI access** with public infrastructure: A common concern about
    regulating AI is that it will limit the number of companies that can produce complicated
    AI systems to a small handful and tend toward monopolistic business practices.
    There are many opportunities to democratize access to AI, however, without relying
    on unsecured AI systems. One is through the creation of [public AI infrastructure](https://cdn.vanderbilt.edu/vu-URL/wp-content/uploads/sites/412/2023/10/09151836/VPA-AI-Capacity.10.9.23.pdf)
    with powerful secured AI models.*'
  id: totrans-42
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '***通过公共基础设施实现 AI 访问民主化**：关于监管 AI 的一个普遍担忧是它将使能够生产复杂 AI 系统的公司数量限制在很少一部分，并趋向于垄断性商业惯例。然而，有许多机会可以在不依赖不安全的
    AI 系统的情况下使 AI 访问民主化。其中一种方式是通过创建具有强大安全 AI 模型的[公共 AI 基础设施](https://cdn.vanderbilt.edu/vu-URL/wp-content/uploads/sites/412/2023/10/09151836/VPA-AI-Capacity.10.9.23.pdf)。*'
- en: '*“I think how we regulate open-source AI is THE most important unresolved issue
    in the immediate term,” [Gary Marcus](http://garymarcus.com/index.html), the cognitive
    scientist, entrepreneur, and professor emeritus at New York University told me
    in a recent email exchange.*'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '*“我认为我们如何监管开源 AI 是眼下最重要的未解决问题，”认知科学家、企业家和纽约大学荣誉教授[加里·马库斯](http://garymarcus.com/index.html)
    在最近的电子邮件交流中告诉我。*'
- en: '*I agree, and these recommendations are only a start. They would initially
    be costly to implement and would require that regulators make certain powerful
    lobbyists and developers unhappy.*'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '*我同意，而且这些建议只是一个开端。最初实施这些建议将会很昂贵，并且需要监管机构让某些强大的游说者和开发者感到不满。*'
- en: '*Unfortunately, given the misaligned incentives in the current AI and information
    ecosystems, it’s unlikely that industry will take these actions unless forced
    to do so. If actions like these are not taken, companies producing unsecured AI
    may bring in billions of dollars in profits while pushing the risks posed by their
    products onto all of us.*'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: '*不幸的是，鉴于当前人工智能和信息生态系统中存在的利益不一致，除非被迫这样做，否则行业不太可能采取这些行动。如果不采取这样的行动，生产不安全人工智能的公司可能会获得数十亿美元的利润，同时将产品带来的风险推卸给我们所有人。*'
- en: '*From Your Site Articles*'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: '*来自您的站点文章*'
- en: '*Related Articles Around the Web*'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '*周围的相关文章*'
