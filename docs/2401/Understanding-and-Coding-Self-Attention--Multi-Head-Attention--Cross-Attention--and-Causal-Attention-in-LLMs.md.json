["```\nsentence = 'Life is short, eat dessert first'\n​\ndc = {s:i for i,s \n      in enumerate(sorted(sentence.replace(',', '').split()))}\n\nprint(dc)\n```", "```\n{'Life': 0, 'dessert': 1, 'eat': 2, 'first': 3, 'is': 4, 'short': 5}\n```", "```\nimport torch\n​\nsentence_int = torch.tensor(\n    [dc[s] for s in sentence.replace(',', '').split()]\n)\nprint(sentence_int)\n```", "```\ntensor([0, 4, 5, 2, 1, 3])\n```", "```\nvocab_size = 50_000\n​\ntorch.manual_seed(123)\nembed = torch.nn.Embedding(vocab_size, 3)\nembedded_sentence = embed(sentence_int).detach()\n​\nprint(embedded_sentence)\nprint(embedded_sentence.shape)\n```", "```\ntensor([[ 0.3374, -0.1778, -0.3035],\n        [ 0.1794,  1.8951,  0.4954],\n        [ 0.2692, -0.0770, -1.0205],\n        [-0.2196, -0.3792,  0.7671],\n        [-0.5880,  0.3486,  0.6603],\n        [-1.1925,  0.6984, -1.4097]])\ntorch.Size([6, 3])\n```", "```\ntorch.manual_seed(123)\n​\nd = embedded_sentence.shape[1]\n​\nd_q, d_k, d_v = 2, 2, 4\n​\nW_query = torch.nn.Parameter(torch.rand(d, d_q))\nW_key = torch.nn.Parameter(torch.rand(d, d_k))\nW_value = torch.nn.Parameter(torch.rand(d, d_v))\n```", "```\nx_2 = embedded_sentence[1]\nquery_2 = x_2 @ W_query\nkey_2 = x_2 @ W_key\nvalue_2 = x_2 @ W_value\n​\nprint(query_2.shape)\nprint(key_2.shape)\nprint(value_2.shape)\n```", "```\ntorch.Size([2])\ntorch.Size([2])\ntorch.Size([4])\n```", "```\nkeys = embedded_sentence @ W_key\nvalues = embedded_sentence @ W_value\n​\nprint(\"keys.shape:\", keys.shape)\nprint(\"values.shape:\", values.shape)\n```", "```\nkeys.shape: torch.Size([6, 2])\nvalues.shape: torch.Size([6, 4])\n```", "```\nomega_24 = query_2.dot(keys[4])\nprint(omega_24)\n```", "```\ntensor(1.2903)\n```", "```\nomega_2 = query_2 @ keys.T\nprint(omega_2)\n```", "```\ntensor([-0.6004,  3.4707, -1.5023,  0.4991,  1.2903, -1.3374])\n```", "```\nimport torch.nn.functional as F\n​\nattention_weights_2 = F.softmax(omega_2 / d_k**0.5, dim=0)\nprint(attention_weights_2)\n```", "```\ntensor([0.0386, 0.6870, 0.0204, 0.0840, 0.1470, 0.0229])\n```", "```\ncontext_vector_2 = attention_weights_2 @ values\n​\nprint(context_vector_2.shape)\nprint(context_vector_2)\n```", "```\ntorch.Size([4])\ntensor([0.5313, 1.3607, 0.7891, 1.3110])\n```", "```\nimport torch.nn as nn\n​\nclass SelfAttention(nn.Module):\n​\n    def __init__(self, d_in, d_out_kq, d_out_v):\n        super().__init__()\n        self.d_out_kq = d_out_kq\n        self.W_query = nn.Parameter(torch.rand(d_in, d_out_kq))\n        self.W_key   = nn.Parameter(torch.rand(d_in, d_out_kq))\n        self.W_value = nn.Parameter(torch.rand(d_in, d_out_v))\n​\n    def forward(self, x):\n        keys = x @ self.W_key\n        queries = x @ self.W_query\n        values = x @ self.W_value\n\n        attn_scores = queries @ keys.T  # unnormalized attention weights    \n        attn_weights = torch.softmax(\n            attn_scores / self.d_out_kq**0.5, dim=-1\n        )\n\n        context_vec = attn_weights @ values\n        return context_vec\n```", "```\ntorch.manual_seed(123)\n​\n# reduce d_out_v from 4 to 1, because we have 4 heads\nd_in, d_out_kq, d_out_v = 3, 2, 4\n​\nsa = SelfAttention(d_in, d_out_kq, d_out_v)\nprint(sa(embedded_sentence))\n```", "```\ntensor([[-0.1564,  0.1028, -0.0763, -0.0764],\n        [ 0.5313,  1.3607,  0.7891,  1.3110],\n        [-0.3542, -0.1234, -0.2627, -0.3706],\n        [ 0.0071,  0.3345,  0.0969,  0.1998],\n        [ 0.1008,  0.4780,  0.2021,  0.3674],\n        [-0.5296, -0.2799, -0.4107, -0.6006]], grad_fn=<MmBackward0>)\n```", "```\nclass MultiHeadAttentionWrapper(nn.Module):\n​\n    def __init__(self, d_in, d_out_kq, d_out_v, num_heads):\n        super().__init__()\n        self.heads = nn.ModuleList(\n            [SelfAttention(d_in, d_out_kq, d_out_v) \n             for _ in range(num_heads)]\n        )\n​\n    def forward(self, x):\n        return torch.cat([head(x) for head in self.heads], dim=-1)\n```", "```\ntorch.manual_seed(123)\n​\nd_in, d_out_kq, d_out_v = 3, 2, 1\n​\nsa = SelfAttention(d_in, d_out_kq, d_out_v)\nprint(sa(embedded_sentence))\n```", "```\ntensor([[-0.0185],\n        [ 0.4003],\n        [-0.1103],\n        [ 0.0668],\n        [ 0.1180],\n        [-0.1827]], grad_fn=<MmBackward0>)\n```", "```\ntorch.manual_seed(123)\n​\nblock_size = embedded_sentence.shape[1]\nmha = MultiHeadAttentionWrapper(\n    d_in, d_out_kq, d_out_v, num_heads=4\n)\n​\ncontext_vecs = mha(embedded_sentence)\n​\nprint(context_vecs)\nprint(\"context_vecs.shape:\", context_vecs.shape)\n```", "```\ntensor([[-0.0185,  0.0170,  0.1999, -0.0860],\n        [ 0.4003,  1.7137,  1.3981,  1.0497],\n        [-0.1103, -0.1609,  0.0079, -0.2416],\n        [ 0.0668,  0.3534,  0.2322,  0.1008],\n        [ 0.1180,  0.6949,  0.3157,  0.2807],\n        [-0.1827, -0.2060, -0.2393, -0.3167]], grad_fn=<CatBackward0>)\ncontext_vecs.shape: torch.Size([6, 4])\n```", "```\nclass CrossAttention(nn.Module):\n​\n    def __init__(self, d_in, d_out_kq, d_out_v):\n        super().__init__()\n        self.d_out_kq = d_out_kq\n        self.W_query = nn.Parameter(torch.rand(d_in, d_out_kq))\n        self.W_key   = nn.Parameter(torch.rand(d_in, d_out_kq))\n        self.W_value = nn.Parameter(torch.rand(d_in, d_out_v))\n​\n    def forward(self, x_1, x_2):           # x_2 is new\n        queries_1 = x_1 @ self.W_query\n\n        keys_2 = x_2 @ self.W_key          # new\n        values_2 = x_2 @ self.W_value      # new\n\n        attn_scores = queries_1 @ keys_2.T # new \n        attn_weights = torch.softmax(\n            attn_scores / self.d_out_kq**0.5, dim=-1)\n\n        context_vec = attn_weights @ values_2\n        return context_vec\n```", "```\ntorch.manual_seed(123)\n​\nd_in, d_out_kq, d_out_v = 3, 2, 4\n​\ncrossattn = CrossAttention(d_in, d_out_kq, d_out_v)\n​\nfirst_input = embedded_sentence\nsecond_input = torch.rand(8, d_in)\n​\nprint(\"First input shape:\", first_input.shape)\nprint(\"Second input shape:\", second_input.shape)\n```", "```\nFirst input shape: torch.Size([6, 3])\nSecond input shape: torch.Size([8, 3])\n```", "```\ncontext_vectors = crossattn(first_input, second_input)\n​\nprint(context_vectors)\nprint(\"Output shape:\", context_vectors.shape)\n```", "```\ntensor([[0.4231, 0.8665, 0.6503, 1.0042],\n        [0.4874, 0.9718, 0.7359, 1.1353],\n        [0.4054, 0.8359, 0.6258, 0.9667],\n        [0.4357, 0.8886, 0.6678, 1.0311],\n        [0.4429, 0.9006, 0.6775, 1.0460],\n        [0.3860, 0.8021, 0.5985, 0.9250]], grad_fn=<MmBackward0>)\nOutput shape: torch.Size([6, 4])\n```", "```\ntorch.manual_seed(123)\n​\nd_in, d_out_kq, d_out_v = 3, 2, 4\n​\nW_query = nn.Parameter(torch.rand(d_in, d_out_kq))\nW_key   = nn.Parameter(torch.rand(d_in, d_out_kq))\nW_value = nn.Parameter(torch.rand(d_in, d_out_v))\n​\nx = embedded_sentence\n​\nkeys = x @ W_key\nqueries = x @ W_query\nvalues = x @ W_value\n​\n# attn_scores are the \"omegas\", \n# the unnormalized attention weights\nattn_scores = queries @ keys.T \n​\nprint(attn_scores)\nprint(attn_scores.shape)\n```", "```\ntensor([[ 0.0613, -0.3491,  0.1443, -0.0437, -0.1303,  0.1076],\n        [-0.6004,  3.4707, -1.5023,  0.4991,  1.2903, -1.3374],\n        [ 0.2432, -1.3934,  0.5869, -0.1851, -0.5191,  0.4730],\n        [-0.0794,  0.4487, -0.1807,  0.0518,  0.1677, -0.1197],\n        [-0.1510,  0.8626, -0.3597,  0.1112,  0.3216, -0.2787],\n        [ 0.4344, -2.5037,  1.0740, -0.3509, -0.9315,  0.9265]],\n       grad_fn=<MmBackward0>)\ntorch.Size([6, 6])\n```", "```\nattn_weights = torch.softmax(attn_scores / d_out_kq**0.5, dim=1)\nprint(attn_weights)\n```", "```\ntensor([[0.1772, 0.1326, 0.1879, 0.1645, 0.1547, 0.1831],\n        [0.0386, 0.6870, 0.0204, 0.0840, 0.1470, 0.0229],\n        [0.1965, 0.0618, 0.2506, 0.1452, 0.1146, 0.2312],\n        [0.1505, 0.2187, 0.1401, 0.1651, 0.1793, 0.1463],\n        [0.1347, 0.2758, 0.1162, 0.1621, 0.1881, 0.1231],\n        [0.1973, 0.0247, 0.3102, 0.1132, 0.0751, 0.2794]],\n       grad_fn=<SoftmaxBackward0>)\n```", "```\nblock_size = attn_scores.shape[0]\nmask_simple = torch.tril(torch.ones(block_size, block_size))\nprint(mask_simple)\n```", "```\ntensor([[1., 0., 0., 0., 0., 0.],\n        [1., 1., 0., 0., 0., 0.],\n        [1., 1., 1., 0., 0., 0.],\n        [1., 1., 1., 1., 0., 0.],\n        [1., 1., 1., 1., 1., 0.],\n        [1., 1., 1., 1., 1., 1.]])\n```", "```\nmasked_simple = attn_weights*mask_simple\nprint(masked_simple)\n```", "```\ntensor([[0.1772, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n        [0.0386, 0.6870, 0.0000, 0.0000, 0.0000, 0.0000],\n        [0.1965, 0.0618, 0.2506, 0.0000, 0.0000, 0.0000],\n        [0.1505, 0.2187, 0.1401, 0.1651, 0.0000, 0.0000],\n        [0.1347, 0.2758, 0.1162, 0.1621, 0.1881, 0.0000],\n        [0.1973, 0.0247, 0.3102, 0.1132, 0.0751, 0.2794]],\n       grad_fn=<MulBackward0>)\n```", "```\nrow_sums = masked_simple.sum(dim=1, keepdim=True)\nmasked_simple_norm = masked_simple / row_sums\nprint(masked_simple_norm)\n```", "```\ntensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n        [0.0532, 0.9468, 0.0000, 0.0000, 0.0000, 0.0000],\n        [0.3862, 0.1214, 0.4924, 0.0000, 0.0000, 0.0000],\n        [0.2232, 0.3242, 0.2078, 0.2449, 0.0000, 0.0000],\n        [0.1536, 0.3145, 0.1325, 0.1849, 0.2145, 0.0000],\n        [0.1973, 0.0247, 0.3102, 0.1132, 0.0751, 0.2794]],\n       grad_fn=<DivBackward0>)\n```", "```\nmask = torch.triu(torch.ones(block_size, block_size), diagonal=1)\nmasked = attn_scores.masked_fill(mask.bool(), -torch.inf)\nprint(masked)\n```", "```\ntensor([[ 0.0613,    -inf,    -inf,    -inf,    -inf,    -inf],\n        [-0.6004,  3.4707,    -inf,    -inf,    -inf,    -inf],\n        [ 0.2432, -1.3934,  0.5869,    -inf,    -inf,    -inf],\n        [-0.0794,  0.4487, -0.1807,  0.0518,    -inf,    -inf],\n        [-0.1510,  0.8626, -0.3597,  0.1112,  0.3216,    -inf],\n        [ 0.4344, -2.5037,  1.0740, -0.3509, -0.9315,  0.9265]],\n       grad_fn=<MaskedFillBackward0>)\n```", "```\nattn_weights = torch.softmax(masked / d_out_kq**0.5, dim=1)\nprint(attn_weights)\n```", "```\ntensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n        [0.0532, 0.9468, 0.0000, 0.0000, 0.0000, 0.0000],\n        [0.3862, 0.1214, 0.4924, 0.0000, 0.0000, 0.0000],\n        [0.2232, 0.3242, 0.2078, 0.2449, 0.0000, 0.0000],\n        [0.1536, 0.3145, 0.1325, 0.1849, 0.2145, 0.0000],\n        [0.1973, 0.0247, 0.3102, 0.1132, 0.0751, 0.2794]],\n       grad_fn=<SoftmaxBackward0>) \n```"]