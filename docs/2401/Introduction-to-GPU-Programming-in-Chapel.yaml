- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 'category: 未分类'
- en: 'date: 2024-05-27 14:50:54'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 时间：2024-05-27 14:50:54
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: Introduction to GPU Programming in Chapel
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Chapel 中 GPU 编程简介
- en: 来源：[https://chapel-lang.org/blog/posts/intro-to-gpus/](https://chapel-lang.org/blog/posts/intro-to-gpus/)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://chapel-lang.org/blog/posts/intro-to-gpus/](https://chapel-lang.org/blog/posts/intro-to-gpus/)
- en: 'Chapel is a programming language for productive parallel computing. In recent
    years, a particular subdomain of parallel computing has exploded in popularity:
    GPU computing. As a result, the Chapel team has been hard at work adding GPU support,
    making it easy to create vendor-neutral and performant GPU programs. This aspect
    of the Chapel compiler has seen rapid improvements since its initial release,
    and continues to receive enhancements and performance fixes over time. This tutorial
    will provide an introduction to Chapel’s GPU programming features.'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: Chapel 是一种用于高效并行计算的编程语言。近年来，并行计算的一个特定子领域变得异常流行：GPU 计算。因此，Chapel 团队一直在努力增加对 GPU
    的支持，使得创建厂商中立且性能优异的 GPU 程序变得轻松。自首次发布以来，Chapel 编译器的这一方面已经迅速改进，并在时间上不断接收增强和性能修复。本教程将介绍
    Chapel 的 GPU 编程功能。
- en: Although frameworks such as CUDA and HIP are commonly used for programming GPUs,
    this tutorial does not assume familiarity with them. Examples will make use of
    some general-purpose features of Chapel, and this post will explain them along
    the way. For a more deliberate introduction to Chapel, see the [Advent of Code](../../series/advent-of-code-2022/)
    series on this blog, or check the [Learning Chapel](https://chapel-lang.org/learning.html)
    page for more resources.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然诸如 CUDA 和 HIP 等框架通常用于编写 GPU 程序，但本教程并不假设读者熟悉它们。示例将利用 Chapel 的一些通用功能，并在此过程中进行解释。想要更加深入地了解
    Chapel，请参阅本博客上的 [Advent of Code](../../series/advent-of-code-2022/) 系列，或查看 [Learning
    Chapel](https://chapel-lang.org/learning.html) 页面以获取更多资源。
- en: In this post, we’ll jump right in to using Chapel’s GPU programming features.
    If you’re interested in a refresher on what problems GPUs are useful for, check
    out the [*Appendix* section containing this information](#appendix-what-sorts-of-problems-benefit-from-gpus).
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们将直接使用 Chapel 的 GPU 编程功能。如果你对 GPU 适用于解决哪些问题感兴趣，请查看 [*Appendix* 部分包含此信息](#appendix-what-sorts-of-problems-benefit-from-gpus)。
- en: 'Locales and `on` Statements: The Foundation of Chapel'
  id: totrans-9
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Locales 和 `on` 语句：Chapel 的基础
- en: 'When GPUs are in play, it’s possible for code to be executing in different
    places: it could be running either on a CPU, as most programmers are used to,
    or on a GPU. GPUs and CPUs differ significantly; code well-suited for one may
    not be well-suited for the other. Chapel gives the programmer control over where
    their code is running through its notion of *locales*. Quoting from the [Chapel
    specification](https://chapel-lang.org/docs/primers/locales.html):'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 当 GPU 参与时，代码可能在不同的地方执行：它可能在 CPU 上执行，就像大多数程序员习惯的那样，也可能在 GPU 上执行。GPU 和 CPU 有很大的不同；对其中一个很合适的代码可能对另一个不合适。Chapel
    通过其 *locales* 概念使程序员能够控制其代码在何处运行。引用自 [Chapel 规范](https://chapel-lang.org/docs/primers/locales.html)：
- en: In Chapel, the `locale` type refers to a unit of the machine resources on which
    your program is running.
  id: totrans-11
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 在 Chapel 中，`locale` 类型指的是程序运行所在的机器资源单元。
- en: In other words, a locale is a part of the computer that can run code; this might
    represent a GPU or a CPU. Chapel’s *`on` statement*, when given a locale, can
    be used to explicitly state where a particular piece of code should be executed.
    For example, the following code computes and prints the even numbers up to ten,
    running the computation on the first GPU locale.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 换句话说，locale 是计算机上可以运行代码的部分；这可能代表一个 GPU 或一个 CPU。当给定一个 locale 时，Chapel 的 *`on`
    语句* 可以用于明确声明特定代码应该在何处执行。例如，下面的代码计算并打印出前十个偶数，将计算在第一个 GPU locale 上执行。
- en: '|'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '[PRE0]'
  id: totrans-14
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '|'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '[PRE1]'
  id: totrans-16
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '|'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: 'The code starts with an `on` block targeting a GPU *sub-locale*. This block
    references `here`, a special variable that refers to the locale currently running
    the code. When GPU support is enabled, locales include a field called `gpus`,
    which is an array of sub-locales, each representing an installed GPU. On a locale
    with a single GPU, `here.gpus` will be a single-element array. On locales with
    more than one GPU (which is [note: I mentioned supercomputers here for a reason.
    Chapel''s GPU support has been tested on large machines, including Frontier, the
    only exascale supercomputer in the TOP500 list at the time of publishing. ]),
    `here.gpus` will have as many elements as there are GPUs. Thus, `here.gpus[0]`
    is the machine’s first GPU.'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 代码以一个针对 GPU *子区域* 的 `on` 块开始。此块引用 `here`，一个特殊的变量，指的是当前运行代码的区域。当启用 GPU 支持时，区域包括一个名为
    `gpus` 的字段，该字段是一个子区域数组，每个子区域代表一个已安装的 GPU。在具有单个 GPU 的区域上，`here.gpus` 将是一个单元素数组。在具有多个
    GPU 的区域上（这是 [注意：我之前提到超级计算机是有原因的。 Chapel 的 GPU 支持已在大型机器上进行了测试，包括 Frontier，目前发布时
    TOP500 列表中唯一的 Exascale 超级计算机。]），`here.gpus` 将具有与 GPU 数量相同的元素。因此，`here.gpus[0]`
    是机器的第一个 GPU。
- en: <details><summary>**(How do I enable GPU support?)**</summary>
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: <details><summary>**（如何启用 GPU 支持？）**</summary>
- en: 'To enable GPU support, Chapel must be built with with the `CHPL_LOCALE_MODEL`
    environment variable set to `gpu`. In a Bash session, the variable can be set
    as follows:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 要启用 GPU 支持，Chapel 必须使用设置为 `gpu` 的 `CHPL_LOCALE_MODEL` 环境变量构建。在 Bash 会话中，可以如下设置该变量：
- en: '[PRE2]'
  id: totrans-21
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'If you have an NVIDIA GPU, this should be enough to compile and run GPU-enabled
    programs. For AMD GPUs, you will also need to specify your GPU’s architecture
    using the `CHPL_GPU_ARCH` environment variable:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你有 NVIDIA GPU，这应该足以编译和运行启用 GPU 的程序。对于 AMD GPU，你还需要使用 `CHPL_GPU_ARCH` 环境变量指定
    GPU 的架构：
- en: '[PRE3]'
  id: totrans-23
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Finally, even if you don’t have a GPU, Chapel provides a mode called ‘cpu-as-device’.
    In this mode, you still get a `here.gpus` array, and can write code targeting
    GPUS; however, your computer’s CPU will be used to execute all code. This makes
    it possible to develop GPU-enabled programs without access to a GPU. Setting the
    `CHPL_GPU` environment variable to `cpu` enables ‘cpu-as-device’ mode:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，即使你没有 GPU，Chapel 也提供了一种称为 'cpu-as-device' 的模式。在此模式下，你仍然会得到一个 `here.gpus`
    数组，并且可以编写针对 GPU 的代码；然而，你的计算机 CPU 将用于执行所有代码。这使得可以在没有 GPU 的情况下开发支持 GPU 的程序。设置 `CHPL_GPU`
    环境变量为 `cpu` 就可以启用 'cpu-as-device' 模式：
- en: Please see the page on [building chapel](https://chapel-lang.org/docs/usingchapel/building.html)
    as well as the [GPU technical note](https://chapel-lang.org/docs/technotes/gpu.html)
    for more information on GPU-related environment variables.</details>
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 请参阅关于 [构建 Chapel](https://chapel-lang.org/docs/usingchapel/building.html) 的页面以及
    [GPU 技术说明](https://chapel-lang.org/docs/technotes/gpu.html) 以获取有关与 GPU 相关的环境变量的更多信息。</details>
- en: The code generates even numbers by iterating over the indices one through five,
    multiplying each by two. GPUs are good at solving the same sub-problem many times
    in parallel; doubling each number can be its own sub-problem, making the loop
    in the example well-suited for GPU execution. The multiplication loop is written
    using the `foreach` keyword, which tells Chapel that it is safe to execute in
    parallel. The language takes care of the rest. Chapel has traditional `for` loops
    as well, like the second loop in the example. We’ll talk about the difference
    between the two further down.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 该代码通过对索引从一到五进行迭代来生成偶数，并将每个数字乘以二。GPU 擅长并行解决相同的子问题；使每个数字加倍可以成为其自己的子问题，使示例中的循环非常适合在
    GPU 上执行。乘法循环使用 `foreach` 关键字编写，告诉 Chapel 可以安全地并行执行。语言会处理剩下的事情。Chapel 还有传统的 `for`
    循环，就像示例中的第二个循环一样。我们将在后面进一步讨论两者之间的区别。
- en: 'Whereas the `foreach` loop in the example runs on the GPU, the `writeln` on
    line 9 doesn’t. Unlike the multiplications, printing a single string to the console
    is not a good match for the GPU. Generally, code that’s suitable for GPU execution
    can be broken up into many similar and independent pieces. For a loop, this translates
    into *order-independence*. A loop is order-independent if no iteration affects
    any of the others. Thinking back to our example, let’s examine the multiplication
    loop again:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 在示例中，`foreach` 循环在 GPU 上运行，而第 9 行的 `writeln` 并没有。与乘法不同，向控制台打印单个字符串并不适合在 GPU
    上执行。通常，适合在 GPU 上执行的代码可以分解为许多相似且独立的部分。对于循环来说，这转化为 *无序性*。如果循环是无序的，则不会有任何迭代影响其他迭代。回顾我们的示例，让我们再次检查乘法循环：
- en: '[PRE4]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'We can observe that the result of `5*2` does not affect the computation of
    `3*2`. Each iteration accesses a different element of `A`, so there are no data
    races. Thus, our example is an instance of an order-independent loop. Chapel leaves
    it up to the programmer to indicate which loops have this property; to assert
    that a loop is order-independent, it should be written [note: Chapel also features
    `forall` loops. These loops allow the the data structure being traversed to decide
    how iteration is parallelized. Data structures that ship with the Chapel standard
    library are smart enough to make use of order-independence, so an eligible `forall`
    loop on a GPU locale would also be executed on the GPU.'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以观察到`5*2`的结果不会影响`3*2`的计算。每次迭代访问`A`的不同元素，因此不存在数据竞争。因此，我们的示例是无序循环的一个实例。Chapel让程序员指示哪些循环具有这个特性；为了断言循环是无序的，应该写成
    [注意：Chapel还具有`forall`循环。这些循环允许被遍历的数据结构决定如何并行迭代。Chapel标准库提供的数据结构足够智能，可以利用无序性，因此，在GPU环境中合格的`forall`循环也会在GPU上执行。
- en: See the [loops primer](https://chapel-lang.org/docs/main/primers/loops.html)
    to learn more about Chapel's various types of loops. ]
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 查看[循环入门](https://chapel-lang.org/docs/main/primers/loops.html)以了解有关Chapel各种类型循环的更多信息。
- en: 'It’s not hard to see why order-independent loops lend themselves well to GPU
    execution. If it doesn’t matter in what order the loop’s iterations are executed,
    then we can think of each iteration as an independent sub-problem to be handed
    off to a GPU core. This observation is the foundation of Chapel’s GPU support:
    **order-independent loops can be executed in parallel on the GPU**. In fact, Chapel
    will automatically convert order-independent loops into GPU code whenever it can.'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 很容易看出为什么无序循环很适合在GPU上执行。如果循环的迭代顺序无关紧要，那么我们可以将每个迭代视为要交给GPU核心处理的独立子问题。这一观察是Chapel
    GPU支持的基础：**无序循环可以在GPU上并行执行**。实际上，Chapel会自动将无序循环转换为GPU代码，只要可能。
- en: <details><summary>**(I know CUDA/HIP. Can you tell me more about how this works?)**</summary>
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: <details><summary>**（我了解CUDA/HIP。你能告诉我更多关于它是如何工作的吗？）**</summary>
- en: In CUDA and HIP, writing a GPU-enabled program generally involves creating a
    function marked as `device` or `global`, and using this function in a kernel launch.
    Under the hood, Chapel does the same.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 在CUDA和HIP中，编写启用GPU的程序通常涉及创建一个标记为`device`或`global`的函数，并在核心启动中使用此函数。在底层，Chapel也是这样做的。
- en: When Chapel encounters a GPU-eligible loop, it converts its body into a function,
    named something like `chpl_gpu_kernel_filename_linenumber`. If the loop body /
    newly defined kernel function contains calls to other functions or methods, Chapel
    also generates `device` versions of these.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 当Chapel遇到一个适合GPU的循环时，它会将其主体转换为一个函数，命名为类似`chpl_gpu_kernel_filename_linenumber`的函数。如果循环主体/新定义的核函数包含对其他函数或方法的调用，Chapel还会生成这些函数的`device`版本。
- en: Chapel inserts a kernel launch alongside the original loop. Since the same code
    could be executed from `here.gpus[0]` (GPU) or the default locale (CPU), Chapel
    preserves the loop as well. Thus, on a GPU it performs the kernel launch, and
    on a CPU, it falls back to the loop.</details>
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: Chapel在原始循环旁边插入一个核心启动。由于相同的代码可以从`here.gpus[0]`（GPU）或默认的环境（CPU）执行，Chapel也保留了循环。因此，在GPU上执行核心启动，在CPU上则退回到循环。
- en: In comparison, the second loop in the example is *order-dependent*.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 相比之下，示例中的第二个循环是*顺序相关的*。
- en: '[PRE5]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: We specifically intend for the elements of `A` to be printed in order. Because
    of this, the iteration that prints the fifth element has to happen after printing
    the fourth; there’s a dependency. In Chapel, loops whose iterations need to be
    executed one after another (called *serial loops*) are written using the `for`
    keyword. Loops written using `for` are not considered by the compiler for GPU
    execution.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 我们特意希望按顺序打印`A`的元素。因此，打印第五个元素的迭代必须发生在打印第四个之后；存在依赖关系。在Chapel中，需要按顺序执行迭代的循环（称为*串行循环*）使用`for`关键字编写。使用`for`编写的循环不被编译器视为GPU执行的对象。
- en: 'We have now scrutinized almost every piece of the example. One significant
    piece remains: we also declared an array `A` to contain the even numbers:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们几乎审查了示例的每个部分。还有一个重要的部分：我们还声明了一个包含偶数的数组`A`：
- en: '[PRE6]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'An important aspect of the `on` statement is that variables declared inside
    an `on` block logically live on the locale targeted by the statement. Typically,
    this means that it’s faster to access these variables from that same locale, and
    slower from other locales. Here’s an example:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '`on` 语句的一个重要方面是，在 `on` 块内部声明的变量在逻辑上位于语句指定的区域上。通常，这意味着从同一区域访问这些变量会更快，而从其他区域访问会更慢。这里有一个例子：'
- en: '[PRE7]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '[note: In the future, Chapel aims to make it possible for GPU code to access
    arrays declared outside of the GPU locale. However, this would require some communication
    between the GPU and CPU, initiated by the GPU. *GPU-driven communication* like
    that is on our roadmap, but not supported at the time of writing. ] to make an
    array accessible from code that runs on the GPU (such as our `foreach` loop),
    the array must live on the GPU locale. This is why we declare `A` inside the `on`
    block, rather than outside of it. Had `A` been declared outside the `on` block,
    it would’ve been on the CPU locale, allocated in CPU memory.'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '[注意：未来，Chapel 的目标是使 GPU 代码能够访问在 GPU 区域外声明的数组。但是，这将需要 GPU 发起的 GPU 和 CPU 之间的一些通信。类似的
    *GPU 驱动通信* 已经在我们的路线图上，但在撰写本文时不受支持。] 为了让在 GPU 上运行的代码（如我们的 `foreach` 循环）可以访问数组，该数组必须位于
    GPU 区域上。这就是为什么我们在 `on` 块内部声明 `A`，而不是在外部声明它的原因。如果 `A` 是在 `on` 块外部声明的，它将位于 CPU 区域上，分配在
    CPU 内存中。'
- en: Now we’ve gone over the whole introductory example, demonstrating how GPUs can
    be targeted in Chapel using `on` blocks and `foreach` loops. However, this example
    is a very simple program, meant to introduce key concepts in Chapel’s GPU support.
    What we’ve seen so far is only the beginning. In the next section, we’ll take
    a look at other features of Chapel that mesh seamlessly with GPU programming.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经介绍了整个入门示例，演示了如何使用 `on` 块和 `foreach` 循环在 Chapel 中针对 GPU 进行定位。然而，这个例子是一个非常简单的程序，旨在介绍
    Chapel 的 GPU 支持的关键概念。到目前为止，我们所看到的只是开始。在下一节中，我们将看看与 GPU 编程无缝结合的 Chapel 的其他功能。
- en: What Else Can You Do on a GPU?
  id: totrans-45
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 你还能在 GPU 上做什么？
- en: A large portion of the Chapel language can run on the GPU. In this section,
    I’ll give a bit of a whirlwind tour of what can be done. To learn more about the
    individual language features I’ll be showing off, please take a look at the resources
    mentioned at the beginning of this article.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: Chapel 语言的大部分功能都可以在 GPU 上运行。在本节中，我将快速浏览一下可以完成的工作。要了解我将展示的个别语言功能，请参阅本文开头提到的资源。
- en: 'As we go through examples, the reader might wonder: how can we be sure that
    the code ran on the GPU? Tracking which code runs where is a slightly more advanced
    topic. For the sake of simplicity, I will define a custom function, `numKernelLaunches`.
    The word *kernel* typically refers to bits of code that run on a GPU. A *kernel
    launch* is the process of a CPU starting the execution of a kernel on the GPU.
    You don’t have to understand how my helper function works; it’s sufficient to
    think of `numKernelLaunches` as returning the total number of kernel launches
    since the last time we checked. I’ll be using this function to ensure that code
    ran on the GPU when we expected it to.'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们逐个例子进行时，读者可能会想知道：我们如何确保代码是在 GPU 上运行的？跟踪代码在哪里运行是一个稍微高级的话题。为了简单起见，我将定义一个自定义函数，`numKernelLaunches`。
    *内核* 一词通常指在 GPU 上运行的代码片段。 *内核启动* 是 CPU 启动在 GPU 上执行内核的过程。你不必理解我的辅助函数是如何工作的；把 `numKernelLaunches`
    想象成返回自上次检查以来启动的内核总数就足够了。我将使用这个函数来确保代码在我们期望的时候在 GPU 上运行。
- en: <details><summary>**(I do want to see how the function is defined!)**</summary>
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: <details><summary>**(我想看看函数是如何定义的！)**</summary>
- en: '|'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '[PRE8]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '|'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '[PRE9]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '|</details>'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: '|</details>'
- en: All examples in this section will be performed on the GPU locale.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 本节中的所有示例都将在 GPU 区域上执行。
- en: Let’s start with something fun. At the beginning of this article, we used the
    GPU to multiply some numbers by two using a `foreach`. We can do this even more
    succinctly using a Chapel feature called [*promotion*](https://chapel-lang.org/docs/users-guide/datapar/promotion.html).
    Promotion allows us to pass an array to an operation that requires a scalar value.
    When we do so, the operation is automatically performed on each element of the
    array. This is automatically done in parallel — potentially on the GPU.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从一些有趣的事情开始。在本文的开头，我们使用 GPU 通过 `foreach` 将一些数字乘以两个。我们可以使用 Chapel 的一个特性称为 [*提升*](https://chapel-lang.org/docs/users-guide/datapar/promotion.html)
    来更简洁地完成这个任务。提升允许我们将一个数组传递给需要标量值的操作。当我们这样做时，操作会自动在数组的每个元素上执行。这个操作会自动并行执行 —— 可能在
    GPU 上。
- en: 'If our operation is “multiply by two”, we can write code as follows:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们的操作是“乘以二”，我们可以编写如下代码：
- en: '|'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '[PRE10]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '|'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '[PRE11]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '|'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: In just one simple line, we were able to write code that runs on the GPU.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 仅用一行简单的代码，我们就能编写在 GPU 上运行的代码。
- en: 'Let’s move on to our next example. We can call most Chapel functions from code
    running on the GPU. The following example samples the built-in sine function `sin`
    on ten increments within the interval \([0, 2\pi)\):'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们继续下一个示例。我们可以从在 GPU 上运行的代码中调用大多数 Chapel 函数。以下示例在区间\([0, 2\pi)\)内的十个增量上对内置正弦函数`sin`进行采样：
- en: '|'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '[PRE12]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '|'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '[PRE13]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '|'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: 'Functions called from the GPU can be user-defined and arbitrarily complex.
    In the following example, we use promotion again to compute the first 20 Fibonacci
    numbers. This example [note: The example is unoptimized in both an algorithmic
    and GPU-specific sense.'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 从 GPU 调用的函数可以是用户自定义的，也可以是任意复杂的。在以下示例中，我们再次使用提升来计算前 20 个斐波那契数。这个例子【注：该示例在算法和
    GPU 特定意义上都未经过优化。
- en: On the algorithmic side, those familiar with the analysis of algorithms will
    likely know that there exists an \(O(n)\) algorithm to compute the \(n\)th Fibonacci
    number. Meanwhile, our naive algorithm is \(O(2^n)\) — quite bad. Furthermore,
    our implementation doesn't make use of any memoization, performing a lot of redundant
    computations between array elements.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 在算法方面，熟悉算法分析的人可能会知道，存在一个 \(O(n)\) 算法来计算第 \(n\) 个斐波那契数。与此同时，我们的朴素算法是 \(O(2^n)\)
    ——相当糟糕。此外，我们的实现没有使用任何记忆化技术，在数组元素之间执行了大量冗余计算。
- en: On the GPU-specific side, we are writing code that will cause *thread divergence*.
    Each thread will perform a slightly different sequence of steps, which makes it
    much harder for the GPU to execute them. ] but serves as a good demonstration
    of using arbitrary functions in a kernel, including recursive ones.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 在 GPU 特定方面，我们正在编写将导致*线程分歧*的代码。每个线程将执行略有不同的步骤序列，这使得 GPU 难以执行它们。] 但是作为使用核函数中的任意函数的良好示例。
- en: '|'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '[PRE14]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: '|'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '[PRE15]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: '|'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: 'Here, `fib` is a normal Chapel function that is being promoted by calling it
    with the integer range `0..#20`. Note that we are able to use it in a GPU kernel
    without needing to do anything special. This is true in general: in Chapel, once
    a function has been defined, it can be called from both the GPU and the CPU.'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，`fib`是一个普通的 Chapel 函数，通过使用整数范围`0..#20`来进行提升。请注意，我们可以在 GPU 核函数中使用它，而不需要进行任何特殊处理。总的来说，在
    Chapel 中，一旦定义了函数，就可以从 GPU 和 CPU 中调用它。
- en: 'Loops can also be executed as part of a kernel. For our last example, we’ll
    define a two-dimensional array `Square`, then sum each of its columns on the GPU.
    The following code will initialize, populate, and print this new square array:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 循环也可以作为核函数的一部分执行。对于我们的最后一个示例，我们将定义一个二维数组`Square`，然后在 GPU 上对其每一列求和。以下代码将初始化、填充和打印这个新的方阵数组：
- en: '|'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '[PRE16]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: '|'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '[PRE17]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: '|'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: With the new `Square` array in hand, we move on to summing its columns. Inside
    the `foreach` loop, we use another `for` loop as we normally would.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 拿着新的`Square`数组，我们继续对其列求和。在`foreach`循环内部，我们像往常一样使用另一个`for`循环。
- en: '|'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '[PRE18]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: '|'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '[PRE19]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: '|'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: Having gone through a few examples, we can conclude our `on` block and return
    to computing on the CPU.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 经过几个示例后，我们可以结束 `on` 块，并返回到在 CPU 上计算。
- en: '|'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '[PRE20]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: '|'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '[PRE21]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: '|'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: Now we have seen several example computations using Chapel’s GPU support. Where
    can we run all of this code? Let’s talk about that next.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经看到了几个使用 Chapel 的 GPU 支持的示例计算。我们可以在哪里运行所有这些代码？让我们接下来谈谈这个问题。
- en: Where Can I Run Chapel Code for GPUs?
  id: totrans-97
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 我可以在哪里运行 Chapel 代码以使用 GPU？
- en: You might recall from the introduction that Chapel’s GPU support is vendor-neutral.
    This means that GPU-enabled Chapel code can be executed on both NVIDIA and AMD
    GPUs, without any modifications! This includes all of the code presented in this
    article.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能还记得介绍中提到 Chapel 的 GPU 支持是供应商中立的。这意味着可以在 NVIDIA 和 AMD 的 GPU 上执行启用了 GPU 的 Chapel
    代码，而无需进行任何修改！这包括本文中提到的所有代码。
- en: In fact, you don’t even need a GPU. Chapel supports a mode called ‘CPU-as-device’,
    which allows code targeting GPUs to transparently execute on the CPU. You can
    prototype GPU-enabled code on a laptop without dedicated graphics, then switch
    to a GPU-enabled machine whenever you like. In fact, this is how this article
    was written.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，你甚至不需要 GPU。Chapel 支持一种称为“CPU 作为设备”的模式，它允许针对 GPU 的代码在 CPU 上透明地执行。你可以在没有专用图形处理器的笔记本电脑上原型化启用了
    GPU 的代码，然后随时切换到启用了 GPU 的机器。事实上，这就是撰写本文的方式。
- en: 'Finally, Chapel’s GPU support, like the rest of the language, is scalable.
    Programs prototyped on a laptop can easily be executed on a supercomputer, with
    good performance. Although writing code in a scalable way requires [note: Not
    using only the first GPU via `here.gpus[0]` is one such step. ] Chapel makes it
    easy to create parallel code that runs everywhere.'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，Chapel 的 GPU 支持与语言的其余部分一样，是可扩展的。在笔记本电脑上原型化的程序可以轻松地在超级计算机上执行，并获得良好的性能。尽管以可扩展的方式编写代码需要注意[注意：不仅使用
    `here.gpus[0]` 可以访问第一个 GPU。]，但 Chapel 让创建能够在任何地方运行的并行代码变得轻松。
- en: Summary
  id: totrans-101
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 总结
- en: 'Chapel’s GPU support goes much deeper, but this might be a good stopping point
    for an introductory article — we’ve already covered a lot of ground! Let’s look
    back at a few of the things we’ve covered:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: Chapel 的 GPU 支持更深入，但这可能是介绍性文章的一个好的停顿点 —— 我们已经涵盖了很多内容！让我们回顾一下我们已经涵盖的一些内容：
- en: Chapel’s *locales* represent parts of the machine that can run code and store
    variables.
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chapel 的*locale*表示可以运行代码和存储变量的机器的部分。
- en: The `on` statement specifies where code should be executed, including on the
    GPU.
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`on` 语句指定代码应该在哪里执行，包括在 GPU 上。'
- en: '*Order-independent* loops, written using `foreach`, are automatically executed
    on the GPU.'
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 `foreach` 编写的*无序*循环会自动在 GPU 上执行。
- en: Much of the Chapel language can be used in GPU code.
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chapel 语言的大部分功能都可以在 GPU 代码中使用。
- en: This includes *promotion*, functions plain and recursive, and loops.
  id: totrans-107
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这包括*推广*，函数（普通和递归）以及循环。
- en: All of this is vendor-neutral; Chapel works with both NVIDIA and AMD GPUs.
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 所有这些都是与供应商无关的；Chapel 可与 NVIDIA 和 AMD 的 GPU 配合使用。
- en: If you’d like to see more information on Chapel’s GPU support in particular,
    the [tech note](https://chapel-lang.org/docs/technotes/gpu.html) contains many
    details and examples of GPU code, and the [**GPU Programming** section of the
    release notes from Chapel 1.32](https://chapel-lang.org/releaseNotes/1.31-1.32/05-gpus.pdf)
    contains a “crash course on GPU programming”.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您想查看有关 Chapel 的 GPU 支持的更多信息，特别是 [技术说明](https://chapel-lang.org/docs/technotes/gpu.html)
    包含了许多 GPU 代码的细节和示例，以及 Chapel 1.32 发行说明的 [**GPU 编程**部分](https://chapel-lang.org/releaseNotes/1.31-1.32/05-gpus.pdf)
    包含了“GPU 编程速成课程”。
- en: 'Of course, we have not yet seen a practical example of solving a problem on
    the GPU. We also have not yet seen how to analyze the performance of GPU-enabled
    programs in Chapel, or how to improve said performance. Finally, we didn’t see
    how Chapel’s GPU support frictionlessly integrates with the rest of the language
    to allow writing code that runs across all GPUs **and** compute nodes. For a little
    sneak peek of that last point, take a look at this version of the “even numbers”
    example that runs on all GPUs in the system:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，我们还没有看到在 GPU 上解决问题的实际示例。我们也还没有看到如何分析 Chapel 中启用 GPU 的程序的性能，或者如何提高该性能。最后，我们没有看到
    Chapel 的 GPU 支持如何无缝集成到语言的其余部分，以允许编写在所有 GPU **和**计算节点上运行的代码。关于最后一点的一点小提示，可以查看系统中所有
    GPU 上运行的“偶数”示例的版本：
- en: '|'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '[PRE22]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: '|'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '[PRE23]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: '|'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: In just six lines of code, we wrote a program that can make use of the computational
    resources of an entire supercomputer.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 仅用六行代码，我们编写了一个可以利用整个超级计算机的计算资源的程序。
- en: We will be coming back to all of the above topics in subsequent posts, starting
    with writing multi-node, multi-GPU programs. Stay tuned!
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在随后的文章中回到上述所有主题，从编写多节点、多 GPU 程序开始。敬请关注！
- en: '**The entire Chapel program presented in this post can be viewed here:**'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: '**本文中介绍的整个 Chapel 程序可以在此处查看：**'
- en: <details>|
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: <details>|
- en: '[PRE24]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: '|'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '[PRE25]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: '|</details>'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: '|</details>'
- en: 'Appendix: What Sorts of Problems Benefit from GPUs?'
  id: totrans-124
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 附录：哪些问题受益于 GPU？
- en: 'A huge difference between a GPU and a CPU is the number of *cores*. A core
    is the part of a processor that’s responsible for executing instructions / machine
    code. Whereas I am writing this from a computer with around 10 CPU cores, a cursory
    Google search indicates that the NVIDIA RTX 4070 GPU (picked arbitrarily by searching
    “consumer NVIDIA GPU”) has 5888 cores — [note: In fact, because of the huge number
    of cores that concurrently execute code, GPUs are an example of a [ massively
    parallel](https://en.wikipedia.org/wiki/Massively_parallel) architecture. ] Of
    course, a CPU core is not the same as a GPU core: GPU cores tend to be much weaker
    individually.'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: GPU 和 CPU 之间的一个巨大区别是*核心数*。核心是处理器的一部分，负责执行指令/机器代码。虽然我现在用的电脑大约有 10 个 CPU 核心，但粗略的谷歌搜索显示，NVIDIA
    RTX 4070 GPU（通过搜索“消费级 NVIDIA GPU”随意选择）有 5888 个核心 — [注：事实上，由于大量并发执行代码的核心数量，GPU
    是[大规模并行](https://en.wikipedia.org/wiki/Massively_parallel)架构的一个例子。] 当然，CPU 核心和
    GPU 核心并不相同：GPU 核心在个体上往往要弱得多。
- en: 'For problems that can be decomposed into a large number of [note: For GPUs,
    the sub-problems being similar is actually very significant. A single GPU core
    is actually solving multiple instances of a sub-problem *at the same time*, executing
    their code in lock-step. As soon as lock-step execution is no longer possible,
    and two sub-problems need different approaches, the GPU has to struggle a lot
    more. This problem is called *thread divergence*. ] pieces, having a large number
    of weak cores is ideal. Each core can receive its own share of the problem, and
    work on it [note: It''s possible, and not too uncommon, to have a small degree
    of coordination between GPU cores when solving problems. GPUs are able allocate
    memory that''s shared between cores, and to synchronize the cores'' execution.
    However, this is slightly more advanced than this introductory article. ] because
    each sub-problem is independent, all of the cores can be working on their share
    at exactly the same time. This can result in a very significant speedup over a
    single, powerful core: such a core would need to go through each of the pieces
    one after another.'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 对于可以分解为大量[注：对于 GPU，子问题的相似性实际上非常重要。单个 GPU 核心实际上在*同时*解决多个子问题的实例，以锁步方式执行它们的代码。一旦锁步执行不再可能，并且两个子问题需要不同的方法，GPU
    就会遇到很大的困难。这个问题被称为*线程分歧*。]片段的问题，拥有大量弱核心是理想的。每个核心可以获得自己问题的一部分，并且可以[注：解决问题时，GPU 核心之间可能有小程度的协调并不少见。GPU
    能够分配共享内存，用于核心间的同步执行。然而，这比起这篇入门文章稍微复杂一些。]工作，因为每个子问题都是独立的，所有核心都可以在完全相同的时间工作在各自的份额上。这可能会比单个强大的核心获得非常显著的加速：这样的核心需要逐个处理每个片段。
- en: One example of a problem amenable to GPU execution is rendering an image to
    a screen. In fact, as the name *Graphics Processing Unit* suggests, this was the
    very problem GPUs were developed to solve. When rendering, the color of each pixel
    can be computed relatively independently, based on depth and texture information;
    each core of the GPU can thus be working on a handful of pixels at a time, all
    in parallel, significantly speeding up the process.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 一个适合在 GPU 上执行的问题的示例是将图像渲染到屏幕上。事实上，正如*图形处理单元*这个名字所暗示的，这正是 GPU 开发出来解决的问题。在渲染时，每个像素的颜色可以相对独立地计算，基于深度和纹理信息；因此，GPU
    的每个核心可以同时并行地处理一些像素，大大加速了这个过程。
- en: An entire problem domain that is well-suited for GPU execution is linear algebra.
    In matrix multiplication, for example, each cell in the output matrix can be computed
    independently from all the other cells; once again, this means that each GPU core
    can be working on a handful of cells in parallel. Many things, including neural
    networks, can be formulated in terms of matrix operations; fast linear algebra
    leads to faster machine learning models.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 一个完全适合在 GPU 上执行的问题域是线性代数。在矩阵乘法中，例如，输出矩阵中的每个单元格都可以独立于其他所有单元格进行计算；再一次，这意味着每个 GPU
    核心可以同时处理一些单元格。许多东西，包括神经网络，都可以用矩阵运算来表述；快速的线性代数导致更快的机器学习模型。
- en: 'The descriptions above should give you a taste of *what* can be done with GPUs.
    The next obvious question is *how* we can do all this with Chapel. This brings
    us to the topic of this blog: getting Chapel to run code on the GPU.'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 上述描述应该让你了解到 GPU 可以做些什么。下一个显而易见的问题是我们如何用 Chapel 实现所有这些。这就带我们来到这篇博客的主题：让 Chapel
    在 GPU 上运行代码。
