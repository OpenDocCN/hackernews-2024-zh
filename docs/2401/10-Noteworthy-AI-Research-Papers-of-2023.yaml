- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 类别：未分类
- en: 'date: 2024-05-27 14:32:38'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 时间：2024-05-27 14:32:38
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: 10 Noteworthy AI Research Papers of 2023
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 2023年值得关注的10篇人工智能研究论文
- en: 来源：[https://magazine.sebastianraschka.com/p/10-ai-research-papers-2023](https://magazine.sebastianraschka.com/p/10-ai-research-papers-2023)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://magazine.sebastianraschka.com/p/10-ai-research-papers-2023](https://magazine.sebastianraschka.com/p/10-ai-research-papers-2023)
- en: This year has felt distinctly different. I've been working in, on, and with
    machine learning and AI for over a decade, yet I can't recall a time when these
    fields were as popular and rapidly evolving as they have been this year.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 今年的感觉明显不同。我在机器学习和人工智能领域工作了十多年，但我记不起还有哪个时期这些领域像今年这样受欢迎和快速发展过。
- en: To conclude an eventful 2023 in machine learning and AI research, I'm excited
    to share 10 noteworthy papers I've read this year. My personal focus has been
    more on large language models, so you'll find a heavier emphasis on large language
    model (LLM) papers than computer vision papers this year.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 总结一个充满事件的2023年的机器学习和人工智能研究，我很高兴分享我今年阅读的10篇值得关注的论文。我的个人重点更多地放在大型语言模型上，所以今年你会发现大型语言模型（LLM）论文比计算机视觉论文更加重视。
- en: I resisted labeling this article "Top AI Research Papers of 2023" because determining
    the "best" paper is subjective. The selection criteria were based on a mix of
    papers I either particularly enjoyed or found impactful and worth noting. (The
    sorting order is a recommended reading order, not an ordering by perceived quality
    or impact.)
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 我抵制将这篇文章标记为“2023年顶级人工智能研究论文”，因为确定“最佳”论文是主观的。选择标准基于我特别喜欢或认为有影响力并值得注意的论文的混合。（排序顺序是推荐阅读顺序，而不是按
    perceived quality or impact 排序。）
- en: '**By the way, if you scroll down to the end of this article, you''ll find a
    little surprise. Thanks for all your support, and I wish you a great start to
    the new year!**'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '**顺便说一下，如果你向下滚动到本文的结尾，你会发现一个小惊喜。感谢你们的支持，祝你们新年快乐！**'
- en: 'With ***[Pythia: A Suite for Analyzing Large Language Models Across Training
    and Scaling](https://arxiv.org/abs/2304.01373)***, the researchers originally
    released 8 LLMs ranging from 70M to 12B parameters (with both weights and data
    publicly released, which is rare).'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '通过 ***[Pythia: A Suite for Analyzing Large Language Models Across Training
    and Scaling](https://arxiv.org/abs/2304.01373)***，研究人员最初发布了8个LLM，参数范围从70M到12B（同时公开发布权重和数据，这是罕见的）。'
- en: But in my opinion, the standout feature of this paper is that they also released
    the training details, analyses, and insights (some of them shown in the annotated
    figure below).
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 但在我看来，这篇论文的突出特点是他们还公布了训练细节、分析和见解（其中一些显示在下面的注释图中）。
- en: 'Here are some questions that the Pythia paper addresses:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是Pythia论文所回答的一些问题：
- en: Does pretraining on duplicated data (i.e., training for >1 epoch) make a difference?
    It turns out that deduplication does not benefit or hurt performance.
  id: totrans-13
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在重复数据上进行预训练（即，进行>1个epoch的训练）是否会有所不同？事实证明，去重并不会对性能造成益处或伤害。
- en: Does training order influence memorization? Unfortunately, it turns out that
    it does not. "Unfortunately," because if this was true, we could mitigate undesirable
    verbatim memorization issues by reordering the training data.
  id: totrans-14
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 训练顺序是否影响记忆？不幸的是，事实证明确实如此。“不幸的是”，因为如果这是真的，我们可以通过重新排序训练数据来减轻不良的逐字记忆问题。
- en: Does pretrained term frequency influence task performance? Yes, few-shot accuracy
    tends to be higher for terms that occur more frequently.
  id: totrans-15
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 预训练的词频是否影响任务性能？是的，频率较高的词的少量精度往往更高。
- en: Does increasing the batch size affect training efficiency and model convergence?
    Doubling the batch size halves the training time but doesn't hurt convergence.
  id: totrans-16
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 增加批量大小是否影响训练效率和模型收敛性？批量大小加倍可以将训练时间减半，但不会影响收敛。
- en: Today, only six months later, the LLMs are by no means groundbreaking. However,
    I am including this paper because it not only tries to answer interesting questions
    about training settings but is also a positive example regarding details and transparency.
    Moreover, the small LLMs in the <1B range are nice templates for small studies
    and tinkering, or starters for pretraining experiments (here's a link to their
    [GitHub repository](https://github.com/EleutherAI/pythia)).
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 今天，仅仅六个月后，大型语言模型已经不再是突破性的。然而，我将这篇论文包含在内，是因为它不仅尝试回答有关训练设置的有趣问题，而且还是有关细节和透明度的积极例子。此外，<1B范围内的小型LLM是进行小型研究和调试的良好模板，或者是预训练实验的起点（这里是他们的[GitHub存储库](https://github.com/EleutherAI/pythia)的链接）。
- en: My wish for 2024 is that we see more studies like this and well-written papers
    in the coming year!
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 我对 2024 年的期望是我们看到更多类似这样的研究和在接下来的一年里写得更好的论文！
- en: '***[Llama 2: Open Foundation and Fine-Tuned Chat Models](https://arxiv.org/abs/2307.09288)***
    is the follow-up paper to Meta''s popular first Llama paper.'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: '***[Llama 2: 开放基础与精细调整的聊天模型](https://arxiv.org/abs/2307.09288)*** 是 Meta 热门第一篇
    Llama 论文的后续论文。'
- en: 'Llama 2 models, which range from 7B to 70B parameters, are one of the reasons
    this paper made it onto this list: these are still among the most capable and
    widely used openly available models. Worth noting is that the [Llama 2 license](https://github.com/facebookresearch/llama/blob/main/LICENSE)
    also permits use in commercial applications (see the [Request to Access page](https://ai.meta.com/resources/models-and-libraries/llama-downloads/)
    for details).'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: Llama 2 模型，参数范围从 7B 到 70B，是本文被列入此列表的原因之一：这些仍然是最具能力且广泛使用的开放可用模型之一。值得注意的是，[Llama
    2 许可证](https://github.com/facebookresearch/llama/blob/main/LICENSE) 还允许在商业应用中使用（有关详情，请参阅[访问请求页面](https://ai.meta.com/resources/models-and-libraries/llama-downloads/)）。
- en: On the model side, what differentiates the Llama 2 suite from many other LLMs
    is that the models come as standard pretrained models and chat models that have
    been finetuned via reinforcement learning with human feedback (RLHF, the method
    used to create ChatGPT) to follow human instructions similar to ChatGPT — RLHF-finetuned
    models are still rare.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 在模型方面，Llama 2 套件与许多其他 LLMs 的不同之处在于，模型提供了标准预训练模型和聊天模型，经过增强学习和人类反馈（RLHF，创建 ChatGPT
    的方法）微调，以遵循类似 ChatGPT 的人类指令 —— 进行 RLHF 微调的模型仍然很少见。
- en: For more details on RLHF and how it's used in Llama 2, see my more comprehensive
    standalone article below.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 关于 RLHF 及其在 Llama 2 中的使用的更多详情，请参阅下文的我撰写的更全面的独立文章。
- en: Next to the fact that Llama 2 models are widely used and come with RLHF instruction-finetuned
    variants, the other reason I decided to include the paper on this list is the
    accompanying in-depth 77-page research report.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 除了 Llama 2 模型被广泛使用且带有 RLHF 指导微调变体这一事实之外，我决定将该论文列入此列表的另一个原因是其配套的深度研究报告共计 77 页。
- en: Here, the authors also nicely illustrated the evolution of the Llama 2 70B Chat
    models, tracing their journey from the initial supervised finetuning (SFT-v1)
    to the final RLHF finetuning stage with PPO (RLHF-v5). The chart reflects consistent
    improvements in both the harmlessness and helpfulness axes, as shown in the annotated
    plots below.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，作者还很好地说明了 Llama 2 70B 聊天模型的演变，追溯了它们从最初的监督微调（SFT-v1）到最终的 PPO RLHF 微调阶段（RLHF-v5）的过程。该图反映了在有害性和有帮助性两个方面的持续改善，如下所示的注释图所示。
- en: '*Annotated figure from Llama 2 paper ([https://arxiv.org/abs/2307.09288](https://arxiv.org/abs/2307.09288))
    showing the performance progression from the first iteration of the supervised
    finetuned model (SFT-1) to the final RLHF-finetuned chat model (RLHF-v5).*'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '*从 Llama 2 论文中的注释图表 ([https://arxiv.org/abs/2307.09288](https://arxiv.org/abs/2307.09288))
    可见，从第一次监督微调模型（SFT-1）到最终的 RLHF 微调聊天模型（RLHF-v5）的性能逐步提升。*'
- en: Even though models such as Mistral-8x7B (more later), DeepSeek-67B, and YI-34B
    top the larger Llama-2-70B models in public benchmarks, Llama 2 still remains
    a common and popular choice when it comes to openly available LLMs and developing
    methods on top of it.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管像 Mistral-8x7B（稍后详述）、DeepSeek-67B 和 YI-34B 等模型在公共基准测试中超越了更大型的 Llama-2-70B
    模型，但在公开可用 LLMs 和在其基础上开发方法方面，Llama 2 仍然是一个常见和流行的选择。
- en: Furthermore, even though some benchmarks indicate that there may be better models,
    one of the bigger challenges this year has been the trustworthiness of benchmarks.
    For instance, how do we know that the models haven't been trained on said benchmarks
    and the scores aren't inflated? In classic machine learning, when someone proposed
    a new gradient boosting model, it was relatively easy to reproduce the results
    and check. Nowadays, given how expensive and complex it is to train LLMs (and
    the fact that most researchers either don't disclose the architecture or the training
    data details), it is impossible to tell.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，尽管一些基准测试表明可能存在更好的模型，但今年面临的一个更大挑战是基准测试的可信度。例如，我们怎么知道模型没有在这些基准测试上进行训练，分数也没有被夸大？在经典机器学习中，当有人提出一个新的梯度提升模型时，很容易重现结果并进行检查。如今，考虑到训练
    LLMs 的成本和复杂性（以及大多数研究人员要么不披露架构，要么不披露训练数据的细节），这是不可能的。
- en: To conclude, it's refreshing to see Meta doubling down on open source even though
    every other major company is now rolling out its own proprietary large language
    models (Google's Bard and Gemini, Amazon's Q, and Twitter/X's Grok, and OpenAI's
    ChatGPT).
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 总之，尽管其他主要公司现在都在推出自己的专有大型语言模型（如 Google 的 Bard 和 Gemini、亚马逊的 Q 以及 Twitter/X 的
    Grok，以及 OpenAI 的 ChatGPT），但看到 Meta 在开源方面加倍努力令人耳目一新。
- en: '***[QLoRA: Efficient Finetuning of Quantized LLMs](https://arxiv.org/abs/2305.14314)***
    has been one of the favorite techniques in the LLM research and finetuning community
    this year because it makes the already popular LoRA (low-rank adaptation) technique
    more memory efficient. In short, this means that you can fit larger models onto
    smaller GPUs.'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: '***[QLoRA：量化 LLMs 的高效微调](https://arxiv.org/abs/2305.14314)*** 是今年 LLM 研究和微调社区中最受欢迎的技术之一，因为它使已经流行的
    LoRA（低秩调整）技术更加内存高效。简而言之，这意味着你可以将更大的模型适配到更小的 GPU 上。'
- en: '*A short visual summary of regular LoRA*'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: '*常规 LoRA 的简短视觉总结*'
- en: QLoRA stands for quantized LoRA (low-rank adaptation). The standard LoRA method
    modifies a pretrained LLM by adding low-rank matrices to the weights of the model's
    layers. These matrices are smaller and, therefore, require fewer resources to
    update during finetuning.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: QLoRA 代表量化 LoRA（低秩调整）。标准的 LoRA 方法通过向模型层的权重中添加低秩矩阵来修改预训练的 LLM。这些矩阵较小，因此在微调过程中需要较少的资源来更新。
- en: In QLoRA, these low-rank matrices are quantized, meaning their numerical precision
    is reduced. This is done by mapping the continuous range of values in these matrices
    to a limited set of discrete levels. This process reduces the model's memory footprint
    and computational demands, as operations on lower-precision numbers are less memory-intensive
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 在 QLoRA 中，这些低秩矩阵被量化，意味着它们的数值精度降低。这是通过将这些矩阵中的值的连续范围映射到有限的一组离散级别来实现的。这个过程减少了模型的内存占用和计算需求，因为对低精度数值的操作不那么内存密集。
- en: According to the [QLoRA paper](https://arxiv.org/abs/2305.14314), QLoRA reduces
    the memory requirements of a 65B Llama model to fit onto a single 48 GB GPU (like
    an A100). The 65B Guanaco model, obtained from quantized 4-bit training of 65B
    Llama, maintains full 16-bit finetuning task performance, reaching 99.3% of the
    ChatGPT performance after only 24 hours of finetuning.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 根据[QLoRA 论文](https://arxiv.org/abs/2305.14314)，QLoRA 将 65B Llama 模型的内存需求降低到适合单个
    48 GB GPU（例如 A100）的程度。通过对 65B Llama 进行量化的 4 位训练，获得的 65B Guanaco 模型在完整的 16 位微调任务性能方面保持不变，仅在微调
    24 小时后即可达到 ChatGPT 性能的 99.3%。
- en: 'I''ve also run many QLoRA experiments this year and found QLoRA a handy tool
    for reducing GPU memory requirements during finetuning. There''s a trade-off,
    though: the extra quantization step results in an additional computation overhead,
    meaning the training will be a bit slower than regular LoRA.'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 今年我也运行了许多 QLoRA 实验，并发现 QLoRA 在微调过程中降低 GPU 内存需求非常方便。不过，存在一个折衷：额外的量化步骤会导致额外的计算开销，意味着训练速度会比常规
    LoRA 稍慢一些。
- en: '*Excerpt from my LoRA & QLoRA experiments that I wrote about previously [here](https://magazine.sebastianraschka.com/p/practical-tips-for-finetuning-llms)*'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: '*摘自我之前写过的关于我的 LoRA 和 QLoRA 实验的文章 [这里](https://magazine.sebastianraschka.com/p/practical-tips-for-finetuning-llms)*'
- en: LLM finetuning remains as relevant as ever as researchers and practitioners
    aim to create custom LLMs. And I appreciate techniques like QLoRA that help make
    this process more accessible by lowering the GPU memory-requirement barrier.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 随着研究人员和实践者致力于创建定制 LLMs，LLM 微调仍然如此重要。我赞赏像 QLoRA 这样的技术，它们通过降低 GPU 内存需求门槛来帮助使这一过程更加可行。
- en: 'Looking at all the papers published this year, ***[BloombergGPT: A Large Language
    Model for Finance](https://arxiv.org/abs/2303.17564)*** may look like an odd choice
    for a top-10 list because it didn''t result in a groundbreaking new insight, methodology,
    or open-source model.'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 从今年发布的所有论文来看，***[BloombergGPT：金融领域的大型语言模型](https://arxiv.org/abs/2303.17564)***
    在前十名榜单中看起来可能是一个奇怪的选择，因为它没有带来突破性的新见解、方法论或开源模型。
- en: 'I include it because it''s an interesting case study where someone pretrained
    a relatively large LLM on a domain-specific dataset. Moreover, the description
    was pretty thorough, which is becoming increasingly rare. This is especially true
    when it comes to papers with authors employed at companies -- one of the trends
    this year was that major companies are becoming increasingly secretive about architecture
    or dataset details to preserve trade secrets in this competitive landscape (PS:
    I don''t fault them for that).'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 我包含这个案例是因为它是一个有趣的案例研究，有人在一个特定领域的数据集上预训练了一个相对较大的语言模型。此外，该描述相当详尽，这在论文中变得越来越少见。尤其是当涉及到公司雇员的论文时--今年的一个趋势是，主要公司在保护商业机密以在这个竞争激烈的环境中保留贸易秘密时，越来越秘密地关于架构或数据集的细节（附言：我不因此而指责他们）。
- en: Also, BloombergGPT made me think of all the different ways we can pretrain and
    finetune models on domain-specific data, as summarized in the figure below (note
    that this was not explored in the BloombergGPT paper, but it would be interesting
    to see future studies on that).
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，BloombergGPT 让我想到了我们可以预训练和微调模型在特定领域数据上的不同方法，如下图所示（请注意，这在 BloombergGPT 论文中没有探讨，但未来对此进行研究将是很有趣的）。
- en: '*The different ways of pretraining and finetuning LLMs.*'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '*语言模型的不同预训练和微调方法。*'
- en: In short, BloombergGPT is a 50-billion parameter language model for finance,
    trained on 363 billion tokens from finance data and 345 billion tokens from a
    general, publicly available dataset. For comparison, GPT-3 is 3.5x larger (175
    billion parameters) but was trained on 1.4x fewer tokens (499 billion).
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，BloombergGPT 是一个用于金融领域的 500 亿参数的语言模型，训练数据来自 3630 亿金融数据令牌和 3450 亿来自一般公开数据集的令牌。作为比较，GPT-3
    的规模是其 3.5 倍（1750 亿参数），但是训练数据令牌却少了 1.4 倍（4990 亿）。
- en: Why did the authors use an architecture with "only" 50 billion parameters since
    GPT-3 is 3.5x larger? That's easier to answer. They adopted the Chinchilla scaling
    laws and found this to be a good size given the available size of the finance
    data.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 作者为什么使用了一个“只有”500亿参数的架构，而 GPT-3 的规模是其3.5倍？这个问题更容易回答。他们采用了金丝猴缩放定律，并发现这是一个很好的规模，考虑到金融数据的可用规模。
- en: Is it worth (pre)training the LLM on the combined dataset from scratch? Based
    on the paper, the model performs really well in the target domain. However, we
    don't know whether it's better than a) further pretraining a pretrained model
    on domain-specific data or b) finetuning a pretrained model on domain-specific
    data.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 是否值得从头开始（预）训练 LLM 来合并数据集？根据论文，该模型在目标领域表现得非常好。然而，我们不知道它是否优于 a) 在领域特定数据上进一步预训练一个预训练模型或
    b) 在领域特定数据上微调一个预训练模型。
- en: Despite the little criticism above, overall, this is an interesting paper that
    serves as an interesting case study and example for domain-specific LLMs; plus,
    it leaves room for further research on pretraining versus finetuning to instill
    knowledge into an LLM.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管上面有些批评，总的来说，这是一篇有趣的论文，为特定领域的LLM提供了一个有趣的案例研究和示例；此外，它为预训练与微调将知识灌输到LLM中留下了进一步研究的空间。
- en: '(PS: For those curious about a comparison to finetuning, as [Rohan Paul shared](https://x.com/rohanpaul_ai/status/1738474868214235163?s=20)
    with me, the "small" [AdaptLLM-7B](https://arxiv.org/abs/2309.09530) model outperforms
    BloombergGPT on one dataset and nearly matches its performance on three other
    finance datasets. Although BloombergGPT appears to be slightly better overall,
    it''s worth noting that training AdaptLLM-7B cost about $100, in contrast to BloombergGPT''s
    multi-million dollar investment.)'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: （附言：对于那些对比微调感兴趣的人，正如[Rohan Paul分享](https://x.com/rohanpaul_ai/status/1738474868214235163?s=20)给我看的，这个“小”[AdaptLLM-7B](https://arxiv.org/abs/2309.09530)模型在一个数据集上的表现优于BloombergGPT，并几乎与其在其他三个金融数据集上的表现相匹配。尽管
    BloombergGPT 在整体上似乎稍微更好，但值得注意的是，训练 AdaptLLM-7B 大约只需花费 100 美元，而 BloombergGPT 则是多次百万美元的投资。）
- en: 'Before discussing the ***[Direct Preference Optimization: Your Language Model
    is Secretly a Reward Model](https://arxiv.org/abs/2305.18290)*** paper, let''s
    take a short step back and discuss the method it aims to replace, Reinforcement
    Learning from Human Feedback (RLHF).'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 在讨论***[直接偏好优化：您的语言模型实际上是一个奖励模型](https://arxiv.org/abs/2305.18290)***论文之前，让我们退一步，讨论它旨在取代的方法，即来自人类反馈的强化学习（RLHF）。
- en: 'RLHF is the main technique behind ChatGPT and Llama 2 Chat models. In RLHF,
    which I described in more detail in a [separate article](https://magazine.sebastianraschka.com/p/llm-training-rlhf-and-its-alternatives),
    we use a multi-step procedure:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: RLHF 是 ChatGPT 和 Llama 2 聊天模型背后的主要技术。在 RLHF 中，我们使用一个多步骤的过程，我在 [单独的文章](https://magazine.sebastianraschka.com/p/llm-training-rlhf-and-its-alternatives)
    中详细描述了这个过程：
- en: 'Supervised finetuning: The model is initially trained on a dataset containing
    instructions and the desired responses.'
  id: totrans-48
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 监督微调：模型最初在包含指令和期望响应的数据集上进行训练。
- en: 'Reward modeling: Human raters provide feedback on the model''s outputs. This
    feedback is used to create a reward model, which learns to predict what kinds
    of outputs are to be preferred.'
  id: totrans-49
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 奖励建模：人类评分员对模型的输出提供反馈。这些反馈用于创建一个奖励模型，该模型学习预测哪些输出是优选的。
- en: 'Proximal policy optimization (PPO): The model generates outputs, and the reward
    model scores each output. The PPO algorithm uses these scores to adjust the model''s
    policy toward'
  id: totrans-50
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 近端策略优化（PPO）：模型生成输出，奖励模型对每个输出进行评分。PPO 算法使用这些分数来调整模型的策略以...
- en: generating higher-quality outputs. (This is a reinforcement learning algorithm
    used to finetune the model's policy.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 生成更高质量的输出。（这是一种用于微调模型策略的强化学习算法。）
- en: '*Example of two training examples from a dataset for the supervised instruction
    finetuning step. Note that the "input" is optional.*'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: '*监督指令微调步骤的数据集中的两个训练示例。请注意，“输入”是可选的。*'
- en: While RLHF is popular and effective, as we've seen with ChatGPT and Llama 2,
    it's also pretty complex to implement and finicky.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管 RLHF 很受欢迎且有效，正如我们在 ChatGPT 和 Llama 2 中所见，但实施起来也相当复杂且挑剔。
- en: '[The Direct Preference Optimization (DPO) paper](https://arxiv.org/abs/2305.18290)
    introduces an algorithm that optimizes language models to align with human preferences
    **without** explicit reward modeling or reinforcement learning. Instead, DPO uses
    a simple classification objective.'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: '[直接偏好优化（DPO）论文](https://arxiv.org/abs/2305.18290) 提出了一种优化语言模型以符合人类偏好的算法，**而无需**
    明确的奖励建模或强化学习。相反，DPO 使用简单的分类目标。'
- en: In DPO, we still keep the supervised finetuning step (step 1 above), but we
    replace steps 2 and 3 with a single step to further finetune the model on the
    preference data. In other words, DPO skips the reward model creation required
    by RLHF entirely, which significantly simplifies the finetuning process.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 在 DPO 中，我们仍然保留了监督微调步骤（上面的第一步），但我们用一个单一步骤来进一步微调模型的偏好数据，以替换步骤 2 和 3。换句话说，DPO 完全跳过了
    RLHF 所需的奖励模型创建步骤，这显著简化了微调过程。
- en: 'How well does it work? There haven''t been many models trained with DPO until
    very recently. (This makes sense because DPO is also a relatively recent method.)
    However, one recent example is the Zephyr 7B model described in *[Zephyr: Direct
    Distillation of LM Alignment](https://arxiv.org/abs/2310.16944)*. Zephyr-7B is
    based on a Mistral-7B base LLM that has been finetuned using DPO. (There will
    be more on Mistral later.)'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: '它的效果如何？直到最近，使用DPO训练的模型并不多见。（这是有道理的，因为DPO也是一种相对较新的方法。）然而，最近的一个例子是 *[Zephyr:
    Direct Distillation of LM Alignment](https://arxiv.org/abs/2310.16944)* 中描述的 Zephyr
    7B 模型。Zephyr-7B 基于一个基础 LLM Mistral-7B，该基础 LLM 使用 DPO 进行了微调。（稍后将详细介绍 Mistral。）'
- en: As the performance tables below reveal, the 7B-parameter Zephyr model outperformed
    all other models in its size class at the time of its release. Even more impressively,
    Zephyr-7B even surpassed the 10 times larger 70B-parameter Llama 2 chat model
    on the conversational [MT-Bench](https://arxiv.org/abs/2306.05685) benchmark as
    well.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 如下的性能表格显示，在发布时，7B 参数的 Zephyr 模型的性能优于其尺寸类别中的所有其他模型。更令人印象深刻的是，Zephyr-7B 甚至在对话性
    [MT-Bench](https://arxiv.org/abs/2306.05685) 基准测试中也超过了规模是其 10 倍的 70B 参数的 Llama
    2 聊天模型。
- en: In summary, the appeal of the DPO paper lies in the simplicity of its method.
    The scarcity of chat models trained using RLHF, with Llama 2 as a notable exception,
    can likely be attributed to the complexity of the RLHF approach. Given this, I
    think it's reasonable to anticipate an increase in the adoption of DPO models
    in the coming year.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 总之，DPO 论文的吸引力在于其方法的简单性。使用 RLHF 进行训练的聊天模型的稀缺性，以 Llama 2 为显著例外，很可能归因于 RLHF 方法的复杂性。鉴于此，我认为可以预期在未来一年中
    DPO 模型的采用将会增加。
- en: I must admit that the ***[Mistral 7B paper](https://arxiv.org/abs/2310.06825)***
    wasn't among my favorites due to its brevity. However, the model it proposed was
    quite impactful.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 我必须承认 ***[Mistral 7B 论文](https://arxiv.org/abs/2310.06825)*** 并不是我最喜欢的论文之一，因为它篇幅较短。但是，它提出的模型却具有相当大的影响力。
- en: 'I decided to include the paper on this list because the Mistral 7B model was
    not only very popular upon release, but also served as the base model, leading
    to the development of two other notable models: Zephyr 7B and the latest Mistral
    Mixture of Experts (MoE) approach. These models are good examples of the trend
    I foresee for small LLMs in (at least) the early half of 2024.'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 我决定将这篇论文列入列表，因为Mistral 7B模型不仅在发布时非常流行，而且作为基础模型，引发了另外两个值得注意的模型的发展：Zephyr 7B和最新的Mistral
    Mixture of Experts（MoE）方法。这些模型是我预见到的小型LLMs的趋势的良好示例，至少在2024年上半年是如此。
- en: Before we discuss the Zephyr 7B and Mistral MoE models, let's briefly talk about
    Mistral 7B itself.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 在讨论Zephyr 7B和Mistral MoE模型之前，让我们简要谈谈Mistral 7B本身。
- en: In short, The Mistral 7B paper introduces a compact yet powerful language model
    that, despite its relatively modest size of 7 billion tokens, outperforms its
    larger counterparts, such as the 13B Llama 2 model, in various benchmarks. (Next
    to the two-times larger [Qwen 14B](https://github.com/QwenLM/Qwen), Mistral 7B
    was also the base model used in the winning solutions of this year's [NeurIPS
    LLM Finetuning & Efficiency challenge](https://llm-efficiency-challenge.github.io/leaderboard).)
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，Mistral 7B论文介绍了一个紧凑而强大的语言模型，尽管其规模相对较小，只有70亿个标记，但在各种基准测试中仍然优于其较大的对应物，如13B
    Llama 2模型。（紧邻两倍大的[Qwen 14B](https://github.com/QwenLM/Qwen)，Mistral 7B也是今年[NeurIPS
    LLM Finetuning & Efficiency challenge](https://llm-efficiency-challenge.github.io/leaderboard)获胜解决方案中使用的基础模型。）
- en: Why exactly it is so good is unclear, but it might likely be due to its training
    data. Neither Llama 2 nor Mistral discloses the training data, so we can only
    speculate.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么它如此出色的原因尚不清楚，但可能是由于其训练数据。既不Llama 2也不Mistral公开其训练数据，因此我们只能推测。
- en: Architecture-wise, the model shares group-query attention with Llama 2\. While
    being very similar to Llama 2, one interesting addition to the Mistral architecture
    is sliding window attention to save memory and improve computational throughput
    for faster training. (Sliding window attention was previously proposed in [Child
    et al. 2019](https://arxiv.org/abs/1904.10509) and [Beltagy et al. 2020](https://arxiv.org/abs/2004.05150).)
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 就架构而言，该模型与Llama 2共享组查询注意。尽管与Llama 2非常相似，但Mistral架构的一个有趣的补充是滑动窗口注意，以节省内存并提高计算吞吐量，从而加快训练速度。（滑动窗口注意先前在[Child
    et al. 2019](https://arxiv.org/abs/1904.10509)和[Beltagy et al. 2020](https://arxiv.org/abs/2004.05150)中提出。）
- en: The sliding window attention mechanism used in Mistral is essentially a fixed-sized
    attention block that allows a current token to attend only a specific number of
    previous tokens (instead of all previous tokens), which is illustrated in the
    figure below.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: Mistral中使用的滑动窗口注意机制本质上是一个固定大小的注意块，允许当前标记仅关注特定数量的前面标记（而不是所有前面标记），如下图所示。
- en: In the specific case of 7B Mistral, the attention block size is 4096 tokens,
    and the researchers were training the model with up to 100k token context sizes.
    To provide a  concrete example, in regular self-attention, a model at the 50,000th
    token can attend all previous 49,999 tokens. In sliding window self-attention,
    the Mistral model can only attend tokens 45,904 to 50,000 (since 50,000 - 4,096
    = 45,904).
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 对于7B Mistral的具体情况，注意力块的大小为4096个标记，研究人员正在训练模型时使用长达100k个标记的上下文大小。举个具体例子，在常规自注意力中，位于第50000个标记的模型可以关注前面的49999个标记。在滑动窗口自注意力中，Mistral模型只能关注标记450904至50000（因为50000
    - 4096 = 450904）。
- en: However, sliding window attention is mainly used to improve computational performance.
    The fact that Mistral outperforms larger Llama 2 models is likely not because
    of sliding window attention but rather despite sliding window attention.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，滑动窗口注意主要用于提高计算性能。Mistral优于较大的Llama 2模型的事实可能不是因为滑动窗口注意，而是尽管有滑动窗口注意。
- en: One reason Mistral 7B is an influential model is that it served as the base
    model for Zephyr 7B, as mentioned earlier in the DPO section. Zephyr 7B, the first
    popular model trained with DPO to outperform other alternatives, has potentially
    set the stage for DPO to become the preferred method for finetuning chat models
    in the coming months.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: Mistral 7B成为有影响力的模型的一个原因是它作为Zephyr 7B的基础模型，如DPO部分中早些时候提到的。 Zephyr 7B是第一个使用DPO进行训练并优于其他替代方法的流行模型，这可能为DPO在未来几个月成为调整聊天模型的首选方法奠定了基础。
- en: Another noteworthy model derived from Mistral 7B is the recently released [Mistral
    Mixture of Experts (MoE) model](https://mistral.ai/news/mixtral-of-experts/),
    also known as Mixtral-8x7B. This model matches or exceeds the performance of the
    larger Llama-2-70B on several public benchmarks.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个从 Mistral 7B 派生出的值得注意的模型是最近发布的[Mistral Mixture of Experts（MoE）模型](https://mistral.ai/news/mixtral-of-experts/)，也称为
    Mixtral-8x7B。这个模型在几个公共基准测试中与或超过了更大的 Llama-2-70B 的性能。
- en: For more benchmarks, also see the official [Mixtral blog post announcement](https://mistral.ai/news/mixtral-of-experts/).
    The team also released a Mixtral-8x7B-Instruct model that has been finetuned with
    DPO (but as of this writing there are no benchmarks comparing it to Llama-2-70-Chat,
    the RLHF-finetuned model).
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 要了解更多基准测试，请参阅官方的[Mixtral 博客文章公告](https://mistral.ai/news/mixtral-of-experts/)。团队还发布了一个已经使用
    DPO 进行微调的 Mixtral-8x7B-Instruct 模型（但截至撰写本文时，尚无将其与 Llama-2-70-Chat、RLHF-微调模型进行比较的基准测试）。
- en: '*Mixtral architecture overview based on the param.json file that the Mistral
    team originally shared via a magnet link on social media*'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: '*基于 Mistral 团队最初通过社交媒体上的磁力链接共享的 param.json 文件的 Mixtral 架构概述*'
- en: GPT-4 is also rumored to be an MoE consisting of 16 submodules. Each of these
    16 submodules is rumored to have 111 billion parameters (for reference, GPT-3
    has 175 billion parameters). If you read my [AI and Open Source in 2023 article](https://magazine.sebastianraschka.com/p/ai-and-open-source-in-2023)
    approximately two months ago, I mentioned that "It will be interesting to see
    if MoE approaches can lift open-source models to new heights in 2024". It looks
    like Mixtral started this trend early, and I am sure that this is just the beginning.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 据传闻，GPT-4 也是一个由 16 个子模块组成的 MoE。据传闻，这 16 个子模块中的每个都具有 1110 亿个参数（供参考，GPT-3 具有 1750
    亿个参数）。如果您大约两个月前阅读了我的[2023 年 AI 和开源文章](https://magazine.sebastianraschka.com/p/ai-and-open-source-in-2023)，我提到“看看
    MoE 方法是否能够在 2024 年将开源模型推向新的高度”将是很有趣的。看来 Mixtral 提前开始了这一趋势，我相信这只是一个开始。
- en: If you are new to MoE models, here's a short explanation.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您对 MoE 模型还不熟悉，请看这里的简短说明。
- en: The figure above shows the architecture behind the Switch Transformer, which
    uses 1 expert per token with 4 experts in total. Mixtral-8x-7B, on the other hand,
    consists of 8 experts and uses 2 experts per token.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 上图显示了 Switch Transformer 背后的架构，该架构每个令牌使用 1 位专家，总共有 4 位专家。另一方面，Mixtral-8x-7B
    由 8 位专家组成，每个令牌使用 2 位专家。
- en: Why MoEs? Combined, the 8 experts in a 7B model like Mixtral are still ~56B
    parameters. Actually, it's less than 56B, because the MoE approach is only applied
    to the FFN (feed forward network, aka fully-connected) layers, not the self-attention
    weight matrices. So, it's likely closer to 40-50B parameters.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么选择 MoE（Mixture of Experts）？在像 Mixtral 这样的 7B 模型中，8 位专家的参数总数仍然约为 ~56B。实际上，这不到
    56B，因为 MoE 方法仅应用于 FFN（前馈网络，又称全连接）层，而不是自注意力权重矩阵。因此，参数总数可能更接近 40-50B。
- en: Note that the router reroutes the tokens such that only <14B parameters (2x
    <7B, instead of all <56B) are used at a time for the forward pass, so the training
    (and especially inference) will be faster compared to the traditional non-MoE
    approach.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，路由器会重新路由令牌，以便每次仅使用 <14B 参数（2x <7B，而不是全部 <56B）进行前向传递，因此与传统非 MoE 方法相比，训练（尤其是推理）速度将更快。
- en: 'If you want to learn more about MoEs, here''s a reading list recommended by
    [Sophia Yang](https://twitter.com/sophiamyang):'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您想了解更多关于 MoE 的信息，请参考[Sophia Yang](https://twitter.com/sophiamyang)推荐的阅读列表：
- en: Furthermore, if you are interested in trying MoE LLMs, also check out the [OpenMoE](https://github.com/XueFuzhao/OpenMoE)
    repository, which implemented and shared MoE LLMs earlier this year.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，如果您有兴趣尝试 MoE LLMs，请还要查看今年早些时候实施并共享了 MoE LLMs 的[OpenMoE](https://github.com/XueFuzhao/OpenMoE)存储库。
- en: Mistral 7B, Zephyr 7B, and Mixtral-8x7B are excellent examples of the progress
    made in 2023 with small yet capable models featuring openly available weights.
    Another notable model, a runner-up on my favorite papers list, is Microsoft's
    phi series.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: Mistral 7B、Zephyr 7B 和 Mixtral-8x7B 是 2023 年取得的进展的优秀示例，这些示例具有小但功能强大的模型，其权重是公开可用的。另一个值得注意的模型是我最喜欢的论文列表中的亚军，即微软的
    phi 系列。
- en: The secret sauce of phi is training on high-quality data (referred to as “textbook
    quality data”) obtained by filtering web data.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: phi 的秘密酱是在高质量数据上进行训练（称为“教科书质量数据”），该数据通过过滤网络数据获得。
- en: Released in stages throughout 2023, the phi models include phi-1 (1.3B parameters),
    phi-1.5 (1.3B parameters), and phi-2 (2.7B parameters). The latter, released just
    two weeks ago, is already said to match or outperform Mistral 7B, despite being
    only half its size.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: phi 模型于 2023 年分阶段发布，包括 phi-1（13 亿参数）、phi-1.5（13 亿参数）和 phi-2（27 亿参数）。后者仅在两周前发布，据说已经能够与
    Mistral 7B 相匹敌或胜过，尽管规模仅为后者的一半。
- en: 'For more information about the phi models, I recommend the following resources:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 关于 phi 模型的更多信息，我推荐以下资源：
- en: '***[Orca 2: Teaching Small Language Models How to Reason](https://arxiv.org/abs/2311.11045)***
    is a relatively new paper, and time will tell whether it has a lasting impact
    on how we train LLMs in the upcoming months or years.'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '***[Orca 2：教导小型语言模型如何推理](https://arxiv.org/abs/2311.11045)*** 是一篇相对较新的论文，时间将告诉我们，它是否会对我们在未来几个月或几年中训练
    LLMs 的方式产生持久的影响。'
- en: I decided to include it because it combines several concepts and ideas.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 我决定包含它，因为它结合了几个概念和想法。
- en: One is the idea of distilling data from large, capable models such as GPT-4
    to create a synthetic dataset to train small but capable LLMs. This idea was described
    in the Self-Instruct paper, which came out last year. Earlier this year, Alpaca
    (a Llama model finetuned on ChatGPT outputs) really popularized this approach.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 其中一个是从像 GPT-4 这样的大型、功能强大的模型中提炼数据，以创建合成数据集，用于训练小型但功能强大的 LLM。这个想法是在去年发布的 Self-Instruct
    论文中描述的。今年早些时候，Alpaca（一种在 ChatGPT 输出上微调的 Llama 模型）真正推广了这种方法。
- en: 'How does this work? In a nutshell, it''s a 4-step process:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 这是如何工作的？简而言之，这是一个 4 步流程：
- en: Seed task pool with a set of human-written instructions (175 in this case) and
    sample instructions;
  id: totrans-87
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用一组人类编写的指令（本例中为 175 个）和样本指令来初始化任务池；
- en: Use a pretrained LLM (like GPT-3) to determine the task category;
  id: totrans-88
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用预训练的 LLM（如 GPT-3）来确定任务类别；
- en: Given the new instruction, let a pretrained LLM generate the response;
  id: totrans-89
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 给定新指令，让预训练的 LLM 生成响应；
- en: Collect, prune, and filter the responses before adding them to the task pool.
  id: totrans-90
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在将响应添加到任务池之前，收集、修剪和过滤响应。
- en: 'The other idea may not be surprising but worth highlighting: high-quality data
    is important for finetuning. For instance, the [LIMA paper](https://arxiv.org/abs/2305.11206)
    proposed a human-generated high-quality dataset consisting of only 1k training
    examples that can be used to finetuning to outperform the same model finetuned
    on 50k ChatGPT-generated responses.'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个想法可能并不令人惊讶，但值得强调的是：高质量的数据对微调非常重要。例如，[LIMA 论文](https://arxiv.org/abs/2305.11206)
    提出了一个人类生成的高质量数据集，其中仅包含 1000 个训练示例，可以用来进行微调，以胜过同样是在 5 万个 ChatGPT 生成的响应上进行微调的模型。
- en: Unlike previous research that heavily relied on imitation learning to replicate
    outputs from larger models, Orca 2 aims to teach "small" (i.e., 7B and 13B) LLMs
    various reasoning techniques (like step-by-step reasoning, recall-then-generate,
    etc.) and to help them determine the most effective strategy for each task. This
    approach has led Orca 2 to outperform similar-sized models noticeably and even
    achieve results comparable to models 5-10 times larger.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 与之前严重依赖模仿学习来复制较大模型输出的研究不同，Orca 2 的目标是教导“小型”（即 70 亿和 130 亿）LLM 们各种推理技术（如逐步推理、回忆再生成等），并帮助它们确定每项任务的最有效策略。这种方法使
    Orca 2 在相似大小的模型中明显表现优异，甚至达到与 5-10 倍大的模型可比拟的结果。
- en: While we haven't seen any extensive studies on this, the Orca 2 approach might
    also be able to address the issue of using synthetic data that was highlighted
    in the [The False Promise of Imitating Proprietary LLMs](https://arxiv.org/abs/2305.15717)
    paper. Here, the researchers investigated the finetuning weaker language models
    to imitate stronger proprietary models like ChatGPT, using examples such as Alpaca
    and Self-Instruct. Initially, the imitation models showed promising results, performing
    well in following instructions and receiving competitive ratings from crowd workers
    compared to ChatGPT. However, more follow-up evaluations revealed that these imitation
    models only seemed to perform well to a human observer but often generated factually
    incorrect responses.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然我们还没有看到任何关于此的广泛研究，但 Orca 2 方法也可能能够解决在 [The False Promise of Imitating Proprietary
    LLMs](https://arxiv.org/abs/2305.15717) 论文中强调的使用合成数据的问题。在这里，研究人员调查了微调较弱的语言模型以模仿较强的专有模型（如
    ChatGPT）的输出的问题，使用了 Alpaca 和 Self-Instruct 等示例。最初，模仿模型显示出令人鼓舞的结果，在遵循指令和接收与 ChatGPT
    相比竞争性评级方面表现良好。然而，更多的跟进评估揭示，这些模仿模型似乎只对人类观察者表现良好，但经常生成事实不准确的响应。
- en: In recent years, I've almost exclusively worked with large language transformers
    or vision transformers (ViTs) due to their good performance.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 近年来，我几乎专门使用大型语言变压器或视觉变压器（ViTs）因为它们性能良好。
- en: 'Switching gears from language to computer vision papers for the last three
    entries, what I find particularly appealing about transformers for computer vision
    is that pretrained ViTs are even easier to finetune than convolutional neural
    networks. (I summarized a short hands-on talk at CVPR earlier this year here:
    https://magazine.sebastianraschka.com/p/accelerating-pytorch-model-training).'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 在过去的三篇论文中，我从语言转向了计算机视觉论文，我发现有关计算机视觉的变压器尤其吸引人的是，预训练的ViTs甚至比卷积神经网络更容易微调。（我今年早些时候在CVPR上总结了一个简短的实践演讲：[https://magazine.sebastianraschka.com/p/accelerating-pytorch-model-training](https://magazine.sebastianraschka.com/p/accelerating-pytorch-model-training)）
- en: To my surprise, I stumbled upon the ***[ConvNets Match Vision Transformers at
    Scale](https://arxiv.org/abs/2310.16764)*** paper showing that convolutional neural
    networks (CNNs) are in fact, competitive with ViTs when given access to large
    enough datasets.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 令我惊讶的是，我偶然发现了***[ConvNets Match Vision Transformers at Scale](https://arxiv.org/abs/2310.16764)***论文，表明当卷积神经网络（CNNs）获得足够大的数据集时，它们实际上与ViTs竞争力相当。
- en: Here, researchers invested compute budgets of up to 110k TPU hours to do a fair
    comparison between ViTs and CNNs. The outcome was that when CNNs are pretrained
    with a compute budget similar to what is typically used for ViTs, they can match
    the performance of ViTs. For this, they pretrained on 4 billion labeled images
    from JFT and subsequently finetuned the models on ImageNet.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，研究人员投入了高达110k TPU小时的计算预算，以对ViTs和CNNs进行公平比较。结果表明，当CNNs使用类似于通常用于ViTs的计算预算进行预训练时，它们可以与ViTs的性能相匹配。为此，他们在来自JFT的40亿标记图像上进行了预训练，并随后在ImageNet上微调了模型。
- en: Object recognition and segmentation in images and videos, along with classification
    and generative modeling, are the main research fields in computer vision.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 对象识别和图像/视频分割以及分类和生成建模是计算机视觉的主要研究领域。
- en: 'To briefly highlight the difference between these two tasks: object detection
    about predicting bounding boxes and the associated labels; segmentation classifies
    each pixel to distinguish between foreground and background objects.'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 简要介绍一下这两个任务之间的区别：对象检测是关于预测边界框和相关标签；分割将每个像素分类以区分前景和背景对象。
- en: Meta's ***[Segment Anything](https://arxiv.org/abs/2304.02643)*** paper is a
    notable milestone for open source and image segmentation research. The paper introduces
    a new task, model, and dataset for image segmentation. The accompanying image
    datasets the largest segmentation dataset to date with over 1 billion masks on
    11 million images.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: Meta的***[Segment Anything](https://arxiv.org/abs/2304.02643)***论文是开源和图像分割研究的一个显著里程碑。该论文介绍了一个新的任务、模型和图像分割的数据集。伴随的图像数据集是迄今为止最大的分割数据集，包含超过11百万张图像上的10亿个蒙版。
- en: '*The Segment Anything Model (SAM) is designed for efficient, prompt-based image
    segmentation. Annotated screenshot from the Segment Anything paper, [https://arxiv.org/abs/2304.02643](https://arxiv.org/abs/2304.02643)*'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: '*Segment Anything Model (SAM)设计用于高效的基于提示的图像分割。分割任何东西论文中的注释截图，[https://arxiv.org/abs/2304.02643](https://arxiv.org/abs/2304.02643)*'
- en: However, what's rare and especially laudable is that the researchers used licensed
    and privacy-respecting images, so the model can be open-sourced without major
    copyright concerns.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，罕见且特别值得称赞的是，研究人员使用了经过许可且尊重隐私的图像，因此该模型可以在没有主要版权问题的情况下开源。
- en: The Segment Anything Model (SAM) consists of three main components, as summarized
    in the annotated figure above.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: Segment Anything Model (SAM)由上面注释的图中总结的三个主要组件组成。
- en: 'In slightly more details, the three components can be summarized as follows:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 稍微详细一点，这三个组件可以总结如下：
- en: An image encoder utilizing a masked autoencoder based on a pretrained vision
    transformer (ViT) that can handle high-resolution inputs. This encoder is run
    once per image and can be applied before prompting the model.
  id: totrans-105
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 利用基于预训练视觉变压器（ViT）的遮罩自动编码器的图像编码器，该编码器可以处理高分辨率输入。此编码器在每个图像上运行一次，并且可以在提示模型之前应用。
- en: 'A prompt encoder that handles two types of prompts: sparse (points, boxes,
    text) and dense (masks). Points and boxes are represented by positional encodings
    combined with learned embeddings for each prompt type. And free-form text uses
    an off-the-shelf text encoder from CLIP. Dense prompts, i.e., masks, are embedded
    using convolutions and summed element-wise with the image embedding.'
  id: totrans-106
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一个提示编码器处理两种类型的提示：稀疏（点、框、文本）和密集（掩码）。点和框由位置编码和每种提示类型的学习嵌入组合表示。自由文本使用 CLIP 中的现成文本编码器。密集提示，即掩码，使用卷积进行嵌入，并与图像嵌入逐元素求和。
- en: A mask decoder maps the image embedding, prompt embeddings, and an output token
    to a mask. This is a decoder-style transformer architecture that computes the
    mask foreground probability at each image location.
  id: totrans-107
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 掩码解码器将图像嵌入、提示嵌入和输出令牌映射到掩码。这是一种解码器样式的变压器架构，它计算每个图像位置的掩码前景概率。
- en: Image segmentation is important for applications like self-driving cars, medical
    imaging, and many others. In the short amount of 6 months, the paper has already
    been [cited more than 1500 times](https://scholar.google.com/scholar?cites=15741444728855576863&as_sdt=5,39&sciodt=0,39&hl=en),
    and there have already been many projects that have been built on top of this
    paper.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 图像分割对于自动驾驶汽车、医学成像和许多其他应用非常重要。在短短 6 个月的时间里，这篇论文已经被引用超过 1500 次，并且已经有许多项目建立在这篇论文之上。
- en: '***[Emu Video: Factorizing Text-to-Video Generation by Explicit Image Conditioning](https://arxiv.org/abs/2311.10709)***
    is another notable computer vision project from Meta''s research division.'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: '***[Emu 视频：通过显式图像条件生成文本到视频的因式分解](https://arxiv.org/abs/2311.10709)*** 是 Meta
    研究部门的另一个值得关注的计算机视觉项目。'
- en: Emu is a text-to-video model that can generate entire videos from text prompts.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: Emu 是一个可以从文本提示生成整个视频的文本到视频模型。
- en: While it's not the first model for impressive text-to-video generation, it compares
    very favorably to previous works.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然这不是令人印象深刻的文本到视频生成模型的第一个模型，但与以前的作品相比，它表现得非常出色。
- en: 'As the authors note, the Emu architecture setup is relatively simple compared
    to previous approaches. One of the main ideas here is that Emu factorizes the
    generation process into two steps: first, generating an image based on text (using
    a diffusion model), then creating a video conditioned on both the text and the
    generated image (using another diffusion model).'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 正如作者所指出的，与以前的方法相比，Emu 的架构设置相对简单。这里的一个主要思想是，Emu 将生成过程分解为两个步骤：首先，基于文本生成图像（使用扩散模型），然后在文本和生成的图像的条件下创建视频（使用另一个扩散模型）。
- en: 2022 has been a big year for text-to-image models like DALL-E 2, Stable Diffusion,
    and Midjourney. While text-to-image models remain very popular in 2023 (even though
    LLMs got most of the attention throughout the year), I think that text-to-video
    models are just about to become more prevalent in online communities in the upcoming
    year.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 2022 年是文本到图像模型的大年，如 DALL-E 2、Stable Diffusion 和 Midjourney。虽然文本到图像模型在 2023 年仍然非常受欢迎（尽管
    LLMs 在全年引起了大多数关注），但我认为文本到视频模型即将在未来一年在网络社区中变得更加普遍。
- en: Since I am not an image or video designer, I don't have use cases for these
    tools at the moment; however, text-to-image and text-to-video models are nonetheless
    interesting to watch as a general measure of progress regarding computer vision.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我不是图像或视频设计师，目前我没有这些工具的用例；然而，文本到图像和文本到视频模型仍然是作为计算机视觉进展的一般衡量标准而非常有趣的。
- en: '* * *'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: '*This magazine is personal passion project that does not offer direct compensation.
    However, for those who wish to support me, please consider purchasing a copy of
    [one of my books](https://sebastianraschka.com/books). If you find them insightful
    and beneficial, please feel free to recommend them to your friends and colleagues.*'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: '*本杂志是一个个人热情项目，不提供直接的报酬。然而，对于那些希望支持我的人，请考虑购买[我的其中一本书](https://sebastianraschka.com/books)。如果你觉得它们有见地并且有益，请随时向你的朋友和同事推荐。*'
- en: '**Your support means a great deal! Thank you!**'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: '**你的支持意义重大！谢谢！**'
