["```\nsize_t utf8_count_rvv(char const *buf, size_t len) {\n size_t sum = **0**;\n for (size_t vl; len > **0**; len -= vl, buf += vl) {\n vl = __riscv_vsetvl_e8m8(len);\n vuint8m8_t v = __riscv_vle8_v_u8m8((uint8_t*)buf, vl);\n vbool1_t b = __riscv_vmsne(__riscv_vsrl(v, **6**, vl), **0b10**, vl); \n sum += __riscv_vcpop(b, vl);\n }\n return sum;\n}\n\n```", "```\n*// the corresponding assembly code*:\n**utf8_count_rvv**:\n li      a2, **0**\n**loop**:\n vsetvli a3, a1, e8, m8, ta, ma\n vle8.v  v8, (a0)\n vsrl.vi v8, v8, **6**\n vmsne.vi        v16, v8, **0b10**\n vcpop.m a4, v16\n add     a2, a2, a4\n sub     a1, a1, a3\n add     a0, a0, a3\n bnez    a1, loop\n mv      a0, a2\n ret\n\n```", "```\nsize_t utf8_to_utf32(char const *src, size_t count, uint32_t *dest) {\n size_t tail = **3**;\n if (count < tail) return utf8_to_utf32_scalar(src, count, dest);\n `size_t n = count - tail;`\n `uint32_t *destBeg = dest;`\n ``for (size_t vl, vlOut; n > **0**; n -= vl, src += vl, dest += vlOut) {`\n `vl = __riscv_vsetvl_e8m2(n);`\n `vuint8m2_t v0 = __riscv_vle8_v_u8m2((uint8_t const*)src, vl);`\n ``*/* TODO: extract b1/b2/b3/b4 */*`\n `*/* TODO: remove prefixes */*`\n `*/* TODO: combine to b1234 */*`\n ``__riscv_vse32(dest, b1234, vlOut);`\n `}`\n ``*/* reparse last character + tail */*`\n `if (count > tail) {`\n `if ((src[**0**] >> **6**) == **0b10**) --dest;`\n `while ((src[**0**] >> **6**) == **0b10** && tail < count)`\n `--src, ++tail;`\n `}`\n `size_t ret = utf8_to_utf32_scalar(src, tail, dest);`\n `if (ret == **0**) return **0**;`\n `return (size_t)(dest - destBeg) + ret;`\n`}```", "```\n\n ``The loop structure itself is relatively simple, but we also need to make sure that we handle our lookahead of three correctly. For simplicity, we'll fall back to a scalar implementation for the tail, but we need to make sure we pass it a pointer to the beginning of a UTF-8 character.\nNotice how we are loading `vuint8m2_t`, this is a bit optimistic, but the idea is that we can get away with using 16 instead of 32 registers. This would essentially unroll the loop and make the scalar and mask operations less expensive.\n\nInitially, my thought was to create a mask of all the first bytes of all UTF-8 characters, compress those, and shift the mask right to extract the second bytes, and so on. Obtaining the mask is trivial, you just find all bytes that aren't continuation bytes: `x >> **6** != **0b10**`. RVV can't shift masks though, unless you treat the mask vector as a normal vector and assume it fits in 64-bit elements, or handle the carry.\nInstead, we can left-shift (`vslide1down`) the elements of the vector itself and leave the mask constant. This now comes back to why the lookahead works so well, as we can specify a scalar to shift into the rightmost element.\nThe following brings the above concepts together:\n\n```", "```\n\nRemoving the prefixes of b2/b3/b4 is trivial, we only need to mask out the two MSBs.\n\n```", "```\n\nFor b1 we need to determine how many bytes to mask-off, depending on the prefix. Fortunately, the first four bytes are enough to determine which bits to mask-off, and we could simply use a `vrgater` lookup table. On current hardware however another approach seems to be faster.\n\nInstead of a mask, we can also use left and then immediately right shifts by the same value to remove prefix bits. The shift amounts for this can almost be calculated using a single saturating subtract 10:\n\nThere is one outlier, we can easily address using a merge:\n\n```", "```\n\nThe `vrgather` implementation should probably be revisited once more hardware is available.\n\nAs described above, well first treat all elements as four-byte UTF-8 characters and then right shift this into place.\n\nThe first part is trivially using widening operations, and assuming we've already calculated the correct right shift amount that we can just widen and apply:\n\n```", "```\n\nComputing the shift amount it's a bit more involved than last time, but it can be done using as follows:\n\nThis maps directly into the following code:\n\n```", "```\n\nFor validation, we use the method described in [\"Validating UTF-8 In Less Than One Instruction Per Byte\"](https://arxiv.org/abs/2010.03090). I won't go into the full detail, but I'll summarise how it works:\n\nThere are seven error patterns in a 2 byte UTF-8 sequence, they can all be detected by only looking at the first 12 bits. We use three 4-bit lookup tables that map to a bitmask of the errors matching that pattern and AND them together to determine if the was an error. The only error not detected by this are related to 3-4 byte UTF-8 characters that have the wrong amount of continuation bytes. To detect those, the last remaining bit in the bitmask of the LUTs is used to indicate a pair of continuation bytes, which combined with a few comparisons manages to detect all invalid UTF-8 sequences.\n\nInstead of doing a three LMUL=2 vrgather, we do six LMUL=1 `vrgather`, as we don't need to cross any lanes, and this performs better because it needs to do a less complex task (see [later discussion](#gather)). To help with that, we define the `VRGATHER_u8m1x2` macro, which unrolls the `vrgather` for us. Before we get into the implementation we also need to define the error lookup tables, and some constants we'll use later:\n\n```", "```\n\n `Now we need to extract the 4-bits and do the lookup three times.\n\n```", "``` \n```", "```\n*/* IMPL: detect 3/4 byte errors */*\nvbool4_t is_3 = __riscv_vmsgtu(v1, **0b11100000**-1, vl);\nvbool4_t is_4 = __riscv_vmsgtu(v0, **0b11110000**-1, vl);\nvbool4_t is_34 = __riscv_vmor(is_3, is_4, vl);\nvbool4_t err34 = __riscv_vmxor(is_34, __riscv_vmslt(errs, **0**, vl), vl);\nvbool4_t errm = __riscv_vmor(__riscv_vmsgt(errs, **0**, vl), err34, vl);\n `if (__riscv_vfirst(errm, vl) >= **0**)`\n `return **0**;` \n```", "```\n*...*\n*if (count < tail) return utf8_to_utf32_scalar(src, count, dest);*\n*/* validate first three bytes */*\n{\n size_t idx = tail;\n while (idx < count && (src[idx] >> **6**) == **0b10**)\n ++idx;\n uint32_t buf[**10**];\n if (idx > tail + **3** || !utf8_to_utf32_scalar(src, idx, buf))\n return **0**;\n}\n\n```", "```\n*...*\n*vuint8m2_t v0 = __riscv_vle8_v_u8m2((uint8_t const*)src, vl);*\nuint64_t max = __riscv_vmv_x(__riscv_vredmaxu(v0, __riscv_vmv_s_x_u8m1(**0**, vl), vl));\n `*/* fast path: ASCII */*`\n`if (max < **0b10000000**) {`\n `vlOut = vl;`\n `__riscv_vse32(dest, __riscv_vzext_vf4(v0, vlOut), vlOut);`\n `continue;`\n`}` \n```", "```\n*...*\n*vuint8m2_t b2 = __riscv_vcompress(v1, m, vl);*\nvlOut = __riscv_vcpop(m, vl); */* needs to be moved up from previous place to here */*\n `*/* fast path: one and two byte */*`\n`if (max < **0b11100000**) {`\n `b2 = __riscv_vand(b2, **0b00111111**, vlOut);`\n `vbool4_t m1 = __riscv_vmsgtu(b1, **0b10111111**, vlOut);`\n `b1 = __riscv_vand_mu(m1, b1, b1, **63**, vlOut);`\n `vuint16m4_t b12 = __riscv_vwmulu(b1, __riscv_vmerge(__riscv_vmv_v_x_u8m2(**1**, vlOut), **1**<<**6**, m1, vlOut), vlOut);`\n `b12 = __riscv_vwaddu_wv_mu(m1, b12, b12, b2, vlOut);`\n `__riscv_vse32(dest, __riscv_vzext_vf2(b12, vlOut), vlOut);`\n `continue;`\n`}` \n```", "```\n*/* fast path: one and two byte */*\n*...*\n `*/* fast path: one, two and three byte */*`\n`if (max < **0b11110000**) {`\n `vuint8m2_t b3 = __riscv_vcompress(v2, m, vl);`\n `b2 = __riscv_vand(b2, **0b00111111**, vlOut);`\n `b3 = __riscv_vand(b3, **0b00111111**, vlOut);`\n ``vbool4_t m1 = __riscv_vmsgtu(b1, **0b10111111**, vlOut);`\n `vbool4_t m3 = __riscv_vmsgtu(b1, **0b11011111**, vlOut);`\n ``vuint8m2_t t1 = __riscv_vand(m1, b1, b1, **63**, vlOut);`\n `b1 = __riscv_vand(m3, t1, b1, **15**, vlOut);`\n ``vuint16m4_t b12 = __riscv_vwmulu(b1, __riscv_vmerge(__riscv_vmv_v_x_u8m2(**1**, vlOut), **1**<<**6**, m1, vlOut), vlOut);`\n `b12 = __riscv_vwaddu_wv(m1, b12, b12, b2, vlOut);`\n `vuint16m4_t b123 = __riscv_vwaddu_wv_(m3, b12, __riscv_vsll_vx_u16m4_mu(m3, b12, b12, **6**, vlOut), b3, vlOut);`\n `__riscv_vse32(dest, __riscv_vzext_vf2(b123, vlOut), vlOut);`\n `continue;`\n`}```", "```\n\n ``Every 1 to 3 byte UTF-8 character can be represented in a single word UTF-16 character, so for the fast paths above, we need to widen and store to 16 instead of 32 bits.\nHey! The fast paths just got a bit faster!\n\nThe next step starts right after we've computed the UTF-32 code point in the general path. We treat all characters as two-word UTF-16 and convert them to surrogate pairs accordingly. Then we `vmerge` between the original UTF-32 code point and the two-word UTF-16 code points. Afterward, we use the original UTF-32 code point to create a compression mask for our UTF-32 words. This can be done by matching the UTF-32 code points that fit into 16 bits, their upper two bytes are zero.\n\nWe predefine a mask `m2even` that masks all odd indexes and two helpers, as we'll need to switch between treating the vector as a vector of 16-bit and 32-bit elements.\n\n```", "```\n\nNow we shift the bits around to create the surrogate representation of each code point and then select between this and the UTF-32 code point using `vmerge`. To obtain the correct UTF-16 output, we need to compress all odd words that are zero, as those are only present in the one single UTF-16 character currently stored as UTF-32.\n\n```", "```\n\nFinally, we need to adjust the tail handling to account for UTF-16 output. Since we want to reparse the last character, we need to decrement the destination pointer, if the last output word was a high surrogate.\n\n```", "```\n\nLet's do one last optimization. If you look at the general case again, you might notice that we are doing twice the needed work needed, if the average input character size is equal to or above two. To circumvent that, we can lower the LMUL from two to one and unroll the code with an early exit between the iterations.\n\nThis is a simple transformation, and you can browse the [final code here](https://github.com/camel-cdr/rvv-bench/blob/main/vector-utf/8toN_gather.c). It's a bit more complex because it implements both the UTF-8 to UTF-16 and UTF-8 to UTF-32 conversions in one function.\n\nI used the Lemires [unicode_lipsum](https://github.com/lemire/unicode_lipsum/) dataset to measure the performance for inputs of different languages. You can look at the [benchmark code here](https://github.com/camel-cdr/rvv-bench/blob/main/vector-utf/bench.c), it reads an input file into a buffer and measures the average time a conversion of that input takes.\n\nThe code for the `C920` needed to be manually backported to the draft 0.7.1 RVV version, which introduced a few pessimization:\n\n*   `vmvNr` needed to be replaced with one `vmv.v.v` and two `vsetvli` instructions.\n*   `vzext.vN` needed to be replaced with N/2 `vwaddu.vx` and N/2+1 `vsetvli` instructions.\n*   `vredmax` needed to be replaced with one `vmsgtu.vx` and one `vfirst.m` instruction.\n    This shouldn't be needed for RVV 0.7.1, but the hardware I've got access to seems to produce the wrong result for vredmax. I haven't looked into this further.\n\nThe [unicode_lipsum](https://github.com/lemire/unicode_lipsum/) dataset is split into two parts. One contains [Lorem ipsum](https://en.wikipedia.org/wiki/Lorem_ipsum) style text in different languages and was created by Hans Kratz, for the [simdutf8](https://github.com/lemire/unicode_lipsum/blob/main/simdutf8) project.\nThis gives us very dense input, the average character sizes are as follows: Arabic: 1.8 Chinese: 3.0 Emoji: 4.0 Hebrew: 1.8 Hindi: 2.7 Japanese: 2.9 Korean: 2.5 Latin: 1.0 Russian: 1.8\n\nExcluding the, all ASCII, Latin input, we get an average speedup of 3x over scalar on the C920, and 3.5x on the C908\\. That's a lot!\nWe are a bit lacking in the Emoji test case, which solely consists of 4-byte UTF-8 emojis. We could add an extra fast path for that, but such input will rarely occur in the real world, so I don't think it's worth optimizing for.\n\nThe second part of the data set includes the source code of different translations for the [Mars](https://en.wikipedia.org/wiki/Mars) Wikipedia entries. This is less dense, no average character length is above 2.0, but probably more representative, as a lot of text comes in some sort of structural format (XML, JSON, ...) or has Arabic numerals, links, ... in between the native language characters.\n\nHere, we get an average speedup of 3.6x over scalar on the C920, and 4.0x on the C908, excluding the all ASCII English case.\n\nWe managed to get a good speedup on real hardware, but we are still in the early days of RVV implementations. Will the code also have similar speedups on future hardware that might support bigger vector lengths?\n\nMost RVV instructions can be expected to perform well across implementations, but the permutation instructions, specifically `vrgather.vv` and `vcompress.vm`, are harder to scale and already exhibit a variety of performance characteristics. I've compiled cycle estimates for all implementations I could get the data for:\n\n**bobcat: Note, that it was explicitly stated, that they didn't optimize the permutation instructions*\n\n**x280: the numbers are from llvm-mca, but I was told they match reality. There is also supposed to be a `vrgather.vv` fast path for vl<=256\\. I think they didn't have much incentive to optimize this, as the x280 targets AI workloads w.*\n\nNow you might see why we've decided to use six LMUL=1 gathers instead of three LMUL=2 gathers for our validation lookup tables. Current implementations don't scale well to larger LMUL, and some won't perform well enough for our implementation on any LMUL.\n\nI think that the C920 results are the most representative of what to expect of future desktop CPUs and we can safely ignore the bobcat/x280 cycle counts for that use case. I suspect we'll see `vrgather.vv` perform well for LMUL=1, and then grow exponentially per element with higher LMUL, as an all-to-all mapping is quite expensive to scale.\n`vcompress.vm` should be better scalable than `vrgather.vv`, since the work is subdividable, and I think we might see a range of implementations that scale almost linearly in respect to LMUL.\n\nWe've seen that `vrgather.vv` is really useful for quick lookup tables, but provides a way more powerful all-to-all mapping than required for that. Four-bit lookup tables specifically are very likely to be used by a bunch of other implementations as well, as the standard V extension guarantees a VLEN of at least 128, so a 4-bit LUT can always be assumed to fit into one vector register.\n\nI don't know anything about hardware design, but implementing a LMUL=1 `vrgather.vv` for very long vector implementations that performs, per element, as fast as one for a smaller implementation, seems almost impossible. If you expect to use it for its full capabilities, it isn't a big problem, sometimes you just need to get the elements into a specific permutation. But for 4-bit lookup tables we only care about selecting one of 16 values.\n\nHardware might be able to have a special case/fast path implementation for smaller indices, but we can't expect most implementations to support it. I think adding a dedicated `vrgatherei4`, which only uses the four LSB for indices, to the specification might be a good idea.\n\nWe've managed to effectively use the RISC-V Vector extension to speed up UTF-8 to UTF-16 conversion by on average 3 to 4 times on real hardware. [This and other conversion functions](https://github.com/simdutf/simdutf/pull/373) have been successfully upstreamed to the [simdutf](https://github.com/simdutf/simdutf/) library. Hopefully, we can see this contribute to bridging the optimization gap of RISC-V compared to other architectures, and maybe even inspire some people to give porting to RVV a shot.\n\nI hope you could follow this write up, have a nice day :3\n\nIf you are wondering how the RISC-V hardware compares to existing hardware of other architectures, here are the measurements for the dataset from above including simdutf implementations for the other architectures.```", "```````"]