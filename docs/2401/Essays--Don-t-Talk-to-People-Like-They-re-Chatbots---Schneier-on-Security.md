<!--yml

category: 未分类

date: 2024-05-27 15:12:47

-->

# 文章：不要像对待聊天机器人一样与人交谈 - Schneier on Security

> 来源：[`www.schneier.com/essays/archives/2024/01/dont-talk-to-people-like-theyre-chatbots.html`](https://www.schneier.com/essays/archives/2024/01/dont-talk-to-people-like-theyre-chatbots.html)

## 不要像对待聊天机器人一样与人交谈

#### AI 可能会使我们的人际交往变得更加平淡、更有偏见或更加粗鲁。

在大部分历史中，与计算机交流不像与人交流。在它们最早的年代，计算机需要通过穿孔卡仔细构造的指令来运行；然后是命令行界面，接着是菜单、选项和文本框。如果你想得到结果，你需要学会计算机的语言。

这种情况正在发生改变。大型语言模型——现代聊天机器人的技术基础——允许用户通过自然对话与计算机进行交互，这一创新引入了一些来自人与人之间交流的包袱。在我们各自探索 ChatGPT 的早期阶段，我们发现自己在向计算机输入一个词，这是我们以前从未对计算机说过的：“请”。礼貌的语法已经渗入我们的每一个交流方面；我们对这个代数组合说话，就好像它是一个人一样——即使我们知道[它不是](https://www.belfercenter.org/publication/ai-and-trust)。

就目前而言，这种交互方式是一种新奇。但随着聊天机器人成为现代生活中无处不在的一个元素，并且渗透到我们许多人机交互中，它们有潜力在微妙地重塑我们对计算机和我们的人类同伴的看法。

这些聊天机器人可能引导我们朝着一个将人性赋予 AI 系统的社会发展，无论是抽象的聊天机器人还是更具体的机器人。正如我们在物体中[生物学上被激发出](https://www.bbc.com/future/article/20140730-why-do-we-see-faces-in-objects)看到面孔，我们在任何能进行对话的东西中都想象出智能。（这并不新鲜：人们曾把智能和同理心投射到非常原始的 60 年代聊天机器人[Eliza](https://www.theatlantic.com/ideas/archive/2022/06/google-lamda-chatbot-sentient-ai/661322/)上。）我们对 LLM 说“请”，是因为不这样做感觉不对。

[聊天机器人](https://www.theatlantic.com/podcasts/archive/2023/08/are-ai-relationships-real/674965/)越来越普遍，有理由相信它们将成为我们生活中[越来越亲密的一部分](https://www.theatlantic.com/podcasts/archive/2023/08/are-ai-relationships-real/674965/)。从朋友到恋人等各种 AI 伴侣的市场已经相当拥挤。有几家公司正在研发类似秘书或男仆的 AI 助手，它们将[预测并满足我们的需求](https://www.cbsnews.com/news/walmart-artificial-intelligence-retail/#:~:text=Called%20%22InHome%20Replenishment%2C%22%20the,%2C%20monthly%20schedule%2C%20for%20example.)。其他公司正在研发 AI 心理医生、调解员和生活教练，甚至是我们已故亲属的仿真。更一般地说，聊天机器人很可能成为我们与各种计算机化流程交互的界面——一种响应我们语言风格、每一丝情感细微差别甚至语调的 AI。

许多用户将被引导将这些 AI 视为朋友，而不是它们实际上是的企业创建的系统。互联网已经通过 Meta 的广告网络等系统对我们进行监视，LLM 很可能也会加入其中：例如，OpenAI 的[隐私政策](https://openai.com/policies/privacy-policy)已经概述了公司收集的许多不同类型的个人信息。不同之处在于，聊天机器人的自然语言界面会让它们感觉更像人类——在双方的每一次礼貌交流中得到强化——我们很容易在心中错误分类它们。

主要的聊天机器人目前还没有改变它们与用户交流的方式来满足其母公司的商业利益，但市场压力可能会推动事情朝着这个方向发展。OpenAI 的发言人在被问及此事时指出了隐私政策中的一节，该节指出公司目前不出售或分享个人信息进行“跨上下文行为广告”，并且公司不会“处理敏感个人信息以推断有关消费者的特征”。在今天早些时候的[*Axios*](https://www.axios.com/2024/01/17/sam-altman-davos-ai-future-interview)采访中，OpenAI 首席执行官 Sam Altman 表示，未来的 AI 可能会涉及“相当多的个性化定制”，“这将让很多人感到不舒服”。

其他计算技术已经被证明可以塑造我们的认知。研究表明，在网站和文字处理器上的自动补全功能可以极大地重新组织我们的写作。一般来说，这些建议导致[更加平淡](https://www.eecs.harvard.edu/~kgajos/papers/2020/arnold20predictive.pdf)、更加可预测的散文。而当自动补全系统给出偏见提示时，它们会导致有偏见的写作。在一个[良性实验](https://www.eecs.harvard.edu/~kgajos/papers/2018/arnold18sentiment.pdf)中，积极的自动补全建议导致更多积极的餐厅评论，而消极的自动补全建议则导致相反的效果。其影响可能远远超出调整我们的写作风格，影响我们的心理健康，就像今天的社交媒体平台可能导致抑郁和焦虑一样。

这些聊天机器人可能带领我们朝着另一个更加令人不安的方向发展：一个世界，在这个世界里，我们与它们的对话导致我们对待我们的人类同伴的冷漠、不尊重和不文明的方式更典型。

今天的聊天机器人在表现最佳时需要以一种对话礼貌度而言会令人震惊的精度指令，剥夺了模型可能会误解的任何对话礼貌用语：“用我的典型写作风格起草一个 250 字的段落，详细说明三个支持以下观点的例子，并引用你的来源。”即使是最超然的公司 CEO 也不太可能这样对待他们的助手，但这在与聊天机器人交流中很常见。

如果聊天机器人真的成为某些人每天的主要对话伙伴，那么这些用户存在一个严重的风险，即当他们与其他人交谈时也会采用 AI 命令的词汇。与其说是用同理心、微妙和细微差别说话，我们更可能被训练成用程序员与计算机交谈的冷酷精确。给对话赋予其本质人类品质的丰富警句和轶事，但这些警句和轶事经常会让大型语言模型感到困惑，可能会开始从人类话语中消失。

以先例为例，我们只需看看机器人账户如何在社交媒体上已经破坏了数字化话语，通过对深情话题的粗略编程响应来激起激情；它们[可以说](https://www.cjr.org/the_media_today/nature_study_trump_bots_twitter.php)在 2016 年选举中起到了播下不和与极端化选民的作用。但人工智能伴侣很可能成为某些用户社交圈的一个更大的组成部分，比起今天的机器人，它们可能对这些人如何使用语言和建立关系产生更大的影响。不清楚的是这是否会对十亿分之一的用户产生负面影响，还是对其中大部分产生负面影响。

这样的转变不太可能一夜之间将人类的对话转变为卡通般的机器化复述，但它可能在多年的过程中悄然而有意义地改变口头交流，就像短信的字符限制影响了口语写作的许多方面一样，将诸如*LOL*、*IMO*和*TMI*之类的术语变成了日常用语。

人工智能聊天机器人总是在你需要的时候出现，无论你需要什么。人们并非如此。想象一下充满了与他们的人工智能朋友或爱人交谈多年的人的未来。就像那些唯一性经历被色情片或情色文学中介的人一样，他们可能对人类伴侣有不切实际的期望。而且，聊天机器人变得越来越普遍和逼真，影响可能会越来越大。

更普遍地说，人工智能可能加速了机构和社会信任的瓦解。像 Facebook 这样的技术本应将世界联系在一起，但在过去的几年中，公众对周围的人越来越怀疑，对公民制度的信任也在减少。人工智能可能会进一步推动人们走向孤立和怀疑，总是不确定他们正在聊天的人是否真的是一个机器，并且对待他们视为非人类。

当然，历史上充斥着人们声称数字天空将坠落的说法，为每一项新发明而哀叹，称其为我们所知的文明的终结。最终，大型语言模型可能只是明天的文字处理器，一种方便的创新，使事情变得更容易，同时让我们生活的大部分内容保持不变。我们选择的道路取决于我们如何训练明天的聊天机器人，但也取决于我们是否投资于今天加强公民社会联系。

分类：[人工智能和大型语言模型](https://www.schneier.com/essays/ai-and-large-language-models/)，[互联网与社会](https://www.schneier.com/essays/society/)

标签：[大西洋](https://www.schneier.com/essays/atlantic/)
