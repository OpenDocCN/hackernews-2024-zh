- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 分类：未分类
- en: 'date: 2024-05-27 14:49:25'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-05-27 14:49:25
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: 'Stable Code 3B: Coding on the Edge — Stability AI'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Stable Code 3B：边缘编码 — Stability AI
- en: 来源：[https://stability.ai/news/stable-code-2024-llm-code-completion-release](https://stability.ai/news/stable-code-2024-llm-code-completion-release)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://stability.ai/news/stable-code-2024-llm-code-completion-release](https://stability.ai/news/stable-code-2024-llm-code-completion-release)
- en: 'Today, we announce our first Large Language Model release of 2024: [Stable
    Code 3B](https://huggingface.co/stabilityai/stable-code-3b). This new LLM is a
    follow-up to our previously released [Stable Code Alpha 3B](https://stability.ai/news/stablecode-llm-generative-ai-coding)
    and the first major Stable Code release, offering a new state-of-the-art model
    designed for code completion with multiple additional capabilities.'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 今天，我们宣布我们 2024 年的第一个大型语言模型发布：[Stable Code 3B](https://huggingface.co/stabilityai/stable-code-3b)。这个新的大型语言模型是我们之前发布的
    [Stable Code Alpha 3B](https://stability.ai/news/stablecode-llm-generative-ai-coding)
    的后续版本，也是第一个重要的 Stable Code 发布，提供了一个新的设计用于代码完成的最新模型，并具有多个额外功能。
- en: Compared to CodeLLaMA 7b, Stable Code 3B is 60% smaller while featuring a similar
    high-level performance across programming languages. Based on our pre-existing
    [Stable LM 3B](https://huggingface.co/stabilityai/stablelm-3b-4e1t) foundational
    model trained on 4 trillion tokens of natural language data, Stable Code was further
    trained on software engineering-specific data, including code. The model's compact
    size allows it to be run privately on the edge in real-time on modern laptops,
    even those without a dedicated GPU.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 相对于 CodeLLaMA 7b，Stable Code 3B 的体积减小了 60%，同时在各种编程语言中保持了类似的高性能水平。基于我们预先存在的 [Stable
    LM 3B](https://huggingface.co/stabilityai/stablelm-3b-4e1t) 基础模型，该模型在 4 万亿个自然语言数据令牌上进行了训练，Stable
    Code 进一步在软件工程特定数据上进行了训练，包括代码。该模型紧凑的体积使其能够在现代笔记本电脑上即使没有专用 GPU 也能够在边缘实时运行。
- en: Stable Code 3B offers more features and significantly better performance across
    multiple languages with additional benefits such as support for Fill in the Middle
    capabilities (FIM) and expanded context size. Stable Code as a base is trained
    on sequences of up to 16,384 tokens but follows a similar approach to CodeLlama
    with the implementation of Rotary Embeddings, optionally allowing modification
    of the rotary base up to 1,000,000, further expanding the model’s context length
    up to 100k tokens.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: Stable Code 3B 提供了更多功能，并且在多种语言中的性能显著提升，还增加了支持填充功能 (FIM) 和扩展上下文大小等额外优势。Stable
    Code 作为基础模型的序列训练长度为最多 16,384 个标记，但是遵循类似于 CodeLlama 的方法实现了 Rotary Embeddings，可选地允许修改旋转基数达到
    1,000,000，进一步扩展模型的上下文长度达到 100k 个标记。
- en: Stable Code is trained on 18 programming languages (selected based on the [2023
    StackOverflow Developer Survey](https://survey.stackoverflow.co/2023/#section-most-popular-technologies-programming-scripting-and-markup-languages))
    and demonstrates state-of-the-art performance (compared to models of similar size)
    on the MultiPL-E metrics across multiple programming languages tested.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: Stable Code 在 18 种编程语言上进行了训练（根据 [2023 年 StackOverflow 开发者调查](https://survey.stackoverflow.co/2023/#section-most-popular-technologies-programming-scripting-and-markup-languages)
    进行选择），并在多种测试的编程语言中的 MultiPL-E 指标上表现出同类大小模型的最新性能。
- en: '**Performance Comparison**'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '**性能比较**'
