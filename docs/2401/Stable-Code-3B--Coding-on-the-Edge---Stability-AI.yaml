- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-05-27 14:49:25'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: 'Stable Code 3B: Coding on the Edge — Stability AI'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://stability.ai/news/stable-code-2024-llm-code-completion-release](https://stability.ai/news/stable-code-2024-llm-code-completion-release)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Today, we announce our first Large Language Model release of 2024: [Stable
    Code 3B](https://huggingface.co/stabilityai/stable-code-3b). This new LLM is a
    follow-up to our previously released [Stable Code Alpha 3B](https://stability.ai/news/stablecode-llm-generative-ai-coding)
    and the first major Stable Code release, offering a new state-of-the-art model
    designed for code completion with multiple additional capabilities.'
  prefs: []
  type: TYPE_NORMAL
- en: Compared to CodeLLaMA 7b, Stable Code 3B is 60% smaller while featuring a similar
    high-level performance across programming languages. Based on our pre-existing
    [Stable LM 3B](https://huggingface.co/stabilityai/stablelm-3b-4e1t) foundational
    model trained on 4 trillion tokens of natural language data, Stable Code was further
    trained on software engineering-specific data, including code. The model's compact
    size allows it to be run privately on the edge in real-time on modern laptops,
    even those without a dedicated GPU.
  prefs: []
  type: TYPE_NORMAL
- en: Stable Code 3B offers more features and significantly better performance across
    multiple languages with additional benefits such as support for Fill in the Middle
    capabilities (FIM) and expanded context size. Stable Code as a base is trained
    on sequences of up to 16,384 tokens but follows a similar approach to CodeLlama
    with the implementation of Rotary Embeddings, optionally allowing modification
    of the rotary base up to 1,000,000, further expanding the model’s context length
    up to 100k tokens.
  prefs: []
  type: TYPE_NORMAL
- en: Stable Code is trained on 18 programming languages (selected based on the [2023
    StackOverflow Developer Survey](https://survey.stackoverflow.co/2023/#section-most-popular-technologies-programming-scripting-and-markup-languages))
    and demonstrates state-of-the-art performance (compared to models of similar size)
    on the MultiPL-E metrics across multiple programming languages tested.
  prefs: []
  type: TYPE_NORMAL
- en: '**Performance Comparison**'
  prefs: []
  type: TYPE_NORMAL
