<!--yml

分类：未分类

日期：2024-05-27 14:39:13

-->

# 约翰尼如何说服 LLM 越狱：通过人性化 LLM 来挑战 AI 安全性重新思考说服

> 来源：[`chats-lab.github.io/persuasive_jailbreaker/`](https://chats-lab.github.io/persuasive_jailbreaker/)

这个项目提供了一种结构化的方式来大规模生成可解释的具有说服力的对抗性提示（PAP），这可能使普通用户能够在不需要太多计算的情况下越狱 LLM。但正如所提到的，[Reddit 用户](https://www.reddit.com/r/ChatGPT/comments/12sn0kk/grandma_exploit)以前已经利用说服攻击了 LLM，因此迫切需要更系统地研究围绕说服性越狱的漏洞，以更好地减轻这些漏洞的影响。因此，尽管存在风险，我们认为有必要全面分享我们的研究结果。我们在整个研究过程中遵循了道德准则。

首先，说服通常对普通人来说是一项艰巨的任务，因此即使有了我们的分类法，对于没有训练的人来说，将简单的有害查询转述为成功的 PAP 可能仍然具有挑战性。因此，来自数百万用户的广泛攻击的现实风险相对较低。我们还决定不公开经过训练的*具有说服力的改述器及相关代码流程*，以防止人们轻易将有害查询转述。

为了最小化现实世界的危害，我们在发布之前向 Meta 和 OpenAI 披露了我们的结果，因此本文中的 PAP 可能不再有效。正如讨论的那样，克劳德成功抵抗了 PAP，展示了一种成功的缓解方法。我们还探讨了不同的防御措施，并提出了新的自适应安全系统提示和基于摘要的防御机制，以减轻风险，这已经显示出了有希望的结果。我们的目标是在未来的工作中改进这些防御措施。

总结一下，我们研究的目标是加强 LLM 的安全性，而不是让其被恶意利用。我们承诺根据技术进步持续监控和更新我们的研究，并将 PAP 微调细节限制在获得批准的认证研究人员中。
