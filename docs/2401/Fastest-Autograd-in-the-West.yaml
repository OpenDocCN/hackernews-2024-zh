- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
- en: 'category: æœªåˆ†ç±»'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-05-27 14:29:17'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
- en: Fastest Autograd in the West
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: æ¥æºï¼š[https://arogozhnikov.github.io/2023/12/28/fastest-autograd.html](https://arogozhnikov.github.io/2023/12/28/fastest-autograd.html)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Who needs fast autograd? Seemingly everyone these days!
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
- en: 'And once upon a time I needed an autograd that is **actually fast**. Leaving
    project details aside, here are the requirements:'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
- en: we test many computation graphs (graph is changing constantly)
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: many-many scalar operations with roughly **10kâ€”100k nodes** in each graph
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: every graph should be compiled and ran around **10k times** both forward and
    backward
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: this should be done **wicked fast**, and with a convenient pythonic interface
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Path that awaits us ahead:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
- en: autograd in torch
  id: totrans-13
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: autograd in jax
  id: totrans-14
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: autograd in python
  id: totrans-15
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: autograd in rust
  id: totrans-16
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: autograd in C
  id: totrans-17
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: autograd in assembly
  id: totrans-18
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Plus a significant amount of sloppy code and timings on M1 macbook.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
- en: Letâ€™s autograd in pytorch
  id: totrans-20
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We start our journey with pytorch â€” the default autograd engine in research.
    Weâ€™ll create a graph with many nodes, and to keep things simple our benchmark
    has only several kinds of operations: unary (softplus), binary (multiplication),
    n-ary (sum) and n-to-n (softmax).'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
- en: This allows using just a few operations, but resembles a realistic load. All
    benchmarks in this post will reimplement the same logic as below.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  id: totrans-23
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Run-time for 10k ops x 100 iterations: 11.3 seconds'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
- en: 'Run-time for 10k ops x 10k iterations: **1130 seconds** (estimate)'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
- en: Given we created 100M python objects, itâ€™s actually quite fast. And yes, thatâ€™s
    not going to deliver an interactive experience.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
- en: Letâ€™s also discuss `torch.compile`, a major innovation in pytorch 2.0.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
- en: 'At 100 operations torch.compile takes 4.5 seconds. Execution gets faster: for
    100 operations and 10k iterations it takes 4.52 seconds with torch.compile and
    10.4 seconds without. Compilation + execution are still in the same ballpark.
    For bigger graphs (1k operations) `torch.compile` crashes.'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
- en: Letâ€™s autograd in jax
  id: totrans-29
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Jax is the new cool kidâ€¦ well, not that new anymore. But in some aspects it
    is very interesting. Jaxâ€™s focus on JIT-compiling static graphs is very suitable
    for the problem at hand.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
- en: 'Implementation for benchmark is similar to pytorch:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Without jit computations are extremely slow:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
- en: 1k ops x 10 iterations => 15.9 seconds
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
- en: 10k ops x 10k iterations => 159,000 seconds (estimate)
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
- en: Thatâ€™s a bit longer than forever! But whole point of jax is to JIT-compile stuff.
    So letâ€™s do it.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
- en: 'jit: compilation of 1k ops = 47 seconds'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
- en: 'jit: run-time for 1k ops x 10k iterations = 0.66 seconds'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
- en: 'jit: 10k ops x 10k iterations (compilation + run-time) => **470 seconds** (estimate)'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
- en: Speed up in execution time is more than impressive, but we spend >99% of time
    compiling.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
- en: Tensorflow
  id: totrans-41
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Someone will mention TF anyway. Iâ€™ll leave this as an exercise for you, TF fans.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
- en: Letâ€™s autograd in python
  id: totrans-43
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Done with baselines, time to see if we can speed things up.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
- en: Letâ€™s create a simplistic pseudo-framework and see how it competes with previous
    candidates. Weâ€™ll implement a tape-like autograd where operations order is explicitly
    tracked in a tape.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
- en: <details><summary class="code-summary">show autograd engine in plain python</summary>
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]</details>'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
- en: 'and reimplement reference task using our new pseudo-framework:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
- en: <details><summary class="code-summary">show benchmarking code</summary>
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]</details>'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
- en: 'Run-time for 10k ops and 10k iterations: **312 seconds**.'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
- en: Expectably not fast. But compared to previous candidates, thatâ€™s actually quite
    competitive!
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
- en: Letâ€™s autograd in python, again
  id: totrans-53
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This time we move all values into tape instead of keeping in variables. Additionally
    tape will keep a â€˜static graphâ€™ of computations by recording indices of variables
    participating in every operation.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
- en: <details><summary class="code-summary">show code for autograd in plain python</summary>
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: and corresponding launching code
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]</details>'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
- en: 'Run-time for 10k ops x 10k iterations: **94 seconds**'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
- en: As we see, moving all values into tape and switching to operating on indices
    is quite an efficient strategy. We still use python, but are now ~5-10 fold faster
    than `pytorch` or `jax`.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
- en: 'At this point, I want to mention one more experiment: code above is organized
    to be `numba`-friendly. [Numba](https://numba.readthedocs.io/en/stable/) is famous
    for speeding up number crunching in python with minimal changes by providing just-in-time
    compilation. Recent addition of `numba.typed.List` makes it possible to efficiently
    handle list of lists.'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
- en: 'Run-time with numba, 10k ops x 10k iterations: **41 second**.'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
- en: At this point weâ€™re >10-fold faster than jax/pytorch (and still writing code
    in python).
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
- en: Letâ€™s autograd in rust
  id: totrans-64
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Once we moved graph tracking to tape, we can now use something fast to run computations
    for us. For instance, rust. For rustâ†”python interop Iâ€™ve used a small wrapper
    around [rustimport](https://github.com/mityax/rustimport). `Rustimport` allows
    to conveniently â€œimportâ€ a single rust file without creating a full-fledged rust
    project.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
- en: 'Some optimization remarks:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
- en: '`softmax` was a bottleneck, so I switched to creating temporary arrays on stack
    instead of Vecs, which required specializing on input sizes'
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: I followed rust-y approach with iterators to reduce number of boundary checks
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: I wondered if match with multiple options checked one-by-one is slow. In synthetic
    tests it seemed to be relatively fast, but I wish jump table optimization was
    implemented here (e.g. it is supported for [enums](https://users.rust-lang.org/t/match-statement-efficiency/4488)
    in rust, and clang [uses](https://stackoverflow.com/questions/60109992/why-is-a-switch-not-optimized-the-same-way-as-chained-if-else-in-c-c)
    this optimization in C for switch-case)
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <details><summary class="code-summary">show rust code for minimal autograd</summary>
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]</details>'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
- en: 'Run-time for 10k ops x 10k iterations: **1.4 seconds**'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
- en: 'Success: we are in the realm of interactive experiences.'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
- en: Recall we started from >1000 seconds. But should we stop here?
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
- en: Letâ€™s autograd in C
  id: totrans-75
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Time to implement autograd logic in C. For interop with python I use [python-cffi](https://cffi.readthedocs.io/en/stable/index.html).
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
- en: 'I went bananas on optimization:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
- en: I used the fact that output nodes are placed consequentially in memory, so we
    pass only index of the first output
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: number of inputs is limited to 8, and those are baked into struct as `int[8]`,
    not `int *` to avoid jumps in memory
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: dynamic stack allocations of variable size (compared to rust, those are straightforward
    in C)
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`-O3`, and unsafe math: `-ffast-math`. Even experimented memory alignment and
    restrict-ing pointers, but no luck'
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <details><summary class="code-summary">show me some code in C</summary>
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]</details>'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
- en: 'Run-time for 10k ops x 10k iterations: **0.99 second**'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
- en: I liked ergonomics of rust better, but achieving high speed in C is way easier.
    Rustâ€™s interop with python is also way more convenient.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
- en: Letâ€™s autograd in C (again)
  id: totrans-86
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Another approach Iâ€™ve taken is to â€˜compileâ€™ traced graph to C. So python produces
    a long C file where operations are called one-by-one with explicit indices, something
    like
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Source code is lengthy, outputs are enormous, and to speed up compilation we
    can set `-O0` in clang. Using `-O0` produces slower binaries, but interestingly
    *did not* speed up compilation. Best results I got are around 1 minute for compilation
    and 1 second for a full run. Surprisingly, eliminating switch/case and memory
    lookups for arguments did not result in faster execution.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
- en: Given that recompilation is needed any time the graph is changed, real time
    experienced by user is 1 minute. Thatâ€™s a no go.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
- en: Assembly
  id: totrans-91
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this endeavor to get maximal speed, I decided to go down to assembly. Otherwise
    it feels like an incomplete journey. We can map a computational graph to just
    a set of low-level instruction, and avoid â€œcostlyâ€ compilation. These days x86/64
    is not a king anymore, but neither armv7/armv8 is â€” and writing assembly for several
    architectures is totally unreasonable.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
- en: 'So â€¦ how about using webassembly? It is low-level, fast to compile, and still
    cross-platform. Projects like `wasmer`/`wasmtime` allow interacting with wasm
    code from other languages. Thatâ€™s my first encounter with WASM, and Iâ€™ve got quite
    positive impression: WASM mixes lisp-style syntax (for efficient streaming parsing)
    and execution model of stack machine. Unlike canonical stack machines, and unlike
    canonical assembly, WASM allows grouping expressions, e.g.'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: This convenience allows writing significantly more readable code in WASM compared
    to ye-olde-assembly. Level of abstraction looks just right to me â€” low-level instructions,
    but no need to manage register allocations.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
- en: Webassembly is still very close to assembly in terms of instructions, i.e. there
    is no `exp`, `log`, let alone `log1p` and alike. Fortunately, there is a WASM
    [implementation](https://gist.github.com/going-digital/02e46c44d89237c07bc99cd440ebfa43)
    of `exp2`/`log2` by Peter Knight.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: ä»æŒ‡ä»¤ä¸Šçœ‹ï¼ŒWebassemblyä»ç„¶ä¸æ±‡ç¼–è¯­è¨€éå¸¸æ¥è¿‘ï¼Œå³æ²¡æœ‰`exp`ï¼Œ`log`ï¼Œæ›´ä¸ç”¨è¯´`log1p`ä¹‹ç±»çš„å‡½æ•°äº†ã€‚å¹¸è¿çš„æ˜¯ï¼ŒPeter Knight
    æä¾›äº†ä¸€ä¸ª WASM çš„ [å®ç°](https://gist.github.com/going-digital/02e46c44d89237c07bc99cd440ebfa43)ï¼ŒåŒ…æ‹¬äº†`exp2`/`log2`ã€‚
- en: My major question was if speed of exponentiation is going to be sufficient,
    as `exp` consumes significant time in C implementation. Alas, in a simple benchmark
    computing just exponents in wasm takes ~1.9 seconds, leaving it behind rust/C.
    For reference, javascript computes the same number of exponents in 0.7 seconds.
    Hence, I take WASM branding of â€˜near-native speedâ€™ with a grain of salt, at least
    in the context of number crunching. Hopefully this will improve, but for now WASM
    is out of competition.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘çš„ä¸»è¦é—®é¢˜æ˜¯æŒ‡æ•°è¿ç®—çš„é€Ÿåº¦æ˜¯å¦è¶³å¤Ÿï¼Œå› ä¸ºåœ¨Cå®ç°ä¸­ï¼Œ`exp`æ¶ˆè€—äº†å¤§é‡æ—¶é—´ã€‚é—æ†¾çš„æ˜¯ï¼Œåœ¨ä¸€ä¸ªç®€å•çš„åŸºå‡†æµ‹è¯•ä¸­ï¼Œä»…åœ¨wasmä¸­è®¡ç®—æŒ‡æ•°å°±éœ€è¦çº¦1.9ç§’çš„æ—¶é—´ï¼Œæ¯”rust/Cæ…¢ã€‚ä½œä¸ºå‚è€ƒï¼Œjavascriptåœ¨0.7ç§’å†…è®¡ç®—äº†ç›¸åŒæ•°é‡çš„æŒ‡æ•°ã€‚å› æ­¤ï¼Œè‡³å°‘åœ¨æ•°å­—å¤„ç†çš„èƒŒæ™¯ä¸‹ï¼Œæˆ‘å¯¹WASMçš„â€œæ¥è¿‘æœ¬æœºé€Ÿåº¦â€çš„å“ç‰ŒæŒä¿ç•™æ€åº¦ã€‚å¸Œæœ›è¿™æ–¹é¢ä¼šæœ‰æ‰€æ”¹å–„ï¼Œä½†ç›®å‰æ¥çœ‹ï¼ŒWASMå¤„äºå¤±å»ç«äº‰åŠ›çš„çŠ¶æ€ã€‚
- en: Summary
  id: totrans-98
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æ¦‚è¦
- en: So, we achieved a **1000X speed up** compared to leading libraries.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: å› æ­¤ï¼Œä¸ä¸»æµåº“ç›¸æ¯”ï¼Œæˆ‘ä»¬å®ç°äº†**1000å€çš„åŠ é€Ÿ**ã€‚
- en: I donâ€™t find this surprising â€” major usecase for autograd system is manipulating
    large ndarrays. Memory management, copy elimination, device synchronization, parallelization
    of computations â€” these things are the main focus, and throughput of 1 million
    ops per second is totally reasonable for the vast majority of scenarios and users.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘è§‰å¾—è¿™å¹¶ä¸å¥‡æ€ª â€” è‡ªåŠ¨å¾®åˆ†ç³»ç»Ÿçš„ä¸»è¦ç”¨é€”æ˜¯æ“ä½œå¤§å‹çš„ndarraysã€‚å†…å­˜ç®¡ç†ï¼Œæ¶ˆé™¤æ‹·è´ï¼Œè®¾å¤‡åŒæ­¥ï¼Œè®¡ç®—å¹¶è¡ŒåŒ– â€” è¿™äº›éƒ½æ˜¯ä¸»è¦å…³æ³¨ç‚¹ï¼Œæ¯ç§’1ç™¾ä¸‡æ¬¡æ“ä½œçš„ååé‡å¯¹äºç»å¤§å¤šæ•°åœºæ™¯å’Œç”¨æˆ·æ¥è¯´éƒ½æ˜¯å®Œå…¨åˆç†çš„ã€‚
- en: Not for me though. My scenario is totally different in terms of numbers and
    setup, and tensor-focused autograds are too slow. For the problem at hand departing
    from the common autograd systems was the right and the only possible choice. Exploring
    different options was quite fun, and my expectations were challenged several times
    along this exploration.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: ä½†å¯¹æˆ‘æ¥è¯´å¹¶éå¦‚æ­¤ã€‚æˆ‘çš„åœºæ™¯åœ¨æ•°å­—å’Œè®¾ç½®æ–¹é¢å®Œå…¨ä¸åŒï¼Œè€Œä¸”ä»¥å¼ é‡ä¸ºé‡ç‚¹çš„è‡ªåŠ¨å¾®åˆ†é€Ÿåº¦å¤ªæ…¢äº†ã€‚å¯¹äºæ‰‹å¤´çš„é—®é¢˜ï¼Œåç¦»å¸¸è§„çš„è‡ªåŠ¨å¾®åˆ†ç³»ç»Ÿæ˜¯æ­£ç¡®ä¸”å”¯ä¸€å¯èƒ½çš„é€‰æ‹©ã€‚æ¢ç´¢ä¸åŒçš„é€‰æ‹©éå¸¸æœ‰è¶£ï¼Œæˆ‘çš„æœŸæœ›åœ¨è¿™ä¸ªæ¢ç´¢è¿‡ç¨‹ä¸­å¤šæ¬¡å—åˆ°æŒ‘æˆ˜ã€‚
- en: ğŸ‘‹
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: ğŸ‘‹
