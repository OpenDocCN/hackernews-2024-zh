- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: æœªåˆ†ç±»'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: ç±»åˆ«ï¼šæœªåˆ†ç±»
- en: 'date: 2024-05-27 14:29:17'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: æ—¥æœŸï¼š2024-05-27 14:29:17
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: Fastest Autograd in the West
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: è¥¿éƒ¨æœ€å¿«çš„è‡ªåŠ¨å¾®åˆ†
- en: æ¥æºï¼š[https://arogozhnikov.github.io/2023/12/28/fastest-autograd.html](https://arogozhnikov.github.io/2023/12/28/fastest-autograd.html)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: æ¥æºï¼š[https://arogozhnikov.github.io/2023/12/28/fastest-autograd.html](https://arogozhnikov.github.io/2023/12/28/fastest-autograd.html)
- en: Who needs fast autograd? Seemingly everyone these days!
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: è°éœ€è¦å¿«é€Ÿè‡ªåŠ¨å¾®åˆ†ï¼Ÿçœ‹èµ·æ¥ç°åœ¨æ¯ä¸ªäººéƒ½éœ€è¦ï¼
- en: 'And once upon a time I needed an autograd that is **actually fast**. Leaving
    project details aside, here are the requirements:'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: ä»å‰ï¼Œæˆ‘éœ€è¦ä¸€ä¸ª**å®é™…å¿«é€Ÿ**çš„è‡ªåŠ¨å¾®åˆ†ã€‚æŠ›å¼€é¡¹ç›®ç»†èŠ‚ï¼Œä»¥ä¸‹æ˜¯è¦æ±‚ï¼š
- en: we test many computation graphs (graph is changing constantly)
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬æµ‹è¯•è®¸å¤šè®¡ç®—å›¾ï¼ˆå›¾å½¢ä¸æ–­å˜åŒ–ï¼‰
- en: many-many scalar operations with roughly **10kâ€”100k nodes** in each graph
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æ¯ä¸ªå›¾ä¸­æœ‰å¤§é‡æ•°é‡çš„æ ‡é‡æ“ä½œï¼Œå¤§çº¦æœ‰**10kâ€”100k ä¸ªèŠ‚ç‚¹**
- en: every graph should be compiled and ran around **10k times** both forward and
    backward
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æ¯ä¸ªå›¾å½¢åº”è¯¥åœ¨å‰å‘å’Œåå‘æ–¹å‘ä¸Šç¼–è¯‘å’Œè¿è¡Œçº¦**10k æ¬¡**
- en: this should be done **wicked fast**, and with a convenient pythonic interface
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: è¿™åº”è¯¥**éå¸¸å¿«**ï¼Œå¹¶ä¸”å…·æœ‰æ–¹ä¾¿çš„ python æ¥å£
- en: 'Path that awaits us ahead:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å‰æ–¹çš„é“è·¯ï¼š
- en: autograd in torch
  id: totrans-13
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: torch ä¸­çš„è‡ªåŠ¨å¾®åˆ†
- en: autograd in jax
  id: totrans-14
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: jax ä¸­çš„è‡ªåŠ¨å¾®åˆ†
- en: autograd in python
  id: totrans-15
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: python ä¸­çš„è‡ªåŠ¨å¾®åˆ†
- en: autograd in rust
  id: totrans-16
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: rust ä¸­çš„è‡ªåŠ¨å¾®åˆ†
- en: autograd in C
  id: totrans-17
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: C ä¸­çš„è‡ªåŠ¨å¾®åˆ†
- en: autograd in assembly
  id: totrans-18
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: æ±‡ç¼–ä¸­çš„è‡ªåŠ¨å¾®åˆ†
- en: Plus a significant amount of sloppy code and timings on M1 macbook.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: å†åŠ ä¸Šç›¸å½“æ•°é‡çš„è‰ç‡ä»£ç å’Œ M1 macbook ä¸Šçš„å®šæ—¶ã€‚
- en: Letâ€™s autograd in pytorch
  id: totrans-20
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬åœ¨ pytorch ä¸­ä½¿ç”¨è‡ªåŠ¨å¾®åˆ†
- en: 'We start our journey with pytorch â€” the default autograd engine in research.
    Weâ€™ll create a graph with many nodes, and to keep things simple our benchmark
    has only several kinds of operations: unary (softplus), binary (multiplication),
    n-ary (sum) and n-to-n (softmax).'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬ä» pytorch å¼€å§‹æˆ‘ä»¬çš„æ—…ç¨‹â€”â€”è¿™æ˜¯ç ”ç©¶ä¸­é»˜è®¤çš„è‡ªåŠ¨å¾®åˆ†å¼•æ“ã€‚æˆ‘ä»¬å°†åˆ›å»ºä¸€ä¸ªå…·æœ‰è®¸å¤šèŠ‚ç‚¹çš„å›¾å½¢ï¼Œä¸ºäº†ä¿æŒç®€å•ï¼Œæˆ‘ä»¬çš„åŸºå‡†æµ‹è¯•åªæœ‰å‡ ç§æ“ä½œï¼šä¸€å…ƒï¼ˆsoftplusï¼‰ï¼ŒäºŒå…ƒï¼ˆä¹˜æ³•ï¼‰ï¼Œn
    å…ƒï¼ˆæ±‚å’Œï¼‰å’Œ n åˆ° nï¼ˆsoftmaxï¼‰ã€‚
- en: This allows using just a few operations, but resembles a realistic load. All
    benchmarks in this post will reimplement the same logic as below.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä½¿å¾—åªä½¿ç”¨å‡ ä¸ªæ“ä½œï¼Œä½†ç±»ä¼¼äºçœŸå®è´Ÿè½½ã€‚æœ¬æ–‡ä¸­çš„æ‰€æœ‰åŸºå‡†æµ‹è¯•éƒ½å°†é‡æ–°å®ç°ä¸ä¸‹æ–‡ç›¸åŒçš„é€»è¾‘ã€‚
- en: '[PRE0]'
  id: totrans-23
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Run-time for 10k ops x 100 iterations: 11.3 seconds'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: è¿è¡Œ 10k ä¸ªæ“ä½œ x 100 æ¬¡è¿­ä»£çš„è¿è¡Œæ—¶é—´ï¼š11.3 ç§’
- en: 'Run-time for 10k ops x 10k iterations: **1130 seconds** (estimate)'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: è¿è¡Œ 10k ä¸ªæ“ä½œ x 10k æ¬¡è¿­ä»£çš„è¿è¡Œæ—¶é—´ï¼š**1130 ç§’**ï¼ˆä¼°è®¡ï¼‰
- en: Given we created 100M python objects, itâ€™s actually quite fast. And yes, thatâ€™s
    not going to deliver an interactive experience.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: é‰´äºæˆ‘ä»¬åˆ›å»ºäº† 100M ä¸ª python å¯¹è±¡ï¼Œè¿™å®é™…ä¸Šæ˜¯ç›¸å½“å¿«çš„ã€‚æ˜¯çš„ï¼Œè¿™ä¸ä¼šæä¾›äº¤äº’å¼ä½“éªŒã€‚
- en: Letâ€™s also discuss `torch.compile`, a major innovation in pytorch 2.0.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬ä¹Ÿè®¨è®ºä¸€ä¸‹`torch.compile`ï¼Œè¿™æ˜¯ pytorch 2.0 ä¸­çš„ä¸€ä¸ªé‡å¤§åˆ›æ–°ã€‚
- en: 'At 100 operations torch.compile takes 4.5 seconds. Execution gets faster: for
    100 operations and 10k iterations it takes 4.52 seconds with torch.compile and
    10.4 seconds without. Compilation + execution are still in the same ballpark.
    For bigger graphs (1k operations) `torch.compile` crashes.'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹äº 100 æ¬¡æ“ä½œï¼Œtorch.compile éœ€è¦ 4.5 ç§’ã€‚æ‰§è¡Œå˜å¾—æ›´å¿«ï¼šå¯¹äº 100 æ¬¡æ“ä½œå’Œ 10k æ¬¡è¿­ä»£ï¼Œä½¿ç”¨ torch.compile
    éœ€è¦ 4.52 ç§’ï¼Œè€Œä¸ä½¿ç”¨éœ€è¦ 10.4 ç§’ã€‚ç¼–è¯‘ + æ‰§è¡Œä»åœ¨åŒä¸€ä¸ªèŒƒå›´å†…ã€‚å¯¹äºæ›´å¤§çš„å›¾å½¢ï¼ˆ1k æ“ä½œï¼‰ï¼Œ`torch.compile` ä¼šå´©æºƒã€‚
- en: Letâ€™s autograd in jax
  id: totrans-29
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬åœ¨ jax ä¸­ä½¿ç”¨è‡ªåŠ¨å¾®åˆ†
- en: Jax is the new cool kidâ€¦ well, not that new anymore. But in some aspects it
    is very interesting. Jaxâ€™s focus on JIT-compiling static graphs is very suitable
    for the problem at hand.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: Jax æ˜¯æ–°çš„æ½®æµ... å—¯ï¼Œä¸é‚£ä¹ˆæ–°äº†ã€‚ä½†åœ¨æŸäº›æ–¹é¢ï¼Œå®ƒéå¸¸æœ‰è¶£ã€‚Jax å…³æ³¨å³æ—¶ç¼–è¯‘é™æ€å›¾ï¼Œéå¸¸é€‚åˆæ‰‹å¤´çš„é—®é¢˜ã€‚
- en: 'Implementation for benchmark is similar to pytorch:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: åŸºå‡†æµ‹è¯•çš„å®ç°ç±»ä¼¼äº pytorchï¼š
- en: '[PRE1]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Without jit computations are extremely slow:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: æ²¡æœ‰å³æ—¶ç¼–è¯‘è®¡ç®—é€Ÿåº¦ææ…¢ï¼š
- en: 1k ops x 10 iterations => 15.9 seconds
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 1k ä¸ªæ“ä½œ x 10 æ¬¡è¿­ä»£ => 15.9 ç§’
- en: 10k ops x 10k iterations => 159,000 seconds (estimate)
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 10k ä¸ªæ“ä½œ x 10k æ¬¡è¿­ä»£ => 159,000 ç§’ï¼ˆä¼°è®¡ï¼‰
- en: Thatâ€™s a bit longer than forever! But whole point of jax is to JIT-compile stuff.
    So letâ€™s do it.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ¯”æ°¸è¿œéƒ½è¦é•¿ä¸€ç‚¹ï¼ä½†æ˜¯ jax çš„æ•´ä¸ªé‡ç‚¹æ˜¯å³æ—¶ç¼–è¯‘ã€‚æ‰€ä»¥è®©æˆ‘ä»¬å¼€å§‹å§ã€‚
- en: 'jit: compilation of 1k ops = 47 seconds'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: å³æ—¶ç¼–è¯‘ï¼š1k ä¸ªæ“ä½œçš„ç¼–è¯‘ = 47 ç§’
- en: 'jit: run-time for 1k ops x 10k iterations = 0.66 seconds'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: å³æ—¶ç¼–è¯‘ï¼šè¿è¡Œ 1k ä¸ªæ“ä½œ x 10k æ¬¡è¿­ä»£ = 0.66 ç§’
- en: 'jit: 10k ops x 10k iterations (compilation + run-time) => **470 seconds** (estimate)'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: å³æ—¶ç¼–è¯‘ï¼š10k ä¸ªæ“ä½œ x 10k æ¬¡è¿­ä»£ï¼ˆç¼–è¯‘ + è¿è¡Œæ—¶é—´ï¼‰=> **470 ç§’**ï¼ˆä¼°è®¡ï¼‰
- en: Speed up in execution time is more than impressive, but we spend >99% of time
    compiling.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰§è¡Œæ—¶é—´çš„åŠ é€Ÿè¶…ä¹æƒ³è±¡ï¼Œä½†æˆ‘ä»¬èŠ±è´¹äº† >99% çš„æ—¶é—´åœ¨ç¼–è¯‘ä¸Šã€‚
- en: Tensorflow
  id: totrans-41
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: Tensorflow
- en: Someone will mention TF anyway. Iâ€™ll leave this as an exercise for you, TF fans.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: æœ‰äººæ€»ä¼šæåŠ TFã€‚æˆ‘å°†æŠŠè¿™ç•™ç»™ TF çš„ç²‰ä¸ä½œä¸ºç»ƒä¹ ã€‚
- en: Letâ€™s autograd in python
  id: totrans-43
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬åœ¨ python ä¸­ä½¿ç”¨è‡ªåŠ¨å¾®åˆ†
- en: Done with baselines, time to see if we can speed things up.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: åŸºå‡†æµ‹è¯•å®Œæˆï¼Œæ˜¯æ—¶å€™çœ‹çœ‹æˆ‘ä»¬æ˜¯å¦å¯ä»¥åŠ é€Ÿäº†ã€‚
- en: Letâ€™s create a simplistic pseudo-framework and see how it competes with previous
    candidates. Weâ€™ll implement a tape-like autograd where operations order is explicitly
    tracked in a tape.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬åˆ›å»ºä¸€ä¸ªç®€å•çš„ä¼ªæ¡†æ¶ï¼Œå¹¶çœ‹çœ‹å®ƒä¸ä¹‹å‰çš„å€™é€‰è€…ç›¸æ¯”å¦‚ä½•ã€‚æˆ‘ä»¬å°†å®ç°ä¸€ä¸ªç±»ä¼¼ç£å¸¦çš„è‡ªåŠ¨æ±‚å¯¼ï¼Œå…¶ä¸­æ“ä½œé¡ºåºåœ¨ç£å¸¦ä¸­è¢«æ˜ç¡®è·Ÿè¸ªã€‚
- en: <details><summary class="code-summary">show autograd engine in plain python</summary>
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: <details><summary class="code-summary">å±•ç¤ºçº¯ Python ä¸­çš„è‡ªåŠ¨æ±‚å¯¼å¼•æ“</summary>
- en: '[PRE2]</details>'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '[PRE2]</details>'
- en: 'and reimplement reference task using our new pseudo-framework:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: å¹¶ä½¿ç”¨æˆ‘ä»¬çš„æ–°ä¼ªæ¡†æ¶é‡æ–°å®ç°å‚è€ƒä»»åŠ¡ï¼š
- en: <details><summary class="code-summary">show benchmarking code</summary>
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: <details><summary class="code-summary">å±•ç¤ºåŸºå‡†æµ‹è¯•ä»£ç </summary>
- en: '[PRE3]</details>'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: '[PRE3]</details>'
- en: 'Run-time for 10k ops and 10k iterations: **312 seconds**.'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 10k æ¬¡æ“ä½œå’Œ 10k æ¬¡è¿­ä»£çš„è¿è¡Œæ—¶é—´ï¼š**312 ç§’**ã€‚
- en: Expectably not fast. But compared to previous candidates, thatâ€™s actually quite
    competitive!
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: é¢„æ–™ä¹‹ä¸­ä¸å¿«ã€‚ä½†ä¸ä¹‹å‰çš„å€™é€‰ç›¸æ¯”ï¼Œè¿™å®é™…ä¸Šæ˜¯ç›¸å½“æœ‰ç«äº‰åŠ›çš„ï¼
- en: Letâ€™s autograd in python, again
  id: totrans-53
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬å†æ¬¡åœ¨ Python ä¸­è¿›è¡Œè‡ªåŠ¨æ±‚å¯¼ã€‚
- en: This time we move all values into tape instead of keeping in variables. Additionally
    tape will keep a â€˜static graphâ€™ of computations by recording indices of variables
    participating in every operation.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ¬¡æˆ‘ä»¬å°†æ‰€æœ‰å€¼éƒ½ç§»åŠ¨åˆ°ç£å¸¦ä¸­ï¼Œè€Œä¸æ˜¯ä¿ç•™åœ¨å˜é‡ä¸­ã€‚æ­¤å¤–ï¼Œç£å¸¦å°†é€šè¿‡è®°å½•æ¯æ¬¡æ“ä½œä¸­å‚ä¸çš„å˜é‡çš„ç´¢å¼•æ¥ä¿ç•™è®¡ç®—çš„â€˜é™æ€å›¾â€™ã€‚
- en: <details><summary class="code-summary">show code for autograd in plain python</summary>
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: <details><summary class="code-summary">å±•ç¤ºçº¯ Python ä¸­è‡ªåŠ¨æ±‚å¯¼çš„ä»£ç </summary>
- en: '[PRE4]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: and corresponding launching code
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: å’Œç›¸åº”çš„å¯åŠ¨ä»£ç 
- en: '[PRE5]</details>'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: '[PRE5]</details>'
- en: 'Run-time for 10k ops x 10k iterations: **94 seconds**'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 10k æ¬¡æ“ä½œ x 10k æ¬¡è¿­ä»£çš„è¿è¡Œæ—¶é—´ï¼š**94 ç§’**
- en: As we see, moving all values into tape and switching to operating on indices
    is quite an efficient strategy. We still use python, but are now ~5-10 fold faster
    than `pytorch` or `jax`.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æˆ‘ä»¬æ‰€è§ï¼Œå°†æ‰€æœ‰å€¼ç§»åŠ¨åˆ°ç£å¸¦ä¸­å¹¶åˆ‡æ¢åˆ°æ“ä½œç´¢å¼•çš„ç­–ç•¥ç›¸å½“æœ‰æ•ˆã€‚æˆ‘ä»¬ä»ç„¶ä½¿ç”¨ Pythonï¼Œä½†ç°åœ¨æ¯” `pytorch` æˆ– `jax` å¿«çº¦ 5-10
    å€ã€‚
- en: 'At this point, I want to mention one more experiment: code above is organized
    to be `numba`-friendly. [Numba](https://numba.readthedocs.io/en/stable/) is famous
    for speeding up number crunching in python with minimal changes by providing just-in-time
    compilation. Recent addition of `numba.typed.List` makes it possible to efficiently
    handle list of lists.'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™ä¸€ç‚¹ä¸Šï¼Œæˆ‘æƒ³æåˆ°å¦ä¸€ä¸ªå®éªŒï¼šä¸Šé¢çš„ä»£ç æ˜¯ä¸ºäº†ä½¿å…¶å‹å¥½äº `numba` è€Œç»„ç»‡çš„ã€‚[Numba](https://numba.readthedocs.io/en/stable/)
    ä»¥æä¾›å³æ—¶ç¼–è¯‘ï¼Œé€šè¿‡æä¾›å³æ—¶ç¼–è¯‘ï¼Œä»¥æœ€å°çš„æ›´æ”¹åŠ å¿« Python ä¸­çš„æ•°å€¼è®¡ç®—è€Œé—»åã€‚æœ€è¿‘æ·»åŠ çš„ `numba.typed.List` ä½¿å¾—å¯ä»¥æœ‰æ•ˆåœ°å¤„ç†åˆ—è¡¨çš„åˆ—è¡¨ã€‚
- en: 'Run-time with numba, 10k ops x 10k iterations: **41 second**.'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: ä½¿ç”¨ numba çš„è¿è¡Œæ—¶é—´ï¼Œ10k æ¬¡æ“ä½œ x 10k æ¬¡è¿­ä»£ï¼š**41 ç§’**ã€‚
- en: At this point weâ€™re >10-fold faster than jax/pytorch (and still writing code
    in python).
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™ä¸€ç‚¹ä¸Šï¼Œæˆ‘ä»¬æ¯” jax/pytorch å¿«äº†è¶…è¿‡ 10 å€ï¼ˆä»ç„¶åœ¨ Python ä¸­ç¼–å†™ä»£ç ï¼‰ã€‚
- en: Letâ€™s autograd in rust
  id: totrans-64
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬åœ¨ Rust ä¸­è¿›è¡Œè‡ªåŠ¨æ±‚å¯¼ã€‚
- en: Once we moved graph tracking to tape, we can now use something fast to run computations
    for us. For instance, rust. For rustâ†”python interop Iâ€™ve used a small wrapper
    around [rustimport](https://github.com/mityax/rustimport). `Rustimport` allows
    to conveniently â€œimportâ€ a single rust file without creating a full-fledged rust
    project.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€æ—¦æˆ‘ä»¬å°†å›¾è·Ÿè¸ªç§»åŠ¨åˆ°ç£å¸¦ä¸­ï¼Œæˆ‘ä»¬ç°åœ¨å¯ä»¥ä½¿ç”¨ä¸€äº›å¿«é€Ÿçš„æ–¹æ³•æ¥è¿è¡Œè®¡ç®—ã€‚ä¾‹å¦‚ï¼ŒRustã€‚å¯¹äº Rust â†” Python çš„äº’æ“ä½œæ€§ï¼Œæˆ‘ä½¿ç”¨äº†ä¸€ä¸ªå›´ç»•
    [rustimport](https://github.com/mityax/rustimport) çš„å°åŒ…è£…å™¨ã€‚`Rustimport` å…è®¸æ–¹ä¾¿åœ°â€œå¯¼å…¥â€ä¸€ä¸ªå•ç‹¬çš„
    Rust æ–‡ä»¶ï¼Œè€Œä¸åˆ›å»ºä¸€ä¸ªå®Œæ•´çš„ Rust é¡¹ç›®ã€‚
- en: 'Some optimization remarks:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€äº›ä¼˜åŒ–è¯´æ˜ï¼š
- en: '`softmax` was a bottleneck, so I switched to creating temporary arrays on stack
    instead of Vecs, which required specializing on input sizes'
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`softmax` æˆä¸ºäº†ä¸€ä¸ªç“¶é¢ˆï¼Œæ‰€ä»¥æˆ‘è½¬è€Œåœ¨å †æ ˆä¸Šåˆ›å»ºä¸´æ—¶æ•°ç»„ï¼Œè€Œä¸æ˜¯ Vecsï¼Œè¿™éœ€è¦å¯¹è¾“å…¥å¤§å°è¿›è¡Œç‰¹åŒ–ã€‚'
- en: I followed rust-y approach with iterators to reduce number of boundary checks
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æˆ‘éµå¾ªäº† Rust é£æ ¼çš„è¿­ä»£å™¨æ–¹æ³•æ¥å‡å°‘è¾¹ç•Œæ£€æŸ¥çš„æ•°é‡ã€‚
- en: I wondered if match with multiple options checked one-by-one is slow. In synthetic
    tests it seemed to be relatively fast, but I wish jump table optimization was
    implemented here (e.g. it is supported for [enums](https://users.rust-lang.org/t/match-statement-efficiency/4488)
    in rust, and clang [uses](https://stackoverflow.com/questions/60109992/why-is-a-switch-not-optimized-the-same-way-as-chained-if-else-in-c-c)
    this optimization in C for switch-case)
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æˆ‘æƒ³çŸ¥é“ä¸€æ¬¡æ£€æŸ¥å¤šä¸ªé€‰é¡¹çš„åŒ¹é…æ˜¯å¦æ…¢ã€‚åœ¨åˆæˆæµ‹è¯•ä¸­ï¼Œå®ƒä¼¼ä¹ç›¸å¯¹å¿«é€Ÿï¼Œä½†æˆ‘å¸Œæœ›åœ¨è¿™é‡Œå®ç°è·³è½¬è¡¨ä¼˜åŒ–ï¼ˆä¾‹å¦‚ï¼Œå¯¹äº Rust ä¸­çš„ [enums](https://users.rust-lang.org/t/match-statement-efficiency/4488)
    æ”¯æŒæ­¤ä¼˜åŒ–ï¼Œå¹¶ä¸” clang [åœ¨ C ä¸­ä½¿ç”¨](https://stackoverflow.com/questions/60109992/why-is-a-switch-not-optimized-the-same-way-as-chained-if-else-in-c-c)
    æ­¤ä¼˜åŒ–æ¥è¿›è¡Œ switch-caseï¼‰ã€‚
- en: <details><summary class="code-summary">show rust code for minimal autograd</summary>
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: <details><summary class="code-summary">å±•ç¤ºæœ€å°è‡ªåŠ¨æ±‚å¯¼çš„ Rust ä»£ç </summary>
- en: '[PRE6]</details>'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: '[PRE6]</details>'
- en: 'Run-time for 10k ops x 10k iterations: **1.4 seconds**'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 10k æ¬¡æ“ä½œ x 10k æ¬¡è¿­ä»£çš„è¿è¡Œæ—¶é—´ï¼š**1.4 ç§’**
- en: 'Success: we are in the realm of interactive experiences.'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: æˆåŠŸï¼šæˆ‘ä»¬å¤„äºäº¤äº’å¼ä½“éªŒçš„é¢†åŸŸã€‚
- en: Recall we started from >1000 seconds. But should we stop here?
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: è®°å¾—æˆ‘ä»¬å¼€å§‹æ—¶è¶…è¿‡äº† 1000 ç§’ã€‚ä½†æ˜¯æˆ‘ä»¬åº”è¯¥åœ¨è¿™é‡Œåœä¸‹æ¥å—ï¼Ÿ
- en: Letâ€™s autograd in C
  id: totrans-75
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬åœ¨ C ä¸­è¿›è¡Œè‡ªåŠ¨å¾®åˆ†ã€‚
- en: Time to implement autograd logic in C. For interop with python I use [python-cffi](https://cffi.readthedocs.io/en/stable/index.html).
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ C ä¸­å®ç°è‡ªåŠ¨å¾®åˆ†é€»è¾‘çš„æ—¶é—´åˆ°äº†ã€‚ä¸ºäº†ä¸ Python è¿›è¡Œäº¤äº’ï¼Œæˆ‘ä½¿ç”¨ [python-cffi](https://cffi.readthedocs.io/en/stable/index.html)ã€‚
- en: 'I went bananas on optimization:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘å¯¹ä¼˜åŒ–è¿›è¡Œäº†ç–¯ç‹‚çš„å°è¯•ï¼š
- en: I used the fact that output nodes are placed consequentially in memory, so we
    pass only index of the first output
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æˆ‘åˆ©ç”¨äº†è¾“å‡ºèŠ‚ç‚¹åœ¨å†…å­˜ä¸­æ˜¯è¿ç»­æ”¾ç½®çš„äº‹å®ï¼Œå› æ­¤æˆ‘ä»¬åªä¼ é€’ç¬¬ä¸€ä¸ªè¾“å‡ºçš„ç´¢å¼•ã€‚
- en: number of inputs is limited to 8, and those are baked into struct as `int[8]`,
    not `int *` to avoid jumps in memory
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: è¾“å…¥æ•°é‡é™åˆ¶ä¸º 8ï¼Œå¹¶ä¸”è¿™äº›è¾“å…¥ä½œä¸º `int[8]` åµŒå…¥åˆ°ç»“æ„ä½“ä¸­ï¼Œè€Œä¸æ˜¯ `int *`ï¼Œä»¥é¿å…å†…å­˜è·³è½¬
- en: dynamic stack allocations of variable size (compared to rust, those are straightforward
    in C)
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: åŠ¨æ€å †æ ˆåˆ†é…å¯å˜å¤§å°ï¼ˆä¸ Rust ç›¸æ¯”ï¼Œåœ¨ C ä¸­è¿™äº›å¾ˆç®€å•ï¼‰
- en: '`-O3`, and unsafe math: `-ffast-math`. Even experimented memory alignment and
    restrict-ing pointers, but no luck'
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`-O3`ï¼Œä»¥åŠä¸å®‰å…¨çš„æ•°å­¦è¿ç®—ï¼š`-ffast-math`ã€‚ç”šè‡³å°è¯•äº†å†…å­˜å¯¹é½å’Œé™åˆ¶æŒ‡é’ˆï¼Œä½†æ²¡æœ‰è¿æ°”'
- en: <details><summary class="code-summary">show me some code in C</summary>
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: <details><summary class="code-summary">ç»™æˆ‘çœ‹ä¸€äº› C ä»£ç </summary>
- en: '[PRE7]</details>'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '[PRE7]</details>'
- en: 'Run-time for 10k ops x 10k iterations: **0.99 second**'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: è¿è¡Œ 10k æ¬¡æ“ä½œ x 10k æ¬¡è¿­ä»£çš„è¿è¡Œæ—¶é—´ï¼š**0.99 ç§’**
- en: I liked ergonomics of rust better, but achieving high speed in C is way easier.
    Rustâ€™s interop with python is also way more convenient.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘æ›´å–œæ¬¢ Rust çš„äººä½“å·¥ç¨‹å­¦ï¼Œä½†åœ¨ C ä¸­å®ç°é«˜é€Ÿåº¦è¦å®¹æ˜“å¾—å¤šã€‚Rust ä¸ Python çš„äº¤äº’ä¹Ÿæ›´æ–¹ä¾¿ã€‚
- en: Letâ€™s autograd in C (again)
  id: totrans-86
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬å†æ¬¡åœ¨ C ä¸­è¿›è¡Œè‡ªåŠ¨å¾®åˆ†ã€‚
- en: Another approach Iâ€™ve taken is to â€˜compileâ€™ traced graph to C. So python produces
    a long C file where operations are called one-by-one with explicit indices, something
    like
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘é‡‡å–çš„å¦ä¸€ç§æ–¹æ³•æ˜¯å°†è·Ÿè¸ªçš„å›¾å½¢â€œç¼–è¯‘â€æˆ Cã€‚å› æ­¤ï¼ŒPython ç”Ÿæˆä¸€ä¸ªé•¿é•¿çš„ C æ–‡ä»¶ï¼Œå…¶ä¸­æ“ä½œè¢«ä¸€ä¸ªæ¥ä¸€ä¸ªåœ°è°ƒç”¨ï¼Œå…·æœ‰æ˜¾å¼ç´¢å¼•ï¼Œç±»ä¼¼äº
- en: '[PRE8]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Source code is lengthy, outputs are enormous, and to speed up compilation we
    can set `-O0` in clang. Using `-O0` produces slower binaries, but interestingly
    *did not* speed up compilation. Best results I got are around 1 minute for compilation
    and 1 second for a full run. Surprisingly, eliminating switch/case and memory
    lookups for arguments did not result in faster execution.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: æºä»£ç å¾ˆé•¿ï¼Œè¾“å‡ºå¾ˆå¤šï¼Œä¸ºäº†åŠ å¿«ç¼–è¯‘é€Ÿåº¦ï¼Œæˆ‘ä»¬å¯ä»¥åœ¨ clang ä¸­è®¾ç½® `-O0`ã€‚ä½¿ç”¨ `-O0` ä¼šäº§ç”Ÿè¾ƒæ…¢çš„äºŒè¿›åˆ¶æ–‡ä»¶ï¼Œä½†æœ‰è¶£çš„æ˜¯ *å¹¶æ²¡æœ‰* åŠ é€Ÿç¼–è¯‘é€Ÿåº¦ã€‚æˆ‘å¾—åˆ°çš„æœ€ä½³ç»“æœæ˜¯å¤§çº¦
    1 åˆ†é’Ÿçš„ç¼–è¯‘æ—¶é—´å’Œ 1 ç§’çš„å®Œæ•´è¿è¡Œæ—¶é—´ã€‚ä»¤äººæƒŠè®¶çš„æ˜¯ï¼Œæ¶ˆé™¤å‚æ•°çš„ switch/case å’Œå†…å­˜æŸ¥æ‰¾å¹¶æ²¡æœ‰å¯¼è‡´æ›´å¿«çš„æ‰§è¡Œã€‚
- en: Given that recompilation is needed any time the graph is changed, real time
    experienced by user is 1 minute. Thatâ€™s a no go.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: é‰´äºæ¯æ¬¡å›¾å½¢å˜åŒ–éƒ½éœ€è¦é‡æ–°ç¼–è¯‘ï¼Œç”¨æˆ·ä½“éªŒçš„å®æ—¶æ€§ä¸º 1 åˆ†é’Ÿã€‚è¿™æ˜¯ä¸å¯æ¥å—çš„ã€‚
- en: Assembly
  id: totrans-91
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: æ±‡ç¼–
- en: In this endeavor to get maximal speed, I decided to go down to assembly. Otherwise
    it feels like an incomplete journey. We can map a computational graph to just
    a set of low-level instruction, and avoid â€œcostlyâ€ compilation. These days x86/64
    is not a king anymore, but neither armv7/armv8 is â€” and writing assembly for several
    architectures is totally unreasonable.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿½æ±‚æœ€å¤§é€Ÿåº¦çš„åŠªåŠ›ä¸­ï¼Œæˆ‘å†³å®šé™åˆ°æ±‡ç¼–å±‚é¢ã€‚å¦åˆ™ï¼Œæ„Ÿè§‰å°±åƒæ˜¯ä¸€æ¬¡ä¸å®Œæ•´çš„æ—…ç¨‹ã€‚æˆ‘ä»¬å¯ä»¥å°†è®¡ç®—å›¾æ˜ å°„åˆ°ä¸€ç»„ä½çº§æŒ‡ä»¤ï¼Œå¹¶é¿å…â€œæ˜‚è´µâ€çš„ç¼–è¯‘ã€‚å¦‚ä»Šï¼Œx86/64
    å·²ç»ä¸å†æ˜¯ç‹è€…ï¼Œä½† armv7/armv8 ä¹Ÿä¸æ˜¯ â€”â€” ä¸ºå‡ ç§æ¶æ„ç¼–å†™æ±‡ç¼–æ˜¯å®Œå…¨ä¸åˆç†çš„ã€‚
- en: 'So â€¦ how about using webassembly? It is low-level, fast to compile, and still
    cross-platform. Projects like `wasmer`/`wasmtime` allow interacting with wasm
    code from other languages. Thatâ€™s my first encounter with WASM, and Iâ€™ve got quite
    positive impression: WASM mixes lisp-style syntax (for efficient streaming parsing)
    and execution model of stack machine. Unlike canonical stack machines, and unlike
    canonical assembly, WASM allows grouping expressions, e.g.'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥ â€¦â€¦ ä½¿ç”¨ WebAssembly å¦‚ä½•ï¼Ÿå®ƒæ˜¯ä½çº§åˆ«çš„ï¼Œç¼–è¯‘é€Ÿåº¦å¿«ï¼Œè€Œä¸”è·¨å¹³å°ã€‚åƒ `wasmer`/`wasmtime` è¿™æ ·çš„é¡¹ç›®å…è®¸ä»å…¶ä»–è¯­è¨€ä¸
    wasm ä»£ç è¿›è¡Œäº¤äº’ã€‚è¿™æ˜¯æˆ‘ç¬¬ä¸€æ¬¡æ¥è§¦ WASMï¼Œæˆ‘å¯¹å®ƒçš„å°è±¡ç›¸å½“ç§¯æï¼šWASM ç»“åˆäº† lisp é£æ ¼çš„è¯­æ³•ï¼ˆç”¨äºé«˜æ•ˆçš„æµå¼è§£æï¼‰å’Œå †æ ˆæœºçš„æ‰§è¡Œæ¨¡å‹ã€‚ä¸ç»å…¸çš„å †æ ˆæœºä¸åŒï¼Œä¹Ÿä¸åŒäºç»å…¸çš„æ±‡ç¼–ï¼ŒWASM
    å…è®¸å¯¹è¡¨è¾¾å¼è¿›è¡Œåˆ†ç»„ï¼Œä¾‹å¦‚
- en: '[PRE9]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: This convenience allows writing significantly more readable code in WASM compared
    to ye-olde-assembly. Level of abstraction looks just right to me â€” low-level instructions,
    but no need to manage register allocations.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ç§ä¾¿åˆ©æ€§ä½¿å¾—åœ¨ WASM ä¸­ç¼–å†™çš„ä»£ç æ¯”ä¼ ç»Ÿçš„æ±‡ç¼–æ›´æ˜“è¯»ã€‚æŠ½è±¡çº§åˆ«çœ‹èµ·æ¥å¯¹æˆ‘æ¥è¯´åˆšåˆšå¥½ â€”â€” ä½çº§æŒ‡ä»¤ï¼Œä½†æ— éœ€ç®¡ç†å¯„å­˜å™¨åˆ†é…ã€‚
- en: Webassembly is still very close to assembly in terms of instructions, i.e. there
    is no `exp`, `log`, let alone `log1p` and alike. Fortunately, there is a WASM
    [implementation](https://gist.github.com/going-digital/02e46c44d89237c07bc99cd440ebfa43)
    of `exp2`/`log2` by Peter Knight.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: ä»æŒ‡ä»¤ä¸Šçœ‹ï¼ŒWebassemblyä»ç„¶ä¸æ±‡ç¼–è¯­è¨€éå¸¸æ¥è¿‘ï¼Œå³æ²¡æœ‰`exp`ï¼Œ`log`ï¼Œæ›´ä¸ç”¨è¯´`log1p`ä¹‹ç±»çš„å‡½æ•°äº†ã€‚å¹¸è¿çš„æ˜¯ï¼ŒPeter Knight
    æä¾›äº†ä¸€ä¸ª WASM çš„ [å®ç°](https://gist.github.com/going-digital/02e46c44d89237c07bc99cd440ebfa43)ï¼ŒåŒ…æ‹¬äº†`exp2`/`log2`ã€‚
- en: My major question was if speed of exponentiation is going to be sufficient,
    as `exp` consumes significant time in C implementation. Alas, in a simple benchmark
    computing just exponents in wasm takes ~1.9 seconds, leaving it behind rust/C.
    For reference, javascript computes the same number of exponents in 0.7 seconds.
    Hence, I take WASM branding of â€˜near-native speedâ€™ with a grain of salt, at least
    in the context of number crunching. Hopefully this will improve, but for now WASM
    is out of competition.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘çš„ä¸»è¦é—®é¢˜æ˜¯æŒ‡æ•°è¿ç®—çš„é€Ÿåº¦æ˜¯å¦è¶³å¤Ÿï¼Œå› ä¸ºåœ¨Cå®ç°ä¸­ï¼Œ`exp`æ¶ˆè€—äº†å¤§é‡æ—¶é—´ã€‚é—æ†¾çš„æ˜¯ï¼Œåœ¨ä¸€ä¸ªç®€å•çš„åŸºå‡†æµ‹è¯•ä¸­ï¼Œä»…åœ¨wasmä¸­è®¡ç®—æŒ‡æ•°å°±éœ€è¦çº¦1.9ç§’çš„æ—¶é—´ï¼Œæ¯”rust/Cæ…¢ã€‚ä½œä¸ºå‚è€ƒï¼Œjavascriptåœ¨0.7ç§’å†…è®¡ç®—äº†ç›¸åŒæ•°é‡çš„æŒ‡æ•°ã€‚å› æ­¤ï¼Œè‡³å°‘åœ¨æ•°å­—å¤„ç†çš„èƒŒæ™¯ä¸‹ï¼Œæˆ‘å¯¹WASMçš„â€œæ¥è¿‘æœ¬æœºé€Ÿåº¦â€çš„å“ç‰ŒæŒä¿ç•™æ€åº¦ã€‚å¸Œæœ›è¿™æ–¹é¢ä¼šæœ‰æ‰€æ”¹å–„ï¼Œä½†ç›®å‰æ¥çœ‹ï¼ŒWASMå¤„äºå¤±å»ç«äº‰åŠ›çš„çŠ¶æ€ã€‚
- en: Summary
  id: totrans-98
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æ¦‚è¦
- en: So, we achieved a **1000X speed up** compared to leading libraries.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: å› æ­¤ï¼Œä¸ä¸»æµåº“ç›¸æ¯”ï¼Œæˆ‘ä»¬å®ç°äº†**1000å€çš„åŠ é€Ÿ**ã€‚
- en: I donâ€™t find this surprising â€” major usecase for autograd system is manipulating
    large ndarrays. Memory management, copy elimination, device synchronization, parallelization
    of computations â€” these things are the main focus, and throughput of 1 million
    ops per second is totally reasonable for the vast majority of scenarios and users.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘è§‰å¾—è¿™å¹¶ä¸å¥‡æ€ª â€” è‡ªåŠ¨å¾®åˆ†ç³»ç»Ÿçš„ä¸»è¦ç”¨é€”æ˜¯æ“ä½œå¤§å‹çš„ndarraysã€‚å†…å­˜ç®¡ç†ï¼Œæ¶ˆé™¤æ‹·è´ï¼Œè®¾å¤‡åŒæ­¥ï¼Œè®¡ç®—å¹¶è¡ŒåŒ– â€” è¿™äº›éƒ½æ˜¯ä¸»è¦å…³æ³¨ç‚¹ï¼Œæ¯ç§’1ç™¾ä¸‡æ¬¡æ“ä½œçš„ååé‡å¯¹äºç»å¤§å¤šæ•°åœºæ™¯å’Œç”¨æˆ·æ¥è¯´éƒ½æ˜¯å®Œå…¨åˆç†çš„ã€‚
- en: Not for me though. My scenario is totally different in terms of numbers and
    setup, and tensor-focused autograds are too slow. For the problem at hand departing
    from the common autograd systems was the right and the only possible choice. Exploring
    different options was quite fun, and my expectations were challenged several times
    along this exploration.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: ä½†å¯¹æˆ‘æ¥è¯´å¹¶éå¦‚æ­¤ã€‚æˆ‘çš„åœºæ™¯åœ¨æ•°å­—å’Œè®¾ç½®æ–¹é¢å®Œå…¨ä¸åŒï¼Œè€Œä¸”ä»¥å¼ é‡ä¸ºé‡ç‚¹çš„è‡ªåŠ¨å¾®åˆ†é€Ÿåº¦å¤ªæ…¢äº†ã€‚å¯¹äºæ‰‹å¤´çš„é—®é¢˜ï¼Œåç¦»å¸¸è§„çš„è‡ªåŠ¨å¾®åˆ†ç³»ç»Ÿæ˜¯æ­£ç¡®ä¸”å”¯ä¸€å¯èƒ½çš„é€‰æ‹©ã€‚æ¢ç´¢ä¸åŒçš„é€‰æ‹©éå¸¸æœ‰è¶£ï¼Œæˆ‘çš„æœŸæœ›åœ¨è¿™ä¸ªæ¢ç´¢è¿‡ç¨‹ä¸­å¤šæ¬¡å—åˆ°æŒ‘æˆ˜ã€‚
- en: ğŸ‘‹
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: ğŸ‘‹
