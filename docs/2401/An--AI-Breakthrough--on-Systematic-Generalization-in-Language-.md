<!--yml

category: 未分类

date: 2024-05-27 14:35:51

-->

# 语言中的“系统泛化”的“人工智能突破”？

> 来源：[`aiguide.substack.com/p/an-ai-breakthrough-on-systematic`](https://aiguide.substack.com/p/an-ai-breakthrough-on-systematic)

**一个有趣的谜题**

这里有一个有趣的谜题给你。我会给你六个外星语言的单词：*saa*、*guu*、*ree*、*fii*、*hoo*和*muo*。图 1 展示了单个单词或单词组合导致彩色圆圈组合的示意图。鉴于示例句子，从查询句子*hoo guu muo hoo fii*应该得到什么组合的彩色圈圈？（继续阅读以获取答案。）

*图 1：* *一个有趣的谜题。*

**语言中的系统泛化**

这个小小的谜题——我将其称为谜题 1——说明了语言理解中的关键概念*组合性*、*系统性*和*生成性*。

+   **组合性：** 短语或句子的含义取决于其组成词的含义以及这些词组合的方式。例如，在谜题中的短语*hoo saa*中，*saa*的含义是“绿色圆圈”，*hoo*的含义是“双倍”，所以*hoo saa*的含义是“双倍绿色圆圈”。

+   **系统性：** 如果你能理解或产生一个特定的句子，你也能理解或产生某些相关的句子。例如，任何能理解“狗睡着了但猫醒着”的人也应该能理解“狗醒着了但猫睡着了”。任何能理解“蓝花瓶放在绿色桌子上”的人也能理解“蓝桌子放在绿花瓶上”。系统性是一种由对语言的组合性理解所赋予的能力，而人类是非常擅长系统性使用语言的。谜题 1 说明了系统性：如果你理解*hoo ree muo saa*，那么你也应该理解*hoo saa muo ree*。

+   **生成性：** 语言使用者有潜在的能力生成——并理解——无限数量的句子。例如，如果你能生成“一个蓝花瓶放在一个绿色桌子上”，那么你也应该能生成“一个蓝花瓶放在一个绿色桌子上，它放在一个蓝桌子上，而它又放在一个绿花瓶上”，等等。或者如果你学会了一个新词像“工作狂”，你可以轻松将它扩展到“购物狂”，“巧克力狂”，“话痨”等等。与系统性一样，生成性是由我们的组合语言能力所赋予的。

总的来说，这些语言能力被称为“系统泛化”。人类非常擅长系统泛化——这就是你能够给出谜题 1 的答案的原因，如图 2 所示：

*图 2：有趣谜题的答案。*

在 1980 年代末，哲学家杰里·福多尔（Jerry Fodor）和泽农·皮利辛（Zenon Pylyshyn）撰写了一篇[有影响力的论文](https://ruccs.rutgers.edu/images/personal-zenon-pylyshyn/proseminars/Proseminar13/ConnectionistArchitecture.pdf)，声称虽然“符号化”的基于规则的 AI 系统可以轻松捕捉语言中的系统泛化，但使用连接主义（即神经网络）结构却无法实现这样的能力。

实际上，多年来的许多研究努力表明，神经网络在语言方面的系统泛化能力很差。虽然今天最有能力的大型语言模型（例如 GPT-4）给人们留下了系统泛化的*表象*——例如，它们生成无瑕疵的英语句法，可以非常好地解释新颖的英语句子——但当它们面临远超出其训练数据范围的任务时，例如 Puzzle 1 中的虚构语言，它们经常在类似人类的泛化方面失败。

[最近一篇论文](https://www.nature.com/articles/s41586-023-06668-3)由 [Brenden Lake](https://cims.nyu.edu/~brenden/) 和 [Marco Baroni](https://marcobaroni.org/) 提供了 Fodor & Pylyshyn 的论断的反例，以一个神经网络形式实现了“类人的系统泛化”。简而言之，Lake & Baroni 创建了一组类似于 Puzzle 1 的谜题，并要求人们解决。他们还训练了一个神经网络，使用一种称为“元学习”的方法来解决这些谜题（下文将详细介绍）。他们发现，神经网络不仅获得了解决此类谜题的强大能力，而且其表现非常类似于人类，包括它所犯的错误类型。

Lake & Baroni 论文在媒体上得到了广泛报道。例如，*[自然](https://www.nature.com/articles/d41586-023-03272-3)* 称其为“AI 突破”，而*[科学美国人](https://www.scientificamerican.com/article/new-training-method-helps-ai-generalize-like-people-do/)* 则描述其为一种帮助 AI “像人类一样泛化”的方法。我们本地的 AI 阅读小组深入研究了这篇论文；我觉得讨论非常有趣，并认为在这里进行介绍会很有用。在这篇文章中，我将讨论这篇论文的内容以及在多大程度上实现了（或未实现）这些热情的描述。

**任务、语法和元语法**

对于他们的研究，Lake & Baroni 创建了大量的“任务”——类似于 Puzzle 1 的谜题。每个任务都是从一个底层“语法”自动生成的，即将符号序列转换为颜色配置的一组规则。例如，Puzzle 1 的语法如图 3 所示：

*图 3：Puzzle 1 的语法。*

这里，“[ ]” 中的符号表示“用相应的颜色模式替换”，变量 *x* 和 *y* 可以分别是原始颜色词或函数（例如 *hoo saa*）或任何函数组合（例如 *hoo saa muo ree*）。这里语法中函数的顺序表示它们必须被应用的顺序（例如，在 *muo* 之前应用 *hoo*）。

你可以验证所有来自 Puzzle 1 的示例句子都可以从这个语法生成。Lake & Baroni 使用这样的语法通过列出一组原始颜色词及其相应的颜色，然后从语法中的各种函数规则中生成少量示例和查询句子，使用随机选择填充变量 *x* 和 *y*。

鉴于由这样一个非常简单的语法生成的大量示例句子，弄清楚语法的基本规则并不难。但 Lake & Baroni 想教会神经网络解决一个更一般的任务：从仅有少量示例开始，在由 *不同* 语法生成的任务上执行系统化泛化。

为了自动生成来自不同语法的任务，Lake & Baroni 需要一种自动生成不同语法的自动方法——即“元语法”。元语法对生成像图 3 中那样的语法具有简单规则：任何语法都包含从单词到彩色圆圈的映射，以及一小组函数，每个函数接受一个或两个参数，并将其映射到参数的新简单配置（每个规则的长度有限）。例如，图 4 显示了我从元语法生成的新语法。

*图 4：另一种可能的语法。*

**人类研究**

为了评估人类在这些任务上的系统化泛化能力，Lake & Baroni 在亚马逊 Mechanical Turk 上招募了 30 名参与者，并对他们进行了各种类似谜题的测试，每个谜题都是由不同的语法生成的。在测试之前，参与者被教导如何解决谜题，从涉及单个函数的查询开始，然后逐渐转向更复杂的函数组合。在学习阶段未成功的参与者不参加测试阶段；最终，有 25 名参与者进行了测试。正如 [Nature](https://www.nature.com/articles/d41586-023-03272-3) 报道的，“[P] 人们在这项任务上表现出色；平均约有 80% 的时间他们选择了正确的彩色圆圈组合。当他们犯错时，研究人员注意到这些错误遵循了反映已知人类偏见的模式。”

**训练神经网络进行系统化泛化**

为了教导神经网络解决这些任务，Lake & Baroni 使用了[变压器架构](https://towardsdatascience.com/transformers-141e32e69591) - 一种特定类型的深度神经网络 - 其参数约为 140 万（与像 GPT-4 这样的巨兽相比较小）。如图 5 下面所示，变压器的输入是一个类似拼图 1 的拼图，其中包含查询句子（要解决的句子）和示例句子。网络被训练输出拼图的答案：与查询句子对应的一系列彩色圆圈的序列。

*图 5：变压器输入和输出的说明。*

Lake & Baroni 的论文的关键贡献是网络的训练方法，一种“元学习”的形式。注意，一个单独的拼图本身就是一个学习任务：拼图解决者从一小组示例（例句）中学习，以推断一个新句子（查询句子）的*含义*（彩色圆圈序列）。通过向网络提供许多这样的学习任务的示例 - 从不同的语法生成的示例 - 可以说网络在“元学习”，即更一般地学习如何执行小学习任务。

Lake & Baroni 将他们的网络训练方法称为“组合性元学习”（MLC）。目标是训练网络不是针对特定任务，而是为了达到人类所见的一种一般系统的组合泛化。MLC 网络在一系列“剧集”中进行训练。对于每个剧集，从元语法生成一个新的语法（如图 3 和图 4 中的语法）。然后使用新的语法生成一组示例句子和一组查询句子。将每个查询句子与所有示例句子配对，然后将其提供给变压器网络，如图 5 所示。对于每个查询，网络预测一个标记序列，并更新权重使得正确序列更有可能。这个过程持续进行了 100,000 个剧集。

不过，有一个转折。我上面提到人类在这些任务上的测试大约 80%的时间都能得到正确答案。由于 Lake & Baroni 希望他们的神经网络在泛化行为上类似于人类，因此网络只在 80%的查询句子上用正确答案进行训练。在另外 20%的情况下，“正确答案”实际上是一个错误答案，反映了人类被发现犯的错误类型。

训练后，MLC 网络在一组由训练中使用的相同元语法生成的新难题上进行了测试。在这些新难题上，网络的表现非常类似于人类：它大约 82%的时间得到了实际（“系统化”）的正确答案（人类的正确率为 81%），而且它所犯的错误与人类所犯的错误相似。这种性能的相似性是论文标题中“类人化系统化概括”的含义。

有趣的是，当 Lake & Baroni 给 GPT-4 相同的新难题时，该系统只有 58%的时间给出了正确答案。当然，GPT-4 没有接受这些类型的难题的培训，除了提示中提供的示例，所以在某种意义上 58%是令人印象深刻的，但它远远低于人类的表现，而人类在这类难题上也只接受了最少的培训。

Lake & Baroni 尝试了 MLC 算法的变化，并且给其他神经网络提供了相同的任务；他们还测试了 MLC 对其他类型的系统化概括问题。我在这里不会详细介绍这些；如果你对细节感兴趣，请看一下[论文](https://www.nature.com/articles/s41586-023-06668-3)。

**我的思考和问题**

我觉得这篇论文是一个迷人的原理证明 —— 也就是说，它表明了 Fodor & Pylyshin 对神经网络的观点在测试系统化概括的特定任务类别时并不成立。正如作者所指出的，他们能够在没有任何“符号机制”的情况下实现系统化概括，而 Fodor & Pylyshin 声称这是必要的。

但是 MLC 方法到底在多大程度上实现了“类人化的系统化概括”？在本文中，“类人化”意味着在特定类别的概括任务上，表现（成功和失败）与人类相似。但即使在这个特定的任务上，MLC 系统也非常不像人类，因为它需要接受成千上万个这些任务的例子的训练，而人类只需要进行最少的训练即可达到相同的表现，因为他们可以建立在非常普遍的技能和生命周期内发生的培训之上。此外，人类很容易将这些技能调整到学习解决不同类别的概括任务（例如，给予 MLC 相同类型的任务，但使用他们以前未见过的单词，或者更长的句子，或者通过不同的“元语法”生成）。相比之下，MLC 将无法解决这些任务——有人可能会说该系统无法“元概括”。正如*科学美国人*所报道的：

> 训练协议帮助模型在一种类型的任务中表现出色：学习一个虚构语言中的模式。但是给定一个全新的任务，它无法应用相同的技能。这在基准测试中是明显的，在那里模型无法处理更长的序列，并且无法理解先前未引入的“单词”。

重要的是，“元学习”和“元泛化”的概念对于人类来说只是普通学习和泛化的一部分。 MLC 系统是 AI 的一项进步，但在无法像人类那样更广泛地泛化其组成技能方面仍然不像人类。 “符号性成分”是否需要用于这种更广泛的泛化能力，这仍然是一个未解之谜，而这种能力是人类所拥有的“一般”智能的核心。

在这篇论文中让我感到困惑的一件事是明确训练使系统更像“人类”。正如我上面所描述的，Lake＆Baroni 在这些任务中记录了人类的错误频率和类型，然后明确地训练他们的网络示例，这些示例具有相同的错误频率和类型。然后，他们观察到，在新任务上，经过训练的模型产生了与人类相似的错误频率和类型。但是鉴于明确的训练，我不明白为什么这会令人惊讶，我也看不出这样的结果提供了什么见解。我认为，如果他们以更一般的方式训练他们的系统，并且“人类”表现出现，那将更有趣。就我所知，我不确定这个结果意味着什么。

总之，这是关于神经网络中系统化泛化的一个非常有趣的原理性论文。我不会将其描述为“AI 的突破”-对我来说，那意味着具有更广泛和更强大的泛化能力的系统-但绝对是一个重要主题上的有前景的方法，值得进一步研究和审查。

感谢 Alessandro Palmarini，Martha Lewis 和 Una-May O’Reilly 帮助我思考这篇论文！

**附言**

在另一个话题上，我有一些最近的文章和演讲，这些文章和演讲可能会让这个 Substack 的读者感兴趣：

我正在为 *Science Magazine* 写一些偶尔的非技术专栏，重点是 AI。到目前为止，我的专栏有：

· [AI 意识世界的挑战](https://www.science.org/doi/full/10.1126/science.adm8175)

· [我们如何知道智能 AI 系统有多聪明？](https://www.science.org/doi/10.1126/science.adj5957)

我正在写一篇关于“AGI”含义的新论文。敬请关注！

在 2023 年 11 月，我在圣菲研究所公开演讲了一场名为“AI 的未来”的讲座-您可以在[这里](https://www.youtube.com/watch?v=GwHDAfAAKd4)观看。

我与 SFI 合作者在我们的 ConceptARC 抽象推理基准测试上比较了人类、GPT-4 和 GPT-4-Vision。这是[论文](https://arxiv.org/abs/2311.09247v3)。

我参加了一项关于“深度学习的现状和未来”的 AI 研究人员的调查。这是[论文](https://arxiv.org/abs/2312.09323)。

下次再见！
