["```\nimport torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\n\nmodel_path = 'ahxt/LiteLlama-460M-1T'\n\nmodel = AutoModelForCausalLM.from_pretrained(model_path)\ntokenizer = AutoTokenizer.from_pretrained(model_path)\nmodel.eval()\n\nprompt = 'Q: What is the largest bird?\\nA:'\ninput_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\ntokens = model.generate(input_ids, max_length=20)\nprint( tokenizer.decode(tokens[0].tolist(), skip_special_tokens=True) ) \n```"]