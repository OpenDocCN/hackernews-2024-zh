- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-05-27 14:35:51'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
- en: An “AI Breakthrough” on Systematic Generalization in Language?
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://aiguide.substack.com/p/an-ai-breakthrough-on-systematic](https://aiguide.substack.com/p/an-ai-breakthrough-on-systematic)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '**A Fun Puzzle**'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
- en: 'Here’s a fun puzzle for you.  I’ll give you six words in an alien language:
    *saa*, *guu*, *ree*, *fii*, *hoo*, and *muo*.  Figure 1 gives a diagram showing
    how either single words or combinations of words result in combinations of colored
    circles.  Given the example sentences, what combination of colored circles should
    result from the query sentence *hoo guu muo hoo fii*?  (Read further for the answer.)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 1:*  *A fun puzzle.*'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
- en: '**Systematic Generalization in Language**'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
- en: This small puzzle—I’ll call it Puzzle 1—illustrates the crucial notions of *compositionality*,
    *systematicity*, and *productivity* in language understanding.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
- en: '**Compositionality:** The meaning of a phrase or sentence is a function of
    the meaning of its component words and the way in which those words are combined. 
    For example, in the puzzle’s phrase *hoo*  *saa*, the meaning of *saa* is “green
    circle” and the meaning of *hoo* is “double,” so the meaning of *hoo saa* is “double
    green circle.”'
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Systematicity:** If you can understand or produce a particular sentence,
    you can also understand or produce certain related sentences.  For example, anyone
    who understands “the dog was asleep but the cat was awake” should also be able
    to understand “the dog was awake but the cat was asleep.”  Anyone who understands
    “the blue vase was on top of the green table” can also understand “the blue table
    was on top of the green vase.”  Systematicity is an ability that is enabled by
    a compositional understanding of language, and humans are very adept systematic
    language users.  Puzzle 1 illustrates systematicity: if you understand *hoo ree
    muo saa* you should also understand *hoo saa muo ree*.'
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Productivity:** Language users have the potential ability to generate—and
    understand—an infinite number of sentences.  For example, if you can generate
    “A blue vase is on top of a green table,” you should also be able to generate
    “A blue vase is on top of a green table, which is on top of a blue table, which
    is on top of a green vase,” and so on.  Or if you learn a new word like “workaholic,”
    you can easily extend it to “shopaholic,” “chocaholic,” “talkaholic,” etc.  Like
    systematicity, productivity is enabled by our compositional language abilities.'
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Taken together, these linguistic abilities have been called “systematic generalization.” 
    Humans are very good at systematic generalization—it’s what enables you to give
    the answer to Puzzle 1, shown in Figure 2:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 2: Answer to the fun puzzle.*'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
- en: In the late 1980s, the philosophers Jerry Fodor and Zenon Pylyshyn wrote an
    [influential paper](https://ruccs.rutgers.edu/images/personal-zenon-pylyshyn/proseminars/Proseminar13/ConnectionistArchitecture.pdf)
    claiming that while “symbolic,” rule-based AI systems could easily capture systematic
    generalization in language, such abilities were not achievable with connectionist
    (i.e., neural network) architectures.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 在 1980 年代末，哲学家杰里·福多尔（Jerry Fodor）和泽农·皮利辛（Zenon Pylyshyn）撰写了一篇[有影响力的论文](https://ruccs.rutgers.edu/images/personal-zenon-pylyshyn/proseminars/Proseminar13/ConnectionistArchitecture.pdf)，声称虽然“符号化”的基于规则的
    AI 系统可以轻松捕捉语言中的系统泛化，但使用连接主义（即神经网络）结构却无法实现这样的能力。
- en: Indeed, it has been shown in many research efforts over the years that neural
    networks struggle with systematic generalization in language. While today’s most
    capable large language models (e.g., GPT-4) give the *appearance* of systematic
    generalization—e.g., they generate flawless English syntax and can interpret novel
    English sentences extremely well—they often fail on human-like generalization
    when given tasks that fall too far outside their training data, such as the made-up
    language in Puzzle 1.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，多年来的许多研究努力表明，神经网络在语言方面的系统泛化能力很差。虽然今天最有能力的大型语言模型（例如 GPT-4）给人们留下了系统泛化的*表象*——例如，它们生成无瑕疵的英语句法，可以非常好地解释新颖的英语句子——但当它们面临远超出其训练数据范围的任务时，例如
    Puzzle 1 中的虚构语言，它们经常在类似人类的泛化方面失败。
- en: A [recent paper](https://www.nature.com/articles/s41586-023-06668-3) by [Brenden
    Lake](https://cims.nyu.edu/~brenden/) and [Marco Baroni](https://marcobaroni.org/)
    offers a counterexample to Fodor & Pylyshyn’s claims, in the form of a neural
    network that achieves “human-like systematic generalization.” In short, Lake &
    Baroni created a set of puzzles similar to Puzzle 1 and gave them to people to
    solve.  They also trained a neural network to solve these puzzles using a method
    called “meta-learning” (more on this below).  They found that not only did the
    neural network gain a strong ability to solve such puzzles, its performance was
    very similar to that of people, including the kinds of errors it made.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: '[最近一篇论文](https://www.nature.com/articles/s41586-023-06668-3)由 [Brenden Lake](https://cims.nyu.edu/~brenden/)
    和 [Marco Baroni](https://marcobaroni.org/) 提供了 Fodor & Pylyshyn 的论断的反例，以一个神经网络形式实现了“类人的系统泛化”。简而言之，Lake
    & Baroni 创建了一组类似于 Puzzle 1 的谜题，并要求人们解决。他们还训练了一个神经网络，使用一种称为“元学习”的方法来解决这些谜题（下文将详细介绍）。他们发现，神经网络不仅获得了解决此类谜题的强大能力，而且其表现非常类似于人类，包括它所犯的错误类型。'
- en: The Lake & Baroni paper was covered broadly in the media. For example, *[Nature](https://www.nature.com/articles/d41586-023-03272-3)*
    called it an “AI Breakthrough” and *[Scientific American](https://www.scientificamerican.com/article/new-training-method-helps-ai-generalize-like-people-do/)*
    described it as a method for helping AI “generalize like people do.”  Our local
    AI reading group delved into this paper; I found the discussion really interesting
    and thought it would be useful to cover it here. In this post I’ll discuss what
    the paper does and to what extent it fulfills (or does not fulfill) these enthusiastic
    characterizations.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: Lake & Baroni 论文在媒体上得到了广泛报道。例如，*[自然](https://www.nature.com/articles/d41586-023-03272-3)*
    称其为“AI突破”，而*[科学美国人](https://www.scientificamerican.com/article/new-training-method-helps-ai-generalize-like-people-do/)*
    则描述其为一种帮助 AI “像人类一样泛化”的方法。我们本地的 AI 阅读小组深入研究了这篇论文；我觉得讨论非常有趣，并认为在这里进行介绍会很有用。在这篇文章中，我将讨论这篇论文的内容以及在多大程度上实现了（或未实现）这些热情的描述。
- en: '**Tasks, Grammars, and Meta-Grammars**'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: '**任务、语法和元语法**'
- en: 'For their study, Lake & Baroni created a large number of “tasks”—puzzles similar
    to Puzzle 1.  Each task was created automatically from an underlying “grammar”,
    a set of rules for translating sequences of symbols to color configurations. 
    For example, a grammar for Puzzle 1 is shown in Figure 3:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 对于他们的研究，Lake & Baroni 创建了大量的“任务”——类似于 Puzzle 1 的谜题。每个任务都是从一个底层“语法”自动生成的，即将符号序列转换为颜色配置的一组规则。例如，Puzzle
    1 的语法如图 3 所示：
- en: '*Figure 3: A grammar for Puzzle 1.*'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 3：Puzzle 1 的语法。*'
- en: Here, “[ ]” around a symbol means “replace by the corresponding color pattern”,
    and the variables *x* and *y* can each be either be a primitive color word or
    a function (like *hoo*  *saa*) or any composition of functions (like *hoo saa
    muo ree*).  Here the order of functions in the grammar indicates the order in
    which they must be applied (e.g., *hoo* is applied before *muo*).
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，“[ ]” 中的符号表示“用相应的颜色模式替换”，变量 *x* 和 *y* 可以分别是原始颜色词或函数（例如 *hoo saa*）或任何函数组合（例如
    *hoo saa muo ree*）。这里语法中函数的顺序表示它们必须被应用的顺序（例如，在 *muo* 之前应用 *hoo*）。
- en: You can verify that all the example sentences from Puzzle 1 can be generated
    from this grammar.  Lake & Baroni used such grammars to generate new tasks by
    listing a set of primitive color words and their corresponding colors, and then
    generating a small number of example and query sentences from the various function
    rules in the grammar, using random choices for filling in variables *x* and *y*.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以验证所有来自 Puzzle 1 的示例句子都可以从这个语法生成。Lake & Baroni 使用这样的语法通过列出一组原始颜色词及其相应的颜色，然后从语法中的各种函数规则中生成少量示例和查询句子，使用随机选择填充变量
    *x* 和 *y*。
- en: 'Given a large number of example sentences generated by a very simple grammar
    like this, it isn’t hard to figure out the underlying rules of the grammar.  But
    Lake & Baroni wanted to teach neural networks to solve a more general task: performing
    systematic generalization from just a few examples on tasks generated from *different*
    grammars.'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 鉴于由这样一个非常简单的语法生成的大量示例句子，弄清楚语法的基本规则并不难。但 Lake & Baroni 想教会神经网络解决一个更一般的任务：从仅有少量示例开始，在由
    *不同* 语法生成的任务上执行系统化泛化。
- en: 'To automatically generate tasks from different grammars, Lake & Baroni needed
    an automatic way to generate different grammars—namely, a “meta-grammar.”  The
    meta-grammar had simple rules for generating grammars like the one in Figure 3:
    any grammar would contain mappings from words to colored circles, as well as a
    small set of functions, each of which takes one or two arguments and maps to a
    new simple configuration of the arguments (with a limitation on the length of
    each rule).  For example, Figure 4 shows a new grammar I generated from the meta-grammar.'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 为了自动生成来自不同语法的任务，Lake & Baroni 需要一种自动生成不同语法的自动方法——即“元语法”。元语法对生成像图 3 中那样的语法具有简单规则：任何语法都包含从单词到彩色圆圈的映射，以及一小组函数，每个函数接受一个或两个参数，并将其映射到参数的新简单配置（每个规则的长度有限）。例如，图
    4 显示了我从元语法生成的新语法。
- en: '*Figure 4: Another possible grammar.*'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 4：另一种可能的语法。*'
- en: '**Human Studies**'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: '**人类研究**'
- en: In order to benchmark humans’ systematic generalization abilities on these tasks,
    Lake & Baroni recruited 30 participants on Amazon Mechanical Turk, and tested
    them on a variety of such puzzles, each generated from a different grammar.  Before
    being tested, the participants were taught how to solve the puzzles starting with
    queries involving single functions and then moving to more complex function compositions. 
    The participants who did not succeed during the learning phase did not participate
    in the test phase; in the end, 25 participants were tested.  As reported in [Nature](https://www.nature.com/articles/d41586-023-03272-3),
    “[P]eople excelled at this task; they chose the correct combination of coloured
    circles about 80% of the time, on average. When they did make errors, the researchers
    noticed that these followed a pattern that reflected known human biases.”
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 为了评估人类在这些任务上的系统化泛化能力，Lake & Baroni 在亚马逊 Mechanical Turk 上招募了 30 名参与者，并对他们进行了各种类似谜题的测试，每个谜题都是由不同的语法生成的。在测试之前，参与者被教导如何解决谜题，从涉及单个函数的查询开始，然后逐渐转向更复杂的函数组合。在学习阶段未成功的参与者不参加测试阶段；最终，有
    25 名参与者进行了测试。正如 [Nature](https://www.nature.com/articles/d41586-023-03272-3) 报道的，“[P]
    人们在这项任务上表现出色；平均约有 80% 的时间他们选择了正确的彩色圆圈组合。当他们犯错时，研究人员注意到这些错误遵循了反映已知人类偏见的模式。”
- en: '**Training a Neural Network for Systematic Generalization**'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: '**训练神经网络进行系统化泛化**'
- en: 'To teach neural networks to solve these tasks, Lake & Baroni used a [transformer
    architecture](https://towardsdatascience.com/transformers-141e32e69591)—a specific
    kind of deep neural network—with about 1.4 million parameters (small compared
    to behemoths like GPT-4).  As illustrated below in Figure 5, the input to the
    transformer is a puzzle like Puzzle 1, with the query sentence (the one to be
    solved) concatenated with the example sentences. The network is trained to output
    the answer to the puzzle: a sequence of colored circles corresponding to the query
    sentence.'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 为了教导神经网络解决这些任务，Lake & Baroni 使用了[变压器架构](https://towardsdatascience.com/transformers-141e32e69591)
    - 一种特定类型的深度神经网络 - 其参数约为140万（与像 GPT-4 这样的巨兽相比较小）。如图5下面所示，变压器的输入是一个类似拼图1的拼图，其中包含查询句子（要解决的句子）和示例句子。网络被训练输出拼图的答案：与查询句子对应的一系列彩色圆圈的序列。
- en: '*Figure 5: Illustration of the transformer’s input and output.*'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: '*图5：变压器输入和输出的说明。*'
- en: 'The key contribution of Lake & Baroni’s paper is the training method for the
    network, a form of “meta-learning.”  Note that an individual puzzle is itself
    a learning task: the puzzle solver learns from a small set of examples (the example
    sentences) to infer the *meaning* (colored circle sequence) of a new sentence
    (the query sentence).  By giving many examples of such learning tasks to the network—ones
    generated from different grammars—the network can be said to be “meta-learning,”
    that is, learning more generally how to perform the small learning tasks.'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: Lake & Baroni 的论文的关键贡献是网络的训练方法，一种“元学习”的形式。注意，一个单独的拼图本身就是一个学习任务：拼图解决者从一小组示例（例句）中学习，以推断一个新句子（查询句子）的*含义*（彩色圆圈序列）。通过向网络提供许多这样的学习任务的示例
    - 从不同的语法生成的示例 - 可以说网络在“元学习”，即更一般地学习如何执行小学习任务。
- en: Lake & Baroni call their network training method “Meta-Learning for Compositionality”
    (MLC).  The goal is to train the network not for a specific task, but rather to
    achieve the kind of general systematic compositional generalization seen in humans.
    The MLC network is trained over a series of “episodes.”  For each episode, a new
    grammar (like the ones in Figures 3 and 4) is generated from the meta-grammar. 
    The new grammar then is used to generate a set of example sentences and a set
    of query sentences.  Each query sentence, paired with all the example sentences,
    is given to the Transformer network, as illustrated in Figure 5.  For each query
    the network predicts a sequence of tokens, and the weights are updated to make
    the correct sequence more likely.  This process continues for 100,000 episodes.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: Lake & Baroni 将他们的网络训练方法称为“组合性元学习”（MLC）。目标是训练网络不是针对特定任务，而是为了达到人类所见的一种一般系统的组合泛化。MLC
    网络在一系列“剧集”中进行训练。对于每个剧集，从元语法生成一个新的语法（如图3和图4中的语法）。然后使用新的语法生成一组示例句子和一组查询句子。将每个查询句子与所有示例句子配对，然后将其提供给变压器网络，如图5所示。对于每个查询，网络预测一个标记序列，并更新权重使得正确序列更有可能。这个过程持续进行了100,000个剧集。
- en: There’s a twist, though.  I mentioned above that humans tested on these tasks
    get the right answer about 80% of the time.  Since Lake & Baroni wanted their
    neural network to be *human-like* in its generalization behavior, the network
    was trained with the correct answer on only 80% of the Query Sentences.  On the
    other 20%, the “correct answer” was actually an incorrect answer that reflected
    the kinds of errors humans were seen to make.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 不过，有一个转折。我上面提到人类在这些任务上的测试大约80%的时间都能得到正确答案。由于 Lake & Baroni 希望他们的神经网络在泛化行为上类似于人类，因此网络只在80%的查询句子上用正确答案进行训练。在另外20%的情况下，“正确答案”实际上是一个错误答案，反映了人类被发现犯的错误类型。
- en: 'After training, the MLC network was tested on a set of new puzzles, produced
    by new grammars generated by the same meta-grammar used in training.  On these
    new puzzles, the network’s performance was very similar to that of humans: it
    got the actual (“systematic”) correct answer about 82% of the time (humans got
    81% correct), and the errors it made were similar to those made by humans.  This
    similarity in performance is the meaning of the term “human-like” in the paper’s
    title (“Human-like systematic generalization through a meta-learning neural network”).'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 训练后，MLC网络在一组由训练中使用的相同元语法生成的新难题上进行了测试。在这些新难题上，网络的表现非常类似于人类：它大约82%的时间得到了实际（“系统化”）的正确答案（人类的正确率为81%），而且它所犯的错误与人类所犯的错误相似。这种性能的相似性是论文标题中“类人化系统化概括”的含义。
- en: Interestingly, when Lake & Baroni gave the same new puzzles to GPT-4, that system
    gave a correct answer only 58% of the time.  Of course, GPT-4 was not trained
    on these kinds of puzzles, beyond the examples given in the prompt, so in some
    sense 58% is impressive, but it is far below the performance of humans, who were
    also only minimally trained on such puzzles.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 有趣的是，当Lake & Baroni给GPT-4相同的新难题时，该系统只有58%的时间给出了正确答案。当然，GPT-4没有接受这些类型的难题的培训，除了提示中提供的示例，所以在某种意义上58%是令人印象深刻的，但它远远低于人类的表现，而人类在这类难题上也只接受了最少的培训。
- en: Lake & Baroni experimented with variations on the MLC algorithm and with giving
    the same tasks to other neural networks; they also tested MLC on other types of
    systematic generalization problems.  I won’t cover all this here; if you’re interested
    in the details, take a look at [the paper](https://www.nature.com/articles/s41586-023-06668-3).
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: Lake & Baroni尝试了MLC算法的变化，并且给其他神经网络提供了相同的任务；他们还测试了MLC对其他类型的系统化概括问题。我在这里不会详细介绍这些；如果你对细节感兴趣，请看一下[论文](https://www.nature.com/articles/s41586-023-06668-3)。
- en: '**My Thoughts and Questions**'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '**我的思考和问题**'
- en: I found this paper to be a fascinating proof-of-principle—that is, it shows
    that Fodor & Pylyshin’s claims about neural networks do not hold for a particular
    class of tasks testing systematic generalization.  As the authors point out, they
    were able to achieve systematic generalization without any “symbolic machinery,”
    which Fodor & Pylyshyn claimed would be necessary.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 我觉得这篇论文是一个迷人的原理证明 —— 也就是说，它表明了Fodor & Pylyshin对神经网络的观点在测试系统化概括的特定任务类别时并不成立。正如作者所指出的，他们能够在没有任何“符号机制”的情况下实现系统化概括，而Fodor
    & Pylyshin声称这是必要的。
- en: 'But to what extent does the MLC method actually achieve “human-like systematic
    generalization”?  In this paper, “human-like” means having performance (both successes
    and failures) similar to that of humans on a specific class of generalization
    task.  But even on this particular task, the MLC system is quite unhuman-like,
    in that it requires being trained on hundreds of thousands of examples of these
    tasks, whereas humans need only minimal training to achieve the same performance,
    because they can build on very general skills and training that has occurred over
    their lifetimes.  Moreover, humans easily adapt these skills to learn to solve
    different classes of generalization tasks (e.g., the same kind of tasks given
    to MLC but with words they hadn’t seen before, or with longer sentences, or generated
    via a different “meta-grammar”).  MLC, in contrast, would not be able to solve
    such tasks—one might say that the system is not able to “meta-generalize.”  As
    *Scientific American* reported:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 但是MLC方法到底在多大程度上实现了“类人化的系统化概括”？在本文中，“类人化”意味着在特定类别的概括任务上，表现（成功和失败）与人类相似。但即使在这个特定的任务上，MLC系统也非常不像人类，因为它需要接受成千上万个这些任务的例子的训练，而人类只需要进行最少的训练即可达到相同的表现，因为他们可以建立在非常普遍的技能和生命周期内发生的培训之上。此外，人类很容易将这些技能调整到学习解决不同类别的概括任务（例如，给予MLC相同类型的任务，但使用他们以前未见过的单词，或者更长的句子，或者通过不同的“元语法”生成）。相比之下，MLC将无法解决这些任务——有人可能会说该系统无法“元概括”。正如*科学美国人*所报道的：
- en: '[T]he training protocol helped the model excel in one type of task: learning
    the patterns in a fake language. But given a whole new task, it couldn’t apply
    the same skill. This was evident in benchmark tests, where the model failed to
    manage longer sequences and couldn’t grasp previously unintroduced ‘words.’'
  id: totrans-42
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 训练协议帮助模型在一种类型的任务中表现出色：学习一个虚构语言中的模式。但是给定一个全新的任务，它无法应用相同的技能。这在基准测试中是明显的，在那里模型无法处理更长的序列，并且无法理解先前未引入的“单词”。
- en: Importantly, notions of “meta-learning” and “meta-generalization” are, for humans,
    simply part and parcel of ordinary learning and generalization.  The MLC system
    is an advance for AI, but remains unhuman-like in its failure to more broadly
    generalize its compositional skills like people can.  It’s still an open question
    whether “symbolic components” *a la* Fodor & Pylyshyn will be needed for such
    broader generalization abilities, which are are the core of the “general” intelligence
    humans possess.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 重要的是，“元学习”和“元泛化”的概念对于人类来说只是普通学习和泛化的一部分。 MLC 系统是 AI 的一项进步，但在无法像人类那样更广泛地泛化其组成技能方面仍然不像人类。
    “符号性成分”是否需要用于这种更广泛的泛化能力，这仍然是一个未解之谜，而这种能力是人类所拥有的“一般”智能的核心。
- en: One thing that confused me in this paper was the explicit training to make the
    system act more “human-like.”  As I described above, after cataloging the frequency
    and kinds of errors made by humans on these tasks, Lake & Baroni trained their
    network explicitly on examples having the same frequency and kinds of errors. 
    They then observed that, on new tasks, the trained model produced error frequencies
    and types similar to those of humans.  But given the explicit training, I didn’t
    understand why this would surprising, and I didn’t see what insights such results
    provide.  It would have been more interesting, I think, if they had trained their
    system in a more general way, and the “human-like” performance had emerged.  As
    is, I wasn’t sure what this result was meant to show.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇论文中让我感到困惑的一件事是明确训练使系统更像“人类”。正如我上面所描述的，Lake＆Baroni在这些任务中记录了人类的错误频率和类型，然后明确地训练他们的网络示例，这些示例具有相同的错误频率和类型。然后，他们观察到，在新任务上，经过训练的模型产生了与人类相似的错误频率和类型。但是鉴于明确的训练，我不明白为什么这会令人惊讶，我也看不出这样的结果提供了什么见解。我认为，如果他们以更一般的方式训练他们的系统，并且“人类”表现出现，那将更有趣。就我所知，我不确定这个结果意味着什么。
- en: In conclusion, this is a very interesting proof-of-principle paper on systematic
    generalization in neural networks.  I wouldn’t characterize it as an “AI breakthrough”—to
    me, that would imply a system with more broad and robust generalization abilities—but
    definitely as a promising method on an important topic, one that deserves further
    research and scrutiny.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 总之，这是关于神经网络中系统化泛化的一个非常有趣的原理性论文。我不会将其描述为“AI 的突破”-对我来说，那意味着具有更广泛和更强大的泛化能力的系统-但绝对是一个重要主题上的有前景的方法，值得进一步研究和审查。
- en: Thanks to Alessandro Palmarini, Martha Lewis, and Una-May O’Reilly for helping
    me think about this paper!
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 感谢Alessandro Palmarini，Martha Lewis和Una-May O’Reilly帮助我思考这篇论文！
- en: '**Postscript**'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '**附言**'
- en: 'On a different topic, there are a few recent articles and talks from me that
    readers of this Substack might find interesting:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 在另一个话题上，我有一些最近的文章和演讲，这些文章和演讲可能会让这个 Substack 的读者感兴趣：
- en: 'I am writing occasional non-technical columns focused on AI for *Science Magazine*. 
    My columns so far:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 我正在为 *Science Magazine* 写一些偶尔的非技术专栏，重点是 AI。到目前为止，我的专栏有：
- en: · [AI's Challenge of Understanding the World](https://www.science.org/doi/full/10.1126/science.adm8175)
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: · [AI 意识世界的挑战](https://www.science.org/doi/full/10.1126/science.adm8175)
- en: · [How Do We Know How Smart AI Systems Are?](https://www.science.org/doi/10.1126/science.adj5957)
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: · [我们如何知道智能 AI 系统有多聪明？](https://www.science.org/doi/10.1126/science.adj5957)
- en: I’m working on a new one about the meaning of “AGI.”  Stay tuned!
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 我正在写一篇关于“AGI”含义的新论文。敬请关注！
- en: In November 2023 I gave a Santa Fe Institute Public Lecture called “The Future
    of AI”— you can watch it [here](https://www.youtube.com/watch?v=GwHDAfAAKd4).
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 在2023年11月，我在圣菲研究所公开演讲了一场名为“AI 的未来”的讲座-您可以在[这里](https://www.youtube.com/watch?v=GwHDAfAAKd4)观看。
- en: My SFI collaborators and I compared humans, GPT-4 and GPT-4-Vision on our ConceptARC
    abstract reasoning benchmark.  Here’s [the paper](https://arxiv.org/abs/2311.09247v3).
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 我与 SFI 合作者在我们的 ConceptARC 抽象推理基准测试上比较了人类、GPT-4 和 GPT-4-Vision。这是[论文](https://arxiv.org/abs/2311.09247v3)。
- en: I participated in a survey of selected AI researchers on “the state and future
    of deep learning.”  Here’s [the paper](https://arxiv.org/abs/2312.09323).
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 我参加了一项关于“深度学习的现状和未来”的 AI 研究人员的调查。这是[论文](https://arxiv.org/abs/2312.09323)。
- en: Till next time!
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 下次再见！
