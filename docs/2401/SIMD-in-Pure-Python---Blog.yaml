- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-05-27 14:32:31'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: SIMD in Pure Python | Blog
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://www.da.vidbuchanan.co.uk/blog/python-swar.html](https://www.da.vidbuchanan.co.uk/blog/python-swar.html)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: SIMD in Pure Python
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '*By David Buchanan, 4^(th) January 2024*'
  prefs: []
  type: TYPE_NORMAL
- en: First of all, this article is an exercise in recreational "because I can" programming.
    If you just want to make your Python code go fast, this is perhaps not the article
    for you. And perhaps Python is not the language you want, either!
  prefs: []
  type: TYPE_NORMAL
- en: By the end, I'll explain how I implemented [Game of Life](https://en.wikipedia.org/wiki/Conway%27s_Game_of_Life)
    in pure Python (plus pysdl2 for graphics output) running in 4K resolution at 180fps,
    which represents a ~3800x speedup over a naive implementation.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/7b57f311eb4ea0c2294d0c82ecd87168.png)'
  prefs: []
  type: TYPE_IMG
- en: If you're already familiar with SIMD and vectorization, you might want to skip
    this next section.
  prefs: []
  type: TYPE_NORMAL
- en: A Brief Introduction to SIMD
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: If you want to get the most performance out of a modern CPU, you're probably
    going to be using [SIMD](https://en.wikipedia.org/wiki/Single_instruction,_multiple_data)
    instructions - Single Instruction, Multiple Data.
  prefs: []
  type: TYPE_NORMAL
- en: For example, if you have two arrays of length 4, containing 32-bit integers,
    most modern CPUs will let you add the pairwise elements together in a single machine
    instruction (assuming you've already loaded the data into registers). Hopefully
    it's obvious why this is more efficient than looping over the individual array
    elements. Intel CPUs have had this *specific* capability since [SSE2](https://en.wikipedia.org/wiki/SSE2)
    was introduced in the year 2000, but SIMD as a concept predates it by [a lot](https://en.wikipedia.org/wiki/Single_instruction,_multiple_data#History).
  prefs: []
  type: TYPE_NORMAL
- en: Newer CPUs cores have been expanding on these capabilities ever since, meaning
    SIMD instructions are more relevant than ever for maximising CPU throughput.
  prefs: []
  type: TYPE_NORMAL
- en: If you're programming in a language like C, an optimising compiler will recognise
    code that can be accelerated using SIMD instructions, and automatically emit appropriate
    machine code. Compilers can't optimise everything perfectly, though, so anyone
    who wants to squeeze out the maximum performance might end up using [Intrinsics](https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html)
    to explicitly tell the compiler which instructions to use. And if *that* still
    isn't enough, you might end up programming directly in assembly.
  prefs: []
  type: TYPE_NORMAL
- en: There are many ways to express SIMD programs, but the rest of them are out of
    scope for this article!
  prefs: []
  type: TYPE_NORMAL
- en: '"Vectorization" is the process of transforming a typical program into one that
    operates over whole arrays of data (i.e. vectors) at once (for example, using
    SIMD). The work done by the optimising compiler described above, or the human
    writing Intrinsic operations, is vectorization.'
  prefs: []
  type: TYPE_NORMAL
- en: A Brief Introduction to CPython
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[CPython](https://github.com/python/cpython) is the reference implementation
    of the Python language, written mostly in C, hence the name. Other implementations
    exist, but when people say "Python" they''re often implicitly referring to CPython.
    I''ll try to only say Python when I''m referring to the language as a whole, and
    CPython when I''m talking about a CPython implementation detail (which we''ll
    be getting into later).'
  prefs: []
  type: TYPE_NORMAL
- en: The TL;DR is that CPython compiles your code into a [bytecode](https://docs.python.org/3/glossary.html#term-bytecode)
    format, and then interprets that bytecode at run-time. I'll be referring to that
    bytecode interpreter as the ["VM"](https://docs.python.org/3/glossary.html#term-virtual-machine).
  prefs: []
  type: TYPE_NORMAL
- en: SIMD in Python
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Python does not natively have a concept of SIMD. However, libraries like [NumPy](https://numpy.org/)
    exist, allowing for relatively efficient vectorized code. NumPy lets you define
    vectors, or even n-dimensional arrays, and perform operations on them in a single
    API call.
  prefs: []
  type: TYPE_NORMAL
- en: '|  |'
  prefs: []
  type: TYPE_TB
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: Without NumPy, the above example would require a loop over array elements (or
    a list comprehension, which is fancy syntax for the same thing, more or less).
  prefs: []
  type: TYPE_NORMAL
- en: Internally, NumPy is implemented using native C extensions, which in turn use
    Intrinsics to express SIMD operations. I'm not an expert on NumPy implementation
    details, but you can peruse their SIMD code [here](https://github.com/numpy/numpy/tree/main/numpy/_core/src/common/simd).
    Note that the code has been customised for various CPU architectures.
  prefs: []
  type: TYPE_NORMAL
- en: CPython itself, being an interpreted Python implementation, is slow. But if
    you can structure your program so that all the "real work" gets done inside a
    library like NumPy, it can be surprisingly efficient overall.
  prefs: []
  type: TYPE_NORMAL
- en: NumPy is excellent and widely used for getting real work done. However, NumPy
    is not "pure" Python!
  prefs: []
  type: TYPE_NORMAL
- en: SIMD in *Pure* Python
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: By "pure," I mean using only functionality built into the [Python language](https://docs.python.org/3/reference/)
    itself, or the [Python standard library](https://docs.python.org/3/library/index.html).
  prefs: []
  type: TYPE_NORMAL
- en: This is an entirely arbitrary and self-imposed constraint, but I think it's
    a fun one to work within. It's also vaguely useful, since libraries like NumPy
    aren't available in certain environments.
  prefs: []
  type: TYPE_NORMAL
- en: 'Earlier, I said Python doesn''t natively have a concept of SIMD. This isn''t
    entirely true; otherwise the article would end here. Python supports bitwise operations
    over pairs of integers: AND (`&`), OR (`|`), XOR (`^`). If you think about these
    as operations over vectors of booleans, each bit being one bool, it is SIMD!'
  prefs: []
  type: TYPE_NORMAL
- en: Unlike many other programming languages, Python integers have unlimited precision.
    That is, they can accurately represent integers containing arbitrarily many digits—at
    least, until you run out of memory. This means we can evaluate an unlimited number
    of conceptually-parallel boolean operations with a single python operator.
  prefs: []
  type: TYPE_NORMAL
- en: 'SIMD over booleans might sound esoteric, but it''s an idea we can immediately
    put to work. A common operation in cryptography is to XOR two byte buffers together.
    An idiomatic implementation might look like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  |'
  prefs: []
  type: TYPE_TB
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: 'This takes each individual byte (as an integer between 0 and 255) in a and
    b, applies the xor operator to each, and constructs a new `bytes` object from
    the results. Arguably, we are already using the boolean-SIMD concept here; each
    of the 8 bits in a byte are getting XORed in parallel. But we can do better than
    that:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  |'
  prefs: []
  type: TYPE_TB
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: This might look a bit ridiculous, and that's because it is. We convert each
    `bytes` object into an arbitrary-precision integer, XOR those two integers together,
    and then convert the resulting integer back to `bytes`. The python `^` operator
    only gets executed once, processing all the data in one step (or more explicitly,
    one CPython VM operation). I'm going to call this approach "pseudo-SIMD". But
    with all this conversion between bytes and integers, surely it must be slower
    overall? Let's benchmark it.
  prefs: []
  type: TYPE_NORMAL
- en: Here are my results. The number is time to execute 1 million iterations. I tested
    on CPython 3.11 on an M1 Pro macbook. [Try it on your own machine!](https://gist.github.com/DavidBuchanan314/51bb8f6219ea8bb7a603e0ad19725f6d)
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Even for the trivial case of n=1, somehow our wacky pseudo-SIMD function wins,
    by about a third!
  prefs: []
  type: TYPE_NORMAL
- en: The difference becomes even more pronounced as the buffer size increases, with
    the pseudo-SIMD function being 12x faster for n=1024.
  prefs: []
  type: TYPE_NORMAL
- en: I also threw a numpy implementation into the mix. This isn't really fair on
    numpy, because converting the bytes to and from numpy arrays appears to have quite
    a high constant overhead. In more realistic numpy code, you'd end up keeping your
    data in numpy format throughout. Because of this, numpy ended up slower all the
    way until n=1024, where it became 3x faster. Evidently, those constant overheads
    become less relevant as <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mi>n</mi></mrow></math>
    grows.
  prefs: []
  type: TYPE_NORMAL
- en: But what if we eliminate the conversion overheads, allowing our functions to
    input and output data in their "native" formats, rather than bytes? For pseudo-SIMD,
    that format is integers, and for numpy it's a `np.array` object.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Pseudo-SIMD has the edge for small inputs (perhaps because it doesn't have to
    do any [FFI](https://en.wikipedia.org/wiki/Foreign_function_interface)), but for
    large buffers, numpy edges ahead. But only barely! How is our pure-python XOR
    function (which at this point is just the XOR operator itself) able to keep up
    with NumPy's optimised SIMD code?
  prefs: []
  type: TYPE_NORMAL
- en: CPython Internals
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Let's take a closer look. Here's the [inner loop](https://github.com/python/cpython/blob/v3.11.5/Objects/longobject.c#L5050-L5051)
    of the code for XORing together two arbitrary-precision integers in CPython 3.11.
  prefs: []
  type: TYPE_NORMAL
- en: '|  |'
  prefs: []
  type: TYPE_TB
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: It's a simple loop over arrays. No SIMD instructions? Well, there aren't any
    explicit ones, but what does the C compiler do to it? Let's take an *even closer*
    look.
  prefs: []
  type: TYPE_NORMAL
- en: I loaded libpython into Ghidra and had a look around. The library on my system
    didn't have full symbols, so I searched for cross-references to the exported symbol
    `_PyLong_New`. There were 82 matches, but by an extremely weird stroke of luck
    it was the first function I clicked on.
  prefs: []
  type: TYPE_NORMAL
- en: 'On my system, the (aarch64) assembly corresponding to the above loop is as
    follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: If you're not a reverse engineer, or even if you are, you're probably thinking
    "WTF is going on here?" This is a fairly typical result of compiler [auto-vectorization](https://en.wikipedia.org/wiki/Automatic_vectorization).
    Ghidra does a terrible job of converting it to meaningful pseudocode, so I'll
    provide my own version (note, this is not a 1:1 mapping, but it should convey
    the general idea)
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: The main loop operates using the `q0` and `q1` registers, which according to
    [ARM docs](https://developer.arm.com/documentation/dht0002/a/Introducing-NEON/NEON-architecture-overview/NEON-registers)
    are 128-bit wide NEON registers. As far as I can tell, NEON doesn't stand for
    anything in particular, but it's what ARM calls its SIMD features (by the way,
    their "next-gen" SIMD instruction set is called [SVE](https://developer.arm.com/documentation/102476/0100/Introducing-SVE)).
  prefs: []
  type: TYPE_NORMAL
- en: After the main loop, it xors the remaining 32-bit words, one at a time.
  prefs: []
  type: TYPE_NORMAL
- en: The key observation here is that our "pseudo-SIMD" implementation is using real
    SIMD instructions under the hood! At least, it is on my system; this may depend
    on your platform, compiler, configuration, etc.
  prefs: []
  type: TYPE_NORMAL
- en: If we limit ourselves to bitwise operators, and our numbers are big enough that
    the interpreter overhead is small (in relative terms), we can get real SIMD performance
    speedups in pure Python code. Well, kinda. If you were implementing a specific
    algorithm in assembly, you could generate much more tightly optimised SIMD routines,
    keeping data in registers where possible, avoiding unnecessary loads and stores
    from memory, etc.
  prefs: []
  type: TYPE_NORMAL
- en: Another big caveat with this approach is that it involves creating a whole new
    integer to contain the result. This wastes memory space, puts pressure on the
    memory allocator/gc, and perhaps most importantly, it wastes memory bandwidth.
    Although you can write `a ^= b` in Python to denote an in-place XOR operation,
    it still ends up internally allocating a new object to store the result.
  prefs: []
  type: TYPE_NORMAL
- en: If you're wondering why the NumPy implementation was still slightly faster,
    I believe the answer lies in the way CPython represents its integers. Each entry
    in the `ob_digit` array only represents 30 bits of the overall number. I'm guessing
    this makes handling carry propagation simpler during arithmetic operations. This
    means the in-memory representation has a ~7% overhead compared to optimal packing.
    While I haven't checked NumPy's implementation details, I imagine they pack array
    elements tightly.
  prefs: []
  type: TYPE_NORMAL
- en: Doing Useful Work
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Now that we know we can do efficient-ish bitwise SIMD operations, can we build
    something useful from that?
  prefs: []
  type: TYPE_NORMAL
- en: One use case is bitsliced cryptography. [Here's](https://github.com/DavidBuchanan314/python-bitsliced-aes)
    my implementation of bitsliced AES-128-ECB in pure Python. It's over 20x faster
    than the next fastest pure-python AES implementation I could find, and in theory
    it's more secure too, due to not having any data-dependent array indexing (but
    I still wouldn't trust it as a secure implementation; use a [proper cryptography
    library!](https://cryptography.io/en/latest/))
  prefs: []
  type: TYPE_NORMAL
- en: For a more detailed introduction to bitslicing, check out [this article](https://timtaubert.de/blog/2018/08/bitslicing-an-introduction/).
    The idea is to express your whole algorithm as a circuit of logic gates, or in
    other words, a bunch of boolean expressions. You can do this for any computation,
    but AES is particularly amenable to it. Once you have a boolean expression, you
    can use bit-parallel operations (i.e., bitwise SIMD operations) to compute multiple
    instances of your algorithm in parallel. Since AES is a block-based cipher, you
    can use this idea to compute multiple AES cipher blocks concurrently.
  prefs: []
  type: TYPE_NORMAL
- en: SWAR
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We can use Python integers for more than just parallel bitwise operations. We
    can use them for parallel additions, too!
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: 'As shown here, we can pack multiple fixed-width integers into a single Python
    integer, and add them all together at once. However, if they''re tightly packed
    and an integer overflow occurs, this causes unwanted carry propagation between
    lanes. The solution is simple: space them out a bit. In this example I use generous
    4-bit-wide padding to make things more obvious, but in principle you only need
    a single bit of padding. Finally, we use the bitwise AND operator to mask off
    any overflow bits. If we didn''t do this, the overflowing bits could accumulate
    over the course of multiple additions and start causing overflows between lanes
    again. The more padding bits you use between lanes, the more chained additions
    you can survive before masking is required.'
  prefs: []
  type: TYPE_NORMAL
- en: You can do similar things for subtraction, multiplication by a small constant,
    and bit shifts/rotations, so long as you have enough padding bits to prevent overflows
    in each scenario.
  prefs: []
  type: TYPE_NORMAL
- en: 'The general term for this concept is SWAR, which stands for [SIMD Within A
    Register](https://en.wikipedia.org/wiki/SWAR). But here, rather than using a machine
    register, we''re using an arbitrarily long Python integer. I''m calling this variant
    SWAB: SIMD Within A Bigint.'
  prefs: []
  type: TYPE_NORMAL
- en: SWAB is a useful idea in Python because it maximises the amount of work done
    per VM instruction, reducing the interpreter overhead; the CPU gets to spend the
    majority of its time in the fast native code that implements the integer operations.
  prefs: []
  type: TYPE_NORMAL
- en: Doing ~~Useful Work~~ Something Fun
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: That's enough theory; now it's time to make Game of Life go fast. First, let
    me describe the "problem" I'm trying to solve. I'll assume you're already broadly
    familiar with Game of Life, but if not, go read the [Wikipedia article.](https://en.wikipedia.org/wiki/Conway%27s_Game_of_Life)
  prefs: []
  type: TYPE_NORMAL
- en: There are some very clever algorithms for making GoL go fast, the most famous
    being [Hashlife](https://johnhw.github.io/hashlife/index.md.html), which works
    by spotting repeating patterns in both space and time. However, my favourite GoL
    pattern to simulate is ["soup"](https://conwaylife.com/wiki/Soup), i.e., a large
    random starting grid. These chaotic patterns aren't well suited to the Hashlife
    algorithm, so we need to go back to the basics.
  prefs: []
  type: TYPE_NORMAL
- en: When you simulate soup in the classic GoL ruleset, it typically dies out after
    a few thousand generations, producing a rather boring arrangement of oscillating
    ["ash"](https://conwaylife.com/wiki/Soup#Ash) (which is back to something Hashlife
    can simulate quickly). I prefer my simulations to live on in eternal chaos, and
    there's a variant of the classic ruleset that more or less guarantees this, called
    [DryLife](https://conwaylife.com/wiki/OCA:DryLife). I like DryLife because it
    still exhibits most of the familiar GoL behaviours (for example, gliders) and
    yet the soup lives on indefinitely, creating a pleasing screensaver-like animation.
  prefs: []
  type: TYPE_NORMAL
- en: 'The "obvious" implementation of the inner loop of the GoL algorithm looks something
    like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: It iterates over every cell in the grid, counts up its immediate 8 neighbours,
    and applies the rules to decide if the cell should be alive or not in the next
    iteration. In big-O terms, this is an <math xmlns="http://www.w3.org/1998/Math/MathML"
    display="inline"><mrow><mi>O</mi><mo stretchy="false">(</mo><mi>n</mi><mo stretchy="false">)</mo></mrow></math>
    algorithm, where <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mi>n</mi></mrow></math>
    is the number of cells in the grid (i.e., width <math xmlns="http://www.w3.org/1998/Math/MathML"
    display="inline"><mrow><mi>×</mi></mrow></math> height).
  prefs: []
  type: TYPE_NORMAL
- en: 'Although it isn''t made explicit in the snippet above, the state of the cells
    is being stored in a big array, accessed via the `get_cell` and `set_cell` helper
    functions. What if instead of using an array, we stored the whole state in one
    very long integer, and used SWAB arithmetic to process the whole thing at once?
    The trickiest part of this process will be counting up the neighbours, which can
    sum to up to 8 (or 9 if we also count the initial cell value). That''s a 4-bit
    value, and we can be sure it will never overflow into a 5-bit value, so we can
    store each cell as a 4-bit wide "SWAB lane". Without further ado, here''s the
    equivalent inner-loop code:'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: (I've omitted some details like wraparound handling; you can see the full code,
    along with definitions of those magic constants, [here](https://github.com/DavidBuchanan314/pyswargol/blob/main/swargol.py))
  prefs: []
  type: TYPE_NORMAL
- en: Aside from the different state representation, this achieves exactly the same
    thing as the previous snippet. The <math xmlns="http://www.w3.org/1998/Math/MathML"
    display="inline"><mrow><mi>O</mi><mo stretchy="false">(</mo><mi>n</mi><mo stretchy="false">)</mo></mrow></math>
    loop over every cell has been completely eliminated! Well, almost. There are <math
    xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mi>O</mi><mo
    stretchy="false">(</mo><mn>1</mn><mo stretchy="false">)</mo></mrow></math> CPython
    VM operations, but there is still <math xmlns="http://www.w3.org/1998/Math/MathML"
    display="inline"><mrow><mi>O</mi><mo stretchy="false">(</mo><mi>n</mi><mo stretchy="false">)</mo></mrow></math>
    work being done, hidden away inside the SIMD-accelerated native code of CPython's
    bigint arithmetic routines. This is a *huge* performance win, which I'll quantify
    later. But first, how do we turn that `state` integer into pixels on the screen?
  prefs: []
  type: TYPE_NORMAL
- en: Blitting
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'To get pixels on the screen, we need to convert the data into a more standard
    format. The very first step is easy: Python integers have a `to_bytes` method
    that serialises them into bytes (like I used in the XOR function example earlier
    in this article). What to do with those bytes next is less obvious.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In the spirit of only using "pure" Python, I came up with a ridiculous hack:
    craft a compressed gzip stream that, when decompressed, converts the weird 4-bits-per-pixel
    buffer into a more standard 8-bits-per-pixel grayscale buffer, and surrounds it
    in the necessary framing data to be a YUV4MPEG video stream. The output of the
    Python script can be piped into a gzip decompressor, and subsequently, a video
    player. That code is [here](https://gist.github.com/DavidBuchanan314/acae2aab38953759aacc114b417ed0b9),
    and probably deserves an article of its own, but I''m not going to go into it
    today.'
  prefs: []
  type: TYPE_NORMAL
- en: While this was a great hack, gzip is not an especially efficient [blitter](https://en.wikipedia.org/wiki/Blitter).
    I was able to get about 24fps at full-screen resolution on my 2021 macbook, but
    I really wanted at least 60fps, and that approach wasn't going to cut it.
  prefs: []
  type: TYPE_NORMAL
- en: The *ideal* approach would probably be to ship the 4bpp data off to the GPU
    as a texture, as-is, and write a [fragment shader](https://www.khronos.org/opengl/wiki/Fragment_Shader)
    capable of unpacking it onto the screen pixels. The only reason I didn't do this
    is because it feels silly. It feels silly because if we're doing GPU programming,
    we might as well just implement the whole GoL algorithm on the GPU. It'd be way
    faster, but it wouldn't be within the spirit of the completely arbitrary constraints
    I'd set for myself.
  prefs: []
  type: TYPE_NORMAL
- en: My compromise here was to use SDL2, via the `pysdl2` bindings. Sure, it's not
    "pure Python," but it *is* very standard and widely available. It feels "right"
    because I can use it to its full extent without completely defeating the purpose
    (like running GoL entirely on the GPU would do).
  prefs: []
  type: TYPE_NORMAL
- en: SDL2 supports a pixel format called `SDL_PIXELFORMAT_INDEX4LSB`, which is a
    4-bit palleted mode. If we set up an appropriate palette (specifying the colours
    for "dead" and "alive" cells, respectively), then we can pass our 4bpp buffer
    to SDL as-is, and it'll know how to convert it into the right format for sending
    to the GPU (in this case, `SDL_PIXELFORMAT_ARGB8888`). This process isn't terribly
    efficient, because the conversion still happens on the CPU, and the amount of
    data sent over to the GPU is much larger than necessary. Despite that, it's a
    whole lot faster than the gzip method, getting around 48fps.
  prefs: []
  type: TYPE_NORMAL
- en: Parallelization
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We still haven''t hit the 60fps target, and to get there I added some parallelization.
    I summarised my approach in a code comment:'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: That's enough to smash past the 60fps target, reaching ~250fps at my full-screen
    resolution of 3024x1890, using 8 parallel Life processes. At 4K resolution (3840x2160),
    it can reach 180fps.
  prefs: []
  type: TYPE_NORMAL
- en: There are plenty of remaining inefficiencies here that could be improved on,
    but 250fps is already *way* faster than I care about, so I'm not going to optimise
    any further. I think 60fps is the most visually interesting speed to watch at,
    anyway (which can be achieved by turning on vsync).
  prefs: []
  type: TYPE_NORMAL
- en: '**Edit:** After having some people test on non-Apple-silicon systems, many
    are struggling to hit 4K60fps. I haven''t done any profiling, but my guess is
    that the bottleneck is the CPU->GPU bandwidth, or maybe just memory bandwidth
    in general. I might revisit this in the future, perhaps implementing the buffer-unpacking
    fragment shader idea I mentioned.'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Earlier, I said that upgrading from the naive implementation to SWAB gave "huge"
    performance wins, so just how huge are they? If I go back to the naive approach,
    I get an incredible 0.4fps at 4K resolution (and that's *with* 8x parallelism),
    representing a ~450x performance difference. If I get extra mean and force it
    to run on a single thread, it's a 3800x difference relative to the fully optimised
    version. Damn!
  prefs: []
  type: TYPE_NORMAL
- en: As a reminder, both approaches are <math xmlns="http://www.w3.org/1998/Math/MathML"
    display="inline"><mrow><mi>O</mi><mo stretchy="false">(</mo><mi>n</mi><mo stretchy="false">)</mo></mrow></math>,
    but the faster version gets to spend more time within efficient native code.
  prefs: []
  type: TYPE_NORMAL
- en: If I rewrote this whole thing in C using SIMD intrinsics (combined with SWAR
    or other tricks), I predict that I'd get somewhere between 10x and 100x further
    speedup, due to more efficient use of SIMD registers and memory accesses. A GPU
    implementation could be faster still. Those speedups would be nice to have, but
    I think it's interesting how far I was able to get just by optimising the Python
    version.
  prefs: []
  type: TYPE_NORMAL
- en: The source code is available [here](https://github.com/DavidBuchanan314/pyswargol).
  prefs: []
  type: TYPE_NORMAL
