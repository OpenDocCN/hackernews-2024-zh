<!--yml

类别：未分类

日期：2024 年 05 月 27 日 14:43:01

-->

# OpenAI 悄悄地删除了禁止使用 ChatGPT 进行“军事和战争”目的的规定。

> 来源：[`theintercept.com/2024/01/12/open-ai-military-ban-chatgpt/`](https://theintercept.com/2024/01/12/open-ai-military-ban-chatgpt/)

本周，OpenAI 悄悄地从其使用政策中删除了明确禁止将其技术用于军事目的的语言，该政策旨在规定 ChatGPT 等功能强大而广受欢迎的工具如何使用。

截至 1 月 10 日，OpenAI 的“使用政策”[网页](https://web.archive.org/web/20240109122522/https:/openai.com/policies/usage-policies)包括禁止“存在高风险物理伤害的活动”，特别是“武器开发”和“军事和战争”。这一明确的禁止军事应用的规定似乎排除了国防部或其他国家军队的任何官方和极具价值的使用。而[新政策](https://openai.com/policies/usage-policies)保留了禁止“使用我们的服务伤害自己或他人”的禁令，并且给出了“开发或使用武器”作为例子，但对“军事和战争”使用的全面禁令已经消失了。

未经宣布的删除是对政策页面的重大重写的一部分，该公司称其旨在使文档“更清晰”“更易读”，并包括许多其他重大的语言和格式更改。

OpenAI 发言人尼科·菲利克斯在一封给《The Intercept》的电子邮件中说：“我们的目标是创建一套通用原则，这些原则既容易记住也易于应用，特别是我们的工具现在被全球普通用户广泛使用，他们现在也可以构建 GPTs。像‘不伤害他人’这样的原则既广泛又容易理解，并在许多情境中都有相关性。此外，我们特别举了武器和对他人造成伤害的例子。”

菲利克斯拒绝透露更模糊的“伤害”禁令是否包罗万象，写道：“任何使用我们的技术，包括军方，用于‘[开发]或[使用]武器，[伤害]他人或[摧毁]财产，或从事违反任何服务或系统安全的未授权活动’，都是被禁止的。”

在随后的一封电子邮件中，菲利克斯补充说，OpenAI 希望追求与其使命一致的“国家安全用例”，并计划与 DARPA 一起创建“网络安全工具”，并且“我们的更新政策的目标是提供清晰性并有这些讨论的能力。”

“OpenAI 充分意识到其技术和服务在军事应用中可能引发的风险和危害，”网络安全公司 Trail of Bits 的工程总监、机器学习和自主系统安全专家 Heidy Khlaaf 说道，她与 OpenAI 研究人员合著的一篇[2022 年的论文](https://arxiv.org/pdf/2207.14157.pdf)专门指出了军事应用的风险。Khlaaf 补充说，新政策似乎强调合法性而不是安全性。“这两个政策之间存在明显的区别，因为前者明确规定了不允许武器开发、军事和战争，而后者强调灵活性和遵守法律，”她说。“武器开发和与军事和战争相关的活动在各种程度上都是合法的。对人工智能安全的潜在影响是巨大的。鉴于大型语言模型（LLM）中存在的偏见和幻觉的已知实例，以及它们整体上缺乏准确性，它们在军事战争中的使用只会导致不精确和带有偏见的操作，可能会加剧伤害和平民伤亡。”

政策的实际后果尚不清楚。去年，《揭示》报道称，OpenAI 不愿透露在五角大楼和美国情报界越来越感兴趣的情况下，是否会执行自己明确的“军事和战争”禁令。

“考虑到人工智能系统在以军的平民定点中的使用，现在是做出决定，从 OpenAI 的可允许使用政策中删除‘军事和战争’这些词的值得注意的时刻，” AI Now Institute 的执行主任、前联邦贸易委员会人工智能政策分析师 Sarah Myers West 说道。“政策中的语言仍然含糊，引发了关于 OpenAI 打算如何执行的问题。”

虽然 OpenAI 今天提供的东西不可能直接用于杀人，无论是军事上还是其他方面 —— ChatGPT 无法操纵无人机或发射导弹 —— 但任何军事机构都在杀人的行业中，或至少维持杀人能力。像 ChatGPT 这样的 LLM 可以增强任何与杀人相关的任务，比如编写代码或处理[采购](https://asc.army.mil/web/news-chatgpt-in-dod-acquisitions/)订单。对 OpenAI 提供的定制 ChatGPT 驱动机器人的审查表明，美国军事人员已经[开始](https://chat.openai.com/g/g-r5Jaw1MFS-following-through-gpt) [使用](https://chat.openai.com/g/g-G0cfHcef9-army-doctrine-publication-6-22) 这项技术来加快文书工作流程。直接支持美国作战行动的国家地理空间情报局已经[公开](https://www.nga.mil/news/NGA_brings_products_closer_to_action_in_Middle_Eas.html) [推测](https://youtu.be/JHLUdsDzTvQ)[使用 ChatGPT 来辅助其人员分析员](https://www.nga.mil/news/NGA_brings_products_closer_to_action_in_Middle_Eas.html)。即使 OpenAI 的工具被部分军事力量用于并非直接暴力的目的，它们仍将帮助一个主要目的是杀伤力的机构。

在 The Intercept 的请求下审查政策变化的专家表示，OpenAI 似乎在默默削弱其反对与军事合作的立场。“我可以想象，从‘军事和战争’转向‘武器’留下了一个空间，OpenAI 可以支持操作基础设施，只要应用程序不直接涉及狭义上的武器开发，”兰开斯特大学科学与技术人类学教授 emerita 卢西·萨查曼说。“当然，我认为声称可以为战争平台做贡献，同时声称不参与武器的开发或使用，这种想法是虚伪的，它将武器从社会技术系统中移除 —— 包括命令和控制基础设施 —— 它是其一部分。”萨查曼是自 1970 年代以来的人工智能学者，也是国际机器人武器控制委员会的成员，她补充说：“新的政策文件似乎回避了军事承包和战争行动的问题，而是专注于武器。”

Suchman 和 Myers West 都指出 OpenAI 与微软的密切合作关系，后者是一家重要的国防承包商，迄今已投资 130 亿美元于 LLM 制造商，并转售该公司的软件工具。

这些变化发生在世界各地的军队都渴望利用机器学习技术获取优势的背景下；五角大楼仍在试探性地探索如何使用 ChatGPT 或其他大型语言模型，这是一种可以快速且灵活地生成复杂文本输出的软件工具。LLM 受到巨量书籍、文章和其他网络数据的训练，以便模拟人类对用户提示的回应。虽然像 ChatGPT 这样的 LLM 的输出通常非常令人信服，但它们被优化为连贯性，而不是对现实的牢固掌握，并且经常出现所谓的幻觉，使准确性和事实性成为问题。尽管如此，LLM 快速吸收文本并快速输出分析（或至少模拟分析）的能力使它们成为充满数据的国防部门的天然选择。

军方领导中有一些人对 LLM 倾向于插入明显的事实错误或其他扭曲以及使用 ChatGPT 分析机密或其他敏感数据可能带来的安全风险表示[担忧](https://www.armscontrol.org/act/2023-06/news/chatgpt-sparks-us-debate-over-military-use-ai)，但五角大楼仍然普遍热衷于采用人工智能工具。在去年十一月的一次讲话中，国防副部长凯瑟琳·希克斯[表示](https://www.defense.gov/News/Speeches/Speech/Article/3578046/remarks-by-deputy-secretary-of-defense-kathleen-h-hicks-on-the-state-of-ai-in-t/)，人工智能是“从一开始我们一直推动的综合、以战士为中心的创新方法的关键部分”，尽管她警告说，大多数当前的产品“在技术上还不够成熟，无法符合我们的道德人工智能原则”。

去年，五角大楼信任人工智能和自主主任金伯利·萨布隆在夏威夷的一次会议上[表示](https://www.nationaldefensemagazine.org/articles/2023/3/8/pentagons-top-ai-official-addresses-chatgpts-possible-benefits-risks)：“在如何利用 ChatGPT 之类的大型语言模型颠覆部门的关键功能方面，有很多好处。”

**更新：2024 年 1 月 16 日**

*本文已经更新，包括 OpenAI 关于其技术用于军事目的的声明，这是在发表后收到的。*
