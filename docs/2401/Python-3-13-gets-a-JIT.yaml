- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-05-27 14:38:27'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
- en: Python 3.13 gets a JIT
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://tonybaloney.github.io/posts/python-gets-a-jit.html](https://tonybaloney.github.io/posts/python-gets-a-jit.html)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Happy New Year everyone! In late December 2023 (Christmas Day to be precise),
    CPython core developer [Brandt Bucher](https://github.com/brandtbucher) submitted
    a [little pull-request to the Python 3.13](https://github.com/python/cpython/pull/113465)
    branch adding a JIT compiler.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
- en: This change, once accepted would be one of the biggest changes to the CPython
    Interpreter since the [Specializing Adaptive Interpreter](https://peps.python.org/pep-0659/)
    added in Python 3.11 (which was also from Brandt along with Mark Shannon).
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
- en: In this blog post, we’re going to have a look at this JIT, what it is, how it
    works and what the benefits are.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
- en: What is a JIT?
  id: totrans-9
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: JIT, or “Just in Time” is a compilation design that implies that compilation
    happens on demand when the code is run the first time. It’s a very broad term
    that could mean many things. I guess, technically the Python compiler is already
    a JIT because it compiles from Python code into Bytecode.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
- en: What people *tend* to mean when they say a JIT compiler, is a compiler that
    emits **machine code**. This is in contrast to an AOT (Ahead of Time) compiler,
    like the GNU C compiler, GCC or the Rust compiler rustc which generates the machine
    code once and distributes as a binary executable.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
- en: 'When you run Python code, it is first compiled into bytecodes. There are plenty
    of talks and videos about this process online so I don’t want to rehash this too
    much, but what is important to note about Python bytecodes is:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
- en: They mean nothing to the CPU and require a special bytecode interpreter loop
    to execute
  id: totrans-13
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: They are high level and can equate to 1000’s of machine instructions
  id: totrans-14
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: They are type agnostic
  id: totrans-15
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: They are cross-platform
  id: totrans-16
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'For a very simple Python function `f()` that defines a variable `a` and assigns
    the value `1`:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  id: totrans-18
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'It compiles to 5 bytecode instructions, which you can see by running `dis.dis`:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  id: totrans-20
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: I have a more interactive disassembler called [dissy](https://github.com/tonybaloney/dissy)
    as well if you want to try something more complicated.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
- en: For this function, Python 3.11 compiled into the instructions `LOAD_CONST`,
    `STORE_FAST`, `LOAD_CONST`, and `RETURN_VALUE`. These instructions are interpreted
    when the function is run by a massive loop written in C.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
- en: 'If you were to write a very crude Python evaluation loop in Python equivalent
    to the one in C, it would look something like this:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'If you gave this interpreter our test function, it would execute them and print
    the results:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: This loop with a big switch/if-else statement is an equivalent, albeit simplified
    version of how CPython’s interpreter loop works. CPython is written in C and compiled
    by a C compiler. For the sake of simplicity I’ll build out this example in Python.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
- en: For our interpreter, everytime you want to run the function, `func` it has to
    loop through each instruction and compare the bytecode name (called the opcode)
    with each if-statement. Both this comparison and the loop itself add an overhead
    to the execution. That overhead seems redundant if you run the function 10,000
    times and the bytecodes never change (because they are immutable). It would be
    more efficient to instead generate the code in a sequence instead of a evaluating
    this loop every time you call the function.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
- en: This is what a JIT does. There are many types of JIT compiler. Numba is a JIT.
    PyPy has a JIT. Java has lots of JITs. Pyston and Pyjion are JITs.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
- en: The JIT that is proposed for Python 3.13 is a copy-and-patch JIT.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
- en: What is a copy-and-patch JIT?
  id: totrans-31
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Never heard of a copy-and-patch JIT? Don’t worry, nor had I and nor have most
    people. It’s an idea [only proposed recently in 2021](https://dl.acm.org/doi/10.1145/3485513)
    and designed as a fast algorithm for dynamic language runtimes.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
- en: I’ll try and explain what a copy-and-patch JIT is by expanding our interpreter
    loop and rewriting it as a JIT. Before, the interpreter loop did two things, first
    it interpreted (looked at the bytecode) then it executed (ran the instruction).
    What we can do instead is to separate those tasks and have the interpreter output
    the instructions and not execute them.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
- en: 'A **copy-and-patch** JIT is the idea that you **copy** the instructions for
    each command and fill-in-the-blanks for that bytecode arguments (or **patch**).
    Here’s a rewritten example, I keep the loop very similar but each time I append
    a code string with the Python code to execute:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'This result for the original function is:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'This time, the code is **sequential** and doesn’t require the loop to execute.
    We can store the resulting string and run it as many times as we like:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: What was the point in that? Well the resulting code does the same thing, but
    it should run faster. I gave the two implementations to [rich bench](https://pypi.org/project/richbench/)
    and the copy-and-patch method runs faster *(Keep in mind though that loops in
    Python are very slow compared to C.)*
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
- en: Why a copy-and-patch JIT?
  id: totrans-41
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This technique of writing out the instructions for each bytecode and patching
    the values has upsides and downsides compared to a “full” JIT compiler. A full
    JIT compiler would normally compile high-level bytecodes like `LOAD_FAST` into
    lower level instructions in an IL (Intermediate Language). Because every CPU architecture
    has different instructions and features, it would be monumentally-complicated
    to write a compiler that converts high-level code directly to machine code and
    supports 32-bit and 64-bit CPUs, as well as Apple’s ARM architecture as well as
    all the other flavours of ARM. Instead most JIT’s compile first to an IL that
    is a generic machine-code-like instruction set. Those instructions are things
    like “PUSH A 64-bit integer”, “POP a 64-bit float”, “MULTIPLY the values on the
    stack”. The JIT can then compile IL into machine-code at runtime by emitting CPU-specific
    instructions and storing them in memory to be later executed (similar to how we
    did in our example).
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
- en: Once you have IL, you can run all sorts of fun optimizations on the code like
    [constant propagation](https://en.wikipedia.org/wiki/Constant_folding) and loop
    hoisting. You can see an example of this in [Pyjion’s live compiler UI](https://live.trypyjion.com).
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
- en: 'The big downside with a “full” JIT is that the process of compiling once into
    IL and then again into machine code is **slow**. Not only is it slow, but it is
    memory intensive. To illustrate this, data from recent research [“Python meets
    JIT compilers: A simple implementation and a comparative evaluation”](https://doi.org/10.1002/spe.3267)
    showed that Java-based JITs for Python like GraalPy, and Jython can take up to
    100 times longer to start than normal CPython and consume up to an additional
    Gigabyte of RAM to compile. There are already full JIT implementations for Python.'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
- en: Copy-and-patch was selected because the compilation from bytecodes to machine
    code is done as a set of “templates” that are then stitched together and patched
    at runtime with the correct values. This means that your average Python user isn’t
    running this complex JIT compiler architecture inside their Python runtime. Python
    writing it’s own IL and JIT would also be unreasonable since so many are available
    off-the-shelf like LLVMs and ryuJIT. But a full-JIT would require those being
    bundled with Python and all the added overheads. A copy-and-patch JIT only requires
    the LLVM JIT tools be installed on the machine where CPython is compiled from
    source, and for most people that means the machines of the CI that builds and
    packages CPython for python.org.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
- en: So how does this JIT work?
  id: totrans-46
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The copy-and-patch compiler for Python works by extending some new (and honestly
    not widely known about) APIs to Python 3.13’s API. These changes enable pluggable
    optimizers to be discoverable at runtime in CPython and control how code is executed.
    This new JIT is an optional optimizer for this new architecture. I assume that
    it will be the default in future versions once the major bugs have been squashed.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
- en: 'When you compile CPython from source, you can provide a flag `--enable-experimental-jit`
    to the configure script. This will generate machine-code templates for the Python
    bytecodes. This happens by first copying the C code for each bytecode, for example
    for LOAD_CONST, the simplest:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'The instructions for this bytecode are first compiled by the C compiler into
    a little shared library and then stored as machine code. Because there are some
    variables normally determined at runtime, like `oparg`, the C code is compiled
    with those parameters left as `0`. There is then a list of the 0 values that need
    to be filled in, called holes. For `LOAD_CONST`, there are 2 holes to be filled
    , the oparg and the next instruction:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'All of the machine code is then stored as a sequence of bytes in the file `jit_stencil.h`
    which is automatically generated by a new build stage. The disassembled code is
    stored as a comment above each bytecode template, where `JIT_OPARG` and `JIT_CONTINUE`
    are the holes to be filled:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: The new JIT compiler, will when activated copy the machine-code instructions
    for each bytecode into a sequence and replace the values for each template with
    the arguments for that bytecode in the code object. The resulting machine code
    is stored in memory and then each time the Python function is run, that machine-code
    is executed directly.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
- en: You can see the JITted code if you compile [my branch](https://github.com/brandtbucher/cpython/pull/32)
    and try it on this [test script](https://gist.github.com/tonybaloney/7e12e416ad69968e297547498f7bcde1)
    then give it to a disassembler like Ada Pro or Hopper. At the moment, the JIT
    is only used if the function contains the `JUMP_BACKWARD` opcode which is used
    in the `while` statement but that will change in the future.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
- en: Is it faster?
  id: totrans-56
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The initial benchmarks show something of [a 2-9% performance improvement](https://github.com/python/cpython/pull/113465#issuecomment-1876225775).
    You might be disappointed by this number, especially since this blog post has
    been talking about assembly and machine code and nothing is faster than that right?
    Well, remember that CPython is already written in C and that was already compiled
    to machine-code by the C compiler. In most cases, this JIT will be executing almost
    the same machine-code instructions as it was before.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
- en: '**However**, think of this JIT as being the cornerstone of a series of much
    larger optimizations. None of which are possible without it. For this change to
    be accepted, understood and maintained in an open-source project it needs to start
    simple.'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
- en: The future is bright, the future is JIT compiled
  id: totrans-59
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 未来是光明的，未来是 JIT 编译的
- en: The challenges with the existing interpreter being compiled ahead-of-time are
    that there are fewer opportunities for serious optimizations. Python 3.11’s adaptive
    interpreter was a step in the right direction, but it needs to go a lot further
    for Python to see a step-change in performance.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 现有解释器提前编译的挑战在于很少有机会进行严重优化。Python 3.11 的自适应解释器是朝着正确方向迈出的一步，但为了让 Python 见到性能上的飞跃，还需要做出更大的努力。
- en: I think that whilst the first version of this JIT isn’t going to seriously dent
    any benchmarks (yet), it opens the door to some huge optimizations and not just
    ones that benefit the toy benchmark programs in the standard benchmark suite.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 我认为，虽然这个 JIT 的第一个版本还不会严重影响任何基准测试（至少目前还没有），但它为一些巨大的优化打开了大门，不仅仅是对标准基准套件中的玩具基准程序有利的优化。
