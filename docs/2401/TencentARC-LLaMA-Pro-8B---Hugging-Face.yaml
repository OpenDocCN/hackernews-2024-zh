- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 'category: 未分类'
- en: 'date: 2024-05-27 14:33:03'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 'date: 2024-05-27 14:33:03'
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: TencentARC/LLaMA-Pro-8B · Hugging Face
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: TencentARC/LLaMA-Pro-8B · Hugging Face
- en: 来源：[https://huggingface.co/TencentARC/LLaMA-Pro-8B](https://huggingface.co/TencentARC/LLaMA-Pro-8B)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://huggingface.co/TencentARC/LLaMA-Pro-8B](https://huggingface.co/TencentARC/LLaMA-Pro-8B)
- en: '[](#llama-pro-8b-model-card)LLaMA-Pro-8B Model Card'
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '[](#llama-pro-8b-model-card)LLaMA-Pro-8B 模型卡'
- en: '[](#model-description)Model Description'
  id: totrans-7
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '[](#model-description)模型描述'
- en: LLaMA-Pro is a progressive version of the original LLaMA model, enhanced by
    the addition of Transformer blocks. It specializes in integrating both general
    language understanding and domain-specific knowledge, particularly in programming
    and mathematics.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: LLaMA-Pro 是原始 LLaMA 模型的进步版本，通过添加 Transformer 块进行增强。它专门用于整合一般语言理解和特定领域的知识，尤其是在编程和数学方面。
- en: '[](#development-and-training)Development and Training'
  id: totrans-9
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '[](#development-and-training)开发和培训'
- en: Developed by Tencent's ARC Lab, LLaMA-Pro is an 8.3 billion parameter model.
    It's an expansion of LLaMA2-7B, further trained on code and math corpora totaling
    80 billion tokens.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 由腾讯 ARC 实验室开发的 LLaMA-Pro 是一个包含 83 亿个参数的模型。它是 LLaMA2-7B 的扩展，进一步在总计 800 亿个标记的代码和数学语料库上进行了训练。
- en: '[](#intended-use)Intended Use'
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '[](#intended-use)预期用途'
- en: This model is designed for a wide range of NLP tasks, with a focus on programming,
    mathematics, and general language tasks. It suits scenarios requiring integration
    of natural and programming languages.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 该模型旨在进行广泛的 NLP 任务，重点关注编程、数学和一般语言任务。它适用于需要自然语言和编程语言集成的场景。
- en: '[](#performance)Performance'
  id: totrans-13
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '[](#performance)性能'
- en: LLaMA-Pro demonstrates advanced performance across various benchmarks. It outperforms
    existing models in the LLaMA series in handling diverse tasks, showcasing its
    capability as an intelligent language agent.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: LLaMA-Pro 在各种基准测试中展示了先进的性能。它在处理多样化任务方面优于 LLaMA 系列中的现有模型，展示了其作为智能语言代理的能力。
- en: '[](#overall-performance-on-languages-math-and-code-tasks)Overall Performance
    on Languages, math and code tasks'
  id: totrans-15
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '[](#overall-performance-on-languages-math-and-code-tasks)语言、数学和代码任务的总体性能'
- en: '| Model | ARC | Hellaswag | MMLU | TruthfulQA | Winogrande | GSM8K | GSM8K-PoT
    | HumanEval | MBPP | Avg |'
  id: totrans-16
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | ARC | Hellaswag | MMLU | TruthfulQA | Winogrande | GSM8K | GSM8K-PoT
    | 人工评估 | MBPP | 平均 |'
- en: '| :-: | :-: | :-: | :-: | :-: | :-: | :-: | :-: | :-: | :-: | :-: |'
  id: totrans-17
  prefs: []
  type: TYPE_TB
  zh: '| :-: | :-: | :-: | :-: | :-: | :-: | :-: | :-: | :-: | :-: | :-: |'
- en: '| LLAMA PRO (8B) | 54.10 | 77.94 | 47.88 | 39.04 | 73.95 | 17.89 | 25.42 |
    28.66 | 33.20 | 44.2 |'
  id: totrans-18
  prefs: []
  type: TYPE_TB
  zh: '| LLAMA PRO (8B) | 54.10 | 77.94 | 47.88 | 39.04 | 73.95 | 17.89 | 25.42 |
    28.66 | 33.20 | 44.2 |'
- en: '| LLaMA2-7B | 53.07 | 78.59 | 46.87 | 38.76 | 74.03 | 14.48 | 17.68 | 13.05
    | 20.09 | 39.62 |'
  id: totrans-19
  prefs: []
  type: TYPE_TB
  zh: '| LLaMA2-7B | 53.07 | 78.59 | 46.87 | 38.76 | 74.03 | 14.48 | 17.68 | 13.05
    | 20.09 | 39.62 |'
- en: '| CodeLLaMA-7B | 39.93 | 60.80 | 31.12 | 37.82 | 64.01 | 5.16 | 25.20 | 33.50
    | 41.40 | 37.66 |'
  id: totrans-20
  prefs: []
  type: TYPE_TB
  zh: '| CodeLLaMA-7B | 39.93 | 60.80 | 31.12 | 37.82 | 64.01 | 5.16 | 25.20 | 33.50
    | 41.40 | 37.66 |'
- en: '| LLAMA PRO-INSTRUCT | 52.30 | 76.88 | 52.57 | 48.80 | 72.53 | 43.59 | 55.61
    | 44.51 | 37.88 | 53.8 |'
  id: totrans-21
  prefs: []
  type: TYPE_TB
  zh: '| LLAMA PRO-INSTRUCT | 52.30 | 76.88 | 52.57 | 48.80 | 72.53 | 43.59 | 55.61
    | 44.51 | 37.88 | 53.8 |'
- en: '[](#performance-on-gpt4-evaluation)Performance on GPT4 Evaluation'
  id: totrans-22
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '[](#performance-on-gpt4-evaluation)在 GPT4 评估上的表现'
- en: '| Model | MT Bench |'
  id: totrans-23
  prefs: []
  type: TYPE_TB
  zh: '| Model | MT Bench |'
- en: '| :-: | :-: |'
  id: totrans-24
  prefs: []
  type: TYPE_TB
  zh: '| :-: | :-: |'
- en: '| Alpaca-13B | 4.53 |'
  id: totrans-25
  prefs: []
  type: TYPE_TB
  zh: '| 羊驼-13B | 4.53 |'
- en: '| CodeLLaMA-7B-Instruct | 5.71 |'
  id: totrans-26
  prefs: []
  type: TYPE_TB
  zh: '| CodeLLaMA-7B-Instruct | 5.71 |'
- en: '| Vicuna-7B | 6.17 |'
  id: totrans-27
  prefs: []
  type: TYPE_TB
  zh: '| 维库娜-7B | 6.17 |'
- en: '| LLaMA2-7B-Chat | 6.27 |'
  id: totrans-28
  prefs: []
  type: TYPE_TB
  zh: '| LLaMA2-7B-Chat | 6.27 |'
- en: '| LLAMA PRO-INSTRUCT | 6.32 |'
  id: totrans-29
  prefs: []
  type: TYPE_TB
  zh: '| LLAMA PRO-INSTRUCT | 6.32 |'
- en: '[](#limitations)Limitations'
  id: totrans-30
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '[](#limitations)局限性'
- en: While LLaMA-Pro addresses some limitations of previous models in the series,
    it may still encounter challenges specific to highly specialized domains or tasks.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然 LLaMA-Pro 解决了系列先前模型的一些局限性，但仍可能遇到特定于高度专业领域或任务的挑战。
- en: '[](#ethical-considerations)Ethical Considerations'
  id: totrans-32
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '[](#ethical-considerations)道德考虑'
- en: Users should be aware of potential biases in the model and use it responsibly,
    considering its impact on various applications.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 用户应该意识到模型中潜在的偏见，并负责任地使用它，考虑其对各种应用的影响。
