- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 'category: 未分类'
- en: 'date: 2024-05-27 14:39:17'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024年5月27日 14:39:17
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: 'Laurence Tratt: Automatic Syntax Error Recovery'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 'Laurence Tratt: 自动语法错误恢复'
- en: 来源：[https://tratt.net/laurie/blog/2020/automatic_syntax_error_recovery.html](https://tratt.net/laurie/blog/2020/automatic_syntax_error_recovery.html)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://tratt.net/laurie/blog/2020/automatic_syntax_error_recovery.html](https://tratt.net/laurie/blog/2020/automatic_syntax_error_recovery.html)
- en: 'Programming is the best antidote to arrogance I’ve come across — I make so
    many errors that I am continually reminded of my own fallibility. Broadly speaking,
    I think of errors as severe or minor. Severe errors are where I have fundamentally
    misunderstood something about the system I am creating. Each severe error is a
    bespoke problem, often requiring custom tooling to understand and fix it. Minor
    errors, in contrast, are repetitive and quickly fixed. However, they’re also *much*
    more numerous than severe errors: even shaving a couple of seconds off of the
    time it takes a programmer to fix a class of minor errors is worthwhile when you
    consider how often they occur.'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 编程是我碰到的傲慢的最好解药 — 我犯了这么多错误，以至于我不断地被提醒到我的自身缺陷。广义地说，我认为错误分为严重和轻微。严重的错误是我对我正在创建的系统的某些事物有根本性的误解。每个严重的错误都是一个定制问题，通常需要定制工具来理解和修复它。相比之下，轻微的错误是重复性的，很快就可以修复。但是，它们也比严重的错误*要*多得多：即使是缩短程序员修复一类轻微错误所需的时间几秒钟，考虑到它们经常发生，也是值得的。
- en: 'The most minor of minor errors, and also I suspect the most frequent, are syntax
    errors. They occur for three main reasons: mental sloppiness; inaccurate typing
    ; or an incomplete understanding of the language’s syntax. The latter case is
    generally part of a brief-ish learning phase we go through and I’m not sure what
    a good solution for it might look like. The former two cases, however, are extremely
    common. When I’ve made a small typo, what I want is the parser in my compiler
    or IDE to pinpoint the location of the syntax error accurately and then *recover*
    from it and continue as if I hadn’t made an error at all. Since compilation is
    often far from instantaneous, and I often make multiple errors (not just syntax
    errors), good quality syntax error recovery improves my programming efficiency.'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 最微小的错误之一，我怀疑也是最频繁的，就是语法错误。它们出现的主要原因有三个：粗心大意；打字不准确；或者对语言语法的理解不完整。后一种情况通常是我们经历的一个简短学习阶段的一部分，我不确定对于它会有什么好的解决方案。然而，前两种情况非常普遍。当我犯了一个小错别字时，我希望编译器或IDE中的解析器能准确地指出语法错误的位置，然后从中恢复，并像我没有犯错误一样继续。由于编译通常远非瞬间完成，而且我经常犯多个错误（不仅仅是语法错误），良好质量的语法错误恢复提高了我的编程效率。
- en: 'Unfortunately, LR parsers – [of which I am particularly fond](/laurie/blog/entries/which_parsing_approach.html)
    – have a poor reputation for syntax error recovery. I’m going to show in this
    article that this isn’t inevitable, and that it’s possible to do surprisingly
    good automatic syntax error recovery for any LR grammar. If you want to know more
    details, you might be interested in the paper [Lukas Diekmann](https://diekmann.co.uk/)
    and I recently published called [Don’t Panic! Better, Fewer, Syntax Errors for
    LR Parsers](https://soft-dev.org/pubs/html/diekmann_tratt__dont_panic/). The paper
    also has a fairly brief accompanying talk, if you find that sort of thing helpful:'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，LR解析器 – [我特别喜欢的](/laurie/blog/entries/which_parsing_approach.html) – 在语法错误恢复方面声誉不佳。我将在本文中展示，这并不是不可避免的，而且对于任何LR语法来说，都可以做出出乎意料的好的自动语法错误恢复。如果你想了解更多细节，你可能会对我和[Lukas
    Diekmann](https://diekmann.co.uk/) 最近发表的论文《别慌！更好，更少，LR解析器的语法错误》感兴趣。该论文还有一个相当简短的附带演讲，如果你觉得这样的东西有帮助的话：
- en: '[https://www.youtube.com/embed/PGRnk-bzTdU](https://www.youtube.com/embed/PGRnk-bzTdU)'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://www.youtube.com/embed/PGRnk-bzTdU](https://www.youtube.com/embed/PGRnk-bzTdU)'
- en: VIDEO
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 视频
- en: For everyone else, let’s continue. To make our lives easier, for the rest of
    this article I’m going to shorten “syntax error recovery” to “error recovery”.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 对于其他人，让我们继续。为了让我们的生活更轻松，在本文的剩余部分中，我将“语法错误恢复”缩写为“错误恢复”。
- en: Outlining the problem
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 概述问题
- en: 'Let’s see error recovery in action in a widely used compiler — `javac`:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们在一个广泛使用的编译器中看看错误恢复的效果 — `javac`：
- en: '[Click to start/stop animation]'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: '[点击开始/停止动画]'
- en: 'As a quick reminder, ‘`int x y;`’ isn’t valid Java syntax. `javac` correctly
    detects a syntax error after the ‘`x`’ token and then tries to recover from that
    error. Since ‘`int x;`’ *is* valid Java, `javac` assumes that I meant to put a
    semi-colon after ‘`x`’, *repairs* my input accordingly, and continues parsing.
    This is the good side of error recovery: my silly syntax error hasn’t stopped
    the compiler from carrying on its good work. However, the bad side of error recovery
    is immediately apparent: ’ `y;`’ isn’t valid Java, so `javac` immediately prints
    out a second, spurious, syntax error that isn’t any fault of mine.'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 作为一个快速提醒，'`int x y;`'不是有效的Java语法。`javac`在‘`x`’标记之后正确检测到语法错误，然后尝试从该错误中恢复。由于'`int
    x;`'是有效的Java语法，`javac`假定我是想在'`x`'之后放一个分号，*修复*我的输入，并继续解析。这是错误恢复的好处：我的愚蠢的语法错误没有阻止编译器继续它的工作。然而，错误恢复的坏处立即显现出来：'`y;`'不是有效的Java语法，因此`javac`立即输出第二个虚假的语法错误，这并不是我的错。
- en: Of course, I have deliberately picked an example where `javac` does a poor job
    but I regret to inform you that it didn’t take me very long to find it. Many parsers
    do such a poor job of error recovery that experienced programmers often scroll
    back to the location of the first syntax error, ignoring both its repair and any
    subsequent syntax errors. Instead of being helpful, error recovery can easily
    have the opposite effect, slowing us down as we look for the real error amongst
    a slew of spurious errors.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，我故意挑选了一个`javac`表现不佳的例子，但我遗憾地告诉你，我没花多少时间就找到了它。许多解析器在错误恢复方面做得很差，以至于经验丰富的程序员经常会滚动到第一个语法错误的位置，忽略修复和任何后续的语法错误。错误恢复不仅没有帮助，而且很容易产生相反的效果，使我们在一大堆虚假错误中寻找真正的错误时变慢下来。
- en: 'Let’s look at the modern compiler that I most often use as an exemplar of good
    error messages, `rustc`. It often does a good job in the face of syntax errors:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看看我最常用的现代编译器，也是一个良好错误消息的典范，`rustc`。它在面对语法错误时通常做得很好：
- en: '[Click to start/stop animation]'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: '[点击开始/停止动画]'
- en: 'However, even `rustc` can be tripped up when presented with simple syntax errors:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，即使是`rustc`在面对简单的语法错误时也可能会出错：
- en: '[Click to start/stop animation]'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: '[点击开始/停止动画]'
- en: 'Some language implementations don’t even bother trying to recover from syntax
    errors. For example, even if I make two easily fixable syntax errors in a file,
    CPython stops as soon as it encounters the first:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 有些语言实现甚至不费力地尝试从语法错误中恢复。例如，即使我在一个文件中制造了两个容易修复的语法错误，CPython一旦遇到第一个错误就会停止：
- en: '[Click to start/stop animation]'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: '[点击开始/停止动画]'
- en: As all this might suggest, error recovery is hard to do well, and it’s unlikely
    that it will ever fully match human intuition about how syntax errors should be
    fixed. The root of the problem is that when we hit an error while parsing, there
    are, in general, an infinite number of ways that we could take to try and get
    parsing back on track. Since exploring infinity takes a while, error recovery
    has to use heuristics of one sort or another.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 正如这一切可能暗示的那样，良好的错误恢复很难做到，并且很难完全符合人类关于如何修复语法错误的直觉。问题的根源在于，当我们在解析时遇到错误时，一般来说有无限多种方式可以尝试恢复解析。由于探索无限需要一段时间，错误恢复必须使用某种启发式规则。
- en: The more knowledge a parser has of the language it is parsing, the more refined
    that heuristic can be. Hand-written parsers have a fundamental advantage here,
    because one can add as much knowledge about the language’s semantics as one wants
    to the parser. However, extending a hand-written parser in this way is no small
    task, especially for languages with large grammars. It’s difficult to get precise
    figures, but I’ve seen more than one parser that has taken a small number of person
    years of effort, much of which is devoted to error recovery. Not many of us have
    that much time to devote to the problem.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 解析器对其解析语言的了解程度越深，其启发式规则就越精细。手写解析器在这方面有着根本的优势，因为你可以向解析器添加关于语言语义的任意多知识。然而，以这种方式扩展手写解析器并不是一件小事，特别是对于语法庞大的语言而言。很难得到准确的数据，但我见过不止一个解析器耗费了少数人年的努力，其中大部分用于错误恢复。我们中没有多少人有那么多时间来解决这个问题。
- en: 'Automatically generated parsers, in contrast, are at a clear disadvantage:
    their only knowledge of the language is that expressed via its grammar. Despite
    that, automatically generated LL parsers are often able to do a tolerable job
    of error recovery .'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 相比之下，自动生成的解析器处于明显的劣势：它们对语言的唯一了解就是通过其语法表达的。尽管如此，自动生成的LL解析器通常能够做到对错误的容忍性工作得还不错。
- en: 'Unfortunately, LR parsers have a not undeserved reputation for doing a poor
    job of error recovery. Yacc, for example, requires users to sprinkle `error` tokens
    throughout their grammar in order to have error recovery in the resulting parser:
    I think I’ve only seen one real grammar which makes use of this feature, and I
    am sceptical that it can be made to work well. *Panic mode* is a fully automatic
    approach to error recovery in LR parsing, but it works by gradually deleting the
    parsing stack, causing it to delete input before a syntax error in order to try
    and recover after it. Frankly, panic mode’s repairs are so bad that I think on
    a modern machine it’s worse than having no error recovery at all.'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，LR分析器因错误恢复而声名狼藉。例如，Yacc要求用户在他们的语法中散布`error`标记，以便在生成的解析器中进行错误恢复：我想我只看到过一个真正使用了这个功能的语法，我对其是否能够很好地工作持怀疑态度。*恐慌模式*
    是LR分析中错误恢复的一种全自动方法，但它是通过逐渐删除解析堆栈来工作的，这导致它在语法错误之前删除输入，以尝试在之后进行恢复。坦率地说，恐慌模式的修复效果非常糟糕，我认为在现代计算机上，它比根本没有错误恢复还要糟糕。
- en: The roots of a solution
  id: totrans-27
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 解决方案的根源
- en: 'At a certain point when working on [grmtools](https://github.com/softdevteam/grmtools/)
    I realised that I should think about error recovery, something which I had only
    ever encountered as a normal user. A quick word about grmtools: it’s intended
    as a collection of parsing related libraries in Rust. At the moment, the parts
    that are most useful to users are [lrpar](https://crates.io/crates/lrpar) – a
    [Yacc](http://dinosaur.compilertools.net/yacc/index.html)-compatible parser –
    and, to a lesser extent , [lrlex](https://crates.io/crates/lrlex) – a [Lex](http://dinosaur.compilertools.net/lex/index.html)-ish
    lexer. For the rest of this article, I’ll almost exclusively be talking about
    lrpar, as that’s the part concerned with error recovery.'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 在处理[grmtools](https://github.com/softdevteam/grmtools/)时，我意识到我应该考虑错误恢复，这是我之前只作为普通用户遇到过的东西。关于grmtools的简短介绍：它旨在成为Rust中与解析相关的库的集合。目前，对用户最有用的部分是[lrpar](https://crates.io/crates/lrpar)——与[Yacc](http://dinosaur.compilertools.net/yacc/index.html)兼容的解析器——以及
    [lrlex](https://crates.io/crates/lrlex)——与[Lex](http://dinosaur.compilertools.net/lex/index.html)相似的词法分析器。在本文的其余部分，我几乎只会谈论lrpar，因为那是与错误恢复相关的部分。
- en: Fortunately for me, I quickly came across [Carl Cerecke’s PhD thesis](https://ir.canterbury.ac.nz/bitstream/handle/10092/5492/cerecke_thesis.pdf)
    which opened my eyes to an entirely different way of doing error recovery. His
    thesis rewards careful reading, and has some very good ideas in it. Ultimately
    I realised that Cerecke’s thesis is a member of what these days I call the [Fischer
    *et al.*](https://minds.wisconsin.edu/bitstream/handle/1793/58168/TR363.pdf?sequence=1&isAllowed=y)
    family of error recovery algorithms for LR parsing, since they all trace their
    lineage back to that paper.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，我很快就找到了[Carl Cerecke的博士论文](https://ir.canterbury.ac.nz/bitstream/handle/10092/5492/cerecke_thesis.pdf)，它让我眼界大开，看到了一种完全不同的错误恢复方式。他的论文值得仔细阅读，并且里面有一些非常好的想法。最终，我意识到Cerecke的论文是我现在称之为[Fischer
    *等人*](https://minds.wisconsin.edu/bitstream/handle/1793/58168/TR363.pdf?sequence=1&isAllowed=y)家族的LR分析错误恢复算法的一员，因为它们都可以追溯到那篇论文。
- en: When error recovery algorithms in the Fischer *et* *al.* family encounter a
    syntax error they try to find a *repair sequence* that, when applied to the input,
    gets parsing back on track. Different algorithms have different repairs at their
    disposal and different mechanisms for creating a repair sequence. For example,
    we ended up using the approach of [Corchuelo *et al.*](https://idus.us.es/bitstream/11441/65631/1/Repairing%20syntax%20errors.pdf)
    — one of the most recent members of the Fischer *et al.* family — as our intellectual
    base.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 当Fischer *等* *人* 家族的错误恢复算法遇到语法错误时，它们会尝试找到一个*修复序列*，将其应用于输入，使解析重新回到正轨。不同的算法有不同的修复方式和不同的创建修复序列的机制。例如，我们最终采用了[Corchuelo
    *等人*](https://idus.us.es/bitstream/11441/65631/1/Repairing%20syntax%20errors.pdf)的方法——Fischer
    *等人* 家族中最新的成员之一——作为我们的智力基础。
- en: CPCT+ in use
  id: totrans-31
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: CPCT+的使用
- en: 'We took the Corchuelo *et al.* algorithm, fixed and semi-formalised it , and
    extended it to produce a new error recovery algorithm CPCT+ that is now part of
    lrpar. We can use [`nimbleparse`](https://softdevteam.github.io/grmtools/master/book/nimbleparse.html)
    — a simple command-line grammar debugging tool — to see CPCT+ in action on our
    original Java example:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 我们采用了Corchuelo *等人* 的算法，对其进行了修复和半正式化，并将其扩展为生成新的错误恢复算法CPCT+，现已成为lrpar的一部分。我们可以使用[`nimbleparse`](https://softdevteam.github.io/grmtools/master/book/nimbleparse.html)——一个简单的命令行语法调试工具——在我们的原始Java示例中查看CPCT+的运行情况：
- en: '[Click to start/stop animation]'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: '[点击开始/停止动画]'
- en: 'As with `javac`’s error recovery, CPCT+ is started when lrpar encounters a
    syntax error at the ‘`y`’ token. Unlike `javac`, CPCT+ presents 3 different *repair
    sequences* (numbered 1, 2, 3) to the user which, in order , would repair the input
    to be equivalent to: ‘`int x, y;`’, ‘`int x = y;`’, or ‘`int x;`’. Importantly,
    repair sequences can contain multiple repairs:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 与 `javac` 的错误恢复类似，当 lrpar 在 ‘`y`’ 标记处遇到语法错误时，CPCT+ 会启动。不同于 `javac`，CPCT+ 向用户呈现
    3 种不同的*修复序列*（编号为 1、2、3），按顺序，可以将输入修复为等效于：‘`int x, y;`’、‘`int x = y;`’ 或 ‘`int x;`’。重要的是，修复序列可以包含多个修复：
- en: '[Click to start/stop animation]'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: '[点击开始/停止动画]'
- en: 'Since you probably don’t want to watch the animation endlessly I’ll put the
    repair sequences that are reported here:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 由于你可能不想无休止地观看动画，我将在这里报告的修复序列放在这里：
- en: '[PRE0]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'This example shows all of the individual repair types that CPCT+ can generate:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 此示例显示了 CPCT+ 可生成的所有单个修复类型：
- en: ‘`Insert x`’ means ‘insert a token `x` at the current position’;
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ‘`插入 x`’ 意味着 ‘在当前位置插入标记 `x`’；
- en: ‘`Shift x`’ means ‘keep the token `x` at the current position unchanged and
    advance the search’;
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ‘`移动 x`’ 意味着 ‘保持当前位置的标记 `x` 不变并继续搜索’；
- en: ‘`Delete x`’ means ‘delete the token `x` at the current position’.
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ‘`删除 x`’ 意味着 ‘删除当前位置的标记 `x`’。
- en: 'A repair sequence is just that: an ordered sequence of repairs. For example,
    the first repair sequence above means that the input will be repaired to be equivalent
    to:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 一个修复序列就是这样的：一个有序的修复序列。例如，上面的第一个修复序列意味着输入将被修复为等效于：
- en: '[PRE1]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'while the second repair sequence will repair the input to be equivalent to:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 而第二个修复序列将修复输入以等效于：
- en: '[PRE2]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'As this shows, CPCT+ is doing something very different to traditional error
    recovery: it’s repairing input spanning multiple tokens in one go. This is perfectly
    complementary to repairing syntax errors at different points in a file as this
    example shows:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 正如这个示例所显示的，CPCT+ 与传统的错误恢复完全不同：它会一次修复跨多个标记的输入。这与在文件中的不同点处修复语法错误完全补充，正如这个示例所示：
- en: '[Click to start/stop animation]'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '[点击开始/停止动画]'
- en: Although CPCT+ can return multiple repair sequences, it would be impractical
    to keep all those possibilities running in parallel — I also doubt that users
    would be able to interpret the resulting errors! lrpar thus takes the first repair
    sequence returned by CPCT+, applies it to the input, and continues parsing.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管 CPCT+ 可以返回多个修复序列，但并行运行所有这些可能性是不切实际的 —— 我也怀疑用户是否能够解释出结果的错误！因此，lrpar 只采用 CPCT+
    返回的第一个修复序列，将其应用于输入，并继续解析。
- en: 'At this point you might be rather sick of Java examples. Fortunately, there’s
    nothing Java specific about CPCT+. If I feed it Lua’s Lex and Yacc files and broken
    input it’ll happily repair that too :'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 到了这一点，你可能已经对 Java 示例感到厌烦了。幸运的是，CPCT+ 并没有与 Java 有关。如果我提供 Lua 的 Lex 和 Yacc 文件以及错误的输入，它也会乐意修复：
- en: '[Click to start/stop animation]'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: '[点击开始/停止动画]'
- en: Indeed, CPCT+ will happily perform error recovery on any other language for
    which you can write a Yacc grammar.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 的确，CPCT+ 会愉快地对任何你可以写出 Yacc 语法的其他语言进行错误恢复。
- en: 'Ultimately, CPCT+ has one main novelty relative to previous members of the
    Fischer *et al.* family: it presents the *complete set of minimum* *cost repair
    sequences* to the user where other approaches non-deterministically present one
    member of that set to the users. In other words, when, for our original Java example,
    CPCT+ presented this to users:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 最终，CPCT+ 相对于 Fischer *等人* 族的先前成员有一个主要的新奇之处：它向用户呈现了*最小成本修复序列的完整集合*，而其他方法是以非确定性方式向用户呈现该集合的一个成员。换句话说，对于我们的原始
    Java 示例，当 CPCT+ 向用户呈现以下内容时：
- en: '[PRE3]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'approaches such as Corchuelo *et al.* would only have presented one repair
    sequence to the user. Since those approaches are non-deterministic, each time
    they’re run they can present a different repair sequence to the one before, which
    is rather confusing. The intuition behind “minimum cost repair sequence” is that
    we want to prioritise repair sequences which do the smallest number of alterations
    to the user’s input: insert and delete repairs increase a repair sequence’s cost,
    although shift repairs are cost-neutral.'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 例如 Corchuelo *等人* 的方法只会向用户呈现一个修复序列。由于这些方法是非确定性的，每次运行它们时，它们可以呈现与之前不同的修复序列，这相当令人困惑。所谓“最小成本修复序列”的直觉是，我们要优先考虑对用户输入进行最小数量的更改的修复序列：插入和删除修复会增加修复序列的成本，尽管移动修复是成本中性的。
- en: To my mind, in the example above, both ‘`Insert ,`’ and ‘`Insert =`’ are equally
    likely to represent what the programmer intended, and it’s helpful to be shown
    both. ‘`Delete y`’ is a bit less likely to represent what the programmer intended,
    but it’s not a ridiculous suggestion, and in other similar contexts would be the
    more likely of the 3 repair sequences presented.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 在我看来，在上面的例子中，‘`Insert ,`’和‘`Insert =`’同样有可能代表程序员的意图，并且展示这两者是有帮助的。‘`Delete y`’有点不太可能代表程序员的意图，但这并不是一个荒谬的建议，在其他类似的情境中，它可能是所呈现的三种修复序列中更可能的。
- en: Under the bonnet
  id: totrans-56
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在引擎盖下
- en: '[The paper](https://soft-dev.org/pubs/html/diekmann_tratt__dont_panic/) and/or
    [the code](https://github.com/softdevteam/grmtools/) are the places to go if you
    want to know exactly how CPCT+ works, but I’ll try and give you a very rough idea
    of how it works here.'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: '[这篇论文](https://soft-dev.org/pubs/html/diekmann_tratt__dont_panic/)和/或[这段代码](https://github.com/softdevteam/grmtools/)是你想要了解CPCT+工作原理的地方，但我会在这里尝试给你一个非常粗略的概念。'
- en: When lrpar encounters a syntax error, CPCT+ is started with the grammar’s statetable
    (the statemachine that an LR parser turns a grammar into; see e.g. [this example](https://en.wikipedia.org/wiki/LR_parser#Action_and_goto_tables)),
    the current parsing stack (telling us where we are in the statetable, and how
    we got there), and the current position in the user’s input. By definition the
    top of the parsing stack will point to an error state in the statetable. CPCT+’s
    main job is to return a parsing stack and position to lrpar that allows lrpar
    to continue parsing; producing repair sequences is a happy by-product of that.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 当lrpar遇到语法错误时，CPCT+会使用文法的状态表（LR解析器将文法转换为的状态机；参见[此示例](https://en.wikipedia.org/wiki/LR_parser#Action_and_goto_tables)），当前的解析栈（告诉我们我们在状态表中的位置以及我们是如何到达那里的），以及用户输入的当前位置。根据定义，解析栈的顶部将指向状态表中的一个错误状态。CPCT+的主要任务是返回一个解析栈和位置给lrpar，以使lrpar能够继续解析；生成修复序列是这个过程的一个愉快副产品。
- en: 'CPCT+ is thus a path-finding algorithm in disguise, and we model it as an instance
    of [Dijkstra’s algorithm](https://en.wikipedia.org/wiki/Dijkstra%27s_algorithm).
    In essence, each edge in the graph is a repair, which has a cost; we’re looking
    to find a path that leads us to success. In this case, “success” can occur in
    two ways: in rare cases where errors happen near the end of a file we might hit
    the statetable’s sole *accept* state; more commonly, we settle for shifting 3
    tokens in a row (i.e. we’ve got to a point where we can parse some of the user’s
    input without causing further errors). As this might suggest, the core search
    is fairly simple.'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，CPCT+实际上是一个伪装的路径查找算法，我们将其建模为[Dijkstra算法](https://en.wikipedia.org/wiki/Dijkstra%27s_algorithm)的一个实例。实质上，图中的每一条边都是一个修复，具有成本；我们正在寻找一条通往成功的路径。在这种情况下，“成功”可能以两种方式发生：在文件末尾附近发生错误的罕见情况下，我们可能会达到状态表的唯一的*接受*状态；更常见的是，我们会选择连续移动3个令牌（即我们已经到达了可以解析用户一部分输入而不会导致进一步错误的点）。正如这可能暗示的那样，核心搜索是相当简单的。
- en: 'Most of CPCT+’s complexity comes from the fact that we try to find all minimum
    cost paths to success and we need ways of optimising the search. There are a few
    techniques we describe in the paper to improve performance, so I’ll use what’s
    probably the most effective as an example. Our basic observation is that, when
    searching, once-distinct paths often end up reaching the same node, at which point
    we can model them as one henceforth. We therefore identify *compatible* nodes
    and *merge* them into one. The challenge is then how compatible nodes can be efficiently
    identified. We make use of an often forgotten facet of hashmaps: a node’s hash
    behaviour need only be a subset of its equality behaviour. In our case, nodes
    have three properties (parsing stack, remaining input, repair sequence): we hash
    based on two of these properties (parsing stack, remaining input) which quickly
    tells us if a node is potentially compatible; equality then checks (somewhat slowly)
    all three properties to confirm definite compatibility. This is a powerful optimisation,
    particularly on the hardest cases, improving average performance by 2x.'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: CPCT+ 的大部分复杂性来自于我们试图找到通往成功的所有最小成本路径，并且我们需要优化搜索的方法。我们在论文中描述了一些提高性能的技术，所以我将使用可能是最有效的一个作为例子。我们的基本观察是，当搜索时，一度不同的路径通常最终达到同一节点，此时我们可以将它们模拟为一个。因此，我们识别
    *兼容* 节点并将它们 *合并* 成一个。然后，挑战在于如何有效地识别兼容节点。我们利用了哈希映射经常被遗忘的一个方面：节点的哈希行为只需要是其相等行为的一个子集。在我们的情况下，节点具有三个属性（解析堆栈、剩余输入、修复序列）：我们基于其中两个属性（解析堆栈、剩余输入）进行哈希，这迅速地告诉我们节点可能是兼容的；然后相等性检查（相对较慢）所有三个属性以确认确定的兼容性。这是一个强大的优化，特别是在最难的情况下，将平均性能提高了
    2 倍。
- en: Ranking repairs
  id: totrans-61
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 修复排序
- en: CPCT+ collects the complete set of minimum cost repair sequences because I thought
    that would best match what a user would hope to see from an error recovery algorithm.
    The costs of creating the complete set of minimum cost repair sequences were clear
    early on but, to my surprise, there are additional benefits.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: CPCT+ 收集了完整的最小成本修复序列，因为我认为这样做能最好地满足用户对错误恢复算法的期望。创建完整的最小成本修复序列的成本早就清楚了，但令我惊讶的是，还有额外的好处。
- en: 'The overarching problem faced by all approaches in the Fischer *et al.* family
    is that the search space is unbounded in size. This is why shifting a mere 3 tokens
    from the user’s input is enough for us to declare a repair sequence successful:
    ideally we would like to check all of the remaining input, but that would lead
    to a combinatorial explosion on all but a handful of inputs. Put another way,
    CPCT’s core search is inherently local in nature: the repair sequences it creates
    can still cause subsequent spurious errors beyond the small part of the input
    that CPCT+ has searched.'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 所有 Fischer *等* 家族方法面临的根本问题是搜索空间的大小是无界的。这就是为什么仅从用户输入中移动 3 个标记就足以使我们宣布修复序列成功的原因：理想情况下，我们希望检查剩余的全部输入，但这会导致除了少数输入之外的所有情况都会导致组合爆炸。换句话说，CPCT
    的核心搜索本质上是局部的：它创建的修复序列仍然可能导致 CPCT+ 搜索的输入的一小部分之外的后续错误。
- en: 'The complete set of minimum cost repair sequences allow us to trivially turn
    the very-local search into a regional search, allowing us to rank repair sequences
    in a wider context. We take advantage of the fact that CPCT+’s core search typically
    only finds a small handful of repair sequences. We then temporarily apply each
    repair sequence to the input and see how far it can parse ahead without causing
    an error (up to a bound of 250 tokens). We then select the (non-strict) subset
    which has parsed furthest ahead and discard the rest. Consider this Java example:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 完整的最小成本修复序列使我们能够轻松地将非常局部的搜索转变为区域性搜索，从而使我们能够在更广泛的上下文中对修复序列进行排名。我们利用了 CPCT+ 的核心搜索通常只能找到少量修复序列的事实。然后我们暂时将每个修复序列应用到输入中，并查看它可以在不引起错误的情况下解析多远（最多
    250 个标记）。然后我们选择已解析最远的（非严格）子集并丢弃其余部分。考虑这个 Java 示例：
- en: '[PRE4]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'When run through the “full” CPCT+ algorithm, two repair sequences are reported
    to the user:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 当通过“全” CPCT+ 算法运行时，向用户报告了两个修复序列：
- en: '[PRE5]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'However if I turn off ranking, we can see that the “core” of CPCT+ in fact
    generated three repair sequences:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，如果关闭排序，我们可以看到 CPCT+ 的“核心”实际上生成了三个修复序列：
- en: '[PRE6]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: In this particular run, lrpar chose to apply the ‘`Insert =`’ repair sequence
    to the input. That caused a spurious second error just beyond the region that
    CPCT+ had searched in. However, the other two repair sequences allow the whole
    file to be parsed successfully (though there is a semantic problem with the resulting
    parse, but that’s another matter). It might not be immediately obvious, but traditional
    Fischer *et al.* algorithms wouldn’t be able to throw away the ‘`Insert =`’ repair
    sequence and keep looking for something better, because they have no point of
    comparison that would allow them to realise that it’s possible to do better. In
    other words, the unexpected advantage of the complete set of minimum cost repair
    sequences is precisely that it allows us to rank repair sequences relative to
    one another and discard the less good.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个特定的运行中，lrpar选择对输入应用‘`Insert =`’修复序列。这导致了在CPCT+搜索区域之外有一个虚假的第二个错误。然而，其他两个修复序列允许整个文件成功解析（尽管解析结果存在语义问题，但那是另一回事）。也许不是立即显而易见的，但传统的Fischer
    *et al.*算法无法丢弃‘`Insert =`’修复序列并继续寻找更好的东西，因为它们没有可以让它们意识到可以做得更好的比较点。换句话说，最小成本修复序列的完整集合的意外优势正是它允许我们对修复序列进行相对排名并丢弃不太好的修复序列。
- en: I’ve also realised over time that the ranking process (which requires about
    20 lines of code) really kills two birds with one stone. First, and most obviously,
    it reduces spurious syntax errors. Second — and it took me a while to appreciate
    this — it reduces the quantity of low-quality repair sequences we present to users,
    making it more likely that users will actually check the repair sequences that
    are presented.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 随着时间的推移，我也意识到排名过程（需要大约20行代码）确实一石二鸟。首先，最明显的是，它减少了虚假的语法错误。其次——这让我花了一段时间才意识到——它减少了我们向用户呈现的低质量修复序列的数量，使得用户更有可能实际检查呈现的修复序列。
- en: Lex errors
  id: totrans-72
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 词法错误
- en: 'Like most parsing systems, grmtools separates out lexing (splitting input up
    into tokens) from parsing (determining if/how a sequence of tokens conforms to
    a grammar). This article isn’t the place to get into the pros and cons of this,
    but one factor is relevant: as soon as the lexer encounters an error in input,
    it will terminate. That means that parsing won’t start and, since error recovery
    as we’ve defined it thus far is part of the parser, the user won’t see error recovery.
    That leads to frustrating situations such as this:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 与大多数解析系统一样，grmtools将词法分析（将输入拆分成标记）与语法分析（确定/如何一个标记序列符合语法）分开。本文不是讨论这一点的场所，但有一个相关的因素：一旦词法分析器在输入中遇到错误，它就会终止。这意味着语法分析不会开始，而且，由于我们迄今定义的错误恢复是解析器的一部分，用户将看不到错误恢复。这导致了这样令人沮丧的情况：
- en: '[Click to start/stop animation]'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: '[点击开始/停止动画]'
- en: 'Although it didn’t occur to me for a very long time, it’s trivial to convert
    lexing errors into parsing errors, and then have error recovery happen as normal.
    Even more surprisingly, this doesn’t require any support from lrpar or CPCT+.
    The user merely needs to catch input that otherwise wouldn’t lex by adding a rule
    such as the following at the end of their Lex file:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然很长一段时间我都没有想到，但将词法错误转换为语法错误，然后进行正常的错误恢复是微不足道的。更令人惊讶的是，这并不需要lrpar或CPCT+的任何支持。用户只需通过在其Lex文件末尾添加以下规则来捕获否则不会识别的输入即可：
- en: '[PRE7]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'This matches a single character (the ‘`.`’) as a new token type called ‘`ERROR`’.
    However, grmtools moans if tokens are defined in the Lexer but not used in the
    Yacc grammar so let’s shut it up by adding a dummy rule to the grammar:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 这匹配一个单个字符（‘`.`’）作为一个名为‘`ERROR`’的新标记类型。然而，grmtools如果在Lexer中定义了标记但在Yacc语法中没有使用，它会抱怨，所以让我们通过向语法中添加一个虚拟规则来关闭它：
- en: '[PRE8]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Now when we run nimbleparse, CPCT+ kicks in on our “lexing error”:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 现在当我们运行nimbleparse时，CPCT+会在我们的“词法错误”上启动：
- en: '[PRE9]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'I find this satisfying for two reasons: first, because users get a useful feature
    for precisely no additional effort on lrpar’s part; and second because lexing
    and parsing errors are now presented uniformly to the user, where before they
    were confusingly separate. It would probably be a good idea to make this a core
    feature, so that we could do things like merge consecutive error tokens, but that
    wouldn’t change the underlying technique.'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 我觉得这个很令人满意，有两个原因：第一，因为用户在lrpar方面没有任何额外的努力就得到了一个有用的功能；第二，因为词法和语法错误现在以统一的方式呈现给用户，而之前它们是令人困惑地分开的。也许把这个作为核心功能会是一个好主意，这样我们就可以做一些事情，比如合并连续的错误标记，但这不会改变底层的技术。
- en: Integrating error recovery into actions
  id: totrans-82
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 将错误恢复集成到操作中
- en: 'As far as I have been able to tell, no “advanced” error recovery algorithm
    has ever made its way into a long-lasting parsing system: I couldn’t find a single
    implementation which I could run. Indeed, a surprising number of error recovery
    papers don’t even mention a corresponding implementation, though there must surely
    have been at least a research prototype at some point.'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 就我所知，没有一个“高级”的错误恢复算法曾经进入一个持久的解析系统：我找不到一个我可以运行的单一实现。事实上，令人惊讶的是，许多错误恢复论文甚至没有提到相应的实现，尽管肯定在某个时候至少有一个研究原型。
- en: 'Whatever software did, or didn’t, exist, none of the papers I’ve read make
    any mention of how error recovery affects the use of parsers. Think about your
    favourite compiler: when it encounters a syntax error and recovers from it, it
    doesn’t just continue parsing, but also runs things like the type checker (though
    it probably refuses to generate code). Of course, the reason your favourite compiler
    is doing this is probably because it has a hand-written parser. How should we
    go about dealing with this in a Yacc-ish setting?'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 无论存在还是不存在任何软件，我阅读过的所有论文都没有提到错误恢复如何影响解析器的使用。想想你最喜欢的编译器：当它遇到语法错误并从中恢复时，它不仅仅继续解析，还运行诸如类型检查器之类的东西（尽管它可能拒绝生成代码）。当然，你最喜欢的编译器这样做的原因可能是因为它有一个手写的解析器。在
    Yacc-ish 设置中，我们应该如何处理这个问题呢？
- en: 'grmtools’ solution is surprisingly simple: action code (i.e. the code that
    is executed when a production is successfully parsed) isn’t given access to tokens
    directly, but instead to an `enum` which allows one to distinguish tokens inserted
    by error recovery from tokens in the user’s input. The reason for this is probably
    best easy seen via an example, in this case [a very simple calculator grammar](https://github.com/softdevteam/grmtools/blob/master/lrpar/examples/calc_actions/src/calc.y)
    which calculates numbers as it parses:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: grmtools 的解决方案非常简单：动作代码（即成功解析一个产生式时执行的代码）不直接访问令牌，而是通过一个`enum`，允许将错误恢复插入的令牌与用户输入中的令牌区分开来。这样做的原因可能最好通过一个例子来看，比如这个
    [非常简单的计算器语法](https://github.com/softdevteam/grmtools/blob/master/lrpar/examples/calc_actions/src/calc.y)
    ，它在解析时计算数字：
- en: '[Click to start/stop animation]'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: '[点击开始/停止动画]'
- en: 'In this case, what I chose to do when writing the calculator evaluator is to
    continue evaluating expressions with syntax errors in, unless an integer was inserted.
    The reason for that is that I simply don’t have a clue what value I should insert
    if CPCT+ generated an ` `Insert INT`’ repair: is 0 reasonable? what about -1?
    or 1? As this suggests, inserting tokens can be quite problematic: while one might
    quibble about whether evaluation should continue when CPCT+ deleted the second
    ‘`+`’ in ‘`2 + + 3`’, at least that case doesn’t require the evaluator to pluck
    an integer value out of thin air.'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，我在编写计算器评估器时选择的做法是继续评估带有语法错误的表达式，除非插入了整数。这样做的原因是我根本不知道如果 CPCT+ 生成了一个‘`Insert
    INT`’修复，我应该插入什么值：0 合理吗？-1 呢？或者是 1？正如这个例子所示，插入令牌可能会引发很多问题：虽然人们可能对当 CPCT+ 删除了‘`2
    + + 3`’中的第二个‘`+`’时是否应该继续评估有所争议，但至少该情况不需要评估器从空中捡取一个整数值。
- en: 'This is an example of what I’ve ended up thinking of as the *semantic* *consequences*
    of error recovery: changing the syntax of the user’s input often changes its semantics,
    and there is no way for grmtools to know which changes have acceptable semantic
    consequences and which don’t. For example, inserting a missing ‘`:`’ in Python
    probably has no semantic consequences, but inserting the integer 999 into a calculator
    expression will have a significant semantic consequence.'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 这是我所谓的错误恢复的*语义* *后果*：更改用户输入的语法通常会改变其语义，而 grmtools 无法知道哪些更改具有可接受的语义后果，哪些不具有。例如，在
    Python 中插入一个缺失的‘`:`’可能没有语义后果，但在计算器表达式中插入整数 999 将具有重大的语义后果。
- en: 'The good news is that lrpar gives users the flexibility to deal with the semantic
    consequences of token insertion. For example here’s the grmtools-compatible grammar
    for the calculator language:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 好消息是，lrpar 为用户提供了灵活性，以处理令牌插入的语义后果。例如，这是用于计算器语言的与 grmtools 兼容的语法：
- en: '[PRE10]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: If you’re not used to Rust, that might look a little scary, so let’s start with
    some of the non-error recovery parts. First, the calculator grammar evaluates
    mathematical expressions as parsing occurs, and it deals exclusively in unsigned
    64-bit integers. Second, unlike traditional Yacc, lrpar requires each rule to
    specify its return type. In this case, each rule has a return type `Result<u64,
    Box<dyn Error>>` which says “if successful this returns a `u64`; if unsuccessful
    it returns a pointer to a description of the error on the heap”. Put another way
    ‘`dyn Error`’ is Rust’s rough equivalent to “this thing will throw an exception”.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你不习惯 Rust，那可能看起来有点吓人，所以让我们从一些非错误恢复的部分开始。首先，计算器语法在解析时评估数学表达式，并且仅处理无符号 64 位整数。其次，与传统的
    Yacc 不同，lrpar 要求每个规则都指定其返回类型。在本例中，每个规则都有一个返回类型 `Result<u64, Box<dyn Error>>`，它表示“如果成功，则返回
    `u64`；如果不成功，则返回指向堆上错误描述的指针”。换句话说，‘`dyn Error`’ 是 Rust 中“此事将引发异常”的粗略等效物。
- en: As with traditional Yacc, the ‘`$n`’ expressions in action code reference a
    symbol’s production, where *n* starts from 1. Symbols reference either rules or
    tokens. As with most parsing systems, symbols that reference rules have the static
    type of that rule (in this example `Result<u64, Box<dyn Error>>`).
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 与传统的 Yacc 类似，动作代码中的 ‘`$n`’ 表达式引用了符号的产生式，其中 *n* 从 1 开始。符号引用的要么是规则，要么是标记。与大多数解析系统一样，引用规则的符号具有该规则的静态类型（在本例中为
    `Result<u64, Box<dyn Error>>`）。
- en: Where grmtools diverges from existing systems is that tokens always have the
    static type `Result<Lexeme, Lexeme>`. If you’re used to Rust that might look a
    bit surprising, as `Result` types nearly always contain two distinct subtypes,
    but in this case we’re saying that in the “good” case you get a `Lexeme` and in
    the “bad” case you also get a `Lexeme`. The reason for this is that the “good”
    case (if you’re familiar with Rust terminology, the ‘`Ok`’ case) represents a
    token from the user’s actual input and the “bad” case (‘`Err`’) represents an
    inserted token. Because `Result` is a common Rust type, one can then use all of
    the standard idioms that Rust programmers are familiar with.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: '`grmtools` 与现有系统不同的地方在于，标记始终具有静态类型 `Result<Lexeme, Lexeme>`。如果你习惯于 Rust，这可能看起来有点令人惊讶，因为
    `Result` 类型几乎总是包含两个不同的子类型，但在这种情况下，我们说在“好”的情况下你会得到一个 `Lexeme`，而在“坏”的情况下你也会得到一个
    `Lexeme`。这样做的原因是“好”情况（如果你熟悉 Rust 术语，则为 ‘`Ok`’ 情况）表示用户实际输入的标记，而“坏”情况（‘`Err`’）表示插入的标记。因为
    `Result` 是 Rust 的常见类型，所以可以使用 Rust 程序员熟悉的所有标准习语。'
- en: 'Let’s first look at a simplified version of the first rule in the calculator
    grammar:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们首先看一下计算器语法中第一个规则的简化版本：
- en: '[PRE11]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'The `Expr` rule has two productions. The second of those (‘`Term`’) simply
    passes through the result of evaluating another rule unchanged. The first production
    tries adding the two integers produced by other rules together, but if either
    of those rules produced a `dyn Error` then the ‘`?`’ operator percolates that
    error upwards (roughly speaking: throws the exception up the call stack).'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: '`Expr` 规则有两个产生式。其中第二个（‘`Term`’）简单地将另一个规则的评估结果原样传递。第一个产生式尝试将其他规则产生的两个整数相加，但如果其中任何一个规则产生了
    `dyn Error`，那么‘`?`’ 操作符会将该错误向上传播（粗略地说：将异常抛出到调用堆栈上）。'
- en: 'Now let’s look at a simplified (to the point of being slightly incorrect) version
    of the third rule in the grammar:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们看一下语法中第三个规则的简化（甚至有些不正确）版本：
- en: '[PRE12]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: The second production references the `INT` token type. The action code then
    contains `$1.map_err(|_| “<evaluation aborted>”)?` which, in English, says “if
    the token `$1` was inserted by error recovery, throw a `dyn Error`”. In other
    words, the calculator grammar stops evaluating expressions when it encounters
    an inserted integer. However, if the token was from the user’s input, it is converted
    to a `u64` (with `parse_int`) and evaluation continues.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 第二个产生式引用了 `INT` 标记类型。动作代码然后包含了 `$1.map_err(|_| “<evaluation aborted>”)?`，它的英文含义是“如果标记
    `$1` 是由错误恢复插入的，则抛出 `dyn Error`”。换句话说，当计算器语法遇到插入的整数时，就停止评估表达式。然而，如果标记来自用户的输入，则将其转换为
    `u64`（使用 `parse_int`）并继续评估。
- en: 'If you look back at the original grammar, you can see that this grammar has
    made the decision that *only* inserted integers have unacceptable semantic consequences:
    inserting a ‘`*`’ for example allows evaluation to continue.'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你回顾原始语法，你会发现这个语法已经做出了一个决定，那就是*只有*插入的整数具有不可接受的语义后果：例如插入‘`*`’允许继续评估。
- en: After parsing has completed, a list of parsing errors (and their repairs) is
    returned to users, so that they can decide how much further they want to continue
    computation. There’s thus no danger of lrpar repairing input and the consumer
    of the parse not being able to tell that error recovery occurred. However, you
    might wonder why lrpar only allows fine-grained control of insert repairs. Surely
    it could also allow users to make fine-grained decisions in the face of delete
    repairs? Yes, it could, but I don’t think that would be a very common desire on
    the part of users, nor can I think how one would provide a nice interface for
    them to deal with such cases. What lrpar has is thus a pragmatic compromise. It’s
    also worth noting that although the above may seem very Rust specific, I’m confident
    that other languages can find a different, natural encoding of the same idea.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 在解析完成后，会将解析错误（及其修复）的列表返回给用户，以便他们决定是否要继续计算。因此，不存在 lrpar 修复输入并且解析消费者无法判断是否发生了错误恢复的危险。但是，您可能会想知道为什么
    lrpar 只允许对插入修复进行精细控制。当然，它也可以允许用户在删除修复面前做出精细决策，但我不认为这会是用户一个很普遍的需求，也不知道如何为他们处理这种情况提供一个良好的界面。因此，lrpar
    拥有的是一种务实的妥协。值得注意的是，尽管上面的内容似乎非常与 Rust 相关，但我相信其他语言可以找到一种不同的、自然的编码方式来表达相同的思想。
- en: Is it fast enough and good enough?
  id: totrans-102
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它足够快而且足够好吗？
- en: 'At this point you might be convinced that CPCT+ is a good idea in theory, but
    are unsure if it’s usable in practise. To me, there are two important questions:
    is CPCT+ fast enough to be usable? and is CPCT+ good enough to be usable? Answering
    such questions isn’t easy: until (and, mostly, after…) Carl Cerecke’s thesis,
    I’m not aware of any error recovery approach that had a vaguely convincing evaluation.'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 此时，您可能已经被说服 CPCT+ 在理论上是一个好主意，但不确定它在实践中是否可用。对我来说，有两个重要的问题：CPCT+ 的速度足够快以便可用吗？CPCT+
    的质量足够好以便可用吗？回答这样的问题并不容易：在卡尔·塞雷克的论文出版之前（以及之后...），我不知道有任何一个错误恢复方法曾经有过一个模糊令人信服的评估。
- en: The first problem is that we need syntactically incorrect code to use for an
    evaluation but nearly all source code you can find in the wild is, unsurprisingly,
    syntactically correct. While there has been some work on artificially creating
    syntax errors in files, my experience is that programmers produce such a mind-boggling
    variety of syntax errors that it’s hard to imagine a tool accurately simulating
    them.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个问题是，我们需要语法错误的代码来用于评估，但你几乎找不到任何野外的源代码是语法错误的，这并不奇怪。尽管已经有一些工作在人工创建文件中的语法错误，但我的经验是程序员产生了如此令人费解的各种语法错误，以至于很难想象有一个工具能够准确地模拟它们。
- en: 'Unlike most previous approaches, we were fortunate that these days the BlueJ
    Java editor has an opt-in data-collection framework called [Blackbox](https://bluej.org/blackbox/)
    which records programmers (mostly beginners) as they’re editing programs. Crucially,
    this includes them attempting to compile syntactically incorrect programs. We
    thus extracted a corpus of 200,000 syntactically incorrect files which programmers
    thought were worth compiling (i.e. we didn’t take files at the point that the
    user was still typing in code). Without access to Blackbox, I don’t know what
    we’d have done: I’m not aware of any other language for which such a rich dataset
    exists.'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 不同于大多数先前的方法，我们很幸运，因为这些天 BlueJ Java 编辑器有一个可选择的数据收集框架叫做 [Blackbox](https://bluej.org/blackbox/)，它记录了程序员（主要是初学者）在编辑程序时的操作。至关重要的是，这包括了他们尝试编译语法错误程序的情况。因此，我们提取了一个由
    200,000 个程序员认为值得编译的语法错误文件的语料库（即我们没有在用户还在输入代码的时候就取得了文件）。如果没有 Blackbox 的访问权限，我不知道我们会做什么：我不知道还有其他哪种语言存在这样丰富的数据集。
- en: There are a number of ways of looking at the “fast enough” question. On our
    corpus, the mean time CPCT+ spends on error recovery per file is 0.014s. To put
    that into context, that’s 3x faster than Corchuelo *et al.*, despite the fact
    that CPCT+ calculates the complete set of minimum cost repair sequences, while
    Corchuelo *et al.* finishes as soon as it finds one minimum(ish) cost repair sequence!
    I also think that the worst case is important. For various reasons, algorithms
    like CPCT+ really need a timeout to stop them running forever, which we set to
    a fairly aggressive 0.5s maximum per file, as it seems reasonable to assume that
    even the most demanding user will tolerate error recovery taking 0.5s. CPCT+ fully
    repaired 98.4% of files within the timeout on our corpus comparison Corchuelo
    *et al.* repaired 94.5% of files. In summary, in most cases CPCT+ runs fast enough
    that you’re probably not going to notice it.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: “足够快”的问题有很多角度可以看待。在我们的语料库中，CPCT+ 每个文件在错误恢复上平均花费的时间是 0.014 秒。将其放入背景中，这比 Corchuelo
    *等人* 要快 3 倍，尽管 CPCT+ 计算了完整的最小成本修复序列集，而 Corchuelo *等人* 一旦找到一个最小（ish）成本修复序列就结束了！我还认为最坏情况很重要。由于各种原因，像
    CPCT+ 这样的算法确实需要超时来阻止它们永远运行下去，我们将其设置为每个文件最多 0.5 秒，因为假设即使是最苛刻的用户也会容忍错误恢复需要 0.5 秒。我们的语料库比较中，CPCT+
    在超时内完全修复了 98.4% 的文件，而 Corchuelo *等人* 修复了 94.5% 的文件。总之，在大多数情况下，CPCT+ 运行速度足够快，你可能都不会注意到它。
- en: A much harder question to answer is whether CPCT+ is good enough. In some sense,
    the only metric that matters is whether real programmers find the errors and repairs
    reported useful. Unfortunately, that’s an impractical criteria to evaluate in
    any sensible period of time. Error recovery papers which try to do so typically
    have fewer than 20 files in their corpus which leads to unconvincing evaluations.
    Realistically, one has to find an alternative, easier to measure, metric which
    serves as a proxy for what we really care about.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 一个更难回答的问题是 CPCT+ 是否足够好。在某种意义上，唯一重要的度量标准是真实程序员是否发现报告的错误和修复有用。不幸的是，这是一个不切实际的评估标准，无法在任何合理的时间内进行评估。试图这样做的错误恢复论文通常只有少于
    20 个文件，这导致评估不令人信服。实际上，我们必须找到一种替代的、更容易测量的度量标准，作为我们真正关心的事情的代理。
- en: 'In our case, we use the total number of syntax errors found as a metric: the
    fewer the better. Although we know our corpus has at least 200,000 errors (at
    least 1 per file), we don’t know for sure how many more than that there are. There’s
    therefore no way of absolutely measuring an error recovery algorithm using this
    metric: all we can do is make relative comparisons. To give you a baseline for
    comparison, panic mode reports 981,628 syntax errors while CPCT+ reports 435,812\.
    One way of putting this into context is that if you use panic mode then, on average,
    you end up with an additional spurious syntax error for each syntax error that
    CPCT+ reports i.e. panic mode is much worse on this metric than CPCT+. Comparing
    CPCT+ with Corchuelo *et al.* is harder, because although Corchuelo *et al.* finds
    15% fewer syntax errors in the corpus than does CPCT+, it also fails on more files
    than CPCT+. This is almost certainly explained by the fact that Corchuelo *et
    al.* is unable to finish parsing the hardest files which sometimes contain an
    astonishingly large number of syntax errors (e.g. because of repeated copy and
    paste errors).'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的情况下，我们使用发现的语法错误总数作为度量标准：数量越少越好。尽管我们知道我们的语料库至少有 200,000 个错误（每个文件至少 1 个），但我们不确定还有多少错误。因此，没有绝对的方法使用这个度量标准来绝对衡量错误恢复算法：我们只能进行相对比较。为了给您一个比较的基准，panic
    mode 报告了 981,628 个语法错误，而 CPCT+ 报告了 435,812 个。把这放入背景中，如果您使用 panic mode，那么平均每个 CPCT+
    报告的语法错误就会额外多出一个虚假的语法错误，也就是说，panic mode 在这个指标上比 CPCT+ 要糟糕得多。与 Corchuelo *等人* 比较
    CPCT+ 更难，因为尽管 Corchuelo *等人* 在语料库中发现的语法错误比 CPCT+ 少 15%，但它也在更多的文件上失败了。这几乎可以肯定地解释为
    Corchuelo *等人* 无法完成解析最难的文件，这些文件有时包含惊人数量的语法错误（例如，由于重复的复制和粘贴错误）。
- en: Ultimately a truly satisfying answer to the “is CPCT+ good enough?” question
    is impossible — we can’t even make a meaningful comparison between CPCT+ and Corchuelo
    *et al.* with our metric. What we can say, however, pretty conclusively is that
    CPCT+ is much better than panic mode, the only other equivalent algorithm that’s
    ever seen somewhat widespread use.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 最终，“CPCT+ 是否足够好？”这个问题的真正令人满意的答案是不可能的 —— 我们甚至无法用我们的指标对 CPCT+ 和 Corchuelo *等人*
    进行有意义的比较。然而，我们可以非常明确地说，CPCT+ 比 panic mode 要好得多，后者是唯一另一个曾经有些广泛使用的等价算法。
- en: Limitations and future work
  id: totrans-110
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 限制和未来工作
- en: Few things in life are perfect, and CPCT+ definitely isn’t. There’s also clear
    scope to do better, and if I had a spare year or two to devote to the problem,
    there are various things I’d look at to make error recovery even better.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 生活中很少有事情是完美的，CPCT+ 明显也不是。还有很明显的改进空间，如果我有一两年的闲暇时间来解决这个问题，我会看一些事情来使错误恢复变得更好。
- en: 'CPCT+ only tries repairing input at the point of a syntax error: however, that
    is often later than the point that a human would consider that they made an error.
    It’s unrealistic to expect CPCT+, or some variant of it, to deal with situations
    where the “cause” and “result” of an error are spread far apart. However, my experience
    is that the cause of an error is frequently just 1 or 2 tokens before the point
    identified as the actual error. It would be interesting to experiment with “rewinding”
    CPCT+ 1 or 2 tokens in the input and seeing if that’s a good trade-off. This isn’t
    trivial in the general case (mostly due to the parsing stack), but might be quite
    doable in many practical cases.'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: CPCT+ 仅在语法错误点尝试修复输入：然而，这往往比人类认为自己出错的点要晚。期望 CPCT+ 或其某个变体能够处理“原因”和“结果”相距甚远的情况是不现实的。然而，我的经验是，错误的原因往往就在被识别为实际错误的点的前面
    1 或 2 个标记处。尝试“倒退” CPCT+ 输入 1 或 2 个标记，并观察这是否是一个好的权衡是很有趣的实验。在一般情况下这并不是微不足道的（主要是由于解析堆栈），但在许多实际情况下可能是相当可行的。
- en: 'As we said earlier, CPCT+’s search is inherently local and even with repair
    ranking, it can suggest repair sequences which cause spurious errors. There are
    two promising, complementary, possibilities that I think might lessen this problem.
    The first is to make use of a little known, and beautiful approach, to dealing
    with syntax errors: [non-correcting error recovery](https://dl.acm.org/doi/10.1145/3916.4019).
    This works by discarding all of the input before the point of a syntax error and
    using a modified parser to parse the remainder: it doesn’t tell the user how to
    repair the input, but it does report the location of syntax errors. Simplifying
    a bit, algorithms such as CPCT+ over-approximate true syntax errors (i.e. they
    report (nearly) all “true” syntax errors alongside some “false” syntax errors)
    whereas non-correcting error recovery under-approximates (i.e. it misses some
    “true” syntax errors but never reports ”false” syntax errors). I think it would
    be possible to use non-correcting error recovery as a superior alternative to
    CPCT+’s current ranking system and, perhaps, even to guide the search from the
    outset. Unfortunately, I don’t think that non-correcting error recovery has currently
    been “ported” to LR parsing, but I don’t think that task is insoluble. The second
    possibility is to make use of machine learning (see e.g. [this paper](https://ieeexplore.ieee.org/document/8330219)).
    Before you get too excited, I doubt that machine learning is a silver bullet for
    error recovery, because the search space is too large and the variety of syntax
    errors that humans make quite astonishing. However, my gut feeling is that machine
    learning approaches will be good at recovering non-local errors in a way that
    algorithms like CPCT+ are not.'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们之前所说，CPCT+ 的搜索本质上是局部的，即使有修复排序，它也可能建议导致虚假错误的修复序列。我认为有两种有前途且互补的可能性可以减轻这个问题。第一种是利用一种鲜为人知且美妙的处理语法错误的方法：[不纠正错误恢复](https://dl.acm.org/doi/10.1145/3916.4019)。这通过丢弃语法错误点之前的所有输入，并使用修改后的解析器来解析剩余部分来实现：它不告诉用户如何修复输入，但会报告语法错误的位置。简化一下，像
    CPCT+ 这样的算法会过度近似真正的语法错误（即它们报告（几乎）所有“真实”语法错误以及一些“假”语法错误），而不纠正错误恢复则会欠近似（即它会漏掉一些“真实”语法错误，但永远不会报告“假”语法错误）。我认为可以将不纠正错误恢复作为
    CPCT+ 当前排名系统的一个更好的替代方案，甚至可能用于从一开始就指导搜索。不幸的是，我认为不纠正错误恢复目前尚未被“移植”到 LR 解析，但我认为这个任务并不是无解的。第二种可能性是利用机器学习（见例如[这篇论文](https://ieeexplore.ieee.org/document/8330219)）。在你激动之前，我怀疑机器学习是否是错误恢复的灵丹妙药，因为搜索空间太大，人类犯的语法错误的种类非常惊人。然而，我的直觉是，机器学习方法在恢复非局部错误方面会比像
    CPCT+ 这样的算法更好。
- en: Less obviously, some Yacc grammars lend themselves to good repairs more than
    others. Without naming any names, some grammars are surprisingly permissive, letting
    through “incorrect” syntax which a later part of the compiler (or, sometimes,
    the parser’s semantic actions) then rejects . The problem with this is that the
    search space becomes very large, causing CPCT+ to either produce large numbers
    of surprising repair sequences, or, in the worst cases, not to be able finish
    its search in the alloted time. One solution to this is to rewrite such parts
    of the grammar to more accurately specify the acceptable syntax, though this is
    much easier for me to suggest than for someone to actually carry out. Another
    solution might be to provide additional hints to CPCT+ (along the lines of lrpar’s
    [`%avoid_insert`](https://softdevteam.github.io/grmtools/master/book/errorrecovery.html#biasing-repair-sequences)
    directive) that enable it to narrow down its search.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 更不明显的是，一些 Yacc 语法比其他语法更容易进行良好的修复。不点名地说，一些语法出人意料地宽容，允许通过“不正确”的语法，后续编译器的某个部分（或者有时是解析器的语义动作）会拒绝。这样做的问题在于搜索空间变得非常大，导致
    CPCT+ 要么生成大量令人惊讶的修复序列，要么在最坏的情况下无法在分配的时间内完成搜索。解决此问题的一种方法是重写语法的这些部分，以更准确地指定可接受的语法，尽管这对我来说更容易建议，而对其他人来说实际执行起来可能更困难。另一种解决方法可能是向
    CPCT+ 提供额外的提示（类似于 lrpar 的 [`%avoid_insert`](https://softdevteam.github.io/grmtools/master/book/errorrecovery.html#biasing-repair-sequences)
    指令），使其能够缩小搜索范围。
- en: Programmers unintentionally leave hints in their input (e.g. indentation), and
    languages have major structural components (e.g. block markers such as curly brackets),
    that error recovery can take into account (see e.g. [this approach](https://repository.tudelft.nl/islandora/object/uuid%3Ac46d95b0-aae6-4dd9-aa9a-759bee35d577)).
    To take advantage of this, the grammar author would need to provide hints such
    as “what are the start / end markers of a block”. Such hints would be optional,
    but my guess is that most grammar authors would find the resulting improvements
    sufficiently worthwhile that they’d be willing to invest the necessary time to
    understand how to use them.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 程序员无意中在他们的输入中留下提示（例如缩进），语言具有重要的结构组件（例如块标记，例如花括号），错误恢复可以考虑到这些（参见例如[这种方法](https://repository.tudelft.nl/islandora/object/uuid%3Ac46d95b0-aae6-4dd9-aa9a-759bee35d577)）。为了利用这一点，语法作者需要提供提示，例如“块的开始/结束标记是什么”。这些提示是可选的，但我猜想大多数语法作者会发现由此产生的改进足够值得他们投入必要的时间去理解如何使用它们。
- en: 'Finally, some parts of grammars necessarily allow huge numbers of alternatives
    and error recovery at those points is hard work. The most obvious example of this
    are binary or logical expressions, where many different operators (e.g. ‘`+`’,
    ‘`||`’ etc.) are possible. This can explode the search space, occasionally causing
    error recovery to fail, or more often, for CPCT+ to generate an overwhelming number
    of repair sequences. My favourite example of this – and this is directly taken
    from a real example, albeit with modified variable names! – is the seemingly innocent,
    though clearly syntactically incorrect, Java expression `x = f(""a""b);`. CPCT+
    generates a comical [23,607 repair sequences for it](/laurie/blog/files/error_recovery/repairs.txt).
    What is the user supposed to do with all that information? I don’t know! One thing
    I experimented with at some points was making the combinatorial aspect explicit
    so that instead of:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，一些语法部分必然允许大量的替代方案，在这些点进行错误恢复是很辛苦的工作。最明显的例子是二元或逻辑表达式，其中许多不同的运算符（例如‘`+`’、‘`||`’等）都是可能的。这可能会导致搜索空间爆炸，偶尔导致错误恢复失败，或者更经常地，导致
    CPCT+ 生成数量庞大的修复序列。我最喜欢的例子是这个——这是直接从一个真实的例子中摘录的，尽管变量名已经修改了！——看起来无害，但显然语法不正确的 Java
    表达式 `x = f(""a""b);`。CPCT+ 为其生成了一个滑稽的 [23,607 个修复序列](/laurie/blog/files/error_recovery/repairs.txt)。用户应该如何处理所有这些信息？我不知道！我有时尝试的一件事是将组合方面明确化，以便不是：
- en: '[PRE13]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'the user would be presented with:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 用户将被呈现：
- en: '[PRE14]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: For various boring reasons, that feature was removed at some point, but writing
    this down makes me think that it should probably be reintroduced. It wouldn’t
    completely solve the “overwhelming number of repair sequences” problem, but it
    would reduce it, probably substantially.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 由于各种乏味的原因，该功能在某个时候被移除了，但写下这些让我觉得它可能应该重新引入。这并不能完全解决“修复序列数量过多”的问题，但可以减少它，可能会相当大程度上减少。
- en: Summary
  id: totrans-121
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 总结
- en: Parsing is the sort of topic that brings conversations at parties to a standstill.
    However, since every programmer relies upon parsing, the better a job we can do
    of helping them fix errors quickly, the better we make their lives. CPCT+ will
    never beat the very best hand-written error recovery algorithms, but what it does
    do is bring pretty decent error recovery to any LR grammar. I hope and expect
    that better error recovery algorithms will come along in the future, but CPCT+
    is here now, and if you use Rust, you might want to take a look at grmtools —
    I’d suggest starting with the [quickstart guide in the grmtools book](https://softdevteam.github.io/grmtools/master/book/quickstart.html).
    Hopefully Yacc parsers for other languages might port CPCT+, or something like
    it, to their implementations, because there isn’t anything very Rust specific
    about CPCT+, and it’s a fairly easy algorithm to implement (under 500 lines of
    code in its Rust incantation).
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 分析是一个会使聚会上的谈话陷入僵局的话题。然而，由于每个程序员都依赖于分析，我们能够帮助他们快速修复错误的工作越好，他们的生活就越好。CPCT+ 永远不会超越最好的手写错误恢复算法，但它确实为任何
    LR 文法带来了相当不错的错误恢复能力。我希望并期待未来会出现更好的错误恢复算法，但 CPCT+ 现在就在这里，如果你使用 Rust，你可能想看看 grmtools
    — 我建议从 [grmtools 书中的快速入门指南](https://softdevteam.github.io/grmtools/master/book/quickstart.html)
    开始。希望其他语言的 Yacc 解析器可以移植 CPCT+，或者类似的东西到它们的实现中，因为 CPCT+ 并没有任何非常 Rust 特定的东西，而且它是一个相当容易实现的算法（在
    Rust 版本中不到 500 行代码）。
- en: Finally, one of the arguments that some people, quite reasonably, use against
    LR parsing is that it has poor quality error recovery. That’s a shame because,
    in my opinion [LR parsing is a beautiful approach to parsing](/laurie/blog/entries/which_parsing_approach.html).
    I hope that CPCT+’s error recovery helps to lessen this particular obstacle to
    the use of LR parsing.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，有些人很合理地反对 LR 分析的一个论点是它的错误恢复质量较差。这很遗憾，因为在我看来，[LR 分析是一种优美的分析方法](/laurie/blog/entries/which_parsing_approach.html)。我希望
    CPCT+ 的错误恢复能够减轻对使用 LR 分析的这一特定障碍。
- en: '**Acknowledgements:** My thanks to Edd Barrett and Lukas Diekmann for comments.'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: '**致谢：** 感谢 Edd Barrett 和 Lukas Diekmann 的评论。'
- en: '[Newer](/laurie/blog/2021/the_evolution_of_a_research_paper.html)'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: '[更新的文章](/laurie/blog/2021/the_evolution_of_a_research_paper.html)'
- en: 2020-11-17 08:00
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 2020-11-17 08:00
- en: '[Older](/laurie/blog/2020/stick_or_twist.html)'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: '[更早的文章](/laurie/blog/2020/stick_or_twist.html)'
- en: 'If you’d like updates on new blog posts: follow me on'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想要获取新博客文章的更新：请关注我
- en: '[Mastodon](https://mastodon.social/@ltratt)'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: '[长毛象](https://mastodon.social/@ltratt)'
- en: or
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 或者
- en: '[Twitter](https://twitter.com/laurencetratt)'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: '[推特](https://twitter.com/laurencetratt)'
- en: ; or
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: ；或者
- en: '[subscribe to the RSS feed](../blog.rss)'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: '[订阅 RSS 源](../blog.rss)'
- en: ; or
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: ；或者
- en: '[subscribe to email updates](/laurie/newsletter/)'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: '[订阅邮件更新](/laurie/newsletter/)'
- en: ':'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: ：
- en: Footnotes
  id: totrans-137
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 脚注
- en: Comments
  id: totrans-138
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 评论
