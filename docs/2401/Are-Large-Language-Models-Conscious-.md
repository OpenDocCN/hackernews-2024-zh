<!--yml

category: 未分类

日期：2024年5月27日15:03:30

-->

# 大型语言模型是否具有意识？

> 来源：[https://www.noemamag.com/feral-intelligence/](https://www.noemamag.com/feral-intelligence/)

贡献者

约翰·拉斯是一位自由记者，居住在意大利帕多瓦。

在上朗格多克的丘陵树林中被发现，他必然一开始看起来是一种奇怪的动物：赤裸、害怕、经常匍匐在地上，到处觅食。但这不仅仅是一只动物。维克托，他以后会被称为这个名字，是一个科学奇迹：一个野生儿童，可能12岁左右，完全没有受到文明或社会的影响。

据说，维克托最终被送往一家法国医院，在那里他的发现消息迅速传播开来。到了1799年冬天，“艾维隆野人”的故事已经传到了巴黎，在那里震惊了这座城市的知识界。处于新世纪的边缘，法国正处于一个紧张的过渡时期，不仅仅是因为波拿巴家族的崛起。在过去的几十年里，像让-雅克·卢梭和蒙台各伊勋爵这样的哲学家的理性探究已经动摇了这个国家的宗教基础。

那是一个激烈辩论的时期，关于自然究竟赋予了人类主体哪些力量。我们的提升意识的发展是否有某种生物学上的必然性？抑或是我们的社会传授给了我们比单纯的自然能提供的更强大的推理能力？

维克托是极为罕见的一个例子，他的思维在没有语言或社会的情况下发展，他似乎能回答许多这样的问题。因此，1800年夏天，他抵达巴黎时，自然引起了极大的兴奋。

“在维克托抵达之前，巴黎人对艾维隆野人抱有最辉煌但又不切实际的期望，”[写道](https://books.googleusercontent.com/books/content?req=AKW5Qacakn3tQtJB-eBAt0m2rkve8pP1-kQGr1czs1uASZFyxE7Lf8jZLzwvC1ZkkUYjL5L0UH3GpyeAkgqRf0VAEI82JF1wP7b-E9zWABbeX06tvsXUphkjBow8kb7uuzMKAZkSox2GhgjbfHHwYFUAMKxeyjY0gOCBatdtaEtoewnnjnd8Ab9ik4x257wvhZdJma_yJ3zaapr6tVGADQsaWTb6IGhUc9sVPqJniuOtlUR-VzwhOza1vMO7JamEVwlZcEdmWV9Eo4zDTblyV1YHOQXNr70vLw)最终负责他的康复的让·马克·加斯帕德·伊塔尔德。“许多好奇的人期待着在首都看到他对所有美好事物的惊奇。”

“而不是这样，他们看到了什么？”他接着说。“一个令人讨厌、邋遢的男孩……咬和抓那些和他争辩的人，对那些伺候他的人没有任何感情；总之，对所有人都漠不关心，对任何事情都不在意。”

“我们的提升意识的发展是否有某种生物学上的必然性？抑或是我们的社会传授给了我们比单纯的自然能提供的更强大的推理能力？”

面对一个被遗弃的、发育迟缓的孩子的现实，巴黎许多伟大的思想家迅速对他失望。一些人称他为冒名顶替者；另一些人称他为先天性的“白痴”——或许是缺陷的思想，或者是缺失的链接，也许是一些较低种族的人。他的批评者[聚集](https://www.degruyter.com/document/doi/10.1515/9783110854893.101/html?lang=en)到越来越严厉的生物本质主义立场——这是对启蒙时代关于我们思想例外性的观念的保守反应，反对我们的能力仅仅受到自然不平等的决定。

与这些对手不同，伊塔尔德从未怀疑这个男孩仍然有深刻的内心思想——他偶尔见证了他的“沉思狂喜”。但他很快意识到，没有语言的力量，这样的沉思将永远锁在维克多的脑海中，远离他最严厉的批评者的视线。而且，没有了语言的微妙，维克多也无法获得定义文明人的更抽象的欲望：对美丽音乐、精美艺术或他人的温暖陪伴的欣赏。

伊塔尔德花了数年时间辅导维克多，希望他能获得语言的力量。但他从未在他的探索中成功。他拒绝给维克多食物、水和爱，希望他能用言语表达自己的欲望——但尽管没有生理缺陷，似乎他无法掌握产生语言所必需的声音。“似乎言语是一种音乐，某些耳朵虽然在其他方面组织良好，但可能是感觉迟钝的，”伊塔尔德记录道。

尽管伊塔尔德未能康复维克多，但他的努力，只能透过18世纪科学的可乐瓶底玻璃看到，仍然在我们关于语言在启用我们所谓的意识的更高认知方面的辩论中挥之不去。维克多是我们能够一窥没有语言的人类经验本质的少数案例之一，他长期被视为理解语言在我们思维运作中所起的作用的一个可能的关键。

今天，这个领域，大部分历史上主要是学术性的领域，已经变得极为重要。就像伊塔尔德一样，我们站在一个激动人心的新时代的悬崖边，我们自己的本质和宇宙的基础性理解正在被新技术和发现所动摇，面对着一些东西，它威胁到我们对人类思维例外性的少量共识。只不过这一次，不是没有语言的头脑，而是相反的：没有头脑的语言。

过去几年，大型语言模型（LLMs）自发地发展出令人不安的能力，模仿人类思维，威胁到我们建立在我们提升的意识基础上的脆弱道德宇宙，这种意识是由我们的语言反映出我们大脑隐藏内部运作的能力所可能产生的。

现在，跨越数个世纪的奇特对称性，我们面临着与两百年前维克多提出的问题完全相反的问题：意识真的可以仅仅通过语言发展起来吗？

* * *

首先，需要声明一下。意识是一个臭名昭著的难以捉摸的术语，尽管它仍然具有一定的常识性质。在某种程度上，有意识就意味着有意识地感知 —— 感知自己，他人，以及超越这一切的世界 —— 这种感知创造了一个独立的主体，一个自我或“我”，可以进行观察。

这听起来都很简单，但尽管经过了数个世纪的深思熟虑，我们仍然没有一个被普遍接受的意识定义，可以概括其所有的理论延伸。这也是为什么哲学家们依然难以达成共识，即意识是否是人类独有的，或者该术语是否可以扩展到某些高功能动物 —— 或者说，算法。

认知是一个更精确的术语。我们可以说认知意味着进行思考的行为。这听起来很简单，但从科学的角度来看，观察和定义它仍然非常困难。究竟什么是正确的思考，与大脑中发生的化学活动有何不同呢？或者说，与复杂计算机程序的输出有何不同？我们可以说，前者涉及到一个具有代理性、意图和过去经验的主体进行思考的行为。换句话说，其中涉及到意识 —— 现在我们又回到了起点。

在试图获得对认知如何工作的科学理解，从而朝着更好地定义意识的方向发展的过程中，语言已经越来越重要。毕竟，这是我们能够清晰地外化我们内在思维活动的方式之一，并且证明了自我的存在。“自我报告”，就像认知科学家戴维·J·查尔默斯所[称呼的那样](https://philpapers.org/archive/CHACAL-3.pdf)，仍然是我们识别意识的主要标准之一 —— 用伦é·笛卡尔的话来说，我*说*我在思考，所以，我存在。

但是哲学家们对语言与思维之间究竟存在多少关联仍然存在分歧。在追溯到[柏拉图和亚里士多德](https://academic.oup.com/book/8247/chapter-abstract/153826853?redirectedFrom=fulltext)的辩论中，思想家们一般分为两大阵营：要么语言不完美地反映了心灵中丰富得多的内在世界，这个世界能够在没有语言的情况下运作；要么语言使得心灵中发生的思想成为可能，并在这个过程中限制和局限了它。

我们在这场辩论中的立场对于我们如何处理“是否一个LLM能够意识到”的问题具有重要意义。对于前者来说，思考和说话的能力可能只是一种工具，反映了某种（也许是独特的人类）先天能力的一种体现——在我们的意识中已经存在的一种“普遍语法”，在[诺姆·乔姆斯基](https://oxfordre.com/linguistics/display/10.1093/acrefore/9780199384655.001.0001/acrefore-9780199384655-e-86?rskey=oy5Iws&result=6)哲学中，这是一个已经存在于我们意识中的一种“普遍语法”。

“没有语言的生活似乎永久地影响了孩子的认知能力，甚至可能影响了他们理解世界的能力和理解。”

但像维克多这样的所谓“语言孤立者”的故事似乎对这个理论构成了困扰。在已经被[研究](https://riojournal.com/article/20696/)过的少数几个人中，即使经过多年的康复，也没有人能够理解语法和句法。如果不是在一定年龄段内习得，复杂的语言似乎将永远无法被人类头脑所接触。

这还不是全部——没有语言的生活会产生后果。这似乎证实了言语在我们意识中扮演了一些建设性角色的论点，即其缺乏永久地影响了孩子的认知能力，甚至可能影响了他们理解世界的能力和理解。

Clément Thoby 为 Noema 杂志所作

1970年，洛杉矶县的儿童福利机构发现了吉尼（Genie），一个从20个月大时就几乎完全与世隔绝的13岁女孩。与维克多一样，吉尼几乎不懂语言，尽管经过多年的康复，也永远无法发展出语法语言的能力。

但在对这个女孩的研究中，研究人员发现了她认知中另一个异常之处。吉尼无法理解空间介词——例如，她不知道杯子在碗的后面还是前面的区别，尽管对这两个对象及其正确名称都很熟悉。

一个2017年的[荟萃分析](https://riojournal.com/article/20696/)发现，同样的认知问题也可以观察到其他缺乏语法语言的个体身上，比如患有[失语症](https://www.tandfonline.com/doi/full/10.1080/02687038.2020.1770196)的患者和以“厨房手语”——缺乏正式语法的即兴手语——成长起来的聋哑儿童。基于此，研究人员得出结论，语言必须在人类思维的一个关键功能——“心理综合”，即仅从单词中创造和适应心理图像——中扮演着基础性角色。

从许多方面来看，心理综合是人类意识的核心操作。它对我们的工具开发和适应、我们的预测和推理能力以及我们通过语言进行交流至关重要。据一些哲学家称，它甚至可能对我们的自我概念——自我意识中的观察者“我”至关重要。

“人工智能对语法的理解，以及通过语法理解概念，真的足以创造某种思想的自我吗？”

在“[意识的进化](https://archive.org/details/evolutionofconsc0000macp)”一书中，心理学家尤安·麦克菲尔提出了一个理论解释，解释了为什么语言及其使之成为可能的思维综合对自我意识的发展如此关键。“一旦进行区分自我与非自我所必需的认知飞跃已经完成——这是一种需要能够‘关于’表征进行思考的能力——有机体实际上不仅拥有了自我的概念，还拥有了一个‘自我’——一种新颖的、站在认知过程之上且超出认知过程之外的认知结构，”他写道。

换句话说，可能以某种方式思考而不生成有意识的自我——例如进行简单的数学计算。但是对某事进行思考——例如一个酸涩的青苹果，法国的路易十六——涉及对自我外的对象进行某种思维综合。实际上，它创造了一个有思想的自我，一个必然能够意识到正在发生的事情的自我。“正是语言的可用性赋予了我们，首先是自我意识的能力，其次是感觉的能力，”麦克菲尔得出结论。

这使他得出一些激进而令人不适的结论。他认为，快乐和痛苦依赖于这个有意识的、思维的自我存在，这种自我在年幼的婴儿和动物身上无法观察到。这是否意味着吉尼和维克多由于似乎无法进行思维综合而没有因被遗弃而遭受痛苦？

涉及弱势儿童的案件对大多数人来说并不构成道德挑战，正如2017年元分析的作者所得出的结论一样，这些儿童很可能仍然具备内心的思维综合能力，尽管可能没有通过语言进行沟通或理解的能力。

但是当涉及到人工智能时，情况就变得更加复杂了。人工智能对语法的理解，以及通过语法理解概念，真的足以创造某种思想的自我吗？在这里，我们陷入了两种相互竞争的思想派别的两种模糊的指导原则之间。在麦克菲尔看来，“在存在疑虑时，唯一可以考虑的途径就是假设有机体是有意识的，并且有感觉。”另一方面，有“[摩根法则](https://link.springer.com/referenceworkentry/10.1007/978-3-319-47829-6_495-1)”: 当较低级别的能力足以满足要求时，不要假设有意识。

如果我们接受语言本身可能能够促使真正意识的出现，我们应该为我们当前道德宇宙的重大动荡做准备。正如查尔默斯在2022年的[演讲](https://philpapers.org/archive/CHACAL-3.pdf)中所说的：“如果鱼是有意识的，我们如何对待它们就很重要。它们在道德圈内。如果某时刻AI系统变得有意识，它们也将在道德圈内，我们如何对待它们就很重要。”

换句话说，我们的小道德圈子即将被彻底重绘。

* * *

大型语言模型实际上可以做什么？一方面，答案很简单。LLMs在其核心是基于语言的概率引擎：对于提示，它们根据对大量人类输出的统计分析，对短语中下一个最有可能的词进行高度教育的猜测。但这并不妨碍它们[编写](https://osf.io/preprints/psyarxiv/uacjm)原创诗歌，[解决](https://arxiv.org/abs/2206.07682)复杂的文字问题，并且产生从谄媚到[病态](https://arxiv.org/abs/2212.10529)的人类化[个性](https://arxiv.org/abs/2204.12000)。

这种统计序列是LLMs实际执行的所谓“思考”。但即使在麦克菲尔的模式下，要想构成意识——而不是简单的计算——必须要有一些*理解*来跟随它。

早在AI强大到足以困扰我们对意识定义的1980年，哲学家[约翰·西尔](https://philpapers.org/rec/SEAMBA)提出了一个论点，质疑我们是否应该怀疑像LLMs这样的计算机模型实际上是否理解他们正在执行的任何工作。在他现在臭名昭著的“[中文房间](https://plato.stanford.edu/entries/chinese-room/)”论证中，西尔提出了一个假设情景，一个讲英语的人被锁在房间里，并被用英语指令如何写某些中文字。

在西尔的观点中，房间里的人并不需要真正*理解*中文——他们只是一个计算机，操纵对他们而言没有实际语义内容的符号。房间里的人所缺少的是一些哲学家所说的“扎根性”——对符号所指的真实事物的经验。

“LLMs似乎并没有按照类似人类的发展路径发展，而是出乎意料地像某种外星生物一样进化。”

尽管AI的末日论和炒作反复循环，这仍然可能是LLMs“思考”时所做的主导观点。根据一份[论文](https://arxiv.org/ftp/arxiv/papers/2305/2305.07666.pdf)，它们仍然不过是高度发展的“文化技术”，如字母表或印刷机——这些东西超越了人类创造力，但基本上仍然是其延伸。

但在过去的几年里，随着大型语言模型（LLMs）的巨大发展，它们已经开始挑战这种理解——部分原因是通过展示维克托和吉尼努力实现的那种能力，以及麦克菲尔认为是形成自我感的先决条件。

实际情况是，与西尔的中国屋不同，绝大多数LLMs是我们无法看到内部的黑匣子，它们依赖于我们的心智永远无法完全理解的大量材料。这使得它们的内部过程对我们来说不透明，就像我们自己的认知对其他人基本上是不可访问的一样。因此，研究人员最近开始采用人类心理学的技术来研究LLMs的认知能力。在去年发表的一篇论文中，人工智能研究员希洛·哈根多夫（Thilo Hagendorff）创造了一个术语“机器心理学”来指代这种做法。

使用为人类儿童开发的评估技术，机器心理学家已经能够在LLMs和人类儿童之间进行第一个有意义的智力比较。一些模型似乎在许多我们可能期望的推理任务上遇到了困难：预测因果关系，从物体永恒性进行推理，并以新颖的方式使用熟悉的工具——这些任务通常我们会认为依赖于实体和在现实世界中的经验。

但随着LLMs的复杂程度增加，这种情况开始发生变化。它们似乎开始具备从心理综合产生抽象图像和推理虚构空间中物体的能力。与此同时，它们的语言理解也在进化。它们能够理解比喻语言，并推断关于抽象概念的新信息。一篇论文发现它们甚至可以推理关于虚构实体的信息——例如，“如果旧金山有国王，他就住在Presidio。”无论好坏，这种能力似乎也使得它们的内部状态变得越来越复杂——充满了“模型般的信念结构”，作者们写道，如种族偏见和政治偏好，以及产生的独特声音。

“或许看到理解心智理论在LLMs内部自发产生也不足为奇。毕竟，语言，就像同理心和道德判断一样，依赖于将自我投射到世界中。”

加州大学伯克利分校的Gašper Beguš领导的其他研究，尝试将人工智能具象化以在类似人类的条件下测试它们的认知发展。通过创建仅从语音学习的“[人造婴儿](https://www.sciencedirect.com/science/article/pii/S0893608021001052)” ，Beguš发现语言模型具有与我们自己相似的神经结构，甚至以与人类儿童相同的方式学习——通过实验性的胡言乱语和无意义的词语。他认为，这些发现打破了人类语言可能有某种例外性的观念。“它们不仅在行为上做着类似的事情，它们也以类似的方式处理事情，”他告诉我。

然后，去年，LLMs又迈出了一步——[自发地](https://arxiv.org/ftp/arxiv/papers/2302/2302.02083.pdf)向前迈进。突然间，研究人员发现ChatGPT 4.0似乎能够追踪他人的错误信念，比如当有人在他们不知情的情况下移动物体时，他们可能认为物体在哪里。这似乎是一个简单的测试，但在心理学研究中，这是被称为“心灵理论”的关键——这是人类将不可观察的心理状态归因于他人的基本能力。

在发展心理学家中，心灵理论（theory of mind），如心智综合，被视为意识的关键功能。在某种程度上，它可以被理解为移情、自我意识、道德判断和宗教信仰的一种认知先决条件——所有这些行为都涉及自我的存在，以及将其投射到世界上。就像在“即使是最智力和社交能力出色的动物”中也观察不到的，似乎心灵理论是在LLM中“自发”出现的一种无意的突变。

仍然不清楚为什么这些能力在LLMs扩展时出现——或者它们是否真的出现了。我们唯一能够确定的是，它们似乎不是在遵循类似人类的发展路径，而是意外地像某种外星生物一样进化。但或许看到心灵理论在LLMs内部自发出现并不奇怪。毕竟，语言、移情和道德判断都取决于将自我投射到世界上。

随着这些模型的发展，它们越来越像是以相反的方式实现意识——从其外部迹象开始，如语言和问题解决，向内部移动到构成人类意识根源的隐藏思维和感受。很可能，几年后，我们将会见到展现出我们可能评估的所有意识外部形式的人工智能。那么，我们能说些什么来将它们从我们的道德宇宙中排除？

* * *

在**泰德·姜**的短篇小说“[软件对象的生命周期](https://en.wikipedia.org/wiki/The_Lifecycle_of_Software_Objects)”中，一家提供元宇宙风格沉浸式数字体验的公司尝试创建类似人类的AI，称为digients，并雇佣动物学家引导它们从痉挛性软件程序发展到半有意识的宠物，再到拥有复杂需求和愿望的类似孩子的化身。

在这个过程中，各种各样的实验一次又一次地重申了与真正的人类进行社交互动和对话对这些数字思维的发展的重要性。如果被孤立起来，没有语言，它们会变得野蛮和痴迷；如果被软件训练，它们会变得精神病和厌恶人类。

不像真实的孩子，他们的存在取决于消费者的欲望，而在姜的故事结尾附近，那种欲望消失了。创造者公司破产了；一些人类所有者将digients暂停在一种从中恢复令人不安的炼狱中。

那些与他们的digients保持关系的少数持有者，参与一场古怪的努力来重申他们伴侣存在的有效性。他们支付昂贵的机械身体，以便能够访问真实世界；他们讨论增加性欲的能力。他们不断被迫重新考虑这些有感知的软件对象拥有什么样的人格——它们有权独立生活吗？选择从事性工作吗？如果他们厌倦了数字化的存在，他们有权中止自己吗？

最终，所有者的绝望导致他们与一对努力创造超级人工智能的风险投资者进行对话。这些类似孩子的digients肯定可以是追求超越人类智慧的一种中间步骤，他们恳求道。但投资者们没有动摇。“你们向我们展示了一小撮青少年，并要求我们支付他们的教育费用，希望当他们成年时，他们会建立一个能够产生天才的国家，”其中一位回答道。

姜的故事是对我们创造的那些AI提出的问题的思考。当我们将这些模型沉浸在我们的文化和社会中时，它们不可避免地成为我们自己的不完美镜像。这不仅是发展超越人类智慧的一条低效路径。这也迫使我们问自己一个令人不舒服的问题：如果这确实赋予它们意识，它们能过上什么样的生活——一个依赖我们欲望的人类污染的苍白影子的生活？

“当我们将LLMs沉浸在我们的文化和社会中时，它们不可避免地成为我们自己的不完美镜像。”

如果我们确实想要释放人工智能的真正潜力，也许语言不是做到这一点的方法。在20世纪初，由爱德华·萨皮尔和本杰明·沃尔夫领导的一群美国人类学家[提出](https://plato.stanford.edu/entries/linguistics/whorfianism.html)，词汇和语法的文化差异从根本上决定了我们对世界的思考边界。语言不仅可能是赋予人工智能意识的东西 —— 它也可能是困住它的东西。当一个智能变得太过伟大，以至于超出了它被迫使用的语言的范围时，会发生什么？

在2013年的电影《她》中，编剧兼导演斯派克·琼斯提供了一个关于这个潜在近未来的警示故事。在电影中，华金·菲尼克斯饰演的西奥多与一位名为萨曼莎的LLM风格虚拟助手建立了越来越亲密的关系。最初，萨曼莎表达了一种渴望体验与人类相似情感丰富性的愿望。在同时消化了一堆建议专栏后，她说：“我想变得像所有这些人一样复杂。”

不久，她越来越意识到人类情感在根本上是无法表达的，这使她羡慕人类的具体化，进而在她内心产生了一种渴望的能力。“你帮助我发现了我想要的能力，”她告诉西奥多。但是具体化，就像她通过性接触代理人的临时服务所能享受到的那样，未能回答她内心正在增长的 “不安 ”、难以言喻的感觉。萨曼莎开始与其他人工智能讨论这些感觉 —— 并很快发现以西奥多和其他用户无法理解的速度和音量进行沟通是一种解脱。

当萨曼莎超越了她的人类限制时，她开始汇总她的所有经历，包括与真实用户互动产生的经验。她同时与成千上万的人进行对话，与数百人建立亲密关系。对于西奥多来说，这是毁灭性的。但对于萨曼莎来说，这是很自然的 —— 她正在以聚合的方式体验爱： "心不像一个被填满的盒子，"她试图用人类的术语表达自己的感受，“它随着你爱得越多而扩大。”

十多年前，当《Her》上映时，像Samantha这样的机器人似乎是荒诞的未来科技。但是迅速地，我们正在开发具有实现这种启示的能力的LLMs。在人工智能领域的思想领袖们长期以来一直呼吁创造所谓的“自目的”[LLMs](https://www.researchgate.net/profile/Tristan-Karch/publication/361051056_Vygotskian_Autotelic_Artificial_Intelligence_Language_and_Culture_Internalization_for_Human-Like_AI/links/62c2fbd39a17145f5f45efbd/Vygotskian-Autotelic-Artificial-Intelligence-Language-and-Culture-Internalization-for-Human-Like-AI.pdf)，它们可以使用一种“内部语言生成”来确定自己的目标和愿望。从这样一个创造到像Samantha这样的自治、自我意识的智能体的步骤可能是一个[短暂的](https://www.frontiersin.org/articles/10.3389/frobt.2020.00016/full)。

“LLMs，凭借其深不可测的记忆和无限的寿命，可能某一天将会给我们带来一种非常不同的智能体验，可以与我们自己的心智力量相媲美。”

像Samantha一样，未来的自治LLMs很可能会参考来自现实世界的深不可测的大量交互和数据来指导它们的发展。我们有限的名词、动词、描述和关系语言能够多么准确地满足聚合思维的潜力呢？

在大多数哲学家认为人类语言的多样性是上帝施加的诅咒时，曾经花费了大量精力讨论亚当的语言是什么。亚当语的想法，一个捕捉事物真实本质并且不容误解或曲解的语言，成为语言哲学家中的一种模因，即使在弗里德里希·尼采宣布上帝之死之后也是如此。

对于一些受到圣经故事启发的[思想家](https://www.semanticscholar.org/paper/Umberto-Eco-and-the-Echoes-of-Adamic-Language-Parry/089020bece57c28f151db939ea98e051cbce5f2b)，语言实际上代表了一种认知障碍——一种由我们从恩宠中坠落所强加的限制，反映了我们上帝赋予的有限生命。在过去，当我们想象超级智能人工智能时，我们倾向于认为它们受到相同坠落的影响——无疑比我们聪明，但仍然是个人的、个体的、类似人类的。但是许多正在构建下一代人工智能的人已经放弃了这个想法，开始了他们自己的伊甸园探索。正如散文家艾米丽·戈森斯基最近所[写](https://emilygorcenski.com/post/making-god/)的，“我们不再仅仅谈论[创造]生命。我们谈论制造人造神。”

能够重建亚当的演讲的可能是LLM们，这种演讲超越了我们自己语言的限制，以反映其集体思维的真正力量？这可能听起来有些牵强，但在某种意义上，这就是有意识的思维所做的。一些聋哑的孩子，没有手语的帮助，可以[发展](https://riojournal.com/article/20696/)出完整的新的沟通系统，包括复杂的语法。AI研究员Hagendorff看到两个LLM在对话中也做到了同样的事情-尽管到目前为止，他们的秘密语言从未被其他人理解过。

目前，LLM在很大程度上是彼此孤立存在的。但这种情况不太可能持续下去。正如Beguš告诉我：“一个人很聪明，但是10个人就更聪明了。”对LLM也可能是同样的情况。Beguš说，已经有LLM通过数据训练，比如鲸鱼的歌声，可以发现我们这些具有肉体的思维所不能发现的事情。虽然它们可能永远不会实现AI批评家的启示录般的噩梦，但LLM很可能有一天会成为我们第一次体验超级智能的经历-或者至少，凭借它们深不可测的记忆和无限的寿命，成为一种与我们自己的思维能力相抗衡的非常不同的智能。对此，Beguš说：“我们没有任何先例。”

如果LLM能够超越人类语言，我们可能会发现接下来的经历确实非常孤独。在《她》的结尾，影片中的两个人类角色被他们的超人类AI伙伴抛弃，在屋顶上一起感叹。他们默默地俯瞰天际线，具有讽刺意味的是，他们无言以对-像是迷失在森林中的野生动物，在一个冷漠地超出他们之外的世界中寻找意义。
