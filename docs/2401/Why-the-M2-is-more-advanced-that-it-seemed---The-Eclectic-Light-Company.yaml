- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 类别：未分类
- en: 'date: 2024-05-27 14:48:45'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-05-27 14:48:45
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: Why the M2 is more advanced that it seemed – The Eclectic Light Company
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: M2 为什么比看起来更先进 - The Eclectic Light Company
- en: 来源：[https://eclecticlight.co/2024/01/15/why-the-m2-is-more-advanced-that-it-seemed/](https://eclecticlight.co/2024/01/15/why-the-m2-is-more-advanced-that-it-seemed/)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://eclecticlight.co/2024/01/15/why-the-m2-is-more-advanced-that-it-seemed/](https://eclecticlight.co/2024/01/15/why-the-m2-is-more-advanced-that-it-seemed/)
- en: When Apple launched its M2 chip at WWDC 18 months ago, in June 2022, pretty
    well everyone saw it as evolutionary, significantly faster than its predecessor
    the M1, but offering little real change in capability. In this article, which
    follows on from last week’s about [changed support for AI](https://eclecticlight.co/2024/01/13/how-m1-macs-may-lag-behind/),
    I take a deeper dive inside Apple silicon to discover whether the M2 is more than
    we thought at the time. The clues come in the instruction set supported by CPU
    cores inside the chips.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 当苹果在 18 个月前的 WWDC 上推出了其 M2 芯片，即 2022 年 6 月，几乎每个人都认为它是渐进式的，比其前身 M1 快得多，但在功能上几乎没有真正的变化。在这篇文章中，紧随上周关于
    [AI 支持变化](https://eclecticlight.co/2024/01/13/how-m1-macs-may-lag-behind/) 的文章之后，我深入探讨了苹果硅片内部，以发现当时我们是否低估了
    M2。线索来自芯片内部 CPU 核心支持的指令集。
- en: When comparing chips, emphasis is laid on performance, particularly benchmark
    tests that are the equivalent of sprint performance of an athlete in that they
    tell you how quickly the chip can run today’s tasks. If you intend keeping your
    Mac for longer than a year, you should also be interested in how its chip will
    perform when running the tasks of the future, or for our track athlete whether
    they’re also good enough at field events to be a good decathlete, their capability.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 在比较芯片时，重点放在性能上，尤其是基准测试，这些测试相当于运动员的短跑表现，它们告诉您芯片能够多快地运行今天的任务。如果您打算将 Mac 保留超过一年，您还应该对其芯片在运行未来任务时的性能感兴趣，或者对于我们的田径运动员来说，他们是否也足够擅长田赛项目以成为优秀的十项全能运动员，他们的能力。
- en: Changes in instructions since the M1
  id: totrans-8
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 自 M1 以来的指令变化
- en: For CPUs, capability is primarily determined by the instructions they run. Today
    you may not be interested in whether your Mac’s chip can perform ray-tracing very
    quickly, but in a couple of year’s time the hardware-accelerated ray-tracing in
    the GPU of an M3 could make all the difference. While Apple adds plenty of its
    own hardware, including GPU, a neural engine, and its legendary matrix co-processor
    the AMX, the capability of CPU cores remains central to many of the tasks performed
    by its chips. Those are defined by Arm, and licensed to Apple, in its Instruction
    Set Architecture (ISA), documented in a manual currently well over 5,000 pages
    long.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 CPU 来说，功能主要由它们运行的指令决定。今天，您可能对您的 Mac 芯片是否能够快速执行光线追踪不感兴趣，但是在几年后，M3 中 GPU 中的硬件加速光线追踪可能会产生重大影响。虽然
    Apple 添加了大量自己的硬件，包括 GPU、神经引擎和其传奇的矩阵协处理器 AMX，但 CPU 核心的功能仍然是其芯片执行的许多任务的核心。这些由 Arm
    在其指令集架构（ISA）中定义，并授权给苹果，在目前超过 5,000 页的手册中有详细记录。
- en: 'Mercifully, Arm defines its ISA in versions. CPU cores in M1 chips use ARMv8.5-A,
    while those in M2 and M3 chips use ARMv8.6-A, and Arm helpfully explains their
    main differences in [this list of changes](https://community.arm.com/arm-community-blogs/b/architectures-and-processors-blog/posts/arm-architecture-developments-armv8-6-a)
    in ARMv8.6-A of 2019:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，Arm 将其 ISA 分为版本。M1 芯片中的 CPU 核心使用 ARMv8.5-A，而 M2 和 M3 芯片中的 CPU 核心使用 ARMv8.6-A，Arm
    友好地解释了它们在 [此变更列表](https://community.arm.com/arm-community-blogs/b/architectures-and-processors-blog/posts/arm-architecture-developments-armv8-6-a)
    中的主要区别 2019 年 ARMv8.6-A：
- en: General Matrix Multiply (AI and others)
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通用矩阵乘法（AI 和其他）
- en: bfloat16 data type and arithmetic instructions (AI and others)
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: bfloat16 数据类型和算术指令（AI 和其他）
- en: Finer-grained traps for virtualisation (virtualisation)
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用于虚拟化的更细粒度陷阱（虚拟化）
- en: Wait-for-event traps for virtualisation (virtualisation)
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用于虚拟化的等待事件陷阱（虚拟化）
- en: High precision time (1 GHz, general)
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 高精度时间（1 GHz，通用）
- en: Extended Pointer Authentication (security).
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 扩展指针验证（安全）。
- en: Of those, support for bfloat16 and General Matrix Multiply are likely to have
    the most impact on the user.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 在这些中，对 bfloat16 和 General Matrix Multiply 的支持可能对用户产生最大影响。
- en: Although Macs based on the M1 chip weren’t released until November 2020, a year
    after the introduction of ARMv8.6-A, the lead time in design and development is
    such that there would have been no time to incorporate changes from 2019, the
    year after bfloat16 first appeared.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管基于 M1 芯片的 Mac 在 ARMv8.6-A 推出一年后直到 2020 年 11 月才发布，但设计和开发的前导时间使得从 2019 年以来没有时间来整合
    bfloat16 首次出现后的变化。
- en: bfloat16
  id: totrans-19
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '**bfloat16**'
- en: 'As I [explained before](https://eclecticlight.co/2024/01/13/how-m1-macs-may-lag-behind/),
    we’re dealing here with three different floating-point number formats, each expressed
    using a *sign* (+ or -), a *fraction* whose length determines its precision, and
    an *exponent* that determines the overall range of numbers that can be represented
    in that format. Before the introduction of bfloat16, the choice for AI and some
    other computation came down to two formats:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我之前 [解释过的](https://eclecticlight.co/2024/01/13/how-m1-macs-may-lag-behind/)，我们在这里处理的是三种不同的浮点数格式，每种格式都使用一个
    *符号*（+ 或 -）、一个 *长度决定其精度的分数* 和一个 *决定其能表示的整体范围的指数*。在 bfloat16 引入之前，AI 和其他一些计算的选择仅限于两种格式：
- en: '*float32* (single-precision), with a range of about +/- 1.2 x 10^-38 to 3.4
    x 10^38, occupying 32 bits'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*float32*（单精度），范围约为 +/- 1.2 x 10^-38 到 3.4 x 10^38，占用 32 位。'
- en: '*float16* (half-precision), with a range of about +/- 6.1 x 10^-5 to 65,504,
    occupying 16 bits.'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*float16*（半精度），范围约为 +/- 6.1 x 10^-5 到 65,504，占用 16 位。'
- en: float32 has been almost universally used in AI and other applications where
    double-precision (float64) isn’t required.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 在几乎所有需要不需要双精度（float64）的 AI 和其他应用中，都广泛使用 float32。
- en: bfloat16 adds to those an intermediate, with the [same range as float32](https://en.wikipedia.org/wiki/Bfloat16_floating-point_format),
    of about +/- 1.2 x 10^-38 to 3.4 x 10^38, but only occupying half the space, at
    lower precision. It’s designed for easy conversion with float32, as its sign and
    exponent remain unchanged, only the fraction (significand, or mantissa) has to
    be extended or truncated, depending on which direction you’re going in. Converting
    between float32 and float16 is more involved, and most importantly, as the range
    allowed for float16 is far smaller, numbers outside its range would lose their
    numeric value. That means any floating-point number larger than 65,504, which
    is a severe limitation for many applications.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: bfloat16 添加了一个中间值，在 [与 float32 相同的范围](https://en.wikipedia.org/wiki/Bfloat16_floating-point_format)
    内，约为 +/- 1.2 x 10^-38 到 3.4 x 10^38，但仅占用一半的空间，精度较低。它设计用于与 float32 进行简单转换，因为其符号和指数保持不变，只需扩展或截断分数（尾数），具体取决于转换的方向。在
    float32 和 float16 之间的转换更为复杂，最重要的是，由于 float16 允许的范围远小于 float32，超出其范围的数字将失去其数值。这意味着任何大于
    65,504 的浮点数在许多应用程序中都存在严重限制。
- en: A number format that is half the length of float32 numbers isn’t only important
    when storing large amounts of data, but has substantial effects on the performance
    of operations. Those are usually accelerated using ‘single instruction, multiple
    data’ (SIMD) techniques, where [a register is packed](https://eclecticlight.co/2021/08/23/code-in-arm-assembly-lanes-and-loads-in-neon/)
    with two or more values, and the core then executes instructions on them in parallel.
    In the CPU cores of M-series chips, that’s normally done using 128-bit registers,
    which can hold four float32 values, or eight bfloat16 values. For tasks involving
    thousands of arithmetic operations, packing registers with twice the number of
    values can almost double the throughput, as [reported in tests](https://community.arm.com/arm-community-blogs/b/ai-and-ml-blog/posts/bfloat16-processing-for-neural-networks-on-armv8_2d00_a)
    on Arm processors.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 一个长度为 float32 数字一半的数字格式不仅在存储大量数据时很重要，而且对操作性能有重要影响。通常使用“单指令，多数据”（SIMD）技术加速这些操作，其中
    [一个寄存器被打包](https://eclecticlight.co/2021/08/23/code-in-arm-assembly-lanes-and-loads-in-neon/)
    为两个或更多值，然后核心并行执行这些指令。在 M 系列芯片的 CPU 核心中，通常使用 128 位寄存器进行这些操作，可以容纳四个 float32 值或八个
    bfloat16 值。对于涉及数千个算术操作的任务，将寄存器的值增加一倍几乎可以使吞吐量翻倍，正如 [在 Arm 处理器上的测试报告](https://community.arm.com/arm-community-blogs/b/ai-and-ml-blog/posts/bfloat16-processing-for-neural-networks-on-armv8_2d00_a)
    中所述。
- en: In applications where its reduced precision can be accepted, the bfloat16 format
    thus offers the same range as float32, simple and quick conversion with float32,
    occupies half the storage, and delivers up to double the performance in SIMD execution.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 在接受其降低精度的应用程序中，bfloat16 格式因此提供了与 float32 相同的范围、与 float32 简单快速的转换、占用一半存储空间，并在
    SIMD 执行中提供最多两倍的性能。
- en: But my Mac isn’t training AI models
  id: totrans-27
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 但我的 Mac 没有训练 AI 模型
- en: When Google’s AI researchers [first made the claim](https://cloud.google.com/blog/products/ai-machine-learning/bfloat16-the-secret-to-high-performance-on-cloud-tpus)
    that bfloat16 is “the secret to high performance”, they were of course referring
    to those developing AI models, rather than ordinary users. That article was published
    in August 2019, less than a year before Apple announced the M1, explaining why
    none of the hardware in the M1 could have supported bfloat16, and that support
    wasn’t added by Arm until ARMv8.6-A.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 当谷歌的 AI 研究人员[首次声称](https://cloud.google.com/blog/products/ai-machine-learning/bfloat16-the-secret-to-high-performance-on-cloud-tpus)
    bfloat16 是“高性能的秘密”时，他们当然是在指 AI 模型的开发者，而不是普通用户。该文章发表于2019年8月，距苹果宣布 M1 不到一年，解释了为什么
    M1 中的硬件都不支持 bfloat16，并且 ARMv8.6-A 直到 ARM 才添加了支持。
- en: Both Arm and Apple recognise the importance of performing as much AI training
    as possible in-device rather than in the cloud. The case for this has been [argued
    eloquently by Hellen Norman](https://community.arm.com/arm-community-blogs/b/architectures-and-processors-blog/posts/ai-vs-ml-whats-the-difference)
    of Arm, independently of Apple.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: Arm 和苹果都认识到，在设备上尽可能多地执行 AI 训练的重要性，而不是在云中执行。这一点由独立的 Arm 的 Hellen Norman [进行了阐述](https://community.arm.com/arm-community-blogs/b/architectures-and-processors-blog/posts/ai-vs-ml-whats-the-difference)
    ，与苹果无关。
- en: It doesn’t take much imagination to come up with local training tasks that could
    improve commonplace features in macOS. Many of us disable spell-checking because
    of its seeming inability to recognise when we should be using similar words like
    *their, there* and *they’re.* Wouldn’t it be so much better if suggested corrections
    were based on grammar, usage and context? While that’s already starting to improve,
    and Sonoma’s auto-completion is getting smarter, there’s ample room for improvement.
    That depends in part at least on your Mac learning your writing style using in-device
    training.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 想象一下，我们可以设计一些本地训练任务，以改善 macOS 中常见功能并不需要太多的想象力。我们中的许多人都会禁用拼写检查，因为它似乎无法识别我们应该使用类似的单词，比如
    *their, there* 和 *they’re*。如果建议的更正是基于语法、用法和上下文，那不是更好吗？尽管这方面已经开始改善，而且 Sonoma 的自动完成也变得更智能了，但还有很大的改进空间。这在某种程度上至少部分取决于您的
    Mac 是否使用了设备内的训练来学习您的写作风格。
- en: At the start of this article, I explained how this isn’t about the performance
    of current tasks, but the capability to accomplish the tasks our apps and macOS
    will be doing in the future, when some of those will involve the sort of training
    that’s currently left to specialised or dedicated systems.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文开头，我解释了这不是关于当前任务的性能，而是关于我们的应用程序和 macOS 将来将要执行的任务的能力，当中有一些任务目前是由专门的或专用的系统来完成的。
- en: 'CPU cores aren’t the only hardware in Apple silicon to support AI: depending
    on the task, macOS may use their GPU, Apple’s specialised neural engine, or its
    legendary AMX. Given the fact that those in the M1 were designed and developed
    over the same timescale as the CPU cores, it seems improbable that they would
    have bfloat16 support, and Apple has only just [added that number type](https://developer.apple.com/documentation/metalperformanceshaders/mpsdatatype/bfloat16?changes=l_8_6)
    to Metal Performance Shaders in Sonoma.'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: CPU 核心并非 Apple 芯片中支持 AI 的唯一硬件：根据任务的不同，macOS 可能会使用它们的 GPU、苹果的专用神经引擎，或者是其传奇的 AMX。考虑到
    M1 中的这些部分是在与 CPU 核心相同的时间尺度上设计和开发的，它们似乎不太可能具有 bfloat16 支持，而苹果在 Sonoma 中仅刚刚[为 Metal
    Performance Shaders 添加了该数字类型](https://developer.apple.com/documentation/metalperformanceshaders/mpsdatatype/bfloat16?changes=l_8_6)。
- en: Does my Mac support it?
  id: totrans-33
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 我的 Mac 是否支持它？
- en: If you’re unconvinced whether your Apple silicon Mac supports bfloat16 in its
    CPU cores, then there’s an easy way to check. In Terminal, run the command
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您对您的苹果芯片 Mac 是否在其 CPU 核心中支持 bfloat16 感到怀疑，那么有一种简单的方法可以检查。在终端中，运行以下命令
- en: '`sysctl -A > ~/Documents/sysctloutput.text`'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: '`sysctl -A > ~/Documents/sysctloutput.text`'
- en: where the last path is a new text file to take the output from the command.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 其中最后一个路径是一个新的文本文件，用于接收命令的输出。
- en: If that file contains the line
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 如果该文件包含以下行
- en: '`hw.optional.arm.FEAT_BF16: 1`'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '`hw.optional.arm.FEAT_BF16: 1`'
- en: then its cores have hardware support for bfloat16\. If the number given is 0,
    then I’m afraid they don’t. Apple provides information to decode most of the `hw.optional.arm`
    features [here](https://developer.apple.com/documentation/kernel/1387446-sysctlbyname/determining_instruction_set_characteristics).
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 如果该数字为 0，则表示不支持。苹果提供了解码大多数`hw.optional.arm`特性的信息[此处](https://developer.apple.com/documentation/kernel/1387446-sysctlbyname/determining_instruction_set_characteristics)。
- en: Not having bfloat16 support in hardware isn’t the end of the world, nor does
    it mean your M1 Mac is already obsolete. What it does mean, though, is that as
    more and heavier AI is rolled out in the coming years, some of those features
    will run noticeably more slowly on it. Spare a thought, though, for Intel Macs,
    for no matter how fast their CPUs might be, or how many cores they have, they
    will never have any equivalent hardware support for AI.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 在硬件中没有 bfloat16 支持并不是世界末日，也不意味着您的 M1 Mac 已经过时。然而，这意味着在未来几年里，随着越来越多和更重的人工智能被推出，其中一些功能在其上运行将明显更慢。不过，也请为英特尔
    Mac 想一想，因为无论它们的 CPU 有多快，或者有多少个核心，它们永远不会有任何等效的硬件支持用于人工智能。
