- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 'category: 未分类'
- en: 'date: 2024-05-27 15:02:49'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 'date: 2024-05-27 15:02:49'
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: 'From slow to SIMD: A Go optimization story'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 从慢到 SIMD：一个 Go 优化故事
- en: 来源：[https://sourcegraph.com/blog/slow-to-simd](https://sourcegraph.com/blog/slow-to-simd)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://sourcegraph.com/blog/slow-to-simd](https://sourcegraph.com/blog/slow-to-simd)
- en: So, there's this function. It's called a lot. More importantly, all those calls
    are on the critical path of a key user interaction. Let's talk about making it
    fast.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，有这个函数。它被频繁调用。更重要的是，所有这些调用都在关键用户交互的关键路径上。让我们谈谈如何让它变快。
- en: 'Spoiler: it''s a dot product.'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 据说：它是一个点积。
- en: At Sourcegraph, we're working on a Code AI tool named [Cody](https://sourcegraph.com/cody).
    In order for Cody to answer questions well, we need to give them enough [context](https://about.sourcegraph.com/blog/cheating-is-all-you-need)
    to work with. One of the ways we do this is by leveraging [embeddings](https://platform.openai.com/docs/guides/embeddings).
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Sourcegraph，我们正在开发一个名为 [Cody](https://sourcegraph.com/cody) 的 Code AI 工具。为了让
    Cody 能够很好地回答问题，我们需要为其提供足够的[上下文](https://about.sourcegraph.com/blog/cheating-is-all-you-need)。我们做到这一点的一种方式是利用[嵌入](https://platform.openai.com/docs/guides/embeddings)。
- en: For our purposes, an [embedding](https://developers.google.com/machine-learning/crash-course/embeddings/video-lecture)
    is a vector representation of a chunk of text. They are constructed in such a
    way that semantically similar pieces of text have more similar vectors. When Cody
    needs more information to answer a query, we run a similarity search over the
    embeddings to fetch a set of related chunks of code and feed those results to
    Cody to improve the relevance of results.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们的目的，[嵌入](https://developers.google.com/machine-learning/crash-course/embeddings/video-lecture)是文本块的向量表示。它们被构造成这样，以便语义上相似的文本片段具有更相似的向量。当
    Cody 需要更多信息来回答查询时，我们会对嵌入进行相似性搜索，以获取一组相关的代码块，并将这些结果提供给 Cody 以提高结果的相关性。
- en: The piece relevant to this blog post is that similarity metric, which is the
    function that determines how similar two vectors are. For similarity search, a
    common metric is [cosine similarity](https://en.wikipedia.org/wiki/Cosine_similarity).
    However, for normalized vectors (vectors with unit magnitude), the [dot product](https://en.wikipedia.org/wiki/Dot_product)
    yields a ranking that's [equivalent to cosine similarity](https://developers.google.com/machine-learning/clustering/similarity/measuring-similarity).
    To run a search, we calculate the dot product for every embedding in our data
    set and keep the top results. And since we cannot start execution of the LLM until
    we get the necessary context, optimizing this step is crucial.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 与此博客文章相关的部分是相似度度量标准，即确定两个向量相似程度的函数。对于相似性搜索，一个常见的度量标准是[余弦相似度](https://en.wikipedia.org/wiki/Cosine_similarity)。然而，对于归一化的向量（具有单位大小的向量），[点积](https://en.wikipedia.org/wiki/Dot_product)产生的排名与[余弦相似度等价](https://developers.google.com/machine-learning/clustering/similarity/measuring-similarity)。为了进行搜索，我们计算数据集中每个嵌入的点积，并保留前几个结果。由于我们在获得必要的上下文之前不能开始执行
    LLM，因此优化此步骤至关重要。
- en: 'You might be thinking: why not just use an indexed vector DB? Outside of adding
    yet another piece of infra that we need to manage, the construction of an index
    adds latency and increases resource requirements. Additionally, standard nearest-neighbor
    indexes only provide approximate retrieval, which adds another layer of fuzziness
    compared to a more easily explainable exhaustive search. Given that, we decided
    to invest a little in our hand-rolled solution to see how far we could push it.'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能会想：为什么不只使用一个索引向量 DB 呢？除了添加我们需要管理的另一个基础设施外，索引的构建还会增加延迟并增加资源需求。此外，标准的最近邻居索引只提供近似检索，这与更容易解释的详尽搜索相比增加了另一层模糊性。鉴于此，我们决定在我们手工制作的解决方案上投入一点时间，看看我们能推动到哪里。
- en: '[](#the-target)The target'
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '[](#the-target)目标'
- en: This is a simple Go implementation of a function that calculates the dot product
    of two vectors. My goal is to outline the journey I took to optimize this function,
    and to share some tools I picked up along the way.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个简单的 Go 实现，用于计算两个向量的点积。我的目标是概述我优化这个函数所经历的过程，并分享一些我在这个过程中学到的工具。
- en: '[PRE0]'
  id: totrans-14
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Unless otherwise stated, all benchmarks are run on an Intel Xeon Platinum 8481C
    2.70GHz CPU. This is a `c3-highcpu-44` GCE VM. The code in this blog post can
    all be found in runnable form [here](https://github.com/camdencheek/simd_blog).
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 除非另有说明，所有基准测试都在 Intel Xeon Platinum 8481C 2.70GHz CPU 上运行。这是一个 `c3-highcpu-44`
    GCE VM。此博客文章中的代码都可以在[此处](https://github.com/camdencheek/simd_blog)找到并运行。
- en: '[](#loop-unrolling)Loop unrolling'
  id: totrans-16
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '[](#loop-unrolling)循环展开'
- en: Modern CPUs do this thing called [*instruction pipelining*](https://chadaustin.me/2009/02/latency-vs-throughput/)
    where it can run multiple instructions simultaneously if it finds no data dependencies
    between them. A data dependency just means that the input of one instruction depends
    on the output of another.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 现代 CPU 有一种叫做[*指令流水线*](https://chadaustin.me/2009/02/latency-vs-throughput/)的东西，如果它发现它们之间没有数据依赖关系，它可以同时运行多条指令。数据依赖意味着一个指令的输入取决于另一个指令的输出。
- en: In our simple implementation, we have data dependencies between our loop iterations.
    A couple, in fact. Both `i` and `sum` have a read/write pair each iteration, meaning
    an iteration cannot start executing until the previous is finished.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们简单的实现中，我们的循环迭代之间存在数据依赖关系。实际上有几个。`i` 和 `sum` 每次迭代都有一个读/写对，这意味着一个迭代在前一个迭代完成之前无法开始执行。
- en: A common method of squeezing more out of our CPUs in situations like this is
    known as [*loop unrolling*](https://en.wikipedia.org/wiki/Loop_unrolling). The
    basic idea is to rewrite our loop so more of our relatively-high-latency multiply
    instructions can execute simultaneously. Additionally, it amortizes the fixed
    loop costs (increment and compare) across multiple operations.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 在像这样的情况下，从我们的 CPU 中挤出更多性能的常见方法被称为[*循环展开*](https://en.wikipedia.org/wiki/Loop_unrolling)。基本思想是重新编写我们的循环，以便更多的相对高延迟的乘法指令可以同时执行。此外，它将固定的循环成本（增量和比较）分摊到多个操作中。
- en: '[PRE1]'
  id: totrans-20
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: In our unrolled code, the dependencies between multiply instructions are removed,
    enabling the CPU to take more advantage of pipelining. This increases our throughput
    by 37% compared to our naive implementation.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们展开的代码中，乘法指令之间的依赖关系被移除，使 CPU 能够更充分地利用流水线。这使我们的吞吐量比我们的天真实现提高了 37%。
- en: '`DotNaive`'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: '`DotNaive`'
- en: 0.94M vec/s
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 0.94M 个向量/秒
- en: '`DotUnroll4`'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: '`DotUnroll4`'
- en: 1.3M vec/s
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 1.3M 个向量/秒
- en: Note that we can actually improve this slightly more by twiddling with the number
    of iterations we unroll. On the benchmark machine, 8 seemed to be optimal, but
    on my laptop, 4 performs best. However, the improvement is quite platform dependent
    and fairly minimal, so for the rest of the post, I'll stick with an unroll depth
    of 4 for readability.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，我们可以通过调整我们展开的迭代次数稍微改进这一点。在基准机器上，8 似乎是最佳的，但在我的笔记本电脑上，4 的表现最佳。但是，改进非常依赖平台，并且相当小，因此在本文的其余部分，我将保持展开深度为
    4 以提高可读性。
- en: '[](#bounds-checking-elimination)Bounds-checking elimination'
  id: totrans-27
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '[](#bounds-checking-elimination)边界检查消除'
- en: In order to keep out-of-bounds slice accesses from being a security vulnerability
    (like the famous [Heartbleed](https://en.wikipedia.org/wiki/Heartbleed) exploit),
    the go compiler inserts checks before each read. You can check it out in the [generated
    assembly](https://go.godbolt.org/z/qT3M7nPGf) (look for `runtime.panic`).
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 为了防止越界切片访问成为安全漏洞（例如著名的[心脏出血](https://en.wikipedia.org/wiki/Heartbleed)漏洞利用），Go
    编译器在每次读取之前插入检查。您可以在[生成的汇编代码](https://go.godbolt.org/z/qT3M7nPGf)中查看（寻找`runtime.panic`）。
- en: 'The compiled code makes it look like we wrote something like this:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 编译后的代码看起来像我们写了这样的东西：
- en: '[PRE2]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: In a hot loop like this, even with modern branch prediction, the additional
    branches per iteration can add up to a pretty significant performance penalty.
    This is especially true in our case because the inserted jumps limit how much
    we can take advantage of pipelining.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 在像这样的热循环中，即使使用现代分支预测，每次迭代额外的分支也会导致相当大的性能损失。在我们的情况下尤其如此，因为插入的跳转限制了我们能够利用流水线的程度。
- en: If we can convince the compiler that these reads can never be out of bounds,
    it won't insert these runtime checks. This technique is known as "bounds-checking
    elimination", and the same patterns can apply to [languages other than Go](https://github.com/Shnatsel/bounds-check-cookbook/).
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们能说服编译器这些读取永远不会超出边界，它就不会插入这些运行时检查。这种技术称为“边界检查消除”，相同的模式也适用于[Go 之外的语言](https://github.com/Shnatsel/bounds-check-cookbook/)。
- en: In theory, we should be able to do all checks once, outside the loop, and the
    compiler would be able to determine that all the slice indexing is safe. However,
    I couldn't find the right combination of checks to convince the compiler that
    what I'm doing is safe. I landed on a combination of asserting the lengths are
    equal and moving all the bounds checking to the top of the loop. This was enough
    to hit nearly the speed of the bounds-check-free version.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 理论上，我们应该能够在循环外执行所有检查，并且编译器能够确定所有切片索引都是安全的。但是，我找不到合适的检查组合来说服编译器我正在做的是安全的。我最终选择了一种方式，断言长度相等并将所有边界检查移动到循环的顶部。这足以接近无边界检查版本的速度。
- en: '[PRE3]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: The minimizing of bounds checking nets a 9% improvement. Consistently non-zero,
    but nothing to write home about.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 减少边界检查带来了 9% 的改进。一直不为零，但也没什么好写家的。
- en: '`DotNaive`'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '`DotNaive`'
- en: 0.94M vec/s
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 0.94M vec/s
- en: '`DotUnroll4`'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '`DotUnroll4`'
- en: 1.3M vec/s
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 1.3M vec/s
- en: '`DotBCE`'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '`DotBCE`'
- en: 1.4M vec/s
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 1.4M vec/s
- en: This technique translates well to many memory-safe compiled languages like [Rust](https://nnethercote.github.io/perf-book/bounds-checks.html).
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 这种技术在许多内存安全的编译语言中效果很好，比如[Rust](https://nnethercote.github.io/perf-book/bounds-checks.html)。
- en: 'Exercise for the reader: why is it significant that we slice like `a[i:i+4:i+4]`
    rather than just `a[i:i+4]`?'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 读者的练习：为什么我们要像 `a[i:i+4:i+4]` 这样切片，而不只是 `a[i:i+4]`？
- en: '[](#quantization)Quantization'
  id: totrans-44
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '[](#quantization)量化'
- en: 'We''ve improved single-core search throughput by ~50% at this point, but now
    we''ve hit a new bottleneck: memory usage. Our vectors are 1536 dimensions. With
    4-byte elements, this comes out to 6KiB per vector, and we generate roughly a
    million vectors per GiB of code. That adds up quickly. We had a few customers
    come to us with some massive monorepos, and we wanted to reduce our memory usage
    so we can support those cases more cheaply.'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 我们目前已将单核搜索吞吐量提高了约50%，但现在我们遇到了一个新的瓶颈：内存使用。我们的向量是 1536 维。每个向量有 4 字节的元素，这意味着每个向量占用
    6KiB，我们大约每 GiB 代码生成一百万个向量。这很快就会累积起来。我们有一些客户给我们带来了一些庞大的单库，我们希望减少我们的内存使用量，以便更便宜地支持这些情况。
- en: One possible mitigation would be to move the vectors to disk, but loading them
    from disk at search time can add [significant latency](https://colin-scott.github.io/personal_website/research/interactive_latency.html),
    especially on slow disks. Instead, we chose to compress our vectors with `int8`
    quantization.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 一种可能的缓解措施是将向量移动到磁盘上，但在搜索时从磁盘加载可能会增加[显著的延迟](https://colin-scott.github.io/personal_website/research/interactive_latency.html)，特别是在慢速磁盘上。相反，我们选择了用
    `int8` 量化来压缩我们的向量。
- en: There are [plenty of ways](https://en.wikipedia.org/wiki/Dimensionality_reduction)
    to compress vectors, but we'll be talking about [*integer quantization*](https://huggingface.co/docs/optimum/concept_guides/quantization),
    which is relatively simple, but effective. The idea is to reduce the precision
    of the 4-byte `float32` vector elements by converting them to 1-byte `int8`s.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 有[很多种方法](https://en.wikipedia.org/wiki/Dimensionality_reduction)可以压缩向量，但我们将讨论相对简单但有效的[*整数量化*](https://huggingface.co/docs/optimum/concept_guides/quantization)。其思想是通过将
    4 字节的 `float32` 向量元素转换为 1 字节的 `int8` 来降低精度。
- en: 'I won''t get into exactly *how* we decide to do the translation between `float32`
    and `int8`, since that''s a pretty [deep topic](https://huggingface.co/docs/optimum/concept_guides/quantization),
    but suffice it to say our function now looks like the following:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 我不会详细讨论我们如何决定在`float32`和`int8`之间进行转换，因为那是一个相当[深入的话题](https://huggingface.co/docs/optimum/concept_guides/quantization)，但可以说我们的函数现在看起来像下面这样：
- en: '[PRE4]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: This change yields a 4x reduction in memory usage at the cost of some accuracy
    (which we carefully measured, but is irrelevant to this blog post).
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 这个改变导致内存使用量减少了 4 倍，但牺牲了一些准确性（我们进行了仔细的测量，但这与本篇博文无关）。
- en: Unfortunately, re-running the benchmarks shows our search speed regressed a
    bit from the change. Taking a look at the generated assembly (with `go tool compile
    -S`), there are some new instructions for converting `int8` to `int32`, which
    might explain the difference. I didn't dig too deep though, since all our performance
    improvements up to this point become irrelevant in the next section.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，重新运行基准测试显示我们的搜索速度从这个改变中有所倒退。通过使用 `go tool compile -S` 查看生成的汇编代码，可以看到一些新的指令用于将
    `int8` 转换为 `int32`，这可能解释了差异。我没有深入研究，因为到目前为止我们的所有性能改进在下一节中都变得无关紧要。
- en: '`DotNaive`'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: '`DotNaive`'
- en: 0.94M vec/s
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 0.94M vec/s
- en: '`DotUnroll4`'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: '`DotUnroll4`'
- en: 1.3M vec/s
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 1.3M vec/s
- en: '`DotBCE`'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: '`DotBCE`'
- en: 1.4M vec/s
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 1.4M vec/s
- en: '`DotInt8BCE`'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: '`DotInt8BCE`'
- en: 1.2M vec/s
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 1.2M vec/s
- en: '[](#simd)SIMD'
  id: totrans-60
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '[](#simd)SIMD'
- en: The speed improvements so far were nice, but still not enough for our largest
    customers. So we started dabbling with more dramatic approaches.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，速度的改善很好，但对于我们最大的客户来说还不够。所以我们开始尝试更加戏剧性的方法。
- en: I always love an excuse to play with SIMD. And this problem seemed like the
    perfect nail for that hammer.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 我总是喜欢借口去玩SIMD。而这个问题似乎是用那个锤子打的完美的钉子。
- en: For those unfamiliar, SIMD stands for "Single Instruction Multiple Data". Just
    like it's says, it lets you run an operation over a bunch of pieces of data with
    a single instruction. As an example, to add two `int32` vectors element-wise,
    we could add them together one by one with the `ADD` instruction and, or we can
    use the `VPADDD` instruction to add 64 pairs at a time with the [same](https://uops.info/html-instr/ADD_01_R32_R32.html)
    [latency](https://uops.info/html-instr/VPADDD_YMM_YMM_M256.html) (depending on
    the architecture).
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 对于不熟悉的人来说，SIMD 代表"单指令多数据"。就像它的名字一样，它让您可以用单个指令对一堆数据执行操作。举个例子，要逐个元素地将两个`int32`向量相加，我们可以使用`ADD`指令逐个将它们相加，或者我们可以使用`VPADDD`指令一次性添加64对元素，具有[相同](https://uops.info/html-instr/ADD_01_R32_R32.html)的[延迟](https://uops.info/html-instr/VPADDD_YMM_YMM_M256.html)(取决于架构)。
- en: 'We have a problem though. Go does not expose SIMD intrinsics like [C](https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html)
    or [Rust](https://doc.rust-lang.org/beta/core/simd/index.html). We have two options
    here: write it in C and use Cgo, or write it by hand for Go''s assembler. I try
    hard to avoid Cgo whenever possible for [many reasons that are not at all original](https://dave.cheney.net/2016/01/18/cgo-is-not-go),
    but one of those reasons is that Cgo imposes a performance penalty, and performance
    of this snippet is paramount. Also, getting my hands dirty with some assembly
    sounds fun, so that''s what I''m going to do.'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 但是我们有一个问题。Go 不像[C](https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html)或[Rust](https://doc.rust-lang.org/beta/core/simd/index.html)那样暴露SIMD内联函数。我们有两个选择：用C编写并使用Cgo，或者为Go的汇编器手动编写。我尽量避免使用Cgo，因为有[许多并非原创的原因](https://dave.cheney.net/2016/01/18/cgo-is-not-go)，但其中一个原因是Cgo会对性能造成影响，而这段代码的性能至关重要。此外，玩一些汇编听起来很有趣，所以我会这么做。
- en: I want this routine to be reasonably portable, so I'm going to restrict myself
    to only AVX2 instructions, which are supported on most `x86_64` server CPUs these
    days. We can use [runtime feature detection](https://sourcegraph.com/github.com/sourcegraph/sourcegraph@3ac2170c6523dd074835919a1804f197cf86e451/-/blob/internal/embeddings/dot_amd64.go?L17-21)
    to fall back to a slower option in pure Go.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 我希望这个例程具有相当的可移植性，因此我将限制自己仅使用 AVX2 指令，这些指令在大多数当前的`x86_64`服务器CPU上都受支持。我们可以使用[运行时特性检测](https://sourcegraph.com/github.com/sourcegraph/sourcegraph@3ac2170c6523dd074835919a1804f197cf86e451/-/blob/internal/embeddings/dot_amd64.go?L17-21)在纯
    Go 中退回到一个较慢的选项。
- en: <details><summary>Full code for `DotAVX2`</summary>
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: <details><summary>`DotAVX2`的完整代码</summary>
- en: '[PRE5]</details>'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '[PRE5]</details>'
- en: 'The core loop of the implementation depends on three main instructions:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 实现的核心循环依赖于三个主要指令：
- en: '[`VPMOVSXBW`](https://www.felixcloutier.com/x86/pmovsx), which loads `int8`s
    into a vector `int16`s'
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[`VPMOVSXBW`](https://www.felixcloutier.com/x86/pmovsx)，它将`int8`加载到向量`int16`中'
- en: '[`VPMADDWD`](https://www.felixcloutier.com/x86/pmaddwd), which multiplies two
    `int16` vectors element-wise, then adds fuzzy stack. together adjacent pairs to
    produce a vector of `int32`s'
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[`VPMADDWD`](https://www.felixcloutier.com/x86/pmaddwd)，它将两个`int16`向量逐元素相乘，然后将相邻对组合在一起以生成一个`int32`向量'
- en: '[`VPADDD`](https://www.felixcloutier.com/x86/paddb:paddw:paddd:paddq), which
    accumulates the resulting `int32` vector into our running sum'
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[`VPADDD`](https://www.felixcloutier.com/x86/paddb:paddw:paddd:paddq)，它将结果`int32`向量累加到我们的运行总和'
- en: '`VPMADDWD` is a real heavy lifter here. By combining the multiply and add steps
    into one, not only does it save instructions, it also helps us avoid overflow
    issues by simultaneously widening the result to an `int32`.'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: '`VPMADDWD` 在这里起到了重要作用。通过将乘法和加法步骤合并为一步，它不仅节省了指令，还通过同时将结果扩展为`int32`来帮助我们避免溢出问题。'
- en: Let's see what this earned us.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看这给我们带来了什么。
- en: '`DotNaive`'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: '`DotNaive`'
- en: 0.94M vec/s
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 0.94M vec/s
- en: '`DotUnroll4`'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: '`DotUnroll4`'
- en: 1.3M vec/s
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 1.3M vec/s
- en: '`DotBCE`'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: '`DotBCE`'
- en: 1.4M vec/s
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 1.4M vec/s
- en: '`DotInt8BCE`'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: '`DotInt8BCE`'
- en: 1.2M vec/s
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 1.2M vec/s
- en: '`DotAVX2`'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: '`DotAVX2`'
- en: 7.0M vec/s
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 7.0M vec/s
- en: Woah, that's a 530% increase in throughput from our previous best! SIMD for
    the win 🚀
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 哇，这比我们之前的最佳结果提高了530％！SIMD 胜利 🚀
- en: 'Now, it wasn''t all sunshine and rainbows. Hand-writing assembly in Go is weird.
    It uses a [custom assembler](https://go.dev/doc/asm), which means that its assembly
    language looks just-different-enough-to-be-confusing compared to the assembly
    snippets you usually find online. It has some weird quirks like [changing the
    order of instruction operands](https://www.quasilyte.dev/blog/post/go-asm-complementary-reference/#operands-order)
    or [using different names for instructions](https://www.quasilyte.dev/blog/post/go-asm-complementary-reference/#mnemonics).
    Some instructions don''t even *have* names in the go assembler and can only be
    used via their [binary encoding](https://go.dev/doc/asm#unsupported_opcodes).
    Shameless plug: I found [sourcegraph.com](https://sourcegraph.com/search) invaluable
    for finding examples of Go assembly to draw from.'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，不是一切都是阳光和彩虹的。在 Go 中手写汇编很奇怪。它使用一个 [自定义汇编器](https://go.dev/doc/asm)，这意味着它的汇编语言看起来与您通常在线找到的汇编片段有一点点不同，足以让人感到困惑。它有一些奇怪的怪癖，比如
    [改变指令操作数的顺序](https://www.quasilyte.dev/blog/post/go-asm-complementary-reference/#operands-order)
    或 [使用不同的指令名称](https://www.quasilyte.dev/blog/post/go-asm-complementary-reference/#mnemonics)。在
    go 汇编器中，有些指令甚至 *没有* 名称，只能通过它们的 [二进制编码](https://go.dev/doc/asm#unsupported_opcodes)
    使用。厚颜无耻的推广：我发现 [sourcegraph.com](https://sourcegraph.com/search) 对于查找 Go 汇编示例非常有用。
- en: That said, compared to Cgo, there are some nice benefits. Debugging still works
    well, the assembly can be stepped through, and registers can be inspected using
    `delve`. There are no extra build steps (a C toolchain doesn't need to be set
    up). It's easy to set up a pure-Go fallback so cross-compilation still works.
    Common problems are caught by `go vet`.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管如此，与 Cgo 相比，还是有一些好处的。调试仍然很有效，汇编代码可以逐步执行，并且可以使用 `delve` 检查寄存器。没有额外的构建步骤（不需要设置
    C 工具链）。很容易设置一个纯 Go 的后备，以便跨平台编译仍然有效。常见问题由 `go vet` 捕获。
- en: '[](#simdbut-bigger)SIMD...but bigger'
  id: totrans-87
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '[](#simdbut-bigger)SIMD...但更大'
- en: Previously, we limited ourselves to AVX2, but what if we *didn't*? The VNNI
    extension to AVX-512 added the [`VPDPBUSD`](https://www.felixcloutier.com/x86/vpdpbusd)
    instruction, which computes the dot product on `int8` vectors rather than `int16`s.
    This means we can process four times as many elements in a single instruction
    because we don't have to convert to `int16` first and our vector width doubles
    with AVX-512!
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 以前，我们局限于 AVX2，但如果我们 *不* 这样做呢？ AVX-512 的 VNNI 扩展添加了 [`VPDPBUSD`](https://www.felixcloutier.com/x86/vpdpbusd)
    指令，它计算 `int8` 向量而不是 `int16`。这意味着我们可以在一条指令中处理四倍数量的元素，因为我们不必先转换为 `int16`，并且我们的矢量宽度在
    AVX-512 中加倍！
- en: The only problem is that the instruction requires one vector to be signed bytes,
    and the other to be *unsigned* bytes. Both of our vectors are signed. We can employ
    [a trick from Intel's developer guide](https://www.intel.com/content/www/us/en/docs/onednn/developer-guide-reference/2023-0/nuances-of-int8-computations.html#DOXID-DEV-GUIDE-INT8-COMPUTATIONS-1DG-I8-COMP-S12)
    to help us out. Given two `int8` elements, `a[n]` and `b[n]`, we do the element-wise
    calculation as `a[n]* (b[n] + 128) - a[n] * 128`. The `a[n] * 128` term is the
    overshoot from adding 128 to bump `b[n]` into `u8` range. We keep track of that
    separately and subtract it at the end. Each of the operations in that expression
    can be vectorized.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 唯一的问题是指令要求一个向量为有符号字节，另一个为 *无符号* 字节。我们的两个向量都是有符号的。我们可以采用 [英特尔开发者指南中的技巧](https://www.intel.com/content/www/us/en/docs/onednn/developer-guide-reference/2023-0/nuances-of-int8-computations.html#DOXID-DEV-GUIDE-INT8-COMPUTATIONS-1DG-I8-COMP-S12)
    来帮助我们。给定两个 `int8` 元素，`a[n]` 和 `b[n]`，我们按元素计算为 `a[n]* (b[n] + 128) - a[n] * 128`。`a[n]
    * 128` 项是从添加 128 以将 `b[n]` 增加到 `u8` 范围的超额。我们单独跟踪它，并在最后减去它。该表达式中的每个操作都可以矢量化。
- en: <details><summary>Full code for `DotVNNI`</summary>
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: <details><summary>`DotVNNI` 的完整代码</summary>
- en: '[PRE6]</details>'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: '[PRE6]</details>'
- en: This implementation yielded another 21% improvement. Not bad!
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 这个实现产生了另外 21% 的改进。还不错！
- en: '`DotNaive`'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: '`DotNaive`'
- en: 0.94M vec/s
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 0.94M 向量/秒
- en: '`DotUnroll4`'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: '`DotUnroll4`'
- en: 1.3M vec/s
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 1.3M 向量/秒
- en: '`DotBCE`'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: '`DotBCE`'
- en: 1.4M vec/s
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 1.4M 向量/秒
- en: '`DotInt8BCE`'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: '`DotInt8BCE`'
- en: 1.2M vec/s
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 1.2M 向量/秒
- en: '`DotAVX2`'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: '`DotAVX2`'
- en: 7.0M vec/s
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 7.0M 向量/秒
- en: '`DotVNNI`'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: '`DotVNNI`'
- en: 8.8M vec/s
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 8.8M 向量/秒
- en: '[](#whats-next)What''s next?'
  id: totrans-105
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '[](#whats-next)接下来是什么？'
- en: Well, I'm pretty happy with an 9.3x increase in throughput and a 4x reduction
    in memory usage, so I'll probably leave it here.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 嗯，我对吞吐量增加了 9.3 倍和内存使用减少了 4 倍非常满意，所以我可能会就此结束。
- en: The real life answer here is probably "use an index". There is a ton of good
    work out there focused on making nearest neighbor search fast, and there are plenty
    of batteries-included vector DBs that make it pretty easy to deploy.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里真实的答案可能是“使用索引”。有很多关于使最近邻搜索快速的优秀工作，而且有很多内置电池的矢量数据库，使得部署变得相当容易。
- en: '*However*, if you want some fun food for thought, a colleague of mine built
    a proof-of-concept [dot product on the GPU](https://github.com/sourcegraph/sourcegraph/compare/simd-post-gpu-embeddings~3...simd-post-gpu-embeddings).'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: '*然而*，如果你想要一些有趣的思考食物，我的一个同事构建了一个概念验证的 [GPU 上的点积](https://github.com/sourcegraph/sourcegraph/compare/simd-post-gpu-embeddings~3...simd-post-gpu-embeddings)。'
- en: '[](#bonus-material)Bonus material'
  id: totrans-109
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '[](#bonus-material)奖励材料'
- en: If you haven't used [benchstat](https://pkg.go.dev/golang.org/x/perf/cmd/benchstat),
    you should. It's great. Super simple statistical comparison of benchmark results.
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果你还没有使用 [benchstat](https://pkg.go.dev/golang.org/x/perf/cmd/benchstat)，你应该试试。它很棒。对基准测试结果进行超级简单的统计比较。
- en: Don't miss the [compiler explorer](https://go.godbolt.org/z/qT3M7nPGf), which
    is an extremely useful tool for digging into compiler codegen.
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 别错过 [编译器浏览器](https://go.godbolt.org/z/qT3M7nPGf)，这是一个非常有用的工具，可以深入研究编译器代码生成。
- en: There's also that time I got [nerd sniped](https://twitter.com/sluongng/status/1654066471230636033)
    into implementing [a version with ARM NEON](https://github.com/camdencheek/simd_blog/blob/main/dot_arm64.s),
    which made for some interesting comparisons.
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 还有那个让我被 [nerd sniped](https://twitter.com/sluongng/status/1654066471230636033)
    去实现 [带有 ARM NEON 的版本](https://github.com/camdencheek/simd_blog/blob/main/dot_arm64.s)
    的时刻，这引发了一些有趣的比较。
- en: If you haven't come across it, the [Agner Fog Instruction Tables](https://www.agner.org/optimize/)
    make for some great reference material for low-level optimizations. For this work,
    I used them to help grok differences instruction latencies and why some pipeline
    better than others.
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果你还没有接触过，[Agner Fog 指令表](https://www.agner.org/optimize/) 是一些用于低级优化的极好的参考资料。在这项工作中，我用它们来帮助理解不同指令的延迟以及为什么有些管道比其他管道更好。
