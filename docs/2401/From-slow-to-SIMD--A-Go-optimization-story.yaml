- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: æœªåˆ†ç±»'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 'category: æœªåˆ†ç±»'
- en: 'date: 2024-05-27 15:02:49'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 'date: 2024-05-27 15:02:49'
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: 'From slow to SIMD: A Go optimization story'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ä»æ…¢åˆ° SIMDï¼šä¸€ä¸ª Go ä¼˜åŒ–æ•…äº‹
- en: æ¥æºï¼š[https://sourcegraph.com/blog/slow-to-simd](https://sourcegraph.com/blog/slow-to-simd)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: æ¥æºï¼š[https://sourcegraph.com/blog/slow-to-simd](https://sourcegraph.com/blog/slow-to-simd)
- en: So, there's this function. It's called a lot. More importantly, all those calls
    are on the critical path of a key user interaction. Let's talk about making it
    fast.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥ï¼Œæœ‰è¿™ä¸ªå‡½æ•°ã€‚å®ƒè¢«é¢‘ç¹è°ƒç”¨ã€‚æ›´é‡è¦çš„æ˜¯ï¼Œæ‰€æœ‰è¿™äº›è°ƒç”¨éƒ½åœ¨å…³é”®ç”¨æˆ·äº¤äº’çš„å…³é”®è·¯å¾„ä¸Šã€‚è®©æˆ‘ä»¬è°ˆè°ˆå¦‚ä½•è®©å®ƒå˜å¿«ã€‚
- en: 'Spoiler: it''s a dot product.'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: æ®è¯´ï¼šå®ƒæ˜¯ä¸€ä¸ªç‚¹ç§¯ã€‚
- en: At Sourcegraph, we're working on a Code AI tool named [Cody](https://sourcegraph.com/cody).
    In order for Cody to answer questions well, we need to give them enough [context](https://about.sourcegraph.com/blog/cheating-is-all-you-need)
    to work with. One of the ways we do this is by leveraging [embeddings](https://platform.openai.com/docs/guides/embeddings).
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ Sourcegraphï¼Œæˆ‘ä»¬æ­£åœ¨å¼€å‘ä¸€ä¸ªåä¸º [Cody](https://sourcegraph.com/cody) çš„ Code AI å·¥å…·ã€‚ä¸ºäº†è®©
    Cody èƒ½å¤Ÿå¾ˆå¥½åœ°å›ç­”é—®é¢˜ï¼Œæˆ‘ä»¬éœ€è¦ä¸ºå…¶æä¾›è¶³å¤Ÿçš„[ä¸Šä¸‹æ–‡](https://about.sourcegraph.com/blog/cheating-is-all-you-need)ã€‚æˆ‘ä»¬åšåˆ°è¿™ä¸€ç‚¹çš„ä¸€ç§æ–¹å¼æ˜¯åˆ©ç”¨[åµŒå…¥](https://platform.openai.com/docs/guides/embeddings)ã€‚
- en: For our purposes, an [embedding](https://developers.google.com/machine-learning/crash-course/embeddings/video-lecture)
    is a vector representation of a chunk of text. They are constructed in such a
    way that semantically similar pieces of text have more similar vectors. When Cody
    needs more information to answer a query, we run a similarity search over the
    embeddings to fetch a set of related chunks of code and feed those results to
    Cody to improve the relevance of results.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹äºæˆ‘ä»¬çš„ç›®çš„ï¼Œ[åµŒå…¥](https://developers.google.com/machine-learning/crash-course/embeddings/video-lecture)æ˜¯æ–‡æœ¬å—çš„å‘é‡è¡¨ç¤ºã€‚å®ƒä»¬è¢«æ„é€ æˆè¿™æ ·ï¼Œä»¥ä¾¿è¯­ä¹‰ä¸Šç›¸ä¼¼çš„æ–‡æœ¬ç‰‡æ®µå…·æœ‰æ›´ç›¸ä¼¼çš„å‘é‡ã€‚å½“
    Cody éœ€è¦æ›´å¤šä¿¡æ¯æ¥å›ç­”æŸ¥è¯¢æ—¶ï¼Œæˆ‘ä»¬ä¼šå¯¹åµŒå…¥è¿›è¡Œç›¸ä¼¼æ€§æœç´¢ï¼Œä»¥è·å–ä¸€ç»„ç›¸å…³çš„ä»£ç å—ï¼Œå¹¶å°†è¿™äº›ç»“æœæä¾›ç»™ Cody ä»¥æé«˜ç»“æœçš„ç›¸å…³æ€§ã€‚
- en: The piece relevant to this blog post is that similarity metric, which is the
    function that determines how similar two vectors are. For similarity search, a
    common metric is [cosine similarity](https://en.wikipedia.org/wiki/Cosine_similarity).
    However, for normalized vectors (vectors with unit magnitude), the [dot product](https://en.wikipedia.org/wiki/Dot_product)
    yields a ranking that's [equivalent to cosine similarity](https://developers.google.com/machine-learning/clustering/similarity/measuring-similarity).
    To run a search, we calculate the dot product for every embedding in our data
    set and keep the top results. And since we cannot start execution of the LLM until
    we get the necessary context, optimizing this step is crucial.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸æ­¤åšå®¢æ–‡ç« ç›¸å…³çš„éƒ¨åˆ†æ˜¯ç›¸ä¼¼åº¦åº¦é‡æ ‡å‡†ï¼Œå³ç¡®å®šä¸¤ä¸ªå‘é‡ç›¸ä¼¼ç¨‹åº¦çš„å‡½æ•°ã€‚å¯¹äºç›¸ä¼¼æ€§æœç´¢ï¼Œä¸€ä¸ªå¸¸è§çš„åº¦é‡æ ‡å‡†æ˜¯[ä½™å¼¦ç›¸ä¼¼åº¦](https://en.wikipedia.org/wiki/Cosine_similarity)ã€‚ç„¶è€Œï¼Œå¯¹äºå½’ä¸€åŒ–çš„å‘é‡ï¼ˆå…·æœ‰å•ä½å¤§å°çš„å‘é‡ï¼‰ï¼Œ[ç‚¹ç§¯](https://en.wikipedia.org/wiki/Dot_product)äº§ç”Ÿçš„æ’åä¸[ä½™å¼¦ç›¸ä¼¼åº¦ç­‰ä»·](https://developers.google.com/machine-learning/clustering/similarity/measuring-similarity)ã€‚ä¸ºäº†è¿›è¡Œæœç´¢ï¼Œæˆ‘ä»¬è®¡ç®—æ•°æ®é›†ä¸­æ¯ä¸ªåµŒå…¥çš„ç‚¹ç§¯ï¼Œå¹¶ä¿ç•™å‰å‡ ä¸ªç»“æœã€‚ç”±äºæˆ‘ä»¬åœ¨è·å¾—å¿…è¦çš„ä¸Šä¸‹æ–‡ä¹‹å‰ä¸èƒ½å¼€å§‹æ‰§è¡Œ
    LLMï¼Œå› æ­¤ä¼˜åŒ–æ­¤æ­¥éª¤è‡³å…³é‡è¦ã€‚
- en: 'You might be thinking: why not just use an indexed vector DB? Outside of adding
    yet another piece of infra that we need to manage, the construction of an index
    adds latency and increases resource requirements. Additionally, standard nearest-neighbor
    indexes only provide approximate retrieval, which adds another layer of fuzziness
    compared to a more easily explainable exhaustive search. Given that, we decided
    to invest a little in our hand-rolled solution to see how far we could push it.'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: ä½ å¯èƒ½ä¼šæƒ³ï¼šä¸ºä»€ä¹ˆä¸åªä½¿ç”¨ä¸€ä¸ªç´¢å¼•å‘é‡ DB å‘¢ï¼Ÿé™¤äº†æ·»åŠ æˆ‘ä»¬éœ€è¦ç®¡ç†çš„å¦ä¸€ä¸ªåŸºç¡€è®¾æ–½å¤–ï¼Œç´¢å¼•çš„æ„å»ºè¿˜ä¼šå¢åŠ å»¶è¿Ÿå¹¶å¢åŠ èµ„æºéœ€æ±‚ã€‚æ­¤å¤–ï¼Œæ ‡å‡†çš„æœ€è¿‘é‚»å±…ç´¢å¼•åªæä¾›è¿‘ä¼¼æ£€ç´¢ï¼Œè¿™ä¸æ›´å®¹æ˜“è§£é‡Šçš„è¯¦å°½æœç´¢ç›¸æ¯”å¢åŠ äº†å¦ä¸€å±‚æ¨¡ç³Šæ€§ã€‚é‰´äºæ­¤ï¼Œæˆ‘ä»¬å†³å®šåœ¨æˆ‘ä»¬æ‰‹å·¥åˆ¶ä½œçš„è§£å†³æ–¹æ¡ˆä¸ŠæŠ•å…¥ä¸€ç‚¹æ—¶é—´ï¼Œçœ‹çœ‹æˆ‘ä»¬èƒ½æ¨åŠ¨åˆ°å“ªé‡Œã€‚
- en: '[](#the-target)The target'
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '[](#the-target)ç›®æ ‡'
- en: This is a simple Go implementation of a function that calculates the dot product
    of two vectors. My goal is to outline the journey I took to optimize this function,
    and to share some tools I picked up along the way.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ˜¯ä¸€ä¸ªç®€å•çš„ Go å®ç°ï¼Œç”¨äºè®¡ç®—ä¸¤ä¸ªå‘é‡çš„ç‚¹ç§¯ã€‚æˆ‘çš„ç›®æ ‡æ˜¯æ¦‚è¿°æˆ‘ä¼˜åŒ–è¿™ä¸ªå‡½æ•°æ‰€ç»å†çš„è¿‡ç¨‹ï¼Œå¹¶åˆ†äº«ä¸€äº›æˆ‘åœ¨è¿™ä¸ªè¿‡ç¨‹ä¸­å­¦åˆ°çš„å·¥å…·ã€‚
- en: '[PRE0]'
  id: totrans-14
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Unless otherwise stated, all benchmarks are run on an Intel Xeon Platinum 8481C
    2.70GHz CPU. This is a `c3-highcpu-44` GCE VM. The code in this blog post can
    all be found in runnable form [here](https://github.com/camdencheek/simd_blog).
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: é™¤éå¦æœ‰è¯´æ˜ï¼Œæ‰€æœ‰åŸºå‡†æµ‹è¯•éƒ½åœ¨ Intel Xeon Platinum 8481C 2.70GHz CPU ä¸Šè¿è¡Œã€‚è¿™æ˜¯ä¸€ä¸ª `c3-highcpu-44`
    GCE VMã€‚æ­¤åšå®¢æ–‡ç« ä¸­çš„ä»£ç éƒ½å¯ä»¥åœ¨[æ­¤å¤„](https://github.com/camdencheek/simd_blog)æ‰¾åˆ°å¹¶è¿è¡Œã€‚
- en: '[](#loop-unrolling)Loop unrolling'
  id: totrans-16
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '[](#loop-unrolling)å¾ªç¯å±•å¼€'
- en: Modern CPUs do this thing called [*instruction pipelining*](https://chadaustin.me/2009/02/latency-vs-throughput/)
    where it can run multiple instructions simultaneously if it finds no data dependencies
    between them. A data dependency just means that the input of one instruction depends
    on the output of another.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: ç°ä»£ CPU æœ‰ä¸€ç§å«åš[*æŒ‡ä»¤æµæ°´çº¿*](https://chadaustin.me/2009/02/latency-vs-throughput/)çš„ä¸œè¥¿ï¼Œå¦‚æœå®ƒå‘ç°å®ƒä»¬ä¹‹é—´æ²¡æœ‰æ•°æ®ä¾èµ–å…³ç³»ï¼Œå®ƒå¯ä»¥åŒæ—¶è¿è¡Œå¤šæ¡æŒ‡ä»¤ã€‚æ•°æ®ä¾èµ–æ„å‘³ç€ä¸€ä¸ªæŒ‡ä»¤çš„è¾“å…¥å–å†³äºå¦ä¸€ä¸ªæŒ‡ä»¤çš„è¾“å‡ºã€‚
- en: In our simple implementation, we have data dependencies between our loop iterations.
    A couple, in fact. Both `i` and `sum` have a read/write pair each iteration, meaning
    an iteration cannot start executing until the previous is finished.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æˆ‘ä»¬ç®€å•çš„å®ç°ä¸­ï¼Œæˆ‘ä»¬çš„å¾ªç¯è¿­ä»£ä¹‹é—´å­˜åœ¨æ•°æ®ä¾èµ–å…³ç³»ã€‚å®é™…ä¸Šæœ‰å‡ ä¸ªã€‚`i` å’Œ `sum` æ¯æ¬¡è¿­ä»£éƒ½æœ‰ä¸€ä¸ªè¯»/å†™å¯¹ï¼Œè¿™æ„å‘³ç€ä¸€ä¸ªè¿­ä»£åœ¨å‰ä¸€ä¸ªè¿­ä»£å®Œæˆä¹‹å‰æ— æ³•å¼€å§‹æ‰§è¡Œã€‚
- en: A common method of squeezing more out of our CPUs in situations like this is
    known as [*loop unrolling*](https://en.wikipedia.org/wiki/Loop_unrolling). The
    basic idea is to rewrite our loop so more of our relatively-high-latency multiply
    instructions can execute simultaneously. Additionally, it amortizes the fixed
    loop costs (increment and compare) across multiple operations.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨åƒè¿™æ ·çš„æƒ…å†µä¸‹ï¼Œä»æˆ‘ä»¬çš„ CPU ä¸­æŒ¤å‡ºæ›´å¤šæ€§èƒ½çš„å¸¸è§æ–¹æ³•è¢«ç§°ä¸º[*å¾ªç¯å±•å¼€*](https://en.wikipedia.org/wiki/Loop_unrolling)ã€‚åŸºæœ¬æ€æƒ³æ˜¯é‡æ–°ç¼–å†™æˆ‘ä»¬çš„å¾ªç¯ï¼Œä»¥ä¾¿æ›´å¤šçš„ç›¸å¯¹é«˜å»¶è¿Ÿçš„ä¹˜æ³•æŒ‡ä»¤å¯ä»¥åŒæ—¶æ‰§è¡Œã€‚æ­¤å¤–ï¼Œå®ƒå°†å›ºå®šçš„å¾ªç¯æˆæœ¬ï¼ˆå¢é‡å’Œæ¯”è¾ƒï¼‰åˆ†æ‘Šåˆ°å¤šä¸ªæ“ä½œä¸­ã€‚
- en: '[PRE1]'
  id: totrans-20
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: In our unrolled code, the dependencies between multiply instructions are removed,
    enabling the CPU to take more advantage of pipelining. This increases our throughput
    by 37% compared to our naive implementation.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æˆ‘ä»¬å±•å¼€çš„ä»£ç ä¸­ï¼Œä¹˜æ³•æŒ‡ä»¤ä¹‹é—´çš„ä¾èµ–å…³ç³»è¢«ç§»é™¤ï¼Œä½¿ CPU èƒ½å¤Ÿæ›´å……åˆ†åœ°åˆ©ç”¨æµæ°´çº¿ã€‚è¿™ä½¿æˆ‘ä»¬çš„ååé‡æ¯”æˆ‘ä»¬çš„å¤©çœŸå®ç°æé«˜äº† 37%ã€‚
- en: '`DotNaive`'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: '`DotNaive`'
- en: 0.94M vec/s
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 0.94M ä¸ªå‘é‡/ç§’
- en: '`DotUnroll4`'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: '`DotUnroll4`'
- en: 1.3M vec/s
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 1.3M ä¸ªå‘é‡/ç§’
- en: Note that we can actually improve this slightly more by twiddling with the number
    of iterations we unroll. On the benchmark machine, 8 seemed to be optimal, but
    on my laptop, 4 performs best. However, the improvement is quite platform dependent
    and fairly minimal, so for the rest of the post, I'll stick with an unroll depth
    of 4 for readability.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: è¯·æ³¨æ„ï¼Œæˆ‘ä»¬å¯ä»¥é€šè¿‡è°ƒæ•´æˆ‘ä»¬å±•å¼€çš„è¿­ä»£æ¬¡æ•°ç¨å¾®æ”¹è¿›è¿™ä¸€ç‚¹ã€‚åœ¨åŸºå‡†æœºå™¨ä¸Šï¼Œ8 ä¼¼ä¹æ˜¯æœ€ä½³çš„ï¼Œä½†åœ¨æˆ‘çš„ç¬”è®°æœ¬ç”µè„‘ä¸Šï¼Œ4 çš„è¡¨ç°æœ€ä½³ã€‚ä½†æ˜¯ï¼Œæ”¹è¿›éå¸¸ä¾èµ–å¹³å°ï¼Œå¹¶ä¸”ç›¸å½“å°ï¼Œå› æ­¤åœ¨æœ¬æ–‡çš„å…¶ä½™éƒ¨åˆ†ï¼Œæˆ‘å°†ä¿æŒå±•å¼€æ·±åº¦ä¸º
    4 ä»¥æé«˜å¯è¯»æ€§ã€‚
- en: '[](#bounds-checking-elimination)Bounds-checking elimination'
  id: totrans-27
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '[](#bounds-checking-elimination)è¾¹ç•Œæ£€æŸ¥æ¶ˆé™¤'
- en: In order to keep out-of-bounds slice accesses from being a security vulnerability
    (like the famous [Heartbleed](https://en.wikipedia.org/wiki/Heartbleed) exploit),
    the go compiler inserts checks before each read. You can check it out in the [generated
    assembly](https://go.godbolt.org/z/qT3M7nPGf) (look for `runtime.panic`).
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†é˜²æ­¢è¶Šç•Œåˆ‡ç‰‡è®¿é—®æˆä¸ºå®‰å…¨æ¼æ´ï¼ˆä¾‹å¦‚è‘—åçš„[å¿ƒè„å‡ºè¡€](https://en.wikipedia.org/wiki/Heartbleed)æ¼æ´åˆ©ç”¨ï¼‰ï¼ŒGo
    ç¼–è¯‘å™¨åœ¨æ¯æ¬¡è¯»å–ä¹‹å‰æ’å…¥æ£€æŸ¥ã€‚æ‚¨å¯ä»¥åœ¨[ç”Ÿæˆçš„æ±‡ç¼–ä»£ç ](https://go.godbolt.org/z/qT3M7nPGf)ä¸­æŸ¥çœ‹ï¼ˆå¯»æ‰¾`runtime.panic`ï¼‰ã€‚
- en: 'The compiled code makes it look like we wrote something like this:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: ç¼–è¯‘åçš„ä»£ç çœ‹èµ·æ¥åƒæˆ‘ä»¬å†™äº†è¿™æ ·çš„ä¸œè¥¿ï¼š
- en: '[PRE2]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: In a hot loop like this, even with modern branch prediction, the additional
    branches per iteration can add up to a pretty significant performance penalty.
    This is especially true in our case because the inserted jumps limit how much
    we can take advantage of pipelining.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨åƒè¿™æ ·çš„çƒ­å¾ªç¯ä¸­ï¼Œå³ä½¿ä½¿ç”¨ç°ä»£åˆ†æ”¯é¢„æµ‹ï¼Œæ¯æ¬¡è¿­ä»£é¢å¤–çš„åˆ†æ”¯ä¹Ÿä¼šå¯¼è‡´ç›¸å½“å¤§çš„æ€§èƒ½æŸå¤±ã€‚åœ¨æˆ‘ä»¬çš„æƒ…å†µä¸‹å°¤å…¶å¦‚æ­¤ï¼Œå› ä¸ºæ’å…¥çš„è·³è½¬é™åˆ¶äº†æˆ‘ä»¬èƒ½å¤Ÿåˆ©ç”¨æµæ°´çº¿çš„ç¨‹åº¦ã€‚
- en: If we can convince the compiler that these reads can never be out of bounds,
    it won't insert these runtime checks. This technique is known as "bounds-checking
    elimination", and the same patterns can apply to [languages other than Go](https://github.com/Shnatsel/bounds-check-cookbook/).
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœæˆ‘ä»¬èƒ½è¯´æœç¼–è¯‘å™¨è¿™äº›è¯»å–æ°¸è¿œä¸ä¼šè¶…å‡ºè¾¹ç•Œï¼Œå®ƒå°±ä¸ä¼šæ’å…¥è¿™äº›è¿è¡Œæ—¶æ£€æŸ¥ã€‚è¿™ç§æŠ€æœ¯ç§°ä¸ºâ€œè¾¹ç•Œæ£€æŸ¥æ¶ˆé™¤â€ï¼Œç›¸åŒçš„æ¨¡å¼ä¹Ÿé€‚ç”¨äº[Go ä¹‹å¤–çš„è¯­è¨€](https://github.com/Shnatsel/bounds-check-cookbook/)ã€‚
- en: In theory, we should be able to do all checks once, outside the loop, and the
    compiler would be able to determine that all the slice indexing is safe. However,
    I couldn't find the right combination of checks to convince the compiler that
    what I'm doing is safe. I landed on a combination of asserting the lengths are
    equal and moving all the bounds checking to the top of the loop. This was enough
    to hit nearly the speed of the bounds-check-free version.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: ç†è®ºä¸Šï¼Œæˆ‘ä»¬åº”è¯¥èƒ½å¤Ÿåœ¨å¾ªç¯å¤–æ‰§è¡Œæ‰€æœ‰æ£€æŸ¥ï¼Œå¹¶ä¸”ç¼–è¯‘å™¨èƒ½å¤Ÿç¡®å®šæ‰€æœ‰åˆ‡ç‰‡ç´¢å¼•éƒ½æ˜¯å®‰å…¨çš„ã€‚ä½†æ˜¯ï¼Œæˆ‘æ‰¾ä¸åˆ°åˆé€‚çš„æ£€æŸ¥ç»„åˆæ¥è¯´æœç¼–è¯‘å™¨æˆ‘æ­£åœ¨åšçš„æ˜¯å®‰å…¨çš„ã€‚æˆ‘æœ€ç»ˆé€‰æ‹©äº†ä¸€ç§æ–¹å¼ï¼Œæ–­è¨€é•¿åº¦ç›¸ç­‰å¹¶å°†æ‰€æœ‰è¾¹ç•Œæ£€æŸ¥ç§»åŠ¨åˆ°å¾ªç¯çš„é¡¶éƒ¨ã€‚è¿™è¶³ä»¥æ¥è¿‘æ— è¾¹ç•Œæ£€æŸ¥ç‰ˆæœ¬çš„é€Ÿåº¦ã€‚
- en: '[PRE3]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: The minimizing of bounds checking nets a 9% improvement. Consistently non-zero,
    but nothing to write home about.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: å‡å°‘è¾¹ç•Œæ£€æŸ¥å¸¦æ¥äº† 9% çš„æ”¹è¿›ã€‚ä¸€ç›´ä¸ä¸ºé›¶ï¼Œä½†ä¹Ÿæ²¡ä»€ä¹ˆå¥½å†™å®¶çš„ã€‚
- en: '`DotNaive`'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '`DotNaive`'
- en: 0.94M vec/s
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 0.94M vec/s
- en: '`DotUnroll4`'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '`DotUnroll4`'
- en: 1.3M vec/s
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 1.3M vec/s
- en: '`DotBCE`'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '`DotBCE`'
- en: 1.4M vec/s
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 1.4M vec/s
- en: This technique translates well to many memory-safe compiled languages like [Rust](https://nnethercote.github.io/perf-book/bounds-checks.html).
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ç§æŠ€æœ¯åœ¨è®¸å¤šå†…å­˜å®‰å…¨çš„ç¼–è¯‘è¯­è¨€ä¸­æ•ˆæœå¾ˆå¥½ï¼Œæ¯”å¦‚[Rust](https://nnethercote.github.io/perf-book/bounds-checks.html)ã€‚
- en: 'Exercise for the reader: why is it significant that we slice like `a[i:i+4:i+4]`
    rather than just `a[i:i+4]`?'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: è¯»è€…çš„ç»ƒä¹ ï¼šä¸ºä»€ä¹ˆæˆ‘ä»¬è¦åƒ `a[i:i+4:i+4]` è¿™æ ·åˆ‡ç‰‡ï¼Œè€Œä¸åªæ˜¯ `a[i:i+4]`ï¼Ÿ
- en: '[](#quantization)Quantization'
  id: totrans-44
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '[](#quantization)é‡åŒ–'
- en: 'We''ve improved single-core search throughput by ~50% at this point, but now
    we''ve hit a new bottleneck: memory usage. Our vectors are 1536 dimensions. With
    4-byte elements, this comes out to 6KiB per vector, and we generate roughly a
    million vectors per GiB of code. That adds up quickly. We had a few customers
    come to us with some massive monorepos, and we wanted to reduce our memory usage
    so we can support those cases more cheaply.'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬ç›®å‰å·²å°†å•æ ¸æœç´¢ååé‡æé«˜äº†çº¦50%ï¼Œä½†ç°åœ¨æˆ‘ä»¬é‡åˆ°äº†ä¸€ä¸ªæ–°çš„ç“¶é¢ˆï¼šå†…å­˜ä½¿ç”¨ã€‚æˆ‘ä»¬çš„å‘é‡æ˜¯ 1536 ç»´ã€‚æ¯ä¸ªå‘é‡æœ‰ 4 å­—èŠ‚çš„å…ƒç´ ï¼Œè¿™æ„å‘³ç€æ¯ä¸ªå‘é‡å ç”¨
    6KiBï¼Œæˆ‘ä»¬å¤§çº¦æ¯ GiB ä»£ç ç”Ÿæˆä¸€ç™¾ä¸‡ä¸ªå‘é‡ã€‚è¿™å¾ˆå¿«å°±ä¼šç´¯ç§¯èµ·æ¥ã€‚æˆ‘ä»¬æœ‰ä¸€äº›å®¢æˆ·ç»™æˆ‘ä»¬å¸¦æ¥äº†ä¸€äº›åºå¤§çš„å•åº“ï¼Œæˆ‘ä»¬å¸Œæœ›å‡å°‘æˆ‘ä»¬çš„å†…å­˜ä½¿ç”¨é‡ï¼Œä»¥ä¾¿æ›´ä¾¿å®œåœ°æ”¯æŒè¿™äº›æƒ…å†µã€‚
- en: One possible mitigation would be to move the vectors to disk, but loading them
    from disk at search time can add [significant latency](https://colin-scott.github.io/personal_website/research/interactive_latency.html),
    especially on slow disks. Instead, we chose to compress our vectors with `int8`
    quantization.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€ç§å¯èƒ½çš„ç¼“è§£æªæ–½æ˜¯å°†å‘é‡ç§»åŠ¨åˆ°ç£ç›˜ä¸Šï¼Œä½†åœ¨æœç´¢æ—¶ä»ç£ç›˜åŠ è½½å¯èƒ½ä¼šå¢åŠ [æ˜¾è‘—çš„å»¶è¿Ÿ](https://colin-scott.github.io/personal_website/research/interactive_latency.html)ï¼Œç‰¹åˆ«æ˜¯åœ¨æ…¢é€Ÿç£ç›˜ä¸Šã€‚ç›¸åï¼Œæˆ‘ä»¬é€‰æ‹©äº†ç”¨
    `int8` é‡åŒ–æ¥å‹ç¼©æˆ‘ä»¬çš„å‘é‡ã€‚
- en: There are [plenty of ways](https://en.wikipedia.org/wiki/Dimensionality_reduction)
    to compress vectors, but we'll be talking about [*integer quantization*](https://huggingface.co/docs/optimum/concept_guides/quantization),
    which is relatively simple, but effective. The idea is to reduce the precision
    of the 4-byte `float32` vector elements by converting them to 1-byte `int8`s.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: æœ‰[å¾ˆå¤šç§æ–¹æ³•](https://en.wikipedia.org/wiki/Dimensionality_reduction)å¯ä»¥å‹ç¼©å‘é‡ï¼Œä½†æˆ‘ä»¬å°†è®¨è®ºç›¸å¯¹ç®€å•ä½†æœ‰æ•ˆçš„[*æ•´æ•°é‡åŒ–*](https://huggingface.co/docs/optimum/concept_guides/quantization)ã€‚å…¶æ€æƒ³æ˜¯é€šè¿‡å°†
    4 å­—èŠ‚çš„ `float32` å‘é‡å…ƒç´ è½¬æ¢ä¸º 1 å­—èŠ‚çš„ `int8` æ¥é™ä½ç²¾åº¦ã€‚
- en: 'I won''t get into exactly *how* we decide to do the translation between `float32`
    and `int8`, since that''s a pretty [deep topic](https://huggingface.co/docs/optimum/concept_guides/quantization),
    but suffice it to say our function now looks like the following:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä¸ä¼šè¯¦ç»†è®¨è®ºæˆ‘ä»¬å¦‚ä½•å†³å®šåœ¨`float32`å’Œ`int8`ä¹‹é—´è¿›è¡Œè½¬æ¢ï¼Œå› ä¸ºé‚£æ˜¯ä¸€ä¸ªç›¸å½“[æ·±å…¥çš„è¯é¢˜](https://huggingface.co/docs/optimum/concept_guides/quantization)ï¼Œä½†å¯ä»¥è¯´æˆ‘ä»¬çš„å‡½æ•°ç°åœ¨çœ‹èµ·æ¥åƒä¸‹é¢è¿™æ ·ï¼š
- en: '[PRE4]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: This change yields a 4x reduction in memory usage at the cost of some accuracy
    (which we carefully measured, but is irrelevant to this blog post).
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¸ªæ”¹å˜å¯¼è‡´å†…å­˜ä½¿ç”¨é‡å‡å°‘äº† 4 å€ï¼Œä½†ç‰ºç‰²äº†ä¸€äº›å‡†ç¡®æ€§ï¼ˆæˆ‘ä»¬è¿›è¡Œäº†ä»”ç»†çš„æµ‹é‡ï¼Œä½†è¿™ä¸æœ¬ç¯‡åšæ–‡æ— å…³ï¼‰ã€‚
- en: Unfortunately, re-running the benchmarks shows our search speed regressed a
    bit from the change. Taking a look at the generated assembly (with `go tool compile
    -S`), there are some new instructions for converting `int8` to `int32`, which
    might explain the difference. I didn't dig too deep though, since all our performance
    improvements up to this point become irrelevant in the next section.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸å¹¸çš„æ˜¯ï¼Œé‡æ–°è¿è¡ŒåŸºå‡†æµ‹è¯•æ˜¾ç¤ºæˆ‘ä»¬çš„æœç´¢é€Ÿåº¦ä»è¿™ä¸ªæ”¹å˜ä¸­æœ‰æ‰€å€’é€€ã€‚é€šè¿‡ä½¿ç”¨ `go tool compile -S` æŸ¥çœ‹ç”Ÿæˆçš„æ±‡ç¼–ä»£ç ï¼Œå¯ä»¥çœ‹åˆ°ä¸€äº›æ–°çš„æŒ‡ä»¤ç”¨äºå°†
    `int8` è½¬æ¢ä¸º `int32`ï¼Œè¿™å¯èƒ½è§£é‡Šäº†å·®å¼‚ã€‚æˆ‘æ²¡æœ‰æ·±å…¥ç ”ç©¶ï¼Œå› ä¸ºåˆ°ç›®å‰ä¸ºæ­¢æˆ‘ä»¬çš„æ‰€æœ‰æ€§èƒ½æ”¹è¿›åœ¨ä¸‹ä¸€èŠ‚ä¸­éƒ½å˜å¾—æ— å…³ç´§è¦ã€‚
- en: '`DotNaive`'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: '`DotNaive`'
- en: 0.94M vec/s
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 0.94M vec/s
- en: '`DotUnroll4`'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: '`DotUnroll4`'
- en: 1.3M vec/s
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 1.3M vec/s
- en: '`DotBCE`'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: '`DotBCE`'
- en: 1.4M vec/s
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 1.4M vec/s
- en: '`DotInt8BCE`'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: '`DotInt8BCE`'
- en: 1.2M vec/s
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 1.2M vec/s
- en: '[](#simd)SIMD'
  id: totrans-60
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '[](#simd)SIMD'
- en: The speed improvements so far were nice, but still not enough for our largest
    customers. So we started dabbling with more dramatic approaches.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: åˆ°ç›®å‰ä¸ºæ­¢ï¼Œé€Ÿåº¦çš„æ”¹å–„å¾ˆå¥½ï¼Œä½†å¯¹äºæˆ‘ä»¬æœ€å¤§çš„å®¢æˆ·æ¥è¯´è¿˜ä¸å¤Ÿã€‚æ‰€ä»¥æˆ‘ä»¬å¼€å§‹å°è¯•æ›´åŠ æˆå‰§æ€§çš„æ–¹æ³•ã€‚
- en: I always love an excuse to play with SIMD. And this problem seemed like the
    perfect nail for that hammer.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘æ€»æ˜¯å–œæ¬¢å€Ÿå£å»ç©SIMDã€‚è€Œè¿™ä¸ªé—®é¢˜ä¼¼ä¹æ˜¯ç”¨é‚£ä¸ªé”¤å­æ‰“çš„å®Œç¾çš„é’‰å­ã€‚
- en: For those unfamiliar, SIMD stands for "Single Instruction Multiple Data". Just
    like it's says, it lets you run an operation over a bunch of pieces of data with
    a single instruction. As an example, to add two `int32` vectors element-wise,
    we could add them together one by one with the `ADD` instruction and, or we can
    use the `VPADDD` instruction to add 64 pairs at a time with the [same](https://uops.info/html-instr/ADD_01_R32_R32.html)
    [latency](https://uops.info/html-instr/VPADDD_YMM_YMM_M256.html) (depending on
    the architecture).
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹äºä¸ç†Ÿæ‚‰çš„äººæ¥è¯´ï¼ŒSIMD ä»£è¡¨"å•æŒ‡ä»¤å¤šæ•°æ®"ã€‚å°±åƒå®ƒçš„åå­—ä¸€æ ·ï¼Œå®ƒè®©æ‚¨å¯ä»¥ç”¨å•ä¸ªæŒ‡ä»¤å¯¹ä¸€å †æ•°æ®æ‰§è¡Œæ“ä½œã€‚ä¸¾ä¸ªä¾‹å­ï¼Œè¦é€ä¸ªå…ƒç´ åœ°å°†ä¸¤ä¸ª`int32`å‘é‡ç›¸åŠ ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨`ADD`æŒ‡ä»¤é€ä¸ªå°†å®ƒä»¬ç›¸åŠ ï¼Œæˆ–è€…æˆ‘ä»¬å¯ä»¥ä½¿ç”¨`VPADDD`æŒ‡ä»¤ä¸€æ¬¡æ€§æ·»åŠ 64å¯¹å…ƒç´ ï¼Œå…·æœ‰[ç›¸åŒ](https://uops.info/html-instr/ADD_01_R32_R32.html)çš„[å»¶è¿Ÿ](https://uops.info/html-instr/VPADDD_YMM_YMM_M256.html)(å–å†³äºæ¶æ„)ã€‚
- en: 'We have a problem though. Go does not expose SIMD intrinsics like [C](https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html)
    or [Rust](https://doc.rust-lang.org/beta/core/simd/index.html). We have two options
    here: write it in C and use Cgo, or write it by hand for Go''s assembler. I try
    hard to avoid Cgo whenever possible for [many reasons that are not at all original](https://dave.cheney.net/2016/01/18/cgo-is-not-go),
    but one of those reasons is that Cgo imposes a performance penalty, and performance
    of this snippet is paramount. Also, getting my hands dirty with some assembly
    sounds fun, so that''s what I''m going to do.'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: ä½†æ˜¯æˆ‘ä»¬æœ‰ä¸€ä¸ªé—®é¢˜ã€‚Go ä¸åƒ[C](https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html)æˆ–[Rust](https://doc.rust-lang.org/beta/core/simd/index.html)é‚£æ ·æš´éœ²SIMDå†…è”å‡½æ•°ã€‚æˆ‘ä»¬æœ‰ä¸¤ä¸ªé€‰æ‹©ï¼šç”¨Cç¼–å†™å¹¶ä½¿ç”¨Cgoï¼Œæˆ–è€…ä¸ºGoçš„æ±‡ç¼–å™¨æ‰‹åŠ¨ç¼–å†™ã€‚æˆ‘å°½é‡é¿å…ä½¿ç”¨Cgoï¼Œå› ä¸ºæœ‰[è®¸å¤šå¹¶éåŸåˆ›çš„åŸå› ](https://dave.cheney.net/2016/01/18/cgo-is-not-go)ï¼Œä½†å…¶ä¸­ä¸€ä¸ªåŸå› æ˜¯Cgoä¼šå¯¹æ€§èƒ½é€ æˆå½±å“ï¼Œè€Œè¿™æ®µä»£ç çš„æ€§èƒ½è‡³å…³é‡è¦ã€‚æ­¤å¤–ï¼Œç©ä¸€äº›æ±‡ç¼–å¬èµ·æ¥å¾ˆæœ‰è¶£ï¼Œæ‰€ä»¥æˆ‘ä¼šè¿™ä¹ˆåšã€‚
- en: I want this routine to be reasonably portable, so I'm going to restrict myself
    to only AVX2 instructions, which are supported on most `x86_64` server CPUs these
    days. We can use [runtime feature detection](https://sourcegraph.com/github.com/sourcegraph/sourcegraph@3ac2170c6523dd074835919a1804f197cf86e451/-/blob/internal/embeddings/dot_amd64.go?L17-21)
    to fall back to a slower option in pure Go.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘å¸Œæœ›è¿™ä¸ªä¾‹ç¨‹å…·æœ‰ç›¸å½“çš„å¯ç§»æ¤æ€§ï¼Œå› æ­¤æˆ‘å°†é™åˆ¶è‡ªå·±ä»…ä½¿ç”¨ AVX2 æŒ‡ä»¤ï¼Œè¿™äº›æŒ‡ä»¤åœ¨å¤§å¤šæ•°å½“å‰çš„`x86_64`æœåŠ¡å™¨CPUä¸Šéƒ½å—æ”¯æŒã€‚æˆ‘ä»¬å¯ä»¥ä½¿ç”¨[è¿è¡Œæ—¶ç‰¹æ€§æ£€æµ‹](https://sourcegraph.com/github.com/sourcegraph/sourcegraph@3ac2170c6523dd074835919a1804f197cf86e451/-/blob/internal/embeddings/dot_amd64.go?L17-21)åœ¨çº¯
    Go ä¸­é€€å›åˆ°ä¸€ä¸ªè¾ƒæ…¢çš„é€‰é¡¹ã€‚
- en: <details><summary>Full code for `DotAVX2`</summary>
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: <details><summary>`DotAVX2`çš„å®Œæ•´ä»£ç </summary>
- en: '[PRE5]</details>'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '[PRE5]</details>'
- en: 'The core loop of the implementation depends on three main instructions:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: å®ç°çš„æ ¸å¿ƒå¾ªç¯ä¾èµ–äºä¸‰ä¸ªä¸»è¦æŒ‡ä»¤ï¼š
- en: '[`VPMOVSXBW`](https://www.felixcloutier.com/x86/pmovsx), which loads `int8`s
    into a vector `int16`s'
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[`VPMOVSXBW`](https://www.felixcloutier.com/x86/pmovsx)ï¼Œå®ƒå°†`int8`åŠ è½½åˆ°å‘é‡`int16`ä¸­'
- en: '[`VPMADDWD`](https://www.felixcloutier.com/x86/pmaddwd), which multiplies two
    `int16` vectors element-wise, then adds fuzzy stack. together adjacent pairs to
    produce a vector of `int32`s'
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[`VPMADDWD`](https://www.felixcloutier.com/x86/pmaddwd)ï¼Œå®ƒå°†ä¸¤ä¸ª`int16`å‘é‡é€å…ƒç´ ç›¸ä¹˜ï¼Œç„¶åå°†ç›¸é‚»å¯¹ç»„åˆåœ¨ä¸€èµ·ä»¥ç”Ÿæˆä¸€ä¸ª`int32`å‘é‡'
- en: '[`VPADDD`](https://www.felixcloutier.com/x86/paddb:paddw:paddd:paddq), which
    accumulates the resulting `int32` vector into our running sum'
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[`VPADDD`](https://www.felixcloutier.com/x86/paddb:paddw:paddd:paddq)ï¼Œå®ƒå°†ç»“æœ`int32`å‘é‡ç´¯åŠ åˆ°æˆ‘ä»¬çš„è¿è¡Œæ€»å’Œ'
- en: '`VPMADDWD` is a real heavy lifter here. By combining the multiply and add steps
    into one, not only does it save instructions, it also helps us avoid overflow
    issues by simultaneously widening the result to an `int32`.'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: '`VPMADDWD` åœ¨è¿™é‡Œèµ·åˆ°äº†é‡è¦ä½œç”¨ã€‚é€šè¿‡å°†ä¹˜æ³•å’ŒåŠ æ³•æ­¥éª¤åˆå¹¶ä¸ºä¸€æ­¥ï¼Œå®ƒä¸ä»…èŠ‚çœäº†æŒ‡ä»¤ï¼Œè¿˜é€šè¿‡åŒæ—¶å°†ç»“æœæ‰©å±•ä¸º`int32`æ¥å¸®åŠ©æˆ‘ä»¬é¿å…æº¢å‡ºé—®é¢˜ã€‚'
- en: Let's see what this earned us.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬çœ‹çœ‹è¿™ç»™æˆ‘ä»¬å¸¦æ¥äº†ä»€ä¹ˆã€‚
- en: '`DotNaive`'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: '`DotNaive`'
- en: 0.94M vec/s
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 0.94M vec/s
- en: '`DotUnroll4`'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: '`DotUnroll4`'
- en: 1.3M vec/s
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 1.3M vec/s
- en: '`DotBCE`'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: '`DotBCE`'
- en: 1.4M vec/s
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 1.4M vec/s
- en: '`DotInt8BCE`'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: '`DotInt8BCE`'
- en: 1.2M vec/s
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 1.2M vec/s
- en: '`DotAVX2`'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: '`DotAVX2`'
- en: 7.0M vec/s
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 7.0M vec/s
- en: Woah, that's a 530% increase in throughput from our previous best! SIMD for
    the win ğŸš€
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: å“‡ï¼Œè¿™æ¯”æˆ‘ä»¬ä¹‹å‰çš„æœ€ä½³ç»“æœæé«˜äº†530ï¼…ï¼SIMD èƒœåˆ© ğŸš€
- en: 'Now, it wasn''t all sunshine and rainbows. Hand-writing assembly in Go is weird.
    It uses a [custom assembler](https://go.dev/doc/asm), which means that its assembly
    language looks just-different-enough-to-be-confusing compared to the assembly
    snippets you usually find online. It has some weird quirks like [changing the
    order of instruction operands](https://www.quasilyte.dev/blog/post/go-asm-complementary-reference/#operands-order)
    or [using different names for instructions](https://www.quasilyte.dev/blog/post/go-asm-complementary-reference/#mnemonics).
    Some instructions don''t even *have* names in the go assembler and can only be
    used via their [binary encoding](https://go.dev/doc/asm#unsupported_opcodes).
    Shameless plug: I found [sourcegraph.com](https://sourcegraph.com/search) invaluable
    for finding examples of Go assembly to draw from.'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨ï¼Œä¸æ˜¯ä¸€åˆ‡éƒ½æ˜¯é˜³å…‰å’Œå½©è™¹çš„ã€‚åœ¨ Go ä¸­æ‰‹å†™æ±‡ç¼–å¾ˆå¥‡æ€ªã€‚å®ƒä½¿ç”¨ä¸€ä¸ª [è‡ªå®šä¹‰æ±‡ç¼–å™¨](https://go.dev/doc/asm)ï¼Œè¿™æ„å‘³ç€å®ƒçš„æ±‡ç¼–è¯­è¨€çœ‹èµ·æ¥ä¸æ‚¨é€šå¸¸åœ¨çº¿æ‰¾åˆ°çš„æ±‡ç¼–ç‰‡æ®µæœ‰ä¸€ç‚¹ç‚¹ä¸åŒï¼Œè¶³ä»¥è®©äººæ„Ÿåˆ°å›°æƒ‘ã€‚å®ƒæœ‰ä¸€äº›å¥‡æ€ªçš„æ€ªç™–ï¼Œæ¯”å¦‚
    [æ”¹å˜æŒ‡ä»¤æ“ä½œæ•°çš„é¡ºåº](https://www.quasilyte.dev/blog/post/go-asm-complementary-reference/#operands-order)
    æˆ– [ä½¿ç”¨ä¸åŒçš„æŒ‡ä»¤åç§°](https://www.quasilyte.dev/blog/post/go-asm-complementary-reference/#mnemonics)ã€‚åœ¨
    go æ±‡ç¼–å™¨ä¸­ï¼Œæœ‰äº›æŒ‡ä»¤ç”šè‡³ *æ²¡æœ‰* åç§°ï¼Œåªèƒ½é€šè¿‡å®ƒä»¬çš„ [äºŒè¿›åˆ¶ç¼–ç ](https://go.dev/doc/asm#unsupported_opcodes)
    ä½¿ç”¨ã€‚åšé¢œæ— è€»çš„æ¨å¹¿ï¼šæˆ‘å‘ç° [sourcegraph.com](https://sourcegraph.com/search) å¯¹äºæŸ¥æ‰¾ Go æ±‡ç¼–ç¤ºä¾‹éå¸¸æœ‰ç”¨ã€‚
- en: That said, compared to Cgo, there are some nice benefits. Debugging still works
    well, the assembly can be stepped through, and registers can be inspected using
    `delve`. There are no extra build steps (a C toolchain doesn't need to be set
    up). It's easy to set up a pure-Go fallback so cross-compilation still works.
    Common problems are caught by `go vet`.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: å°½ç®¡å¦‚æ­¤ï¼Œä¸ Cgo ç›¸æ¯”ï¼Œè¿˜æ˜¯æœ‰ä¸€äº›å¥½å¤„çš„ã€‚è°ƒè¯•ä»ç„¶å¾ˆæœ‰æ•ˆï¼Œæ±‡ç¼–ä»£ç å¯ä»¥é€æ­¥æ‰§è¡Œï¼Œå¹¶ä¸”å¯ä»¥ä½¿ç”¨ `delve` æ£€æŸ¥å¯„å­˜å™¨ã€‚æ²¡æœ‰é¢å¤–çš„æ„å»ºæ­¥éª¤ï¼ˆä¸éœ€è¦è®¾ç½®
    C å·¥å…·é“¾ï¼‰ã€‚å¾ˆå®¹æ˜“è®¾ç½®ä¸€ä¸ªçº¯ Go çš„åå¤‡ï¼Œä»¥ä¾¿è·¨å¹³å°ç¼–è¯‘ä»ç„¶æœ‰æ•ˆã€‚å¸¸è§é—®é¢˜ç”± `go vet` æ•è·ã€‚
- en: '[](#simdbut-bigger)SIMD...but bigger'
  id: totrans-87
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '[](#simdbut-bigger)SIMD...ä½†æ›´å¤§'
- en: Previously, we limited ourselves to AVX2, but what if we *didn't*? The VNNI
    extension to AVX-512 added the [`VPDPBUSD`](https://www.felixcloutier.com/x86/vpdpbusd)
    instruction, which computes the dot product on `int8` vectors rather than `int16`s.
    This means we can process four times as many elements in a single instruction
    because we don't have to convert to `int16` first and our vector width doubles
    with AVX-512!
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: ä»¥å‰ï¼Œæˆ‘ä»¬å±€é™äº AVX2ï¼Œä½†å¦‚æœæˆ‘ä»¬ *ä¸* è¿™æ ·åšå‘¢ï¼Ÿ AVX-512 çš„ VNNI æ‰©å±•æ·»åŠ äº† [`VPDPBUSD`](https://www.felixcloutier.com/x86/vpdpbusd)
    æŒ‡ä»¤ï¼Œå®ƒè®¡ç®— `int8` å‘é‡è€Œä¸æ˜¯ `int16`ã€‚è¿™æ„å‘³ç€æˆ‘ä»¬å¯ä»¥åœ¨ä¸€æ¡æŒ‡ä»¤ä¸­å¤„ç†å››å€æ•°é‡çš„å…ƒç´ ï¼Œå› ä¸ºæˆ‘ä»¬ä¸å¿…å…ˆè½¬æ¢ä¸º `int16`ï¼Œå¹¶ä¸”æˆ‘ä»¬çš„çŸ¢é‡å®½åº¦åœ¨
    AVX-512 ä¸­åŠ å€ï¼
- en: The only problem is that the instruction requires one vector to be signed bytes,
    and the other to be *unsigned* bytes. Both of our vectors are signed. We can employ
    [a trick from Intel's developer guide](https://www.intel.com/content/www/us/en/docs/onednn/developer-guide-reference/2023-0/nuances-of-int8-computations.html#DOXID-DEV-GUIDE-INT8-COMPUTATIONS-1DG-I8-COMP-S12)
    to help us out. Given two `int8` elements, `a[n]` and `b[n]`, we do the element-wise
    calculation as `a[n]* (b[n] + 128) - a[n] * 128`. The `a[n] * 128` term is the
    overshoot from adding 128 to bump `b[n]` into `u8` range. We keep track of that
    separately and subtract it at the end. Each of the operations in that expression
    can be vectorized.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: å”¯ä¸€çš„é—®é¢˜æ˜¯æŒ‡ä»¤è¦æ±‚ä¸€ä¸ªå‘é‡ä¸ºæœ‰ç¬¦å·å­—èŠ‚ï¼Œå¦ä¸€ä¸ªä¸º *æ— ç¬¦å·* å­—èŠ‚ã€‚æˆ‘ä»¬çš„ä¸¤ä¸ªå‘é‡éƒ½æ˜¯æœ‰ç¬¦å·çš„ã€‚æˆ‘ä»¬å¯ä»¥é‡‡ç”¨ [è‹±ç‰¹å°”å¼€å‘è€…æŒ‡å—ä¸­çš„æŠ€å·§](https://www.intel.com/content/www/us/en/docs/onednn/developer-guide-reference/2023-0/nuances-of-int8-computations.html#DOXID-DEV-GUIDE-INT8-COMPUTATIONS-1DG-I8-COMP-S12)
    æ¥å¸®åŠ©æˆ‘ä»¬ã€‚ç»™å®šä¸¤ä¸ª `int8` å…ƒç´ ï¼Œ`a[n]` å’Œ `b[n]`ï¼Œæˆ‘ä»¬æŒ‰å…ƒç´ è®¡ç®—ä¸º `a[n]* (b[n] + 128) - a[n] * 128`ã€‚`a[n]
    * 128` é¡¹æ˜¯ä»æ·»åŠ  128 ä»¥å°† `b[n]` å¢åŠ åˆ° `u8` èŒƒå›´çš„è¶…é¢ã€‚æˆ‘ä»¬å•ç‹¬è·Ÿè¸ªå®ƒï¼Œå¹¶åœ¨æœ€åå‡å»å®ƒã€‚è¯¥è¡¨è¾¾å¼ä¸­çš„æ¯ä¸ªæ“ä½œéƒ½å¯ä»¥çŸ¢é‡åŒ–ã€‚
- en: <details><summary>Full code for `DotVNNI`</summary>
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: <details><summary>`DotVNNI` çš„å®Œæ•´ä»£ç </summary>
- en: '[PRE6]</details>'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: '[PRE6]</details>'
- en: This implementation yielded another 21% improvement. Not bad!
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¸ªå®ç°äº§ç”Ÿäº†å¦å¤– 21% çš„æ”¹è¿›ã€‚è¿˜ä¸é”™ï¼
- en: '`DotNaive`'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: '`DotNaive`'
- en: 0.94M vec/s
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 0.94M å‘é‡/ç§’
- en: '`DotUnroll4`'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: '`DotUnroll4`'
- en: 1.3M vec/s
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 1.3M å‘é‡/ç§’
- en: '`DotBCE`'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: '`DotBCE`'
- en: 1.4M vec/s
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 1.4M å‘é‡/ç§’
- en: '`DotInt8BCE`'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: '`DotInt8BCE`'
- en: 1.2M vec/s
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 1.2M å‘é‡/ç§’
- en: '`DotAVX2`'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: '`DotAVX2`'
- en: 7.0M vec/s
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 7.0M å‘é‡/ç§’
- en: '`DotVNNI`'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: '`DotVNNI`'
- en: 8.8M vec/s
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 8.8M å‘é‡/ç§’
- en: '[](#whats-next)What''s next?'
  id: totrans-105
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '[](#whats-next)æ¥ä¸‹æ¥æ˜¯ä»€ä¹ˆï¼Ÿ'
- en: Well, I'm pretty happy with an 9.3x increase in throughput and a 4x reduction
    in memory usage, so I'll probably leave it here.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: å—¯ï¼Œæˆ‘å¯¹ååé‡å¢åŠ äº† 9.3 å€å’Œå†…å­˜ä½¿ç”¨å‡å°‘äº† 4 å€éå¸¸æ»¡æ„ï¼Œæ‰€ä»¥æˆ‘å¯èƒ½ä¼šå°±æ­¤ç»“æŸã€‚
- en: The real life answer here is probably "use an index". There is a ton of good
    work out there focused on making nearest neighbor search fast, and there are plenty
    of batteries-included vector DBs that make it pretty easy to deploy.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™é‡ŒçœŸå®çš„ç­”æ¡ˆå¯èƒ½æ˜¯â€œä½¿ç”¨ç´¢å¼•â€ã€‚æœ‰å¾ˆå¤šå…³äºä½¿æœ€è¿‘é‚»æœç´¢å¿«é€Ÿçš„ä¼˜ç§€å·¥ä½œï¼Œè€Œä¸”æœ‰å¾ˆå¤šå†…ç½®ç”µæ± çš„çŸ¢é‡æ•°æ®åº“ï¼Œä½¿å¾—éƒ¨ç½²å˜å¾—ç›¸å½“å®¹æ˜“ã€‚
- en: '*However*, if you want some fun food for thought, a colleague of mine built
    a proof-of-concept [dot product on the GPU](https://github.com/sourcegraph/sourcegraph/compare/simd-post-gpu-embeddings~3...simd-post-gpu-embeddings).'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: '*ç„¶è€Œ*ï¼Œå¦‚æœä½ æƒ³è¦ä¸€äº›æœ‰è¶£çš„æ€è€ƒé£Ÿç‰©ï¼Œæˆ‘çš„ä¸€ä¸ªåŒäº‹æ„å»ºäº†ä¸€ä¸ªæ¦‚å¿µéªŒè¯çš„ [GPU ä¸Šçš„ç‚¹ç§¯](https://github.com/sourcegraph/sourcegraph/compare/simd-post-gpu-embeddings~3...simd-post-gpu-embeddings)ã€‚'
- en: '[](#bonus-material)Bonus material'
  id: totrans-109
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '[](#bonus-material)å¥–åŠ±ææ–™'
- en: If you haven't used [benchstat](https://pkg.go.dev/golang.org/x/perf/cmd/benchstat),
    you should. It's great. Super simple statistical comparison of benchmark results.
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å¦‚æœä½ è¿˜æ²¡æœ‰ä½¿ç”¨ [benchstat](https://pkg.go.dev/golang.org/x/perf/cmd/benchstat)ï¼Œä½ åº”è¯¥è¯•è¯•ã€‚å®ƒå¾ˆæ£’ã€‚å¯¹åŸºå‡†æµ‹è¯•ç»“æœè¿›è¡Œè¶…çº§ç®€å•çš„ç»Ÿè®¡æ¯”è¾ƒã€‚
- en: Don't miss the [compiler explorer](https://go.godbolt.org/z/qT3M7nPGf), which
    is an extremely useful tool for digging into compiler codegen.
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: åˆ«é”™è¿‡ [ç¼–è¯‘å™¨æµè§ˆå™¨](https://go.godbolt.org/z/qT3M7nPGf)ï¼Œè¿™æ˜¯ä¸€ä¸ªéå¸¸æœ‰ç”¨çš„å·¥å…·ï¼Œå¯ä»¥æ·±å…¥ç ”ç©¶ç¼–è¯‘å™¨ä»£ç ç”Ÿæˆã€‚
- en: There's also that time I got [nerd sniped](https://twitter.com/sluongng/status/1654066471230636033)
    into implementing [a version with ARM NEON](https://github.com/camdencheek/simd_blog/blob/main/dot_arm64.s),
    which made for some interesting comparisons.
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: è¿˜æœ‰é‚£ä¸ªè®©æˆ‘è¢« [nerd sniped](https://twitter.com/sluongng/status/1654066471230636033)
    å»å®ç° [å¸¦æœ‰ ARM NEON çš„ç‰ˆæœ¬](https://github.com/camdencheek/simd_blog/blob/main/dot_arm64.s)
    çš„æ—¶åˆ»ï¼Œè¿™å¼•å‘äº†ä¸€äº›æœ‰è¶£çš„æ¯”è¾ƒã€‚
- en: If you haven't come across it, the [Agner Fog Instruction Tables](https://www.agner.org/optimize/)
    make for some great reference material for low-level optimizations. For this work,
    I used them to help grok differences instruction latencies and why some pipeline
    better than others.
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å¦‚æœä½ è¿˜æ²¡æœ‰æ¥è§¦è¿‡ï¼Œ[Agner Fog æŒ‡ä»¤è¡¨](https://www.agner.org/optimize/) æ˜¯ä¸€äº›ç”¨äºä½çº§ä¼˜åŒ–çš„æå¥½çš„å‚è€ƒèµ„æ–™ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ç”¨å®ƒä»¬æ¥å¸®åŠ©ç†è§£ä¸åŒæŒ‡ä»¤çš„å»¶è¿Ÿä»¥åŠä¸ºä»€ä¹ˆæœ‰äº›ç®¡é“æ¯”å…¶ä»–ç®¡é“æ›´å¥½ã€‚
