<!--yml

类别：未分类

时间：2024-05-27 14:32:38

-->

# 2023 年值得关注的 10 篇人工智能研究论文

> 来源：[`magazine.sebastianraschka.com/p/10-ai-research-papers-2023`](https://magazine.sebastianraschka.com/p/10-ai-research-papers-2023)

今年的感觉明显不同。我在机器学习和人工智能领域工作了十多年，但我记不起还有哪个时期这些领域像今年这样受欢迎和快速发展过。

总结一个充满事件的 2023 年的机器学习和人工智能研究，我很高兴分享我今年阅读的 10 篇值得关注的论文。我的个人重点更多地放在大型语言模型上，所以今年你会发现大型语言模型（LLM）论文比计算机视觉论文更加重视。

我抵制将这篇文章标记为“2023 年顶级人工智能研究论文”，因为确定“最佳”论文是主观的。选择标准基于我特别喜欢或认为有影响力并值得注意的论文的混合。（排序顺序是推荐阅读顺序，而不是按 perceived quality or impact 排序。）

**顺便说一下，如果你向下滚动到本文的结尾，你会发现一个小惊喜。感谢你们的支持，祝你们新年快乐！**

通过 ***[Pythia: A Suite for Analyzing Large Language Models Across Training and Scaling](https://arxiv.org/abs/2304.01373)***，研究人员最初发布了 8 个 LLM，参数范围从 70M 到 12B（同时公开发布权重和数据，这是罕见的）。

但在我看来，这篇论文的突出特点是他们还公布了训练细节、分析和见解（其中一些显示在下面的注释图中）。

以下是 Pythia 论文所回答的一些问题：

1.  在重复数据上进行预训练（即，进行>1 个 epoch 的训练）是否会有所不同？事实证明，去重并不会对性能造成益处或伤害。

1.  训练顺序是否影响记忆？不幸的是，事实证明确实如此。“不幸的是”，因为如果这是真的，我们可以通过重新排序训练数据来减轻不良的逐字记忆问题。

1.  预训练的词频是否影响任务性能？是的，频率较高的词的少量精度往往更高。

1.  增加批量大小是否影响训练效率和模型收敛性？批量大小加倍可以将训练时间减半，但不会影响收敛。

今天，仅仅六个月后，大型语言模型已经不再是突破性的。然而，我将这篇论文包含在内，是因为它不仅尝试回答有关训练设置的有趣问题，而且还是有关细节和透明度的积极例子。此外，<1B 范围内的小型 LLM 是进行小型研究和调试的良好模板，或者是预训练实验的起点（这里是他们的[GitHub 存储库](https://github.com/EleutherAI/pythia)的链接）。

我对 2024 年的期望是我们看到更多类似这样的研究和在接下来的一年里写得更好的论文！

***[Llama 2: 开放基础与精细调整的聊天模型](https://arxiv.org/abs/2307.09288)*** 是 Meta 热门第一篇 Llama 论文的后续论文。

Llama 2 模型，参数范围从 7B 到 70B，是本文被列入此列表的原因之一：这些仍然是最具能力且广泛使用的开放可用模型之一。值得注意的是，[Llama 2 许可证](https://github.com/facebookresearch/llama/blob/main/LICENSE) 还允许在商业应用中使用（有关详情，请参阅[访问请求页面](https://ai.meta.com/resources/models-and-libraries/llama-downloads/)）。

在模型方面，Llama 2 套件与许多其他 LLMs 的不同之处在于，模型提供了标准预训练模型和聊天模型，经过增强学习和人类反馈（RLHF，创建 ChatGPT 的方法）微调，以遵循类似 ChatGPT 的人类指令 —— 进行 RLHF 微调的模型仍然很少见。

关于 RLHF 及其在 Llama 2 中的使用的更多详情，请参阅下文的我撰写的更全面的独立文章。

除了 Llama 2 模型被广泛使用且带有 RLHF 指导微调变体这一事实之外，我决定将该论文列入此列表的另一个原因是其配套的深度研究报告共计 77 页。

在这里，作者还很好地说明了 Llama 2 70B 聊天模型的演变，追溯了它们从最初的监督微调（SFT-v1）到最终的 PPO RLHF 微调阶段（RLHF-v5）的过程。该图反映了在有害性和有帮助性两个方面的持续改善，如下所示的注释图所示。

*从 Llama 2 论文中的注释图表 ([`arxiv.org/abs/2307.09288`](https://arxiv.org/abs/2307.09288)) 可见，从第一次监督微调模型（SFT-1）到最终的 RLHF 微调聊天模型（RLHF-v5）的性能逐步提升。*

尽管像 Mistral-8x7B（稍后详述）、DeepSeek-67B 和 YI-34B 等模型在公共基准测试中超越了更大型的 Llama-2-70B 模型，但在公开可用 LLMs 和在其基础上开发方法方面，Llama 2 仍然是一个常见和流行的选择。

此外，尽管一些基准测试表明可能存在更好的模型，但今年面临的一个更大挑战是基准测试的可信度。例如，我们怎么知道模型没有在这些基准测试上进行训练，分数也没有被夸大？在经典机器学习中，当有人提出一个新的梯度提升模型时，很容易重现结果并进行检查。如今，考虑到训练 LLMs 的成本和复杂性（以及大多数研究人员要么不披露架构，要么不披露训练数据的细节），这是不可能的。

总之，尽管其他主要公司现在都在推出自己的专有大型语言模型（如 Google 的 Bard 和 Gemini、亚马逊的 Q 以及 Twitter/X 的 Grok，以及 OpenAI 的 ChatGPT），但看到 Meta 在开源方面加倍努力令人耳目一新。

***[QLoRA：量化 LLMs 的高效微调](https://arxiv.org/abs/2305.14314)*** 是今年 LLM 研究和微调社区中最受欢迎的技术之一，因为它使已经流行的 LoRA（低秩调整）技术更加内存高效。简而言之，这意味着你可以将更大的模型适配到更小的 GPU 上。

*常规 LoRA 的简短视觉总结*

QLoRA 代表量化 LoRA（低秩调整）。标准的 LoRA 方法通过向模型层的权重中添加低秩矩阵来修改预训练的 LLM。这些矩阵较小，因此在微调过程中需要较少的资源来更新。

在 QLoRA 中，这些低秩矩阵被量化，意味着它们的数值精度降低。这是通过将这些矩阵中的值的连续范围映射到有限的一组离散级别来实现的。这个过程减少了模型的内存占用和计算需求，因为对低精度数值的操作不那么内存密集。

根据[QLoRA 论文](https://arxiv.org/abs/2305.14314)，QLoRA 将 65B Llama 模型的内存需求降低到适合单个 48 GB GPU（例如 A100）的程度。通过对 65B Llama 进行量化的 4 位训练，获得的 65B Guanaco 模型在完整的 16 位微调任务性能方面保持不变，仅在微调 24 小时后即可达到 ChatGPT 性能的 99.3%。

今年我也运行了许多 QLoRA 实验，并发现 QLoRA 在微调过程中降低 GPU 内存需求非常方便。不过，存在一个折衷：额外的量化步骤会导致额外的计算开销，意味着训练速度会比常规 LoRA 稍慢一些。

*摘自我之前写过的关于我的 LoRA 和 QLoRA 实验的文章 [这里](https://magazine.sebastianraschka.com/p/practical-tips-for-finetuning-llms)*

随着研究人员和实践者致力于创建定制 LLMs，LLM 微调仍然如此重要。我赞赏像 QLoRA 这样的技术，它们通过降低 GPU 内存需求门槛来帮助使这一过程更加可行。

从今年发布的所有论文来看，***[BloombergGPT：金融领域的大型语言模型](https://arxiv.org/abs/2303.17564)*** 在前十名榜单中看起来可能是一个奇怪的选择，因为它没有带来突破性的新见解、方法论或开源模型。

我包含这个案例是因为它是一个有趣的案例研究，有人在一个特定领域的数据集上预训练了一个相对较大的语言模型。此外，该描述相当详尽，这在论文中变得越来越少见。尤其是当涉及到公司雇员的论文时--今年的一个趋势是，主要公司在保护商业机密以在这个竞争激烈的环境中保留贸易秘密时，越来越秘密地关于架构或数据集的细节（附言：我不因此而指责他们）。

此外，BloombergGPT 让我想到了我们可以预训练和微调模型在特定领域数据上的不同方法，如下图所示（请注意，这在 BloombergGPT 论文中没有探讨，但未来对此进行研究将是很有趣的）。

*语言模型的不同预训练和微调方法。*

简而言之，BloombergGPT 是一个用于金融领域的 500 亿参数的语言模型，训练数据来自 3630 亿金融数据令牌和 3450 亿来自一般公开数据集的令牌。作为比较，GPT-3 的规模是其 3.5 倍（1750 亿参数），但是训练数据令牌却少了 1.4 倍（4990 亿）。

作者为什么使用了一个“只有”500 亿参数的架构，而 GPT-3 的规模是其 3.5 倍？这个问题更容易回答。他们采用了金丝猴缩放定律，并发现这是一个很好的规模，考虑到金融数据的可用规模。

是否值得从头开始（预）训练 LLM 来合并数据集？根据论文，该模型在目标领域表现得非常好。然而，我们不知道它是否优于 a) 在领域特定数据上进一步预训练一个预训练模型或 b) 在领域特定数据上微调一个预训练模型。

尽管上面有些批评，总的来说，这是一篇有趣的论文，为特定领域的 LLM 提供了一个有趣的案例研究和示例；此外，它为预训练与微调将知识灌输到 LLM 中留下了进一步研究的空间。

（附言：对于那些对比微调感兴趣的人，正如[Rohan Paul 分享](https://x.com/rohanpaul_ai/status/1738474868214235163?s=20)给我看的，这个“小”[AdaptLLM-7B](https://arxiv.org/abs/2309.09530)模型在一个数据集上的表现优于 BloombergGPT，并几乎与其在其他三个金融数据集上的表现相匹配。尽管 BloombergGPT 在整体上似乎稍微更好，但值得注意的是，训练 AdaptLLM-7B 大约只需花费 100 美元，而 BloombergGPT 则是多次百万美元的投资。）

在讨论***[直接偏好优化：您的语言模型实际上是一个奖励模型](https://arxiv.org/abs/2305.18290)***论文之前，让我们退一步，讨论它旨在取代的方法，即来自人类反馈的强化学习（RLHF）。

RLHF 是 ChatGPT 和 Llama 2 聊天模型背后的主要技术。在 RLHF 中，我们使用一个多步骤的过程，我在 [单独的文章](https://magazine.sebastianraschka.com/p/llm-training-rlhf-and-its-alternatives) 中详细描述了这个过程：

1.  监督微调：模型最初在包含指令和期望响应的数据集上进行训练。

1.  奖励建模：人类评分员对模型的输出提供反馈。这些反馈用于创建一个奖励模型，该模型学习预测哪些输出是优选的。

1.  近端策略优化（PPO）：模型生成输出，奖励模型对每个输出进行评分。PPO 算法使用这些分数来调整模型的策略以...

生成更高质量的输出。（这是一种用于微调模型策略的强化学习算法。）

*监督指令微调步骤的数据集中的两个训练示例。请注意，“输入”是可选的。*

尽管 RLHF 很受欢迎且有效，正如我们在 ChatGPT 和 Llama 2 中所见，但实施起来也相当复杂且挑剔。

[直接偏好优化（DPO）论文](https://arxiv.org/abs/2305.18290) 提出了一种优化语言模型以符合人类偏好的算法，**而无需** 明确的奖励建模或强化学习。相反，DPO 使用简单的分类目标。

在 DPO 中，我们仍然保留了监督微调步骤（上面的第一步），但我们用一个单一步骤来进一步微调模型的偏好数据，以替换步骤 2 和 3。换句话说，DPO 完全跳过了 RLHF 所需的奖励模型创建步骤，这显著简化了微调过程。

它的效果如何？直到最近，使用 DPO 训练的模型并不多见。（这是有道理的，因为 DPO 也是一种相对较新的方法。）然而，最近的一个例子是 *[Zephyr: Direct Distillation of LM Alignment](https://arxiv.org/abs/2310.16944)* 中描述的 Zephyr 7B 模型。Zephyr-7B 基于一个基础 LLM Mistral-7B，该基础 LLM 使用 DPO 进行了微调。（稍后将详细介绍 Mistral。）

如下的性能表格显示，在发布时，7B 参数的 Zephyr 模型的性能优于其尺寸类别中的所有其他模型。更令人印象深刻的是，Zephyr-7B 甚至在对话性 [MT-Bench](https://arxiv.org/abs/2306.05685) 基准测试中也超过了规模是其 10 倍的 70B 参数的 Llama 2 聊天模型。

总之，DPO 论文的吸引力在于其方法的简单性。使用 RLHF 进行训练的聊天模型的稀缺性，以 Llama 2 为显著例外，很可能归因于 RLHF 方法的复杂性。鉴于此，我认为可以预期在未来一年中 DPO 模型的采用将会增加。

我必须承认 ***[Mistral 7B 论文](https://arxiv.org/abs/2310.06825)*** 并不是我最喜欢的论文之一，因为它篇幅较短。但是，它提出的模型却具有相当大的影响力。

我决定将这篇论文列入列表，因为 Mistral 7B 模型不仅在发布时非常流行，而且作为基础模型，引发了另外两个值得注意的模型的发展：Zephyr 7B 和最新的 Mistral Mixture of Experts（MoE）方法。这些模型是我预见到的小型 LLMs 的趋势的良好示例，至少在 2024 年上半年是如此。

在讨论 Zephyr 7B 和 Mistral MoE 模型之前，让我们简要谈谈 Mistral 7B 本身。

简而言之，Mistral 7B 论文介绍了一个紧凑而强大的语言模型，尽管其规模相对较小，只有 70 亿个标记，但在各种基准测试中仍然优于其较大的对应物，如 13B Llama 2 模型。（紧邻两倍大的[Qwen 14B](https://github.com/QwenLM/Qwen)，Mistral 7B 也是今年[NeurIPS LLM Finetuning & Efficiency challenge](https://llm-efficiency-challenge.github.io/leaderboard)获胜解决方案中使用的基础模型。）

为什么它如此出色的原因尚不清楚，但可能是由于其训练数据。既不 Llama 2 也不 Mistral 公开其训练数据，因此我们只能推测。

就架构而言，该模型与 Llama 2 共享组查询注意。尽管与 Llama 2 非常相似，但 Mistral 架构的一个有趣的补充是滑动窗口注意，以节省内存并提高计算吞吐量，从而加快训练速度。（滑动窗口注意先前在[Child et al. 2019](https://arxiv.org/abs/1904.10509)和[Beltagy et al. 2020](https://arxiv.org/abs/2004.05150)中提出。）

Mistral 中使用的滑动窗口注意机制本质上是一个固定大小的注意块，允许当前标记仅关注特定数量的前面标记（而不是所有前面标记），如下图所示。

对于 7B Mistral 的具体情况，注意力块的大小为 4096 个标记，研究人员正在训练模型时使用长达 100k 个标记的上下文大小。举个具体例子，在常规自注意力中，位于第 50000 个标记的模型可以关注前面的 49999 个标记。在滑动窗口自注意力中，Mistral 模型只能关注标记 450904 至 50000（因为 50000 - 4096 = 450904）。

但是，滑动窗口注意主要用于提高计算性能。Mistral 优于较大的 Llama 2 模型的事实可能不是因为滑动窗口注意，而是尽管有滑动窗口注意。

Mistral 7B 成为有影响力的模型的一个原因是它作为 Zephyr 7B 的基础模型，如 DPO 部分中早些时候提到的。 Zephyr 7B 是第一个使用 DPO 进行训练并优于其他替代方法的流行模型，这可能为 DPO 在未来几个月成为调整聊天模型的首选方法奠定了基础。

另一个从 Mistral 7B 派生出的值得注意的模型是最近发布的[Mistral Mixture of Experts（MoE）模型](https://mistral.ai/news/mixtral-of-experts/)，也称为 Mixtral-8x7B。这个模型在几个公共基准测试中与或超过了更大的 Llama-2-70B 的性能。

要了解更多基准测试，请参阅官方的[Mixtral 博客文章公告](https://mistral.ai/news/mixtral-of-experts/)。团队还发布了一个已经使用 DPO 进行微调的 Mixtral-8x7B-Instruct 模型（但截至撰写本文时，尚无将其与 Llama-2-70-Chat、RLHF-微调模型进行比较的基准测试）。

*基于 Mistral 团队最初通过社交媒体上的磁力链接共享的 param.json 文件的 Mixtral 架构概述*

据传闻，GPT-4 也是一个由 16 个子模块组成的 MoE。据传闻，这 16 个子模块中的每个都具有 1110 亿个参数（供参考，GPT-3 具有 1750 亿个参数）。如果您大约两个月前阅读了我的[2023 年 AI 和开源文章](https://magazine.sebastianraschka.com/p/ai-and-open-source-in-2023)，我提到“看看 MoE 方法是否能够在 2024 年将开源模型推向新的高度”将是很有趣的。看来 Mixtral 提前开始了这一趋势，我相信这只是一个开始。

如果您对 MoE 模型还不熟悉，请看这里的简短说明。

上图显示了 Switch Transformer 背后的架构，该架构每个令牌使用 1 位专家，总共有 4 位专家。另一方面，Mixtral-8x-7B 由 8 位专家组成，每个令牌使用 2 位专家。

为什么选择 MoE（Mixture of Experts）？在像 Mixtral 这样的 7B 模型中，8 位专家的参数总数仍然约为 ~56B。实际上，这不到 56B，因为 MoE 方法仅应用于 FFN（前馈网络，又称全连接）层，而不是自注意力权重矩阵。因此，参数总数可能更接近 40-50B。

请注意，路由器会重新路由令牌，以便每次仅使用 <14B 参数（2x <7B，而不是全部 <56B）进行前向传递，因此与传统非 MoE 方法相比，训练（尤其是推理）速度将更快。

如果您想了解更多关于 MoE 的信息，请参考[Sophia Yang](https://twitter.com/sophiamyang)推荐的阅读列表：

此外，如果您有兴趣尝试 MoE LLMs，请还要查看今年早些时候实施并共享了 MoE LLMs 的[OpenMoE](https://github.com/XueFuzhao/OpenMoE)存储库。

Mistral 7B、Zephyr 7B 和 Mixtral-8x7B 是 2023 年取得的进展的优秀示例，这些示例具有小但功能强大的模型，其权重是公开可用的。另一个值得注意的模型是我最喜欢的论文列表中的亚军，即微软的 phi 系列。

phi 的秘密酱是在高质量数据上进行训练（称为“教科书质量数据”），该数据通过过滤网络数据获得。

phi 模型于 2023 年分阶段发布，包括 phi-1（13 亿参数）、phi-1.5（13 亿参数）和 phi-2（27 亿参数）。后者仅在两周前发布，据说已经能够与 Mistral 7B 相匹敌或胜过，尽管规模仅为后者的一半。

关于 phi 模型的更多信息，我推荐以下资源：

***[Orca 2：教导小型语言模型如何推理](https://arxiv.org/abs/2311.11045)*** 是一篇相对较新的论文，时间将告诉我们，它是否会对我们在未来几个月或几年中训练 LLMs 的方式产生持久的影响。

我决定包含它，因为它结合了几个概念和想法。

其中一个是从像 GPT-4 这样的大型、功能强大的模型中提炼数据，以创建合成数据集，用于训练小型但功能强大的 LLM。这个想法是在去年发布的 Self-Instruct 论文中描述的。今年早些时候，Alpaca（一种在 ChatGPT 输出上微调的 Llama 模型）真正推广了这种方法。

这是如何工作的？简而言之，这是一个 4 步流程：

1.  使用一组人类编写的指令（本例中为 175 个）和样本指令来初始化任务池；

1.  使用预训练的 LLM（如 GPT-3）来确定任务类别；

1.  给定新指令，让预训练的 LLM 生成响应；

1.  在将响应添加到任务池之前，收集、修剪和过滤响应。

另一个想法可能并不令人惊讶，但值得强调的是：高质量的数据对微调非常重要。例如，[LIMA 论文](https://arxiv.org/abs/2305.11206) 提出了一个人类生成的高质量数据集，其中仅包含 1000 个训练示例，可以用来进行微调，以胜过同样是在 5 万个 ChatGPT 生成的响应上进行微调的模型。

与之前严重依赖模仿学习来复制较大模型输出的研究不同，Orca 2 的目标是教导“小型”（即 70 亿和 130 亿）LLM 们各种推理技术（如逐步推理、回忆再生成等），并帮助它们确定每项任务的最有效策略。这种方法使 Orca 2 在相似大小的模型中明显表现优异，甚至达到与 5-10 倍大的模型可比拟的结果。

虽然我们还没有看到任何关于此的广泛研究，但 Orca 2 方法也可能能够解决在 [The False Promise of Imitating Proprietary LLMs](https://arxiv.org/abs/2305.15717) 论文中强调的使用合成数据的问题。在这里，研究人员调查了微调较弱的语言模型以模仿较强的专有模型（如 ChatGPT）的输出的问题，使用了 Alpaca 和 Self-Instruct 等示例。最初，模仿模型显示出令人鼓舞的结果，在遵循指令和接收与 ChatGPT 相比竞争性评级方面表现良好。然而，更多的跟进评估揭示，这些模仿模型似乎只对人类观察者表现良好，但经常生成事实不准确的响应。

近年来，我几乎专门使用大型语言变压器或视觉变压器（ViTs）因为它们性能良好。

在过去的三篇论文中，我从语言转向了计算机视觉论文，我发现有关计算机视觉的变压器尤其吸引人的是，预训练的 ViTs 甚至比卷积神经网络更容易微调。（我今年早些时候在 CVPR 上总结了一个简短的实践演讲：[`magazine.sebastianraschka.com/p/accelerating-pytorch-model-training`](https://magazine.sebastianraschka.com/p/accelerating-pytorch-model-training)）

令我惊讶的是，我偶然发现了***[ConvNets Match Vision Transformers at Scale](https://arxiv.org/abs/2310.16764)***论文，表明当卷积神经网络（CNNs）获得足够大的数据集时，它们实际上与 ViTs 竞争力相当。

在这里，研究人员投入了高达 110k TPU 小时的计算预算，以对 ViTs 和 CNNs 进行公平比较。结果表明，当 CNNs 使用类似于通常用于 ViTs 的计算预算进行预训练时，它们可以与 ViTs 的性能相匹配。为此，他们在来自 JFT 的 40 亿标记图像上进行了预训练，并随后在 ImageNet 上微调了模型。

对象识别和图像/视频分割以及分类和生成建模是计算机视觉的主要研究领域。

简要介绍一下这两个任务之间的区别：对象检测是关于预测边界框和相关标签；分割将每个像素分类以区分前景和背景对象。

Meta 的***[Segment Anything](https://arxiv.org/abs/2304.02643)***论文是开源和图像分割研究的一个显著里程碑。该论文介绍了一个新的任务、模型和图像分割的数据集。伴随的图像数据集是迄今为止最大的分割数据集，包含超过 11 百万张图像上的 10 亿个蒙版。

*Segment Anything Model (SAM)设计用于高效的基于提示的图像分割。分割任何东西论文中的注释截图，[`arxiv.org/abs/2304.02643`](https://arxiv.org/abs/2304.02643)*

然而，罕见且特别值得称赞的是，研究人员使用了经过许可且尊重隐私的图像，因此该模型可以在没有主要版权问题的情况下开源。

Segment Anything Model (SAM)由上面注释的图中总结的三个主要组件组成。

稍微详细一点，这三个组件可以总结如下：

1.  利用基于预训练视觉变压器（ViT）的遮罩自动编码器的图像编码器，该编码器可以处理高分辨率输入。此编码器在每个图像上运行一次，并且可以在提示模型之前应用。

1.  一个提示编码器处理两种类型的提示：稀疏（点、框、文本）和密集（掩码）。点和框由位置编码和每种提示类型的学习嵌入组合表示。自由文本使用 CLIP 中的现成文本编码器。密集提示，即掩码，使用卷积进行嵌入，并与图像嵌入逐元素求和。

1.  掩码解码器将图像嵌入、提示嵌入和输出令牌映射到掩码。这是一种解码器样式的变压器架构，它计算每个图像位置的掩码前景概率。

图像分割对于自动驾驶汽车、医学成像和许多其他应用非常重要。在短短 6 个月的时间里，这篇论文已经被引用超过 1500 次，并且已经有许多项目建立在这篇论文之上。

***[Emu 视频：通过显式图像条件生成文本到视频的因式分解](https://arxiv.org/abs/2311.10709)*** 是 Meta 研究部门的另一个值得关注的计算机视觉项目。

Emu 是一个可以从文本提示生成整个视频的文本到视频模型。

虽然这不是令人印象深刻的文本到视频生成模型的第一个模型，但与以前的作品相比，它表现得非常出色。

正如作者所指出的，与以前的方法相比，Emu 的架构设置相对简单。这里的一个主要思想是，Emu 将生成过程分解为两个步骤：首先，基于文本生成图像（使用扩散模型），然后在文本和生成的图像的条件下创建视频（使用另一个扩散模型）。

2022 年是文本到图像模型的大年，如 DALL-E 2、Stable Diffusion 和 Midjourney。虽然文本到图像模型在 2023 年仍然非常受欢迎（尽管 LLMs 在全年引起了大多数关注），但我认为文本到视频模型即将在未来一年在网络社区中变得更加普遍。

由于我不是图像或视频设计师，目前我没有这些工具的用例；然而，文本到图像和文本到视频模型仍然是作为计算机视觉进展的一般衡量标准而非常有趣的。

* * *

*本杂志是一个个人热情项目，不提供直接的报酬。然而，对于那些希望支持我的人，请考虑购买[我的其中一本书](https://sebastianraschka.com/books)。如果你觉得它们有见地并且有益，请随时向你的朋友和同事推荐。*

**你的支持意义重大！谢谢！**
