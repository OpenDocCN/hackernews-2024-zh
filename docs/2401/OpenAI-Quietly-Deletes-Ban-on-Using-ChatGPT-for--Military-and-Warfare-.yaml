- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-05-27 14:43:01'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: OpenAI Quietly Deletes Ban on Using ChatGPT for “Military and Warfare”
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://theintercept.com/2024/01/12/open-ai-military-ban-chatgpt/](https://theintercept.com/2024/01/12/open-ai-military-ban-chatgpt/)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: OpenAI this week quietly deleted language expressly prohibiting the use of its
    technology for military purposes from its usage policy, which seeks to dictate
    how powerful and immensely popular tools like ChatGPT can be used.
  prefs: []
  type: TYPE_NORMAL
- en: Up until January 10, OpenAI’s “usage policies” [page](https://web.archive.org/web/20240109122522/https:/openai.com/policies/usage-policies)
    included a ban on “activity that has high risk of physical harm, including,” specifically,
    “weapons development” and “military and warfare.” That plainly worded prohibition
    against military applications would seemingly rule out any official, and extremely
    lucrative, use by the Department of Defense or any other state military. The [new
    policy](https://openai.com/policies/usage-policies) retains an injunction not
    to “use our service to harm yourself or others” and gives “develop or use weapons”
    as an example, but the blanket ban on “military and warfare” use has vanished.
  prefs: []
  type: TYPE_NORMAL
- en: The unannounced redaction is part of a major rewrite of the policy page, which
    the company said was intended to make the document “clearer” and “more readable,”
    and which includes many other substantial language and formatting changes.
  prefs: []
  type: TYPE_NORMAL
- en: “We aimed to create a set of universal principles that are both easy to remember
    and apply, especially as our tools are now globally used by everyday users who
    can now also build GPTs,” OpenAI spokesperson Niko Felix said in an email to The
    Intercept. “A principle like ‘Don’t harm others’ is broad yet easily grasped and
    relevant in numerous contexts. Additionally, we specifically cited weapons and
    injury to others as clear examples.”
  prefs: []
  type: TYPE_NORMAL
- en: Felix declined to say whether the vaguer “harm” ban encompassed all military
    use, writing, “Any use of our technology, including by the military, to ‘[develop]
    or [use] weapons, [injure] others or [destroy] property, or [engage] in unauthorized
    activities that violate the security of any service or system,’ is disallowed.”
  prefs: []
  type: TYPE_NORMAL
- en: In a subsequent email, Felix added that OpenAI wanted to pursue certain “national
    security use cases that align with our mission,” citing a plan to create “cybersecurity
    tools” with DARPA, and that “the goal with our policy update is to provide clarity
    and the ability to have these discussions.”
  prefs: []
  type: TYPE_NORMAL
- en: “OpenAI is well aware of the risk and harms that may arise due to the use of
    their technology and services in military applications,” said Heidy Khlaaf, engineering
    director at the cybersecurity firm Trail of Bits and an expert on machine learning
    and autonomous systems safety, citing a [2022 paper](https://arxiv.org/pdf/2207.14157.pdf)
    she co-authored with OpenAI researchers that specifically flagged the risk of
    military use. Khlaaf added that the new policy seems to emphasize legality over
    safety. “There is a distinct difference between the two policies, as the former
    clearly outlines that weapons development, and military and warfare is disallowed,
    while the latter emphasizes flexibility and compliance with the law,” she said.
    “Developing weapons, and carrying out activities related to military and warfare
    is lawful to various extents. The potential implications for AI safety are significant.
    Given the well-known instances of bias and hallucination present within Large
    Language Models (LLMs), and their overall lack of accuracy, their use within military
    warfare can only lead to imprecise and biased operations that are likely to exacerbate
    harm and civilian casualties.”
  prefs: []
  type: TYPE_NORMAL
- en: The real-world consequences of the policy are unclear. Last year, The Intercept
    reported that OpenAI was [unwilling to say](https://theintercept.com/2023/05/08/chatgpt-ai-pentagon-military/)
    whether it would enforce its own clear “military and warfare” ban in the face
    of increasing interest from the Pentagon and U.S. intelligence community.
  prefs: []
  type: TYPE_NORMAL
- en: “Given the use of AI systems in the targeting of civilians in Gaza, it’s a notable
    moment to make the decision to remove the words ‘military and warfare’ from OpenAI’s
    permissible use policy,” said Sarah Myers West, managing director of the AI Now
    Institute and a former AI policy analyst at the Federal Trade Commission. “The
    language that is in the policy remains vague and raises questions about how OpenAI
    intends to approach enforcement.”
  prefs: []
  type: TYPE_NORMAL
- en: While nothing OpenAI offers today could plausibly be used to directly kill someone,
    militarily or otherwise — ChatGPT can’t maneuver a drone or fire a missile — any
    military is in the business of killing, or at least maintaining the capacity to
    kill. There are any number of killing-adjacent tasks that a LLM like ChatGPT could
    augment, like writing code or processing [procurement](https://asc.army.mil/web/news-chatgpt-in-dod-acquisitions/)
    orders. A review of custom ChatGPT-powered bots offered by OpenAI suggests U.S.
    military personnel are [already](https://chat.openai.com/g/g-r5Jaw1MFS-following-through-gpt)
    [using](https://chat.openai.com/g/g-G0cfHcef9-army-doctrine-publication-6-22)
    the technology to expedite paperwork. The National Geospatial-Intelligence Agency,
    which directly aids U.S. combat efforts, has [openly](https://www.nga.mil/news/NGA_brings_products_closer_to_action_in_Middle_Eas.html)
    [specul](https://youtu.be/JHLUdsDzTvQ)[ated about using ChatGPT to aid its human
    analysts](https://www.nga.mil/news/NGA_brings_products_closer_to_action_in_Middle_Eas.html).
    Even if OpenAI tools were deployed by portions of a military force for purposes
    that aren’t directly violent, they would still be aiding an institution whose
    main purpose is lethality.
  prefs: []
  type: TYPE_NORMAL
- en: Experts who reviewed the policy changes at The Intercept’s request said OpenAI
    appears to be silently weakening its stance against doing business with militaries.
    “I could imagine that the shift away from ‘military and warfare’ to ‘weapons’
    leaves open a space for OpenAI to support operational infrastructures as long
    as the application doesn’t directly involve weapons development narrowly defined,”
    said Lucy Suchman, professor emerita of anthropology of science and technology
    at Lancaster University. “Of course, I think the idea that you can contribute
    to warfighting platforms while claiming not to be involved in the development
    or use of weapons would be disingenuous, removing the weapon from the sociotechnical
    system – including command and control infrastructures – of which it’s part.”
    Suchman, a scholar of artificial intelligence since the 1970s and member of the
    International Committee for Robot Arms Control, added, “It seems plausible that
    the new policy document evades the question of military contracting and warfighting
    operations by focusing specifically on weapons.”
  prefs: []
  type: TYPE_NORMAL
- en: Suchman and Myers West both pointed to OpenAI’s close partnership with Microsoft,
    a major defense contractor, which has invested $13 billion in the LLM maker to
    date and resells the company’s software tools.
  prefs: []
  type: TYPE_NORMAL
- en: The changes come as militaries around the world are eager to incorporate machine
    learning techniques to gain an advantage; the Pentagon is still tentatively exploring
    how it might use ChatGPT or other large-language models, a type of software tool
    that can rapidly and dextrously generate sophisticated text outputs. LLMs are
    trained on giant volumes of books, articles, and other web data in order to approximate
    human responses to user prompts. Though the outputs of an LLM like ChatGPT are
    often extremely convincing, they are optimized for coherence rather than a firm
    grasp on reality and often suffer from so-called hallucinations that make accuracy
    and factuality a problem. Still, the ability of LLMs to quickly ingest text and
    rapidly output analysis — or at least the simulacrum of analysis — makes them
    a natural fit for the data-laden Defense Department.
  prefs: []
  type: TYPE_NORMAL
- en: While some within U.S. military leadership have expressed [concern](https://www.armscontrol.org/act/2023-06/news/chatgpt-sparks-us-debate-over-military-use-ai)
    about the tendency of LLMs to insert glaring factual errors or other distortions,
    as well as security risks that might come with using ChatGPT to analyze classified
    or otherwise sensitive data, the Pentagon remains generally eager to adopt artificial
    intelligence tools. In a November address, Deputy Secretary of Defense Kathleen
    Hicks [stated](https://www.defense.gov/News/Speeches/Speech/Article/3578046/remarks-by-deputy-secretary-of-defense-kathleen-h-hicks-on-the-state-of-ai-in-t/)
    that AI is “a key part of the comprehensive, warfighter-centric approach to innovation
    that Secretary [Lloyd] Austin and I have been driving from Day 1,” though she
    cautioned that most current offerings “aren’t yet technically mature enough to
    comply with our ethical AI principles.”
  prefs: []
  type: TYPE_NORMAL
- en: Last year, Kimberly Sablon, the Pentagon’s principal director for trusted AI
    and autonomy, [told](https://www.nationaldefensemagazine.org/articles/2023/3/8/pentagons-top-ai-official-addresses-chatgpts-possible-benefits-risks)
    a conference in Hawaii that “[t]here’s a lot of good there in terms of how we
    can utilize large-language models like [ChatGPT] to disrupt critical functions
    across the department.”
  prefs: []
  type: TYPE_NORMAL
- en: '**Update: January 16, 2024**'
  prefs: []
  type: TYPE_NORMAL
- en: '*This article has been updated to include a statement from OpenAI about the
    use of its technology for military purposes* *that was received after publication.*'
  prefs: []
  type: TYPE_NORMAL
