<!--yml

分类: 未分类

日期: 2024-05-27 14:30:02

-->

# 高效的 LLM 推理 - 由 Finbarr Timbers 撰写

> 来源：[https://www.artfintel.com/p/efficient-llm-inference](https://www.artfintel.com/p/efficient-llm-inference)

最近，我一直在思考推理，特别是如何更高效地为给定的 LLM 提供服务。情景如下：您的老板来找您，说*嘿，Finbarr，我们快要破产了，因为我们正在花光所有投资者的钱来为我们的 300B 参数模型提供服务，这个模型以[约翰·肯尼思·加尔布雷斯](https://en.wikipedia.org/wiki/John_Kenneth_Galbraith)的风格说唱。我们能做些什么？*

广义上来说，您可以做三类主要的事情：

1.  您可以对模型的参数进行量化（*量化*），在这种情况下，您保持模型完全相同，但对每个参数使用较低的精度。

1.  您可以蒸馏模型的较小版本（*蒸馏*），在这种情况下，您复制模型的架构以使其更小和/或更有效，然后训练这个新的、较小的模型来模拟原始大模型的输出。

1.  您可以花费大量时间对代码进行分析，并减少开销，而无需改变架构或参数（*优化*）。

显然，开始优化的地方是最佳选择。大多数程序的开销都是荒谬的，通过简单地分析代码，您往往可以找到令人惊讶的开销。例如，我曾经有一个同事请求帮助优化他的代码。他正在训练一个神经网络来执行复杂的计算，并实施了一堆性能优化以加快速度，但他*还*在性能关键的循环中使用列表进行查找。我将列表更改为字典后，代码运行速度提高了 200 倍。

这并不是一个罕见的情况。每次我分析代码时，最终的分析结果都让我感到惊讶。所以，如果您遇到性能问题（别担心，这种情况发生在我们所有人身上，这是自然的），您应该做的第一件事就是对代码进行分析。

对于大多数人来说，这已经足够了。仅仅通过去除代码中的开销并以朴素的方式批处理请求，就可以使您能够以成本有效的方式提供请求，*尤其是*如果您拥有传统的软件利润空间。但假设您已经进行了大量分析，现在您所剩下的唯一优化是在 Triton 中实现奥秘内核，这需要从 Nvidia 招聘经验丰富的老 CUDA 专家。那么，您可以采取哪些措施来有效地利用您的 GPU？

现在，你剩下的是量化和蒸馏。最近，人们经常谈论量化，即在不改变神经网络其他任何内容的情况下，使用精度较低的权重。例如，Llama.cpp 就很好地利用了这一点，将储存 Llama 权重所需的内存减少了 4 倍。

虽然蒸馏受到的关注较少，但在历史上一直是服务大规模模型的重要组成部分。这是因为蒸馏通常比量化要好得多，如果你有资源，应该是你做事情的方式。

这里有一个关键的警告：如果你有资源。

[团体订阅享 20% 折扣](https://www.artfintel.com/subscribe?group=true&coupon=9ee370da)

让我们回到我们的假设场景。你是一名勤奋的 ML 工程师，在 CoherentOpenStability 工作，在那里，你正在尝试减少最新和最伟大的 LLM，StableClaudius-4 的推理成本。你已经对代码进行了分析，并减少了所有可能的开销。现在你有几个选择：

1.  你提出了一个研究突破，让你以更便宜的方式完成同样的事情。例如，你设计了一个新的稀疏注意机制，它效果很好。

1.  你使你的模型变小。

如果我要比较这些，显而易见的赢家是 #1\. 如果你能提出一个神奇地改进你的模型的新研究贡献，你显然应该这样做。如果你是这样的人，请停止阅读本文，去写一篇论文，申请 OpenAI/Anthropic/DeepMind，然后领取一份作为大型语言模型解密者的非常高的工资。大多数人不能做到这一点。所以我们只能设法提出一个较小的模型，以完成相同的事情。

我们应该如何提出一个较小的模型？几个选项：

1.  你按照与原始模型完全相同的方式训练一个较小的模型。

1.  你将你的大模型蒸馏成一个较小的模型。

1.  你量化你现有的模型。

在我看来，文献表明有一个清晰明显的排序：蒸馏比训练一个较小的模型要好，而量化*可能*比训练一个较小的模型要好。

我想要的蒸馏论文不多，但我想到的两篇是 [DistilBERT](https://arxiv.org/abs/1910.01108) 和 Hinton 等人的[原始蒸馏论文](https://arxiv.org/abs/1503.02531)。在 DistilBERT 中，作者将模型大小减小了 40%，同时只减少了 3% 的性能。

在 Hinton 等人的论文中，他们能够用一个单一的蒸馏模型匹配 10 个模型集合的性能，并且性能仅从 61.1% 的准确率下降到 60.8% 的准确率（与原始性能的 99.5%，仅为原始大小的 10%）。现在，Hinton 的论文正在与一个集合进行比较，这是一种特别浪费模型大小的方式，但这仍然是一个令人印象深刻的结果，比从头开始训练模型执行相同任务要好得多（仅有 58.9% 的准确率）。

然而，蒸馏的问题在于，它需要从头开始训练一个较小的模型 ***并且*** 用大模型在整个数据集上运行推理。如果您有一个像 GPT-3 那样大小的数据集（500B），这将花费 $1M，按公共 API 价格计算（5e11 个令牌 * 2e-6 $/令牌 = $1e6），或者如果我们假设 OpenAI 有 60% 的利润率，那么就是 $400k。鉴于最初训练 GPT-3 的成本约为 $5M，这将使已经很高的成本增加 10-20%。不是禁止性的，但很昂贵。

如果你能承担这个成本，太好了！去做吧。这几乎肯定会给你最佳的性能。如果你想要更便宜的东西，你需要在从头开始训练一个较小的模型和量化一个现有模型之间做出决定。为了帮助，我们有一篇论文 [k 位推理缩放规律](https://arxiv.org/abs/2212.09720)。这个想法是，从推理的角度来看，我们对以一定精度服务一个 30B 模型和以两倍精度服务一个 60B 模型之间是中立的，因为大多数 GPU 在运行精度减半的模型时速度是原来的两倍快（例如 [A100s](https://www.nvidia.com/content/dam/en-zz/Solutions/Data-Center/a100/pdf/nvidia-a100-datasheet-us-nvidia-1758950-r4-web.pdf)）。

此图显示了使用不同模型大小和不同精度水平之间的权衡。让我们比较一下 [OPT](https://arxiv.org/abs/2205.01068) 的两个工作点。

模型精度 位精度 平均零样本准确度 $10^{11}$ 8 0.675 $10^{11}$ 16 0.65 $10^{12}$ 8 0.725 $10^{12}$ 16 0.7

我们看到的是，给定模型位数，我们更喜欢每个参数*较少*位数的模型。直觉上讲，这是有道理的：使用 fp64 和 fp32 训练一半的参数并没有带来好处。

如果我们看另一张图，这次是来自 [OPT 论文](https://arxiv.org/abs/2205.01068)，我们可以分析性能如何随参数数量变化而变化。由于 OPT 使用 FP16，即每个参数使用 2 字节（或 16 位），1e11 个参数相当于 1.6e12 位。通过减少 10 倍的参数，从 1.6e12 降到 1.6e11 位，OPT 的平均准确度从 0.7 降到 0.65：准确度下降了 8%，成本减少了 10 倍。虽然不及我们在蒸馏中看到的模型大小/准确度权衡那么好，但我认为大多数企业必须认真考虑这种权衡。

关于量化的另一点要记住的是，它的成本非常便宜！量化的 SOTA 方法是 [GPTQ](https://arxiv.org/abs/2210.17323)，它可以在 4 个 GPU 小时内量化一个 175B 参数模型（在公共云价格下大约为 $4）。然而，从头开始训练模型成本很高；对于训练一个 GPT-3 风格模型的成本的粗略估计是 $5M，成本随参数数量线性增加，因此一个 20B 模型的成本将约为 ~$500k，并且需要大量数据（~100B 令牌才能达到 [Chinchilla 最优](https://arxiv.org/abs/2203.15556)）。

所以量化很棒。但是，到底，*什么*是量化，它是如何工作的？

量化背后的想法很简单。由于计算机的离散性质，它们不能本地存储浮点数。数字表示是基于**位**，即1或0的。这些位被组装成**二进制**。在二进制整数表示中，你可以表示一系列的

使用带符号整数，其中n是位数。其中一位用于表示数字是正数还是负数，剩下的n - 1位用于表示幅度。

这个方法很好，而且相当高效。然而，当你想表示[实数](https://en.wikipedia.org/wiki/Real_number)，即可以取得整数之间值的数字时，问题就来了。最常见的方法是保留1位来指示数字的正负性，m位来表示数字的大小（***指数***），以及（n - m - 1）位来表示数字的精度（尾数）。

尾数只是一个(n-m-1)位的无符号整数，因此可以表示的值最大为2^{n - m - 1}。

在32位浮点数（单精度）中，1位用于符号，8位用于指数，23位用于尾数。

在16位浮点数（半精度）中，1位用于符号，5位用于指数，10位用于尾数。

在64位浮点数（双精度）中，1位用于符号，11位用于指数，52位用于尾数。

注意额外的位都去了哪里——它们大部分都去了尾数，这增加了*精度*，而不是*幅度*。换句话说，这让我们区分了*更小*的数字，而不是允许我们表示*更大*的数字。

默认情况下，所有主要的张量编程框架都使用32位精度来存储可训练参数。这样做是有原因的：32位精度通常是一个很好的默认值。很少有应用程序会受益于额外的精度（主要是科学计算应用程序）。然而，在实践中，现在大多数的前沿工作都使用16位。

但好吧。现在你已经读完了我关于浮点数精度工作方式的离题讨论，让我们假设我们已经选择了一个精度级别。你如何*实际地*降低你的权重呢？天真的方法是简单地截断你的权重到给定的精度水平。举个简单的例子，如果你的权重是0.534345，天真地截断权重将把它转换为0.534。

[将到 4 位或更低的量化模型](https://twitter.com/Tim_Dettmers/status/1642885684057997313?s=20) 是 GPTQ。还有其他一些方法是 LLM.int8() 和 ZeroQuant。我将在未来的文章中深入讨论这些，但在这里，我将专注于 GPTQ。GPTQ 的基本思想是，虽然通过减少比特数来减少网络中包含的信息必然会导致信息下降，但我们可以通过训练权重直接减少两者之间的差异来减少其对推理准确性的影响：

让我们通过一个例子来说明。假设 x = 0.323，并且如上所述，w = 0.534345。然后，保持所有内容为 float32，激活输出是：

\(x \cdot w = 0.323 \cdot 0.534345 = 0.172593435\)

四舍五入到 6 个小数点（float32 的精度），我们得到输出为 0.172593。

天真地四舍五入，我们的输出是

\(x \cdot w_{\text{naive}} = 0.323 \cdot 0.534 = 0.172482\)

这里的差异是 1.114e-4。如果我们使用 GPTQ，我们解决了

\(\text{argmin}_{\hat{w}} (0.172593435 - 0.323 \cdot \hat{w})^2\)

这给我们带来了

\(\hat{w} = \frac{0.172593435}{0.323} = 0.534345\)

四舍五入到 3 个小数点（float16 的精度）... 与天真地四舍五入给出了完全相同的答案，但需要更多的工作。

据推测，在其他情况下这可能更重要？我无法想出一个简单的例子来证明 GPTQ 值得。但在实际部署场景中，GPTQ 声称有显著差异（RTN 意为“四舍五入”）：

所以这是一种比天真地四舍五入更好的方法，而且很便宜。

量化并不是魔法。最终，你总是在性能上牺牲精度。也许你不会损失很多。但你永远不会 *获得* 精度，所以充其量你只是保持不变。

这种权衡值得多少次也不清楚。[Tim Dettmers 的量化扩展定律](https://arxiv.org/abs/2212.09720)。如果你使用了一半的精度，也许值得使用 *相同的精度* 和一半的权重，并且在[更多数据上训练两倍长的时间](https://finbarr.ca/llms-not-trained-enough/)。例如，[replit 就是这么做的](https://blog.replit.com/replit-developer-day-recap#newmodel)。对于许多从业者来说，为模型提供服务的成本远远超过了训练模型的成本。如果你是这样的人，你可能不关心量化。

即使你这样做，蒸馏通常也会优于量化。所以如果你 *可以* 蒸馏模型，你可能应该这样做。只有当你没有资源这样做时，量化才明显值得。

最后，通过量化，你只会在减少字节数量时获得线性加速。这相当不错！但理想情况下，我们希望看到更好的缩放。也许某种稀疏性会做得更好。
