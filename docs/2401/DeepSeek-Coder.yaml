- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-05-27 15:22:41'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: DeepSeek Coder
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://deepseekcoder.github.io/](https://deepseekcoder.github.io/)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: DeepSeek Coder comprises a series of code language models trained from scratch
    on both 87% code and 13% natural language in English and Chinese, with each model
    pre-trained on 2T tokens. We provide various sizes of the code model, ranging
    from 1B to 33B versions. Each model is pre-trained on repo-level code corpus by
    employing a window size of 16K and a extra fill-in-the-blank task, resulting in
    foundational models (DeepSeek-Coder-Base). We further fine-tune the base model
    with 2B tokens of instruction data to get instruction-tuned models, namedly DeepSeek-Coder-Instruct.
  prefs: []
  type: TYPE_NORMAL
- en: Pretrained on **2 Trillion** tokens over more than 80 programming languages.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Various model sizes (**1.3B**, **5.7B**, **6.7B** and **33B**) to support different
    requirements.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A window size of **16K window** size, supporting **project-level** code completion
    and infilling.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**State-of-the-Art** performance among open code models.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Open source and free for research and commercial use**.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
