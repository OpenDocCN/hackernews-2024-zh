- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 'category: 未分类'
- en: 'date: 2024-05-27 14:43:40'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 'date: 2024-05-27 14:43:40'
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: Is the reproducibility crisis reproducible? - by Ben Recht
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 复制危机是否可复制？- Ben Recht
- en: 来源：[https://www.argmin.net/p/is-the-reproducibility-crisis-reproducible](https://www.argmin.net/p/is-the-reproducibility-crisis-reproducible)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://www.argmin.net/p/is-the-reproducibility-crisis-reproducible](https://www.argmin.net/p/is-the-reproducibility-crisis-reproducible)
- en: Right before I left for the winter break, [Andrew Gelman and his collaborators
    lit up social media with a paper claiming that most clinical trials were underpowered](https://statmodeling.stat.columbia.edu/2023/12/23/bayesians-moving-from-defense-to-offense-i-really-think-its-kind-of-irresponsible-now-not-to-use-the-information-from-all-those-thousands-of-medical-trials-that-came-before-is-that-very/).
    [In a paper in](https://evidence.nejm.org/doi/full/10.1056/EVIDoa2300003) *[NEJM
    Evidence](https://evidence.nejm.org/doi/full/10.1056/EVIDoa2300003)*[,](https://evidence.nejm.org/doi/full/10.1056/EVIDoa2300003)
    they set their sights on the summary statistic everyone loves to hate, the p-value.
    Gelman referred to this paper as “Bayesians moving from defense to offense.” The
    coauthors included econ Nobel winner Guido Imbens and prolific epidemiologist
    Sander Greenland. The paper contained claims like
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 在我冬季休假前不久，[Andrew Gelman 和他的合作者们发表了一篇论文，声称大多数临床试验的功效不足](https://statmodeling.stat.columbia.edu/2023/12/23/bayesians-moving-from-defense-to-offense-i-really-think-its-kind-of-irresponsible-now-not-to-use-the-information-from-all-those-thousands-of-medical-trials-that-came-before-is-that-very/)，引起了社交媒体的轰动。在一篇
    *[NEJM Evidence](https://evidence.nejm.org/doi/full/10.1056/EVIDoa2300003)* 的论文中，他们把目光投向了每个人都喜欢批评的总结统计量，即
    p 值。Gelman 将这篇论文称为“贝叶斯人由防守转向进攻”。合作者包括经济学诺贝尔奖获得者 Guido Imbens 和多产的流行病学家 Sander
    Greenland。论文中包含了诸如以下的声明：
- en: “The probability of a replication study yielding P<0.05 in the same direction
    is small, even in the stratum from 0.005 to 0.001\. Thus, a replication with P>0.05
    does not imply that the original finding was a fluke at least not in the context
    of historical clinical trials just as P<0.05 in the original study does not imply
    that the initial conclusion was correct, especially when P is near 0.05.”
  id: totrans-7
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: “在相同方向上，复制研究得到 P<0.05 的概率很小，即使在从 0.005 到 0.001 的层面上。因此，复制研究中得到 P>0.05 并不意味着原始发现是偶然的，至少不是在历史临床试验的背景下，就像原始研究中的
    P<0.05 并不意味着最初的结论是正确的，特别是当 P 接近 0.05 时。”
- en: Certainly, proponents of the replication crisis love to say things like this.
    We all love to terrify our students with claims that science is wrong and unreplicable
    because our colleagues are not rigorous enough. But does this new paper provide
    good evidence of such stern warnings?
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，复制危机的支持者们喜欢说这样的话。我们都喜欢用“科学是错误的，不可复制的”这样的说法来吓唬我们的学生，因为我们的同行们不够严谨。但这篇新论文是否提供了这样严厉警告的良好证据呢？
- en: Let’s dig in! What did they do exactly? The paper irked me from the get-go by
    triggering one of my biggest methodological pet peeves. They aimed to show that
    all randomized experiments are false by running a deeply confounded observational
    study. Boo! But the details turned out to be worse than that.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们深入研究吧！他们到底做了什么？这篇论文从一开始就激怒了我，因为它触发了我最大的方法论怨恨之一。他们的目标是通过进行一项深度混杂的观察性研究来证明所有随机实验都是错误的。噢！但事情的细节证明比那更糟糕。
- en: 'Let’s start at the highest level of how the authors try to make their case.
    I need one technical bit of annoying statistics to be precise in my description:
    the paper drew conclusions about a model of the z-score of outcomes in clinical
    trials. What is a z-score? It is a quantity that is morally equivalent to a p-value.
    I don’t want to go into the nitty-gritty of what a p-value is here because it’s
    annoying and distracting. The following will suffice. Every outcome in a randomized
    trial gets labeled by a p-value. The smaller the p-value, the more we’re convinced
    that the measured difference between the groups is due to the treatment and not
    the random assignment of people into two groups. Instead of a p-value, you could
    report the z-score. There’s a formula that lets you go from one to the other.
    The larger the z-score, the smaller the p-value, and vice versa. This paper worked
    with z-scores rather than p-values because, for technical reasons, z-scores are
    easier to work with mathematically. The authors aimed to show that their model
    of z-scores of clinical trials displayed some worrying properties that would suggest
    a replication crisis.'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从作者试图阐明他们的论点的最高水平开始。我需要在描述中准确地给出一个技术上令人讨厌的统计学方面：这篇论文得出了有关临床试验中结果的 z 分数模型的结论。什么是
    z 分数？这是一种在道德上等同于 p 值的量。我不想在这里深入讨论 p 值的细节，因为这很烦人并且会分散注意力。以下内容足以。随机试验中的每个结果都会被标记为
    p 值。 p 值越小，我们就越相信两组之间测量差异是由于治疗而不是将人们随机分配到两组中。你可以报告 z 分数而不是 p 值。有一个公式可以让你从一个到另一个转换。z
    分数越大，p 值越小，反之亦然。这篇论文使用 z 分数而不是 p 值进行工作，因为出于技术原因，z 分数在数学上更容易处理。作者的目标是显示他们的临床试验
    z 分数模型显示出一些令人担忧的特性，这些特性将暗示着复制危机。
- en: 'In this paper, the proposed method was:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇论文中，提出的方法是：
- en: Collect a huge data set of the z-scores of randomized controlled trials from
    the Cochrane database
  id: totrans-12
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 收集来自 Cochrane 数据库的随机对照试验的 z 分数的大型数据集
- en: Build a statistical model of these z-scores
  id: totrans-13
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 构建这些 z 分数的统计模型
- en: From this statistical model, draw a bunch of samples to simulate expected rates
    of reproducibility or misestimation of primary effect sizes.
  id: totrans-14
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从这个统计模型中，抽取一堆样本来模拟预期的可重现性率或主要效应大小的错误估计。
- en: Everything here rests on step 1\. Is their data set of z-scores legitimately
    representative of all clinical trials in medicine? It turned out that this *NEJM
    Evidence* paper didn’t actually do step 1 at all. In fact, this paper started
    at step 3! They took a model directly from a [2021](https://onlinelibrary.wiley.com/doi/full/10.1002/sim.9173)
    *[Statistics in Medicine](https://onlinelibrary.wiley.com/doi/full/10.1002/sim.9173)*
    [paper by van Zwet, Schwab, and Senn](https://onlinelibrary.wiley.com/doi/full/10.1002/sim.9173).
    This earlier model fit a mixture of four zero-mean Gaussians to the symmetrized
    set of z-scores from some trials they had scraped. Why is it plausible that a
    z-score in a clinical trial is a sample from a mixture of Gaussians? It’s not.
    It’s ridiculous. But I’m not even going to dwell on the model because I still
    want to focus on where the data came from.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的一切都取决于第一步。他们的 z 分数数据集是否合法地代表了医学中所有临床试验？结果表明，这篇 *NEJM Evidence* 论文根本没有进行第一步。事实上，这篇论文直接从第三步开始！他们直接从
    [2021 年](https://onlinelibrary.wiley.com/doi/full/10.1002/sim.9173) 的 *[Statistics
    in Medicine](https://onlinelibrary.wiley.com/doi/full/10.1002/sim.9173)* 论文中的
    van Zwet、Schwab 和 Senn 那里拿了一个模型。这个较早的模型将四个零均值的高斯分布混合拟合到了他们抓取的一些试验的 z 分数的对称集合上。为什么一个临床试验中的
    z 分数是从高斯混合分布中抽取的样本是可信的？并不是。这太荒谬了。但我甚至不想深入讨论模型，因为我仍然想专注于数据的来源。
- en: van Zwet, Schwab, and Senn’s 2021 paper did not have code, but it linked to
    their [data set](https://osf.io/xjv9g/). They claimed they fit their model to
    23,551 randomized trials in the data set. I downloaded the data and counted 34,836
    unique trials. What happened to the remaining 11,285? No one knows. I decided
    to dig deeper to see if I could find out.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: van Zwet、Schwab 和 Senn 的 2021 年论文没有代码，但它链接到了他们的 [数据集](https://osf.io/xjv9g/)。他们声称他们将他们的模型拟合到了数据集中的
    23,551 个随机试验中。我下载了数据并统计了 34,836 个唯一的试验。剩下的 11,285 个发生了什么？没有人知道。我决定深入挖掘，看看能不能找到答案。
- en: 'van Zwet, Schwab, and Senn assert that [Schwab, Kreiliger, and Held initially
    collected and processed the data in 2019.](https://osf.io/preprints/metaarxiv/t6cjx/)
    I looked at this 2019 paper and found a shocking passage:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: van Zwet、Schwab 和 Senn 坚称 [Schwab、Kreiliger 和 Held 最初在 2019 年收集和处理了这些数据。](https://osf.io/preprints/metaarxiv/t6cjx/)
    我查看了这篇 2019 年的论文，并发现了一个令人震惊的段落：
- en: “Comparing reported effect sizes from primary studies across medical specialties
    required harmonization. We recalculated all effect sizes for the primary studies
    using escalc() from the R package metafor . For continuous outcomes the SMD (Hedges’
    g ), for dichotomous outcomes the odds ratios was used. Odds ratios were transformed
    into Hedges’ g which were then transformed into Pearson’s r. Statistical significance
    was calculated with a Wald’s test which was performed on the original effect measure
    as reported in the CDSR (e.g., risk ratio, hazard ratio, e tc.); for mean differences
    we applied a two-sample Student’s t-test.”
  id: totrans-18
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: “需要对各医学专业的主要研究报告中报告的效应大小进行比较。我们使用 R 包 metafor 中的 escalc() 重新计算了所有主要研究的效应大小。对于连续结果，使用了
    SMD（Hedges' g），对于二分结果，使用了几率比。几率比被转换为 Hedges' g，然后再转换为 Pearson's r。统计显著性是通过 Wald
    检验计算的，该检验是在 CDSR（例如，风险比、风险比等）中报告的原始效应度量上进行的；对于均值差异，我们应用了两个样本的学生 t 检验。”
- en: Wait, hold on. These z-scores aren’t derived from the reported p-values in RCTs?
    They are p-values computed by other metascientists for another project. That’s
    weird! In the *NEJM Evidence* article Van Zwet et al. claim “We have collected
    the absolute z statistics of the primary efficacy outcome for all of these RCTs.”
    But this is misleading at best. These are not the reported z-statistics, but rather
    the derived z-statistics by a method of other authors. We’re looking at something
    different than what the authors have led us to believe. In that case, we shouldn’t
    believe any of the conclusions in the *NEJM Evidence* paper.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 等等，等一下。这些 z 分数不是从 RCTs 报告的 p 值中导出的？它们是其他元科学家为另一个项目计算的 p 值。这太奇怪了！在《NEJM 证据》文章中，范·兹维特等人声称：“我们已经收集了所有这些
    RCT 的主要疗效结果的绝对 z 统计量。”但这至多是误导性的。这些不是报告的 z 统计量，而是其他作者方法导出的 z 统计量。我们看的是与作者让我们相信的不同的东西。在这种情况下，我们不应该相信《NEJM
    证据》论文中的任何结论。
- en: 'This is why you shouldn’t form “priors” based on “big data scraping.” When
    you write scripts to digest tens of thousands of trial reports, and you don’t
    look at any of them, no one has any idea what the content or value of all of this
    data is. Someone just shows you a univariate histogram and wants you to believe
    that it’s true. But you shouldn’t believe anything. Especially nothing that comes
    from an *unreproducible* process of web crawling. Let me reiterate: *this paper
    about reproducibility is itself unreproducible*.'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是为什么你不应该根据“大数据抓取”形成“先验”。当你编写脚本来处理成千上万的试验报告，并且你不看其中任何一个时，没有人知道所有这些数据的内容或价值是什么。有人只是给你展示一个单变量直方图，希望你相信它是真实的。但你不应该相信任何东西。特别是来自于无法复制的网络爬取过程的东西。让我重申一下：*这篇关于可复制性的论文本身就是不可复制的*。
- en: Erik van Zwet asserts “I really think it’s kind of irresponsible now not to
    use the information from all those thousands of medical trials that came before.
    Is that very radical?” But how we use the information from past trials matters.
    Scaping all trials with an R-script and blindly stitching together a table of
    p-values without ever looking at any of the trial reports is radical. This is
    an absurd path to knowledge building. Am I surprised that the authors of the *NEJM
    Evidence* article concluded that they can explain the reproduction crisis? No.
    As is the case with all observational studies, the authors found exactly what
    they were looking for. But are the conclusions supported by the “data?” No. The
    evidence is weak.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 埃里克·范·兹维特声称：“我真的认为现在不利用所有那些之前的成千上万个医学试验的信息有点不负责任。这很激进吗？”但我们如何利用过去试验的信息很重要。用
    R 脚本抓取所有试验，并盲目地拼凑出一个 p 值的表格，而不看任何试验报告，这是激进的。这是一条通向知识建构的荒谬之路。《NEJM 证据》文章的作者得出结论称他们能够解释复制危机，我感到吃惊吗？不。和所有观察性研究一样，作者找到了他们寻找的东西。但结论是否由“数据”支持？没有。证据很薄弱。
- en: 'I respect what the authors are trying to do, and I’m a fan of a lot of their
    work! I write this blog just as a warning though: it’s really hard to play by
    the rules of rigor that the metascience community thinks will fix science. And
    I actually think that the proposed fixes of more statistics and methodological
    handcuffs won’t make a dent in the problem.'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 我尊重作者们试图做的事情，我也是他们很多工作的粉丝！我写这篇博客只是作为一个警告：严格遵守元科学界认为能够修复科学的严谨规则是非常困难的。而且我实际上认为更多统计数据和方法学的限制提议不会解决问题。
- en: Instead, the fix is to stop this sort of big data mining and get back to basics.
    People who are experts at reading clinical trials know that the p-value is the
    tiniest piece of the puzzle. In fact, the p-value in a clinical trial is never
    needed. Clinical trials use the most primitive tests to compare outcomes, and
    p-values are just one way of summarizing the three numbers associated with an
    outcome. Any outcome has a number of bad events in the treatment arm, a number
    of bad events in the control arm, and a number of patients in the study. From
    these three numbers, you can compute a p-value. But the p-value here just serves
    as a sanity check to confirm the measured effect of the intervention is large
    enough. After that, you have to look at the trial implementation for other biases
    and potential alternative explanations for the differences between groups. A lot
    goes into building this intuition for reading trial reports, and it takes time
    to learn what to look for. But if you want to fix experiments in clinical medicine,
    this is where you start. A strict Bayesian statistical model of the p-value won’t
    be part of your process at all.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 相反，解决的方法是停止这种大数据挖掘，回归基础。擅长阅读临床试验的专家知道，p 值只是整个 puzzle 中的一小部分。实际上，在临床试验中从未需要 p
    值。临床试验使用最基本的测试来比较结果，而 p 值只是总结与结果相关的三个数字的一种方式。任何结果都有治疗组中的不良事件数量、对照组中的不良事件数量以及研究中的患者数量。根据这三个数字，你可以计算出一个
    p 值。但这里的 p 值只是作为一个合理性检查，以确认干预措施的测量效应是否足够大。之后，你必须查看试验实施是否存在其他偏见以及可能的不同组间差异的潜在替代解释。阅读试验报告需要建立很多直觉，学习需要时间来了解需要关注的内容。但如果你想修复临床医学中的实验，这就是你开始的地方。严格的贝叶斯统计模型中的
    p 值将根本不是你过程的一部分。
