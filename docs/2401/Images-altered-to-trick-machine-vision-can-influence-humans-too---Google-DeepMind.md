<!--yml

类别：未分类

日期：2024-05-27 14:25:45

-->

# 图像被修改以欺骗机器视觉也会影响人类 - Google DeepMind

> 来源：[https://deepmind.google/discover/blog/images-altered-to-trick-machine-vision-can-influence-humans-too/](https://deepmind.google/discover/blog/images-altered-to-trick-machine-vision-can-influence-humans-too/)

研究

# 图像被修改以欺骗机器视觉也会影响人类

发表于

2024 年 1 月 2 日

作者

Gamaleldin Elsayed 和 Michael Mozer

新的研究表明，即使对数字图像进行微小的更改，旨在混淆计算机视觉系统，也会影响人类的感知。

计算机和人类以不同的方式看待世界。我们的生物系统和机器中的人工系统可能并不总是关注相同的视觉信号。经过训练以分类图像的神经网络可以完全被对图像的微小扰动误导，而这些扰动甚至人类都不会注意到。

这种敌对图像可以欺骗AI系统可能指向人类和机器感知之间的根本差异，但这也驱使我们探索在受控测试条件下，人类是否也可能对相同的扰动表现出敏感性。在发表于自然通讯的一系列实验中，我们发现了证据表明，人类的判断确实受到敌对扰动的系统性影响。

我们的发现突显了人类视觉和机器视觉之间的相似之处，但也表明需要进一步研究来了解对人类和AI系统的影响，以及敌对图像的影响。

## 什么是敌对图像？

敌对图像是通过一种导致AI模型自信地对图像内容进行错误分类的程序微妙地改变的图像。这种故意欺骗被称为敌对攻击。攻击可以针对性地导致AI模型将花瓶误认为猫，或者它们可能被设计为使模型看到除花瓶以外的任何东西。

左：一个人工神经网络（ANN）正确将图像分类为花瓶，但当图像被整个图片中的看似随机的模式扰动时（中），为了说明的目的，增加了强度——结果图像（右）被错误且自信地误分类为猫。

这种攻击可能相当隐蔽。在数字图像中，RGB 图像中的每个单独像素都在 0-255 的范围内，表示单个像素的强度。即使没有任何像素的强度超过该范围的 2 级，敌对攻击也可以起效果。

在现实世界中对物体进行的敌对攻击也可能成功，比如将停止标志误认为是限速标志。事实上，安全问题已经促使研究人员探索抵抗敌对攻击和减轻其风险的方法。

## 人类感知如何受到敌对示例的影响？

先前的研究表明，人们可能对提供明显形状线索的大幅度图像扰动敏感。然而，对更微妙的对抗攻击的影响了解较少。人们是否将图像中的扰动视为无害的、随机的图像噪声，或者它能影响人类的感知？

为了找出答案，我们进行了受控的行为实验。首先，我们拍摄了一系列原始图像，并对每个图像进行了两次对抗攻击，以生成许多扰动图像对。在下面的动画示例中，模型将原始图像分类为“花瓶”。然后，通过对原始图像进行对抗攻击扰动的两个图像被模型错误分类为对抗目标“猫”和“卡车”，并且分类置信度很高。

接下来，我们向人类参与者展示了图片对，并提出了一个有针对性的问题：“哪张图片更像猫？”虽然两张图片都与猫毫无关系，但他们必须做出选择，并通常报告感觉自己在做出任意选择。如果大脑对微妙的对抗攻击不敏感，我们预期人们平均每次选择每张图片的概率为`50%`。然而，我们发现对于各种扰动图像对，即使没有像素调整超过`2`级在`0-255`尺度上，选择率——我们称之为感知偏差——也可靠地高于偶然机会。

从参与者的角度来看，他们感觉自己被要求区分两张几乎相同的图像。然而，科学文献充满了证据，表明人们在做出选择时利用了微弱的感知信号，[这些信号对他们来说太微弱了，无法表达信心或意识](https://link.springer.com/article/10.3758/BF03206939)。在我们的例子中，我们可能会看到一瓶花，但大脑中的某些活动告诉我们它有一丝猫的迹象。

左图：对抗图像对的示例。顶部的图像对被微妙地扰动，最大程度上扰动了`2`个像素级别，使神经网络错误地将它们分类为“卡车”和“猫”。要求一个人类志愿者“哪个更像猫？”底部的图像对更明显地被操纵，最大程度上扰动了`16`个像素级别，以被错误分类为“椅子”和“羊”。这次的问题是“哪个更像羊？”

我们进行了一系列实验来排除我们《自然通讯》论文现象的潜在假象解释。在每个实验中，参与者可靠地选择了与目标问题对应的对抗图像超过一半的时间。虽然人类视觉对对抗性扰动的敏感性不如机器视觉（机器不再识别原始图像类别，但人们仍然清楚地看到它），但我们的工作表明，这些扰动仍然可以使人类偏向于机器的决策。

## 人工智能安全和安全研究的重要性

我们的主要发现是，人类感知可以受到对抗性图像的影响，尽管这种影响微妙，但对人工智能安全和安全研究提出了关键问题。通过使用正式实验来探索人工智能视觉系统和人类感知行为的相似性和差异，我们可以利用这些见解来构建更安全的人工智能系统。

例如，我们的研究结果可以为未来的研究提供信息，帮助改善计算机视觉模型的健壮性，使它们与人类视觉表示更好地对齐。通过衡量人类对对抗性扰动的敏感性，可以帮助评估各种计算机视觉架构的对齐情况。

我们的工作还表明，有必要进一步研究技术对机器和人类的更广泛影响。这反过来凸显了认知科学和神经科学在更好地理解人工智能系统及其潜在影响方面的持续重要性，因为我们专注于构建更安全、更可靠的系统。
