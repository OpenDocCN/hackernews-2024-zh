- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 类别：未分类
- en: 'date: 2024-05-27 15:19:48'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024年05月27日15:19:48
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: '🦅 Eagle 7B : Soaring past Transformers with 1 Trillion Tokens Across 100+ Languages
    (RWKV-v5)'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 🦅 Eagle 7B：跨越100多种语言使用1万亿标记的变压器（RWKV-v5）。
- en: 来源：[https://blog.rwkv.com/p/eagle-7b-soaring-past-transformers](https://blog.rwkv.com/p/eagle-7b-soaring-past-transformers)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://blog.rwkv.com/p/eagle-7b-soaring-past-transformers](https://blog.rwkv.com/p/eagle-7b-soaring-past-transformers)
- en: An eagle, flying past a transformer-looking robot
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 一只鹰，飞过一个看起来像变压器的机器人。
- en: 'Eagle 7B is a 7.52B parameter model that:'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: Eagle 7B是一个7.52B参数模型，它：
- en: We are releasing RWKV-v5 Eagle 7B, [licensed as Apache 2.0 license, under the
    Linux Foundation](https://blog.rwkv.com/p/rwkv-joins-the-linux-foundation-as),
    and can be used personally or commercially without restrictions
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 我们发布了RWKV-v5 Eagle 7B，[在Linux基金会下以Apache 2.0许可证授权](https://blog.rwkv.com/p/rwkv-joins-the-linux-foundation-as)，可以个人或商业用途无限制地使用
- en: 'We performed multi-lingual performance across the following benchmarks: [xLAMBDA](https://github.com/EleutherAI/lm-evaluation-harness?tab=readme-ov-file#advanced-usage-tips),
    [xStoryCloze](https://huggingface.co/datasets/Muennighoff/xstory_cloze), [xWinograd](https://huggingface.co/datasets/Muennighoff/xwinograd),
    [xCopa](https://huggingface.co/datasets/xcopa)'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 我们对以下基准测试进行了多语言性能测试：[xLAMBDA](https://github.com/EleutherAI/lm-evaluation-harness?tab=readme-ov-file#advanced-usage-tips)，[xStoryCloze](https://huggingface.co/datasets/Muennighoff/xstory_cloze)，[xWinograd](https://huggingface.co/datasets/Muennighoff/xwinograd)，[xCopa](https://huggingface.co/datasets/xcopa)
- en: Across a total of 23 languages
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在总共23种语言中
- en: Most of these benchmarks cover common sense reasoning, in their respective languages.
    And show a huge overall jump in multi-lingual performance for RWKV v4-to-v5 architecture.
    And the v2 world dataset.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 这些基准大部分涵盖了常识推理，使用各自的语言。并且显示了RWKV v4到v5架构的多语言性能的巨大整体跃升。以及v2世界数据集。
- en: It should also be noted, that there is a lack of multi-lingual benchmarks, as
    the above covers approximately the top 23 languages.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 还应该注意到，由于上述涵盖了大约前23种语言，所以缺乏多语言基准测试。
- en: This makes it hard to evaluate model language performance directly over the
    remaining 75+ languages, over the total 100+ trained languages. A shortcoming
    we hope to improve in future models.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 这使得直接评估剩余的75多种语言的模型语言性能变得困难，超过了总计100多种受过培训的语言。这是我们希望在未来的模型中改进的一个缺点。
- en: English performance was measured across 12 separate benchmarks, across commonsense
    reasoning, and world knowledge
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 英语表现是在12个独立的基准测试中进行的，涵盖常识推理和世界知识。
- en: Once again we see a huge overall jump from RWKV v4-to-v5 architecture. And the
    v2 world dataset.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 我们再次看到从RWKV v4到v5架构的巨大整体跃升。以及v2世界数据集。
- en: Where v4 previously lost out to MPT-7b, the top model in the 1T token tier.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 在v4之前输给了MPT-7b，即1T token层中的顶级模型。
- en: v5 begins trading blows in benchmarks, in some cases even coming on top in certain
    benchmarks ( LAMBADA, StoryCloze16, WinoGrande, HeadQA_en, Sciq ) over Falcon,
    or even llama2.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: v5开始在基准测试中争夺，某些情况下甚至在某些基准测试中（LAMBADA，StoryCloze16，WinoGrande，HeadQA_en，Sciq）超过Falcon，甚至羊驼2。
- en: In addition, v5 performance starts to fall in line with the expected transformer
    performance level, with its given approximate token training count.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 另外，v5的性能开始与预期的变压器性能水平保持一致，考虑到其给定的近似标记训练计数。
- en: With Mistral-7B maintaining its lead with its rumored 2~7 Trillion token training.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 随着狂风-7B保持其领先地位，据说其训练了2到7万亿个标记。
- en: We expect to narrow the gap, as we train an additional 1T token, to cross the
    llama2 line and hopefully reach the mistral line.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 我们期望随着我们额外训练1T token，缩小差距，越过羊驼2线，并希望达到狂风线。
- en: Alternatively, as a base model, which is lightly tuned (really small instruct
    set mixed in), we are eager to see how the various community and instruct-tuned
    variants
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，作为一个基础模型，它轻微调整了（真的很小的指令集混合），我们渴望看到各种社区和指令调整的变体如何。
- en: '* * *'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: A notable observation was that our checkpoints near the 300 Billion token point,
    show similar performance to pythia-6.9b
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 一个显着的观察是，我们的检查点在接近3000亿标记点附近，显示出与pythia-6.9b相似的性能。
- en: This is consistent with previous pile-based experiments on our RWKV-v4 architecture,
    that linear transformers like RWKV scale similarly in performance levels to transformers,
    with the same token count training.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 这与我们以前在RWKV-v4架构上进行的堆实验一致，即线性变压器像RWKV一样按照相同的标记计数进行性能级别的类似规模。
- en: If so, it does repeat the question. If the exact architecture, matter less than
    the data for the model eval performance?
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 如果是这样，它是否重复了问题。如果确切的架构，对于模型评估性能，数据是否比较重要？
- en: CUDA computational cost, for RWKV-based architecture vs transformer models -
    that quadratic-vs-linear really scales!
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: RWKV 基于架构与变压器模型的 CUDA 计算成本——这真是一个二次 vs 线性的比较！
- en: If true, perhaps we should seek more efficient and scalable architecture, to
    increase accessibility, drive the cost of AI downwards for everyone, and [lessen
    the impact on our environment.](https://blog.rwkv.com/p/the-worlds-greenest-ai-model-rwkvs)
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 如果是真的，也许我们应该寻求更高效、可扩展的架构，以提高可访问性，降低人人都能承受得起的 AI 成本，并[减少对环境的影响。](https://blog.rwkv.com/p/the-worlds-greenest-ai-model-rwkvs)
- en: '* * *'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: A common feedback we receive for the RWKV multi-lingual approach is
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 我们接收到的 RWKV 多语言方法的一条常见反馈是
- en: And for most parts, we agree on both points.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 对于大多数部分，我们都同意这两点。
- en: But we have no plans on changing this, as we are building AI for the world -
    which is not just an English world.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 但我们没有计划改变这一点，因为我们正在为全世界构建 AI——这不仅仅是一个英语世界。
- en: '[In 2023, only 17% of the world''s population speaks English](https://preply.com/en/blog/english-language-statistics/#:~:text=Current%20research%20suggests%20that%20the,widely%20spoken%20language%20in%202022%3F)'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: '[2023 年，全球只有 17% 的人口讲英语](https://preply.com/en/blog/english-language-statistics/#:~:text=Current%20research%20suggests%20that%20the,widely%20spoken%20language%20in%202022%3F)'
- en: '[( 1.3 billion people )](https://preply.com/en/blog/english-language-statistics/#:~:text=Current%20research%20suggests%20that%20the,widely%20spoken%20language%20in%202022%3F)'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: （13 亿人口）
- en: 'World Map showing the distribution of regions and people who are fluent in
    English (source: [Wikipedia](https://en.wikipedia.org/wiki/List_of_countries_by_English-speaking_population))'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 世界地图显示流利讲英语的地区和人口分布（来源：[维基百科](https://en.wikipedia.org/wiki/List_of_countries_by_English-speaking_population)）
- en: However, by ensuring support for the top 25 languages in the world and beyond,
    we can cover approximately [4 billion people, or 50% of the world](https://en.wikipedia.org/wiki/List_of_languages_by_number_of_native_speakers#Top_languages_by_population)
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，通过确保支持世界前 25 大语言及以上，我们可以覆盖大约[40 亿人口，约占全球人口的 50%](https://en.wikipedia.org/wiki/List_of_languages_by_number_of_native_speakers#Top_languages_by_population)
- en: Flawed map, highlighting where the eagle language model will support entirely
    or partially - the goal is to be able paint the whole map green with confidence
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 有缺陷的地图，突出显示鹰语言模型将完全或部分支持的区域——目标是能够充满信心地将整个地图涂成绿色
- en: This aligns well with the team’s common goal, of getting AI to support everyone,
    not just by allowing it to run cheaply and affordably even on lower-end hardware.
    But by supporting their language.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 这与团队的共同目标非常一致，即让 AI 支持每个人，不仅仅是通过让其在低端硬件上便宜且可负担，而是通过支持他们的语言。
- en: Over time, we intend to grow the multi-lingual dataset, to support a wider variety
    of languages, and to slowly grow that coverage to 100% of the world - to ensure
    no language gets left behind.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 随着时间的推移，我们打算扩大多语言数据集，支持更广泛的语言种类，并逐步将覆盖范围扩大到全球 100%——以确保没有任何一种语言被落下。
- en: A major example of this in our community is the [Indonesian-NLP discord group](https://discord.gg/dy9YWXjV),
    which finetunes an Indonesian language model from the RWKV line of base models.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 我们社区中的一个主要例子是[印尼 NLP Discord 群](https://discord.gg/dy9YWXjV)，该群会从 RWKV 系列基础模型中微调印尼语言模型。
- en: Allowing them to build strong language-specific models - on a cheap affordable
    basis (ie. single node), without needing to do half a million dollars of pre-training.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 使他们能够在廉价可负担的基础上（即单节点）构建强大的语言特定模型，而不需要进行五十万美元的预训练。
- en: '* * *'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: This release marks the release of the strongest linear transformer (in terms
    of eval benchmarks) to date.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 此版本标志着迄今为止最强的线性变压器（在评估基准方面）的发布。
- en: While it may not have succeeded in passing LLaMA2 and Mistral. It provides strong
    evidence of the following
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然它可能没有成功通过 LLaMA2 和 Mistral。它提供了以下强有力的证据。
- en: The RWKV-v5 model architecture scales similarly to transformer performance with
    a similar token count
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: RWKV-v5 模型架构的规模与变压器性能类似，具有相似的令牌数
- en: You can achieve a near LLaMA2-like level of performance, with a substantially
    lower inference cost
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您可以以极低的推理成本实现接近 LLaMA2 的性能水平
- en: While supporting multi-lingual levels of performance
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 同时支持多语言水平的性能
- en: We plan to follow by pushing further ahead with
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 我们计划继续推进
- en: '[Feb 2024] An updated RWKV v5: Eagle paper, where we will go deeper in-depth
    on the architecture changes since v4, and the model benchmarks and evals'
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[2024 年 2 月] 更新的 RWKV v5：鹰论文，我们将深入探讨自 v4 以来的架构变化，以及模型基准和评估'
- en: '[Feb 2024] A further 1T token in training (2T total), for direct comparisons
    with the LLaMA2 7B model'
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Feb 2024] 另外 1T token 在训练中（总共 2T），以与 LLaMA2 7B 模型进行直接比较'
- en: '[Mar 2024] An MoE model based on the v5 Eagle 2T model'
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Mar 2024] 基于 v5 鹰 2T 模型的 An MoE 模型'
- en: '[Mar 2024] RWKV-v6: “Finch” 1.5B, 3B world models'
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Mar 2024] RWKV-v6：“雀鸟” 1.5B，3B 世界模型'
- en: 'Disclaimer: All dates are approximate, and is heavily subjected to compute
    avaliability from our sponsors/provider'
  id: totrans-52
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 免责声明：所有日期均为近似值，并且受到我们赞助商/供应商计算资源的严重影响
- en: Find more about the RWKV Project at
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 了解更多有关 RWKV 项目的信息，请访问
- en: '* * *'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: 'We are grateful and would like to thank the following key groups:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 我们非常感激并要感谢以下关键团体：
- en: Along with the various developers, working on the growing collection of [RWKV-related
    projects](https://wiki.rwkv.com).
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 与各种开发人员一起，致力于发展 [RWKV 相关项目](https://wiki.rwkv.com) 的人员。
