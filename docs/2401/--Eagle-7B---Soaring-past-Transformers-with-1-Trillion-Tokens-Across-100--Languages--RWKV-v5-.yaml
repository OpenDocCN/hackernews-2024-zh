- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: Êú™ÂàÜÁ±ª'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-05-27 15:19:48'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: 'ü¶Ö Eagle 7B : Soaring past Transformers with 1 Trillion Tokens Across 100+ Languages
    (RWKV-v5)'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Êù•Ê∫êÔºö[https://blog.rwkv.com/p/eagle-7b-soaring-past-transformers](https://blog.rwkv.com/p/eagle-7b-soaring-past-transformers)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: An eagle, flying past a transformer-looking robot
  prefs: []
  type: TYPE_NORMAL
- en: 'Eagle 7B is a 7.52B parameter model that:'
  prefs: []
  type: TYPE_NORMAL
- en: We are releasing RWKV-v5 Eagle 7B, [licensed as Apache 2.0 license, under the
    Linux Foundation](https://blog.rwkv.com/p/rwkv-joins-the-linux-foundation-as),
    and can be used personally or commercially without restrictions
  prefs: []
  type: TYPE_NORMAL
- en: 'We performed multi-lingual performance across the following benchmarks: [xLAMBDA](https://github.com/EleutherAI/lm-evaluation-harness?tab=readme-ov-file#advanced-usage-tips),
    [xStoryCloze](https://huggingface.co/datasets/Muennighoff/xstory_cloze), [xWinograd](https://huggingface.co/datasets/Muennighoff/xwinograd),
    [xCopa](https://huggingface.co/datasets/xcopa)'
  prefs: []
  type: TYPE_NORMAL
- en: Across a total of 23 languages
  prefs: []
  type: TYPE_NORMAL
- en: Most of these benchmarks cover common sense reasoning, in their respective languages.
    And show a huge overall jump in multi-lingual performance for RWKV v4-to-v5 architecture.
    And the v2 world dataset.
  prefs: []
  type: TYPE_NORMAL
- en: It should also be noted, that there is a lack of multi-lingual benchmarks, as
    the above covers approximately the top 23 languages.
  prefs: []
  type: TYPE_NORMAL
- en: This makes it hard to evaluate model language performance directly over the
    remaining 75+ languages, over the total 100+ trained languages. A shortcoming
    we hope to improve in future models.
  prefs: []
  type: TYPE_NORMAL
- en: English performance was measured across 12 separate benchmarks, across commonsense
    reasoning, and world knowledge
  prefs: []
  type: TYPE_NORMAL
- en: Once again we see a huge overall jump from RWKV v4-to-v5 architecture. And the
    v2 world dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Where v4 previously lost out to MPT-7b, the top model in the 1T token tier.
  prefs: []
  type: TYPE_NORMAL
- en: v5 begins trading blows in benchmarks, in some cases even coming on top in certain
    benchmarks ( LAMBADA, StoryCloze16, WinoGrande, HeadQA_en, Sciq ) over Falcon,
    or even llama2.
  prefs: []
  type: TYPE_NORMAL
- en: In addition, v5 performance starts to fall in line with the expected transformer
    performance level, with its given approximate token training count.
  prefs: []
  type: TYPE_NORMAL
- en: With Mistral-7B maintaining its lead with its rumored 2~7 Trillion token training.
  prefs: []
  type: TYPE_NORMAL
- en: We expect to narrow the gap, as we train an additional 1T token, to cross the
    llama2 line and hopefully reach the mistral line.
  prefs: []
  type: TYPE_NORMAL
- en: Alternatively, as a base model, which is lightly tuned (really small instruct
    set mixed in), we are eager to see how the various community and instruct-tuned
    variants
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: A notable observation was that our checkpoints near the 300 Billion token point,
    show similar performance to pythia-6.9b
  prefs: []
  type: TYPE_NORMAL
- en: This is consistent with previous pile-based experiments on our RWKV-v4 architecture,
    that linear transformers like RWKV scale similarly in performance levels to transformers,
    with the same token count training.
  prefs: []
  type: TYPE_NORMAL
- en: If so, it does repeat the question. If the exact architecture, matter less than
    the data for the model eval performance?
  prefs: []
  type: TYPE_NORMAL
- en: CUDA computational cost, for RWKV-based architecture vs transformer models -
    that quadratic-vs-linear really scales!
  prefs: []
  type: TYPE_NORMAL
- en: If true, perhaps we should seek more efficient and scalable architecture, to
    increase accessibility, drive the cost of AI downwards for everyone, and [lessen
    the impact on our environment.](https://blog.rwkv.com/p/the-worlds-greenest-ai-model-rwkvs)
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: A common feedback we receive for the RWKV multi-lingual approach is
  prefs: []
  type: TYPE_NORMAL
- en: And for most parts, we agree on both points.
  prefs: []
  type: TYPE_NORMAL
- en: But we have no plans on changing this, as we are building AI for the world -
    which is not just an English world.
  prefs: []
  type: TYPE_NORMAL
- en: '[In 2023, only 17% of the world''s population speaks English](https://preply.com/en/blog/english-language-statistics/#:~:text=Current%20research%20suggests%20that%20the,widely%20spoken%20language%20in%202022%3F)'
  prefs: []
  type: TYPE_NORMAL
- en: '[( 1.3 billion people )](https://preply.com/en/blog/english-language-statistics/#:~:text=Current%20research%20suggests%20that%20the,widely%20spoken%20language%20in%202022%3F)'
  prefs: []
  type: TYPE_NORMAL
- en: 'World Map showing the distribution of regions and people who are fluent in
    English (source: [Wikipedia](https://en.wikipedia.org/wiki/List_of_countries_by_English-speaking_population))'
  prefs: []
  type: TYPE_NORMAL
- en: However, by ensuring support for the top 25 languages in the world and beyond,
    we can cover approximately [4 billion people, or 50% of the world](https://en.wikipedia.org/wiki/List_of_languages_by_number_of_native_speakers#Top_languages_by_population)
  prefs: []
  type: TYPE_NORMAL
- en: Flawed map, highlighting where the eagle language model will support entirely
    or partially - the goal is to be able paint the whole map green with confidence
  prefs: []
  type: TYPE_NORMAL
- en: This aligns well with the team‚Äôs common goal, of getting AI to support everyone,
    not just by allowing it to run cheaply and affordably even on lower-end hardware.
    But by supporting their language.
  prefs: []
  type: TYPE_NORMAL
- en: Over time, we intend to grow the multi-lingual dataset, to support a wider variety
    of languages, and to slowly grow that coverage to 100% of the world - to ensure
    no language gets left behind.
  prefs: []
  type: TYPE_NORMAL
- en: A major example of this in our community is the [Indonesian-NLP discord group](https://discord.gg/dy9YWXjV),
    which finetunes an Indonesian language model from the RWKV line of base models.
  prefs: []
  type: TYPE_NORMAL
- en: Allowing them to build strong language-specific models - on a cheap affordable
    basis (ie. single node), without needing to do half a million dollars of pre-training.
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: This release marks the release of the strongest linear transformer (in terms
    of eval benchmarks) to date.
  prefs: []
  type: TYPE_NORMAL
- en: While it may not have succeeded in passing LLaMA2 and Mistral. It provides strong
    evidence of the following
  prefs: []
  type: TYPE_NORMAL
- en: The RWKV-v5 model architecture scales similarly to transformer performance with
    a similar token count
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You can achieve a near LLaMA2-like level of performance, with a substantially
    lower inference cost
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: While supporting multi-lingual levels of performance
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We plan to follow by pushing further ahead with
  prefs: []
  type: TYPE_NORMAL
- en: '[Feb 2024] An updated RWKV v5: Eagle paper, where we will go deeper in-depth
    on the architecture changes since v4, and the model benchmarks and evals'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Feb 2024] A further 1T token in training (2T total), for direct comparisons
    with the LLaMA2 7B model'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Mar 2024] An MoE model based on the v5 Eagle 2T model'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Mar 2024] RWKV-v6: ‚ÄúFinch‚Äù 1.5B, 3B world models'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Disclaimer: All dates are approximate, and is heavily subjected to compute
    avaliability from our sponsors/provider'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Find more about the RWKV Project at
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: 'We are grateful and would like to thank the following key groups:'
  prefs: []
  type: TYPE_NORMAL
- en: Along with the various developers, working on the growing collection of [RWKV-related
    projects](https://wiki.rwkv.com).
  prefs: []
  type: TYPE_NORMAL
