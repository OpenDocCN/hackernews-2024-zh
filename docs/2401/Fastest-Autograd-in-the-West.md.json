["```\ndef run_graph(initial_variables, n_operations: int):\n    nodes = [*initial_variables]\n\n    for op in range(n_operations):\n        match op % 4:\n            case 0:\n                # softplus\n                nodes.append(F.softplus(nodes[-10]))\n            case 1:\n                # sum\n                nodes.append(sum(nodes[-30:-10:5]))\n            case 2:\n                # prod\n                nodes.append(nodes[-20] * nodes[-10])\n            case 3:\n                # softmax\n                softmaxes = F.softmax(torch.stack(nodes[-4:], dim=0), dim=0)\n                nodes.extend(softmaxes)\n\n    return nodes\n\ndef run_benchmark_pytorch(n_iterations, n_operations):\n    init_vars = torch.arange(100, dtype=torch.float32, requires_grad=True)\n    for _ in range(n_iterations):\n        nodes = run_graph(\n            initial_variables=init_vars,\n            n_operations=n_operations,\n        )\n        nodes[-1].backward() \n```", "```\nimport jax\nimport numpy as np\n\ndef run_graph_jax(initial_variables):\n    nodes = [*initial_variables]\n    for op in range(n_operations):\n        match op % 4:\n            case 0:\n                # softplus\n                nodes.append(jax.nn.softplus(nodes[-10]))\n            case 1: \n                # sum\n                nodes.append(sum(nodes[-30:-10:5]))\n            case 2: \n                # prod \n                nodes.append(nodes[-20] * nodes[-10])\n            case 3: \n                # softmax\n                softmaxes = jax.nn.softmax(jax.numpy.stack(nodes[-4:]), axis=0)\n                nodes.extend(softmaxes)\n\n    return nodes[-1]\n\nrun_graph_and_grad = jax.value_and_grad(run_graph_jax)\n# or run_graph_and_grad = jax.jit(jax.value_and_grad(run_graph_jax)) \n```", "```\nclass NaiveVar:\n    def __init__(self, val):\n        self.val = val\n        self.grad = 0.\n\nclass NaiveTape:\n    def __init__(self, input_values):\n        self.ops = []\n\n    def sum(self, *vars):\n        res = NaiveVar(sum(v.val for v in vars))\n        self.ops.append(('sum', vars, res))\n        return res\n\n    def prod(self, var1, var2):\n        res = NaiveVar(var1.val * var2.val)\n        self.ops.append(('prod', [var1, var2], res))\n        return res\n\n    def softmax(self, *vars):\n        vals = [v.val for v in vars]\n        maxval = max(vals)\n        vals = [v - maxval for v in vals]\n        denom = sum(math.exp(v) for v in vals)\n        res = [NaiveVar(math.exp(v) / denom) for v in vals]\n        self.ops.append(('softmax', vars, denom))\n        return res\n\n    def softplus(self, var):\n        res = NaiveVar(math.log1p(math.exp(var.val)))\n        self.ops.append(('splus', var, res))\n        return res\n\n    def backward(self, var):\n        assert var.grad == 0\n        var.grad += 1\n        for op, inputs, outputs in self.ops[::-1]:\n            match op:\n                case 'sum':\n                    out = outputs\n                    for v in inputs:\n                        v.grad += out.grad\n                case 'prod':\n                    out = outputs\n                    in1, in2 = inputs\n                    in1.grad += in2.val * out.grad\n                    in2.grad += in1.val * out.grad\n                case 'splus':\n                    inputs.grad += out.grad / (1 + math.exp(-inputs.val))\n                case 'softmax':\n                    pass # skip for now\n                case _:\n                    raise NotImplementedError() \n```", "```\ndef run_graph_python_and_backward(initial_variables, n_operations):\n    nodes = [NaiveVar(x) for x in initial_variables]\n    tape = NaiveTape(nodes)\n    for op in range(n_operations):\n        match op % 4:\n            case 0: \n                # softplus\n                nodes.append(tape.softplus(nodes[-10]))\n            case 1: \n                # sum\n                nodes.append(tape.sum(*nodes[-30:-10:5]))\n            case 2: \n                # prod \n                nodes.append(tape.prod(nodes[-20], nodes[-10]))\n            case 3: \n                # softmax\n                nodes.extend(tape.softmax(*nodes[-4:]))\n\n    tape.backward(nodes[-1])\n    return tape \n```", "```\nimport numba\nimport math\n\nclass VarInd:\n    def __init__(self, index):\n        self.index = index # variable is just a unique index in tape\n\nclass TapeInd:\n    def __init__(self):\n        self.ops = []\n        self.vals = []  # flat memory with values\n        self.grads = [] # flat memory with gradients \n    def make_var(self, value):\n        self.vals.append(value)\n        self.grads.append(0.)\n        return VarInd(len(self.vals) - 1)\n\n    def val(self, v: VarInd):\n        return self.vals[v.index]\n\n    def add_op(self, kls, input_vars, output_vars):\n\t    # translate variable to indices. self.ops keeps only indices\n        self.ops.append((kls, [x.index for x in input_vars], [x.index for x in output_vars]))        \n\n    def sum(self, *vars):\n        res = self.make_var(sum(self.val(v) for v in vars))\n        self.add_op('sum', vars, [res])\n        return res\n\n    def prod(self, var1, var2):\n        res = self.make_var(self.val(var1) * self.val(var2))\n        self.add_op('prod', [var1, var2], [res])\n        return res\n\n    def softmax(self, *vars):\n        vals = [self.val(v) for v in vars]\n        maxval = max(vals)\n        vals = [v - maxval for v in vals]\n        denom = sum(math.exp(v) for v in vals)\n        res = [self.make_var(math.exp(v) / denom ) for v in vals]\n        self.add_op('softmax', vars, res)\n        return res\n\n    def softplus(self, var):\n        res = self.make_var(math.log1p( math.exp(self.val(var)) ))\n        self.add_op('splus', [var], [res])\n        return res\n\n    def forward_backward_external(self, grad_var: VarInd):\n        return forward_backward_optimal(self.vals, self.grads, self.ops, grad_var_index=grad_var.index)\n\ndef forward_backward_external(\n\tvals: list[float], \n\tgrads: list[float], \n\tops: list[tuple[str, list[int], list[int]]],\n\tgrad_var_index: int\n):\n    v: list[float] = vals\n    g: list[float] = grads\n    # forward pass\n    for op, ins, outs in ops:\n        match op:\n            case 'sum':\n                v[outs[0]] = sum(v[i] for i in ins)\n            case 'prod':\n                v[outs[0]] = v[ins[0]] * v[ins[1]]\n            case 'splus':\n                v[outs[0]] = math.log1p(math.exp( v[ins[0]] ))\n            case 'softmax':\n                maximal = max(v[i] for i in ins)\n                exps = [math.exp(v[i] - maximal) for i in ins]\n                denom = sum(outs)\n                for i, exp in zip(outs, exps):\n                    v[i] = exp / denom\n\n    g[grad_var_index] += 1\n\n\t# backward pass\n    for op, ins, outs in ops[::-1]:\n        match op:\n            case 'sum':\n                for i in ins:\n                    g[i] += g[outs[0]]\n            case 'prod':\n                out: int = outs[0]\n                in1, in2 = ins\n                g[in1] += v[in2] * g[out]\n                g[in2] += v[in1] * g[out]\n            case 'splus':\n                g[ins[0]] += g[outs[0]] / (1 + math.exp(-v[ins[0]]))\n            case 'softmax':\n\t\t\t\tavg_grad = sum(v[j] * g[j] for j in outs)\n\t\t\t\tfor i, j in zip(ins, outs):\n\t\t\t\t\tg[i] += v[j] * (g[j] - avg_grad) \n```", "```\ndef run_graph_python_and_backward(n_operations, n_iterations):\n    tape = TapeInd()\n    nodes = [tape.make_var(float(x)) for x in range(100)]\n\n    for op in range(n_operations):\n        match op % 4:\n            case 0: \n                # softplus\n                nodes.append(tape.softplus(nodes[-10]))\n            case 1: \n                # sum\n                nodes.append(tape.sum(*nodes[-30:-10:5]))\n            case 2: \n                # prod \n                nodes.append(tape.prod(nodes[-20], nodes[-10]))\n            case 3: \n                # softmax\n                softmaxes = tape.softmax(*nodes[-4:])\n                nodes.extend(softmaxes)\n\n    for _ in range(n_iterations):\n        tape.forward_backward(nodes[-1]) \n```", "```\n// rustimport:pyo3\nuse pyo3::prelude::*;\n\n// slower softmax version for larger number of inputs\nfn softmax_varlength(vals: &mut Vec<f32>, ins: &[usize], outs: &[usize]) {\n    let mut max = -1e20_f32;\n    let loc_vals: Vec<f32> = ins.into_iter().map(|i| { let x = vals[*i]; max = max.max(x); x} ).collect();\n    let mut sum: f32 = 0.0_f32;\n    let exps: Vec<f32> = loc_vals.iter().map(|v| {let _exp = f32::exp(*v - max); sum += _exp; _exp}).collect();\n    outs.iter().zip(exps.iter()).for_each(|(j, exp)| vals[*j] = exp / sum );\n}\n\n// vecs are slow! so allocate slices on stack, and explicit grouping of computations also helps\nfn softmax<const N: usize>(vals: &mut Vec<f32>, ins: &[usize], outs: &[usize]) {\n    let mut loc_vals: [f32; N] = [0_f32; N];\n    let mut exps: [f32; N] = [0_f32; N];\n    let mut max = -1e20_f32;\n    let mut sum: f32 = 0.;\n    for (n, i) in ins.into_iter().enumerate() {\n        let v = vals[*i];\n        loc_vals[n] = v;\n        max = max.max(v);\n    }\n    for (n, _i) in ins.into_iter().enumerate() {\n        let exp = f32::exp(loc_vals[n] - max);\n        exps[n] = exp;\n        sum += exp;\n    }\n    let invsum = 1.0_f32 / sum;\n    for (n, j) in outs.into_iter().enumerate() {\n        vals[*j] = exps[n] * invsum;\n    }\n}\n\nfn sigmoid(x: f32) -> f32 {\n    1.0 / (1.0 + (-x).exp())\n}\n\n#[pyfunction]\nunsafe fn autograd(\n    vals_input: Vec<f32>,\n    ops: Vec<i32>,\n    input_ids: Vec<Vec<usize>>, \n    output_ids: Vec<Vec<usize>>,\n    backward_node_id: usize,\n    n_iteration: i32,\n) -> (Vec<f32>, Vec<f32>) {\n    let mut vals: Vec<f32> = vals_input.iter().map(|x| *x).collect();\n    let mut grad: Vec<f32> = vals_input.into_iter().map(|_| 0.0_f32).collect();\n\n    for _ in 0..n_iteration {\n        for (i_op, op) in ops.iter().enumerate(){\n            let ins: &Vec<usize> = &input_ids[i_op];\n            let outs: &Vec<usize> = &output_ids[i_op];\n\n            match op {\n                0 => {\n                    // softplus\n                    let x = vals[ins[0]];\n                    let max = f32::max(0., x);\n                    let min = f32::min(0., x);\n                    vals[outs[0]] = max + f32::ln_1p(f32::exp(min - max));\n                }\n                1 => {\n                    // sum\n                    vals[outs[0]] = ins.iter().map(|i| vals.get_unchecked(*i)).sum();\n                }\n                2 => {\n                    // prod\n                    vals[outs[0]] = vals[ins[0]] * vals[ins[1]];\n                }\n                3 => {\n                    // softmax. we will need switch-case resolution here for most common cases\n                    match ins.len() {\n                        1 => {softmax::<1>(&mut vals, &ins, &outs)}\n                        2 => {softmax::<2>(&mut vals, &ins, &outs)}\n                        3 => {softmax::<3>(&mut vals, &ins, &outs)}\n                        4 => {softmax::<4>(&mut vals, &ins, &outs)}\n                        5 => {softmax::<5>(&mut vals, &ins, &outs)}\n                        _ => {softmax_varlength(&mut vals, &ins, &outs)}\n                    }\n                }\n                _ => { panic!(\"\"); }\n           }\n        }\n        grad[backward_node_id] = 1.;\n\n        for (i_op, op) in ops.iter().enumerate(){\n            let ins: &Vec<usize> = &input_ids[i_op];\n            let outs: &Vec<usize> = &output_ids[i_op];\n\n            match op {\n                0 => {\n                    // softplus\n                    grad[ins[0]] += grad[outs[0]] * sigmoid(vals[ins[0]]);\n                }\n                1 => {\n                    // sum\n                    ins.iter().for_each(|i| grad[*i] += grad[outs[0]]);\n                }\n                2 => {\n                    // prod\n                    grad[ins[0]] += grad[outs[0]] * vals[ins[1]];\n                    grad[ins[1]] += grad[outs[0]] * vals[ins[0]];\n                }\n                3 => {\n\t                // softmax\n                    let avg_grad: f32 = outs.iter().map(|j| grad[*j] * vals[*j] ).sum();\n                    for (i, j) in ins.iter().zip(outs.iter()) {\n                        grad[*i] += vals[*j] * (grad[*j] - avg_grad);\n                    }\n                }\n                _ => { panic!(\"\"); }\n           }\n        }        \n    }\n    (vals, grad)\n} \n```", "```\n#include <math.h>  \ntypedef struct { \n    int opcode;\n    size_t n_arguments; // used for softmax and sum\n    int ins[8];         // at most 8 inputs\n    int out;            // points to the first output variable\n} MyOperation;\n\nMyOperation * allocate_memory(int n_elements) {\n    return (MyOperation *) malloc(sizeof(MyOperation) * n_elements);\n}\n\n// stable implementation\ndouble logaddexp(double x, double y) {\n    if (x > y) { return x + log1p(exp(y - x)); }\n    else       { return y + log1p(exp(x - y)); }\n}\n\ndouble sigmoid(double x) { return 1.0 / (1.0 + exp(-x)); }\n\nvoid run_multiple_passes(\n    int n_operations,\n    MyOperation *ops,\n    double *values,\n    double *grads,\n    int n_iterations\n) {\n    for(int iteration = 0; iteration < n_iterations; iteration++) {\n        for(int operation = 0; operation < n_operations; operation++) {\n            MyOperation op = ops[operation];\n            switch(op.opcode) {\n                case 1: \n                    values[op.out] = logaddexp(0., values[op.ins[0]]);\n                    break;\n                case 2: \n                    {\n                        double out = 0.;\n                        for(size_t i=0; i < op.n_arguments; i++) {\n                            out += values[op.ins[i]];\n                        }\n                        values[op.out] = out;\n                    }\n                    break;\n                case 3:\n                    values[op.out] = values[op.ins[0]] * values[op.ins[1]];\n                    break;\n                case 4:\n                    {\n                        double maximal = -1e20;\n                        size_t n_arg = (size_t) op.n_arguments;\n                        for(size_t i = 0; i < n_arg; i++) {\n                            maximal = fmax(maximal, values[op.ins[i]]);\n                        }\n                        double exps[n_arg];\n                        double sum = 0;\n                        for(size_t i = 0; i < n_arg; i++) {\n                            exps[i] = exp(op.ins[i] - maximal);\n                            sum += exps[i];\n                        }\n                        for(size_t i = 0; i < n_arg; i++) {\n                            values[op.out + i] = exps[i] / sum;\n                        }\n                    }\n                    break;\n            }\n        }  // end forward\n\n        // TODO set grad for target variable.\n\n        for(int operation = 0; operation < n_operations; operation++) {\n            MyOperation op = ops[n_operations - 1 - operation];\n            switch(op.opcode) {\n                case 1: \n                    grads[op.ins[0]] += grads[op.out] * sigmoid(values[op.ins[0]]);\n                    break;\n                case 2: \n                    {\n                        for(size_t i=0; i < op.n_arguments; i++) { grads[op.ins[i]] += grads[op.out]; }\n                    }\n                    break;\n                case 3:\n                    grads[op.ins[0]] += grads[op.out] * values[op.ins[1]];\n                    grads[op.ins[1]] += grads[op.out] * values[op.ins[0]];\n                    break;\n                case 4:\n                    {\n                        size_t n_arg = (size_t) op.n_arguments;\n                        double avg_grad = 0.0;\n                        for(size_t i = 0; i < n_arg; i++) {\n                            avg_grad += values[op.out + i] * grads[op.out + i];\n                        }\n                        for(size_t i = 0; i < n_arg; i++) {\n                            grads[op.ins[i]] += values[op.out + i] * (grads[op.out + i] - avg_grad);\n                        }\n                    }\n                    break;\n            }\n        }  // end backward\n    }\n} \n```", "```\n...\nvals[215] = vals[195] * vals[205];\nvals[216] = vals[196] + vals[201] + vals[204];\n... // etcetc, and then backward steps are also written the same way \n```", "```\n;; canonical stack-machine way to compute a * b + c\n(local.get $a)\n(local.get $b)\nf32.mul\n(local.get $c)\nf32.add\n\n;; another way to say write the same, also perfectly legal in wasm\n(f32.add \n    (f32.mul (local.get $a) (local.get $b))  \n    (local.get $c) \n) \n```"]