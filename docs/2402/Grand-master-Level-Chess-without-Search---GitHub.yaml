- en: <!--yml
  id: totrans-split-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-split-1
  prefs: []
  type: TYPE_NORMAL
  zh: 类别：未分类
- en: 'date: 2024-05-27 14:44:40'
  id: totrans-split-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024年5月27日14:44:40
- en: -->
  id: totrans-split-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: Grand-master Level Chess without Search · GitHub
  id: totrans-split-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 无搜索的国际象棋大师级别 · GitHub
- en: 来源：[https://gist.github.com/yoavg/8b98bbd70eb187cf1852b3485b8cda4f](https://gist.github.com/yoavg/8b98bbd70eb187cf1852b3485b8cda4f)
  id: totrans-split-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://gist.github.com/yoavg/8b98bbd70eb187cf1852b3485b8cda4f](https://gist.github.com/yoavg/8b98bbd70eb187cf1852b3485b8cda4f)
- en: 'Grand-master Level Chess without Search: Modeling Choices and their Implications'
  id: totrans-split-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 无搜索的国际象棋大师级别：建模选择及其影响
- en: '[](#grand-master-level-chess-without-search-modeling-choices-and-their-implications)'
  id: totrans-split-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[](#grand-master-level-chess-without-search-modeling-choices-and-their-implications)'
- en: Yoav Golderg, February 2024.
  id: totrans-split-8
  prefs: []
  type: TYPE_NORMAL
  zh: Yoav Golderg，2024年2月。
- en: '* * *'
  id: totrans-split-9
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Researchers at Google DeepMind released a [paper](https://arxiv.org/pdf/2402.04494.pdf)
    about a learned systems that is able to play blitz-chess at a grandmaster level,
    without using search. This is interesting and imagination-capturing, because up
    to now computer-chess systems that play at this level, either based on machine-learning
    or not, did use a search component.^([1](#user-content-fn-1-1db3ccd16c3fe9d8fc359c6d4aaee58b))
  id: totrans-split-10
  prefs: []
  type: TYPE_NORMAL
  zh: 谷歌DeepMind的研究人员发布了一篇关于一个学习系统的[论文](https://arxiv.org/pdf/2402.04494.pdf)，该系统能够在大师级别上玩闪电国际象棋，而不使用搜索。这很有趣，也很具有想象力，因为到目前为止，无论基于机器学习还是不是，都使用了搜索组件。^([1](#user-content-fn-1-1db3ccd16c3fe9d8fc359c6d4aaee58b))
- en: Indeed, my first reaction when reading the paper was to tweet `wow, crazy and
    interesting`. I still find it crazy and interesting, but upon a closer read, it
    may not be as crazy and as interesting as I initially thought. Many reactions
    on twitter, reddit, etc, were super-impressed, going into implications about projected
    learning abilities of AI systems, the ability of neural networks to learn semantics
    from observations, etc, which are really over-the-top. The paper does not claim
    any of them, but they are still perceived and talked about by many readers.
  id: totrans-split-11
  prefs: []
  type: TYPE_NORMAL
  zh: 事实上，当我阅读这篇论文时的第一反应是发推文 `哇，太疯狂和有趣了`。我仍然觉得这很疯狂和有趣，但在更仔细地阅读后，它可能并不像我最初想象的那么疯狂和有趣。Twitter、Reddit等的许多反应都非常印象深刻，涉及到AI系统预期学习能力的影响、神经网络能够从观察中学习语义等等，这些真的过于夸张了。论文并没有声称任何这些，但仍然被许多读者所感知和谈论。
- en: On the other side of being utterly impressed and making far-reaching assertions
    about the abilities of machine-learning, manu reactions turned to diminishing
    the result based on the fact that it was only achieved for blitz-chess, a fast
    moving game with a strict time limit, where players don't have much time think
    about each move, and not for a full-on chess game.
  id: totrans-split-12
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，对于机器学习能力的深远断言而言，许多反应则转向于减少基于事实，即此成就仅限于快速移动、严格时间限制的闪电国际象棋，玩家没有太多时间考虑每一步棋，而非整场国际象棋比赛。
- en: I am also not utterly impressed, but I will tackle it from a different angle
    than focusing on the blitz nature.
  id: totrans-split-13
  prefs: []
  type: TYPE_NORMAL
  zh: 我也不是完全印象深刻，但我将从一个与关注闪电性质不同的角度来处理它。
- en: 'In short, a naive reader (and I was such a naive reader on my first read) may
    conclude the following:'
  id: totrans-split-14
  prefs: []
  type: TYPE_NORMAL
  zh: 简言之，一个天真的读者（我在第一次阅读时也是这样的天真读者）可能得出以下结论：
- en: 'DeepMind demonstrated a machine learning model which satisfies all of:'
  id: totrans-split-15
  prefs: []
  type: TYPE_NORMAL
  zh: DeepMind展示了一种机器学习模型，它满足以下所有条件：
- en: Learned and is able to follow the rules of chess
  id: totrans-split-16
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 学习并能够遵循国际象棋的规则。
- en: Learned and is able to play the game at a grandmaster level
  id: totrans-split-17
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 学习并能够以大师级别玩游戏。
- en: Learned purely from observing chess games and their outcomes
  id: totrans-split-18
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 纯粹通过观察国际象棋比赛及其结果进行学习。
- en: Learned using a purely-ML system, without any game specific biases encoded in
    the training procedure
  id: totrans-split-19
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用纯ML系统学习，没有在训练过程中对游戏特定偏见进行编码。
- en: In particular, believing points 1-4 lead people to conclusions about other learning
    systems, such as large language models, along the lines of "If a transformer network
    can learn the rules/semantics of chess, including the ability to perform search,
    just from looking at games, then it is further evidence that we might be able
    to learn the true semanttics of language just from observing written texts".
  id: totrans-split-20
  prefs: []
  type: TYPE_NORMAL
  zh: 特别是，认为1-4点导致人们对其他学习系统（例如大型语言模型）的结论，例如“如果一个变压器网络可以仅通过观察游戏来学习国际象棋的规则/语义，包括执行搜索的能力，那么这进一步证明我们可能能够仅通过观察书面文本来学习语言的真正语义”。
- en: 'However, **none of these points are true**! And to be fair, the paper doesn''t
    claim them. It is well written, and doesn''t over-claim (expcept for claiming
    a resulting system with no search... we''ll get to that). Upon some more careful
    reading and thinking, one can actually realize that:'
  id: totrans-split-21
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，**这些观点都不成立**！公平地说，论文并没有宣称它们。它写得很好，没有过度宣称（除了声称一个没有搜索的结果系统...我们会谈到这一点）。在更仔细地阅读和思考后，人们实际上可以意识到：
- en: Points (1) and (2) were not achieved, and in fact are not possible at all with
    the learning setup used in the paper.
  id: totrans-split-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 点(1)和(2)没有实现，并且实际上在论文中使用的学习设置中根本不可能实现。
- en: Moreover, point (1) and (2) are likely not possible in *any* setup that follows
    points (3) and (4).
  id: totrans-split-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 此外，点(1)和(2)在*任何*遵循点(3)和(4)的设置中可能都不可行。
- en: Points (3) and (4) are not supported by the setup used in the paper.
  id: totrans-split-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 点(3)和(4)不受论文中使用的设置支持。
- en: So, the paper and its results does not support any further claims or hypotheses
    about other learning scenarios and the ability of transformers to learn semantics
    from form.
  id: totrans-split-25
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，这篇论文及其结果并不支持关于其他学习场景和变压器从形式中学习语义能力的任何进一步主张或假设。
- en: Still, many readers did in fact have points (1-4) above in mind when reading
    the paper or its abstract (if you didn't, you can stop reading now). In what follows,
    I'll dig in into why points (1-4) do not hold. Beyond better understanding of
    the current paper, I think it also serves as a useful demonstration as to how
    seemingly neutral and natural modeling choices and data representation in fact
    pack a lot of punch w.r.t to what's actually being learned and why.
  id: totrans-split-26
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管如此，在阅读论文或其摘要时，许多读者确实考虑到了上述(1-4)点（如果你没有，现在可以停止阅读）。接下来，我将深入探讨为什么(1-4)点不成立。在更好地理解当前论文的基础上，我认为这也是一个有用的演示，展示了看似中立和自然的建模选择以及数据表示实际上对实际学习内容以及其原因有重大影响。
- en: The researchers took 10M human-human chess games from an online chess arena.
    These correspond to roughly 500M board positions. For each board position and
    legal move (a total of somewhat above 15 billion pairs), they asked the Stockfish
    16 engine for its estimate of the "winning probability" for this board+move, based
    on Stockfish's analysis of up to 0.05 seconds per move. They then trained a transformer-based
    model whose input is the board position+move, and the target output is the winning
    probability of the move.^([2](#user-content-fn-2-1db3ccd16c3fe9d8fc359c6d4aaee58b))
    This is called action-value prediction.^([3](#user-content-fn-3-1db3ccd16c3fe9d8fc359c6d4aaee58b))
  id: totrans-split-27
  prefs: []
  type: TYPE_NORMAL
  zh: 研究人员从在线国际象棋竞技场获取了1000万场人类对弈棋局。这些对应大约500百万个棋盘位置。对于每个棋盘位置和合法移动（总计略高于150亿对），他们要求Stockfish
    16引擎基于每步最多0.05秒的分析给出“获胜概率”的估计。然后训练了一个基于变压器的模型，其输入是棋盘位置+移动，目标输出是移动的获胜概率。^([2](#user-content-fn-2-1db3ccd16c3fe9d8fc359c6d4aaee58b))
    这被称为动作价值预测。^([3](#user-content-fn-3-1db3ccd16c3fe9d8fc359c6d4aaee58b))
- en: 'Conceptually^([4](#user-content-fn-4-1db3ccd16c3fe9d8fc359c6d4aaee58b)) the
    resulting model was then used to play games by the following, simple procedure:
    at each turn, use the trained model by having it look at the board position +
    each possible move, and predict the winning probability for this board+move pair.
    Then play the move with the highest probability of winning. That''s it. Only model
    predictions, no looking-ahead search.'
  id: totrans-split-28
  prefs: []
  type: TYPE_NORMAL
  zh: 从概念上讲^([4](#user-content-fn-4-1db3ccd16c3fe9d8fc359c6d4aaee58b))，随后使用了以下简单的过程来玩游戏：在每个回合，通过训练模型让其查看棋盘位置+每个可能的移动，并预测这个棋盘+移动对的获胜概率。然后选择具有最高获胜概率的移动。就是这样。仅仅是模型预测，没有向前搜索。
- en: This procedure proved to be remarkably strong, achieving grand-master level
    when playing against humans in a blitz-chess setting.
  id: totrans-split-29
  prefs: []
  type: TYPE_NORMAL
  zh: 这一过程被证明非常强大，在与人类在闪电国际象棋中对战时达到了大师级水平。
- en: 'This is impressive. The AI system was able to only look at board positions
    and their outcomes, and use this to learn a policy that enable it to pick a strong
    move for each position, strong enough to actually win against very strong human
    players when applied repeatedly. This was not done by mere memorization: the games
    against humans included many board positions that were not seen by the system
    during its training. The system learned to generalize from seen position to these
    novel ones, and still suggest strong moves leading to wins. And all this without
    "looking ahead".'
  id: totrans-split-30
  prefs: []
  type: TYPE_NORMAL
  zh: 这是令人印象深刻的。AI系统只能查看棋盘位置及其结果，并利用这一点学习策略，使其能够在应用时反复选择出强力着法，强到足以击败非常强大的人类选手。这不是单纯的记忆：与人类的比赛中包含了系统在训练期间未见过的许多棋盘位置。系统学会了从已见到的位置泛化到这些新颖的位置，并仍然建议出能导致胜利的强力着法。而且所有这些都是在"不预先查看"的情况下完成的。
- en: 'One possible interpretation of this result, which I''ve seen many people make
    online (and which also crossed my own mind) is:'
  id: totrans-split-31
  prefs: []
  type: TYPE_NORMAL
  zh: 这个结果的一个可能解释是，我在网上见过很多人提出的（而且我自己也曾有过类似的想法）：
- en: '[PRE0]'
  id: totrans-split-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Points (1-4) above are another variant of the same interpretation. However,
    this interpretation *is wrong*. None of it is true.
  id: totrans-split-33
  prefs: []
  type: TYPE_NORMAL
  zh: 上述（1-4）点是同一解释的另一种变体。然而，这种解释*是错误的*。没有一点是真的。
- en: Before understanding why, let's take a small detour to discuss the scale involved
    in learning chess patterns from observing 10M games, and how it relates to how
    humans may learn to play.
  id: totrans-split-34
  prefs: []
  type: TYPE_NORMAL
  zh: 在理解原因之前，让我们稍作偏离，讨论从观察1000万场比赛中学习棋局模式涉及的规模，以及它如何与人类学习玩棋的方式相关。
- en: Humans learn very differently.
  id: totrans-split-35
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 人类学习方式非常不同。
- en: '[](#humans-learn-very-differently)'
  id: totrans-split-36
  prefs: []
  type: TYPE_NORMAL
  zh: '[](#humans-learn-very-differently)'
- en: The system learned from 10M games. That's a lot. Let's assume a hypothetical
    extremely dedicated professional human player, who studies chess 16 hours a day,
    and is able to analyze a game per minute, 365 days a year. Such a player will
    go over 10M games in about 28.5 years. If this player takes not 1 minute but 5
    minutes to analyze a game, they will need 142 years to go over these 10M games.
    There are grandmaster-level chess players who are 13 years old. So clearly human
    learn differently and much more efficiently. They likely have a very different
    kind of knowledge of the game, at least in the process of learning it. We'll return
    to this calculation later.
  id: totrans-split-37
  prefs: []
  type: TYPE_NORMAL
  zh: 系统从1000万场比赛中学习了。那很多。假设有一个假想的极其专注的职业人类棋手，每天学习国际象棋16小时，并能分析每一局棋，一年365天。这样的棋手将在大约28.5年内研究完这1000万场比赛。如果这位棋手不是1分钟分析一场，而是5分钟分析一场，他们将需要142年来研究这1000万场比赛。有13岁的国际象棋大师级别的棋手。因此很明显人类学习方式不同，并且效率要高得多。至少在学习过程中，他们可能对游戏有一种非常不同的知识。稍后我们会回到这个计算。
- en: Everything is a modeling choice. Modeling choices affect what can be learned.
  id: totrans-split-38
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 一切都是建模选择。建模选择影响可以学到的内容。
- en: '[](#everything-is-a-modeling-choice-modeling-choices-affect-what-can-be-learned)'
  id: totrans-split-39
  prefs: []
  type: TYPE_NORMAL
  zh: '[](#everything-is-a-modeling-choice-modeling-choices-affect-what-can-be-learned)'
- en: 'The authors seemingly chose a neutral representation and task: input is board
    position, output is the winning probability of a move. Simple, natural, "neutral".
    Note however, that this is still *a choice*, and more over, this choice (as discussed
    in the paper''s section 5, "Discussion") resulted in a system that did not, in
    fact, learn the rules of the game (point 1), and which *cannot* learn the rules
    of the game, no matter how strong the learner is. This modeling choice also prevents
    *any learner* from playing "like a grandmaster" (point 2). Let''s see why, using
    two failure modes discussed in the paper. The first demonstrates inability to
    learn the rules of the game. The second demonstrates inability to learn to play
    like a grandmaster. These are very nice examples of consequences of modeling choices.'
  id: totrans-split-40
  prefs: []
  type: TYPE_NORMAL
  zh: 作者们显然选择了一个中立的表征和任务：输入是棋盘位置，输出是移动的获胜概率。简单、自然、"中立"。然而请注意，这仍然*是一个选择*，而且，更重要的是，这个选择（正如本文第5节“讨论”中所讨论的）导致了一个系统，实际上并没有学习游戏的规则（点1），也*不能*学习游戏的规则，无论学习者多么强大。这种建模选择也阻止了*任何学习者*像国际象棋大师一样下棋（点2）。让我们看看为什么，使用论文中讨论的两种失败模式。第一种演示了无法学习游戏规则。第二种演示了无法像国际象棋大师一样学习下棋。这些都是建模选择后果的非常好的例子。
- en: Inability to learn the rules of chess.
  id: totrans-split-41
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 无法学习国际象棋的规则。
- en: '[](#inability-to-learn-the-rules-of-chess)'
  id: totrans-split-42
  prefs: []
  type: TYPE_NORMAL
  zh: '[](#inability-to-learn-the-rules-of-chess)'
- en: The paper mentions that the model could not follow the "don't repeat the same
    board position three times in a row" rule (aka "threefold repetition"). Indeed,
    when looking only at single board positions and predicting the next move, it is
    impossible to learn to not repeat past board positions three times in a row, simply
    because you do not have access to the previous board positions. There is insufficient
    signal.
  id: totrans-split-43
  prefs: []
  type: TYPE_NORMAL
  zh: 论文提到该模型无法遵循“连续三次不重复相同棋盘位置”的规则（即“三次重复规则”）。实际上，仅仅查看单个棋盘位置并预测下一步是不可能学会不连续重复过去的棋盘位置三次，因为你无法获取到之前的棋盘位置。这里面信号不足。
- en: 'In order to really learn chess "from observations", including this rule, you
    will an input representation that provides access to move history.^([5](#user-content-fn-5-1db3ccd16c3fe9d8fc359c6d4aaee58b))
    To encode the history, the "natural" option is *a sequence of board states*.^([6](#user-content-fn-6-1db3ccd16c3fe9d8fc359c6d4aaee58b))
    Now, compare learning chess by looking at a single board state and having to learn
    the next move, compared to learning chess by looking at a sequence of board positions
    and learning to predict the next move. If you do not know the games, the second
    one is significantly harder to do. At the minimum you need to learn to ignore
    all previous boards when predicting the next move (except for a few special cases,
    like the threefold-repetition rule or castling rules, where the history is extremely
    important). This is not trivial to do, and a learning model without any knowledge
    of the game may very well pick up on spurious patterns that involve previous board
    positions and which are incorrect. By restricting the input to a single board
    position, the *model designer* is using *their own knowledge of the game* to *inject
    game knowledge* to the learning process: the knowledge that previous moves are
    irrelevant in most situations. This does not seem much, but in terms of learning
    without any knowledge, it is significant. So, focusing on learning to map from
    a single board position to the best move is *a modeling choice* that makes the
    problem *much easier to learn* but also make it *impossible to learn some aspects
    of it*. It''s a tradeoff, and it injects important knowledge and "inductive bias"
    into the learned model. This is not an innocent, neutral decision. No decision
    is.'
  id: totrans-split-44
  prefs: []
  type: TYPE_NORMAL
  zh: 为了真正从观察中学习国际象棋，包括这个规则，你需要一个能够提供移动历史访问权限的输入表示方法。^([5](#user-content-fn-5-1db3ccd16c3fe9d8fc359c6d4aaee58b))
    要编码历史，使用“自然”的选项是*一系列棋盘状态*。^([6](#user-content-fn-6-1db3ccd16c3fe9d8fc359c6d4aaee58b))
    现在，比较通过查看单个棋盘状态学习国际象棋并学习下一步移动，与通过查看一系列棋盘位置学习国际象棋并学习预测下一步的差异。如果你不了解这些游戏，后者显然更难。至少在预测下一步时，你需要学会忽略所有先前的棋盘（除了一些特殊情况，例如三次重复规则或王车易位规则，其中历史记录非常重要）。这并不是一件简单的事情，一个没有任何游戏知识的学习模型很可能会捕捉到涉及先前棋盘位置且不正确的虚假模式。通过将输入限制为单个棋盘位置，*模型设计者*正在使用*他们对游戏的知识*向学习过程*注入游戏知识*：即在大多数情况下，先前的移动是无关紧要的知识。这看起来并不重要，但对于没有任何知识的学习来说，这是重要的。因此，专注于学习从单个棋盘位置到最佳移动的映射是*一个建模选择*，使问题*更容易学习*，但也*不可能学习其中的某些方面*。这是一个权衡，并且向学习模型注入了重要的知识和“归纳偏差”。这不是一个无辜的、中立的决定。没有决定是。
- en: '**Can it be learned from observations at all?** Another interesting issue that
    arise here is that even if you had access to the entire game history (so no missing
    signal), and a perfect learner (that don''t get distracted by spurious patterns),
    it would be very challenging to learn the threefold-repetition rule only from
    observing human-human games, and even harder to learn when restricting yourself
    to looking at games by strong human players: the players know this rule, know
    that such repetitions will lead to a draw, and hence avoid threefold repetitions
    in their games. There will be very few game examples where this behavior is manifested,
    and the few examples that do exist, will be in situations where both players realize
    the game is "stuck", and wish to draw. It will be very hard, if not impossible,
    to correctly infer the general threefold rule in this setup, and learn that you
    should not repeat the same board position three times in a row in a strong board
    position. (This means capable players who do know the rule, and this model''s
    blindspot, can use it to manipulate the model into drawing by mistake when it
    is in a very strong leading position).'
  id: totrans-split-45
  prefs: []
  type: TYPE_NORMAL
  zh: '**观察是否可以学习？** 另一个有趣的问题是，即使你可以访问整个比赛历史（所以没有信号缺失），并且有一个完美的学习者（不会被虚假模式分散注意力），从观察人类对人类比赛中仅仅通过观察学习三次重复规则是非常具有挑战性的，甚至在限制自己只看强大的人类玩家的比赛时更加困难：玩家们知道这个规则，知道这样的重复会导致平局，因此他们在比赛中避免三次重复。很少有比赛示例会表现出这种行为，而少数存在的例子，是在两位玩家意识到比赛“僵局”并希望平局的情况下。在这种情况下，正确推断出通用的三次重复规则将非常困难，如果不是不可能的话，也学不会在强势局面下不连续重复相同的棋盘位置三次。（这意味着知道规则的有能力的玩家和这个模型的盲点，可以在模型处于非常强势领先位置时误导模型陷入平局）。'
- en: I find this to be a great demonstration of a semantic rule that cannot be learned
    from observations, in this case because the semantics affect the observed states.
    It is worth noting that large language models also learn from observations alone,
    and reflect on possible semantics *they* cannot learn, for various reasons.
  id: totrans-split-46
  prefs: []
  type: TYPE_NORMAL
  zh: 我发现这是一个很好的展示语义规则的例子，这种规则在这种情况下不能从观察中学习，因为语义影响了观察到的状态。值得注意的是，大型语言模型也仅仅通过观察学习，并反映可能由于各种原因无法学习的语义。
- en: Inability to play at real grandmaster level.
  id: totrans-split-47
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 无法在真正的大师水平上进行比赛。
- en: '[](#inability-to-play-at-real-grandmaster-level)'
  id: totrans-split-48
  prefs: []
  type: TYPE_NORMAL
  zh: '[](#inability-to-play-at-real-grandmaster-level)'
- en: 'Grandmaster level human players know the game well. They won''t be caught exhibiting
    behaviors of very weak players. In a sense we already saw such a blind-spot: not
    knowing the threefold repetition rule, the model can be manipulated into a draw
    when it''s in fact leading. However, the next example is even more problematic,
    and stems directly from modeling choices:'
  id: totrans-split-49
  prefs: []
  type: TYPE_NORMAL
  zh: 大师级别的人类玩家对游戏了解很深。他们不会展示非常弱的玩家行为。从某种意义上说，我们已经看到这样的盲点：不知道三次重复规则，模型可能被操纵成平局，尽管它实际上处于领先位置。然而，下一个例子更加棘手，直接源于建模选择：
- en: 'In cases where the position of one player is much stronger than the others,
    the "winning probability" can be 100% for several different moves: each of them
    will lead to winning the game down the line. However, some moves will achieve
    victory using fewer future moves than others. In our case, the model cannot choose
    between these moves: it only knows to maximize winning probability of a single
    move, it does not even "know" that the move will be followed by future moves,
    doesn''t know how winning will be achieved, and doesn''t even known what it means
    to win.'
  id: totrans-split-50
  prefs: []
  type: TYPE_NORMAL
  zh: 在一个玩家的位置比其他玩家强得多的情况下，“获胜概率”可以对几个不同的移动都达到100%：每个移动都会导致最终赢得比赛。然而，有些移动将使用比其他更少的未来移动来实现胜利。在我们的案例中，模型无法在这些移动之间做出选择：它只知道最大化单一移动的获胜概率，甚至不“知道”这个移动将跟随未来的移动，不知道如何赢得比赛，甚至不知道什么是赢。
- en: Hence, the model may choose a poor move that takes more future moves to lead
    to winning. After taking such a move, it may again select a poor move that will
    take a long time to achieve victory. Indeed, it has no notion of "a plan" nor
    a notion of making a move that will get it closer to victory. In some extreme
    cases, the model may get stuck forever in situation where winning is guaranteed,
    yet the model never reaches it. Consider, for example, the model having two rooks,
    while the opponent has a king. This is a textbook example of check-mating, so
    every move will be a guaranteed 100% probability of winning. However, in order
    to actually win, one has to make a specific series of moves with their rooks,
    not just move them aimlessly around. But the board is large enough to allow rooks
    moving aimlessly around. This will be an absurd behavior, associated with very
    weak chess players, and which will never ever happen at grand-master level. Yet,
    the model, as trained in this paper's setup, has no way of avoiding it, and has
    a real chance of exhibiting it.
  id: totrans-split-51
  prefs: []
  type: TYPE_NORMAL
- en: 'The *modeling decision* crippled the learner and *prevented it* from exhibiting
    grandmaster level play. This modeling decision in question is learning to associate
    moves (or boards) with their "winning probability", without factoring in the estimated
    number of steps that will lead to the win. The paper proposes and ad-hoc solution,
    that actually contradicts it main claim: the resulting chess program does use
    search.^([7](#user-content-fn-7-1db3ccd16c3fe9d8fc359c6d4aaee58b))'
  id: totrans-split-52
  prefs: []
  type: TYPE_NORMAL
- en: Modeling choices matter.
  id: totrans-split-53
  prefs: []
  type: TYPE_NORMAL
- en: The paper's setup is not learning from observations.
  id: totrans-split-54
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](#the-papers-setup-is-not-learning-from-observations)'
  id: totrans-split-55
  prefs: []
  type: TYPE_NORMAL
- en: 'So, we saw how modeling choices prevented the model from learning the rules
    of the games (and that some rules will be hard to learn from observing games under
    any setup). This handles points (1) and (2) above, and to some extent also point
    (4): there certainly are game-specific information embedded in the modeling choices,
    and other choices could make learning much harder.'
  id: totrans-split-56
  prefs: []
  type: TYPE_NORMAL
- en: 'Now let''s tackle point 3: the paper''s setup **is not learning from observing
    games**. This means, in my view, that any attempts to infer from this paper anything
    about learning via language models is futile. I will now describe some of the
    reasons making learning from observing games very hard, and what the paper actually
    does instead.'
  id: totrans-split-57
  prefs: []
  type: TYPE_NORMAL
- en: What does it mean to learn from observations?
  id: totrans-split-58
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[](#what-does-it-mean-to-learn-from-observations)'
  id: totrans-split-59
  prefs: []
  type: TYPE_NORMAL
- en: Consider what it means to learn chess from observations. You see a chess game,
    and at the end, you get told who won. You can watch as many games as you want,
    but you can only watch. What can you learn from that about the rules of the game?
    What can you learn about the game's semantics and strategy? What does it take
    to learn to play well?
  id: totrans-split-60
  prefs: []
  type: TYPE_NORMAL
- en: Learning that players alternate in their turns will likely be an easy pattern
    to pick up.^([8](#user-content-fn-8-1db3ccd16c3fe9d8fc359c6d4aaee58b)) Learning
    the legal moves for each player is likely also going to be easy (with the possible
    exception of castling, which may be harder). But how about learning the semantics
    of the game? or, let's pick an easier target, learning to make good moves, which
    will maximize your chances of winning?
  id: totrans-split-61
  prefs: []
  type: TYPE_NORMAL
  zh: 学会玩家轮流行动很可能是一个容易捕捉的模式。^([8](#user-content-fn-8-1db3ccd16c3fe9d8fc359c6d4aaee58b))
    学习每个玩家的合法走法也很可能会比较容易（除了可能的王车易位可能会更难）。但是，如何学习游戏的语义呢？或者，让我们选择一个更容易的目标，学会做出好的走法，这将最大化你赢得比赛的机会？
- en: 'To learn to pick up good moves, you could, for example, focus on only the moves
    of the winning player in each game. Now, at each board position you note what
    move this player made. And you try to find patterns that associate the board state
    and the move. There is a big problem, though: this will not necessarily (and will
    likely not) result in learning to play well and make good moves: not all the moves
    a winning player made in their game are good moves. Some of them might actually
    be mistakes, that the player managed to recover from. Other might just be "meh"
    moves. Learning to distill the actual good moves in a sequence that eventually
    lead to victory, simply by observing the sequence and its outcome, is very hard.
    In Reinforcement Learning ("RL") terminology, this translates to a situation of
    "sparse reward", and identifying the actual good moves that led to the sparse
    reward is called the "credit assignment problem". Credit assignment is perhaps
    *the* biggest challenge in RL, and is far from being solved. And whatever solutions
    exist for it in RL, assume you can interact, propose moves and observe their outcomes.
    Situations when you are only allowed to observe and now allowed to interact are
    much, much harder.'
  id: totrans-split-62
  prefs: []
  type: TYPE_NORMAL
  zh: 为了学会选择好的走法，你可以例如，专注于每场比赛中获胜玩家的走法。现在，在每个棋盘位置上，你记录这位玩家做了什么走法。然后，你试图找到与棋盘状态和走法相关联的模式。然而，这里有一个大问题：这不一定（而且很可能不会）导致学会打得好和做出好的走法：并不是获胜玩家在他们的比赛中做的所有走法都是好走法。其中一些可能实际上是错误，但玩家成功从中恢复了。其他可能只是“一般般”的走法。通过观察序列及其结果来提取最终导致胜利的实际好走法是非常困难的。在强化学习（"RL"）术语中，这被称为“稀疏奖励”的情况，而识别导致稀疏奖励的实际好走法则被称为“信用分配问题”。信用分配可能是RL中*最*大的挑战之一，并且远未解决。而在RL中可能存在的任何解决方案，都假定你可以互动，提出走法并观察其结果。而仅允许观察而不允许互动的情况则要难得多，难得多得多。
- en: Ignoring credit-assignment and just learning to predict the move the winning
    player made in each board position will likely lead to very poor, and confused,
    strategy, especially when you do not have any knowledge of the players in the
    game you observe. (Maybe the player you observed is a weak one, and only managed
    to win because they played against other weak player? maybe many games are from
    such weak players? In this case, you will learn to associate board-positions with
    very non-impressive moves. Likewise, maybe most moves in a game are "meh" in the
    sense that they can lead to either winning or defeat depending on the future,
    but are very indecisive, and the patterns you will learn will be dominated by
    such meh moves rather than by strong winning moves? etc).
  id: totrans-split-63
  prefs: []
  type: TYPE_NORMAL
  zh: 忽略信用分配，只学习预测在每个棋盘位置上获胜玩家的走法，很可能会导致非常差和混乱的策略，尤其是当你对观察的游戏中的玩家没有任何了解时。（也许你观察到的玩家是一个弱者，只能因为他们对阵其他弱者而获胜？也许很多游戏都来自这样的弱者？在这种情况下，你将学会将棋盘位置与非常不起眼的走法关联起来。同样，也许游戏中大多数走法在未来可能导致胜利或失败，但是非常犹豫不决，而你学到的模式将被这种犹豫不决的走法而不是强势的获胜走法所主导？等等）。
- en: So, learning from observation is far from trivial. The authors of course realized
    that, and did not even attempt to learn from observations. They did something
    very different. Did you spot what it was?
  id: totrans-split-64
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，仅从观察中学习远非简单。当然，作者们意识到了这一点，并且甚至没有尝试从观察中学习。他们做了非常不同的事情。你注意到了吗？
- en: What the paper actually did?
  id: totrans-split-65
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 论文实际做了什么？
- en: '[](#what-the-paper-actually-did)'
  id: totrans-split-66
  prefs: []
  type: TYPE_NORMAL
  zh: '[](#what-the-paper-actually-did)'
- en: The setup in the paper is not learning from observations. It didn't look at
    game positions and tried to learn and predict the next move of the game at certain
    positions. Rather, it looked at a game position, then used a strong chess-engine
    to expand a tree of possible game continuations from this position spanning hundreds
    of thousands if not millions of moves into the future. The game engine then used
    its internal knowledge of chess to assess the percentage of winning board positions
    within the possible continuations, which is the "probability of win" for a position.
    Then, the learner tried to learn *this* number. This is not learning from observations,
    it is learning from a super-human expert.
  id: totrans-split-67
  prefs: []
  type: TYPE_NORMAL
  zh: 本文中的设置并非通过观察学习。它没有查看游戏位置并试图学习和预测在某些位置下游戏的下一步。相反，它查看游戏位置，然后使用强大的国际象棋引擎，从这个位置开始扩展可能的游戏延续树，涵盖未来数十万甚至数百万个移动。游戏引擎然后利用其对国际象棋的内部知识评估可能延续中获胜局面的百分比，这就是该位置的“获胜概率”。然后，学习者试图学习*这个*数字。这不是通过观察学习，而是通过超人类专家学习。
- en: Knowing the "win percentage" or "win probability" of a move in a position is
    very hard. Grandmaster-level players can estimate this number for some board positions
    based on their past experience (and based on their history of interactions with
    chess engines, including querying them for this precise number for many different
    positions, and analyzing both the number and the some select winning and losing
    paths). But this is virtually impossible to give a good estimate of this "win
    probability" of a move without either being exposed to many correct estimates
    of other moves, knowing the rules of the game and being able to "play them out",
    or observing someone else doing such playing-out. In any case, this is well beyond
    what is available by just observing played games.
  id: totrans-split-68
  prefs: []
  type: TYPE_NORMAL
  zh: 知道一个位置中的“获胜百分比”或“获胜概率”是非常困难的。大师级玩家可以根据他们的过去经验估计某些棋盘位置的这个数字（以及他们与国际象棋引擎的互动历史，包括查询它们用于许多不同位置的精确数字，以及分析这些数字和某些选择的赢得和失去的路径）。但如果没有被暴露于其他移动的许多正确估计，不知道游戏规则并且不能“玩出来”，或者观察其他人这样做，这几乎不可能给出一个关于移动的“获胜概率”的良好估计。在任何情况下，这远远超出了仅仅观察已玩游戏可得的内容。
- en: So, the learner certainly did not learn from observation, it learned from a
    significantly reacher signal, one that cannot be obtained without very strong
    knowledge of the game (the authors call it "distilling the knowledge of stockfish
    16". This is an apt name).
  id: totrans-split-69
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，学习者确实没有通过观察学习，而是通过一个显著更丰富的信号进行学习，这是无法在没有对游戏非常强的知识的情况下获得的（作者称其为“蒸馏Stockfish
    16的知识”。这是一个合适的名称）。
- en: Model Learning vs Human Learning.
  id: totrans-split-70
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 模型学习 vs 人类学习。
- en: '[](#model-learning-vs-human-learning)'
  id: totrans-split-71
  prefs: []
  type: TYPE_NORMAL
  zh: '[](#model-learning-vs-human-learning)'
- en: Above, we said that a human learner is much more efficient than the learning
    method of this paper, because observing and analyzing 10M games in a rate of 1
    game per minute 16 hours a day will require 28.5 years, and humans are much more
    efficient than that.
  id: totrans-split-72
  prefs: []
  type: TYPE_NORMAL
  zh: 上文提到，人类学习者比本文的学习方法要高效得多，因为以每分钟观察和分析10M场比赛的速度，每天16小时，将需要28.5年，而人类的效率要远远高于此。
- en: But in fact, the story is even more extreme. For every game, the machine-learner
    looked not only at the game, but also at the win-probability estimates of each
    of the 15B board-move pairs in these games, and tried to learn and replicate these
    win-probabilities. The win-probability estimates are not only time-consuming for
    human-learners to produce, they are also impossible to derive without very strong
    knowledge of the rules of the game (or access to a chess engine that encodes these
    rules). Human learners clearly learn **very** differently than that, and much
    more efficiently, at least in the middle parts of their learning. (Once beyond
    some level, they may as well be learning win probabilities. But this will build
    on a very different foundation than a system that was trained only on such probabilities,
    and will also look at significantly fewer board positions.)
  id: totrans-split-73
  prefs: []
  type: TYPE_NORMAL
  zh: 但事实上，情况更为极端。对于每场比赛，机器学习者不仅查看了游戏，还查看了这些游戏中15B个棋盘移动对的获胜概率估计，并试图学习和复制这些获胜概率。对于人类学习者来说，产生这些获胜概率估计不仅耗时，而且几乎不可能，除非具备游戏规则的强大知识（或者可以访问编码这些规则的国际象棋引擎）。人类学习者显然学习**非常**不同，并且在学习的中间部分要高效得多（一旦超过某个水平，他们可能也会学习获胜概率。但这将建立在一个与仅在这些概率上训练的系统完全不同的基础上，并且还将查看显著较少的棋盘位置）。
- en: We saw that the way in which the problem is encoded is never natural or neutral,
    it is always a modeling choice among various alternatives, and these choices have
    benefits and consequences.
  id: totrans-split-74
  prefs: []
  type: TYPE_NORMAL
  zh: 我们看到，问题的编码方式从未是自然的或中立的，它总是在各种选择中进行建模，而这些选择都有其利弊。
- en: 'In this paper, the prominent modeling choices were:'
  id: totrans-split-75
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，显著的建模选择包括：
- en: representing the input as a single board position, rather than a sequence of
    positions or moves.
  id: totrans-split-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将输入表示为单个棋盘位置，而不是一系列位置或走法。
- en: representing the output as a winning probability.
  id: totrans-split-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将输出表示为获胜概率。
- en: While not explicitly discussed in the paper, these two choices encode game-specific
    knowledge, and make the learning problem significantly easier. However, the first
    choice also prohibited the model from properly learning some rules of the game,
    while the second choice prevented the model for ever achieving real grandmaster
    level play by prohibiting it from concisely and efficiently winning in some very
    easy-to-win situations.
  id: totrans-split-78
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然论文中没有明确讨论，但这两种选择编码了游戏特定的知识，并显著简化了学习问题。然而，第一种选择也限制了模型正确学习游戏规则的能力，而第二种选择则通过阻止在某些非常容易获胜的情况下简洁高效地取胜，从而阻止了模型达到真正的国际象棋大师水平。
- en: Moreover, the second choice introduces a huge amount of game-knowledge into
    the training process, and resulted in a process which is very different from learning-by-observing.
    While there is no search at playing time, there is *a lot* of search going on
    in training, and *a lot* of fine-grained chess knowledge as well. (While the authors
    don't deny it, and even say it up-front, this is not the focus, and easy to miss
    on a first read by a casual reader. Or at least, I missed it.)
  id: totrans-split-79
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，第二种选择在训练过程中引入了大量的游戏知识，并导致了一种与通过观察学习截然不同的过程。虽然在游戏时没有搜索过程，但在训练中进行了*大量*的搜索，以及*大量*的细粒度的棋类知识。
    （尽管作者们没有否认它，甚至在开篇就提到了这一点，但这并不是重点，初次阅读时易被不经意的读者忽略。或者至少，我忽略了。）
- en: 'While the end result is indeed impressive, it is also much less impressive
    than many readers portray it to be: the model learned to pattern match from a
    given position to a strong move, in a generalizable way. But it did not learn
    from observations, did not learn the actual rules of the game, and likely learned
    in a very different way than any human would. In many ways, it is telling much
    more about properties of the game of chess, than it does about the learning abilities
    of ML-models. We can certainly not support claims of potential strong abilities
    of langage models based on this work.'
  id: totrans-split-80
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管最终结果确实令人印象深刻，但它也比许多读者所描绘的要逊色得多：该模型学会了从给定位置到强力走法的模式匹配，以一种可推广的方式。但它并未从观察中学习，也没有学习到实际的游戏规则，而且其学习方式很可能与任何人类完全不同。在很多方面，这更多地反映了国际象棋游戏的特性，而不是机器学习模型的学习能力。我们无法仅凭这项工作支持语言模型潜在强大能力的主张。
- en: 'But if you take one message from this piece, I urge that that is not "this
    result is meh" or the specifics of learning to play chess without search, but
    rather the more general one: "everything is a design choice. design choices have
    big implications, and are often glossed over in papers. looking for important
    but hidden design choices and implications is a critical paper reading skill."'
  id: totrans-split-81
  prefs: []
  type: TYPE_NORMAL
  zh: 但如果你从这篇文章中得到一个信息，我要强调的不是“这个结果一般般”或者学习无搜索下下棋的具体内容，而是更一般性的观点：“一切都是设计选择。设计选择具有重大影响，并且经常在论文中被忽视。寻找重要但隐藏的设计选择和影响是一项关键的论文阅读技能。”
- en: '* * *'
  id: totrans-split-82
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
