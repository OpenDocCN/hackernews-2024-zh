- en: <!--yml
  id: totrans-split-0
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  id: totrans-split-1
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-05-27 14:38:08'
  id: totrans-split-2
  prefs: []
  type: TYPE_NORMAL
- en: -->
  id: totrans-split-3
  prefs: []
  type: TYPE_NORMAL
- en: Sensors | Free Full-Text | Text-Guided Image Editing Based on Post Score for
    Gaining Attention on Social Media
  id: totrans-split-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://www.mdpi.com/1424-8220/24/3/921](https://www.mdpi.com/1424-8220/24/3/921)
  id: totrans-split-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 1\. Introduction
  id: totrans-split-6
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: With significant advances in Internet technologies, social media has become
    an important part of our daily lives. Social media provides a wide range of interactions
    such as communicating with other users, seeking and providing news, buying and
    selling products, and advertising. The number of users using social media is expected
    to increase further in the future [
  id: totrans-split-7
  prefs: []
  type: TYPE_NORMAL
- en: '[1](#B1-sensors-24-00921)'
  id: totrans-split-8
  prefs: []
  type: TYPE_NORMAL
- en: '], and interactions within social media have the potential to have a significant
    impact on society. Instagram, which has grown particularly rapidly in recent years,
    has more than 1.3 billion monthly active users in 2023 ['
  id: totrans-split-9
  prefs: []
  type: TYPE_NORMAL
- en: '[2](#B2-sensors-24-00921)'
  id: totrans-split-10
  prefs: []
  type: TYPE_NORMAL
- en: ']. Instagram is a platform specialized in posting images and videos and allows
    users to “like” and comment on posts. The extraordinary success of Instagram confirms
    a recent report by Pew Research Center ('
  id: totrans-split-11
  prefs: []
  type: TYPE_NORMAL
- en: '[https://www.pewresearch.org/](https://www.pewresearch.org/)'
  id: totrans-split-12
  prefs: []
  type: TYPE_NORMAL
- en: (accessed on 20 November 2023)) that photos and videos have become the primary
    social currency online [
  id: totrans-split-13
  prefs: []
  type: TYPE_NORMAL
- en: '[3](#B3-sensors-24-00921)'
  id: totrans-split-14
  prefs: []
  type: TYPE_NORMAL
- en: ']. In other words, images posted on Instagram have significant worth. To enhance
    the worth, there are more opportunities to edit images, and research on automatic
    image editing is becoming important ['
  id: totrans-split-15
  prefs: []
  type: TYPE_NORMAL
- en: '[4](#B4-sensors-24-00921)'
  id: totrans-split-16
  prefs: []
  type: TYPE_NORMAL
- en: ','
  id: totrans-split-17
  prefs: []
  type: TYPE_NORMAL
- en: '[5](#B5-sensors-24-00921)'
  id: totrans-split-18
  prefs: []
  type: TYPE_NORMAL
- en: ','
  id: totrans-split-19
  prefs: []
  type: TYPE_NORMAL
- en: '[6](#B6-sensors-24-00921)'
  id: totrans-split-20
  prefs: []
  type: TYPE_NORMAL
- en: '].'
  id: totrans-split-21
  prefs: []
  type: TYPE_NORMAL
- en: Representative automatic image editing approaches include image colorization
    [
  id: totrans-split-22
  prefs: []
  type: TYPE_NORMAL
- en: '[7](#B7-sensors-24-00921)'
  id: totrans-split-23
  prefs: []
  type: TYPE_NORMAL
- en: '], image inpainting ['
  id: totrans-split-24
  prefs: []
  type: TYPE_NORMAL
- en: '[8](#B8-sensors-24-00921)'
  id: totrans-split-25
  prefs: []
  type: TYPE_NORMAL
- en: '], and style transfer ['
  id: totrans-split-26
  prefs: []
  type: TYPE_NORMAL
- en: '[9](#B9-sensors-24-00921)'
  id: totrans-split-27
  prefs: []
  type: TYPE_NORMAL
- en: '], which are expected to eliminate the tedious human work involved in image
    editing. These approaches are usually constructed for one-pattern transformations.
    For example, the approach of style transfer automatically transforms a style in
    an input image to a specific artist’s style, and there are limitations in transforming
    it to different artist’s styles using one model and in reflecting a user’s request
    into the edited image. To address these problems, several user-friendly approaches,
    named text-guided image editing, have been proposed ['
  id: totrans-split-28
  prefs: []
  type: TYPE_NORMAL
- en: '[10](#B10-sensors-24-00921)'
  id: totrans-split-29
  prefs: []
  type: TYPE_NORMAL
- en: ','
  id: totrans-split-30
  prefs: []
  type: TYPE_NORMAL
- en: '[11](#B11-sensors-24-00921)'
  id: totrans-split-31
  prefs: []
  type: TYPE_NORMAL
- en: ','
  id: totrans-split-32
  prefs: []
  type: TYPE_NORMAL
- en: '[12](#B12-sensors-24-00921)'
  id: totrans-split-33
  prefs: []
  type: TYPE_NORMAL
- en: ','
  id: totrans-split-34
  prefs: []
  type: TYPE_NORMAL
- en: '[13](#B13-sensors-24-00921)'
  id: totrans-split-35
  prefs: []
  type: TYPE_NORMAL
- en: ','
  id: totrans-split-36
  prefs: []
  type: TYPE_NORMAL
- en: '[14](#B14-sensors-24-00921)'
  id: totrans-split-37
  prefs: []
  type: TYPE_NORMAL
- en: ','
  id: totrans-split-38
  prefs: []
  type: TYPE_NORMAL
- en: '[15](#B15-sensors-24-00921)'
  id: totrans-split-39
  prefs: []
  type: TYPE_NORMAL
- en: ']. Given an input image and a text prompt describing the contents of image
    editing from a user, text-guided image editing aims to edit the text-related region
    in accordance with the text prompt and maintain text-unrelated regions of the
    input image. With the great success of generative models such as StyleGAN ['
  id: totrans-split-40
  prefs: []
  type: TYPE_NORMAL
- en: '[16](#B16-sensors-24-00921)'
  id: totrans-split-41
  prefs: []
  type: TYPE_NORMAL
- en: '] and the diffusion model ['
  id: totrans-split-42
  prefs: []
  type: TYPE_NORMAL
- en: '[17](#B17-sensors-24-00921)'
  id: totrans-split-43
  prefs: []
  type: TYPE_NORMAL
- en: '] in recent years, text-guided image editing has become a hot topic.'
  id: totrans-split-44
  prefs: []
  type: TYPE_NORMAL
- en: While text-guided image editing based on the diffusion model achieves particularly
    high performance, the user must expend effort devising how to give the text prompt
    to obtain the desired result. Specifically, as shown in
  id: totrans-split-45
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 1](#sensors-24-00921-f001)'
  id: totrans-split-46
  prefs: []
  type: TYPE_NORMAL
- en: ', the results of text-guided image editing differ depending on the way the
    text prompt is represented, even if it has the same meaning. It is up to the user
    to decide which result best matches the intended use of the edited image. When
    the intended use of the edited image is to post to social media, it is difficult
    for the user without knowledge of social media marketing to decide which result
    will gain attention from a greater audience. As shown in the upper case of'
  id: totrans-split-47
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 2](#sensors-24-00921-f002)'
  id: totrans-split-48
  prefs: []
  type: TYPE_NORMAL
- en: ', it is unclear how much attention the result of the previous text-guided image
    editing method would gain when it is posted on social media. Moreover, as shown
    in the lower case of'
  id: totrans-split-49
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 2](#sensors-24-00921-f002)'
  id: totrans-split-50
  prefs: []
  type: TYPE_NORMAL
- en: ', by performing text-guided image editing while considering the attention on
    social media in advance, the edited image can be displayed to a greater audience.
    From the above, it is necessary to construct a text-guided image editing method
    that can generate an edited image to gain attention on social media.'
  id: totrans-split-51
  prefs: []
  type: TYPE_NORMAL
- en: Here, there is much research on post and influencer classifications for social
    media marketing [
  id: totrans-split-52
  prefs: []
  type: TYPE_NORMAL
- en: '[18](#B18-sensors-24-00921)'
  id: totrans-split-53
  prefs: []
  type: TYPE_NORMAL
- en: ','
  id: totrans-split-54
  prefs: []
  type: TYPE_NORMAL
- en: '[19](#B19-sensors-24-00921)'
  id: totrans-split-55
  prefs: []
  type: TYPE_NORMAL
- en: ','
  id: totrans-split-56
  prefs: []
  type: TYPE_NORMAL
- en: '[20](#B20-sensors-24-00921)'
  id: totrans-split-57
  prefs: []
  type: TYPE_NORMAL
- en: ','
  id: totrans-split-58
  prefs: []
  type: TYPE_NORMAL
- en: '[21](#B21-sensors-24-00921)'
  id: totrans-split-59
  prefs: []
  type: TYPE_NORMAL
- en: ']. In post classification, several works ['
  id: totrans-split-60
  prefs: []
  type: TYPE_NORMAL
- en: '[18](#B18-sensors-24-00921)'
  id: totrans-split-61
  prefs: []
  type: TYPE_NORMAL
- en: ','
  id: totrans-split-62
  prefs: []
  type: TYPE_NORMAL
- en: '[19](#B19-sensors-24-00921)'
  id: totrans-split-63
  prefs: []
  type: TYPE_NORMAL
- en: '] focus on the categories (e.g., fashion, travel, and food) and the virality
    (a situation in which a post gets more interactions than others on Twitter (currently
    X) and thus gains the attention from a relatively large number of users ['
  id: totrans-split-64
  prefs: []
  type: TYPE_NORMAL
- en: '[19](#B19-sensors-24-00921)'
  id: totrans-split-65
  prefs: []
  type: TYPE_NORMAL
- en: ']) of posts on social media. In particular, the research classifying the virality
    of posts is useful in selecting edited images for posting on social media from
    the multiple results of text-guided image editing. However, since the conventional
    method ['
  id: totrans-split-66
  prefs: []
  type: TYPE_NORMAL
- en: '[19](#B19-sensors-24-00921)'
  id: totrans-split-67
  prefs: []
  type: TYPE_NORMAL
- en: '] predicts virality only from posted text, it is impossible to predict the
    attention of edited images on social media in advance. Therefore, it is necessary
    to construct a new model that predicts in advance the attention from the audience
    on social media based on the posted image in addition to the posted text.'
  id: totrans-split-68
  prefs: []
  type: TYPE_NORMAL
- en: In light of the above, we propose novel text-guided image editing considering
    the response in social media in this paper. The goal of the proposed method is
    to provide the user with images that are edited in accordance with the text prompt
    and will gain attention from the greater audience on social media. Here, the degree
    of attention from the audience is defined as the engagement rate calculated from
    the number of likes and comments on a post. The key idea of the proposed method
    is to newly introduce a model to predict post scores representing engagement rates
    on social media from posted images and text, thereby generating edited images
    that gain attention from the greater audience. To construct the novel model, based
    on previous works [
  id: totrans-split-69
  prefs: []
  type: TYPE_NORMAL
- en: '[22](#B22-sensors-24-00921)'
  id: totrans-split-70
  prefs: []
  type: TYPE_NORMAL
- en: ','
  id: totrans-split-71
  prefs: []
  type: TYPE_NORMAL
- en: '[23](#B23-sensors-24-00921)'
  id: totrans-split-72
  prefs: []
  type: TYPE_NORMAL
- en: '] that analyzed the relationship between the content or aesthetic of posted
    images and the engagement rate from the perspective of computer vision, the proposed
    method focuses on aesthetics and categories of the posted images in addition to
    features of posted images and texts and calculates post scores. Then, the proposed
    method obtains several other expressions similar to the text prompt given by the
    user, based on a large language model that has attracted much attention in recent
    years. We apply a pre-trained text-guided image editing method and generate edited
    images from each of these several text prompts. Using multiple text prompts increases
    the possibility of obtaining edited images that gain attention from a greater
    audience on social media while performing image editing desired by the user. Among
    these, by leveraging the novel model that predicts post scores representing engagement
    rates, we finally select the edited images that will gain more attention from
    the audience on social media. To the best of our knowledge, this is the first
    text-guided image editing method considering the response from the audience on
    social media. The proposed method can provide users with edited images that have
    the potential to obtain the highest engagement rate and is expected to reduce
    the burden of creating posts for users without knowledge of social media marketing.'
  id: totrans-split-73
  prefs: []
  type: TYPE_NORMAL
- en: The rest of this paper is organized as follows. We introduce related works on
    social media marketing, image–text matching, and text-guided image editing in
  id: totrans-split-74
  prefs: []
  type: TYPE_NORMAL
- en: '[Section 2](#sec2-sensors-24-00921)'
  id: totrans-split-75
  prefs: []
  type: TYPE_NORMAL
- en: . In
  id: totrans-split-76
  prefs: []
  type: TYPE_NORMAL
- en: '[Section 3](#sec3-sensors-24-00921)'
  id: totrans-split-77
  prefs: []
  type: TYPE_NORMAL
- en: ', we then explain the proposed method that performs text-guided image editing
    considering the response in social media. In'
  id: totrans-split-78
  prefs: []
  type: TYPE_NORMAL
- en: '[Section 4](#sec4-sensors-24-00921)'
  id: totrans-split-79
  prefs: []
  type: TYPE_NORMAL
- en: ', as a preliminary validation, we verify the accuracy of the proposed model
    to predict post scores.'
  id: totrans-split-80
  prefs: []
  type: TYPE_NORMAL
- en: '[Section 5](#sec5-sensors-24-00921)'
  id: totrans-split-81
  prefs: []
  type: TYPE_NORMAL
- en: demonstrates extensive experimental results for verifying the effectiveness
    of the proposed method. Finally, we conclude our work in
  id: totrans-split-82
  prefs: []
  type: TYPE_NORMAL
- en: '[Section 6](#sec6-sensors-24-00921)'
  id: totrans-split-83
  prefs: []
  type: TYPE_NORMAL
- en: .
  id: totrans-split-84
  prefs: []
  type: TYPE_NORMAL
- en: 3\. Proposed Text-Guided Image Editing Based on Post Score for Gaining Attention
    on Social Media
  id: totrans-split-85
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We show the architecture of the proposed method in
  id: totrans-split-86
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 3](#sensors-24-00921-f003)'
  id: totrans-split-87
  prefs: []
  type: TYPE_NORMAL
- en: . In the proposed method, we first obtain four paraphrase expressions for the
    text prompt
  id: totrans-split-88
  prefs: []
  type: TYPE_NORMAL
- en: P
  id: totrans-split-89
  prefs: []
  type: TYPE_NORMAL
- en: . Then, the proposed method applies the pre-trained text-guided image editing
    method (i.e., Instruct-Pix2Pix) and generates four edited images. Finally, we
    leverage the proposed model to predict the post score and select the edited image
  id: totrans-split-90
  prefs: []
  type: TYPE_NORMAL
- en: <math display="inline"><semantics><msup><mi>I</mi> <mo>′</mo></msup></semantics></math>
  id: totrans-split-91
  prefs: []
  type: TYPE_NORMAL
- en: with the highest post score from all results of the text-guided image editing
    method.
  id: totrans-split-92
  prefs: []
  type: TYPE_NORMAL
- en: In the proposed model, we calculate the post score representing the engagement
    rate on social media from the posted image and text, as shown in
  id: totrans-split-93
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 4](#sensors-24-00921-f004)'
  id: totrans-split-94
  prefs: []
  type: TYPE_NORMAL
- en: . Specifically, based on previous works [
  id: totrans-split-95
  prefs: []
  type: TYPE_NORMAL
- en: '[22](#B22-sensors-24-00921)'
  id: totrans-split-96
  prefs: []
  type: TYPE_NORMAL
- en: ','
  id: totrans-split-97
  prefs: []
  type: TYPE_NORMAL
- en: '[23](#B23-sensors-24-00921)'
  id: totrans-split-98
  prefs: []
  type: TYPE_NORMAL
- en: '] that analyzed the relationship between the content or aesthetic of posted
    images and the engagement rate, we focus on aesthetics and categories of the posted
    images in addition to features of posted images and texts and calculate post scores.
    We describe the details of the proposed model to predict post scores in'
  id: totrans-split-99
  prefs: []
  type: TYPE_NORMAL
- en: '[Section 3.1](#sec3dot1-sensors-24-00921)'
  id: totrans-split-100
  prefs: []
  type: TYPE_NORMAL
- en: . Then, the overall flow of the proposed method applying that model is described
    in
  id: totrans-split-101
  prefs: []
  type: TYPE_NORMAL
- en: '[Section 3.2](#sec3dot2-sensors-24-00921)'
  id: totrans-split-102
  prefs: []
  type: TYPE_NORMAL
- en: .
  id: totrans-split-103
  prefs: []
  type: TYPE_NORMAL
- en: 3.1\. Calculation of Post Score Representing Engagement Rate on Social Media
  id: totrans-split-104
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: This section explains the proposed model that takes an image
  id: totrans-split-105
  prefs: []
  type: TYPE_NORMAL
- en: I
  id: totrans-split-106
  prefs: []
  type: TYPE_NORMAL
- en: and a text
  id: totrans-split-107
  prefs: []
  type: TYPE_NORMAL
- en: T
  id: totrans-split-108
  prefs: []
  type: TYPE_NORMAL
- en: as inputs and predicts a post score representing the engagement rate when they
    are posted on social media. As shown in
  id: totrans-split-109
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 4](#sensors-24-00921-f004)'
  id: totrans-split-110
  prefs: []
  type: TYPE_NORMAL
- en: ', to predict the post score, the proposed model uses three features (1) a multimodal
    feature extracted from the image'
  id: totrans-split-111
  prefs: []
  type: TYPE_NORMAL
- en: I
  id: totrans-split-112
  prefs: []
  type: TYPE_NORMAL
- en: and text
  id: totrans-split-113
  prefs: []
  type: TYPE_NORMAL
- en: T
  id: totrans-split-114
  prefs: []
  type: TYPE_NORMAL
- en: ', (2) an aesthetics feature of the image'
  id: totrans-split-115
  prefs: []
  type: TYPE_NORMAL
- en: I
  id: totrans-split-116
  prefs: []
  type: TYPE_NORMAL
- en: ', and (3) a category feature (e.g., fashion, travel, and food).'
  id: totrans-split-117
  prefs: []
  type: TYPE_NORMAL
- en: '**Figure 4.** Details of the proposed model to predict post score. To calculate
    the post score, the proposed model predicts the class probability of the engagement
    rate using three integrated features <math display="inline"><semantics><mi mathvariant="bold-italic">h</mi></semantics></math>
    .'
  id: totrans-split-118
  prefs: []
  type: TYPE_NORMAL
- en: '**Figure 4.** Details of the proposed model to predict post score. To calculate
    the post score, the proposed model predicts the class probability of the engagement
    rate using three integrated features <math display="inline"><semantics><mi mathvariant="bold-italic">h</mi></semantics></math>
    .'
  id: totrans-split-119
  prefs: []
  type: TYPE_NORMAL
- en: 3.1.1\. Calculation of Post Score Using Multiple Features
  id: totrans-split-120
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: To obtain the multimodal feature, we utilize contrastive language-image pre-training
    (CLIP) [
  id: totrans-split-121
  prefs: []
  type: TYPE_NORMAL
- en: '[37](#B37-sensors-24-00921)'
  id: totrans-split-122
  prefs: []
  type: TYPE_NORMAL
- en: ']. CLIP includes two neural networks of an image encoder'
  id: totrans-split-123
  prefs: []
  type: TYPE_NORMAL
- en: <math display="inline"><semantics><mrow><msub><mi>CLIP</mi> <mi>image</mi></msub>
    <mrow><mo>(</mo> <mo>·</mo> <mo>)</mo></mrow></mrow></semantics></math>
  id: totrans-split-124
  prefs: []
  type: TYPE_NORMAL
- en: and a text encoder
  id: totrans-split-125
  prefs: []
  type: TYPE_NORMAL
- en: <math display="inline"><semantics><mrow><msub><mi>CLIP</mi> <mi>text</mi></msub>
    <mrow><mo>(</mo> <mo>·</mo> <mo>)</mo></mrow></mrow></semantics></math>
  id: totrans-split-126
  prefs: []
  type: TYPE_NORMAL
- en: . CLIP is trained on a very large image–text pair dataset, thus extracting image
    and text features in highly expressive spaces. The proposed method obtains features
  id: totrans-split-127
  prefs: []
  type: TYPE_NORMAL
- en: <math display="inline"><semantics><mrow><msup><mrow><mi mathvariant="bold-italic">f</mi></mrow>
    <mi>I</mi></msup> <mo>∈</mo> <msup><mi mathvariant="double-struck">R</mi> <mi>D</mi></msup></mrow></semantics></math>
  id: totrans-split-128
  prefs: []
  type: TYPE_NORMAL
- en: and
  id: totrans-split-129
  prefs: []
  type: TYPE_NORMAL
- en: <math display="inline"><semantics><mrow><msup><mrow><mi mathvariant="bold-italic">f</mi></mrow>
    <mi>T</mi></msup> <mo>∈</mo> <msup><mi mathvariant="double-struck">R</mi> <mi>D</mi></msup></mrow></semantics></math>
  id: totrans-split-130
  prefs: []
  type: TYPE_NORMAL
- en: extracted from the image
  id: totrans-split-131
  prefs: []
  type: TYPE_NORMAL
- en: I
  id: totrans-split-132
  prefs: []
  type: TYPE_NORMAL
- en: and text
  id: totrans-split-133
  prefs: []
  type: TYPE_NORMAL
- en: T
  id: totrans-split-134
  prefs: []
  type: TYPE_NORMAL
- en: 'in the CLIP space as follows:'
  id: totrans-split-135
  prefs: []
  type: TYPE_NORMAL
- en: <math display="block"><semantics><mtable displaystyle="true"><mtr><mtd columnalign="right"><mrow><msup><mrow><mi
    mathvariant="bold-italic">f</mi></mrow> <mi>I</mi></msup> <mo>=</mo> <msub><mi>CLIP</mi>
    <mi>image</mi></msub> <mrow><mo>(</mo> <mi>I</mi> <mo>)</mo></mrow> <mo>,</mo></mrow></mtd></mtr></mtable></semantics></math>
  id: totrans-split-136
  prefs: []
  type: TYPE_NORMAL
- en: <math display="block"><semantics><mtable displaystyle="true"><mtr><mtd columnalign="right"><mrow><msup><mrow><mi
    mathvariant="bold-italic">f</mi></mrow> <mi>T</mi></msup> <mo>=</mo> <msub><mi>CLIP</mi>
    <mi>text</mi></msub> <mrow><mo>(</mo> <mi>T</mi> <mo>)</mo></mrow> <mo>,</mo></mrow></mtd></mtr></mtable></semantics></math>
  id: totrans-split-137
  prefs: []
  type: TYPE_NORMAL
- en: After deriving the image and text features, we concatenate these features and
    calculate the final multimodal feature
  id: totrans-split-138
  prefs: []
  type: TYPE_NORMAL
- en: <math display="inline"><semantics><mrow><mi mathvariant="bold-italic">m</mi>
    <mo>∈</mo> <msup><mi mathvariant="double-struck">R</mi> <mrow><mn>2</mn> <mi>D</mi></mrow></msup></mrow></semantics></math>
  id: totrans-split-139
  prefs: []
  type: TYPE_NORMAL
- en: 'as follows:'
  id: totrans-split-140
  prefs: []
  type: TYPE_NORMAL
- en: This multimodal feature
  id: totrans-split-141
  prefs: []
  type: TYPE_NORMAL
- en: <math display="inline"><semantics><mi mathvariant="bold-italic">m</mi></semantics></math>
  id: totrans-split-142
  prefs: []
  type: TYPE_NORMAL
- en: is an important factor in predicting post scores because it provides a comprehensive
    representation of the image
  id: totrans-split-143
  prefs: []
  type: TYPE_NORMAL
- en: I
  id: totrans-split-144
  prefs: []
  type: TYPE_NORMAL
- en: and text
  id: totrans-split-145
  prefs: []
  type: TYPE_NORMAL
- en: T
  id: totrans-split-146
  prefs: []
  type: TYPE_NORMAL
- en: contained in a post.
  id: totrans-split-147
  prefs: []
  type: TYPE_NORMAL
- en: With a previous work [
  id: totrans-split-148
  prefs: []
  type: TYPE_NORMAL
- en: '[23](#B23-sensors-24-00921)'
  id: totrans-split-149
  prefs: []
  type: TYPE_NORMAL
- en: '] that clears the relationship between the aesthetic of posted images and the
    engagement rates, we calculate an aesthetics feature of the image'
  id: totrans-split-150
  prefs: []
  type: TYPE_NORMAL
- en: I
  id: totrans-split-151
  prefs: []
  type: TYPE_NORMAL
- en: to predict post scores. To calculate the aesthetics feature, we apply a neural
    image assessment (NIMA) model [
  id: totrans-split-152
  prefs: []
  type: TYPE_NORMAL
- en: '[54](#B54-sensors-24-00921)'
  id: totrans-split-153
  prefs: []
  type: TYPE_NORMAL
- en: ']. NIMA can predict the distribution of human opinion scores for image aesthetics
    using convolutional neural networks. The NIMA model takes an image as input and
    outputs a 10-dimensional softmaxed probability distribution representing the quality
    score. We use this distribution, which is output by inputting the image'
  id: totrans-split-154
  prefs: []
  type: TYPE_NORMAL
- en: I
  id: totrans-split-155
  prefs: []
  type: TYPE_NORMAL
- en: into the NIMA model, as the aesthetic feature
  id: totrans-split-156
  prefs: []
  type: TYPE_NORMAL
- en: <math display="inline"><semantics><mrow><mi mathvariant="bold-italic">n</mi>
    <mo>∈</mo> <msup><mi mathvariant="double-struck">R</mi> <mn>10</mn></msup></mrow></semantics></math>
  id: totrans-split-157
  prefs: []
  type: TYPE_NORMAL
- en: in the proposed model to predict the post score. This feature
  id: totrans-split-158
  prefs: []
  type: TYPE_NORMAL
- en: <math display="inline"><semantics><mi mathvariant="bold-italic">n</mi></semantics></math>
  id: totrans-split-159
  prefs: []
  type: TYPE_NORMAL
- en: can represent the aesthetics of the image
  id: totrans-split-160
  prefs: []
  type: TYPE_NORMAL
- en: I
  id: totrans-split-161
  prefs: []
  type: TYPE_NORMAL
- en: ', which potentially has a significant impact on the overall quality of the
    posts.'
  id: totrans-split-162
  prefs: []
  type: TYPE_NORMAL
- en: Considering the relationship between post categories and the engagement rates
    [
  id: totrans-split-163
  prefs: []
  type: TYPE_NORMAL
- en: '[22](#B22-sensors-24-00921)'
  id: totrans-split-164
  prefs: []
  type: TYPE_NORMAL
- en: '], we additionally focus on a category feature. Following ['
  id: totrans-split-165
  prefs: []
  type: TYPE_NORMAL
- en: '[18](#B18-sensors-24-00921)'
  id: totrans-split-166
  prefs: []
  type: TYPE_NORMAL
- en: '], we distinguish eight categories of posts: beauty, family, fashion, fitness,
    food, interior, pet, and travel. However, since manually labeling posts into those
    eight categories requires a great deal of effort, the proposed model leverages
    the power of CLIP. Specifically, we first calculate the text features of the eight
    texts'
  id: totrans-split-167
  prefs: []
  type: TYPE_NORMAL
- en: <math display="inline"><semantics><msubsup><mrow><mo>{</mo> <msub><mi>C</mi>
    <mi>i</mi></msub> <mo>}</mo></mrow> <mrow><mi>i</mi> <mo>=</mo> <mn>1</mn></mrow>
    <mn>8</mn></msubsup></semantics></math>
  id: totrans-split-168
  prefs: []
  type: TYPE_NORMAL
- en: 'representing the category names in the CLIP space as follows:'
  id: totrans-split-169
  prefs: []
  type: TYPE_NORMAL
- en: <math display="block"><semantics><mtable displaystyle="true"><mtr><mtd columnalign="right"><mrow><msubsup><mrow><mo>{</mo>
    <msup><mrow><mi mathvariant="bold-italic">f</mi></mrow> <msub><mi>C</mi> <mi>i</mi></msub></msup>
    <mo>}</mo></mrow> <mrow><mi>i</mi> <mo>=</mo> <mn>1</mn></mrow> <mn>8</mn></msubsup>
    <mo>=</mo> <msubsup><mrow><mo>{</mo> <msub><mi>CLIP</mi> <mi>text</mi></msub>
    <mrow><mo>(</mo> <msub><mi>C</mi> <mi>i</mi></msub> <mo>)</mo></mrow> <mo>}</mo></mrow>
    <mrow><mi>i</mi> <mo>=</mo> <mn>1</mn></mrow> <mn>8</mn></msubsup> <mo>.</mo></mrow></mtd></mtr></mtable></semantics></math>
  id: totrans-split-170
  prefs: []
  type: TYPE_NORMAL
- en: The proposed model calculates the cosine similarity
  id: totrans-split-171
  prefs: []
  type: TYPE_NORMAL
- en: <math display="inline"><semantics><msub><mi>s</mi> <mi>i</mi></msub></semantics></math>
  id: totrans-split-172
  prefs: []
  type: TYPE_NORMAL
- en: between the text feature
  id: totrans-split-173
  prefs: []
  type: TYPE_NORMAL
- en: <math display="inline"><semantics><msup><mrow><mi mathvariant="bold-italic">f</mi></mrow>
    <msub><mi>C</mi> <mi>i</mi></msub></msup></semantics></math>
  id: totrans-split-174
  prefs: []
  type: TYPE_NORMAL
- en: and the image feature
  id: totrans-split-175
  prefs: []
  type: TYPE_NORMAL
- en: <math display="inline"><semantics><msup><mrow><mi mathvariant="bold-italic">f</mi></mrow>
    <mi>I</mi></msup></semantics></math>
  id: totrans-split-176
  prefs: []
  type: TYPE_NORMAL
- en: obtained in Equation (
  id: totrans-split-177
  prefs: []
  type: TYPE_NORMAL
- en: '[1](#FD1-sensors-24-00921)'
  id: totrans-split-178
  prefs: []
  type: TYPE_NORMAL
- en: ') as follows:'
  id: totrans-split-179
  prefs: []
  type: TYPE_NORMAL
- en: <math display="block"><semantics><mtable displaystyle="true"><mtr><mtd columnalign="right"><mrow><msub><mi>s</mi>
    <mi>i</mi></msub> <mo>=</mo> <mfrac><mrow><msup><mrow><mi mathvariant="bold-italic">f</mi></mrow>
    <mi>I</mi></msup> <mo>·</mo> <msup><mrow><mi mathvariant="bold-italic">f</mi></mrow>
    <msub><mi>C</mi> <mi>i</mi></msub></msup></mrow> <mrow><mrow><mo>|</mo> <mo>|</mo></mrow>
    <msup><mrow><mi mathvariant="bold-italic">f</mi></mrow> <mi>I</mi></msup> <msub><mrow><mo>|</mo>
    <mo>|</mo></mrow> <mn>2</mn></msub> <mrow><mo>|</mo> <mo>|</mo></mrow> <msup><mrow><mi
    mathvariant="bold-italic">f</mi></mrow> <msub><mi>C</mi> <mi>i</mi></msub></msup>
    <msub><mrow><mo>|</mo> <mo>|</mo></mrow> <mn>2</mn></msub></mrow></mfrac> <mo>.</mo></mrow></mtd></mtr></mtable></semantics></math>
  id: totrans-split-180
  prefs: []
  type: TYPE_NORMAL
- en: We obtain a vector
  id: totrans-split-181
  prefs: []
  type: TYPE_NORMAL
- en: <math display="inline"><semantics><mi mathvariant="bold-italic">s</mi></semantics></math>
  id: totrans-split-182
  prefs: []
  type: TYPE_NORMAL
- en: representing the category of the image
  id: totrans-split-183
  prefs: []
  type: TYPE_NORMAL
- en: I
  id: totrans-split-184
  prefs: []
  type: TYPE_NORMAL
- en: by concatenating the calculated cosine similarities
  id: totrans-split-185
  prefs: []
  type: TYPE_NORMAL
- en: <math display="inline"><semantics><msubsup><mrow><mo>{</mo> <msub><mi>s</mi>
    <mi>i</mi></msub> <mo>}</mo></mrow> <mrow><mi>i</mi> <mo>=</mo> <mn>1</mn></mrow>
    <mn>8</mn></msubsup></semantics></math>
  id: totrans-split-186
  prefs: []
  type: TYPE_NORMAL
- en: 'as follows:'
  id: totrans-split-187
  prefs: []
  type: TYPE_NORMAL
- en: <math display="block"><semantics><mtable displaystyle="true"><mtr><mtd columnalign="right"><mrow><mi
    mathvariant="bold-italic">s</mi> <mo>=</mo> <mo>[</mo> <msub><mi>s</mi> <mn>1</mn></msub>
    <mo>;</mo> <msub><mi>s</mi> <mn>2</mn></msub> <mo>;</mo> <mo>…</mo> <mo>;</mo>
    <msub><mi>s</mi> <mn>8</mn></msub> <mo>]</mo> <mo>.</mo></mrow></mtd></mtr></mtable></semantics></math>
  id: totrans-split-188
  prefs: []
  type: TYPE_NORMAL
- en: The proposed model finally obtains the category feature
  id: totrans-split-189
  prefs: []
  type: TYPE_NORMAL
- en: <math display="inline"><semantics><mrow><mi mathvariant="bold-italic">c</mi>
    <mo>∈</mo> <msup><mi mathvariant="double-struck">R</mi> <mn>8</mn></msup></mrow></semantics></math>
  id: totrans-split-190
  prefs: []
  type: TYPE_NORMAL
- en: by applying the softmax function to the vector
  id: totrans-split-191
  prefs: []
  type: TYPE_NORMAL
- en: <math display="inline"><semantics><mi mathvariant="bold-italic">s</mi></semantics></math>
  id: totrans-split-192
  prefs: []
  type: TYPE_NORMAL
- en: . The feature
  id: totrans-split-193
  prefs: []
  type: TYPE_NORMAL
- en: <math display="inline"><semantics><mi mathvariant="bold-italic">c</mi></semantics></math>
  id: totrans-split-194
  prefs: []
  type: TYPE_NORMAL
- en: can represent the category to which the image
  id: totrans-split-195
  prefs: []
  type: TYPE_NORMAL
- en: I
  id: totrans-split-196
  prefs: []
  type: TYPE_NORMAL
- en: belongs without manual labeling.
  id: totrans-split-197
  prefs: []
  type: TYPE_NORMAL
- en: The proposed model predicts the class of the engagement rate based on the three
    calculated features
  id: totrans-split-198
  prefs: []
  type: TYPE_NORMAL
- en: <math display="inline"><semantics><mi mathvariant="bold-italic">m</mi></semantics></math>
  id: totrans-split-199
  prefs: []
  type: TYPE_NORMAL
- en: ','
  id: totrans-split-200
  prefs: []
  type: TYPE_NORMAL
- en: <math display="inline"><semantics><mi mathvariant="bold-italic">n</mi></semantics></math>
  id: totrans-split-201
  prefs: []
  type: TYPE_NORMAL
- en: ', and'
  id: totrans-split-202
  prefs: []
  type: TYPE_NORMAL
- en: <math display="inline"><semantics><mi mathvariant="bold-italic">c</mi></semantics></math>
  id: totrans-split-203
  prefs: []
  type: TYPE_NORMAL
- en: . Specifically, we use the three features and first calculate the concatenated
    feature
  id: totrans-split-204
  prefs: []
  type: TYPE_NORMAL
- en: <math display="inline"><semantics><mrow><mi mathvariant="bold-italic">h</mi>
    <mo>∈</mo> <msup><mi mathvariant="double-struck">R</mi> <mrow><mn>2</mn> <mi>D</mi>
    <mo>+</mo> <mn>18</mn></mrow></msup></mrow></semantics></math>
  id: totrans-split-205
  prefs: []
  type: TYPE_NORMAL
- en: 'as follows:'
  id: totrans-split-206
  prefs: []
  type: TYPE_NORMAL
- en: This concatenated feature
  id: totrans-split-207
  prefs: []
  type: TYPE_NORMAL
- en: <math display="inline"><semantics><mi mathvariant="bold-italic">h</mi></semantics></math>
  id: totrans-split-208
  prefs: []
  type: TYPE_NORMAL
- en: can consider the aesthetics and categories of the image
  id: totrans-split-209
  prefs: []
  type: TYPE_NORMAL
- en: I
  id: totrans-split-210
  prefs: []
  type: TYPE_NORMAL
- en: while comprehensively representing the image
  id: totrans-split-211
  prefs: []
  type: TYPE_NORMAL
- en: I
  id: totrans-split-212
  prefs: []
  type: TYPE_NORMAL
- en: and text
  id: totrans-split-213
  prefs: []
  type: TYPE_NORMAL
- en: T
  id: totrans-split-214
  prefs: []
  type: TYPE_NORMAL
- en: contained in the post. The feature
  id: totrans-split-215
  prefs: []
  type: TYPE_NORMAL
- en: <math display="inline"><semantics><mi mathvariant="bold-italic">h</mi></semantics></math>
  id: totrans-split-216
  prefs: []
  type: TYPE_NORMAL
- en: is passed through a fully connected layer to obtain the class distribution
  id: totrans-split-217
  prefs: []
  type: TYPE_NORMAL
- en: <math display="inline"><semantics><mover accent="true"><mi mathvariant="bold-italic">y</mi>
    <mo stretchy="false">^</mo></mover></semantics></math>
  id: totrans-split-218
  prefs: []
  type: TYPE_NORMAL
- en: of the engagement rate. By using
  id: totrans-split-219
  prefs: []
  type: TYPE_NORMAL
- en: <math display="inline"><semantics><mover accent="true"><mi mathvariant="bold-italic">y</mi>
    <mo stretchy="false">^</mo></mover></semantics></math>
  id: totrans-split-220
  prefs: []
  type: TYPE_NORMAL
- en: ', the proposed model calculates the post score as follows:'
  id: totrans-split-221
  prefs: []
  type: TYPE_NORMAL
- en: <math display="block"><semantics><mtable displaystyle="true"><mtr><mtd columnalign="right"><mrow><mi>Post</mi>
    <mi>score</mi> <mo>=</mo> <munderover><mo>∑</mo> <mrow><mi>k</mi> <mo>=</mo> <mn>1</mn></mrow>
    <mi>K</mi></munderover> <mrow><mo>(</mo> <mi>k</mi> <mo>−</mo> <mn>1</mn> <mo>)</mo></mrow>
    <mo>×</mo> <msub><mover accent="true"><mi>y</mi> <mo stretchy="false">^</mo></mover>
    <mi>k</mi></msub> <mo>,</mo></mrow></mtd></mtr></mtable></semantics></math>
  id: totrans-split-222
  prefs: []
  type: TYPE_NORMAL
- en: where
  id: totrans-split-223
  prefs: []
  type: TYPE_NORMAL
- en: <math display="inline"><semantics><msub><mover accent="true"><mi>y</mi> <mo
    stretchy="false">^</mo></mover> <mi>k</mi></msub></semantics></math>
  id: totrans-split-224
  prefs: []
  type: TYPE_NORMAL
- en: is the
  id: totrans-split-225
  prefs: []
  type: TYPE_NORMAL
- en: k
  id: totrans-split-226
  prefs: []
  type: TYPE_NORMAL
- en: th elements of the predicted class distribution
  id: totrans-split-227
  prefs: []
  type: TYPE_NORMAL
- en: <math display="inline"><semantics><mover accent="true"><mi mathvariant="bold-italic">y</mi>
    <mo stretchy="false">^</mo></mover></semantics></math>
  id: totrans-split-228
  prefs: []
  type: TYPE_NORMAL
- en: ', and'
  id: totrans-split-229
  prefs: []
  type: TYPE_NORMAL
- en: K
  id: totrans-split-230
  prefs: []
  type: TYPE_NORMAL
- en: is the number of classes for engagement rates. The more skewed the distribution
    is toward classes with high engagement rates, the higher the post score.
  id: totrans-split-231
  prefs: []
  type: TYPE_NORMAL
- en: 3.1.2\. Loss Function
  id: totrans-split-232
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: To predict the class distribution of the engagement rate, we apply cross entropy
    [
  id: totrans-split-233
  prefs: []
  type: TYPE_NORMAL
- en: '[55](#B55-sensors-24-00921)'
  id: totrans-split-234
  prefs: []
  type: TYPE_NORMAL
- en: '] as a loss function and treat the classification task for the class of the
    engagement rate. The loss function'
  id: totrans-split-235
  prefs: []
  type: TYPE_NORMAL
- en: <math display="inline"><semantics><mi mathvariant="script">L</mi></semantics></math>
  id: totrans-split-236
  prefs: []
  type: TYPE_NORMAL
- en: 'is described as follows:'
  id: totrans-split-237
  prefs: []
  type: TYPE_NORMAL
- en: <math display="block"><semantics><mtable displaystyle="true"><mtr><mtd columnalign="right"><mrow><mi
    mathvariant="script">L</mi> <mo>=</mo> <mo>−</mo> <munderover><mo>∑</mo> <mrow><mi>k</mi>
    <mo>=</mo> <mn>1</mn></mrow> <mi>K</mi></munderover> <msub><mi>y</mi> <mi>k</mi></msub>
    <mi>log</mi> <msub><mover accent="true"><mi>y</mi> <mo stretchy="false">^</mo></mover>
    <mi>k</mi></msub> <mo>,</mo></mrow></mtd></mtr></mtable></semantics></math>
  id: totrans-split-238
  prefs: []
  type: TYPE_NORMAL
- en: where
  id: totrans-split-239
  prefs: []
  type: TYPE_NORMAL
- en: <math display="inline"><semantics><msub><mi>y</mi> <mi>k</mi></msub></semantics></math>
  id: totrans-split-240
  prefs: []
  type: TYPE_NORMAL
- en: is the
  id: totrans-split-241
  prefs: []
  type: TYPE_NORMAL
- en: k
  id: totrans-split-242
  prefs: []
  type: TYPE_NORMAL
- en: th elements of the ground truth class distribution
  id: totrans-split-243
  prefs: []
  type: TYPE_NORMAL
- en: <math display="inline"><semantics><mi mathvariant="bold-italic">y</mi></semantics></math>
  id: totrans-split-244
  prefs: []
  type: TYPE_NORMAL
- en: . By minimizing the loss calculated from this function
  id: totrans-split-245
  prefs: []
  type: TYPE_NORMAL
- en: <math display="inline"><semantics><mi mathvariant="script">L</mi></semantics></math>
  id: totrans-split-246
  prefs: []
  type: TYPE_NORMAL
- en: ', it is possible to predict the class distribution of the engagement rate given
    the image'
  id: totrans-split-247
  prefs: []
  type: TYPE_NORMAL
- en: I
  id: totrans-split-248
  prefs: []
  type: TYPE_NORMAL
- en: and text
  id: totrans-split-249
  prefs: []
  type: TYPE_NORMAL
- en: T
  id: totrans-split-250
  prefs: []
  type: TYPE_NORMAL
- en: as inputs.
  id: totrans-split-251
  prefs: []
  type: TYPE_NORMAL
- en: 3.2\. Editing of Post Image Based on Post Score
  id: totrans-split-252
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: To obtain multiple results of text-guided image editing, the proposed method
    first leverages a recently proposed large language model (i.e., GPT-3 [
  id: totrans-split-253
  prefs: []
  type: TYPE_NORMAL
- en: '[53](#B53-sensors-24-00921)'
  id: totrans-split-254
  prefs: []
  type: TYPE_NORMAL
- en: ']). We create the sentence “Generate four different paraphrases of'
  id: totrans-split-255
  prefs: []
  type: TYPE_NORMAL
- en: <math display="inline"><semantics><mrow><mo>{</mo> <mi>P</mi> <mo>}</mo></mrow></semantics></math>
  id: totrans-split-256
  prefs: []
  type: TYPE_NORMAL
- en: ”. using the text prompt
  id: totrans-split-257
  prefs: []
  type: TYPE_NORMAL
- en: P
  id: totrans-split-258
  prefs: []
  type: TYPE_NORMAL
- en: to be used for image editing. The proposed method can obtain four representations
    similar to the text prompt
  id: totrans-split-259
  prefs: []
  type: TYPE_NORMAL
- en: P
  id: totrans-split-260
  prefs: []
  type: TYPE_NORMAL
- en: by giving the created sentence to the large language model. Using multiple text
    prompts increases the possibility of obtaining edited images that gain attention
    from a greater audience on social media while performing image editing desired
    by the user. The proposed method uses the four representations similar to the
    text prompt
  id: totrans-split-261
  prefs: []
  type: TYPE_NORMAL
- en: P
  id: totrans-split-262
  prefs: []
  type: TYPE_NORMAL
- en: and generates four edited images based on the text-guided image editing method
    [
  id: totrans-split-263
  prefs: []
  type: TYPE_NORMAL
- en: '[15](#B15-sensors-24-00921)'
  id: totrans-split-264
  prefs: []
  type: TYPE_NORMAL
- en: '].'
  id: totrans-split-265
  prefs: []
  type: TYPE_NORMAL
- en: To select the edited images that will receive the attention from a greater audience
    on social media, we apply the proposed model to predict post scores constructed
    in
  id: totrans-split-266
  prefs: []
  type: TYPE_NORMAL
- en: '[Section 3.1](#sec3dot1-sensors-24-00921)'
  id: totrans-split-267
  prefs: []
  type: TYPE_NORMAL
- en: . The proposed method inputs the text to be posted and the four edited images,
    respectively, into that model and obtains four post scores. Finally, the proposed
    method selects the edited image
  id: totrans-split-268
  prefs: []
  type: TYPE_NORMAL
- en: <math display="inline"><semantics><msup><mi>I</mi> <mo>′</mo></msup></semantics></math>
  id: totrans-split-269
  prefs: []
  type: TYPE_NORMAL
- en: with the highest post score from all edited images. From the above, it is possible
    to generate the image
  id: totrans-split-270
  prefs: []
  type: TYPE_NORMAL
- en: <math display="inline"><semantics><msup><mi>I</mi> <mo>′</mo></msup></semantics></math>
  id: totrans-split-271
  prefs: []
  type: TYPE_NORMAL
- en: that is performed text-guided image editing and will gain attention on social
    media. By using the proposed method, the burden of using text-guided image editing
    can be reduced for users without knowledge of social media marketing.
  id: totrans-split-272
  prefs: []
  type: TYPE_NORMAL
- en: 6\. Conclusions
  id: totrans-split-273
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This paper has proposed a novel text-guided image editing method based on post
    scores for gaining attention on social media. The proposed method newly introduces
    the model to predict post scores on social media from posted images and text,
    thereby generating edited images that gain much attention from the audience. In
    the proposed method, we apply the pre-trained text-guided image editing method
    and obtain multiple edited images from the multiple text prompts generated from
    the large language model. Among these, leveraging the novel model that predicts
    post scores representing engagement rates, the proposed method selects the edited
    images that will gain the most attention from the audience on social media. Results
    of subjective experiments demonstrated that the edited images generated from the
    proposed method accurately reflect the content of the text prompts and provide
    a positive impression to the audience on social media. These results are supported
    by the subjective evaluation that subjects are most willing to give a “like” or
    comment when they find posts including edited images generated from the proposed
    method on social media.
  id: totrans-split-274
  prefs: []
  type: TYPE_NORMAL
- en: In the future, we will improve the proposed model to predict the post score
    by devising a new integration technique for the image and text features. Also,
    the accuracy of image editing can be ensured by selecting edited images based
    on scores calculated from the evaluation metrics of text-guided image editing
    in addition to post scores, enhancing the overall performance of the proposed
    method.
  id: totrans-split-275
  prefs: []
  type: TYPE_NORMAL
