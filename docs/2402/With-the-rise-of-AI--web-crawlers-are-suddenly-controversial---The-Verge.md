<!--yml

category: 未分类

date: 2024-05-27 14:58:11

-->

# 随着人工智能的兴起，网络爬虫突然成为了有争议的话题 - The Verge

> 来源：[https://www.theverge.com/24067997/robots-txt-ai-text-file-web-crawlers-spiders](https://www.theverge.com/24067997/robots-txt-ai-text-file-web-crawlers-spiders)

三十年来，一个微小的文本文件阻止了互联网陷入混乱。这个文本文件没有特定的法律或技术权威，甚至并不特别复杂。它代表了互联网的早期先驱之间的一种握手协议，以尊重彼此的意愿，并以有利于每个人的方式构建互联网。它是互联网的迷你宪法，以代码形式书写。

它被称为robots.txt，通常位于yourwebsite.com/robots.txt。这个文件允许任何运行网站的人 —— 不论是大公司还是小博客，烹饪博客还是跨国企业 —— 告诉网络谁可以进入，谁不可以。哪些搜索引擎可以索引你的网站？什么存档项目可以抓取你页面的版本并保存它？竞争对手可以监视你的页面用于他们自己的文件吗？这些都由你决定，并向网络宣告。

这不是一个完美的系统，但它确实有效。不过以前是这样。数十年来，robots.txt的主要焦点是搜索引擎；你允许它们抓取你的网站，作为回报，它们会承诺将人们带回你的网站。现在AI改变了这个方程式：全网的公司都在使用你的网站和其数据来构建大量的训练数据，以便建立可能完全不承认你存在的模型和产品。

robots.txt文件管理的是一种互惠互利的关系；对许多人来说，AI感觉就像是只会索取而不会付出。但现在AI带来了如此多的资金，技术前沿变化如此之快，以至于许多网站所有者无法跟上。而robots.txt背后的基本协议，以及整个网络 —— 长期以来的“大家都冷静点”的共识 —— 也许也无法跟得上了。

* * *

在互联网早期，机器人有许多名称：蜘蛛、爬虫、虫子、WebAnts、网络爬虫。大多数情况下，它们都是出于善意构建的。通常是开发者试图建立一个酷炫新网站的目录，确保他们自己的网站正常运行，或者建立一个研究数据库 —— 那时大约是1993年，远在搜索引擎遍地开花之前，当你的电脑硬盘几乎可以装下整个互联网的时代。

当时唯一的真正问题是流量：访问互联网对于查看网站的人和托管网站的人来说都很慢且昂贵。如果你把网站托管在自己的电脑上，就像许多人那样，或者通过家庭互联网连接运行匆忙搭建的服务器软件，只需几个过于热心下载你网页的机器人就足以导致事情崩溃，电话费飙升。

在1994年的几个月里，一位名叫马蒂茹恩·科斯特（Martijn Koster）的软件工程师和开发者，与其他一群网络管理员和开发者一起，提出了一种解决方案，他们称之为机器人排除协议（Robots Exclusion Protocol）。这个提议相当简单：要求网络开发者在其域中添加一个纯文本文件，指定哪些机器人不被允许扫描他们的网站，或列出对所有机器人禁止访问的页面。（再次强调，那时候你可以维护每一个现存的机器人列表 —— 科斯特和其他几个人友好地做到了。）对于机器人制造商来说，协议更简单：遵守文本文件的规定。

从一开始，科斯特就明确表示他并不讨厌机器人，也不打算摆脱它们。“机器人是网络中少数几个会引起运行问题和给人们带来困扰的方面之一，”他在1994年初向一个名为WWW-Talk的邮件列表（其中包括互联网早期先驱如Tim Berners-Lee和Marc Andreessen）发送的一封初始电子邮件中说。“同时它们也提供有用的服务。”科斯特警告说不要争论机器人是好还是坏 —— 因为这并不重要，它们已经存在，而且不会消失。他只是试图设计一个可能“最大程度地减少问题并可能最大化好处”的系统。

“机器人是网络中少数几个会引起运行问题和给人们带来困扰的方面之一。同时，它们也提供有用的服务。”

到了那年夏天，他的提议已经成为一种标准 —— 虽然不是官方标准，但基本上是被普遍接受的。科斯特在六月再次向WWW-Talk组发送了更新信息。“简而言之，这是一种通过在服务器上提供简单的文本文件来指导机器人远离Web服务器URL空间中某些区域的方法，”他写道。“如果你有大型档案、带有大量URL子树的CGI脚本、临时信息或者你根本不想为机器人服务，这将特别方便。”他建立了一个专门的邮件列表，其成员已经就这些文本文件的基本语法和结构达成了一致，将文件的名称从RobotsNotWanted.txt更改为简单的robots.txt，并且几乎所有人都同意支持它。

在接下来的30年中，这种方法运行得相当不错。

但是互联网已经不能再容纳在硬盘中了，而且机器人的能力也大大增强。谷歌利用它们来爬取和索引整个网络，作为其搜索引擎的接口，这使得公司每年赚取数十亿美元。必应的爬虫也是如此，微软将其数据库授权给其他搜索引擎和公司使用。互联网档案馆利用爬虫保存网页供后人参考。亚马逊的爬虫在网络上寻找产品信息，根据最近的反垄断诉讼，该公司利用这些信息惩罚那些在亚马逊之外提供更好交易的卖家。像 OpenAI 这样的 AI 公司正在爬取网络，以训练大型语言模型，这可能再次从根本上改变我们获取和分享信息的方式。

下载、存储、组织和查询现代互联网的能力使任何公司或开发者能够利用世界积累的知识。在过去的一年左右，像 ChatGPT 这样的 AI 产品及其背后的大型语言模型已经使高质量的训练数据成为互联网最有价值的商品之一。这导致各种互联网服务提供商重新评估其服务器上数据的价值，并重新思考谁能访问这些数据。太放任可能会使你的网站失去所有价值；太严格可能会使你变得无形。你必须在新公司、新合作伙伴和新利益上不断做出这样的选择。

* * *

有几种类型的网络机器人。你可能会构建一个完全无害的机器人来爬行并确保所有页面上的链接仍然指向其他活跃页面；你可能会派遣一个更为可疑的机器人在网络上收集每一个电子邮件地址或电话号码。但最常见的，也是当前最具争议的，是简单的网络爬虫。它的任务是尽可能多地找到和下载互联网的内容。

网络爬虫通常相当简单。它们从知名网站开始，如 cnn.com 或 wikipedia.org 或 health.gov。（如果你运行一个通用搜索引擎，会从各种主题的高质量域名开始；如果只关心体育或汽车，就直接从汽车网站开始。）爬虫下载第一页并存储在某处，然后自动点击页面上的每个链接，下载所有这些页面，点击每个页面上的所有链接，如此在网络中传播。有足够的时间和计算资源，爬虫最终会找到并下载数十亿个网页。

这种权衡相当简单：如果谷歌可以爬取你的页面，它就可以对其进行索引并在搜索结果中显示出来。

Google 在2019年估计，超过5亿个网站有一个 robots.txt 页面，规定这些爬虫是否可以访问以及可以访问什么内容。这些页面的结构通常大致相同：它命名了一个“User-agent”，这是爬虫在向服务器标识自己时使用的名称。Google 的代理是 Googlebot；亚马逊的是 Amazonbot；必应的是 Bingbot；OpenAI 的是 GPTBot。Pinterest、LinkedIn、Twitter 和许多其他站点和服务都有自己的机器人，不一定在每个页面上都提到。（[维基百科](https://zh.wikipedia.org/robots.txt) 和 [Facebook](https://facebook.com/robots.txt) 是两个具有特别详细的机器人记录的平台。）在下面，robots.txt 页面列出了某个代理不允许访问的站点的部分或页面，以及允许的特定例外。如果行中只写“Disallow: /”，则完全不欢迎爬虫。

对于大多数人来说，“服务器超载”已经不再是一个真正的问题。“现在，通常更多地是关于个人偏好，而不是网站上使用的资源，” Google 的搜索倡导者 John Mueller 表示。“你想让哪些内容被爬取和索引？”

对大多数网站所有者历史上最大的问题是是否允许 Googlebot 爬取他们的网站。这种权衡相当简单：如果 Google 可以爬取你的页面，它可以索引并在搜索结果中显示。任何你希望在 Google 中可见的页面，Googlebot 都需要看到。（Google 如何在搜索结果中实际显示该页面是完全不同的故事。）问题在于，你是否愿意让 Google 消耗一些带宽并下载你网站的副本，以换取搜索带来的可见性。

对于大多数网站来说，这是一笔易于交易的生意。Medium 的 CEO Tony Stubblebine 表示：“Google 是我们最重要的蜘蛛。” Google 可以下载 Medium 的所有页面，“作为交换，我们获得了大量的流量。这是双赢。每个人都这么认为。” 这就是 Google 与整个互联网达成的协议，将流量引导到其他网站，同时在搜索结果中销售广告。据所有报告显示，Google 在 robots.txt 方面表现良好。“几乎所有知名的搜索引擎都遵守它，” Google 的 Mueller 表示。“他们乐于爬取网络，但不想用它来打扰人们……这只是为了让每个人的生活更轻松。”

* * *

然而，在过去一年左右，人工智能的兴起颠覆了这一方程式。对于许多出版商和平台来说，他们的数据被用于训练数据感觉更像是偷窃而不是交换。“我们很快发现AI公司”，斯塔布尔宾说，“不仅没有交换价值，我们一无所获。真的是零。”当斯塔布尔宾在去年秋天宣布Medium将[屏蔽AI爬虫](https://blog.medium.com/default-no-to-ai-training-on-your-stories-abb5b4589c8)时，他写道，“AI公司已经从作家身上吸取了价值，以便向互联网读者发送垃圾信息。”

在过去的一年里，大部分媒体行业都在回应斯塔布尔宾的观点。BBC国家总监罗德里·塔尔凡·戴维斯在去年秋天写道，[“我们不认为当前未经我们许可而‘爬取’BBC数据以训练Gen AI模型符合公共利益”](https://www.bbc.co.uk/mediacentre/articles/2023/generative-ai-at-the-bbc/)，宣布BBC也将屏蔽OpenAI的爬虫。*纽约时报*几个月前也屏蔽了GPTBot，并在之后对OpenAI提起诉讼，指控OpenAI的模型“通过复制和使用数百万*时报*的受版权保护的新闻文章、深度调查、观点文章、评论、操作指南等建立”。[由本·威尔士进行的一项研究](https://palewi.re/docs/news-homepages/openai-gptbot-robotstxt.html)，*路透社*的新闻应用编辑，发现在1,156家受调查的出版商中，有606家在其robots.txt文件中屏蔽了GPTBot。

不仅仅是出版商，亚马逊、Facebook、Pinterest、WikiHow、WebMD等许多平台明确禁止GPTBot访问它们的一些或所有网站。在这些robots.txt页面中，OpenAI的GPTBot是唯一明确和完全不允许的爬虫。但是还有许多其他专门用于AI的机器人开始爬行网络，如Anthropic的anthropic-ai和Google的新Google-Extended。根据Originality.AI去年秋天的一项研究，排名前1,000的网站中有306家屏蔽了GPTBot，但只有85家屏蔽了Google-Extended，28家屏蔽了anthropic-ai。

还有用于网页搜索和AI的爬虫。由Common Crawl组织运行的CCBot为搜索引擎目的在网络上搜索，但其数据也被OpenAI、Google和其他公司用于训练模型。Microsoft的Bingbot既是搜索爬虫又是AI爬虫。这些只是自我标识的爬虫，还有许多试图以相对隐秘的方式运行，使其难以停止甚至找到它们在其他网络流量中的存在。对于任何足够受欢迎的网站来说，找到一个偷偷摸摸的爬虫就像大海中的一根针一样困难。

在很大程度上，GPTBot 已经成为 robots.txt 的主要对手，因为 OpenAI 允许它存在。该公司发布并推广了一篇关于如何阻止 GPTBot 的页面，并且构建了其爬虫，每次接近一个网站时都会大声自识别。当然，这些都是在训练了使其如此强大的基础模型之后才进行的，并且只有在它成为技术生态系统中重要组成部分之后才这样做。但是，OpenAI 的首席战略官 Jason Kwon 表示，这正是重点所在。“我们是生态系统中的一员，”他说，“如果你想以开放的方式参与这个生态系统，那么这就是每个人都感兴趣的互惠交易。” 他说，没有这种交易，网络就会开始收缩，关闭 —— 这对 OpenAI 和所有人来说都是不利的。“我们做所有这些是为了让网络保持开放。”

默认情况下，机器人排除协议始终是宽容的。它认为，正如 30 年前科斯特所认为的那样，大多数机器人都是好的，并且是由好人制造的，因此默认情况下允许它们。总体而言，这是正确的决定。“我认为互联网在本质上是一个社会性生物，”OpenAI 的 Kwon 说道，“而这种几十年来持续存在的握手似乎是有效的。” 他说，OpenAI 在维护该协议方面的角色包括保持 ChatGPT 对大多数用户的免费使用 — 从而返还这种价值 — 并尊重机器人的规则。

但是，robots.txt 并不是法律文件 —— 30 年后的今天，它仍然依赖所有相关方的善意。

但是，robots.txt 并不是法律文件 —— 30 年后的今天，它仍然依赖所有相关方的善意。在 robots.txt 页面上禁止机器人就像在你的树屋上挂上“禁止女孩入内”的牌子 —— 它传达了一种信息，但在法庭上不会站得住脚。任何想要忽略 robots.txt 的爬虫都可以轻易地这样做，几乎不用担心后果。（关于网络抓取的一般法律先例也有些许，尽管其中大部分都是允许爬行和抓取的。）例如，互联网档案馆在2017年宣布不再遵守 robots.txt 的规则。“随着时间的推移，我们观察到专门针对搜索引擎爬虫的 robots.txt 文件并不一定符合我们的档案目的”，互联网档案馆的 Wayback Machine 主管 Mark Graham 在[当时写道](https://blog.archive.org/2017/04/17/robots-txt-meant-for-search-engines-dont-work-well-for-web-archives/)。就这样。

随着人工智能公司的不断增加，它们的网络爬虫变得越来越不道德，任何想要置身事外或等待人工智能接管的人都必须进行无休止的抵抗。如果AI确实是搜索的未来，就像Google和其他公司预测的那样，阻止AI爬虫可能是一个短期的胜利，但长期可能是一场灾难。

有人认为我们需要更好、更强、更严格的工具来管理网络爬虫。他们认为，涉及的利益太大，而且出现了太多新的未受监管的使用案例，不能仅仅依靠所有人自觉遵守规则。“尽管许多行为者在使用爬虫时有一些自我管理的规则，”两位专注于科技的律师在[2019年的一篇论文](https://digitalcommons.law.uw.edu/wjlta/vol13/iss3/4/)中写道，“但总体上规则还是太弱，而且很难追究其责任。”

一些出版商希望对爬取的内容及其使用有更详细的控制，而不是仅仅依赖于 robots.txt 的全盘允许或拒绝权限。几年前，Google 努力使 Robots Exclusion Protocol 成为官方正式标准，但也主张减少对 robots.txt 的重视，理由是这是一个过时的标准，而且太多网站没有注意到它的存在。Google 的信任副总裁 Danielle Romain [去年写道](https://blog.google/technology/ai/ai-web-publisher-controls-sign-up/)：“我们意识到现有的网络发布控制机制是在新的人工智能和研究用例出现之前开发的。”“我们认为现在是网络和人工智能社区探索额外的可机读方式，以便对新兴的人工智能和研究用例进行选择和控制的时候了。”

尽管人工智能公司在如何构建和训练其模型方面面临着监管和法律问题，这些模型仍在不断改进，并且每天都有新公司涌现。无论大小网站都面临一个抉择：是顺应人工智能革命还是对抗它。“对于选择退出的人来说，他们最强大的武器是三十年前由一些最早和最乐观的真正信仰者制定的协议。”他们相信互联网是一个良好的地方，充满了善良的人，最重要的是，希望互联网成为一个好东西。在那个世界和互联网上，在一个简单的纯文本文件中表达你的意愿已经足够管理。现在，随着人工智能再次重新塑造互联网的文化和经济，一个朴素的纯文本文件开始显得有点过时。
