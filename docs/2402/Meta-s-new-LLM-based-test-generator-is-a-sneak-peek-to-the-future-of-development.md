<!--yml

category: 未分类

date: 2024-05-29 13:18:46

-->

# Meta最新基于LLM的测试生成器是未来开发的一个预览。

> 来源：[https://read.engineerscodex.com/p/metas-new-llm-based-test-generator](https://read.engineerscodex.com/p/metas-new-llm-based-test-generator)

*Engineer’s Codex是一个关于现实世界软件工程的出版物。

* * *

Meta最近发布了一篇名为“Meta在自动化单元测试改进中使用大语言模型”的论文（https://arxiv.org/abs/2402.09171）。这是一个深入了解大型科技公司如何内部使用AI来加快开发速度和减少软件缺陷的好机会。例如，[Google正在使用AI加速代码审查](https://read.engineerscodex.com/i/139414745/critique-googles-code-review-tool)。

本文的一个重要亮点是，它将LLMs集成到开发者的工作流程中，同时推荐经过验证的完全形成的软件改进，这些改进既正确又能提高当前代码覆盖率。这**不是灵丹妙药，但它是使LLMs更有用的一个良好开端**。与ChatGPT相比，建议仍然需要手动验证才能生效，我们都知道，[调试代码比编写代码难上两倍](https://read.engineerscodex.com/p/clever-code-is-probably-the-worst)。

Meta声称“这是第一篇报告LLM生成的代码，这些代码在没有人类干预（最终审查除外）的情况下开发，并且已经进入了大规模工业生产系统，并保证比现有代码库有所改进。”（哎呀！这话说起来真费口舌。）

此外，开发者可以掌握一些坚实的原则，以便自己有效地使用人工智能。

**目录：**

* * *

***[SWE Quiz](http://swequiz.com)**是为有雄心的开发者设计的，他们想确保自己的软件基础牢固。揭示你知识中的空白，并确保你在工作和面试时确实知道自己在做什么。*

*为[身份验证](https://www.swequiz.com/learn/authentication-roadmap)、[缓存](https://swequiz.com/learn/caching-roadmap)、[数据库](https://www.swequiz.com/learn/databases-roadmap)、[API设计](https://www.swequiz.com/learn/api-design-roadmap)和[更多](https://www.swequiz.com/learn)进行测试。*

[查看SWE Quiz](https://swequiz.com)

* * *

TestGen-LLM采用一种称为‘保证LLM基础软件工程’（Assured LLMSE）的方法，使用私有的、内部的LLMs，这些LLMs可能与Meta的代码库进行了微调。这意味着它使用LLMs生成的代码改进，支持“**可验证的保证**，确保改进和非回归”。

TestGen-LLM采用**合奏方法**生成代码改进。这意味着它使用**“LLMs、提示和超参数”**生成一组候选改进，然后选择最佳的一个。这种方法有助于提高生成改进的质量。

TestGen-LLM专门设计用于**改进现有的人工编写的测试**，而不是**从头生成代码**。可以这样理解这个LLM：它是一个初级开发人员，负责为现有代码创建更全面的测试。其他开发人员有更重要的事情要做，因此这个LLM有幸负责改进单元测试。

它在拉取请求中创建的测试通常是好的，有时是微不足道或无意义的。偶尔，它生成的测试确实很好，或者无意中揭示了一个bug。无论如何，这项工作由于优先事项原因，人类本来也不会做。所有的拉取请求在推入代码库之前都需要人工审查。

TestGen-LLM已经**集成到Meta的软件工程工作流程**中。这意味着它可以作为开发过程的一部分自动改进测试。看到它如何确切地集成将会很酷，但论文没有提供任何截图。

这些统计数据要么是直接引用要么是引用的摘录。需要提醒一下，所有成功投入的测试都需要人工审查批准。

+   上图显示：在Instagram的Reels和Stories产品的评估中，75%的TestGen-LLM生成的测试用例正确构建，57%可靠通过，而*25%的覆盖率增加*。

+   TestGen-LLM能够改进应用于的所有类的**10%**，其测试改进**73%**被开发人员接受，并投入生产。

+   在Meta工程师进行的“测试马拉松”中，各种Meta工程师创建测试以增加Instagram的测试覆盖率，“**TestGen-LLM测试平均增加的代码行数为2.5**。”

+   然而，有一个测试案例“大获成功”，覆盖了1326行代码。

+   在“测试马拉松”期间生成的所有改进案例，“至少覆盖了一个额外的有效边界情况，例如提前返回和/或特殊值（如null和空列表）的特殊处理。”

**TestGen-LLM是LLMs如何在时间高效的情况下用于提高开发效率和软件可靠性的良好示例。**（注：这些大部分是我从论文中得出的个人观点。）

在大型代码库中，小的上下文窗口和分散的依赖使LLMs几乎无法用于非样板解决方案。除了隐私问题外，将多个代码文件粘贴到LLM中可能不切实际，例如C++头文件中可能有20个以上的依赖关系。即使粘贴了多个文件，使用LLM在聊天窗口或GitHub Copilot代码编辑器中输出的代码也需要时间和认知成本。

额外的认知负荷的代价不容小觑。[Hacker News的评论者发现基于GPT的工具的不准确性令人筋疲力尽和不可靠。](https://news.ycombinator.com/item?id=39460788) 这正是**验证输出是否既有效又不回退非常重要的地方**。TestGen-LLM的每个测试都需要人类签署批准，所以这里的任何程序化保证都是有用的。

对于大型代码库的长期生产力提升来说，**改进可能会逐步出现在增量、专业化的用例上**，比如测试生成和[代码审查期间的自动建议](https://blog.research.google/2023/05/resolving-code-review-comments-with-ml.html)。这些也是节省累积开发时间的低风险方式。基本上，“GPT包装”将继续发挥作用 🙂。

**LLM的真正价值在于显示出边缘情况**。写好代码的悖论在于，[从未发生过的问题修复往往不会得到任何认可](https://web.mit.edu/nelsonr/www/Repenning=Sterman_CMR_su01_.pdf)。

[威尔·威尔逊写道](https://antithesis.com/blog/is_something_bugging_you/)：

> *“软件测试的根本问题在于……软件必须处理开发者从未考虑过或永远无法预料的许多情况。这限制了测试的价值，因为如果你有预见性地为特定情况编写测试，那么你可能也有预见性地让代码处理该情况。这使得传统测试在捕捉回归方面非常出色，但在捕捉生活、宇宙和你的无穷创意用户将向你投掷的所有“未知未知”方面确实非常糟糕。”*

**LLM实际上无法“超出盒子”的想法，因为它们只真正了解它们的训练数据。然而，它们的盒子可能比人类的大，因此它们有潜力考虑到人类可能会忽略的情况。**

Meta的TestGen-LLM创建的大多数测试案例仅覆盖了额外的2.5行。然而，一个测试案例覆盖了1326行！有两种方式来思考这个案例：

+   乐观的情况是，LLM可能捕捉到了在测试中被人类忽略的重要边缘情况。

+   更现实的情况是LLM捕捉到了一个完全被忽略的代码路径，而不是捕捉到了一个边缘情况。

**更多的代码覆盖率并不等于更好的代码**。我们不希望将覆盖的代码行数作为可以追求的指标，尽管这可能是任何糟糕的工程经理的梦想。更多的代码覆盖率是可以操纵的，但由LLM生成的机械覆盖的缺点是，缺陷也被编码了，这意味着它们在生产中可能被忽略，直到出现问题。

这里的价值主要在于将开发者不太关注但仍在LLM培训数据范围内的边缘情况带到开发者的注意力中。

蓝色框表示LLM在这里可能会有用的空间。

正如我之前提到的，理解这个LLM的一个好方法是把它看作是一个初级开发人员，负责为现有代码创建更全面的测试。**LLM可能不会在第一次尝试时生成完美的代码，但至少提供了一些否则可能没有被考虑到的选项。**其他开发人员有更重要的事情要做，所以这个LLM得到了改进单元测试的有趣任务。它在拉取请求中创建的测试通常很好，有时是琐碎或毫无意义的。偶尔，它生成的测试非常好，或者无意中发现了一个bug。

实际上，这一高度是如此之高，以至于FoundationDB创始人的创业公司[Antithesis](https://antithesis.com/)的创造者，完全基于软件测试边缘情况最好通过不断搜索软件来发现问题。[作为参考，FoundationDB被苹果收购，并成为苹果iCloud数十亿数据库的基础。](https://read.engineerscodex.com/p/how-apple-built-icloud-to-store-billions)

基础模型LLM并非“即插即用”，合理地不应期望如此。当然，它们可能会输出完美的React和Tailwind CSS代码，但在大多数生产代码库中，这只是一个狭窄的用例。它们需要大量的处理和过滤，以处理需要正确性的代码生成任务。这种处理的一部分意味着用例化LLM与示例。谷歌和Meta都基于现有代码提出**基于现有代码的建议**，在这些情况下，结果比原始生成要好得多。在生产中使用的LLM应该借鉴Meta处理和过滤LLM输出的方法，并且大多数输出都应该预期被丢弃。

LLM最有效的工作方式是集成到工作流程中。这也是为什么GitHub Copilot如此受欢迎，以及Google Workspace集成的另一个原因。像聊天机器人询问一样，对于某些用例，比如生成样板代码，效果很好，但在更复杂的用例中，聊天机器人经常会失败。

TestGen-LLM对Meta内部LLM生成的候选解决方案应用一系列语义筛选器，确保只保留最有价值的测试。以下是它的工作原理：

**筛选器1：可构建性：** 最初，TestGen-LLM检查生成的代码是否可以在应用程序的现有基础设施中构建。任何不能构建的代码都会立即被丢弃。

**筛选器2：执行（测试是否通过？）：** 接下来，系统运行通过了构建性筛选器的测试。任何不能通过的测试都会被丢弃。这一步骤非常关键，因为如果没有一种自动确定失败测试的有效性的方法（无论是由于bug还是不正确的断言），TestGen-LLM选择保留那些可以用于回归测试的测试（即确保它们可以保护当前代码免受未来回归的影响）。

**过滤器 3：易碎性：** 为了解决[易碎性](https://www.swequiz.com/learn/what-are-flaky-tests)（在相同条件下测试结果不一致的问题），TestGen-LLM采用重复执行。测试必须在多次（五次）执行中始终通过才能被视为非易碎。

**过滤器 3：覆盖率改进：** 最后，为了确保新测试实际上增加了价值，TestGen-LLM评估它们对测试覆盖率的贡献。不增加通过探索新代码路径或条件来增强覆盖率的测试将被丢弃。只有提供新见解或防止回归的测试才会被保留。

这些处理过滤器非常重要，因为它们保证了对测试套件的改进。这也表明LLM远非“即插即用”。

通过所有这些过滤器成功通过的测试可以保证增强现有的测试套件，提供可靠的回归测试能力，而不会重复努力或浪费资源。在TestGen-LLM中，测试前后处理步骤有助于提取和重建测试类，简化新测试集成到软件开发工作流程中。

这篇论文很好地形式化了一个使用案例，许多开发人员可能已经在像ChatGPT、Gemini和Mistral/LLaMA这样的LLM中使用。将其写下来是追踪未来在软件可靠性空间中改进LLM的进展的好方法。到目前为止，我们看到LLM最适合作为人类的延伸，最好被视为需要帮助和推动的初级开发人员。随着时间的推移，我们肯定会看到LLM能够在日益复杂的软件系统中捕捉和测试错误。

问题是 - 长远来看，这是否会使软件开发变得更加容易，还是会导致未来软件复杂性的蔓延？
