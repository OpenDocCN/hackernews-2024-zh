- en: <!--yml
  id: totrans-split-0
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  id: totrans-split-1
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-05-27 14:39:40'
  id: totrans-split-2
  prefs: []
  type: TYPE_NORMAL
- en: -->
  id: totrans-split-3
  prefs: []
  type: TYPE_NORMAL
- en: New localllm lets you develop gen AI apps locally, without GPUs | Google Cloud
    Blog
  id: totrans-split-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://cloud.google.com/blog/products/application-development/new-localllm-lets-you-develop-gen-ai-apps-locally-without-gpus](https://cloud.google.com/blog/products/application-development/new-localllm-lets-you-develop-gen-ai-apps-locally-without-gpus)
  id: totrans-split-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: In today's fast-paced AI landscape, developers face numerous challenges when
    it comes to building applications that use large language models (LLMs). In particular,
    the scarcity of GPUs, which are traditionally required for running LLMs, poses
    a significant hurdle. In this post, we introduce you to a novel solution that
    allows developers to harness the power of LLMs locally on CPU and memory, right
    within [Cloud Workstations](https://cloud.google.com/workstations), Google Cloud’s
    fully managed development environment. The models we use in this walkthrough are
    located on [Hugging Face](https://huggingface.co/) and are specifically in a repo
    from “The Bloke” and are compatible with the quantization method used to allow
    them to run on CPUs or low power GPUs. This innovative approach not only eliminates
    the need for GPUs but also opens up a world of possibilities for seamless and
    efficient application development. By using a combination of “quantized models,”
    Cloud Workstations, a new open-source tool named [localllm](https://github.com/googlecloudplatform/localllm),
    and generally available resources, you can develop AI-based applications on a
    well-equipped development workstation, leveraging existing processes and workflows.
  id: totrans-split-6
  prefs: []
  type: TYPE_NORMAL
- en: '**Quantized models + Cloud Workstations == Productivity**'
  id: totrans-split-7
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Quantized models are AI models that have been optimized to run on local devices
    with limited computational resources. These models are designed to be more efficient
    in terms of memory usage and processing power, allowing them to run smoothly on
    devices such as smartphones, laptops, and other edge devices. In this case, we
    are running them on Cloud Workstations with ample available resources. Here are
    some great examples of why leveraging quantized models in your development loop
    may unblock your efforts:'
  id: totrans-split-8
  prefs: []
  type: TYPE_NORMAL
- en: '**Improved performance:** Quantized models are optimized to perform computations
    using lower-precision data types such as 8-bit integers, instead of standard 32-bit
    floating-point numbers. This reduction in precision allows for faster computations
    and improved performance on devices with limited resources.'
  id: totrans-split-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Reduced memory footprint:** Quantization techniques help reduce the memory
    requirements of AI models. By representing weights and activations with fewer
    bits, the overall size of the model is reduced, making it easier to fit on devices
    with limited storage capacity.'
  id: totrans-split-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Faster inference:** Quantized models can perform computations more quickly
    due to their reduced precision and smaller model size. This enables faster inference
    times, allowing AI applications to run more smoothly and responsively on local
    devices.'
  id: totrans-split-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Combining quantized models with Cloud Workstations allows you to take advantage
    of the flexibility, scalability and cost effectiveness of Cloud Workstations.
    Moreover, the traditional approach of relying on remote servers or cloud-based
    GPU instances for LLM-based application development can introduce latency, security
    concerns, and dependency on third-party services. A solution that lets you leverage
    LLMs locally, within your Cloud Workstations, without compromising performance,
    security, or control over your data, can have a lot of benefits.
  id: totrans-split-12
  prefs: []
  type: TYPE_NORMAL
- en: '**Introducing** **localllm**'
  id: totrans-split-13
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Today, we’re introducing `localllm`, a set of tools and libraries that provides
    easy access to quantized models from HuggingFace through a command-line utility.
    `localllm` can be a game-changer for developers seeking to leverage LLMs without
    the constraints of GPU availability. This repository provides a comprehensive
    framework and tools to run LLMs locally on CPU and memory, right within the Google
    Cloud Workstation, using this method (though you can also run LLM models on your
    local machine or anywhere with sufficient CPU). By eliminating the dependency
    on GPUs, you can unlock the full potential of LLMs for your application development
    needs.
  id: totrans-split-14
  prefs: []
  type: TYPE_NORMAL
- en: '**Key features and benefits**'
  id: totrans-split-15
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**GPU-free LLM execution**: `localllm` lets you execute LLMs on CPU and memory,
    removing the need for scarce GPU resources, so you can integrate LLMs into your
    application development workflows, without compromising performance or productivity.'
  id: totrans-split-16
  prefs: []
  type: TYPE_NORMAL
- en: '**Enhanced productivity**: With `localllm`, you use LLMs directly within the
    Google Cloud ecosystem. This integration streamlines the development process,
    reducing the complexities associated with remote server setups or reliance on
    external services. Now, you can focus on building innovative applications without
    managing GPUs.'
  id: totrans-split-17
  prefs: []
  type: TYPE_NORMAL
- en: '**Cost efficiency**: By leveraging `localllm`, you can significantly reduce
    infrastructure costs associated with GPU provisioning. The ability to run LLMs
    on CPU and memory within the Google Cloud environment lets you optimize resource
    utilization, resulting in cost savings and improved return on investment.'
  id: totrans-split-18
  prefs: []
  type: TYPE_NORMAL
- en: '**Improved data security**: Running LLMs locally on CPU and memory helps keep
    sensitive data within your control. With `localllm`, you can mitigate the risks
    associated with data transfer and third-party access, enhancing data security
    and privacy.'
  id: totrans-split-19
  prefs: []
  type: TYPE_NORMAL
- en: '**Seamless integration with Google Cloud services**: `localllm` integrates
    with various Google Cloud services, including data storage, machine learning APIs,
    or other Google Cloud services, so you can leverage the full potential of the
    Google Cloud ecosystem.'
  id: totrans-split-20
  prefs: []
  type: TYPE_NORMAL
- en: '**Getting started with** **localllm**'
  id: totrans-split-21
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: To get started with the `localllm`, visit the GitHub repository at [https://github.com/googlecloudplatform/localllm](https://github.com/googlecloudplatform/localllm).
    The repository provides detailed documentation, code samples, and step-by-step
    instructions to set up and utilize LLMs locally on CPU and memory within the Google
    Cloud environment. You can explore the repository, contribute to its development,
    and leverage its capabilities to enhance your application development workflows.
  id: totrans-split-22
  prefs: []
  type: TYPE_NORMAL
- en: Once you’ve cloned the repo locally, the following simple steps will run localllm
    with a quantized model of your choice from the HuggingFace repo “The Bloke,” then
    execute an initial sample prompt query. For example we are using Llama.
  id: totrans-split-23
  prefs: []
  type: TYPE_NORMAL
