- en: <!--yml
  id: totrans-split-0
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  id: totrans-split-1
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-05-27 14:47:29'
  id: totrans-split-2
  prefs: []
  type: TYPE_NORMAL
- en: -->
  id: totrans-split-3
  prefs: []
  type: TYPE_NORMAL
- en: On the Proposed California SB 1047 - by Zvi Mowshowitz
  id: totrans-split-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://thezvi.substack.com/p/on-the-proposed-california-sb-1047](https://thezvi.substack.com/p/on-the-proposed-california-sb-1047)
  id: totrans-split-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[California Senator Scott Wiener](https://twitter.com/Scott_Wiener/status/1755650108287578585)
    of San Francisco introduces [SB 1047](https://t.co/JWaOLP44Iu)  [to regulate AI](https://www.washingtonpost.com/technology/2024/02/08/california-legislation-artificial-intelligence-regulation/).
    I have put up a market on [how likely it is to become law](https://manifold.markets/ZviMowshowitz/will-california-bill-sb-1047-become).'
  id: totrans-split-6
  prefs: []
  type: TYPE_NORMAL
- en: “If Congress at some point is able to pass a strong pro-innovation, pro-safety
    AI law, I’ll be the first to cheer that, but I’m not holding my breath,” Wiener
    said in an interview. “We need to get ahead of this so we maintain public trust
    in AI.”
  id: totrans-split-7
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Congress is certainly highly dysfunctional. I am still generally against California
    trying to act like it is the federal government, even when the cause is good,
    but I understand.
  id: totrans-split-8
  prefs: []
  type: TYPE_NORMAL
- en: Can California effectively impose its will here?
  id: totrans-split-9
  prefs: []
  type: TYPE_NORMAL
- en: On the biggest players, for now, presumably yes.
  id: totrans-split-10
  prefs: []
  type: TYPE_NORMAL
- en: In the longer run, when things get actively dangerous, then my presumption is
    no.
  id: totrans-split-11
  prefs: []
  type: TYPE_NORMAL
- en: There is a potential trap here. If we put our rules in a place where someone
    with enough upside can ignore them, and we never then pass anything in Congress.
  id: totrans-split-12
  prefs: []
  type: TYPE_NORMAL
- en: So what does it do, according to the bill’s author?
  id: totrans-split-13
  prefs: []
  type: TYPE_NORMAL
- en: 'California Senator Scott Wiener: SB 1047 does a few things:'
  id: totrans-split-14
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Establishes clear, predictable, common-sense safety standards for developers
    of the largest and most powerful AI systems. These standards apply only to the
    largest models, not startups.
  id: totrans-split-15
  prefs:
  - PREF_BQ
  - PREF_OL
  type: TYPE_NORMAL
- en: Establish CalCompute, a public AI cloud compute cluster. CalCompute will be
    a resource for researchers, startups, & community groups to fuel innovation in
    CA, bring diverse perspectives to bear on AI development, & secure our continued
    dominance in AI.
  id: totrans-split-16
  prefs:
  - PREF_BQ
  - PREF_OL
  type: TYPE_NORMAL
- en: prevent price discrimination & anticompetitive behavior
  id: totrans-split-17
  prefs:
  - PREF_BQ
  - PREF_OL
  type: TYPE_NORMAL
- en: institute know-your-customer requirements
  id: totrans-split-18
  prefs:
  - PREF_BQ
  - PREF_OL
  type: TYPE_NORMAL
- en: protect whistleblowers at large AI companies
  id: totrans-split-19
  prefs:
  - PREF_BQ
  - PREF_OL
  type: TYPE_NORMAL
- en: '@geoffreyhinton called SB 1047 “a very sensible approach” to balancing these
    needs. Leaders representing a broad swathe of the AI community have expressed
    support.'
  id: totrans-split-20
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: People are rightfully concerned that the immense power of AI models could present
    serious risks. For these models to succeed the way we need them to, users must
    trust that AI models are safe and aligned w/ core values. Fulfilling basic safety
    duties is a good place to start.
  id: totrans-split-21
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: With AI, we have the opportunity to apply the hard lessons learned over the
    past two decades. Allowing social media to grow unchecked without first understanding
    the risks has had disastrous consequences, and we should take reasonable precautions
    this time around.
  id: totrans-split-22
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: As usual, RTFC (Read the Card, or here [the bill](https://leginfo.legislature.ca.gov/faces/billNavClient.xhtml?bill_id=202320240SB1047))
    applies.
  id: totrans-split-23
  prefs: []
  type: TYPE_NORMAL
- en: Section 1 names the bill.
  id: totrans-split-24
  prefs: []
  type: TYPE_NORMAL
- en: Section 2 says California is winning in AI ([see this song](https://www.youtube.com/watch?v=BATf_eUcb8M&ab_channel=ThePresidentsoftheUnitedStatesofAmerica)),
    AI has great potential but could do harm. A missed opportunity to mention existential
    risks.
  id: totrans-split-25
  prefs: []
  type: TYPE_NORMAL
- en: Section 3 22602 offers definitions. I have some notes.
  id: totrans-split-26
  prefs: []
  type: TYPE_NORMAL
- en: Usual concerns with the broad definition of AI.
  id: totrans-split-27
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Odd that ‘a model autonomously engaging in a sustained sequence of unsafe behavior’
    only counts as an ‘AI safety incident’ if it is not ‘at the request of a user.’
    If a user requests that, aren’t you supposed to ensure the model doesn’t do it?
    Sounds to me like a safety incident.
  id: totrans-split-28
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Covered model is defined primarily via compute, not sure why this isn’t a ‘foundation’
    model, I like the secondary extension clause: “The artificial intelligence model
    was trained using a quantity of computing power greater than 10^26 integer or
    floating-point operations in 2024, or a model that could reasonably be expected
    to have similar performance on benchmarks commonly used to quantify the performance
    of state-of-the-art foundation models, as determined by industry best practices
    and relevant standard setting organizations OR The artificial intelligence model
    has capability below the relevant threshold on a specific benchmark but is of
    otherwise similar general capability..”'
  id: totrans-split-29
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Critical harm is either mass casualties or 500 million in damage, or comparable.
  id: totrans-split-30
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Full shutdown means full shutdown but only within your possession and control.
    So when we really need a full shutdown, this definition won’t work. The whole
    point of a shutdown is that it happens everywhere whether you control it or not.
  id: totrans-split-31
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Open-source artificial intelligence model is defined to only include models
    that ‘may be freely modified and redistributed’ so that raises the question of
    whether that is legal or practical. Such definitions need to be practical, if
    I can do it illegally but can clearly still do it, that needs to count.
  id: totrans-split-32
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Definition (s): [“Positive safety determination” means a determination, pursuant
    to subdivision (a) or (c) of Section 22603, with respect to a covered model that
    is not a derivative model that a developer can reasonably exclude the possibility
    that a covered model has a hazardous capability or may come close to possessing
    a hazardous capability when accounting for a reasonable margin for safety and
    the possibility of posttraining modifications.]'
  id: totrans-split-33
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Very happy to see the mention of post-training modifications, which is later
    noted to include access to tools and data, so scaffolding explicitly counts.
  id: totrans-split-34
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Section 3 22603 (a) says that before you train a new non-derivative model, you
    need to determine whether you can make a positive safety determination.
  id: totrans-split-35
  prefs: []
  type: TYPE_NORMAL
- en: I like that this happens before you start training. But of course, this raises
    the question of how you know how it will score on the benchmarks?
  id: totrans-split-36
  prefs: []
  type: TYPE_NORMAL
- en: One thing I worry about is the concept that if you score below another model
    on various benchmarks, that this counts as a positive safety determination. There
    are at least four obvious failure modes for this.
  id: totrans-split-37
  prefs: []
  type: TYPE_NORMAL
- en: The developer might choose to sabotage performance against the benchmarks, either
    by excluding relevant data and training, or otherwise. Or, alternatively, a previous
    developer might have gamed the benchmarks, which happens all the time, such that
    all you have to do to score lower is to not game those benchmarks yourself.
  id: totrans-split-38
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The model might have situational awareness, and choose to get a lower score.
    This could be various degrees of intentional on the part of the developers.
  id: totrans-split-39
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The model might not adhere to your predictions or scaling laws. So perhaps you
    say it will score lower on benchmarks, but who is to say you are right?
  id: totrans-split-40
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The benchmarks might simply not be good at measuring what we care about.
  id: totrans-split-41
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Similarly, it is good to make a safety determination before beginning training,
    but also if the model is worth training then you likely cannot actually know its
    safety in advance, especially since this is not only existential safety.
  id: totrans-split-42
  prefs: []
  type: TYPE_NORMAL
- en: 'Section 3 22603 (b) covers what you must do if you cannot make the positive
    safety determination. Here are the main provisions:'
  id: totrans-split-43
  prefs: []
  type: TYPE_NORMAL
- en: You must prevent unauthorized access.
  id: totrans-split-44
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: You must be capable of a full shutdown.
  id: totrans-split-45
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: You must implement all covered guidance. Okie dokie.
  id: totrans-split-46
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: You must implement a written and separate safety and security protocol, that
    provides ‘reasonable assurance’ that it would ensure the model will have safeguards
    that prevent critical harms. This has to include clear tests that verify if you
    have succeeded.
  id: totrans-split-47
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: You must say how you are going to do all that, how you would change how you
    are doing it, and what would trigger a shutdown.
  id: totrans-split-48
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Provide a copy of your protocol and keep it updated.
  id: totrans-split-49
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: You can then make a ‘positive safety determination’ after training and testing,
    subject to the safety protocol.
  id: totrans-split-50
  prefs: []
  type: TYPE_NORMAL
- en: Section (d) says that if your model is ‘not subject to a positive safety determination,’
    in order to deploy it (you can still deploy it at all?!) you need to implement
    ‘reasonable safeguards and requirements’ that allow you prevent harms and to trace
    any harms that happen. I worry this section is not taking such scenarios seriously.
    To not be subject to such determination, the model needs to be breaking new ground
    in capabilities, and you were unable to assure that it wouldn’t be dangerous.
    So what are these ‘reasonable safeguards and requirements’ that would make deploying
    it acceptable? Perhaps I am misunderstanding here.
  id: totrans-split-51
  prefs: []
  type: TYPE_NORMAL
- en: Section (g) says safety incidents must be reported.
  id: totrans-split-52
  prefs: []
  type: TYPE_NORMAL
- en: Section (h) says if your positive safety determination is unreasonable it does
    not count, and that to be reasonable you need to consider any risk that has already
    been identified elsewhere.
  id: totrans-split-53
  prefs: []
  type: TYPE_NORMAL
- en: Overall, this seems like a good start, but I worry it has loopholes, and I worry
    that it is not thinking about the future scenarios where the models are potentially
    existentially dangerous, or might exhibit unanticipated capabilities or situational
    awareness and so on. There is still the DC-style ‘anticipate and check specific
    harm’ approach throughout.
  id: totrans-split-54
  prefs: []
  type: TYPE_NORMAL
- en: Section 22604 is about KYC, a large computing cluster has to collect the information
    and check to see if customers are trying to train a covered model.
  id: totrans-split-55
  prefs: []
  type: TYPE_NORMAL
- en: Section 22605 requires sellers of inference or a computing cluster to provide
    a transparent, uniform, publicly available price schedule, banning price discrimination,
    and bans ‘unlawful discrimination or noncompetitive activity in determining price
    or access.’
  id: totrans-split-56
  prefs: []
  type: TYPE_NORMAL
- en: I always wonder about laws that say ‘you cannot do things that are already illegal,’
    I mean I thought that was the whole point of them already being illegal.
  id: totrans-split-57
  prefs: []
  type: TYPE_NORMAL
- en: I am not sure to what extent this rule has an impact in practice, and whether
    it effectively means that anyone selling such services has to be a kind of common
    carrier unable to pick who gets its limited services, and unable to make deals
    of any kind. I see the appeal, but also I see clear economic downsides to forcing
    this.
  id: totrans-split-58
  prefs: []
  type: TYPE_NORMAL
- en: Section 22606 covers penalties. The fines are relatively limited in scope, the
    main relief is injunction against and possible deletion of the model. I worry
    in practice that there is not enough teeth here.
  id: totrans-split-59
  prefs: []
  type: TYPE_NORMAL
- en: Section 2207 is whistleblower protections. Odd that this is necessary, one would
    think there would be such protections universally by now? There are no unexpectedly
    strong provisions here, only the normal stuff.
  id: totrans-split-60
  prefs: []
  type: TYPE_NORMAL
- en: Section 4 11547.6 tasks the new Frontier Model Division with its official business,
    including collecting reports and issuing guidance.
  id: totrans-split-61
  prefs: []
  type: TYPE_NORMAL
- en: Section 5 11547.7 is for the CalCompute public cloud computing cluster. This
    seems like a terrible idea, there is no reason for public involvement here, also
    there is no stated or allocated budget. Assuming it is small, it does not much
    matter.
  id: totrans-split-62
  prefs: []
  type: TYPE_NORMAL
- en: Sections 6-9 are standard boilerplate disclaimers and rules.
  id: totrans-split-63
  prefs: []
  type: TYPE_NORMAL
- en: What should we think about all that?
  id: totrans-split-64
  prefs: []
  type: TYPE_NORMAL
- en: It seems like a good faith effort to put forward a helpful bill. It has a lot
    of good ideas in it. I believe it would be net helpful. In particular, it is structured
    such that if your model is not near the frontier, your burden here is very small.
  id: totrans-split-65
  prefs: []
  type: TYPE_NORMAL
- en: My worry is that this has potential loopholes in various places, and does not
    yet strongly address the nature of the future more existential threats. If you
    want to ignore this law, you probably can.
  id: totrans-split-66
  prefs: []
  type: TYPE_NORMAL
- en: But it seems like a good beginning, especially on dealing with relatively mundane
    but still potentially catastrophic threats, without imposing an undo burden on
    developers. This could then be built upon.
  id: totrans-split-67
  prefs: []
  type: TYPE_NORMAL
- en: Ah, Tyler Cowen has a link on this and it’s… [California’s Effort to Strange
    AI](https://hyperdimensional.substack.com/p/californias-effort-to-strangle-ai?utm_source=post-email-title&publication_id=2244049&post_id=141516001&utm_campaign=email-post-title&isFreemail=false&r=3j06n&utm_medium=email).
  id: totrans-split-68
  prefs: []
  type: TYPE_NORMAL
- en: Because of course it is. We do this every time. People keep saying ‘this law
    will ban satire’ or spreadsheets or pictures of cute puppies or whatever, based
    on what on its best day would be a maximalist anti-realist reading of the proposal,
    if it were enacted straight with no changes and everyone actually enforced it
    to the letter.
  id: totrans-split-69
  prefs: []
  type: TYPE_NORMAL
- en: 'Dean Ball: This week, California’s legislature introduced [SB 1047: The Safe
    and Secure Innovation for Frontier Artificial Intelligence Systems Act](https://leginfo.legislature.ca.gov/faces/billTextClient.xhtml?bill_id=202320240SB1047).
    The bill, introduced by State Senator Scott Wiener (liked by many, myself included,
    for his pro-housing stance), would create a sweeping regulatory regime for AI,
    apply the precautionary principle to all AI development, and effectively outlaw
    all new open source AI models—possibly throughout the United States.'
  id: totrans-split-70
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: This is a line pulled out whenever anyone proposes that AI be governed by any
    regulatory regime whatsoever even with zero teeth of any kind. When someone says
    that someone, somewhere might be legally required to write an email.
  id: totrans-split-71
  prefs: []
  type: TYPE_NORMAL
- en: At least one of myself and Dean Ball is extremely mistaken about what this bill
    says.
  id: totrans-split-72
  prefs: []
  type: TYPE_NORMAL
- en: The definition of covered model seems to me to be clearly intended to apply
    only to models that are effectively at the frontier of model capabilities.
  id: totrans-split-73
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s look again at the exact definition:'
  id: totrans-split-74
  prefs: []
  type: TYPE_NORMAL
- en: (1) The artificial intelligence model was trained using a quantity of computing
    power greater than 10^26 integer or floating-point operations in 2024, **or a
    model that could reasonably be expected to have similar performance on benchmarks
    commonly used to quantify the performance of state-of-the-art foundation models**,
    as determined by industry best practices and relevant standard setting organizations.
  id: totrans-split-75
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: (2) **The artificial intelligence model has capability below the relevant threshold
    on a specific benchmark but is of otherwise similar general capability.**
  id: totrans-split-76
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'That seems clear as day on what it means, and what it means is this:'
  id: totrans-split-77
  prefs: []
  type: TYPE_NORMAL
- en: If your model is over 10^26 we assume it counts.
  id: totrans-split-78
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: If it isn’t, but it is as good as state-of-the-art current models, it counts.
  id: totrans-split-79
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Being ‘as good as’ is a general capability thing, not hitting specific benchmarks.
  id: totrans-split-80
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Under this definition, if no one was actively gaming benchmarks, at most three
    existing models would plausibly qualify for this definition: GPT-4, Gemini Ultra
    and Claude. I am not even sure about Claude.'
  id: totrans-split-81
  prefs: []
  type: TYPE_NORMAL
- en: If the open source models are gaming the benchmarks so much that they end up
    looking like a handful of them are matching GPT-4 on benchmarks, then what can
    I say, maybe stop gaming the benchmarks?
  id: totrans-split-82
  prefs: []
  type: TYPE_NORMAL
- en: Or point out quite reasonably that the real benchmark is user preference, and
    in those terms, you suck, so it is fine. Either way.
  id: totrans-split-83
  prefs: []
  type: TYPE_NORMAL
- en: But notice that this isn’t what the bill does. The bill applies to large models
    *and* to any models that reach the same performance regardless of the compute
    budget required to make them. This means that the bill applies to startups as
    well as large corporations.
  id: totrans-split-84
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Um, no, because the open model weights models do not remotely reach the performance
    level of OpenAI?
  id: totrans-split-85
  prefs: []
  type: TYPE_NORMAL
- en: Maybe some will in the future.
  id: totrans-split-86
  prefs: []
  type: TYPE_NORMAL
- en: But this very clearly does not ‘ban all open source.’ There are zero existing
    open model weights models that this bans.
  id: totrans-split-87
  prefs: []
  type: TYPE_NORMAL
- en: There are a handful of companies that might plausibly have to worry about this
    in the future, if OpenAI doesn’t release GPT-5 for a while, but we’re talking
    Mistral and Meta, not small start-ups. And we’re talking about them exactly because
    they would be trying to fully play with the big boys in that scenario.
  id: totrans-split-88
  prefs: []
  type: TYPE_NORMAL
- en: Bell is also wrong about the precautionary principle being imposed before training.
  id: totrans-split-89
  prefs: []
  type: TYPE_NORMAL
- en: I do not see any such rule here. What I see is that if you cannot show that
    your model will definitely be safe before training, then you have to wait until
    after the training run to certify that it is safe.
  id: totrans-split-90
  prefs: []
  type: TYPE_NORMAL
- en: In other words, this is an escape clause. Are we seriously objecting to that?
  id: totrans-split-91
  prefs: []
  type: TYPE_NORMAL
- en: Then, if you also can’t certify that it is safe after the training run, then
    we talk precautions. But no one is saying you cannot train, unless I am missing
    something?
  id: totrans-split-92
  prefs: []
  type: TYPE_NORMAL
- en: As usual, people such as Ball are imagining a standard of ‘my product could
    never be used to do harm’ that no one is trying to apply here in any way. That
    is why any model not at the frontier can automatically get a positive safety determination,
    which flies in the face of this theory. Then, if you are at the frontier, you
    have to obey industry standard safety procedures and let California know what
    procedures you are following. Woe is you. And of course, the moment someone else
    has a substantially better model, guess who is now positively safe?
  id: totrans-split-93
  prefs: []
  type: TYPE_NORMAL
- en: 'The ‘covered guidance’ that Ball claims to be alarmed about does not mean ‘do
    everything any safety organization says and if they are contradictory you are
    banned.’ The law does not work that way. Here is what it actually says:'
  id: totrans-split-94
  prefs: []
  type: TYPE_NORMAL
- en: '(e) “Covered guidance” means any of the following:'
  id: totrans-split-95
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: (1) Applicable guidance issued by the National Institute of Standards and Technology
    and by the Frontier Model Division.
  id: totrans-split-96
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: (2) Industry best practices, including relevant safety practices, precautions,
    or testing procedures undertaken by developers of comparable models, and any safety
    standards or best practices commonly or generally recognized by relevant experts
    in academia or the nonprofit sector.
  id: totrans-split-97
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: (3) Applicable safety-enhancing standards set by standards setting organizations.
  id: totrans-split-98
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: So what that means is, we will base our standards off an extension of NIST’s,
    and also we expect you to be liable to implement anything that is considered ‘industry
    best practice’ even if we did not include it in the requirements. But obviously
    it’s not going to be best practices if it is illegal. Then we have the third rule,
    which only counts ‘applicable’ standards. California will review them and decide
    what is applicable, so that is saying they will use outside help.
  id: totrans-split-99
  prefs: []
  type: TYPE_NORMAL
- en: Also, note the term ‘non-derivative’ when talking about all the models. If you
    are a derivative model, then you are fine by default. And almost all models with
    open weights are derivative models, because of course that is the point, distillation
    and refinement rather than starting over all the time.
  id: totrans-split-100
  prefs: []
  type: TYPE_NORMAL
- en: 'So here’s what the law would actually do, as far as I can tell:'
  id: totrans-split-101
  prefs: []
  type: TYPE_NORMAL
- en: If your model is not projected to be state of the art level and it is not over
    the 10^26 limit no one has hit yet and no one except the big three are anywhere
    near, this law has only trivial impact upon you, it is a trivial amount of paperwork.
    Every other business in America and especially the state of California is jealous.
  id: totrans-split-102
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: If your model is a derivative of an existing model, you’re fine, that’s it.
  id: totrans-split-103
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: If your model you want to train is projected to be state of the art, but you
    can show it is safe before you even train it, good job, you’re golden.
  id: totrans-split-104
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: If your model is projected to be state of the art, and can’t show it is safe
    before training it, you can still train it as long as you don’t release it and
    you make sure it isn’t stolen or released by others. Then if you show it is safe
    or show it is not state of the art, you’re golden again.
  id: totrans-split-105
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: If your model is state of the art, and you train it and still don’t know if
    it is ‘safe,’ and by safe we do not mean ‘no one ever does anything wrong’ we
    mean things more like ‘no one ever causes 500 million dollars in damages or mass
    casualties,’ then you have to implement a series of safety protocols (regulatory
    requirements) to be determined by California, and you have to tell them what you
    are doing to ensure safety.
  id: totrans-split-106
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: You have to have to have abilities like ‘shut down AIs running on computers
    under my control’ and ‘plausibly prevent unauthorized people from accessing the
    model if they are not supposed to.’ Which does not even apply to copies of the
    program you no longer control. Is that is going to be a problem?
  id: totrans-split-107
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: You also have to report any ‘safety incidents’ that happen.
  id: totrans-split-108
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Also some ‘pro-innovation’ stuff of unknown size and importance.
  id: totrans-split-109
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Not only does SB 1047 not attempt to ‘strangle AI,’ not only does it not attempt
    regulatory capture or target startups, it would do essentially nothing to anyone
    but a handful of companies unless they have active safety incidents. If there
    are active safety incidents, then we get to know about them, which could introduce
    liability concerns or publicity concerns, and that seems like the main downside?
    That people might learn about your failures and existing laws might sometimes
    apply?
  id: totrans-split-110
  prefs: []
  type: TYPE_NORMAL
- en: The arguments against such rules often come from the implicit assumption that
    we enforce our laws as written, reliably and without discretion. Which we don’t.
    What would happen if, as Eliezer recently joked, the law actually worked the way
    critics of such regulations claim that it does? If every law was strictly enforced
    as written, with no common sense used, as they warn will happen? And someone our
    courts could handle the case loads involved? Everyone would be in jail within
    the week.
  id: totrans-split-111
  prefs: []
  type: TYPE_NORMAL
- en: When people see proposals for treating AI slightly more like anything else,
    and subjecting it to remarkably ordinary regulation, with an explicit and deliberate
    effort to only target frontier models that are exclusively fully closed, and they
    say that this ‘bans open source’ what are they talking about?
  id: totrans-split-112
  prefs: []
  type: TYPE_NORMAL
- en: They are saying that Open Model Weights Are Unsafe and Nothing Can Fix This,
    and we want to do things that are patently and obviously unsafe, so asking any
    form of ‘is this safe?’ and having an issue with the answer being ‘no’ is a ban
    on open model weights. Or, alternatively, they are saying that their business
    model and distribution plans are utterly incompatible with complying with any
    rules whatsoever, so we should never pass any, or they should be exempt from any
    rules.
  id: totrans-split-113
  prefs: []
  type: TYPE_NORMAL
- en: The idea that this would “spell the end of America’s leadership in AI” is laughable.
    If you think America’s technology industry cannot stand a whiff of regulation,
    I mean, do they know anything about America or California? And have they seen
    the other guy? Have they seen American innovation across the board, almost entirely
    in places with rules orders of magnitude more stringent? This here is so insanely
    nothing.
  id: totrans-split-114
  prefs: []
  type: TYPE_NORMAL
- en: But then, when did such critics let that stop them? It’s the same rhetoric every
    time, no matter what. And some people seem willing to amplify such voices, without
    asking whether their words make sense.
  id: totrans-split-115
  prefs: []
  type: TYPE_NORMAL
- en: What would happen [if there was actually a wolf](https://www.storyarts.org/library/aesops/stories/boy.html)?
  id: totrans-split-116
  prefs: []
  type: TYPE_NORMAL
