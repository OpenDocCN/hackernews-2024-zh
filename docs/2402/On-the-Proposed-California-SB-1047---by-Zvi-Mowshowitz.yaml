- en: <!--yml
  id: totrans-split-0
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  id: totrans-split-1
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-05-27 14:47:29'
  id: totrans-split-2
  prefs: []
  type: TYPE_NORMAL
- en: -->
  id: totrans-split-3
  prefs: []
  type: TYPE_NORMAL
- en: On the Proposed California SB 1047 - by Zvi Mowshowitz
  id: totrans-split-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://thezvi.substack.com/p/on-the-proposed-california-sb-1047](https://thezvi.substack.com/p/on-the-proposed-california-sb-1047)
  id: totrans-split-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[California Senator Scott Wiener](https://twitter.com/Scott_Wiener/status/1755650108287578585)
    of San Francisco introduces [SB 1047](https://t.co/JWaOLP44Iu)  [to regulate AI](https://www.washingtonpost.com/technology/2024/02/08/california-legislation-artificial-intelligence-regulation/).
    I have put up a market on [how likely it is to become law](https://manifold.markets/ZviMowshowitz/will-california-bill-sb-1047-become).'
  id: totrans-split-6
  prefs: []
  type: TYPE_NORMAL
- en: “If Congress at some point is able to pass a strong pro-innovation, pro-safety
    AI law, I’ll be the first to cheer that, but I’m not holding my breath,” Wiener
    said in an interview. “We need to get ahead of this so we maintain public trust
    in AI.”
  id: totrans-split-7
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Congress is certainly highly dysfunctional. I am still generally against California
    trying to act like it is the federal government, even when the cause is good,
    but I understand.
  id: totrans-split-8
  prefs: []
  type: TYPE_NORMAL
- en: Can California effectively impose its will here?
  id: totrans-split-9
  prefs: []
  type: TYPE_NORMAL
- en: On the biggest players, for now, presumably yes.
  id: totrans-split-10
  prefs: []
  type: TYPE_NORMAL
- en: In the longer run, when things get actively dangerous, then my presumption is
    no.
  id: totrans-split-11
  prefs: []
  type: TYPE_NORMAL
- en: There is a potential trap here. If we put our rules in a place where someone
    with enough upside can ignore them, and we never then pass anything in Congress.
  id: totrans-split-12
  prefs: []
  type: TYPE_NORMAL
- en: So what does it do, according to the bill’s author?
  id: totrans-split-13
  prefs: []
  type: TYPE_NORMAL
- en: 'California Senator Scott Wiener: SB 1047 does a few things:'
  id: totrans-split-14
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Establishes clear, predictable, common-sense safety standards for developers
    of the largest and most powerful AI systems. These standards apply only to the
    largest models, not startups.
  id: totrans-split-15
  prefs:
  - PREF_BQ
  - PREF_OL
  type: TYPE_NORMAL
- en: Establish CalCompute, a public AI cloud compute cluster. CalCompute will be
    a resource for researchers, startups, & community groups to fuel innovation in
    CA, bring diverse perspectives to bear on AI development, & secure our continued
    dominance in AI.
  id: totrans-split-16
  prefs:
  - PREF_BQ
  - PREF_OL
  type: TYPE_NORMAL
- en: prevent price discrimination & anticompetitive behavior
  id: totrans-split-17
  prefs:
  - PREF_BQ
  - PREF_OL
  type: TYPE_NORMAL
- en: institute know-your-customer requirements
  id: totrans-split-18
  prefs:
  - PREF_BQ
  - PREF_OL
  type: TYPE_NORMAL
- en: protect whistleblowers at large AI companies
  id: totrans-split-19
  prefs:
  - PREF_BQ
  - PREF_OL
  type: TYPE_NORMAL
- en: '@geoffreyhinton called SB 1047 “a very sensible approach” to balancing these
    needs. Leaders representing a broad swathe of the AI community have expressed
    support.'
  id: totrans-split-20
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: People are rightfully concerned that the immense power of AI models could present
    serious risks. For these models to succeed the way we need them to, users must
    trust that AI models are safe and aligned w/ core values. Fulfilling basic safety
    duties is a good place to start.
  id: totrans-split-21
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: With AI, we have the opportunity to apply the hard lessons learned over the
    past two decades. Allowing social media to grow unchecked without first understanding
    the risks has had disastrous consequences, and we should take reasonable precautions
    this time around.
  id: totrans-split-22
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: As usual, RTFC (Read the Card, or here [the bill](https://leginfo.legislature.ca.gov/faces/billNavClient.xhtml?bill_id=202320240SB1047))
    applies.
  id: totrans-split-23
  prefs: []
  type: TYPE_NORMAL
- en: Section 1 names the bill.
  id: totrans-split-24
  prefs: []
  type: TYPE_NORMAL
  zh: 第 1 节命名了该法案。
- en: Section 2 says California is winning in AI ([see this song](https://www.youtube.com/watch?v=BATf_eUcb8M&ab_channel=ThePresidentsoftheUnitedStatesofAmerica)),
    AI has great potential but could do harm. A missed opportunity to mention existential
    risks.
  id: totrans-split-25
  prefs: []
  type: TYPE_NORMAL
  zh: 第 2 节说加利福尼亚州在人工智能方面处于领先地位（[请参见这首歌](https://www.youtube.com/watch?v=BATf_eUcb8M&ab_channel=ThePresidentsoftheUnitedStatesofAmerica)），人工智能有很大的潜力，但可能会造成伤害。错失了提及存在风险的机会。
- en: Section 3 22602 offers definitions. I have some notes.
  id: totrans-split-26
  prefs: []
  type: TYPE_NORMAL
  zh: 第 3 节 22602 提供了定义。我有一些笔记。
- en: Usual concerns with the broad definition of AI.
  id: totrans-split-27
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对AI广泛定义的通常关注点。
- en: Odd that ‘a model autonomously engaging in a sustained sequence of unsafe behavior’
    only counts as an ‘AI safety incident’ if it is not ‘at the request of a user.’
    If a user requests that, aren’t you supposed to ensure the model doesn’t do it?
    Sounds to me like a safety incident.
  id: totrans-split-28
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 奇怪的是，“一个模型在未经用户请求的情况下自主进行持续序列的不安全行为”只有在“用户的请求之外”才算作“AI安全事件”。如果用户请求了，难道不应该确保模型不会这样做吗？听起来对我来说像是一个安全事件。
- en: 'Covered model is defined primarily via compute, not sure why this isn’t a ‘foundation’
    model, I like the secondary extension clause: “The artificial intelligence model
    was trained using a quantity of computing power greater than 10^26 integer or
    floating-point operations in 2024, or a model that could reasonably be expected
    to have similar performance on benchmarks commonly used to quantify the performance
    of state-of-the-art foundation models, as determined by industry best practices
    and relevant standard setting organizations OR The artificial intelligence model
    has capability below the relevant threshold on a specific benchmark but is of
    otherwise similar general capability..”'
  id: totrans-split-29
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 覆盖的模型主要通过计算来定义，不确定为什么这不是一个“基础”模型，我喜欢次要扩展条款：“2024 年使用超过 10^26 个整数或浮点操作进行训练的人工智能模型，或者在常用于量化最先进基础模型性能的基准测试中合理预期具有类似性能的模型，如行业最佳实践和相关标准制定组织所确定的
    OR 人工智能模型在特定基准测试上具有相关阈值以下的能力，但在其他方面具有类似的一般能力。”
- en: Critical harm is either mass casualties or 500 million in damage, or comparable.
  id: totrans-split-30
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 严重伤害要么是大规模伤亡，要么是 5 亿美元的损害，或者可比的情况。
- en: Full shutdown means full shutdown but only within your possession and control.
    So when we really need a full shutdown, this definition won’t work. The whole
    point of a shutdown is that it happens everywhere whether you control it or not.
  id: totrans-split-31
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 完全关闭意味着完全关闭，但仅在您的控制和控制范围内。所以当我们真正需要完全关闭时，这个定义就行不通了。关闭的整个意义在于，无论您是否控制它，它都会发生。
- en: Open-source artificial intelligence model is defined to only include models
    that ‘may be freely modified and redistributed’ so that raises the question of
    whether that is legal or practical. Such definitions need to be practical, if
    I can do it illegally but can clearly still do it, that needs to count.
  id: totrans-split-32
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 开源人工智能模型的定义仅包括“可能被自由修改和重新分发”的模型，因此引发了这样一个问题，即这是否合法或实际可行。这样的定义需要实际可行，如果我可以非法做到但显然仍然可以做到，那就需要计入。
- en: 'Definition (s): [“Positive safety determination” means a determination, pursuant
    to subdivision (a) or (c) of Section 22603, with respect to a covered model that
    is not a derivative model that a developer can reasonably exclude the possibility
    that a covered model has a hazardous capability or may come close to possessing
    a hazardous capability when accounting for a reasonable margin for safety and
    the possibility of posttraining modifications.]'
  id: totrans-split-33
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义（s）：“积极的安全判定”意味着根据第 22603 条的分项（a）或（c），针对一个非派生模型的覆盖模型的确定，开发者可以合理地排除覆盖模型具有危险能力或在考虑到合理安全保障和后期培训修改的可能性时可能接近拥有危险能力的可能性。
- en: Very happy to see the mention of post-training modifications, which is later
    noted to include access to tools and data, so scaffolding explicitly counts.
  id: totrans-split-34
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 非常高兴看到后期修改的提及，稍后还指出包括访问工具和数据，因此支架明确计入。
- en: Section 3 22603 (a) says that before you train a new non-derivative model, you
    need to determine whether you can make a positive safety determination.
  id: totrans-split-35
  prefs: []
  type: TYPE_NORMAL
  zh: 第 3 节 22603 (a) 规定，在训练新的非派生模型之前，您需要确定是否可以做出积极的安全判定。
- en: I like that this happens before you start training. But of course, this raises
    the question of how you know how it will score on the benchmarks?
  id: totrans-split-36
  prefs: []
  type: TYPE_NORMAL
  zh: 我喜欢这在您开始训练之前发生。但是，这当然引发了一个问题，即您如何知道它将在基准测试中得分如何？
- en: One thing I worry about is the concept that if you score below another model
    on various benchmarks, that this counts as a positive safety determination. There
    are at least four obvious failure modes for this.
  id: totrans-split-37
  prefs: []
  type: TYPE_NORMAL
  zh: 我担心的一件事是，如果你在各种基准测试中得分低于另一个模型，那么这算作积极的安全认定。至少有四种显而易见的失败模式。
- en: The developer might choose to sabotage performance against the benchmarks, either
    by excluding relevant data and training, or otherwise. Or, alternatively, a previous
    developer might have gamed the benchmarks, which happens all the time, such that
    all you have to do to score lower is to not game those benchmarks yourself.
  id: totrans-split-38
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 开发者可能选择通过排除相关数据和训练或其他方式来破坏基准测试的性能。或者，之前的开发者可能已经操纵了基准测试，这种情况经常发生，因此你只需不自己操纵这些基准测试就能得分更低。
- en: The model might have situational awareness, and choose to get a lower score.
    This could be various degrees of intentional on the part of the developers.
  id: totrans-split-39
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 模型可能具有情境感知，并选择获得较低的分数。这可能是开发者有意为之的各种程度。
- en: The model might not adhere to your predictions or scaling laws. So perhaps you
    say it will score lower on benchmarks, but who is to say you are right?
  id: totrans-split-40
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 模型可能不会遵循你的预测或缩放定律。所以也许你说它在基准测试上得分较低，但谁能说你是对的？
- en: The benchmarks might simply not be good at measuring what we care about.
  id: totrans-split-41
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这些基准可能根本不能很好地衡量我们关心的内容。
- en: Similarly, it is good to make a safety determination before beginning training,
    but also if the model is worth training then you likely cannot actually know its
    safety in advance, especially since this is not only existential safety.
  id: totrans-split-42
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，最好在开始训练之前做出安全认定，但如果模型值得训练，那么你很可能实际上无法事先知道它的安全性，尤其是因为这不仅涉及生存安全。
- en: 'Section 3 22603 (b) covers what you must do if you cannot make the positive
    safety determination. Here are the main provisions:'
  id: totrans-split-43
  prefs: []
  type: TYPE_NORMAL
  zh: 第3 22603（b）节涵盖了如果你无法作出积极的安全认定时你必须做的事情。以下是主要规定：
- en: You must prevent unauthorized access.
  id: totrans-split-44
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你必须防止未经授权的访问。
- en: You must be capable of a full shutdown.
  id: totrans-split-45
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你必须能够进行全面关闭。
- en: You must implement all covered guidance. Okie dokie.
  id: totrans-split-46
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你必须实施所有涵盖的指导。好啦，好啦。
- en: You must implement a written and separate safety and security protocol, that
    provides ‘reasonable assurance’ that it would ensure the model will have safeguards
    that prevent critical harms. This has to include clear tests that verify if you
    have succeeded.
  id: totrans-split-47
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你必须制定书面和单独的安全和安全协议，提供‘合理保证’，确保模型将具有防止关键危害的保障。这必须包括明确的测试，以验证你是否成功。
- en: You must say how you are going to do all that, how you would change how you
    are doing it, and what would trigger a shutdown.
  id: totrans-split-48
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你必须说明你打算如何做到这一切，你会如何改变你的做法，并且是什么将触发关闭。
- en: Provide a copy of your protocol and keep it updated.
  id: totrans-split-49
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 提供你的协议的副本并保持其更新。
- en: You can then make a ‘positive safety determination’ after training and testing,
    subject to the safety protocol.
  id: totrans-split-50
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练和测试后，你可以做出‘积极的安全认定’，但必须遵守安全协议。
- en: Section (d) says that if your model is ‘not subject to a positive safety determination,’
    in order to deploy it (you can still deploy it at all?!) you need to implement
    ‘reasonable safeguards and requirements’ that allow you prevent harms and to trace
    any harms that happen. I worry this section is not taking such scenarios seriously.
    To not be subject to such determination, the model needs to be breaking new ground
    in capabilities, and you were unable to assure that it wouldn’t be dangerous.
    So what are these ‘reasonable safeguards and requirements’ that would make deploying
    it acceptable? Perhaps I am misunderstanding here.
  id: totrans-split-51
  prefs: []
  type: TYPE_NORMAL
  zh: 第（d）节规定，如果你的模型‘不受积极安全认定的约束’，为了部署它（你究竟能否部署它？！），你需要实施‘合理的保障和要求’，使你能够防止危害并追溯任何发生的危害。我担心这一部分没有认真对待这样的情景。为了不受此类认定的约束，模型需要在能力上开辟新的领域，并且你无法保证它不会危险。那么，什么是那些‘合理的保障和要求’，使得部署它是可以接受的？也许我在这里有误解。
- en: Section (g) says safety incidents must be reported.
  id: totrans-split-52
  prefs: []
  type: TYPE_NORMAL
  zh: 第（g）节规定必须报告安全事故。
- en: Section (h) says if your positive safety determination is unreasonable it does
    not count, and that to be reasonable you need to consider any risk that has already
    been identified elsewhere.
  id: totrans-split-53
  prefs: []
  type: TYPE_NORMAL
  zh: 第（h）节说，如果你的积极安全认定是不合理的，那么它就不算数，并且为了合理，你需要考虑已在其他地方识别的任何风险。
- en: Overall, this seems like a good start, but I worry it has loopholes, and I worry
    that it is not thinking about the future scenarios where the models are potentially
    existentially dangerous, or might exhibit unanticipated capabilities or situational
    awareness and so on. There is still the DC-style ‘anticipate and check specific
    harm’ approach throughout.
  id: totrans-split-54
  prefs: []
  type: TYPE_NORMAL
  zh: 总体来看，这似乎是一个很好的开端，但我担心它存在漏洞，并且我担心它没有考虑到模型可能存在潜在的存在性危险，或可能表现出意外的能力或情境意识等。整个法案仍然采用了华盛顿特区风格的‘预期和检查特定危害’方法。
- en: Section 22604 is about KYC, a large computing cluster has to collect the information
    and check to see if customers are trying to train a covered model.
  id: totrans-split-55
  prefs: []
  type: TYPE_NORMAL
  zh: Section 22604 是关于 KYC 的，大型计算集群必须收集信息并检查客户是否试图训练一个涵盖模型。
- en: Section 22605 requires sellers of inference or a computing cluster to provide
    a transparent, uniform, publicly available price schedule, banning price discrimination,
    and bans ‘unlawful discrimination or noncompetitive activity in determining price
    or access.’
  id: totrans-split-56
  prefs: []
  type: TYPE_NORMAL
  zh: Section 22605 要求推理或计算集群的销售者提供透明、统一、公开的价格计划，禁止价格歧视，并禁止在确定价格或获取途径上进行‘非法歧视或非竞争活动’。
- en: I always wonder about laws that say ‘you cannot do things that are already illegal,’
    I mean I thought that was the whole point of them already being illegal.
  id: totrans-split-57
  prefs: []
  type: TYPE_NORMAL
  zh: 我总是想知道那些说‘你不能做已经非法的事情’的法律，我以为这正是它们已经非法的全部意义所在。
- en: I am not sure to what extent this rule has an impact in practice, and whether
    it effectively means that anyone selling such services has to be a kind of common
    carrier unable to pick who gets its limited services, and unable to make deals
    of any kind. I see the appeal, but also I see clear economic downsides to forcing
    this.
  id: totrans-split-58
  prefs: []
  type: TYPE_NORMAL
  zh: 我不确定这条规则在实践中有多大影响力，以及它是否有效地意味着任何销售这类服务的人都必须成为一种无法挑选谁获得其有限服务的普通运营者，并且不能进行任何形式的交易。我理解其吸引力，但同时也看到迫使此举可能带来的明显经济下降。
- en: Section 22606 covers penalties. The fines are relatively limited in scope, the
    main relief is injunction against and possible deletion of the model. I worry
    in practice that there is not enough teeth here.
  id: totrans-split-59
  prefs: []
  type: TYPE_NORMAL
  zh: Section 22606 涵盖了处罚事项。罚款的范围相对有限，主要的救济是禁令以及可能删除模型。我担心实际操作中这里的执行力度不够。
- en: Section 2207 is whistleblower protections. Odd that this is necessary, one would
    think there would be such protections universally by now? There are no unexpectedly
    strong provisions here, only the normal stuff.
  id: totrans-split-60
  prefs: []
  type: TYPE_NORMAL
  zh: Section 2207 是举报者保护条款。奇怪的是，这是必要的，人们可能会认为现在普遍应该有这种保护措施？这里没有意外强烈的条款，只有正常的内容。
- en: Section 4 11547.6 tasks the new Frontier Model Division with its official business,
    including collecting reports and issuing guidance.
  id: totrans-split-61
  prefs: []
  type: TYPE_NORMAL
  zh: Section 4 11547.6 赋予新的 Frontier Model Division 其官方业务，包括收集报告和发布指导意见。
- en: Section 5 11547.7 is for the CalCompute public cloud computing cluster. This
    seems like a terrible idea, there is no reason for public involvement here, also
    there is no stated or allocated budget. Assuming it is small, it does not much
    matter.
  id: totrans-split-62
  prefs: []
  type: TYPE_NORMAL
  zh: Section 5 11547.7 是关于 CalCompute 公共云计算集群的。这看起来像是一个糟糕的想法，这里没有公众参与的理由，而且也没有说明或分配预算。假设它很小，那就不太重要了。
- en: Sections 6-9 are standard boilerplate disclaimers and rules.
  id: totrans-split-63
  prefs: []
  type: TYPE_NORMAL
  zh: Sections 6-9 是标准的免责声明和规则。
- en: What should we think about all that?
  id: totrans-split-64
  prefs: []
  type: TYPE_NORMAL
  zh: 我们应该如何看待所有这些？
- en: It seems like a good faith effort to put forward a helpful bill. It has a lot
    of good ideas in it. I believe it would be net helpful. In particular, it is structured
    such that if your model is not near the frontier, your burden here is very small.
  id: totrans-split-65
  prefs: []
  type: TYPE_NORMAL
  zh: 它看起来像是一个诚意满满的努力，提出一个有帮助的法案。它里面有很多好主意。我相信这将是有益的。特别是，它的结构使得如果你的模型不接近前沿，那么你在这里的负担将会很小。
- en: My worry is that this has potential loopholes in various places, and does not
    yet strongly address the nature of the future more existential threats. If you
    want to ignore this law, you probably can.
  id: totrans-split-66
  prefs: []
  type: TYPE_NORMAL
  zh: 我担心这在各个地方都有潜在的漏洞，并且还没有强力解决未来更多存在性威胁的性质。如果你想无视这个法律，你很可能可以做到。
- en: But it seems like a good beginning, especially on dealing with relatively mundane
    but still potentially catastrophic threats, without imposing an undo burden on
    developers. This could then be built upon.
  id: totrans-split-67
  prefs: []
  type: TYPE_NORMAL
  zh: 但这似乎是一个好的开始，特别是在处理相对乏味但仍然潜在灾难性威胁方面，而不会给开发者造成不必要的负担。这可以进一步发展。
- en: Ah, Tyler Cowen has a link on this and it’s… [California’s Effort to Strange
    AI](https://hyperdimensional.substack.com/p/californias-effort-to-strangle-ai?utm_source=post-email-title&publication_id=2244049&post_id=141516001&utm_campaign=email-post-title&isFreemail=false&r=3j06n&utm_medium=email).
  id: totrans-split-68
  prefs: []
  type: TYPE_NORMAL
- en: Because of course it is. We do this every time. People keep saying ‘this law
    will ban satire’ or spreadsheets or pictures of cute puppies or whatever, based
    on what on its best day would be a maximalist anti-realist reading of the proposal,
    if it were enacted straight with no changes and everyone actually enforced it
    to the letter.
  id: totrans-split-69
  prefs: []
  type: TYPE_NORMAL
- en: 'Dean Ball: This week, California’s legislature introduced [SB 1047: The Safe
    and Secure Innovation for Frontier Artificial Intelligence Systems Act](https://leginfo.legislature.ca.gov/faces/billTextClient.xhtml?bill_id=202320240SB1047).
    The bill, introduced by State Senator Scott Wiener (liked by many, myself included,
    for his pro-housing stance), would create a sweeping regulatory regime for AI,
    apply the precautionary principle to all AI development, and effectively outlaw
    all new open source AI models—possibly throughout the United States.'
  id: totrans-split-70
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: This is a line pulled out whenever anyone proposes that AI be governed by any
    regulatory regime whatsoever even with zero teeth of any kind. When someone says
    that someone, somewhere might be legally required to write an email.
  id: totrans-split-71
  prefs: []
  type: TYPE_NORMAL
- en: At least one of myself and Dean Ball is extremely mistaken about what this bill
    says.
  id: totrans-split-72
  prefs: []
  type: TYPE_NORMAL
- en: The definition of covered model seems to me to be clearly intended to apply
    only to models that are effectively at the frontier of model capabilities.
  id: totrans-split-73
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s look again at the exact definition:'
  id: totrans-split-74
  prefs: []
  type: TYPE_NORMAL
- en: (1) The artificial intelligence model was trained using a quantity of computing
    power greater than 10^26 integer or floating-point operations in 2024, **or a
    model that could reasonably be expected to have similar performance on benchmarks
    commonly used to quantify the performance of state-of-the-art foundation models**,
    as determined by industry best practices and relevant standard setting organizations.
  id: totrans-split-75
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: (2) **The artificial intelligence model has capability below the relevant threshold
    on a specific benchmark but is of otherwise similar general capability.**
  id: totrans-split-76
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'That seems clear as day on what it means, and what it means is this:'
  id: totrans-split-77
  prefs: []
  type: TYPE_NORMAL
- en: If your model is over 10^26 we assume it counts.
  id: totrans-split-78
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: If it isn’t, but it is as good as state-of-the-art current models, it counts.
  id: totrans-split-79
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Being ‘as good as’ is a general capability thing, not hitting specific benchmarks.
  id: totrans-split-80
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Under this definition, if no one was actively gaming benchmarks, at most three
    existing models would plausibly qualify for this definition: GPT-4, Gemini Ultra
    and Claude. I am not even sure about Claude.'
  id: totrans-split-81
  prefs: []
  type: TYPE_NORMAL
- en: If the open source models are gaming the benchmarks so much that they end up
    looking like a handful of them are matching GPT-4 on benchmarks, then what can
    I say, maybe stop gaming the benchmarks?
  id: totrans-split-82
  prefs: []
  type: TYPE_NORMAL
- en: Or point out quite reasonably that the real benchmark is user preference, and
    in those terms, you suck, so it is fine. Either way.
  id: totrans-split-83
  prefs: []
  type: TYPE_NORMAL
  zh: 或者相当合理地指出真正的基准是用户偏好，从这些方面来看，你的产品不尽如人意，所以没问题。不管怎样。
- en: But notice that this isn’t what the bill does. The bill applies to large models
    *and* to any models that reach the same performance regardless of the compute
    budget required to make them. This means that the bill applies to startups as
    well as large corporations.
  id: totrans-split-84
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 但请注意，这并不是法案的内容。该法案适用于大型模型*以及*无论需要多少计算预算来达到相同性能的模型。这意味着该法案适用于初创企业和大公司。
- en: Um, no, because the open model weights models do not remotely reach the performance
    level of OpenAI?
  id: totrans-split-85
  prefs: []
  type: TYPE_NORMAL
  zh: 呃，不，因为开放模型权重模型远远达不到OpenAI的性能水平？
- en: Maybe some will in the future.
  id: totrans-split-86
  prefs: []
  type: TYPE_NORMAL
  zh: 或许将来会有一些人这么做。
- en: But this very clearly does not ‘ban all open source.’ There are zero existing
    open model weights models that this bans.
  id: totrans-split-87
  prefs: []
  type: TYPE_NORMAL
  zh: 但这很明显并不是“禁止所有开源”。目前没有任何开放模型权重模型受此禁止。
- en: There are a handful of companies that might plausibly have to worry about this
    in the future, if OpenAI doesn’t release GPT-5 for a while, but we’re talking
    Mistral and Meta, not small start-ups. And we’re talking about them exactly because
    they would be trying to fully play with the big boys in that scenario.
  id: totrans-split-88
  prefs: []
  type: TYPE_NORMAL
  zh: 未来可能有少数几家公司可能会对此感到担忧，如果OpenAI不发布GPT-5一段时间的话，但我们在谈论的是Mistral和Meta，而不是小型初创公司。我们之所以谈论它们，正是因为它们将在这种情况下完全与大公司竞争。
- en: Bell is also wrong about the precautionary principle being imposed before training.
  id: totrans-split-89
  prefs: []
  type: TYPE_NORMAL
  zh: Bell也错误地认为在训练之前会实施预防原则。
- en: I do not see any such rule here. What I see is that if you cannot show that
    your model will definitely be safe before training, then you have to wait until
    after the training run to certify that it is safe.
  id: totrans-split-90
  prefs: []
  type: TYPE_NORMAL
  zh: 我在这里没有看到任何这样的规定。我看到的是，如果您不能在训练之前显示您的模型绝对安全，那么您必须等到训练运行之后才能证明它是安全的。
- en: In other words, this is an escape clause. Are we seriously objecting to that?
  id: totrans-split-91
  prefs: []
  type: TYPE_NORMAL
  zh: 换句话说，这是一个逃避条款。我们真的反对这个吗？
- en: Then, if you also can’t certify that it is safe after the training run, then
    we talk precautions. But no one is saying you cannot train, unless I am missing
    something?
  id: totrans-split-92
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，如果您在训练后还不能证明其安全性，那么我们谈谈预防措施。但没有人说您不能进行训练，除非我漏掉了什么？
- en: As usual, people such as Ball are imagining a standard of ‘my product could
    never be used to do harm’ that no one is trying to apply here in any way. That
    is why any model not at the frontier can automatically get a positive safety determination,
    which flies in the face of this theory. Then, if you are at the frontier, you
    have to obey industry standard safety procedures and let California know what
    procedures you are following. Woe is you. And of course, the moment someone else
    has a substantially better model, guess who is now positively safe?
  id: totrans-split-93
  prefs: []
  type: TYPE_NORMAL
  zh: 和往常一样，像Ball这样的人想象一个“我的产品绝不会被用来造成伤害”的标准，这在任何方面都不适用。这就是为什么任何未达到前沿水平的模型都可以自动获得正面的安全评估，这与这一理论完全相悖。然后，如果你处于前沿水平，你必须遵守行业标准的安全程序，并告知加利福尼亚你正在遵循哪些程序。真是糟糕。当然，一旦有其他人拥有大幅度更好的模型，猜猜现在谁又会被认为是绝对安全？
- en: 'The ‘covered guidance’ that Ball claims to be alarmed about does not mean ‘do
    everything any safety organization says and if they are contradictory you are
    banned.’ The law does not work that way. Here is what it actually says:'
  id: totrans-split-94
  prefs: []
  type: TYPE_NORMAL
  zh: Ball声称引起警惕的“覆盖指南”并不意味着“执行所有安全组织说的一切，如果它们互相矛盾，那么你就被禁止。”法律并非如此运作。这是它实际上的内容：
- en: '(e) “Covered guidance” means any of the following:'
  id: totrans-split-95
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: (e) “覆盖指南”指以下任何内容：
- en: (1) Applicable guidance issued by the National Institute of Standards and Technology
    and by the Frontier Model Division.
  id: totrans-split-96
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: (1) 由国家标准与技术研究院和前沿模型部门发布的适用指南。
- en: (2) Industry best practices, including relevant safety practices, precautions,
    or testing procedures undertaken by developers of comparable models, and any safety
    standards or best practices commonly or generally recognized by relevant experts
    in academia or the nonprofit sector.
  id: totrans-split-97
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: (2) 行业最佳实践，包括开发类似模型的开发者采取的相关安全实践、预防措施或测试程序，以及学术界或非营利部门相关专家普遍认可的任何安全标准或最佳实践。
- en: (3) Applicable safety-enhancing standards set by standards setting organizations.
  id: totrans-split-98
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: (3) 由标准制定组织设定的适用于增强安全性的标准。
- en: So what that means is, we will base our standards off an extension of NIST’s,
    and also we expect you to be liable to implement anything that is considered ‘industry
    best practice’ even if we did not include it in the requirements. But obviously
    it’s not going to be best practices if it is illegal. Then we have the third rule,
    which only counts ‘applicable’ standards. California will review them and decide
    what is applicable, so that is saying they will use outside help.
  id: totrans-split-99
  prefs: []
  type: TYPE_NORMAL
  zh: 那么这意味着，我们将以NIST的扩展为基础制定我们的标准，而且我们也期望您有责任实施任何被认为是‘行业最佳实践’的东西，即使我们没有将其包括在要求中。但显然，如果违法的话，那就不会是最佳实践。接着我们有第三条规则，只计算‘适用’标准。加利福尼亚州将对它们进行审查，并决定什么是适用的，这就意味着他们将寻求外部帮助。
- en: Also, note the term ‘non-derivative’ when talking about all the models. If you
    are a derivative model, then you are fine by default. And almost all models with
    open weights are derivative models, because of course that is the point, distillation
    and refinement rather than starting over all the time.
  id: totrans-split-100
  prefs: []
  type: TYPE_NORMAL
  zh: 还要注意在谈论所有模型时的‘非衍生’术语。如果您的模型是衍生模型，那么您默认就没有问题。而且几乎所有带有开放权重的模型都是衍生模型，因为这当然是关键，蒸馏和精炼，而不是一直从头开始。
- en: 'So here’s what the law would actually do, as far as I can tell:'
  id: totrans-split-101
  prefs: []
  type: TYPE_NORMAL
  zh: 那么根据我所知，这个法律实际上会做到以下几点：
- en: If your model is not projected to be state of the art level and it is not over
    the 10^26 limit no one has hit yet and no one except the big three are anywhere
    near, this law has only trivial impact upon you, it is a trivial amount of paperwork.
    Every other business in America and especially the state of California is jealous.
  id: totrans-split-102
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果您的模型没有预计到达最先进的水平，而且也没有超过迄今为止尚未达到的10^26限制，并且除了大三家之外，没有其他任何人处于接近状态，这个法律对您的影响微乎其微，仅仅是一些琐碎的文书工作。美国的其他每一个企业，特别是加利福尼亚州，都会嫉妒。
- en: If your model is a derivative of an existing model, you’re fine, that’s it.
  id: totrans-split-103
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果您的模型是现有模型的衍生物，那么您就没问题，就这样。
- en: If your model you want to train is projected to be state of the art, but you
    can show it is safe before you even train it, good job, you’re golden.
  id: totrans-split-104
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果您想要训练的模型预计到达最先进的水平，但您可以在训练之前就证明它是安全的，那么干得好，您又是金光闪闪了。
- en: If your model is projected to be state of the art, and can’t show it is safe
    before training it, you can still train it as long as you don’t release it and
    you make sure it isn’t stolen or released by others. Then if you show it is safe
    or show it is not state of the art, you’re golden again.
  id: totrans-split-105
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果您的模型预计到达最先进的水平，并且在训练之前不能证明它是安全的，您仍然可以训练它，只要不发布它，并确保它不被他人窃取或发布。然后，如果您证明它是安全的或者证明它不是最先进的，那么您又是金光闪闪了。
- en: If your model is state of the art, and you train it and still don’t know if
    it is ‘safe,’ and by safe we do not mean ‘no one ever does anything wrong’ we
    mean things more like ‘no one ever causes 500 million dollars in damages or mass
    casualties,’ then you have to implement a series of safety protocols (regulatory
    requirements) to be determined by California, and you have to tell them what you
    are doing to ensure safety.
  id: totrans-split-106
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果您的模型是最先进的，并且您训练了它但仍不知道它是否‘安全’，这里的安全不是指‘从未发生过任何错误’，而是更像‘从未造成5亿美元的损失或大规模伤亡’，那么您必须实施一系列由加利福尼亚州确定的安全协议（监管要求），并告知他们您正在采取什么措施确保安全。
- en: You have to have to have abilities like ‘shut down AIs running on computers
    under my control’ and ‘plausibly prevent unauthorized people from accessing the
    model if they are not supposed to.’ Which does not even apply to copies of the
    program you no longer control. Is that is going to be a problem?
  id: totrans-split-107
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 您必须具备‘关闭我控制下运行的计算机上的AI’和‘合理防止未经授权的人员访问模型’等能力，即使这不适用于您不再控制的程序副本。这会成为问题吗？
- en: You also have to report any ‘safety incidents’ that happen.
  id: totrans-split-108
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 您还必须报告发生的任何‘安全事故’。
- en: Also some ‘pro-innovation’ stuff of unknown size and importance.
  id: totrans-split-109
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 还有一些未知大小和重要性的‘支持创新’的东西。
- en: Not only does SB 1047 not attempt to ‘strangle AI,’ not only does it not attempt
    regulatory capture or target startups, it would do essentially nothing to anyone
    but a handful of companies unless they have active safety incidents. If there
    are active safety incidents, then we get to know about them, which could introduce
    liability concerns or publicity concerns, and that seems like the main downside?
    That people might learn about your failures and existing laws might sometimes
    apply?
  id: totrans-split-110
  prefs: []
  type: TYPE_NORMAL
  zh: SB 1047不仅没有试图‘扼杀AI’，也没有试图实施监管困境或针对初创企业，除非它们有活跃的安全事故。如果存在活跃的安全事故，那么我们会了解到这些事故，这可能会引发责任或公开担忧，这似乎是主要的缺点？人们可能会了解到你的失败，现行法律有时可能会适用？
- en: The arguments against such rules often come from the implicit assumption that
    we enforce our laws as written, reliably and without discretion. Which we don’t.
    What would happen if, as Eliezer recently joked, the law actually worked the way
    critics of such regulations claim that it does? If every law was strictly enforced
    as written, with no common sense used, as they warn will happen? And someone our
    courts could handle the case loads involved? Everyone would be in jail within
    the week.
  id: totrans-split-111
  prefs: []
  type: TYPE_NORMAL
  zh: 反对这些规则的论点往往源于这样一个假设：我们按照书面法律可靠地、无差错地实施我们的法律。但我们并没有。如果，正如埃利泽最近开玩笑说的那样，法律实际上按照批评这些法规的人所宣称的方式运作，会发生什么？如果每一条法律都严格按照书面法规执行，不使用常识，正如他们警告的那样会发生的？法院能处理涉及的案件负担吗？每个人都会在一周内被关进监狱。
- en: When people see proposals for treating AI slightly more like anything else,
    and subjecting it to remarkably ordinary regulation, with an explicit and deliberate
    effort to only target frontier models that are exclusively fully closed, and they
    say that this ‘bans open source’ what are they talking about?
  id: totrans-split-112
  prefs: []
  type: TYPE_NORMAL
  zh: 当人们看到将AI稍微像其他任何东西一样对待，并对其施加相当普通的规定，有明确而有意识的努力，仅针对全面封闭的前沿模型，他们说这‘禁止开源’是什么意思？
- en: They are saying that Open Model Weights Are Unsafe and Nothing Can Fix This,
    and we want to do things that are patently and obviously unsafe, so asking any
    form of ‘is this safe?’ and having an issue with the answer being ‘no’ is a ban
    on open model weights. Or, alternatively, they are saying that their business
    model and distribution plans are utterly incompatible with complying with any
    rules whatsoever, so we should never pass any, or they should be exempt from any
    rules.
  id: totrans-split-113
  prefs: []
  type: TYPE_NORMAL
  zh: 他们说开放模型权重是不安全的，没有任何方法可以修复这一点，我们希望做的事情显然是不安全的，所以问任何形式的‘这安全吗？’并且对答案‘不’有异议就是对开放模型权重的禁令。或者，换句话说，他们说他们的商业模式和分发计划与任何规则完全不兼容，因此我们不应该通过任何规则，或者他们应该免于遵守任何规则。
- en: The idea that this would “spell the end of America’s leadership in AI” is laughable.
    If you think America’s technology industry cannot stand a whiff of regulation,
    I mean, do they know anything about America or California? And have they seen
    the other guy? Have they seen American innovation across the board, almost entirely
    in places with rules orders of magnitude more stringent? This here is so insanely
    nothing.
  id: totrans-split-114
  prefs: []
  type: TYPE_NORMAL
  zh: 认为这将‘标志着美国在AI领导地位的终结’是可笑的。如果你认为美国的技术产业不能承受一点监管，我是说，他们了解美国或加利福尼亚吗？他们见过另一位吗？他们见过在几乎完全适用规则次序下的各行各业的美国创新吗？这简直是荒谬的。
- en: But then, when did such critics let that stop them? It’s the same rhetoric every
    time, no matter what. And some people seem willing to amplify such voices, without
    asking whether their words make sense.
  id: totrans-split-115
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，那些批评者何时让这些成为阻碍呢？无论什么情况下，他们的论调总是一样的。有些人似乎愿意放大这些声音，而不去问他们的话是否讲得通。
- en: What would happen [if there was actually a wolf](https://www.storyarts.org/library/aesops/stories/boy.html)?
  id: totrans-split-116
  prefs: []
  type: TYPE_NORMAL
  zh: 如果真的有只狼出现会发生什么？
