- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 'category: 未分类'
- en: 'date: 2024-05-27 14:46:14'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024年05月27日 14:46:14
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: A look at the Mojo language for bioinformatics
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 了解Mojo语言在生物信息学中的应用
- en: 来源：[https://viralinstruction.com/posts/mojo/](https://viralinstruction.com/posts/mojo/)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://viralinstruction.com/posts/mojo/](https://viralinstruction.com/posts/mojo/)
- en: ''
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '*Written 2024-02-09*'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '*写于2024年02月09日*'
- en: A few days ago, [a blog post](https://www.modular.com/blog/outperforming-rust-benchmarks-with-mojo)
    was posted on the website of [Modular](https://www.modular.com/blog/outperforming-rust-benchmarks-with-mojo),
    the company behind the new high-performance programming language [Mojo](https://www.modular.com/max/mojo).
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 几天前，[Modular](https://www.modular.com/blog/outperforming-rust-benchmarks-with-mojo)的网站上发布了[一篇博客文章](https://www.modular.com/blog/outperforming-rust-benchmarks-with-mojo)，该公司是新高性能编程语言[Mojo](https://www.modular.com/max/mojo)背后的公司。
- en: The post made the case for using Mojo in bioinformatics due to Mojo's dual features
    of being high-level language with high performance, and the blog author substantiated
    the case by presenting two benchmarks related to the processing of [FASTQ files](https://en.wikipedia.org/wiki/FASTQ_format),
    showing impressive speed.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 该帖子为在生物信息学中使用Mojo提供了支持，因为Mojo具有高性能的双重特性，博客作者通过提供与[FASTQ文件](https://en.wikipedia.org/wiki/FASTQ_format)处理相关的两个基准来证实这一案例，显示了令人印象深刻的速度。
- en: As a bioinformatician who is obsessed with high-performance, high-level programming,
    that's right in my wheelhouse! I decided to dig deeper into the benchmark, and
    this post is about what I found out.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 作为一个痴迷于高性能、高级编程的生物信息学家，这正是我的专长所在！我决定深入研究这个基准，这篇文章就是我发现的内容。
- en: ''
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The basic premise of the post is that the field of bioinformatics is struggling
    to handle its increasingly large datasets. These datasets are so large that they
    have to be processed programmatically, but programming is the field is split between
    high-level dynamic languages used to do the actual data analysis, and the high-performance,
    static languages that Python calls into to do the computation underlying the analysis.
    As the post states:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 帖子的基本前提是，生物信息学领域正在努力处理日益庞大的数据集。这些数据集非常庞大，以至于必须通过编程方式进行处理，但是该领域的编程分为两个层次：用于实际数据分析的高级动态语言，以及Python调用的高性能、静态语言，用于进行分析底层计算的语言。如帖子所述：
- en: This creates a two-world problem where bioinformaticians who are not skilled
    in low-level languages, are prohibited from understanding, customizing, and implementing
    low-level operations.
  id: totrans-13
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 这造成了一个双重世界的问题，生物信息学家不熟悉低级语言，无法理解、定制和实现低级操作。
- en: The blog post goes on to suggest Mojo could bridge the gap between the two worlds,
    by being a "Pythonic", but fast language.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 博客文章继续暗示Mojo可以通过成为一种“Pythonic”，但快速的语言来弥合这两个世界之间的差距。
- en: I have a lot more to say on the topic of the two-language problem in bioinformatics,
    so much that I'll reserve it for another blog post. In summary, I wholeheartedly
    agree with that analysis, except that I'd encourage using Julia rather than Mojo.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 我对生物信息学中的双语问题有更多的看法，太多了，我会把它保留到另一篇博客文章中。总之，我完全同意那个分析，除了我会鼓励使用Julia而不是Mojo。
- en: The post then describes how the author implemented a benchmark in Mojo and managed
    to beat a fairly optimised Rust library. That certainly made me curious, so I
    cloned [the git repo with the Mojo code](https://github.com/MoSafi2/MojoFastTrim)^([[1]](#fndef:1))
    and took a look myself.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 帖子随后描述了作者如何在Mojo中实施了一个基准，并成功击败了一个相当优化的Rust库。这确实让我感到好奇，所以我克隆了[带有Mojo代码的git存储库](https://github.com/MoSafi2/MojoFastTrim)^([[1]](#fndef:1))并自己看了一眼。
- en: ''
  id: totrans-17
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'After cloning the repo, the first step is to download and install Mojo:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 克隆存储库后，第一步是下载并安装Mojo：
- en: '[PRE0]'
  id: totrans-19
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Well, that's disappointing. Apparently, Mojo currently only runs on Ubuntu and
    MacOS, and I run neither. So, I can't *run* any Mojo code, but must rely on just
    *reading* the code. Fortunately, the code is quite simple, and only a few hundred
    lines of code.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 嗯，这令人失望。显然，Mojo目前只能在Ubuntu和MacOS上运行，而我都没有。所以，我不能*运行*任何Mojo代码，而必须仅依靠*阅读*代码。幸运的是，代码非常简单，只有几百行代码。
- en: ''
  id: totrans-21
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'At first glance, it''s clear that *Mojo isn''t anything like Python*. Here
    are some things that I found in the few hundred lines of the supposedly Pythonic
    language:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 乍一看，很明显*Mojo与Python完全不同*。以下是我在几百行所谓的Pythonic语言中发现的一些事物：
- en: 'Generic functions parameterized by type parameters: `fn foo[x: T](arg: Int)`'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '通过类型参数参数化的通用函数：`fn foo[x: T](arg: Int)`'
- en: 'Speaking of which, two distinct function definitions: `def foo` vs `fn foo`'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 说到这一点，有两个不同的函数定义：`def foo` vs `fn foo`
- en: '...and different kinds of integers, here `Int` as opposed to `int`'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '...还有不同类型的整数，这里是`Int`而不是`int`'
- en: 'Mutable vs immutable variables, initialized by `var x: T = y` vs `let x: T
    = y`'
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '可变vs不可变变量，通过`var x: T = y` vs `let x: T = y`进行初始化'
- en: 'Also, type declarations before assignment, C-style: `let foo: T`'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '此外，类型声明在赋值之前，C风格：`let foo: T`'
- en: '*Mandatory* type declarations in type signatures: `fn foo(x: Int)`'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '在类型签名中的*强制*类型声明：`fn foo(x: Int)`'
- en: 'Mutability declaration of arguments via the `inout` keyword: `fn foo(inout
    self)`'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过`inout`关键字声明参数的可变性：`fn foo(inout self)`
- en: The ability of a function to raise errors must be marked with the `raises` keyword
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 函数引发错误的能力必须使用`raises`关键字标记
- en: Data can be stored in `struct`s as well as `class`es.
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据可以存储在`struct`中，也可以存储在`class`中。
- en: Compiler directives, notably `@always_inline` to control inlining heuristics
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 编译器指令，特别是`@always_inline`来控制内联启发式算法
- en: Mojo apparently even implements a [Rust-style borrow checker](https://docs.modular.com/mojo/programming-manual.html),
    though I couldn't tell from just reading the code.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: Mojo显然甚至实现了一种[Rust风格的借用检查器](https://docs.modular.com/mojo/programming-manual.html)，尽管我只是从代码中阅读，并无法得知。
- en: Does this strike you as the features of a high-level, dynamic language?
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 这让你觉得这是一个高级动态语言的特征吗？
- en: On one hand, the presence of low-level features is reassuring. When Mojo was
    initially announced, I didn't quite understand what Mojo was supposed to be. Was
    it supposed to be a faster Python implementation, like PyPy? Or perhaps a compiler
    to optimise selected parts of Python, like Numba? That left a lot of questions
    with me about how they were going to pull that off given that neither PyPy nor
    Numba can reliably produce fast code.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 一方面，低级特性的存在是令人放心的。当Mojo最初宣布时，我并不完全理解Mojo应该是什么。它是应该成为一个更快的Python实现，就像PyPy吗？还是一个编译器，优化Python的部分选定部分，就像Numba？这让我对他们如何完成这项工作感到困惑，因为PyPy和Numba都不能可靠地生成快速的代码。
- en: Well, it looks like it's neither - instead, it's a *different, static language*
    that presumably aims to provide excellent interoperation with Python. That's a
    *much* more doable proposal! There is no reason to doubt that a static language
    can reliably generate fast code. And it could still provide great value for Pythonistas
    by essentially being a better version of Cython that they can selectively reach
    for when they have a need for speed. Especially so if Mojo can provide a kind
    of gradual performance where users from a Python background can gradually and
    selectively opt into each of these features as they get more familiar with lower
    level computing.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 好吧，看起来它既不是 - 相反，它是一个*不同的、静态的语言*，据推测旨在与Python提供卓越的互操作性。这是一个*更可行的提议*！没有理由怀疑静态语言能够可靠地生成快速代码。而且它仍然可以为Python爱好者提供巨大的价值，因为本质上它是一个更好的Cython版本，当他们需要速度时可以有选择地使用。特别是如果Mojo可以提供一种逐渐性能提升的方式，用户可以从Python背景逐渐和有选择地选择这些特性，随着他们对低级计算的了解越来越多。
- en: On the other hand, it's also a much less exciting vision to provide a fast static
    language with good interop, compared to smashing [Ousterhout's dichotomy](https://en.wikipedia.org/wiki/Ousterhout%27s_dichotomy)
    by providing a dynamic language that is also fast. It does make me question the
    use case somewhat. After all, static languages can already interoperate with Python
    relatively easily, e.g. with Rust's crate PyO3\. Presumably, Mojo's interop is
    going to be even easier. But is the improved interop going to outweigh the benefits
    that come from designing a language to be ergonomic on its own terms?
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，与通过提供一个同时又快速又具有良好互操作性的静态语言来打破[奥斯特豪特的二分法](https://en.wikipedia.org/wiki/Ousterhout%27s_dichotomy)相比，提供一个令人兴奋程度较低的愿景也许并不那么令人兴奋。这让我对使用情况产生了一些质疑。毕竟，静态语言已经可以相对容易地与Python进行互操作，例如使用Rust的crate
    PyO3。据推测，Mojo的互操作性将会更加容易。但是，改进的互操作性是否会超过设计一种符合自身特点的语言所带来的好处呢？
- en: On that point, I don't really buy the idea that Mojo benefits terribly much
    from being "Pythonic" - which presumably means that its syntax is inspired by
    Python. What's the claim here, really? That it'd be *too hard* for people to learn
    the superficial syntax of a new language, while it'd simultaneously be *easy*
    for people to learn about function monomorphization, copy- vs borrow semantics,
    compiler directives and much more?
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一点上，我并不认同Mojo从“Pythonic”中获益很多的想法-这可能意味着它的语法受到Python的启发。这里真的有什么要求吗？真的有人认为学习一个新语言的表面语法会*太难*，而同时又认为学习函数单态化、复制与借用语义、编译器指令等会*很容易*吗？
- en: The main intended application of Mojo appears to be deep learning, which has
    struggled with the same 'two-language problem' as bioinformatics, since models
    are prototyped in Python but all the tensor operations are written in C++ or CUDA.
    It's not clear to me how Mojo is going to change the game there, though. It doesn't
    seem like Mojo can replace a framework like PyTorch, since those are at entirely
    different levels of the stack. Can it integrate into PyTorch, such that tensor
    gradients are preserved across Mojo functions? That would allow users to keep
    using PyTorch while implementing a single custom kernel in Mojo. But it's seems
    unlikely Mojo is compatible with PyTorch's C++ interface. Perhaps Mojo is aimed
    at being a language suitable for developing new, future frameworks from scratch
    when people are ready to ditch the existing Python ecosystem? But if that's the
    goal, you might as well ditch Python entirely and all its 35-year old baggage
    and come to Julia for a clean start.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: Mojo的主要应用似乎是深度学习，与生物信息学一样，深度学习一直受到同样的“双语言问题”的困扰，因为模型在Python中进行原型设计，但所有张量操作都是用C++或CUDA编写的。我不太清楚Mojo如何改变游戏规则。不过，Mojo似乎不能取代PyTorch这样的框架，因为它们在完全不同的堆栈层次上。它能够集成到PyTorch中吗，以便在Mojo函数之间保留张量梯度？这将允许用户继续使用PyTorch，同时在Mojo中实现单个自定义内核。但Mojo似乎不太可能与PyTorch的C++接口兼容。也许Mojo旨在成为一种适合从头开发新的未来框架的语言，当人们准备放弃现有的Python生态系统时？但如果这是目标，你可能也可以完全放弃Python及其35年的历史包袱，并转而使用Julia来进行清洁的开始。
- en: Let me also say some nice things about Mojo.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 让我也说一些关于Mojo的好话。
- en: First, judging by the language features I listed above it should be clear that
    Mojo is extremely serious about performance. This is not some PyPy-like attempt
    to speed up vanilla Python *somewhat*, this is an attempt to make a language that
    is *actually fast*. Second, Mojo's built-in SIMD capabilities are enviable. It
    might make a big difference if developers are pushed towards writing SIMD-friendly
    code by default. Also, keep in mind I probably just don't understand the intended
    use case of Mojo. I haven't paid *that* close attention to how Mojo is intended
    to be used, and I probably won't, until I can get my hands on Mojo and run it
    on my own computer.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，从我上面列出的语言特性来判断，应该清楚Mojo非常重视性能。这不是像PyPy那样试图稍微加速原始Python的尝试，这是试图创建一种*实际上快速*的语言。其次，Mojo内置的SIMD功能令人羡慕。如果开发人员默认采用编写SIMD友好代码，可能会产生很大的影响。此外，请记住我可能并不理解Mojo的预期用例。我对Mojo的预期使用方式并没有特别关注，直到我能够亲自在我的计算机上运行Mojo。
- en: ''
  id: totrans-42
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Before we get back to the benchmark, we need to understand *why* the work done
    in the benchmark is meaningful. Well, we don't *need to*, but I find it interesting
    because it's my field of research, so let's take a detour into DNA sequencing.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 在回到基准之前，我们需要了解*为什么*在基准测试中所做的工作是有意义的。嗯，我们不*需要*，但我觉得这很有趣，因为这是我的研究领域，因此让我们偏离一下，了解DNA测序。
- en: Most people know that biological inheritance is controlled by DNA^([[2]](#fndef:2)).
    DNA is a linear polymer molecule of consisting of *nucleotides* stringed together
    in a chain, with each nucleotide containing one of four distinct *bases* which
    are abbreviated A, C, G or T. A DNA molecule can therefore be faithfully represented
    by a sequence of symbols, e.g. a string such as `TAGGCTATGCC`. Thus, DNA is a
    type of *digital* storage that controls much of how living organisms are built
    and how we behave. Reading the sequence of a physical sample containing DNA molecules
    is called *sequencing*, and is done by machines called *sequencers*.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数人都知道生物遗传是由DNA来控制的^([[2]](#fndef:2))。DNA是一个由线性聚合物分子组成的链状分子，每个核苷酸链中包含着4个不同的*碱基*，这些碱基简称为A、C、G和T。因此，DNA分子可以被一个符号序列所表示，比如一个字符串，例如`TAGGCTATGCC`。因此，DNA是一种控制生物体大部分构建方式和行为的*数字*存储方式。读取包含DNA分子的物理样本的序列被称为*测序*，这是由叫做*测序仪*的机器来完成的。
- en: 'Incidentally, the applicability of the field of bioinformatics stem from these
    facts: 1\. That much of molecular biology can be explained by the polymer molecule
    DNA (and RNA and protein), 2\. That these polymers are easily and faithfully represented
    in a computer, and 3\. That it''s possible to construct sequencers which can computerize
    massive amounts of these polymers cheaply. Biochemistry on Earth didn''t *have*
    to be this amenable to analysis, and we''re very lucky that it happened to be
    so.'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 顺便提一下，生物信息学的应用能从这些事实中得以解释：1. 分子生物学的很多内容可以通过聚合物分子DNA（还有RNA和蛋白质）来解释，2. 这些聚合物可以在计算机上容易而准确地表示，3.
    可以构建能够廉价计算大量这些聚合物的测序器。地球上的生物化学不一定*要*适合分析，我们很幸运，它恰好适合分析。
- en: There are different competing sequencers with different characteristics, but
    let's focus on the machines produced by the company Illumina, which currently
    dominate with around 80% market share. Illumina sequencers uses a chemical reaction
    to read DNA linearly from one end of the molecule. The output of reading one molecule
    of DNA is termed a *read*. Due to imperfections in the chemistry, the chemical
    reaction deteriorates to unreadability after around 150 bases, putting an upper
    limit on read length that is far too low to sequence full DNA molecules, which
    in humans are on the order of 100 million bases (100 Mbp) in length. To overcome
    this limitation, the DNA is broken apart to smaller fragments of around 500 bp,
    e.g. using ultrasound, and tens of millions of these fragments are then sequenced
    in parallel. Because we expect the sample to contain many near-identical DNA molecules
    that are fragmented independently and randomly, we can reconstruct the entire
    original sequence by merging partially overlapping reads, if only we sequence
    sufficiently many reads from each sample to ensure uniform coverage of the original
    sequence.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 有不同的竞争测序器，具有不同的特征，但让我们着重关注目前市场份额占据了80%的Illumina公司生产的机器。Illumina测序仪使用化学反应从分子的一端线性读取DNA。读取一分子DNA的输出被称为*读取*。由于化学反应的不完美，在约150个碱基后，化学反应会变得难以读取，这就给读取长度设定了一个上限，这个长度远远不足以测序全长约1亿碱基（100
    Mbp）的DNA分子，而在人类中，DNA分子的长度大约为1亿碱基。为了克服这个局限，DNA被分解为大约500 bp的较小片段，例如使用超声波，然后以并行方式对数千万个这些片段进行测序。因为我们期望样品中含有许多近似相同的DNA分子，这些分子是独立和随机地断裂的，我们可以通过合并部分重叠的读取来重构整个原始序列，只要我们对每个样本测序足够多的读数，以确保对原始序列的均匀覆盖。
- en: The number of reads is typically expressed in *depth of coverage* (or just *depth*),
    which is the average number of times each position in the original DNA molecule
    is present across all sequenced reads. A typical experiment might target ~2 %
    of the human genome's total size of 3 Gbp and aim for a depth of 100x, producing
    around 5 Gbp of data. With a read length of 150 bp, this is around 35 million
    reads.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，读取的数量以*覆盖深度*（或者只是*深度*）来表示，这是原始DNA分子的每个位置在所有测序读取中的平均存在次数。一个典型的实验可能定位于人类基因组总长度的约2%，目标是100x的深度，产生约5
    Gbp的数据。以150 bp的读取长度，这大约是3500万条读取。
- en: 'Sequencers typically output the reads in the FASTQ format, which is a simple
    ASCII-encoded format^([[3]](#fndef:3)). One read in FASTQ format looks like this:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 测序器通常以FASTQ格式输出读取，这是一种简单的ASCII编码格式^([[3]](#fndef:3))。FASTQ格式的一个读取看起来像这样：
- en: '[PRE1]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'That is:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 即：
- en: A read is always composed of four lines.
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一条读取始终由四行组成。
- en: The top line starts with `@` and contain a unique identifier of the read. It
    has no other restrictions. In the example read above, the name encodes a bunch
    of metadata about where the read originated.
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 顶行以`@`开头，包含了读取的唯一标识符。它没有其他限制。在上面的例子中，标识符编码了关于读取来源的大量元数据。
- en: The next line contain the DNA sequence.
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 接下来的行包含了DNA序列。
- en: The third line starts with a `+` and then may optionally repeat the same string
    as after the `@` on the first line
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第三行以`+`开头，随后可能可选地重复第一行中`@`后面的相同字符串
- en: 'The fourth header line contains the quality. This line must be the same length
    as the DNA sequence. It gives the estimated probability that the given DNA nucleotide
    is wrong. There are different encoding schemes, but by far the most common is
    Phred+33, where the error probability is:'
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第四行标题包含质量信息。这一行必须和DNA序列的长度相同。它给出了给定的DNA核苷酸是错误的估计概率。有不同的编码方案，但最常见的是Phred+33，其中错误概率为：
- en: <math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>p</mi><mo>=</mo><mn>1</mn><msup><mn>0</mn><mfrac><mrow><mn>33</mn><mo>−</mo><mi>c</mi></mrow><mn>10</mn></mfrac></msup></mrow><annotation
    encoding="application/x-tex">p = 10^\frac{33 - c}{10}</annotation></semantics></math>p=101033−c​
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: <math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>p</mi><mo>=</mo><mn>1</mn><msup><mn>0</mn><mfrac><mrow><mn>33</mn><mo>−</mo><mi>c</mi></mrow><mn>10</mn></mfrac></msup></mrow><annotation
    encoding="application/x-tex">p = 10^\frac{33 - c}{10}</annotation></semantics></math>p=101033−c​
- en: Where <math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>c</mi></mrow><annotation
    encoding="application/x-tex">c</annotation></semantics></math>c is the ASCII value
    of the symbol in the quality line.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 其中<math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>c</mi></mrow><annotation
    encoding="application/x-tex">c</annotation></semantics></math>c是质量行中符号的ASCII值。
- en: A FASTQ file is then simply the concatenation of multiple reads like the one
    above. Since a research project may contain terabytes of FASTQ files, having a
    fast parser is important.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: FASTQ文件就是简单地连接上面所示的多个读取。由于研究项目可能包含TB级别的FASTQ文件，因此具有快速解析器非常重要。
- en: ''
  id: totrans-59
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The Mojo blog post implements two benchmarks - I will only focus on one of
    them here. That''s the FASTQ parsing benchmark, which is taken from one of [the
    biofast benchmarks](https://github.com/lh3/biofast). The task is simple: Given
    a 1.4 GB FASTQ file with ~5.5M reads, count the number of reads, number of bases,
    and number of quality characters, by using a parser to loop over the individual
    reads in the file.'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: Mojo的博客文章实现了两个基准测试-在这里我只关注其中一个。那就是FASTQ解析基准测试，它取自[生物快速基准测试](https://github.com/lh3/biofast)之一。任务很简单：给定一个包含约550万个读取的1.4GB
    FASTQ文件，通过使用解析器循环遍历文件中的各个读取，统计读取数、基数和质量字符数。
- en: Currently, the Needletail parser, written in Rust, tops the benchmark. On my
    four year old laptop, it rips through the file in 458 ms, about 3.05 GB/s. In
    comparison, my own `FASTX.jl` parser written in Julia is under half the speed,
    taking 986 ms (1.42 GB/s). I'll get back to discrepancy later.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 目前，用Rust编写的Needletail解析器在基准测试中名列前茅。在我四年前的笔记本上，它以458毫秒的速度快速解析文件，约为3.05 GB/s。相比之下，我自己使用Julia编写的`FASTX.jl`解析器速度只有它的一半，耗时986毫秒（1.42
    GB/s）。我稍后再来谈谈这个差异。
- en: ''
  id: totrans-62
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Since I can't time the Mojo implementation myself, I'll use the numbers from
    their git repo. It lists Needletail taking 0.27s versus 0.21s for Mojo on a more
    modern, faster machine than my own. If we assume Mojo ran with the same relative
    speed versus Rust on my machine, it'd clock in at 356 ms (3.92 GB/s).
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我无法自己测试Mojo的实现，我将使用它们git库中的数字。它列出Needletail从更现代、更快的机器上获取的0.27秒的速度，而Mojo获得了0.21秒。如果假设Mojo在我的机器上与Rust有相同的相对速度，它的速度将为356毫秒（3.92
    GB/s）。
- en: Nearly four GB/s is crazy fast. How does it do it? Let's dive into the Mojo
    code.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 几乎四GB/s的速度真是太快了。它是如何做到的呢？让我们来深入研究Mojo的代码。
- en: 'The `main()` function is defined as:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '`main()`函数的定义如下：'
- en: '[PRE2]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Where nearly all the work happens in `FastParser.parse_all`. That is defined
    as
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 几乎所有工作都发生在`FastParser.parse_all`中。它的定义是
- en: '[PRE3]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: The `fill_buffer` function seeks to the right location in the underlying file,
    then fills the internal buffer of `FastParser`. Either that or `self.check_EOF`
    can raise a (non-specific) `Error` on EOF, which breaks the loop in `parse_all`.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '`fill_buffer`函数寻找底层文件的正确位置，然后填充`FastParser`的内部缓冲区。`fill_buffer`或者`self.check_EOF`在EOF时可能会引发（非特定的）`Error`，这会中断`parse_all`的循环。'
- en: I'm not crazy about the mandatory seeking of `fill_buffer`. This happens if
    there are extra unused bytes in the buffer. Instead of copying them to the beginning
    of the buffer, the reader rewinds the underlying stream and simply re-reads the
    bytes from the stream - but what if the parser wraps a non-seekable stream? In
    any case that's not important - it could probably be solved with almost no performance
    cost.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 我对`fill_buffer`的强制寻找并不满意。如果缓冲区中有额外的未使用字节，就会发生这种情况。解决方案应该是将它们复制到缓冲区的开头，而不是回绕底层流，然后从流中重新读取字节
    - 但如果解析器包装了一个不可寻址的流会怎么样？在任何情况下，这都不重要-几乎不会带来性能成本。
- en: 'The function `parse_chunk` parses all the reads in the current buffer. Its
    definition is:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 函数`parse_chunk`解析当前缓冲区中的所有读取。它的定义是：
- en: '[PRE4]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Here, the `.tally(read)` method increments the read number and number of bases
    seen. The cost of that is insignificant. The exception is expected to be raised
    when the buffer reaches the end, such that the remaining part of the buffer only
    contains a partial read.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，`.tally(read)`方法递增读取编号和已查看的基数。这个成本是微不足道的。可以预料到，当缓冲区达到末尾时，会引发异常，使得缓冲区的剩余部分只包含部分读取。
- en: Also note the lack of any kind of error handling here. No matter why `parse_chunk`
    throws, it's caught in `parse_all` and terminates the reading without propagating
    the error or examining what kind of error it is. The same issue is repeated further
    down the call chain.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 还要注意这里完全缺少任何形式的错误处理。无论`parse_chunk`出现问题的原因是什么，`parse_all`都会捕捉到并终止读取，而不会传播错误或检查错误的类型。这个问题在调用链的更深处也是如此。
- en: 'Most work here happens in `parse_read` where the real parsing happens:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 大部分工作都发生在`parse_read`中，真正的解析就在这里发生：
- en: '[PRE5]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: That's the secret sauce, really. Each read is parsed by scanning four times
    to the next newline, then emitting the positions of the newlines with zero validation
    of any kind.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是秘密武器。每个读取都通过扫描四次到下一个换行符进行解析，然后发出换行符的位置，而非进行任何验证。
- en: 'I''m sorry but this is not a serious parser. To be fair, the repo is pretty
    clear that:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 抱歉，这不是一个严肃的解析器。开山式，该代码库非常明确地指出：
- en: 'Disclaimer: MojoFastTrim🔥 is for demonstration purposes only and shouldn''t
    be used as part of bioinformatic pipelines'
  id: totrans-79
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 免责声明：MojoFastTrim🔥仅用于演示目的，不应作为生物信息学管道的一部分使用。
- en: However, in my opinion, this lack of validation (really, lack of *parsing* in
    any meaningful sense) means that the performance between this parser and Needletail
    is incomparable. So what exactly does it demonstrate? You get to claim your implementation
    is faster than someone else if you do the same task in less time, but not if you
    skip half the job.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，在我看来，这种缺乏验证（实际上，缺乏有意义的 *解析*）意味着这个解析器和 Needletail 之间的性能是无法比较的。那到底展示了什么？如果你在更短的时间内完成了相同的任务，你就可以声称你的实现比别人更快，但如果你跳过了一半的工作，就不行。
- en: 'Anyway, `get_next_line_index` is kind of neat. First, it statically checks
    if SIMD is enabled. If so, it calls `find_chr_next_occurance_simd`, which is essentially
    an implementation of [`memchr`](https://man7.org/linux/man-pages/man3/memchr.3.html).
    It''s implemented as:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 无论如何，`get_next_line_index` 还是挺不错的。首先，它静态地检查 SIMD 是否启用。如果是的话，它就调用 `find_chr_next_occurance_simd`，这实质上是
    [`memchr`](https://man7.org/linux/man-pages/man3/memchr.3.html) 的实现。它的实现如下：
- en: '[PRE6]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Here we see Mojo's nice SIMD abstractions. First it uses `math.align_down` to
    get the last index from which it's safe to load a SIMD vector. The `simd_width`
    is automatically computed as `simdwidthof[DType.int8]()` and is presumably a compile
    time constant, so I assume its value is constant folded. I think that's pretty
    cool.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 这里我们看到了 Mojo 不错的 SIMD 抽象。首先它使用 `math.align_down` 获取最后一个索引，从这个索引开始加载 SIMD 向量是安全的。`simd_width`
    自动计算为 `simdwidthof[DType.int8]()`，应该是一个编译时常量，所以我假设它的值是被折叠的。我觉得这很酷。
- en: In the first loop, each vector (`mask`) is then loaded and compared to the byte
    `chr`. If any of the bytes are true, then `arg_true` is called (which loops over
    the vector to find the first true). I'm guessing this loops compiles effectively
    to a `vmovdqu` load instruction, and the reduction can be expressed as `vpcmpeqb`
    (compare vector to byte), `vpmovmskb` (extract upper bits of each byte in vector
    to a 32-bit integer), and then a comparison to zero.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 在第一个循环中，每个向量（`mask`）都被加载并与字节 `chr` 进行比较。如果任何一个字节为 true，则调用 `arg_true`（它循环遍历向量以找到第一个
    true）。我猜这个循环有效地编译成了一个 `vmovdqu` 加载指令，并且约简可以表达为 `vpcmpeqb`（向量与字节比较）、`vpmovmskb`（将向量中的每个字节的高位提取为一个
    32 位整数），然后与零比较。
- en: The function `arg_true` could be compiled to a single `tzcnt` instruction, but
    when I tried to emulate it in Julia I couldn't get the compiler to realise that,
    though that may just be Julia not having the right SIMD abstractions.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 函数 `arg_true` 可以编译成单个 `tzcnt` 指令，但当我尝试在 Julia 中模拟它时，我无法让编译器意识到这一点，尽管这可能只是 Julia
    没有正确的 SIMD 抽象。
- en: Finally, the last elements of the chunk which can't be safely SIMD loaded are
    handled in the last loop. This is much less frequently hit, around once every
    250 reads.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，不能安全 SIMD 加载的块的最后几个元素在最后一个循环中处理。这种情况发生的频率要少得多，大约每 250 次读取一次。
- en: ''
  id: totrans-87
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The Mojo parser does zero validation and will even accept random bytes as input,
    which I think everyone can agree is not acceptable for real-life situations. But
    how much validation *should* a parser do? That's honestly a hard question to answer,
    and the performance you can expect from parsers hinges on the answer to that question.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: Mojo 解析器不进行任何验证，甚至接受随机字节作为输入，我认为这对于真实场景是不可接受的。但解析器应该做多少验证呢？这实在是一个难以回答的问题，解析器的性能取决于这个问题的答案。
- en: 'Let''s return to comparing Needletail and the FASTQ parser I maintain, FASTX.jl.
    Needletail uses a quite similar algorithm to the Mojo parser: It uses a buffered
    reader and memchr''s to find newlines before returning a record containing a view
    directly into the file buffer. However, it also handles `\r\n` newlines, and validates
    that the first and third lines begin with `@` and `+`, respectively, and that
    the seq and qual lines have the same length. That''s certainly more validation
    than the Mojo parser, but is it enough? If the quality line contains pure `\x00`
    bytes, how is this a valid FASTQ file? What if the header is `"@\r\v\r"`?'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们回到比较 Needletail 和我维护的 FASTQ 解析器 FASTX.jl。Needletail 使用了与 Mojo 解析器非常相似的算法：它使用了一个缓冲读取器和
    memchr 来找到换行符，然后返回一个包含直接查看文件缓冲区的记录。然而，它还处理了 `\r\n` 换行符，并验证第一行和第三行分别以 `@` 和 `+`
    开头，并且序列和质量行的长度相同。这肯定比 Mojo 解析器多了一些验证，但是够吗？如果质量行包含纯 `\x00` 字节，这怎么是一个有效的 FASTQ 文件呢？如果标头是
    `"@\r\v\r"` 呢？
- en: My own parser validates more - that the quality line is a printable ASCII character,
    that the sequence are ASCII letters, and that the second header is identical to
    the first. That's achieved through parsing the file with a state machine, which
    can therefore be much stricter. It's also partially the reason it's slower than
    Needletail^([[4]](#fndef:4)).
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 我自己的解析器验证得更多 - 质量行是可打印的 ASCII 字符，序列是 ASCII 字母，并且第二个标头与第一个标头相同。这是通过使用状态机解析文件实现的，因此可以更严格。这也部分是它比
    Needletail 慢的原因^([[4]](#fndef:4))。
- en: On one side of the argument, one could say it's nice to provide as much validation
    as possible - suppose someone reads in a FASTQ file with non-ASCII sequences using
    Needletail, and the parser wrongly claims the seq and quality lines have a different
    number of symbols because they are encoded in a different number of bytes. That
    error is no good and will leave the user scratching their heads when they count
    the sequence and quality lengths and verifies that they match. Wouldn't it be
    nicer to instead have the parser check that the input is ASCII?
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 从某种角度来看，我们可以说尽可能提供尽可能多的验证是好的 - 假设有人使用 Needletail 读取带有非 ASCII 序列的 FASTQ 文件，并且解析器错误地声称序列和质量行的符号数不同，因为它们使用不同数量的字节进行编码。那种错误是不好的，当用户数一数序列和质量的长度并验证它们匹配时，他们会想不明白。相反，让解析器检查输入是否为
    ASCII 不是更好吗？
- en: Also in that favour - when do we ever need to parse files at 3 GB/s? What could
    we possibly *do* to the files that will be anywhere near that speed? Surely dropping
    to 2 or even 1 GB/s will have essentially no impact on the overall speed of a
    real life analysis.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 还有一个方面 - 我们什么时候需要以 3 GB/s 的速度解析文件？我们能对文件做的事情会有多快？降到 2 GB/s 甚至 1 GB/s 对实际分析的整体速度几乎没有影响。
- en: The other side of the argument is that parsers should do as *little* validation
    as possible. For example, my parser spends time checking that the first and third
    headers of FASTQ reads are identical, because the format says so. But what if
    a user has a record where they're not? Does it really help the user to have their
    program crash with otherwise perfectly fine records? After all, a sensible idiom
    of parsing goes "be liberal in what you accept, and conservative in what you send".
    Maybe parsers ought to do as little validation as they can get away with while
    still ensuring they don't give garbage answers.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面的论点是解析器应尽量少进行验证。例如，我的解析器花费时间检查 FASTQ 读取的第一个和第三个头是否相同，因为格式是这样规定的。但如果用户有一条记录，它们不相同怎么办？将程序崩溃对用户真的有帮助吗？毕竟，解析的明智惯例是“在接受方面要宽容，在发送方面要保守”。也许解析器应尽可能少地进行验证，同时确保它们不提供垃圾答案。
- en: One could also say that during the course of a project, the same file might
    be read tens of times, but it really only needs to be validated once. If the validation
    is a separate step from the parsing, it can be skipped all but the first time
    the file is read. In [a Reddit comment](https://www.reddit.com/r/rust/comments/1al8cuc/comment/kpgjkkd/?utm_source=share&utm_medium=web3x&utm_name=web3xcss&utm_term=1&utm_content=share_button)
    the maintainer of Needletail say they have an external tool to validate FASTQ
    files for this reason. That's also reasonable, but it does strike me as un-Rust
    like to opt-in to validation, especially when the cost is so low - after all,
    my parser still does more than 1 GB/s.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 也可以说，在项目进行过程中，同一个文件可能会被读取数十次，但实际上只需要验证一次。如果验证是解析的一个单独步骤，则除了第一次读取文件之外，它可以被跳过。在[一条
    Reddit 评论中](https://www.reddit.com/r/rust/comments/1al8cuc/comment/kpgjkkd/?utm_source=share&utm_medium=web3x&utm_name=web3xcss&utm_term=1&utm_content=share_button)
    ，Needletail 的维护者表示出于这个原因他们有一个外部工具来验证 FASTQ 文件。这也是合理的，但我觉得选择验证的方式有些不像 Rust 风格，尤其是当成本如此之低时
    - 毕竟，我的解析器仍然每秒执行超过 1 GB 的操作。
- en: ''
  id: totrans-95
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: My claim is that Mojo's high speed in this benchmarks comes from the implementation
    and not from Mojo being particularly fast. To back it up, I [ported the implementation
    to Julia](https://github.com/jakobnissen/MojoFQBenchmark) with all the same lack
    of error handling or validation. It's currently 78 lines of code, but to be fair,
    it does only the absolutely minimal necessary to complete the FASTQ benchmark.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 我声称 Mojo 在这个基准测试中的高速度来自于实现，而不是 Mojo 特别快。为了支持这一点，我将实现[移植到了 Julia](https://github.com/jakobnissen/MojoFQBenchmark)，所有的错误处理或验证都是相同的。目前它只有
    78 行代码，但公平地说，它只完成了完成 FASTQ 基准测试所需的绝对最低限度。
- en: It "parses" the file in 200 ms (6.98 GB/s), 78% faster than Mojo's (imputed)
    speed. That's pretty fucking fast. `cat input.fq > /dev/null` takes 122 ms for
    comparison.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 它在 200 ms（6.98 GB/s）内“解析”文件，比 Mojo 的（推测）速度快了 78%。那速度相当快。相比之下，`cat input.fq >
    /dev/null` 需要 122 ms。
- en: 'I think there is only one real conclusion here:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 我认为这里只有一个真正的结论：
- en: 🔥🔥🔥JULIA🔥🔥🔥 IS FASTER THAN MOJO🔥!!!!111
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 🔥🔥🔥JULIA🔥🔥🔥 比 MOJO 🔥🔥 快！！！111
- en: Just kidding. I don't know why my implementation is faster - I don't strictly
    *know* that it's even faster since I can't run Mojo on my own machine. Maybe it's
    the fact that my implementation doesn't seek the underlying file, or maybe 200
    ms is fast enough that Python's startup time begin to matter. If I include the
    time for Julia to start up and compile the script, my implementation takes 354
    ms total, on the same level as Mojo's.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 开玩笑的。我不知道为什么我的实现更快 - 我甚至不能确定它是否更快，因为我无法在自己的机器上运行 Mojo。也许是因为我的实现不会查找底层文件，或者也许是因为
    200 ms 足够快，以至于 Python 的启动时间开始变得重要。如果将 Julia 启动和编译脚本的时间计算在内，我的实现总共需要 354 ms，与 Mojo
    的水平相当。
- en: One interesting observation is that replacing the manual `memchr` implementation
    with a call to glibc's `memchr` slows it down by about 25%, despite glibc's `memchr`
    being around 70% faster when used on long haystacks. Julia's ccall has close to
    zero overhead, so I'm not sure what's up with that.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 一个有趣的观察是，用 glibc 的 `memchr` 替换手动实现的 `memchr` 实现会使速度减慢约 25%，尽管在长的字符集中使用时，glibc
    的 `memchr` 速度要快约 70%。Julia 的 ccall 几乎没有额外开销，所以我不确定具体原因。
- en: Maybe it's that `memchr` doesn't inline, whereas the manual implementation is
    forcefully inlined into `parse_read`. If so, this might explain most of the performance
    difference to Needletail. Removing the `@inline` directive from my Julia code
    slows it down about 20%. Interestingly, setting `lto = "thin"` and `codegen-units
    = 1` in my Cargo.toml file reduces the runtime of Needletail to 357 ms, matching
    Mojo's imputed runtime nearly exactly.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 可能是因为 `memchr` 没有进行内联，而手动实现被强制内联到 `parse_read` 中。如果是这样，这可能解释了与 Needletail 之间的性能差异大部分。从我的
    Julia 代码中移除 `@inline` 指令会使速度减慢约 20%。有趣的是，在我的 Cargo.toml 文件中设置 `lto = "thin"` 和
    `codegen-units = 1` 可以将 Needletail 的运行时间降低到 357 ms，几乎与 Mojo 的推测运行时间完全匹配。
- en: These differences are trivialities. I don't know why my Julia implementation
    is twice as fast as Needletail, but subtracting the lack of validation, I doubt
    it's something substantial. There are often real important reasons why some languages
    are faster than others - whether they provide good zero-cost abstractions for
    high-level data types, whether they provide good multithreading and SIMD support,
    how well they support generics and how well libraries compose together, how defensive
    vs adventurous they make programmers, and much else. I don't think this Mojo implementation
    shows any of this.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 这些差异都是微不足道的。我不知道为什么我的 Julia 实现比 Needletail 快两倍，但减去缺乏验证，我怀疑这并不是什么重要的事情。有时候，某些语言比其他语言更快的原因是非常重要的
    - 例如它们是否为高级数据类型提供了良好的零成本抽象，它们是否提供了良好的多线程和 SIMD 支持，它们是否支持泛型以及它们是否支持库的良好组合，以及它们是否使程序员更加保守还是更加冒险，等等。我不认为这个
    Mojo 实现展示了其中的任何内容。
- en: ''
  id: totrans-104
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'I don''t want to coldly dismiss the Mojo blog post. After all, its two main
    points are essentially right: That bioinformatics needs a language to bridge high-level
    and high-performance programming, and that Mojo is capable of producing fast code.
    I don''t put too much value in the actual reported numbers in the benchmark, but
    they don''t matter in the big picture. It''s also feels a little like overkill
    to go to this length to tear apart a blog post from from a guy who is just excited
    about what Mojo could bring to bioinformatics. It''s just his bad luck that there
    are people like me out there - a bioinformatician who is passionate about high
    performance computing for science, maintain my own FASTQ parsing library, and
    is particularly sceptical about Mojo.'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 我不想冷漠地否定沃霍博客文章。毕竟，它的两个主要观点基本是正确的：生物信息学需要一种语言来连接高级和高性能编程，并且沃霍能够产生快速的代码。我对基准测试中实际报告的数字并不是很看重，但这些数据在大局面上并不重要。批评一位充满激情地讨论沃霍可能给生物信息学带来的未来的人的博客文章似乎有点过分了。命运使我遇到像我这样的人——一个对科学的高性能计算充满激情的生物信息学家，保持自己的FASTQ解析库，并对沃霍持怀疑态度。
- en: Introspecting, I think I'm a little oversensitive to Mojo's marketing hype.
    Ostensibly because the original Mojo announcements (and also this Mojo blog post),
    made a lot of bold claims that could be construed as hyperbolic, while keeping
    the compiler to themselves, giving it the smell of vaporware. But if I'm being
    honest with myself, it's probably because I'm so invested in the prospect of Julia
    for bioinformatics.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 反省自己，我觉得我对沃霍的营销炒作有些过敏。表面上是因为最初的沃霍公告（以及这篇沃霍博客文章）提出了很多可能被解释为夸张的大胆声明，同时他们对编译器守口如瓶，这给人一种没有实质的感觉。但实话实说，可能是因为我对朱莉娅在生物信息学领域的前景非常投资。
- en: To me, Julia seems like *such an obvious* solution to the two-language problem
    in bioinformatics (and in deep learning). All the hard problems with bridging
    speed and dynamism have essentially been solved in Julia. At the same time, the
    language remains niche, mostly because it still has too many rough edges and usability
    issues, such as latency, the inability to statically analyse Julia or compile
    executable binaries. But these issues are not fundamental to the language - they're
    rather in the category of ordinary engineering problems. Solving them is mostly
    "just" a matter of putting in tens of thousands of professional dev hours, which
    is a matter of getting tens of millions of euros to pay for hiring people to do
    the job.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 对我来说，朱莉娅似乎是在生物信息学（以及深度学习）中解决双语言问题的*如此明显*的解决方案。在朱莉娅中，解决了所有与速度和动力学的连接问题，同时，该语言仍然是一个小众，主要是因为它仍然存在太多粗糙的边缘和可用性问题，如延迟、不能静态分析朱莉娅或编译可执行二进制文件。但这些问题并不是该语言的根本问题——它们更多地属于普通工程问题的范畴。解决它们大部分是“只”需要投入数万小时的专业开发时间，这只是获取数百万欧元来支付雇佣人员来完成工作的问题。
- en: It does grate me then, when *someone else* manages to raise 100M dollars on
    the premise of reinventing the wheel to solve the exact same problem, but from
    a worse starting point because they start from zero *and* they want to retain
    Python compatibility. Think of what money like that could do to Julia!
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 当*其他人*设法筹集1亿美元，以重复发明轮子来解决完全相同的问题时，这让我感到不舒服，因为他们从零开始*并且*他们希望保留Python的兼容性。想想这样的资金可以给朱莉娅带来什么！
- en: A bigger person than me might have an attitude of 'let a thousand flowers bloom'
    to solve the two language problem, and, sure, it's probable that Julia will learn
    from Mojo as Mojo already has learned from Julia. But I can't help the feeling
    that the two languages compete in a zero-sum game, at least to some extent. When
    I talk to my colleagues, half of them have no interest in high performance computing,
    and most others have resigned themselves to only doing the analyses that existing
    C libraries allow them to do, believing that writing new low-level routines is
    the job of someone else, probably computer scientists. Because they're not programming
    language nerds like me, they will use the tools that are at hand, without caring
    about their technical merit. If more money is spent on sanding the edges off a
    technically worse solution, then they will stick with it until the end of time,
    and not demand something better.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 比我更大度的人也许会持有“让一千朵鲜花绽放”的态度来解决双语问题，当然，朱莉娅可能会向沃霍学习，就像沃霍已经从朱莉娅学习过一样。但我不能不感到，这两种语言在某种程度上竞争为零和博弈。当我和同事交谈时，一半的人对高性能计算不感兴趣，而其他大部分人已经接受了只能使用现有的C库来进行分析的事实，他们相信编写新的低级程序是别人的任务，可能是计算机科学家。因为他们不像我一样是编程语言迷，他们将使用手头的工具，而不关心它们的技术价值。如果更多的钱花在修正技术上更差的解决方案上，那么他们会一直坚持下去，不会要求更好的东西。
- en: Does Mojo bring real value to the Python ecosystem? To me it's still too early
    to tell. I'm glad someone of the calibre of Chris Lattner is working on breaking
    the two-language barrier, but I wished he had joined forces with those who have
    been solving the problem the last decade in Julia-land.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 沃霍是否给Python生态带来了真正的价值？对我来说，现在还为时过早。我很高兴像克里斯·拉特纳这样的人正在努力打破双语言壁垒，但我希望他能与那些在过去十年里一直在朱莉娅领域解决这一问题的人合作。
- en: ''
  id: totrans-111
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '*Added 2024-04-01*'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: '*添加于2024-04-01*'
- en: After its original publication, this post has made the rounds on various forums.
    In this part, I want to address some responses.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 在最初发布后，这篇文章在各种论坛上广为流传。在这一部分，我想回应一些回应。
- en: ''
  id: totrans-114
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The commit I read was 42ba5bc. This commit didn't do any validation.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 我看的提交是42ba5bc。这个提交没有进行任何验证。
- en: While the `record_coord.mojo` did contain a `.validate(self, chunk)` method,
    this method was not called by the `FastParser.parse_all` function. This is the
    function mentioned in the README.md under the "Usage" section, as well as the
    function called in `main.mojo`.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管`record_coord.mojo`包含了一个`.validate(self, chunk)`方法，但这个方法并没有被`FastParser.parse_all`函数调用。这是README.md中“Usage”部分提到的函数，也是`main.mojo`中调用的函数。
- en: 'The benchmarking section said the following:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 基准测试部分如下所述：
- en: '[ content elided .... ]'
  id: totrans-118
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '[内容已省略...]'
- en: It's entirely possible that the original Mojo post used timings obtained from
    running `parser.next()` in a loop, which *does* validate the records - the "Benchmarks"
    section does not say exactly *what function* from the `FastParser` module was
    used.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 完全有可能，原始的 Mojo 帖子使用了在循环中运行 `parser.next()` 获得的时间，这会验证记录 - "Benchmarks" 部分并没有准确说明
    `FastParser` 模块中使用了哪个函数。
- en: However, I feel it's unfair to blame me for looking at the published code and
    assuming that's the code that was being run.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 不过，我觉得把责任推给我，说我只看了发布的代码，就假定那就是正在运行的代码，这是不公平的。
- en: Also, note that the MojoFastTrim repo, and the Mojo blog post has been *changed*
    since I wrote my post. The changes to the repo includes added validation to the
    `parse_all` function, and adding a new `benchmark` directory to make the benchmarking
    instructions clearer. The changes to the post include adding a link to the updated
    repo.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，请注意，自从我写了我的帖子以来，MojoFastTrim 仓库和 Mojo 博客文章已经 *更改*。对仓库的更改包括为 `parse_all` 函数添加了验证，并添加了一个新的
    `benchmark` 目录，以使基准测试说明更清晰。对文章的更改包括添加了指向更新后仓库的链接。
- en: 'At any rate, it''s worth keeping in mind that:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 无论如何，值得记住的是：
- en: Even with the `validate` function, the Mojo implementation does less validation
    than Needletail. For example, it doesn't handle file IO errors at all, or handle
    reads longer than the buffer, or handle Windows line endings.
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 即使使用了 `validate` 函数，Mojo 的实现也比 Needletail 进行的验证要少。例如，它根本不处理文件 IO 错误，也不处理超过缓冲区的读取，也不处理
    Windows 的行尾。
- en: 'However, Needletail doesn''t do a ton of validation, either. For example, it
    doesn''t check that the sequences and qualities are ASCII, which the code assumes.
    For example, this is an excerpt from the Needletail code, which where the check
    that is commented out is that the quality line must be bytes between `!` and `~`:'
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 但是 Needletail 也没有进行大量的验证。例如，它不检查序列和质量是否是 ASCII，而代码却假定了这一点。例如，这是 Needletail 代码的一部分，被注释掉的检查是质量行必须是位于
    `!` 和 `~` 之间的字节：
- en: ''
  id: totrans-125
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: It's not necessary to run Mojo on my own machine to show that Mojo's speed is
    due to its implementation. This post 100% grants that Mojo's implementation is
    exactly as much faster than Needletail as is claimed on the MojoFastTrim repository.
    All I have to show is that I get at least as large an improvement over Needletail
    as they claim Mojo does, by implementing the same algorithm in Julia.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 并不需要在我的机器上运行 Mojo 来证明 Mojo 的速度是由其实现决定的。本文百分之百肯定了 Mojo 的实现与 MojoFastTrim 仓库上所声称的一样快。我只需要证明，通过在
    Julia 中实现相同的算法，我至少可以获得与 Mojo 所声称的一样大的改进，超越 Needletail。
- en: But that's a different machine! How can we know your Julia implementation is
    fast on machines in general?
  id: totrans-127
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 但那是另一台机器！我们怎么知道您的 Julia 实现在一般情况下运行得快呢？
- en: If it all comes down to which machine it's run on, I might as well claim that
    the Mojo post doesn't prove anything because it's not run on my machine. See how
    it works?
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 如果一切都取决于在哪台机器上运行，那么我可能也会声称 Mojo 的帖子并没有证明任何事情，因为它没有在我的机器上运行。看看它是如何运作的？
- en: At the *very least*, I've shown that the Mojo implementation is *insufficient
    evidence* that the reason the Mojo implementation is fast is due to features unique
    to Mojo which Julia (or Rust) doesn't have. If it's really due to Mojo's groundbreaking
    compiler advances, why is my Julia implementation relatively faster compared to
    Needletail?
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 至少，我已经证明了 Mojo 实现不足以证明 Mojo 实现之所以快，是由于 Mojo 具有而 Julia（或 Rust）没有的独特特性。如果真的是由于
    Mojo 的突破性编译器进展，为什么我的 Julia 实现与 Needletail 相比相对更快呢？
- en: 'But *fine.* I downloaded a VM and installed Julia and Mojo in it. Hmm, but
    what was the CLI for the Mojo program? Let''s see:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 但是 *好吧*。我下载了一个虚拟机，并在其中安装了 Julia 和 Mojo。嗯，但是 Mojo 程序的命令行是什么？让我们看看：
- en: '[PRE7]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Fun fun fun. Anyway Here are my timings, all run in the same box on the later
    commit 38bb68:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 好玩好玩好玩。无论如何，这是我的时间记录，都是在同一台机器上的后期提交 38bb68 上运行的：
- en: 'My Julia code: 213 ms'
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我的 Julia 代码：213 毫秒
- en: 'The provided Mojo code with validation: 332 ms'
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 提供的 Mojo 代码验证后：332 毫秒
- en: 'The provided Mojo code without validation: 320 ms'
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 提供的没有验证的 Mojo 代码：320 毫秒
- en: 'Needletail + w. `lto` and `codegen_units=1`: 356 ms'
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Needletail + `lto` 和 `codegen_units=1`：356 毫秒
- en: 'Needletail: 471 ms'
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Needletail：471 毫秒
- en: So yeah, the point stands.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 所以是的，论点成立。
- en: '| [[1]](#fnref:1) | I''ve looked at commit 42ba5bc. The repository has been
    updated since, so the code listed in this blog post might be out of date by the
    time you read this. |'
  id: totrans-139
  prefs: []
  type: TYPE_TB
  zh: '| [[1]](#fnref:1) | 我看了提交 42ba5bc。仓库自那时以来已经更新，因此在您阅读此文章时，此博客文章中列出的代码可能已过时。
    |'
- en: '| [[2]](#fnref:2) | I''ve found that when you mention that DNA is the basis
    of heritability, people will appear from thin air and argue about epigenetics.
    But I believe epigenetics is a rounding error compared to the DNA sequence when
    we talk about heritability and the medium of evolution. I don''t doubt that e.g.
    chromatin accessibility is an important parameter in cells, but let''s not conflate
    the biological state of a cell with a *heiritable signal* which is stable enough
    to be acted on over evolutionary time. |'
  id: totrans-140
  prefs: []
  type: TYPE_TB
  zh: '| [[2]](#fnref:2) | 我发现当你提到 DNA 是遗传性的基础时，人们会从空中出现并争论表观遗传学。但我认为，与 DNA 序列相比，表观遗传学在遗传性和进化媒介方面只是一个舍入误差。我不怀疑例如染色质可及性在细胞中是一个重要参数，但让我们不要将细胞的生物状态与
    *稳定到可以在进化时间内发挥作用的遗传信号* 混为一谈。'
- en: '| [[3]](#fnref:3) | Some programmers wonder why DNA is usually saved encoded
    in plaintext. Isn''t that inefficient, considering the cost of storage for terabyte-sized
    DNA datasets? Nope. It''s usually stored gzip-compressed at decompressed on the
    fly when used. DNA compresses well, and the plaintext format allows extra metadata
    to be written directly into the file, as well as being much easier to parse. There
    are some more efficient formats, like CRAM, which are used in some large-scale
    projects, but in my subfield of microbial metagenomics, I can''t recall ever having
    worked with a CRAM file. |'
  id: totrans-141
  prefs: []
  type: TYPE_TB
  zh: '| [[3]](#fnref:3) | 一些程序员想知道为什么DNA通常以明文编码保存。考虑到TB级别的DNA数据集的存储成本，这不是低效吗？不是的。通常存储为gzip压缩格式，在使用时动态解压缩。DNA压缩效果很好，并且明文格式允许将额外的元数据直接写入文件，同时更容易解析。还有一些更高效的格式，比如CRAM，在一些大型项目中使用，但在我的微生物宏基因组学子领域，我几乎没有使用CRAM文件的经历。
    |'
- en: '| [[4]](#fnref:4) | Only partially the reason - Needletail has two more reasons
    it''s faster. First, Rust''s `memchr` crate used by Needletail is much more optimised
    than Julia''s Automa.jl used by FASTX.jl, and Automa.jl probably can''t be optimised
    to the same level because Julia doesn''t support platform-specific SIMD code yet.
    Second, Rust''s borrowchecker makes it safe for Needletail to return a view into
    the active file buffer. This would be totally reckless in Julia, so we need to
    copy the bytes out to a separate buffer first (we actually need to do *two copies*
    of each byte, since Julia''s IO is buffered by default, using an inaccessible
    buffer). |'
  id: totrans-142
  prefs: []
  type: TYPE_TB
  zh: '| [[4]](#fnref:4) | 只有部分原因 - Needletail 更快的两个原因还在于。首先，Rust的`memchr`库用于Needletail，比起FASTX.jl使用的Julia的Automa.jl要优化得多，而且Automa.jl可能无法达到同样的优化水平，因为Julia目前还不支持特定平台的SIMD代码。其次，Rust的借用检查器使Needletail能够安全地返回到活动文件缓冲区的视图。在Julia中这样做将是完全鲁莽的，所以我们需要先将字节复制到一个单独的缓冲区中（实际上我们需要*复制两次*每个字节，因为Julia的IO默认使用不可访问的缓冲区）。
    |'
