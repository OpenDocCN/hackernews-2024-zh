- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-05-27 14:53:33'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: Introducing Gemini 1.5, Google's next-generation AI model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://blog.google/technology/ai/google-gemini-next-generation-model-february-2024/](https://blog.google/technology/ai/google-gemini-next-generation-model-february-2024/)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Introducing Gemini 1.5
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '*By Demis Hassabis, CEO of Google DeepMind, on behalf of the Gemini team*'
  prefs: []
  type: TYPE_NORMAL
- en: This is an exciting time for AI. New advances in the field have the potential
    to make AI more helpful for billions of people over the coming years. Since [introducing
    Gemini 1.0](https://blog.google/technology/ai/google-gemini-ai/), we’ve been testing,
    refining and enhancing its capabilities.
  prefs: []
  type: TYPE_NORMAL
- en: 'Today, we’re announcing our next-generation model: Gemini 1.5.'
  prefs: []
  type: TYPE_NORMAL
- en: Gemini 1.5 delivers dramatically enhanced performance. It represents a step
    change in our approach, building upon research and engineering innovations across
    nearly every part of our foundation model development and infrastructure. This
    includes making Gemini 1.5 more efficient to train and serve, with a new [Mixture-of-Experts](https://arxiv.org/abs/1701.06538)
    (MoE) architecture.
  prefs: []
  type: TYPE_NORMAL
- en: The first Gemini 1.5 model we’re releasing for early testing is Gemini 1.5 Pro.
    It’s a mid-size multimodal model, optimized for scaling across a wide-range of
    tasks, and [performs at a similar level to 1.0 Ultra](https://goo.gle/GeminiV1-5),
    our largest model to date. It also introduces a breakthrough experimental feature
    in long-context understanding.
  prefs: []
  type: TYPE_NORMAL
- en: Gemini 1.5 Pro comes with a standard 128,000 token context window. But starting
    today, a limited group of developers and enterprise customers can try it with
    a context window of up to 1 million tokens via [AI Studio](https://aistudio.google.com/)
    and [Vertex AI](https://cloud.google.com/vertex-ai) in private preview.
  prefs: []
  type: TYPE_NORMAL
- en: As we roll out the full 1 million token context window, we’re actively working
    on optimizations to improve latency, reduce computational requirements and enhance
    the user experience. We’re excited for people to try this breakthrough capability,
    and we share more details on future availability below.
  prefs: []
  type: TYPE_NORMAL
- en: These continued advances in our next-generation models will open up new possibilities
    for people, developers and enterprises to create, discover and build using AI.
  prefs: []
  type: TYPE_NORMAL
