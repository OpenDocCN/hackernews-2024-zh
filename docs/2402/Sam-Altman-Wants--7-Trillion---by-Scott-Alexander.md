<!--yml

category: 未分类

date: 2024-05-27 14:52:10

-->

# 《Sam Altman Wants $7 Trillion - by Scott Alexander》

> 来源：[https://www.astralcodexten.com/p/sam-altman-wants-7-trillion](https://www.astralcodexten.com/p/sam-altman-wants-7-trillion)

*[这里的所有数字都非常粗略，并且以松散的方式呈现。想要更严谨版本的，请阅读[Tom Davidson](https://www.astralcodexten.com/p/davidson-on-takeoff-speeds)，[Yafah Edelman](https://www.lesswrong.com/posts/nXcHe7t4rqHMjhzau/report-on-frontier-model-training)，和[EpochAI](https://epochai.org/blog/trends-in-the-dollar-training-cost-of-machine-learning-systems))*]

**I.**

Sam Altman [想要7万亿美元](https://www.cnbc.com/2024/02/09/openai-ceo-sam-altman-reportedly-seeking-trillions-of-dollars-for-ai-chip-project.html)。

从某种意义上说，这不是新闻。每个人都想要7万亿美元。我也想要7万亿美元。我得不到，Sam Altman可能也不会得到。

尽管如此，媒体把这件事当作值得评论的事情，我也同意。这是一个有用的提醒，AI在未来几年要扩展需要什么。

基本逻辑：GPT-1训练成本几乎为零。[GPT-2](https://blog.dataiku.com/pre-trained-models-ais-object-oriented-programming)训练成本为4万美元。[GPT-3](https://www.pcguide.com/apps/gpt-3-cost/)训练成本为400万美元。[GPT-4](https://en.wikipedia.org/wiki/GPT-4#Training)训练成本为1亿美元。关于GPT-5的细节仍然是秘密，但[一个极不可靠的估计](https://mpost.io/gpt-5-training-will-cost-2-5-billion-and-start-next-year/)说是25亿美元，这似乎是正确数量级，考虑到微软给OpenAI的8亿美元。

所以每个GPT的成本是上一个的25倍到100倍。我们平均说是30倍。这意味着我们可以预期GPT-6的成本为750亿美元，GPT-7的成本为2万亿美元。

（除非他们在一个并非完全领先于GPT-5的模型上贴上“GPT-6”的名字。考虑这些数字代表的模型，它们可能与GPT-4相比如同GPT-4与GPT-3的距离一样远，无论它们如何标榜。）

让我们试着分解这个成本。在一个非常抽象的意义上，训练一个AI需要三个要素：

+   计算（即计算能力、硬件、芯片）

+   电力（用于计算）

+   训练数据

*计算*

计算使用浮点运算（FLOPs）来衡量。GPT-3训练使用了[10^23](https://airtable.com/appDFXXgaG1xLtXGL/shrBucz1oynb4AUab/tblhmFk3gP7psWh3C?backgroundColor=cyanDusty&viewControls=on) FLOPs，而GPT-4可能使用了[10^25](https://www.kdnuggets.com/2023/07/gpt4-details-leaked.html)。

全球所有计算机的容量约为[10^21](https://wiki.aiimpacts.org/ai_timelines/hardware_and_ai_timelines/computing_capacity_of_all_gpus_and_tpus) FLOP/秒，因此它们可以在10^4秒内（即两小时内）训练GPT-4。由于OpenAI的计算机并不多，他们花了六个月的时间。这表明在那段时间内，OpenAI使用了全球计算机的约1/2000。

如果我们保持30倍的缩放系数，GPT-5将占据全球计算机的1/70，GPT-6将占据1/2，而GPT-7将占据现有计算机的15倍。世界的计算能力增长很快 - [此来源](https://www.metaculus.com/notebooks/10688/how-much-of-ai-progress-is-from-scaling-compute-and-how-far-will-it-scale/)称其每1.5年翻一番，这意味着每五年增长一个数量级，这意味着这些数字可能是过高估计。如果我们假设每个GPT之间相隔五年，那么GPT-6实际上只需全球计算机的1/10，而GPT-7只需全球计算机的1/3。但是，全球计算机的1/3仍然是相当多的。

可能你无法得到全球计算机的1/3，特别是当所有其他AI公司也想要它们时。你需要大幅扩展芯片制造。

*能源*

GPT-4的训练消耗了[50 gigawatt-hours](https://www.ri.se/en/news/blog/generative-ai-does-not-run-on-thin-air)的能源。按我们的30倍缩放系数，我们预计GPT-5将需要1,500，GPT-6将需要45,000，而GPT-7将需要1.3百万。

假设训练过程持续六个月，即4,320小时。这意味着GPT-6将需要10 GW的能量 - 大约是世界上最大的三峡大坝发电量的一半。GPT-7将需要十五座三峡大坝。这不仅仅是“世界需要总产生这么多能量，你可以购买它”。你需要的能量必须非常接近你的数据中心。你最好的选择是要么像北欧石油公司那样有一整条输气管道直接连接到你的数据中心，要么选择核聚变反应堆。

([萨姆·阿尔特曼正在致力于核聚变能源](https://www.cnbc.com/2023/05/10/microsoft-agrees-to-buy-power-from-sam-altman-backed-helion-in-2028.html)，但这似乎是巧合。至少，自2016年以来，他一直对核聚变感兴趣，这对于他早于任何事情都知晓是为时过早的。)

*训练数据*

这是AI用来理解其领域工作方式的文本或图像。[GPT-3](https://lambdalabs.com/blog/demystifying-gpt-3)使用了3000亿令牌。[GPT-4](https://www.springboard.com/blog/data-science/machine-learning-gpt-3-open-ai/)使用了13万亿令牌（另一来源称为6万亿）。这看起来我们的30倍缩放系数似乎仍然有些适用，但理论上训练数据应该与计算的平方根成比例 - 所以你应该预期一个5.5倍的缩放系数。这意味着GPT-5将需要大约500万亿令牌，GPT-6将需要几百万亿令牌，而GPT-7将需要几千万亿令牌。

整个世界上没有那么多的文本。你也许可以通过结合所有已发布的书籍、Facebook消息、推文、短信和电子邮件来得到几万亿更多。一旦AI学会理解这些，你可以通过添加所有图像、视频和电影来获取更多。我仍然认为你不会达到一百万亿，更不用说一千万亿了。

你可以尝试制造一个可以用较少训练数据学习东西的人工智能。这可能是可能的，因为人脑也可以在不阅读世界上所有的文字的情况下学习东西。但这很难，目前还没有人有很好的想法如何去做。

更有希望的是合成数据，即人工智能为自己生成数据。这听起来像一个行不通的永动机，但有一些技巧可以规避这个问题。例如，你可以让一个象棋人工智能在合成数据上自己对弈一百万次。你可以让一个数学人工智能随机生成证明的步骤，最终偶然间发现了一个正确的证明，自动检测到了这个正确的证明，然后在其上进行训练。你可以训练一个玩游戏的人工智能，让它进行随机动作，然后看哪一个得分最高。总的来说，当你不知道如何创建好的数据时，你可以使用合成数据，但你确实知道一旦它存在你如何识别它（比如象棋人工智能赢得了对弈，数学人工智能得到了正确的证明，游戏人工智能获得了一个好的得分）。但目前还没有人知道如何为书面文本做到这一点。

也许你可以通过一些文本、象棋、数学和电子游戏的组合创造一个聪明的人工智能 - 一些人类会追求这样的课程，并且对他们来说也是有效的。

这有点奇怪 - 计算和电力可以通过大量的资金解决，但这可能需要更多的突破。

*算法进展*

这意味着“人们会突破并变得更擅长构建人工智能”。似乎这又是另一件每五年就会实现数量级进展的事情，所以我稍微降低了上面的估计。

*将其全部放在一起*

GPT-5可能需要全球约1%的计算机，小型发电厂的能量，以及大量的训练数据。

GPT-6可能需要全球约10%的计算机，相当于一个大型发电厂的能量，以及更多的训练数据，甚至超过了现有的数据总和。可能看起来像是连接着许多太阳能面板或核反应堆的城镇大小的数据中心。

GPT-7可能需要全球所有的计算机，比任何当前存在的巨大的发电厂更大，并且超过了现有的训练数据 *远远*。可能看起来像是连接着一个核聚变发电厂的城市大小的数据中心。

目前建造GPT-8是不可能的。即使你解决了合成数据和核聚变能源，并控制了整个半导体行业，你也不会接近。你唯一的希望是GPT-7是超智能的，并帮助你完成这一点，要么告诉你如何为廉价建造人工智能，要么通过促进全球经济增长来资助目前不可能的事情。

所有有关GPTs >5的东西都只是对现有趋势的天真预测，可能是错误的。只是估算的数量级。

你可能会称这是“投机的”、“疯狂的”。但如果Sam Altman不相信至少有这种投机和疯狂的东西，他也不会要求7万亿美元。

**II.**

让我们重新审视一下。

GPT-6 可能会花费 750 亿美元或更多。OpenAI 无法承担这个费用。微软或谷歌可能能负担得起，但这将占用公司资源的重要部分（也许一半？）。

如果 GPT-5 失败，或者只是渐进改进，没有人愿意花 750 亿美元制造 GPT-6，所有这些都将无所谓。

另一方面，如果 GPT-5 接近人类水平，并且彻底改变了整个行业，并且似乎即将引发人类事务的工业革命级变化，那么 750 亿美元用于下一代将会显得很划算。

另外，如果你正在引发人类事务的工业革命级变化，也许事情会变得更便宜。我不指望 GPT-5 足够优秀以至于能够为 GPT-6 的规划做出重大贡献。但你必须逐步考虑这一点。它能做足够的事情使得大型项目（如 GPT-6、其相关的芯片工厂或相关的发电厂）便宜 10% 吗？也许可以。

这的要点是我们正在看一个指数过程，就像流行病的 R 值一样。如果指数大于1，它会迅速增长。如果指数小于1，它就会逐渐消退。

在这种情况下，如果每一代新的人工智能足够令人兴奋以激发更多的投资，并且/或者足够聪明以降低下一代的成本，那么这两个因素的结合允许在正反馈环路中创建另一代人工智能。

但是，如果每一代新的人工智能都不足以激发投入下一代所需的巨额投资，并且也不足够聪明以帮助降低下一代的价格，那么在某个时刻，没有人愿意资助更先进的人工智能，当前的人工智能繁荣就会消退（R < 1）。这并不意味着你再也听不到有关人工智能的消息 - 人们可能会生成令人惊叹的人工智能艺术作品、视频、机器人和机器人女友。这只是意味着最大模型的原始智能增长速度不会那么快。

即使 R < 1，我们最终仍然会得到更大的模型。芯片工厂可以逐步生产更多的芯片。研究人员可以逐步推出更多的算法突破。如果没有其他选择，你可以花十年慢慢地训练 GPT-7。这意味着我们在21世纪中叶会得到人类或超越人类水平的人工智能，而不是早期。

**III.**

当 Sam Altman 要求 7 万亿美元时，我理解他希望通过集中、快速、高效的方式进行此过程。一个人建造芯片工厂和发电厂，并在他需要训练下一个大模型时都准备就绪。

或许他不会得到他的7万亿美元。那么同样的过程会发生，但更慢、更零散。他们会推出GPT-5。如果它好，有人会想要建造GPT-6。正常的资本主义会导致人们逐渐增加芯片容量。人们会制造很多GPT-5.1和GPT-5.2，直到最终有人决定在某处建造巨型发电厂。所有这些将需要几十年时间，自然发生，没有一个人或一个公司会垄断。

我更愿意接受第二种情况：[安全视角](https://www.astralcodexten.com/p/pause-for-thought-the-ai-pause-debate)是，我们希望尽可能多地准备应对颠覆性AI所需的时间。

之前Sam Altman支持过这个立场！他说OpenAI的努力对安全有益，因为你想避免*计算过剩*。也就是说，你希望AI的进展尽可能缓慢，而不是突然跳跃。而你可以保持事物渐进的方式之一是，用当前的芯片达到你能建造的AI的最高水平，然后AI可以（在最糟糕的情况下）以芯片供应的速度增长，这自然增长相当缓慢。

…*除非*你要求提供7万亿美元，以尽快大幅增加芯片供应！基于计算过剩论点信任OpenAI的善意的人们[现在感到被背叛](https://forum.effectivealtruism.org/posts/vBjSyNNnmNtJvmdAg/sam-altman-s-chip-ambitions-undercut-openai-s-safety)。

我目前对OpenAI的印象[多个相互矛盾的观点在此](https://www.astralcodexten.com/p/openais-planning-for-agi-and-beyond)是，他们确实对安全感兴趣 - 但仅限于这与尽快扩展AI的可行性相容。这远非AI公司可能采取的最糟糕的方式。但也并不令人放心。
