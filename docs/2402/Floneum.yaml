- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-05-29 13:26:57'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: Floneum
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://floneum.com/blog/kalosm_0_2/](https://floneum.com/blog/kalosm_0_2/)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We''re excited to announce the release of Kalosm v0.2.0! This release includes
    a number of new features, improvements, and bug fixes including:'
  prefs: []
  type: TYPE_NORMAL
- en: Tasks and Agents
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Task Evaluation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Prompt Auto-Tuning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Regex Validation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Surreal Database Integration
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: RAG improvements
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Performance Improvements
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: New Models
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Kalosm now includes utilities for running, evaluating, and improving tasks and
    agents.
  prefs: []
  type: TYPE_NORMAL
- en: Let's build a simple task and agent to demonstrate the new functionality.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Or you can use more complex constraints to
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Tasks can efficiently reuse the session between runs, which can significantly
    speed up the process:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: The third question required a more complex calculation and took more tokens
    to solve, but the time to solve the question was still significantly faster than
    the first question. The session from the first question was reused for the second
    and third questions which made the second and third questions run faster.
  prefs: []
  type: TYPE_NORMAL
- en: '**Evaluation Abstraction:** Introducing an evaluation abstraction, providing
    enhanced functionality. ([#113](https://github.com/floneum/floneum/pull/113))'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '**Prompt Auto-Tuning:** Prompts can now be automatically tuned for better performance.
    ([#132](https://github.com/floneum/floneum/pull/132))'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Lets take a look at the new prompt auto-tuning feature with an example. As
    part of the [RAG improvements](#Improved-Chunking-Strategies), kalosm includes
    a task that generates hypothetical questions about a text for an embedding model
    that can be used to find similar documents based on the meaning of the text for
    the section. We can tune that task to find the best examples for the task with
    a PromptAnnealer:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Here is the best set of examples that the prompt annealer found for the task:'
  prefs: []
  type: TYPE_NORMAL
- en: '| Input | Output |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| While traditional databases rely on a fixed schema, NoSQL databases like
    MongoDB offer a flexible structure, allowing you to store and retrieve data in
    a more dynamic way. This flexibility is particularly beneficial for applications
    with evolving data requirements. | "How does MongoDB differ from traditional databases?
    |'
  prefs: []
  type: TYPE_TB
- en: '| Blockchain technology, beyond cryptocurrencies, is being explored for applications
    like smart contracts. Smart contracts are self-executing contracts with the terms
    of the agreement directly written into code. | "How is blockchain technology utilized
    in the concept of smart contracts? |'
  prefs: []
  type: TYPE_TB
- en: Feeding those two examples into the task achieves a similarity score of 0.71
    for all of the other examples compared to choosing two random examples from the
    task which only achieves a similarity score of 0.62 for all of the other examples.
  prefs: []
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Some feedback we got from the initial release of kalosm, was that constraints
    for constrained generation was too complex. Constraints in Kalosm serve two purposes:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Validation: Constraints can be used to validate the output of the model. The
    model will only output text that can be parsed by the constraints. This lets you
    ensure that the output of the model is in the format you expect.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'For example, you may want to force the model response to always start with
    a prefix that guides the model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Parsing: Constraints can be used to parse the output of the model. This can
    be extremely useful when you want to generate a specific structure from an LLM
    without writing separate logic for validation and parsing.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'For example, you may want to generate a list of 10 numbers:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'If you only need to validate the output of the model, the existing constraints
    can be more complex than what you need. In this release, we''ve added support
    for regex validation. This makes it easier to validate the output of the model
    without handling parsing:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: ''
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Vector databases can be very useful when combined with LLMs. They can be used
    to store and retrieve similar documents based on the meaning of the text, not
    just the words used. However, vector databases only handle a very limited number
    of use cases. In this release, we've added support for Surreal DB for more traditional
    database use cases. Surreal DB can be embedded into your application and used
    to store and retrieve data locally as well as over the network.
  prefs: []
  type: TYPE_NORMAL
- en: Kalosm 0.2 allows you to create tables within Surreal DB that are indexed by
    vectors. You can then insert documents (or other embeddings) into the table and
    query the table for similar documents based on the meaning of the text.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s take a look at how you can use the Surreal DB integration to store and
    retrieve similar documents based on the meaning of the text:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: ''
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'RAG (Retrieval-Augmented Generation) is a powerful tool for generating text
    with up-to-date or proprietary information. Retrieval-augmented generation generally
    follows the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Gather context from some local files, your database, or web data. In kalosm,
    you can retrieve data from any source that implements [`IntoDocument`](https://docs.rs/kalosm/0.2.0/kalosm/language/trait.IntoDocument.html)
    or [`IntoDocuments`](https://docs.rs/kalosm/0.2.0/kalosm/language/trait.IntoDocuments.html).
    You can gather your sources from [local documents](https://docs.rs/kalosm/0.2.0/kalosm/language/struct.DocumentFolder.html),
    a [search term](https://docs.rs/kalosm/0.2.0/kalosm/language/struct.SearchQuery.html),
    [specific web page](https://docs.rs/kalosm/0.2.0/kalosm/language/struct.Url.html),
    an [RSS feed](https://docs.rs/kalosm/0.2.0/kalosm/language/struct.RssFeed.html),
    or even a [custom web crawler](https://docs.rs/kalosm/0.2.0/kalosm/language/enum.Page.html#method.crawl).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Insert that context into a searchable database. Kalosm includes a [vector database](https://docs.rs/kalosm/0.2.0/kalosm/language/struct.VectorDB.html)
    that can be used to store and retrieve similar documents.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Vector databases use an embedding model which generates a vector for a chunk
    of text (typically smaller than the entire document). The vector is then stored
    in the database. When you want to retrieve similar documents, you can embed a
    query and search for similar vectors in the database.
  prefs: []
  type: TYPE_NORMAL
- en: The vectors represent the meaning of the text, so you can search for similar
    documents based on the meaning of the text, not just the words used.
  prefs: []
  type: TYPE_NORMAL
- en: Use the context to generate text. You can find text similar to the question
    or a search generated by the LLM and then generate a response based on the context.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In this release, we've made several improvements to RAG! ([#126](https://github.com/floneum/floneum/pull/126))
  prefs: []
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'When you insert a document into a vector database, it needs to be split into
    smaller chunks before the text is embedded. The chunks you choose can have a significant
    impact on the performance of the results you get from the vector database. In
    this release, we''ve added two new chunking strategies to the vector database:'
  prefs: []
  type: TYPE_NORMAL
- en: Instead of generating embeddings based on the content of the document, this
    chunking strategy generates embeddings based on hypothetical questions generated
    about the document. This can be extremely useful when building a chatbot that
    needs to find context that is relevant to a question.
  prefs: []
  type: TYPE_NORMAL
- en: For example, if you have a document about the history of the United States,
    you can generate hypothetical questions like "What is the capital of the United
    States?" and "Who was the first president of the United States?" and then generate
    embeddings based on those questions.
  prefs: []
  type: TYPE_NORMAL
- en: Then if you query the vector database with a question like "Who was the leader
    of the US?" you can find the document about the history of the United States.
  prefs: []
  type: TYPE_NORMAL
- en: Notice that the question "Who was the leader of the US?" doesn't contain many
    of the same words as the hypothetical questions, but it does convey a similar
    meaning, so the vector database can still find the relevant document.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: This chunking strategy generates embeddings based on the summary of the document.
    This can be useful when you have a large document and you want to find similar
    documents based on the main points of the document.
  prefs: []
  type: TYPE_NORMAL
- en: Generating embeddings based on the summary of the document can create better
    embeddings that contain more information about the document than embeddings that
    only contain one small chunk of the document.
  prefs: []
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In addition to the new chunking strategies, we've also added support for incremental
    indexing. This means you can add new documents to the vector database without
    having to recreate the entire database. This can be extremely useful when you
    have a large database or you have constantly updating context you want to provide
    to your LLM.
  prefs: []
  type: TYPE_NORMAL
- en: Kalosm's Vector database is now backed by [arroy](https://github.com/meilisearch/arroy),
    a space-efficient and incrementally indexed vector database backed by MeiliSearch!
  prefs: []
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The llama implementation has been rewritten and optimized for better performance
    and modularity. The new implementation is now 7-25% faster than the previous version.
    In future releases, we plan to add support for fine tuning models and training
    new heads for existing models. ([#122](https://github.com/floneum/floneum/pull/122))
  prefs: []
  type: TYPE_NORMAL
- en: Language models like Llama and Phi output probabilities for each token in the
    vocabulary. To generate text you need to sample from the probability distribution.
    Sampling from the probability distribution can be slow, especially with large
    vocabularies. Kalosm 0.2 uses an optimization introduced in [llm-samplers](https://github.com/KerfuffleV2/llm-samplers/pull/9)
    to only sample top 512 tokens. This optimization can make sampling up to 2x faster.
    ([#123](https://github.com/floneum/floneum/pull/123))
  prefs: []
  type: TYPE_NORMAL
- en: Large sections of text that are static within a constraint in structured generation
    is now loaded in a batch which can significantly speed up the process.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example if you have the constraints:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: The text "The title of the book is " will be loaded in a batch instead of one
    token at a time. Batched loading has been restored in constrained generation.
    ([#131](https://github.com/floneum/floneum/pull/131))
  prefs: []
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Kalosm 0.2 adds support for several new models, including:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Dolphin Phi v2** A tiny chat model'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Solar-11b Models** A set of models for chat, text, and code generation'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Tiny Llama 1.0** A tiny set of models for chat, and text text generation'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: For a detailed list of changes between v0.1.0 and v0.2.0, please see the [full
    changelog](https://github.com/floneum/floneum/compare/v0.2.0...v0.2.0-kalosm).
  prefs: []
  type: TYPE_NORMAL
- en: I hope you enjoy using Kalosm v0.2.0! Your feedback is invaluable to us, so
    please don't hesitate to share your thoughts and report any issues you encounter.
  prefs: []
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the next release, we plan to add support for fine tuning models and training
    new heads for existing models. We also plan to continue improving the performance
    of the language models and adding support for more models.
  prefs: []
  type: TYPE_NORMAL
- en: If any of those features sound interesting or you want to propose a new feature,
    consider contributing on [Github](https://github.com/floneum/floneum/tree/main/interfaces/kalosm).
  prefs: []
  type: TYPE_NORMAL
- en: If you are interested in building an application with Kalosm, [join the Discord](https://discord.gg/dQdmhuB8q5)
    and get involved with the community!
  prefs: []
  type: TYPE_NORMAL
