<!--yml

类别：未分类

日期：2024-05-27 14:35:08

-->

# transformers如何工作？+设计一个多类情感分析用于客户评论

> 来源：[https://nintyzeros.substack.com/p/how-do-transformer-workdesign-a-multi](https://nintyzeros.substack.com/p/how-do-transformer-workdesign-a-multi)

*👋 嗨，这里是[](https://twitter.com/gergelyorosz) Venkat，并且在这里免费提供《ZenMode工程师通讯》的完整内容。在每一期中，我涵盖一个关于计算机技术及其它领域的主题，用更简单的术语解释。*

* * *

Transformers已经成为尤其在自然语言处理（NLP）领域的前沿AI的代名词。

但究竟是什么让它们如此高效准确地工作？这些模型如何在语言的复杂性中航行？

系好安全带，因为我们即将了解transformer架构的核心。

但是…… 在我们深入探讨之前，让我们了解它被使用在哪里…… 如果你使用过Google翻译/ChatGPT，它们都依赖于这些。

> ***谷歌翻译：** 这一广泛使用的平台依赖于transformers，实现快速且准确的跨100多种语言的翻译。它考虑整个句子的语境，而不仅仅是单个单词，从而产生更自然的翻译。*
> 
> ***Netflix推荐系统：** 你是否想知道Netflix是如何推荐你可能喜欢的节目和电影的？Transformers分析你的观看历史和其他用户的数据，识别模式和连接，最终推荐根据你偏好量身定制的内容。*

**整体情况：编码器和解码器的舞蹈**

想象一个工厂，但不是用来组装物理物体的，而是处理语言。这个工厂有两个主要部门：

1.  **编码器：** 这是信息提取器，精细解剖输入文本，理解其各个元素，并揭示它们之间隐藏的连接。

1.  **解码器：** 在编码器的洞察力指导下，解码器制定所需的输出，无论是翻译句子、简洁摘要，甚至是全新的诗歌。

**编码器：解码输入迷宫**

编码器的旅程始于**输入嵌入**，在这里，每个单词从其文本形式转换为数值表示（向量）。可以将其看作是为每个单词分配一个唯一标识符。

* * *

考虑这个例子。

1.  **输入文本：** 这个过程从原始文本句子开始，比如“猫坐在垫子上。”

1.  **输入嵌入层：**

    +   这一层充当翻译器的角色，将每个单词转换为数值向量。

    +   想象一个大型词典，其中每个单词都有对应的向量地址。

    +   这些向量捕捉单词含义的各种方面：

        +   语义关系（例如，“cat”更接近“pet”而不是“chair”）。

        +   句法角色（例如，“cat”通常是名词，而“sat”是动词）。

        +   句子内上下文（例如，“mat”这里可能指地毯）。

1.  **向量表示：**

    * * *

但编码器并不止于此。它采用以下关键机制深入探索：

+   **自注意力层：** 这是改变游戏规则的地方。想象一下，对每个词都投射出聚光灯，但不仅照亮它本身，还突显它与句子中所有其他词的连接方式。这使得编码器能够理解文本中的上下文、细微差别和关系，而不仅仅是个别词语。

    引用自 Raimi Karim 博客（仅供参考）

    * * *

    再次考虑这个例句：“***The quick brown fox jumps over the lazy dog.***”

    1.  **词嵌入：** 首先，每个词被转换为一个称为“词嵌入”的数值表示。可以将其视为在一个巨大的词汇映射中为每个词分配一个唯一的标识符。

    1.  **查询、键、值：** 接下来，自注意力机制为每个词创建三个特殊向量：

        +   **查询（Q）：** 这个向量询问“我需要从其他词语中获取什么信息？”

        +   **键（K）：** 这个向量充当标签，表示“这是我要提供的信息”。

        +   **值（V）：** 这个向量保存实际信息，比如词的含义和上下文。

    1.  **注意力分数：** 现在来到有趣的部分。自注意力层将每个词的查询向量与句子中所有其他词的键向量进行比较。

        这有助于理解每个词与当前词的相关性。基于这种比较，它为每对词计算一个**注意力分数**。

        想象一下，对每个词都投射出聚光灯。其他词上的聚光灯越亮，注意力分数越高，意味着该词对当前词的相关性越高。

    1.  **加权值：** 最后，自注意力层使用注意力分数来加权所有其他词的值向量。注意力分数较高的词获得更大的权重，对当前词的最终表示贡献更大。

        可以将其视为从其他词语获取信息的加权平均值，其中权重由它们的相关性决定。

    1.  **新词表示：** 通过考虑其他词提供的上下文，自注意力层为每个词创建了一个新的、丰富的表示。这种新的表示不仅捕捉了词语本身的含义，还捕捉了它与句子中其他词语的关系及其受影响的方式。

        * * *

+   **多头注意力：** 这就像有多个分析团队，每个团队关注词语之间连接的不同方面。它使编码器能够捕捉关系的多个方面，丰富其理解。

    * * *

    **句子：** “The quick brown fox jumps over the lazy dog.”

    1.  **单独的头部：** 多头注意力不使用单一的自注意力机制，而是使用几个独立的“头部”（通常为4-8个）。每个头部为每个词都有自己的查询、键和值向量集。

    1.  **多样化的注意力：** 每个头部以不同的方式计算注意力分数，关注词语关系的各个方面：

        +   一个头可能关注语法角色（例如，“狐狸”和“跳”）。

        +   另一个可能专注于词序（例如，“the”和“quick”）。

        +   另一个可能捕捉同义词或相关概念（例如，“quick”和“fast”）。

    1.  **结合多个视角：** 每个头生成其加权值后，它们的输出被串联起来。这结合了来自不同注意机制的多样洞察。

    1.  **最终表示：** 这种结合表示持有句子更丰富的理解，包括词之间的各种关系，而不是单一的焦点。

        * * *

+   **位置编码：** 由于transformer不直接处理词序，这一层注入了关于每个单词在句子中位置的信息。这就像给分析人员一张地图，让他们知道考虑单词的顺序。

    * * *

    当然，让我们通过一个示例句子深入探讨位置编码：

    **句子：** "The quick brown fox jumps over the lazy dog."

    **以下是位置编码逐步工作的方式：**

    1.  **词嵌入：**

        +   每个单词（“The”，“quick”等）被转换为数值表示，称为词嵌入，就像在庞大词汇表中的唯一标识符。

        +   想象这些嵌入向量为向量：

            +   "The": [0.2, 0.5, -0.1, ...]

            +   "quick": [0.8, -0.3, 0.4, ...]

            +   "brown": [..., ...]

            +   ...

    1.  **位置信息：**

        +   每个单词的嵌入与基于句子中位置的附加值结合。

        +   这些值是使用不同频率的正弦和余弦函数计算的：

            +   低频捕捉长距离依赖关系（例如，“quick”和“fox”相关）。

            +   高频编码短距离关系（例如，“jumps”和“over”接近）。

        +   把这些附加值想象成“位置向量”：

            +   "The": [第1位置向量]

            +   "quick": [第2位置向量]

            +   "brown": [第3位置向量]

            +   ...

    1.  **结合嵌入和位置：**

    1.  **理解顺序：**

        +   即使句子顺序改变（例如，“狗 懒 跳...”），位置向量确保相对位置保持不变。

        +   模型仍然可以学习到“jumps”与“over”之间的关系大于与“The”之间的关系。

    * * *

+   **前馈网络（FFN）：** 这增加了一层非线性，使模型能够学习到单纯通过注意机制难以捕捉的更复杂关系。

    * * *

    你已经通过之前的层深入探讨了这个句子。你理解单词，它们之间的关系以及它们的位置。现在，前馈网络（FFN）像侦探的放大镜一样，准备揭示不易看到的复杂细节。

    **FFN通过三个关键步骤完成这一过程：**

    1.  **非线性转换：** FFN不使用直接的计算，而是使用ReLU等非线性函数增加复杂性。把它看作是对现有信息应用特殊过滤器，揭示隐藏的模式和连接，这使得FFN能够捕捉简单算术可能忽略的更微妙的单词之间关系。

    1.  **多层分析：** FFN不只是一个步骤；它通常是两个或更多全连接层的链条。每一层都建立在前一层之上，逐步转换信息。想象你在逐渐增大的放大倍率下审视句子，每一层揭示出更精细的细节。

    1.  **维度转换：** FFN在第一层扩展信息的大小（例如，从512维到2048维）。这使得它能够分析更广泛的特征并捕捉更复杂的模式。可以将其想象为在更大的画布上展开信息进行深入检查。然后，在最后一层将其收缩回原始大小（例如，再次为512维），以确保与后续层兼容。

    **将此应用到我们的句子中：**

    +   想象一下，前馈网络（FFN）帮助识别“quick”和“brown”不仅描述“fox”，而且通过它们的联合意义微妙地连接到它的感知速度。

    +   或者，它可能深入探讨“jumps”和“over”之间的关系，理解动作和空间背景，不仅仅是它们各自的定义。

    * * *

+   **重复、精炼、再重复：** 这些层（自注意力、多头注意力等）会被堆叠和重复多次。每次迭代时，编码器都会精炼其理解，构建输入文本的全面表示。

图片来源：pillow lab 博客

**解码器：编织输出的图景**

现在，解码器接过接力棒。但与编码器不同的是，它面临额外的挑战：逐字生成输出，不能窥视未来。为了实现这一点，它利用以下机制：

+   **掩码自注意力：** 类似于编码器的自注意力，但有所不同。解码器只关注先前生成的单词，确保不作弊和使用未来信息。这就像一次只写一句话的故事，不知道它如何结束。

+   **编码器-解码器注意力：** 这个机制允许解码器参考编码的输入，就像写作时参考参考文献一样。它确保生成的输出保持连贯并与原始文本对齐。

+   **多头注意力和前馈网络：** 就像编码器一样，这些层帮助解码器精炼其对文本内上下文和关系的理解。

+   **输出层：** 最后，解码器将其内部表示逐个翻译为实际输出词。这就像最终的装配线，将各个部件组装起来形成期望的结果。

**超越基础：**

记住，这只是转换器引人入胜世界的一瞥。具体的架构可能因任务和数据集而异，具有不同数量的层和配置。

此外，每一层都涉及复杂的数学操作，超出本解释的范围。

但希望这让你对转换器如何工作及其为何革新自然语言处理领域有了基本理解。

因此，下次你遇到无缝的机器翻译或者对AI驱动的文本生成器的创造力感到惊叹时，请记住变压器内编码器和解码器的复杂舞蹈，用注意力和并行处理的力量编织魔法。

*论文：https://arxiv.org/abs/1706.03762*

感谢您阅读《ZenMode》。本文是公开的，所以请随意分享。

[分享](https://nintyzeros.substack.com/p/how-do-transformer-workdesign-a-multi?utm_source=substack&utm_medium=email&utm_content=share&action=share)

* * *
