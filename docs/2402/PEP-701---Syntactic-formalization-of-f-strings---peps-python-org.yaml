- en: <!--yml
  id: totrans-split-0
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  id: totrans-split-1
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-05-27 14:50:18'
  id: totrans-split-2
  prefs: []
  type: TYPE_NORMAL
- en: -->
  id: totrans-split-3
  prefs: []
  type: TYPE_NORMAL
- en: PEP 701 – Syntactic formalization of f-strings | peps.python.org
  id: totrans-split-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://peps.python.org/pep-0701/](https://peps.python.org/pep-0701/)
  id: totrans-split-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: PEP 701 – Syntactic formalization of f-strings
  id: totrans-split-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Author:'
  id: totrans-split-7
  prefs: []
  type: TYPE_NORMAL
- en: Pablo Galindo <pablogsal at python.org>, Batuhan Taskaya <batuhan at python.org>,
    Lysandros Nikolaou <lisandrosnik at gmail.com>, Marta Gómez Macías <cyberwitch
    at google.com>
  id: totrans-split-8
  prefs: []
  type: TYPE_NORMAL
- en: 'Discussions-To:'
  id: totrans-split-9
  prefs: []
  type: TYPE_NORMAL
- en: '[Discourse thread](https://discuss.python.org/t/pep-701-syntactic-formalization-of-f-strings/22046)'
  id: totrans-split-10
  prefs: []
  type: TYPE_NORMAL
- en: 'Status:'
  id: totrans-split-11
  prefs: []
  type: TYPE_NORMAL
- en: Accepted
  id: totrans-split-12
  prefs: []
  type: TYPE_NORMAL
- en: 'Type:'
  id: totrans-split-13
  prefs: []
  type: TYPE_NORMAL
- en: Standards Track
  id: totrans-split-14
  prefs: []
  type: TYPE_NORMAL
- en: 'Created:'
  id: totrans-split-15
  prefs: []
  type: TYPE_NORMAL
- en: 15-Nov-2022
  id: totrans-split-16
  prefs: []
  type: TYPE_NORMAL
- en: 'Python-Version:'
  id: totrans-split-17
  prefs: []
  type: TYPE_NORMAL
- en: '3.12'
  id: totrans-split-18
  prefs: []
  type: TYPE_NORMAL
- en: 'Post-History:'
  id: totrans-split-19
  prefs: []
  type: TYPE_NORMAL
- en: '[19-Dec-2022](https://discuss.python.org/t/pep-701-syntactic-formalization-of-f-strings/22046
    "Discourse thread")'
  id: totrans-split-20
  prefs: []
  type: TYPE_NORMAL
- en: 'Resolution:'
  id: totrans-split-21
  prefs: []
  type: TYPE_NORMAL
- en: '[Discourse message](https://discuss.python.org/t/pep-701-syntactic-formalization-of-f-strings/22046/119)'
  id: totrans-split-22
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  id: totrans-split-23
  prefs: []
  type: TYPE_NORMAL
- en: This document proposes to lift some of the restrictions originally formulated
    in [PEP 498](../pep-0498/ "PEP 498 – Literal String Interpolation") and to provide
    a formalized grammar for f-strings that can be integrated into the parser directly.
    The proposed syntactic formalization of f-strings will have some small side-effects
    on how f-strings are parsed and interpreted, allowing for a considerable number
    of advantages for end users and library developers, while also dramatically reducing
    the maintenance cost of the code dedicated to parsing f-strings.
  id: totrans-split-24
  prefs: []
  type: TYPE_NORMAL
- en: 'When f-strings were originally introduced in [PEP 498](../pep-0498/ "PEP 498
    – Literal String Interpolation"), the specification was provided without providing
    a formal grammar for f-strings. Additionally, the specification contains several
    restrictions that are imposed so the parsing of f-strings could be implemented
    into CPython without modifying the existing lexer. These limitations have been
    recognized previously and previous attempts have been made to lift them in [PEP
    536](../pep-0536/ "PEP 536 – Final Grammar for Literal String Interpolation"),
    but [none of this work was ever implemented](https://mail.python.org/archives/list/python-dev@python.org/thread/N43O4KNLZW4U7YZC4NVPCETZIVRDUVU2/#NM2A37THVIXXEYR4J5ZPTNLXGGUNFRLZ).
    Some of these limitations (collected originally by [PEP 536](../pep-0536/ "PEP
    536 – Final Grammar for Literal String Interpolation")) are:'
  id: totrans-split-25
  prefs: []
  type: TYPE_NORMAL
- en: 'It is impossible to use the quote character delimiting the f-string within
    the expression portion:'
  id: totrans-split-26
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  id: totrans-split-27
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'A previously considered way around it would lead to escape sequences in executed
    code and is prohibited in f-strings:'
  id: totrans-split-28
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  id: totrans-split-29
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Comments are forbidden even in multi-line f-strings:'
  id: totrans-split-30
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  id: totrans-split-31
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Arbitrary nesting of expressions without expansion of escape sequences is available
    in many other languages that employ a string interpolation method that uses expressions
    instead of just variable names. Some examples:'
  id: totrans-split-32
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  id: totrans-split-33
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: These limitations serve no purpose from a language user perspective and can
    be lifted by giving f-string literals a regular grammar without exceptions and
    implementing it using dedicated parse code.
  id: totrans-split-34
  prefs: []
  type: TYPE_NORMAL
- en: 'The other issue that f-strings have is that the current implementation in CPython
    relies on tokenising f-strings as `STRING` tokens and a post processing of these
    tokens. This has the following problems:'
  id: totrans-split-35
  prefs: []
  type: TYPE_NORMAL
- en: It adds a considerable maintenance cost to the CPython parser. This is because
    the parsing code needs to be written by hand, which has historically led to a
    considerable number of inconsistencies and bugs. Writing and maintaining parsing
    code by hand in C has always been considered error prone and dangerous as it needs
    to deal with a lot of manual memory management over the original lexer buffers.
  id: totrans-split-36
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The f-string parsing code is not able to use the new improved error message
    mechanisms that the new PEG parser, originally introduced in [PEP 617](../pep-0617/
    "PEP 617 – New PEG parser for CPython"), has allowed. The improvements that these
    error messages brought has been greatly celebrated but unfortunately f-strings
    cannot benefit from them because they are parsed in a separate piece of the parsing
    machinery. This is especially unfortunate, since there are several syntactical
    features of f-strings that can be confusing due to the different implicit tokenization
    that happens inside the expression part (for instance `f"{y:=3}"` is not an assignment
    expression).
  id: totrans-split-37
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Other Python implementations have no way to know if they have implemented f-strings
    correctly because contrary to other language features, they are not part of the
    [official Python grammar](https://docs.python.org/3/reference/lexical_analysis.html#f-strings
    "(in Python v3.12)"). This is important because several prominent alternative
    implementations are using CPython’s PEG parser, [such as PyPy](https://foss.heptapod.net/pypy/pypy/-/commit/fe120f89bf07e64a41de62b224e4a3d80e0fe0d4/pipelines?ref=branch%2Fpy3.9),
    and/or are basing their grammars on the official PEG grammar. The fact that f-strings
    use a separate parser prevents these alternative implementations from leveraging
    the official grammar and benefiting from improvements in error messages derived
    from the grammar.
  id: totrans-split-38
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A version of this proposal was originally [discussed on Python-Dev](https://mail.python.org/archives/list/python-dev@python.org/thread/54N3MOYVBDSJQZTU6MTCPLUPIFSDN5IS/#SAYU6SMP4KT7G7AQ6WVQYUDOSZPKHJMS)
    and [presented at the Python Language Summit 2022](https://pyfound.blogspot.com/2022/05/the-2022-python-language-summit-f.html)
    where it was enthusiastically received.
  id: totrans-split-39
  prefs: []
  type: TYPE_NORMAL
- en: 'By building on top of the new Python PEG Parser ([PEP 617](../pep-0617/ "PEP
    617 – New PEG parser for CPython")), this PEP proposes to redefine “f-strings”,
    especially emphasizing the clear separation of the string component and the expression
    (or replacement, `{...}`) component. [PEP 498](../pep-0498/ "PEP 498 – Literal
    String Interpolation") summarizes the syntactical part of “f-strings” as the following:'
  id: totrans-split-40
  prefs: []
  type: TYPE_NORMAL
- en: In Python source code, an f-string is a literal string, prefixed with ‘f’, which
    contains expressions inside braces. The expressions are replaced with their values.
  id: totrans-split-41
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: However, [PEP 498](../pep-0498/ "PEP 498 – Literal String Interpolation") also
    contained a formal list of exclusions on what can or cannot be contained inside
    the expression component (primarily due to the limitations of the existing parser).
    By clearly establishing the formal grammar, we now also have the ability to define
    the expression component of an f-string as truly “any applicable Python expression”
    (in that particular context) without being bound by the limitations imposed by
    the details of our implementation.
  id: totrans-split-42
  prefs: []
  type: TYPE_NORMAL
- en: The formalization effort and the premise above also has a significant benefit
    for Python programmers due to its ability to simplify and eliminate the obscure
    limitations. This reduces the mental burden and the cognitive complexity of f-string
    literals (as well as the Python language in general).
  id: totrans-split-43
  prefs: []
  type: TYPE_NORMAL
- en: 'The expression component can include any string literal that a normal Python
    expression can include. This opens up the possibility of nesting string literals
    (formatted or not) inside the expression component of an f-string with the same
    quote type (and length):'
  id: totrans-split-44
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  id: totrans-split-45
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: This “feature” is not universally agreed to be desirable, and some users find
    this unreadable. For a discussion on the different views on this, see the [considerations
    regarding quote reuse](#considerations-regarding-quote-reuse) section.
  id: totrans-split-46
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Another issue that has felt unintuitive to most is the lack of support for
    backslashes within the expression component of an f-string. One example that keeps
    coming up is including a newline character in the expression part for joining
    containers. For example:'
  id: totrans-split-47
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  id: totrans-split-48
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'A common work-around for this was to either assign the newline to an intermediate
    variable or pre-create the whole string prior to creating the f-string:'
  id: totrans-split-49
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE6]'
  id: totrans-split-50
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: It only feels natural to allow backslashes in the expression part now that the
    new PEG parser can easily support it.
  id: totrans-split-51
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE7]'
  id: totrans-split-52
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Before the changes proposed in this document, there was no explicit limit in
    how f-strings can be nested, but the fact that string quotes cannot be reused
    inside the expression component of f-strings made it impossible to nest f-strings
    arbitrarily. In fact, this is the most nested-fstring that can be written:'
  id: totrans-split-53
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  id: totrans-split-54
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'As this PEP allows placing **any** valid Python expression inside the expression
    component of the f-strings, it is now possible to reuse quotes and therefore is
    possible to nest f-strings arbitrarily:'
  id: totrans-split-55
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE9]'
  id: totrans-split-56
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Although this is just a consequence of allowing arbitrary expressions, the authors
    of this PEP do not believe that this is a fundamental benefit and we have decided
    that the language specification will not explicitly mandate that this nesting
    can be arbitrary. This is because allowing arbitrarily-deep nesting imposes a
    lot of extra complexity to the lexer implementation (particularly as lexer/parser
    pipelines need to allow “untokenizing” to support the ‘f-string debugging expressions’
    and this is especially taxing when arbitrary nesting is allowed). Implementations
    are therefore free to impose a limit on the nesting depth if they need to. Note
    that this is not an uncommon situation, as the CPython implementation already
    imposes several limits all over the place, including a limit on the nesting depth
    of parentheses and brackets, a limit on the nesting of the blocks, a limit in
    the number of branches in `if` statements, a limit on the number of expressions
    in star-unpacking, etc.
  id: totrans-split-57
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'The formal proposed PEG grammar specification for f-strings is (see [PEP 617](../pep-0617/
    "PEP 617 – New PEG parser for CPython") for details on the syntax):'
  id: totrans-split-58
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  id: totrans-split-59
  prefs: []
  type: TYPE_PRE
- en: The new tokens (`FSTRING_START`, `FSTRING_MIDDLE`, `FSTRING_END`) are defined
    [later in this document](#new-tokens).
  id: totrans-split-60
  prefs: []
  type: TYPE_NORMAL
- en: This PEP leaves up to the implementation the level of f-string nesting allowed
    (f-strings within the expression parts of other f-strings) but **specifies a lower
    bound of 5 levels of nesting**. This is to ensure that users can have a reasonable
    expectation of being able to nest f-strings with “reasonable” depth. This PEP
    implies that limiting nesting is **not part of the language specification** but
    also the language specification **doesn’t mandate arbitrary nesting**.
  id: totrans-split-61
  prefs: []
  type: TYPE_NORMAL
- en: 'Similarly, this PEP leaves up to the implementation the level of expression
    nesting in format specifiers but **specifies a lower bound of 2 levels of nesting**.
    This means that the following should always be valid:'
  id: totrans-split-62
  prefs: []
  type: TYPE_NORMAL
- en: 'but the following can be valid or not depending on the implementation:'
  id: totrans-split-63
  prefs: []
  type: TYPE_NORMAL
- en: The new grammar will preserve the Abstract Syntax Tree (AST) of the current
    implementation. This means that no semantic changes will be introduced by this
    PEP on existing code that uses f-strings.
  id: totrans-split-64
  prefs: []
  type: TYPE_NORMAL
- en: 'Since Python 3.8, f-strings can be used to debug expressions by using the `=`
    operator. For example:'
  id: totrans-split-65
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  id: totrans-split-66
  prefs: []
  type: TYPE_PRE
- en: This semantics were not introduced formally in a PEP and they were implemented
    in the current string parser as a special case in [bpo-36817](https://bugs.python.org/issue?@action=redirect&bpo=36817)
    and documented in [the f-string lexical analysis section](https://docs.python.org/3/reference/lexical_analysis.html#f-strings).
  id: totrans-split-67
  prefs: []
  type: TYPE_NORMAL
- en: This feature is not affected by the changes proposed in this PEP but is important
    to specify that the formal handling of this feature requires the lexer to be able
    to “untokenize” the expression part of the f-string. This is not a problem for
    the current string parser as it can operate directly on the string token contents.
    However, incorporating this feature into a given parser implementation requires
    the lexer to keep track of the raw string contents of the expression part of the
    f-string and make them available to the parser when the parse tree is constructed
    for f-string nodes. A pure “untokenization” is not enough because as specified
    currently, f-string debug expressions preserve whitespace in the expression, including
    spaces after the `{` and the `=` characters. This means that the raw string contents
    of the expression part of the f-string must be kept intact and not just the associated
    tokens.
  id: totrans-split-68
  prefs: []
  type: TYPE_NORMAL
- en: How parser/lexer implementations deal with this problem is of course up to the
    implementation.
  id: totrans-split-69
  prefs: []
  type: TYPE_NORMAL
- en: 'Three new tokens are introduced: `FSTRING_START`, `FSTRING_MIDDLE` and `FSTRING_END`.
    Different lexers may have different implementations that may be more efficient
    than the ones proposed here given the context of the particular implementation.
    However, the following definitions will be used as part of the public APIs of
    CPython (such as the `tokenize` module) and are also provided as a reference so
    that the reader can have a better understanding of the proposed grammar changes
    and how the tokens are used:'
  id: totrans-split-70
  prefs: []
  type: TYPE_NORMAL
- en: '`FSTRING_START`: This token includes the f-string prefix (`f`/`F`/`fr`) and
    the opening quote(s).'
  id: totrans-split-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`FSTRING_MIDDLE`: This token includes a portion of text inside the string that’s
    not part of the expression part and isn’t an opening or closing brace. This can
    include the text between the opening quote and the first expression brace (`{`),
    the text between two expression braces (`}` and `{`) and the text between the
    last expression brace (`}`) and the closing quote.'
  id: totrans-split-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`FSTRING_END`: This token includes the closing quote.'
  id: totrans-split-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These tokens are always string parts and they are semantically equivalent to
    the `STRING` token with the restrictions specified. These tokens must be produced
    by the lexer when lexing f-strings. This means that **the tokenizer cannot produce
    a single token for f-strings anymore**. How the lexer emits this token is **not
    specified** as this will heavily depend on every implementation (even the Python
    version of the lexer in the standard library is implemented differently to the
    one used by the PEG parser).
  id: totrans-split-74
  prefs: []
  type: TYPE_NORMAL
- en: 'As an example:'
  id: totrans-split-75
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  id: totrans-split-76
  prefs: []
  type: TYPE_PRE
- en: 'will be tokenized as:'
  id: totrans-split-77
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  id: totrans-split-78
  prefs: []
  type: TYPE_PRE
- en: 'while `f"""some words"""` will be tokenized simply as:'
  id: totrans-split-79
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  id: totrans-split-80
  prefs: []
  type: TYPE_PRE
- en: The [`tokenize`](https://docs.python.org/3/library/tokenize.html#module-tokenize
    "(in Python v3.12)") module will be adapted to emit these tokens as described
    in the previous section when parsing f-strings so tools can take advantage of
    this new tokenization schema and avoid having to implement their own f-string
    tokenizer and parser.
  id: totrans-split-81
  prefs: []
  type: TYPE_NORMAL
- en: 'One way existing lexers can be adapted to emit these tokens is to incorporate
    a stack of “lexer modes” or to use a stack of different lexers. This is because
    the lexer needs to switch from “regular Python lexing” to “f-string lexing” when
    it encounters an f-string start token and as f-strings can be nested, the context
    needs to be preserved until the f-string closes. Also, the “lexer mode” inside
    an f-string expression part needs to behave as a “super-set” of the regular Python
    lexer (as it needs to be able to switch back to f-string lexing when it encounters
    the `}` terminator for the expression part as well as handling f-string formatting
    and debug expressions). For reference, here is a draft of the algorithm to modify
    a CPython-like tokenizer to emit these new tokens:'
  id: totrans-split-82
  prefs: []
  type: TYPE_NORMAL
- en: If the lexer detects that an f-string is starting (by detecting the letter ‘f/F’
    and one of the possible quotes) keep advancing until a valid quote is detected
    (one of `"`, `"""`, `'` or `'''`) and emit a `FSTRING_START` token with the contents
    captured (the ‘f/F’ and the starting quote). Push a new tokenizer mode to the
    tokenizer mode stack for “F-string tokenization”. Go to step 2.
  id: totrans-split-83
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Keep consuming tokens until a one of the following is encountered:'
  id: totrans-split-84
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A closing quote equal to the opening quote.
  id: totrans-split-85
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: If in “format specifier mode” (see step 3), an opening brace (`{`), a closing
    brace (`}`), or a newline token (`\n`).
  id: totrans-split-86
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: If not in “format specifier mode” (see step 3), an opening brace (`{`) or a
    closing brace (`}`) that is not immediately followed by another opening/closing
    brace.
  id: totrans-split-87
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In all cases, if the character buffer is not empty, emit a `FSTRING_MIDDLE`
    token with the contents captured so far but transform any double opening/closing
    braces into single opening/closing braces. Now, proceed as follows depending on
    the character encountered:'
  id: totrans-split-88
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: If a closing quote matching the opening quite is encountered go to step 4.
  id: totrans-split-89
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: If an opening bracket (not immediately followed by another opening bracket)
    is encountered, go to step 3.
  id: totrans-split-90
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: If a closing bracket (not immediately followed by another closing bracket) is
    encountered, emit a token for the closing bracket and go to step 2.
  id: totrans-split-91
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Push a new tokenizer mode to the tokenizer mode stack for “Regular Python tokenization
    within f-string” and proceed to tokenize with it. This mode tokenizes as the “Regular
    Python tokenization” until a `:` or a `}` character is encountered with the same
    level of nesting as the opening bracket token that was pushed when we enter the
    f-string part. Using this mode, emit tokens until one of the stop points are reached.
    When this happens, emit the corresponding token for the stopping character encountered
    and, pop the current tokenizer mode from the tokenizer mode stack and go to step
    2\. If the stopping point is a `:` character, enter step 2 in “format specifier”
    mode.
  id: totrans-split-92
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Emit a `FSTRING_END` token with the contents captured and pop the current tokenizer
    mode (corresponding to “F-string tokenization”) and go back to “Regular Python
    mode”.
  id: totrans-split-93
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Of course, as mentioned before, it is not possible to provide a precise specification
    of how this should be done for an arbitrary tokenizer as it will depend on the
    specific implementation and nature of the lexer to be changed.
  id: totrans-split-94
  prefs: []
  type: TYPE_NORMAL
- en: 'All restrictions mentioned in the PEP are lifted from f-string literals, as
    explained below:'
  id: totrans-split-95
  prefs: []
  type: TYPE_NORMAL
- en: Expression portions may now contain strings delimited with the same kind of
    quote that is used to delimit the f-string literal.
  id: totrans-split-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Backslashes may now appear within expressions just like anywhere else in Python
    code. In case of strings nested within f-string literals, escape sequences are
    expanded when the innermost string is evaluated.
  id: totrans-split-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'New lines are now allowed within expression brackets. This means that these
    are now allowed:'
  id: totrans-split-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE15]'
  id: totrans-split-99
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Comments, using the `#` character, are allowed within the expression part of
    an f-string. Note that comments require that the closing bracket (`}`) of the
    expression part to be present in a different line as the one the comment is in
    or otherwise it will be ignored as part of the comment.
  id: totrans-split-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'One of the consequences of the grammar proposed here is that, as mentioned
    above, f-string expressions can now contain strings delimited with the same kind
    of quote that is used to delimit the external f-string literal. For example:'
  id: totrans-split-101
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  id: totrans-split-102
  prefs: []
  type: TYPE_PRE
- en: In the [discussion thread for this PEP](https://discuss.python.org/t/pep-701-syntactic-formalization-of-f-strings/22046),
    several concerns have been raised regarding this aspect and we want to collect
    them here, as these should be taken into consideration when accepting or rejecting
    this PEP.
  id: totrans-split-103
  prefs: []
  type: TYPE_NORMAL
- en: 'Some of these objections include:'
  id: totrans-split-104
  prefs: []
  type: TYPE_NORMAL
- en: 'Many people find quote reuse within the same string confusing and hard to read.
    This is because allowing quote reuse will violate a current property of Python
    as it stands today: the fact that strings are fully delimited by two consecutive
    pairs of the same kind of quote, which by itself is a very simple rule. One of
    the reasons quote reuse may be harder for humans to parse, leading to less readable
    code, is that the quote character is the same for both start and end (as opposed
    to other delimiters).'
  id: totrans-split-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Some users have raised concerns that quote reuse may break some lexer and syntax
    highlighting tools that rely on simple mechanisms to detect strings and f-strings,
    such as regular expressions or simple delimiter matching tools. Introducing quote
    reuse in f-strings will either make it trickier to keep these tools working or
    will break the tools altogether (as, for instance, regular expressions cannot
    parse arbitrary nested structures with delimiters). The IDLE editor, included
    in the standard library, is an example of a tool which may need some work to correctly
    apply syntax highlighting to f-strings.
  id: totrans-split-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Here are some of the arguments in favour:'
  id: totrans-split-107
  prefs: []
  type: TYPE_NORMAL
- en: Many languages that allow similar syntactic constructs (normally called “string
    interpolation”) allow quote reuse and arbitrary nesting. These languages include
    JavaScript, Ruby, C#, Bash, Swift and many others. The fact that many languages
    allow quote reuse can be a compelling argument in favour of allowing it in Python.
    This is because it will make the language more familiar to users coming from other
    languages.
  id: totrans-split-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As many other popular languages allow quote reuse in string interpolation constructs,
    this means that editors that support syntax highlighting for these languages will
    already have the necessary tools to support syntax highlighting for f-strings
    with quote reuse in Python. This means that although the files that handle syntax
    highlighting for Python will need to be updated to support this new feature, is
    not expected to be impossible or very hard to do.
  id: totrans-split-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'One advantage of allowing quote reuse is that it composes cleanly with other
    syntax. Sometimes this is referred to as “referential transparency”. An example
    of this is that if we have `f(x+1)`, assuming `a` is a brand new variable, it
    should behave the same as `a = x+1; f(a)`. And vice versa. So if we have:'
  id: totrans-split-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE17]'
  id: totrans-split-111
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'It should be expected that if we replace the variable `prefix` with its definition,
    the answer should be the same:'
  id: totrans-split-112
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE18]'
  id: totrans-split-113
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Code generators (like [ast.unparse](https://docs.python.org/3/library/ast.html#ast.unparse)
    from standard library) in their current form rely on complicated algorithms to
    ensure expressions within an f-string are properly suited for the context in which
    they are being used. These non-trivial algorithms come with challenges such as
    finding an unused quote type (by tracking the outer quotes), and generating string
    representations which would not include backslashes if possible. Allowing quote
    reuse and backslashes would simplify the code generators which deal with f-strings
    considerably, as the regular Python expression logic can be used inside and outside
    of f-strings without any special treatment.
  id: totrans-split-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Limiting quote reuse will considerably increase the complexity of the implementation
    of the proposed changes. This is because it will force the parser to have the
    context that is parsing an expression part of an f-string with a given quote in
    order to know if it needs to reject an expression that reuses the quote. Carrying
    this context around is not trivial in parsers that can backtrack arbitrarily (such
    as the PEG parser). The issue becomes even more complex if we consider that f-strings
    can be arbitrarily nested and therefore several quote types may need to be rejected.
  id: totrans-split-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To gather feedback from the community, [a poll](https://discuss.python.org/t/pep-701-syntactic-formalization-of-f-strings/22046/24)
    has been initiated to get a sense of how the community feels about this aspect
    of the PEP.
  id: totrans-split-116
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: This PEP does not introduce any backwards incompatible syntactic or semantic
    changes to the Python language. However, the [`tokenize`](https://docs.python.org/3/library/tokenize.html#module-tokenize
    "(in Python v3.12)") module (a quasi-public part of the standard library) will
    need to be updated to support the new f-string tokens (to allow tool authors to
    correctly tokenize f-strings). See [changes to the tokenize module](#changes-to-the-tokenize-module)
    for more details regarding how the public API of `tokenize` will be affected.
  id: totrans-split-117
  prefs: []
  type: TYPE_NORMAL
- en: As the concept of f-strings is already ubiquitous in the Python community, there
    is no fundamental need for users to learn anything new. However, as the formalized
    grammar allows some new possibilities, it is important that the formal grammar
    is added to the documentation and explained in detail, explicitly mentioning what
    constructs are possible since this PEP is aiming to avoid confusion.
  id: totrans-split-118
  prefs: []
  type: TYPE_NORMAL
- en: 'It is also beneficial to provide users with a simple framework for understanding
    what can be placed inside an f-string expression. In this case the authors think
    that this work will make it even simpler to explain this aspect of the language,
    since it can be summarized as:'
  id: totrans-split-119
  prefs: []
  type: TYPE_NORMAL
- en: You can place any valid Python expression inside an f-string expression.
  id: totrans-split-120
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'With the changes in this PEP, there is no need to clarify that string quotes
    are limited to be different from the quotes of the enclosing string, because this
    is now allowed: as an arbitrary Python string can contain any possible choice
    of quotes, so can any f-string expression. Additionally there is no need to clarify
    that certain things are not allowed in the expression part because of implementation
    restrictions such as comments, new line characters or backslashes.'
  id: totrans-split-121
  prefs: []
  type: TYPE_NORMAL
- en: 'The only “surprising” difference is that as f-strings allow specifying a format,
    expressions that allow a `:` character at the top level still need to be enclosed
    in parenthesis. This is not new to this work, but it is important to emphasize
    that this restriction is still in place. This allows for an easier modification
    of the summary:'
  id: totrans-split-122
  prefs: []
  type: TYPE_NORMAL
- en: You can place any valid Python expression inside an f-string expression, and
    everything after a `:` character at the top level will be identified as a format
    specification.
  id: totrans-split-123
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Although we think the readability arguments that have been raised against allowing
    quote reuse in f-string expressions are valid and very important, we have decided
    to propose not rejecting quote reuse in f-strings at the parser level. The reason
    is that one of the cornerstones of this PEP is to reduce the complexity and maintenance
    of parsing f-strings in CPython and this will not only work against that goal,
    but it may even make the implementation even more complex than the current one.
    We believe that forbidding quote reuse should be done in linters and code style
    tools and not in the parser, the same way other confusing or hard-to-read constructs
    in the language are handled today.
  id: totrans-split-124
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'We have decided not to lift the restriction that some expression portions need
    to wrap `'':''` and `''!''` in parentheses at the top level, e.g.:'
  id: totrans-split-125
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  id: totrans-split-126
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The reason is that this will introduce a considerable amount of complexity for
    no real benefit. This is due to the fact that the `:` character normally separates
    the f-string format specification. This format specification is currently tokenized
    as a string. As the tokenizer MUST tokenize what’s on the right of the `:` as
    either a string or a stream of tokens, this won’t allow the parser to differentiate
    between the different semantics as that would require the tokenizer to backtrack
    and produce a different set of tokens (this is, first try as a stream of tokens,
    and if it fails, try as a string for a format specifier).
  id: totrans-split-127
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'As there is no fundamental advantage in being able to allow lambdas and similar
    expressions at the top level, we have decided to keep the restriction that these
    must be parenthesized if needed:'
  id: totrans-split-128
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE20]'
  id: totrans-split-129
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: We have decided to disallow (for the time being) using escaped braces (`\{`
    and `\}`) in addition to the `{{` and `}}` syntax. Although the authors of the
    PEP believe that allowing escaped braces is a good idea, we have decided to not
    include it in this PEP, as it is not strictly necessary for the formalization
    of f-strings proposed here, and it can be added independently in a regular CPython
    issue.
  id: totrans-split-130
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: This document is placed in the public domain or under the CC0-1.0-Universal
    license, whichever is more permissive.
  id: totrans-split-131
  prefs: []
  type: TYPE_NORMAL
