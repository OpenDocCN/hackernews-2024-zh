- en: <!--yml
  id: totrans-split-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-split-1
  prefs: []
  type: TYPE_NORMAL
  zh: 类别：未分类
- en: 'date: 2024-05-27 14:50:18'
  id: totrans-split-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024年05月27日 14:50:18
- en: -->
  id: totrans-split-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: PEP 701 – Syntactic formalization of f-strings | peps.python.org
  id: totrans-split-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: PEP 701 – f-string 语法形式化 | peps.python.org
- en: 来源：[https://peps.python.org/pep-0701/](https://peps.python.org/pep-0701/)
  id: totrans-split-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://peps.python.org/pep-0701/](https://peps.python.org/pep-0701/)
- en: PEP 701 – Syntactic formalization of f-strings
  id: totrans-split-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: PEP 701 – f-string 语法形式化
- en: 'Author:'
  id: totrans-split-7
  prefs: []
  type: TYPE_NORMAL
  zh: 作者：
- en: Pablo Galindo <pablogsal at python.org>, Batuhan Taskaya <batuhan at python.org>,
    Lysandros Nikolaou <lisandrosnik at gmail.com>, Marta Gómez Macías <cyberwitch
    at google.com>
  id: totrans-split-8
  prefs: []
  type: TYPE_NORMAL
  zh: Pablo Galindo <pablogsal at python.org>, Batuhan Taskaya <batuhan at python.org>,
    Lysandros Nikolaou <lisandrosnik at gmail.com>, Marta Gómez Macías <cyberwitch
    at google.com>
- en: 'Discussions-To:'
  id: totrans-split-9
  prefs: []
  type: TYPE_NORMAL
  zh: 讨论至：
- en: '[Discourse thread](https://discuss.python.org/t/pep-701-syntactic-formalization-of-f-strings/22046)'
  id: totrans-split-10
  prefs: []
  type: TYPE_NORMAL
  zh: '[讨论线程](https://discuss.python.org/t/pep-701-syntactic-formalization-of-f-strings/22046)'
- en: 'Status:'
  id: totrans-split-11
  prefs: []
  type: TYPE_NORMAL
  zh: 状态：
- en: Accepted
  id: totrans-split-12
  prefs: []
  type: TYPE_NORMAL
  zh: 已接受
- en: 'Type:'
  id: totrans-split-13
  prefs: []
  type: TYPE_NORMAL
  zh: 类型：
- en: Standards Track
  id: totrans-split-14
  prefs: []
  type: TYPE_NORMAL
  zh: 标准轨道
- en: 'Created:'
  id: totrans-split-15
  prefs: []
  type: TYPE_NORMAL
  zh: 创建日期：
- en: 15-Nov-2022
  id: totrans-split-16
  prefs: []
  type: TYPE_NORMAL
  zh: 2022年11月15日
- en: 'Python-Version:'
  id: totrans-split-17
  prefs: []
  type: TYPE_NORMAL
  zh: Python 版本：
- en: '3.12'
  id: totrans-split-18
  prefs: []
  type: TYPE_NORMAL
  zh: '3.12'
- en: 'Post-History:'
  id: totrans-split-19
  prefs: []
  type: TYPE_NORMAL
  zh: 发布历史：
- en: '[19-Dec-2022](https://discuss.python.org/t/pep-701-syntactic-formalization-of-f-strings/22046
    "Discourse thread")'
  id: totrans-split-20
  prefs: []
  type: TYPE_NORMAL
  zh: '[2022年12月19日](https://discuss.python.org/t/pep-701-syntactic-formalization-of-f-strings/22046
    "讨论线程")'
- en: 'Resolution:'
  id: totrans-split-21
  prefs: []
  type: TYPE_NORMAL
  zh: 决议：
- en: '[Discourse message](https://discuss.python.org/t/pep-701-syntactic-formalization-of-f-strings/22046/119)'
  id: totrans-split-22
  prefs: []
  type: TYPE_NORMAL
  zh: '[讨论消息](https://discuss.python.org/t/pep-701-syntactic-formalization-of-f-strings/22046/119)'
- en: '* * *'
  id: totrans-split-23
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: This document proposes to lift some of the restrictions originally formulated
    in [PEP 498](../pep-0498/ "PEP 498 – Literal String Interpolation") and to provide
    a formalized grammar for f-strings that can be integrated into the parser directly.
    The proposed syntactic formalization of f-strings will have some small side-effects
    on how f-strings are parsed and interpreted, allowing for a considerable number
    of advantages for end users and library developers, while also dramatically reducing
    the maintenance cost of the code dedicated to parsing f-strings.
  id: totrans-split-24
  prefs: []
  type: TYPE_NORMAL
  zh: 本文提议放宽最初在[PEP 498](../pep-0498/ "PEP 498 – Literal String Interpolation")中制定的一些限制，并为可以直接集成到解析器中的
    f-string 提供正式的语法。所提议的 f-string 语法形式化将在解析和解释 f-string 方面产生一些小的副作用，为终端用户和库开发人员提供了相当多的优势，同时显著降低了专用于解析
    f-string 的代码的维护成本。
- en: 'When f-strings were originally introduced in [PEP 498](../pep-0498/ "PEP 498
    – Literal String Interpolation"), the specification was provided without providing
    a formal grammar for f-strings. Additionally, the specification contains several
    restrictions that are imposed so the parsing of f-strings could be implemented
    into CPython without modifying the existing lexer. These limitations have been
    recognized previously and previous attempts have been made to lift them in [PEP
    536](../pep-0536/ "PEP 536 – Final Grammar for Literal String Interpolation"),
    but [none of this work was ever implemented](https://mail.python.org/archives/list/python-dev@python.org/thread/N43O4KNLZW4U7YZC4NVPCETZIVRDUVU2/#NM2A37THVIXXEYR4J5ZPTNLXGGUNFRLZ).
    Some of these limitations (collected originally by [PEP 536](../pep-0536/ "PEP
    536 – Final Grammar for Literal String Interpolation")) are:'
  id: totrans-split-25
  prefs: []
  type: TYPE_NORMAL
  zh: 当最初在[PEP 498](../pep-0498/ "PEP 498 – Literal String Interpolation")中引入 f-string
    时，并未提供 f-string 的正式语法。此外，规范包含几个限制，以便可以将 f-string 的解析实现到 CPython 中，而无需修改现有的词法分析器。这些限制先前已被认识到，并曾尝试在[PEP
    536](../pep-0536/ "PEP 536 – Final Grammar for Literal String Interpolation")中取消，但是[这些工作从未得到实施](https://mail.python.org/archives/list/python-dev@python.org/thread/N43O4KNLZW4U7YZC4NVPCETZIVRDUVU2/#NM2A37THVIXXEYR4J5ZPTNLXGGUNFRLZ)。其中一些限制（最初由[PEP
    536](../pep-0536/ "PEP 536 – Final Grammar for Literal String Interpolation")收集）包括：
- en: 'It is impossible to use the quote character delimiting the f-string within
    the expression portion:'
  id: totrans-split-26
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 不可能在表达式部分内部使用引号字符来界定 f-string：
- en: '[PRE0]'
  id: totrans-split-27
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'A previously considered way around it would lead to escape sequences in executed
    code and is prohibited in f-strings:'
  id: totrans-split-28
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 以前考虑的一个绕过方法会导致执行代码中的转义序列，并且在 f-string 中被禁止使用：
- en: '[PRE1]'
  id: totrans-split-29
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Comments are forbidden even in multi-line f-strings:'
  id: totrans-split-30
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 即使在多行 f-string 中也禁止评论：
- en: '[PRE2]'
  id: totrans-split-31
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Arbitrary nesting of expressions without expansion of escape sequences is available
    in many other languages that employ a string interpolation method that uses expressions
    instead of just variable names. Some examples:'
  id: totrans-split-32
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 许多其他语言中都有可用的表达式任意嵌套而不扩展转义序列的字符串插值方法。例如：
- en: '[PRE3]'
  id: totrans-split-33
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: These limitations serve no purpose from a language user perspective and can
    be lifted by giving f-string literals a regular grammar without exceptions and
    implementing it using dedicated parse code.
  id: totrans-split-34
  prefs: []
  type: TYPE_NORMAL
  zh: 从语言用户的角度看，这些限制毫无意义，并且可以通过给 f-string 文字一个没有例外的常规语法并使用专用解析代码来取消这些限制。
- en: 'The other issue that f-strings have is that the current implementation in CPython
    relies on tokenising f-strings as `STRING` tokens and a post processing of these
    tokens. This has the following problems:'
  id: totrans-split-35
  prefs: []
  type: TYPE_NORMAL
  zh: f-string的另一个问题是，当前CPython的实现依赖于将f-string标记为`STRING`标记，并对这些标记进行后处理。这带来了以下问题：
- en: It adds a considerable maintenance cost to the CPython parser. This is because
    the parsing code needs to be written by hand, which has historically led to a
    considerable number of inconsistencies and bugs. Writing and maintaining parsing
    code by hand in C has always been considered error prone and dangerous as it needs
    to deal with a lot of manual memory management over the original lexer buffers.
  id: totrans-split-36
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 它给CPython解析器增加了相当大的维护成本。这是因为解析代码需要手动编写，这在历史上导致了大量的不一致性和错误。在C中手动编写和维护解析代码一直被认为是容易出错和危险的，因为它需要处理原始词法分析器缓冲区的大量手动内存管理。
- en: The f-string parsing code is not able to use the new improved error message
    mechanisms that the new PEG parser, originally introduced in [PEP 617](../pep-0617/
    "PEP 617 – New PEG parser for CPython"), has allowed. The improvements that these
    error messages brought has been greatly celebrated but unfortunately f-strings
    cannot benefit from them because they are parsed in a separate piece of the parsing
    machinery. This is especially unfortunate, since there are several syntactical
    features of f-strings that can be confusing due to the different implicit tokenization
    that happens inside the expression part (for instance `f"{y:=3}"` is not an assignment
    expression).
  id: totrans-split-37
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: f-string解析代码无法使用新的改进的错误消息机制，这些机制最初在[PEP 617](../pep-0617/ "PEP 617 – CPython的新PEG解析器")中引入，为此感到非常高兴。这些错误消息的改进受到了极大的欢迎，但不幸的是，f-string无法从中受益，因为它们在解析机制的一个独立部分中解析。这特别令人遗憾，因为f-string的几个语法特性可能会因为表达式部分内部发生的不同隐式标记化而令人困惑（例如`f"{y:=3}"`不是一个赋值表达式）。
- en: Other Python implementations have no way to know if they have implemented f-strings
    correctly because contrary to other language features, they are not part of the
    [official Python grammar](https://docs.python.org/3/reference/lexical_analysis.html#f-strings
    "(in Python v3.12)"). This is important because several prominent alternative
    implementations are using CPython’s PEG parser, [such as PyPy](https://foss.heptapod.net/pypy/pypy/-/commit/fe120f89bf07e64a41de62b224e4a3d80e0fe0d4/pipelines?ref=branch%2Fpy3.9),
    and/or are basing their grammars on the official PEG grammar. The fact that f-strings
    use a separate parser prevents these alternative implementations from leveraging
    the official grammar and benefiting from improvements in error messages derived
    from the grammar.
  id: totrans-split-38
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 其他Python实现无法知道他们是否正确实现了f-string，因为与其他语言特性相反，它们不是[官方Python语法](https://docs.python.org/3/reference/lexical_analysis.html#f-strings
    "(在Python v3.12)")的一部分。这一点很重要，因为几个知名的替代实现正在使用CPython的PEG解析器，[如PyPy](https://foss.heptapod.net/pypy/pypy/-/commit/fe120f89bf07e64a41de62b224e4a3d80e0fe0d4/pipelines?ref=branch%2Fpy3.9)，和/或者基于官方PEG语法构建他们的语法。由于f-string使用一个单独的解析器，这些替代实现无法利用官方语法和从语法中得到的错误消息的改进。
- en: A version of this proposal was originally [discussed on Python-Dev](https://mail.python.org/archives/list/python-dev@python.org/thread/54N3MOYVBDSJQZTU6MTCPLUPIFSDN5IS/#SAYU6SMP4KT7G7AQ6WVQYUDOSZPKHJMS)
    and [presented at the Python Language Summit 2022](https://pyfound.blogspot.com/2022/05/the-2022-python-language-summit-f.html)
    where it was enthusiastically received.
  id: totrans-split-39
  prefs: []
  type: TYPE_NORMAL
  zh: 这个提案的一个版本最初在[Python-Dev讨论过](https://mail.python.org/archives/list/python-dev@python.org/thread/54N3MOYVBDSJQZTU6MTCPLUPIFSDN5IS/#SAYU6SMP4KT7G7AQ6WVQYUDOSZPKHJMS)，并且[在2022年Python语言峰会上做过演讲](https://pyfound.blogspot.com/2022/05/the-2022-python-language-summit-f.html)，得到了热烈的反响。
- en: 'By building on top of the new Python PEG Parser ([PEP 617](../pep-0617/ "PEP
    617 – New PEG parser for CPython")), this PEP proposes to redefine “f-strings”,
    especially emphasizing the clear separation of the string component and the expression
    (or replacement, `{...}`) component. [PEP 498](../pep-0498/ "PEP 498 – Literal
    String Interpolation") summarizes the syntactical part of “f-strings” as the following:'
  id: totrans-split-40
  prefs: []
  type: TYPE_NORMAL
  zh: 通过建立在新的Python PEG解析器（[PEP 617](../pep-0617/ "PEP 617 – CPython的新PEG解析器")）之上，本PEP建议重新定义“f-strings”，特别强调字符串部分与表达式（或替换，`{...}`）部分的明确分离。[PEP
    498](../pep-0498/ "PEP 498 – 字面字符串插值")总结了“f-strings”的语法部分如下：
- en: In Python source code, an f-string is a literal string, prefixed with ‘f’, which
    contains expressions inside braces. The expressions are replaced with their values.
  id: totrans-split-41
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 在 Python 源代码中，f-string 是一个字面字符串，前缀为‘f’，其中包含大括号内的表达式。这些表达式将被它们的值替换。
- en: However, [PEP 498](../pep-0498/ "PEP 498 – Literal String Interpolation") also
    contained a formal list of exclusions on what can or cannot be contained inside
    the expression component (primarily due to the limitations of the existing parser).
    By clearly establishing the formal grammar, we now also have the ability to define
    the expression component of an f-string as truly “any applicable Python expression”
    (in that particular context) without being bound by the limitations imposed by
    the details of our implementation.
  id: totrans-split-42
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，[PEP 498](../pep-0498/ "PEP 498 – Literal String Interpolation") 也包含了一份正式的排除列表，列出了表达式组件中可以包含或不包含的内容（主要是由于现有解析器的限制）。通过清晰地建立正式的语法，我们现在还能够定义
    f-strings 的表达式组件为真正的“任何适用的 Python 表达式”（在特定上下文中），而不受我们实现细节所施加的限制束缚。
- en: The formalization effort and the premise above also has a significant benefit
    for Python programmers due to its ability to simplify and eliminate the obscure
    limitations. This reduces the mental burden and the cognitive complexity of f-string
    literals (as well as the Python language in general).
  id: totrans-split-43
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 Python 程序员来说，上述的正式化努力和前提也因其简化和消除晦涩限制的能力而带来了显著好处。这减少了 f-string 字面值的心理负担和认知复杂性（以及
    Python 语言总体上的复杂性）。
- en: 'The expression component can include any string literal that a normal Python
    expression can include. This opens up the possibility of nesting string literals
    (formatted or not) inside the expression component of an f-string with the same
    quote type (and length):'
  id: totrans-split-44
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 表达式组件可以包含任何普通 Python 表达式可以包含的字符串字面值。这打开了在 f-strings 的表达式组件中嵌套字符串字面值（格式化或非格式化）的可能性，使用相同的引号类型（及其长度）：
- en: '[PRE4]'
  id: totrans-split-45
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: This “feature” is not universally agreed to be desirable, and some users find
    this unreadable. For a discussion on the different views on this, see the [considerations
    regarding quote reuse](#considerations-regarding-quote-reuse) section.
  id: totrans-split-46
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这种“特性”并非所有人都认为是理想的，一些用户认为这种写法难以阅读。有关此问题不同观点的讨论，请参阅关于引号重用的考虑部分。
- en: 'Another issue that has felt unintuitive to most is the lack of support for
    backslashes within the expression component of an f-string. One example that keeps
    coming up is including a newline character in the expression part for joining
    containers. For example:'
  id: totrans-split-47
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 另一个大多数人觉得不直观的问题是，在 f-string 的表达式组件中不支持反斜杠。一个经常出现的例子是在表达式部分中包含换行符，以用于连接容器。例如：
- en: '[PRE5]'
  id: totrans-split-48
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'A common work-around for this was to either assign the newline to an intermediate
    variable or pre-create the whole string prior to creating the f-string:'
  id: totrans-split-49
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 对此的一个常见解决方案是将换行符分配给一个中间变量，或者在创建 f-string 之前预先创建整个字符串：
- en: '[PRE6]'
  id: totrans-split-50
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: It only feels natural to allow backslashes in the expression part now that the
    new PEG parser can easily support it.
  id: totrans-split-51
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 现在新的 PEG 解析器可以轻松支持，因此在表达部分允许使用反斜杠只是很自然的。
- en: '[PRE7]'
  id: totrans-split-52
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Before the changes proposed in this document, there was no explicit limit in
    how f-strings can be nested, but the fact that string quotes cannot be reused
    inside the expression component of f-strings made it impossible to nest f-strings
    arbitrarily. In fact, this is the most nested-fstring that can be written:'
  id: totrans-split-53
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在本文档提出的更改之前，f-strings 的嵌套方式并没有明确的限制，但事实上，由于字符串引号无法在 f-strings 的表达式组件中重复使用，导致无法任意嵌套
    f-strings。事实上，这是可以编写的最嵌套的 f-string：
- en: '[PRE8]'
  id: totrans-split-54
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'As this PEP allows placing **any** valid Python expression inside the expression
    component of the f-strings, it is now possible to reuse quotes and therefore is
    possible to nest f-strings arbitrarily:'
  id: totrans-split-55
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 正如这个 PEP 允许在 f-strings 的表达式组件中放置**任何**有效的 Python 表达式一样，现在可以重新使用引号并因此可以任意嵌套 f-strings：
- en: '[PRE9]'
  id: totrans-split-56
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Although this is just a consequence of allowing arbitrary expressions, the authors
    of this PEP do not believe that this is a fundamental benefit and we have decided
    that the language specification will not explicitly mandate that this nesting
    can be arbitrary. This is because allowing arbitrarily-deep nesting imposes a
    lot of extra complexity to the lexer implementation (particularly as lexer/parser
    pipelines need to allow “untokenizing” to support the ‘f-string debugging expressions’
    and this is especially taxing when arbitrary nesting is allowed). Implementations
    are therefore free to impose a limit on the nesting depth if they need to. Note
    that this is not an uncommon situation, as the CPython implementation already
    imposes several limits all over the place, including a limit on the nesting depth
    of parentheses and brackets, a limit on the nesting of the blocks, a limit in
    the number of branches in `if` statements, a limit on the number of expressions
    in star-unpacking, etc.
  id: totrans-split-57
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 虽然这只是允许任意表达式的一个结果，但本 PEP 的作者并不认为这是一个基本的好处，我们已经决定语言规范将不会明确规定这种嵌套可以是任意的。这是因为允许任意深度的嵌套会给词法分析器的实现带来很多额外复杂性（特别是在词法分析器/语法分析器流水线需要允许“反解标记化”以支持“f-string
    调试表达式”时，这种允许任意嵌套的情况尤为紧张）。因此，实现可以自由地对嵌套深度施加限制，如果需要的话。请注意，这并不是一个罕见的情况，因为 CPython
    实现已经在各处施加了几个限制，包括括号和方括号的嵌套深度限制，在 `if` 语句中的分支数限制，在星号解包表达式中表达式数限制等等。
- en: 'The formal proposed PEG grammar specification for f-strings is (see [PEP 617](../pep-0617/
    "PEP 617 – New PEG parser for CPython") for details on the syntax):'
  id: totrans-split-58
  prefs: []
  type: TYPE_NORMAL
  zh: f-string 的正式提议 PEG 语法规范为（详见 [PEP 617](../pep-0617/ "PEP 617 – CPython 的新 PEG
    解析器") 中的语法细节）：
- en: '[PRE10]'
  id: totrans-split-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: The new tokens (`FSTRING_START`, `FSTRING_MIDDLE`, `FSTRING_END`) are defined
    [later in this document](#new-tokens).
  id: totrans-split-60
  prefs: []
  type: TYPE_NORMAL
  zh: 新的标记（`FSTRING_START`，`FSTRING_MIDDLE`，`FSTRING_END`）在[本文档的后续部分](#new-tokens)中定义。
- en: This PEP leaves up to the implementation the level of f-string nesting allowed
    (f-strings within the expression parts of other f-strings) but **specifies a lower
    bound of 5 levels of nesting**. This is to ensure that users can have a reasonable
    expectation of being able to nest f-strings with “reasonable” depth. This PEP
    implies that limiting nesting is **not part of the language specification** but
    also the language specification **doesn’t mandate arbitrary nesting**.
  id: totrans-split-61
  prefs: []
  type: TYPE_NORMAL
  zh: 本 PEP 将 f-string 允许的嵌套级别留给实现决定（在其他 f-string 的表达式部分中使用 f-string 嵌套），但**规定嵌套层级的下限为
    5 级**。这是为了确保用户能够合理期望能够嵌套具有“合理”深度的 f-strings。这个 PEP 暗示限制嵌套**不是语言规范的一部分**，但也**不强制要求任意嵌套**。
- en: 'Similarly, this PEP leaves up to the implementation the level of expression
    nesting in format specifiers but **specifies a lower bound of 2 levels of nesting**.
    This means that the following should always be valid:'
  id: totrans-split-62
  prefs: []
  type: TYPE_NORMAL
  zh: 类似地，这个 PEP 将格式说明符中的表达式嵌套级别留给实现决定，但**规定嵌套层级的下限为 2 级**。这意味着以下内容始终应该是有效的：
- en: 'but the following can be valid or not depending on the implementation:'
  id: totrans-split-63
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，以下内容可以有效或无效，具体取决于实现：
- en: The new grammar will preserve the Abstract Syntax Tree (AST) of the current
    implementation. This means that no semantic changes will be introduced by this
    PEP on existing code that uses f-strings.
  id: totrans-split-64
  prefs: []
  type: TYPE_NORMAL
  zh: 新的语法将保留当前实现的抽象语法树（AST）。这意味着，使用 f-string 的现有代码在本 PEP 引入的改变下不会引入语义上的变化。
- en: 'Since Python 3.8, f-strings can be used to debug expressions by using the `=`
    operator. For example:'
  id: totrans-split-65
  prefs: []
  type: TYPE_NORMAL
  zh: 自 Python 3.8 起，可以使用 `=` 运算符来使用 f-string 调试表达式。例如：
- en: '[PRE11]'
  id: totrans-split-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: This semantics were not introduced formally in a PEP and they were implemented
    in the current string parser as a special case in [bpo-36817](https://bugs.python.org/issue?@action=redirect&bpo=36817)
    and documented in [the f-string lexical analysis section](https://docs.python.org/3/reference/lexical_analysis.html#f-strings).
  id: totrans-split-67
  prefs: []
  type: TYPE_NORMAL
  zh: 这些语义并未在 PEP 中正式引入，而是作为特例在当前字符串解析器中实现（[bpo-36817](https://bugs.python.org/issue?@action=redirect&bpo=36817)）并在[f-string
    词法分析部分](https://docs.python.org/3/reference/lexical_analysis.html#f-strings)中有所记录。
- en: This feature is not affected by the changes proposed in this PEP but is important
    to specify that the formal handling of this feature requires the lexer to be able
    to “untokenize” the expression part of the f-string. This is not a problem for
    the current string parser as it can operate directly on the string token contents.
    However, incorporating this feature into a given parser implementation requires
    the lexer to keep track of the raw string contents of the expression part of the
    f-string and make them available to the parser when the parse tree is constructed
    for f-string nodes. A pure “untokenization” is not enough because as specified
    currently, f-string debug expressions preserve whitespace in the expression, including
    spaces after the `{` and the `=` characters. This means that the raw string contents
    of the expression part of the f-string must be kept intact and not just the associated
    tokens.
  id: totrans-split-68
  prefs: []
  type: TYPE_NORMAL
  zh: 此功能不受本PEP中提出的更改影响，但重要的是指出，正式处理此功能要求词法分析器能够“解标记化”f-string的表达式部分。对于当前的字符串解析器来说，这不是问题，因为它可以直接操作字符串标记内容。然而，将此功能合并到给定的解析器实现中，要求词法分析器跟踪f-string表达式部分的原始字符串内容，并在为f-string节点构造解析树时向解析器提供这些内容。仅仅进行“解标记化”是不够的，因为按当前规范，f-string调试表达式保留表达式中的空白，包括`{`和`=`字符之后的空格。这意味着f-string表达式部分的原始字符串内容必须保持完整，而不仅仅是相关的标记。
- en: How parser/lexer implementations deal with this problem is of course up to the
    implementation.
  id: totrans-split-69
  prefs: []
  type: TYPE_NORMAL
  zh: 如何处理这个问题是由解析器/词法分析器实现决定的。
- en: 'Three new tokens are introduced: `FSTRING_START`, `FSTRING_MIDDLE` and `FSTRING_END`.
    Different lexers may have different implementations that may be more efficient
    than the ones proposed here given the context of the particular implementation.
    However, the following definitions will be used as part of the public APIs of
    CPython (such as the `tokenize` module) and are also provided as a reference so
    that the reader can have a better understanding of the proposed grammar changes
    and how the tokens are used:'
  id: totrans-split-70
  prefs: []
  type: TYPE_NORMAL
  zh: 引入了三个新的标记：`FSTRING_START`、`FSTRING_MIDDLE` 和 `FSTRING_END`。不同的词法分析器可能有不同的实现方式，可能比本文提出的更高效，具体取决于特定实现的上下文。然而，以下定义将作为CPython公共API的一部分（例如`tokenize`模块），也作为参考，使读者能更好地理解所提议的语法更改以及标记的使用方式：
- en: '`FSTRING_START`: This token includes the f-string prefix (`f`/`F`/`fr`) and
    the opening quote(s).'
  id: totrans-split-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`FSTRING_START`: 这个标记包括f-string前缀（`f`/`F`/`fr`）和开头的引号。'
- en: '`FSTRING_MIDDLE`: This token includes a portion of text inside the string that’s
    not part of the expression part and isn’t an opening or closing brace. This can
    include the text between the opening quote and the first expression brace (`{`),
    the text between two expression braces (`}` and `{`) and the text between the
    last expression brace (`}`) and the closing quote.'
  id: totrans-split-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`FSTRING_MIDDLE`: 这个标记包括字符串中的一部分文本，不是表达式部分，也不是开放或关闭大括号。这可以包括在开头引号和第一个表达式大括号（`{`）之间的文本，两个表达式大括号（`}`
    和 `{`）之间的文本，以及最后一个表达式大括号（`}`）和结束引号之间的文本。'
- en: '`FSTRING_END`: This token includes the closing quote.'
  id: totrans-split-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`FSTRING_END`: 这个标记包括结束的引号。'
- en: These tokens are always string parts and they are semantically equivalent to
    the `STRING` token with the restrictions specified. These tokens must be produced
    by the lexer when lexing f-strings. This means that **the tokenizer cannot produce
    a single token for f-strings anymore**. How the lexer emits this token is **not
    specified** as this will heavily depend on every implementation (even the Python
    version of the lexer in the standard library is implemented differently to the
    one used by the PEG parser).
  id: totrans-split-74
  prefs: []
  type: TYPE_NORMAL
  zh: 这些标记始终是字符串部分，并且从语义上等同于带有指定限制的`STRING`标记。这些标记必须由词法分析器在词法分析f-strings时生成。这意味着**标记生成器不能再为f-strings生成单个标记**。词法分析器如何生成此标记**未指定**，因为这将在很大程度上取决于每个实现（即使标准库中的Python版本的词法分析器与PEG解析器使用的版本也有所不同）。
- en: 'As an example:'
  id: totrans-split-75
  prefs: []
  type: TYPE_NORMAL
  zh: 例如：
- en: '[PRE12]'
  id: totrans-split-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'will be tokenized as:'
  id: totrans-split-77
  prefs: []
  type: TYPE_NORMAL
  zh: 将被标记为：
- en: '[PRE13]'
  id: totrans-split-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'while `f"""some words"""` will be tokenized simply as:'
  id: totrans-split-79
  prefs: []
  type: TYPE_NORMAL
  zh: 而`f"""some words"""`将被简单地标记为：
- en: '[PRE14]'
  id: totrans-split-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: The [`tokenize`](https://docs.python.org/3/library/tokenize.html#module-tokenize
    "(in Python v3.12)") module will be adapted to emit these tokens as described
    in the previous section when parsing f-strings so tools can take advantage of
    this new tokenization schema and avoid having to implement their own f-string
    tokenizer and parser.
  id: totrans-split-81
  prefs: []
  type: TYPE_NORMAL
  zh: '[`tokenize`](https://docs.python.org/3/library/tokenize.html#module-tokenize)
    模块将被调整为在解析f-strings时如前述部分所述生成这些标记，因此工具可以利用这种新的标记化模式，避免必须实现自己的f-string词法分析器和解析器。'
- en: 'One way existing lexers can be adapted to emit these tokens is to incorporate
    a stack of “lexer modes” or to use a stack of different lexers. This is because
    the lexer needs to switch from “regular Python lexing” to “f-string lexing” when
    it encounters an f-string start token and as f-strings can be nested, the context
    needs to be preserved until the f-string closes. Also, the “lexer mode” inside
    an f-string expression part needs to behave as a “super-set” of the regular Python
    lexer (as it needs to be able to switch back to f-string lexing when it encounters
    the `}` terminator for the expression part as well as handling f-string formatting
    and debug expressions). For reference, here is a draft of the algorithm to modify
    a CPython-like tokenizer to emit these new tokens:'
  id: totrans-split-82
  prefs: []
  type: TYPE_NORMAL
  zh: 一种现有的词法分析器可以被调整以发出这些 token 的方法是，将“词法分析器模式”堆栈或不同词法分析器的堆栈整合在一起。这是因为在遇到 f-string
    开始 token 时，词法分析器需要从“常规 Python 词法分析”切换到“f-string 词法分析”，并且由于 f-strings 可能是嵌套的，因此需要保留上下文直到
    f-string 结束。此外，在 f-string 表达式部分内部的“词法分析器模式”需要表现为“常规 Python 词法分析”的“超集”（因为它需要能够在遇到`}`结束符以及处理
    f-string 格式和调试表达式时切换回 f-string 词法分析）。供参考，这里是修改类似 CPython 的标记器以发出这些新 token 的算法草稿：
- en: If the lexer detects that an f-string is starting (by detecting the letter ‘f/F’
    and one of the possible quotes) keep advancing until a valid quote is detected
    (one of `"`, `"""`, `'` or `'''`) and emit a `FSTRING_START` token with the contents
    captured (the ‘f/F’ and the starting quote). Push a new tokenizer mode to the
    tokenizer mode stack for “F-string tokenization”. Go to step 2.
  id: totrans-split-83
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果词法分析器检测到 f-string 的开始（通过检测到字母‘f/F’和可能的引号之一），则继续前进直到检测到有效的引号（`"`, `"""`, `'`
    或 `'''`）并发出一个 `FSTRING_START` token，并捕获其中的内容（‘f/F’ 和起始引号）。为“F-string 词法分析”推送一个新的词法分析器模式到词法分析器模式栈中。转到步骤
    2。
- en: 'Keep consuming tokens until a one of the following is encountered:'
  id: totrans-split-84
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 继续消耗 token，直到遇到以下情况之一：
- en: A closing quote equal to the opening quote.
  id: totrans-split-85
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 遇到与开引号相匹配的闭引号。
- en: If in “format specifier mode” (see step 3), an opening brace (`{`), a closing
    brace (`}`), or a newline token (`\n`).
  id: totrans-split-86
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果处于“格式说明符模式”（见步骤 3），则遇到左大括号（`{`）、右大括号（`}`）或换行符（`\n`）。
- en: If not in “format specifier mode” (see step 3), an opening brace (`{`) or a
    closing brace (`}`) that is not immediately followed by another opening/closing
    brace.
  id: totrans-split-87
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果不处于“格式说明符模式”（见步骤 3），则遇到不是紧随另一个开/闭括号的左大括号（`{`）或右大括号（`}`）。
- en: 'In all cases, if the character buffer is not empty, emit a `FSTRING_MIDDLE`
    token with the contents captured so far but transform any double opening/closing
    braces into single opening/closing braces. Now, proceed as follows depending on
    the character encountered:'
  id: totrans-split-88
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在所有情况下，如果字符缓冲区不为空，则发出一个 `FSTRING_MIDDLE` token，捕获到目前为止的内容，但将任何双开/闭括号转换为单个开/闭括号。然后，根据遇到的字符进行如下操作：
- en: If a closing quote matching the opening quite is encountered go to step 4.
  id: totrans-split-89
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果遇到与开引号匹配的闭引号，则转到步骤 4。
- en: If an opening bracket (not immediately followed by another opening bracket)
    is encountered, go to step 3.
  id: totrans-split-90
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果遇到一个开括号（紧随另一个开括号之后），则转到步骤 3。
- en: If a closing bracket (not immediately followed by another closing bracket) is
    encountered, emit a token for the closing bracket and go to step 2.
  id: totrans-split-91
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果遇到一个关闭括号（不是紧随另一个关闭括号之后），则发出一个关闭括号的 token 并转到步骤 2。
- en: Push a new tokenizer mode to the tokenizer mode stack for “Regular Python tokenization
    within f-string” and proceed to tokenize with it. This mode tokenizes as the “Regular
    Python tokenization” until a `:` or a `}` character is encountered with the same
    level of nesting as the opening bracket token that was pushed when we enter the
    f-string part. Using this mode, emit tokens until one of the stop points are reached.
    When this happens, emit the corresponding token for the stopping character encountered
    and, pop the current tokenizer mode from the tokenizer mode stack and go to step
    2\. If the stopping point is a `:` character, enter step 2 in “format specifier”
    mode.
  id: totrans-split-92
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为“f-string 内部的常规 Python 词法分析”推送一个新的词法分析器模式到词法分析器模式栈中，并使用它进行词法分析。此模式将按照“常规 Python
    词法分析”词法分析，直到遇到与进入 f-string 部分时推送的开括号标记具有相同嵌套级别的 `:` 或 `}` 字符为止。使用此模式，发出 token
    直到达到停止点之一。当发生这种情况时，发出相应的停止字符遇到的 token，并从词法分析器模式栈中弹出当前的词法分析器模式并转到步骤 2。如果停止点是 `:`
    字符，则进入“格式说明符”模式的步骤 2。
- en: Emit a `FSTRING_END` token with the contents captured and pop the current tokenizer
    mode (corresponding to “F-string tokenization”) and go back to “Regular Python
    mode”.
  id: totrans-split-93
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 发出一个 `FSTRING_END` token，并捕获到目前为止的内容，并弹出当前的词法分析器模式（对应于“F-string 词法分析”），然后返回“常规
    Python 模式”。
- en: Of course, as mentioned before, it is not possible to provide a precise specification
    of how this should be done for an arbitrary tokenizer as it will depend on the
    specific implementation and nature of the lexer to be changed.
  id: totrans-split-94
  prefs: []
  type: TYPE_NORMAL
- en: 'All restrictions mentioned in the PEP are lifted from f-string literals, as
    explained below:'
  id: totrans-split-95
  prefs: []
  type: TYPE_NORMAL
- en: Expression portions may now contain strings delimited with the same kind of
    quote that is used to delimit the f-string literal.
  id: totrans-split-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Backslashes may now appear within expressions just like anywhere else in Python
    code. In case of strings nested within f-string literals, escape sequences are
    expanded when the innermost string is evaluated.
  id: totrans-split-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'New lines are now allowed within expression brackets. This means that these
    are now allowed:'
  id: totrans-split-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE15]'
  id: totrans-split-99
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Comments, using the `#` character, are allowed within the expression part of
    an f-string. Note that comments require that the closing bracket (`}`) of the
    expression part to be present in a different line as the one the comment is in
    or otherwise it will be ignored as part of the comment.
  id: totrans-split-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'One of the consequences of the grammar proposed here is that, as mentioned
    above, f-string expressions can now contain strings delimited with the same kind
    of quote that is used to delimit the external f-string literal. For example:'
  id: totrans-split-101
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  id: totrans-split-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: In the [discussion thread for this PEP](https://discuss.python.org/t/pep-701-syntactic-formalization-of-f-strings/22046),
    several concerns have been raised regarding this aspect and we want to collect
    them here, as these should be taken into consideration when accepting or rejecting
    this PEP.
  id: totrans-split-103
  prefs: []
  type: TYPE_NORMAL
- en: 'Some of these objections include:'
  id: totrans-split-104
  prefs: []
  type: TYPE_NORMAL
- en: 'Many people find quote reuse within the same string confusing and hard to read.
    This is because allowing quote reuse will violate a current property of Python
    as it stands today: the fact that strings are fully delimited by two consecutive
    pairs of the same kind of quote, which by itself is a very simple rule. One of
    the reasons quote reuse may be harder for humans to parse, leading to less readable
    code, is that the quote character is the same for both start and end (as opposed
    to other delimiters).'
  id: totrans-split-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Some users have raised concerns that quote reuse may break some lexer and syntax
    highlighting tools that rely on simple mechanisms to detect strings and f-strings,
    such as regular expressions or simple delimiter matching tools. Introducing quote
    reuse in f-strings will either make it trickier to keep these tools working or
    will break the tools altogether (as, for instance, regular expressions cannot
    parse arbitrary nested structures with delimiters). The IDLE editor, included
    in the standard library, is an example of a tool which may need some work to correctly
    apply syntax highlighting to f-strings.
  id: totrans-split-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Here are some of the arguments in favour:'
  id: totrans-split-107
  prefs: []
  type: TYPE_NORMAL
- en: Many languages that allow similar syntactic constructs (normally called “string
    interpolation”) allow quote reuse and arbitrary nesting. These languages include
    JavaScript, Ruby, C#, Bash, Swift and many others. The fact that many languages
    allow quote reuse can be a compelling argument in favour of allowing it in Python.
    This is because it will make the language more familiar to users coming from other
    languages.
  id: totrans-split-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 许多允许类似语法结构（通常称为“字符串插值”）的语言允许引号重用和任意嵌套。这些语言包括 JavaScript、Ruby、C#、Bash、Swift 等等。许多语言允许引号重用的事实可以成为支持在
    Python 中引入这一特性的有力论据。这是因为它会使语言对从其他语言转换过来的用户更加熟悉。
- en: As many other popular languages allow quote reuse in string interpolation constructs,
    this means that editors that support syntax highlighting for these languages will
    already have the necessary tools to support syntax highlighting for f-strings
    with quote reuse in Python. This means that although the files that handle syntax
    highlighting for Python will need to be updated to support this new feature, is
    not expected to be impossible or very hard to do.
  id: totrans-split-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 由于许多其他流行语言允许在字符串插值构造中重用引号，这意味着支持这些语言的语法高亮编辑器已经具备了支持 Python 中具有引号重用的 f-strings
    的必要工具。这意味着尽管需要更新处理 Python 的语法高亮的文件来支持这一新特性，但这并不是不可能或非常困难的。
- en: 'One advantage of allowing quote reuse is that it composes cleanly with other
    syntax. Sometimes this is referred to as “referential transparency”. An example
    of this is that if we have `f(x+1)`, assuming `a` is a brand new variable, it
    should behave the same as `a = x+1; f(a)`. And vice versa. So if we have:'
  id: totrans-split-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 允许引号重用的一个优点是它与其他语法的组合非常干净。有时这被称为“引用透明性”。一个例子是，如果我们有 `f(x+1)`，假设 `a` 是一个全新的变量，它应该与
    `a = x+1; f(a)` 的行为相同。反之亦然。因此，如果我们有：
- en: '[PRE17]'
  id: totrans-split-111
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'It should be expected that if we replace the variable `prefix` with its definition,
    the answer should be the same:'
  id: totrans-split-112
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 应该预期，如果我们用其定义替换变量 `prefix`，答案应该是相同的：
- en: '[PRE18]'
  id: totrans-split-113
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Code generators (like [ast.unparse](https://docs.python.org/3/library/ast.html#ast.unparse)
    from standard library) in their current form rely on complicated algorithms to
    ensure expressions within an f-string are properly suited for the context in which
    they are being used. These non-trivial algorithms come with challenges such as
    finding an unused quote type (by tracking the outer quotes), and generating string
    representations which would not include backslashes if possible. Allowing quote
    reuse and backslashes would simplify the code generators which deal with f-strings
    considerably, as the regular Python expression logic can be used inside and outside
    of f-strings without any special treatment.
  id: totrans-split-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 代码生成器（例如来自标准库的[ast.unparse](https://docs.python.org/3/library/ast.html#ast.unparse)）在当前形式上依赖复杂的算法，以确保
    f-string 中的表达式在其被使用的上下文中适当地运行。这些非平凡的算法面临着挑战，例如找到未使用的引号类型（通过跟踪外部引号），并生成字符串表示，尽量不包含反斜杠。允许引号重用和反斜杠将极大简化处理
    f-string 的代码生成器，因为可以在 f-string 内外都使用常规的 Python 表达式逻辑，而无需特殊处理。
- en: Limiting quote reuse will considerably increase the complexity of the implementation
    of the proposed changes. This is because it will force the parser to have the
    context that is parsing an expression part of an f-string with a given quote in
    order to know if it needs to reject an expression that reuses the quote. Carrying
    this context around is not trivial in parsers that can backtrack arbitrarily (such
    as the PEG parser). The issue becomes even more complex if we consider that f-strings
    can be arbitrarily nested and therefore several quote types may need to be rejected.
  id: totrans-split-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 限制引号重用将显著增加所提议更改的实现复杂性。这是因为它将迫使解析器在解析带有给定引号的 f-string 表达式部分时具有上下文，以便知道是否需要拒绝重用引号的表达式。在可以任意回溯的解析器（如
    PEG 解析器）中传递这种上下文并不简单。如果考虑到 f-string 可以任意嵌套，因此可能需要拒绝多种引号类型，问题变得更加复杂。
- en: To gather feedback from the community, [a poll](https://discuss.python.org/t/pep-701-syntactic-formalization-of-f-strings/22046/24)
    has been initiated to get a sense of how the community feels about this aspect
    of the PEP.
  id: totrans-split-116
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 为了收集社区的反馈，已经启动了[一项民意调查](https://discuss.python.org/t/pep-701-syntactic-formalization-of-f-strings/22046/24)，以了解社区对
    PEP 的这一方面的看法。
- en: This PEP does not introduce any backwards incompatible syntactic or semantic
    changes to the Python language. However, the [`tokenize`](https://docs.python.org/3/library/tokenize.html#module-tokenize
    "(in Python v3.12)") module (a quasi-public part of the standard library) will
    need to be updated to support the new f-string tokens (to allow tool authors to
    correctly tokenize f-strings). See [changes to the tokenize module](#changes-to-the-tokenize-module)
    for more details regarding how the public API of `tokenize` will be affected.
  id: totrans-split-117
  prefs: []
  type: TYPE_NORMAL
- en: As the concept of f-strings is already ubiquitous in the Python community, there
    is no fundamental need for users to learn anything new. However, as the formalized
    grammar allows some new possibilities, it is important that the formal grammar
    is added to the documentation and explained in detail, explicitly mentioning what
    constructs are possible since this PEP is aiming to avoid confusion.
  id: totrans-split-118
  prefs: []
  type: TYPE_NORMAL
- en: 'It is also beneficial to provide users with a simple framework for understanding
    what can be placed inside an f-string expression. In this case the authors think
    that this work will make it even simpler to explain this aspect of the language,
    since it can be summarized as:'
  id: totrans-split-119
  prefs: []
  type: TYPE_NORMAL
- en: You can place any valid Python expression inside an f-string expression.
  id: totrans-split-120
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'With the changes in this PEP, there is no need to clarify that string quotes
    are limited to be different from the quotes of the enclosing string, because this
    is now allowed: as an arbitrary Python string can contain any possible choice
    of quotes, so can any f-string expression. Additionally there is no need to clarify
    that certain things are not allowed in the expression part because of implementation
    restrictions such as comments, new line characters or backslashes.'
  id: totrans-split-121
  prefs: []
  type: TYPE_NORMAL
- en: 'The only “surprising” difference is that as f-strings allow specifying a format,
    expressions that allow a `:` character at the top level still need to be enclosed
    in parenthesis. This is not new to this work, but it is important to emphasize
    that this restriction is still in place. This allows for an easier modification
    of the summary:'
  id: totrans-split-122
  prefs: []
  type: TYPE_NORMAL
- en: You can place any valid Python expression inside an f-string expression, and
    everything after a `:` character at the top level will be identified as a format
    specification.
  id: totrans-split-123
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Although we think the readability arguments that have been raised against allowing
    quote reuse in f-string expressions are valid and very important, we have decided
    to propose not rejecting quote reuse in f-strings at the parser level. The reason
    is that one of the cornerstones of this PEP is to reduce the complexity and maintenance
    of parsing f-strings in CPython and this will not only work against that goal,
    but it may even make the implementation even more complex than the current one.
    We believe that forbidding quote reuse should be done in linters and code style
    tools and not in the parser, the same way other confusing or hard-to-read constructs
    in the language are handled today.
  id: totrans-split-124
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'We have decided not to lift the restriction that some expression portions need
    to wrap `'':''` and `''!''` in parentheses at the top level, e.g.:'
  id: totrans-split-125
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们决定不取消对某些表达式部分在顶层需要用括号包裹`':'`和`'!'`的限制，例如：
- en: '[PRE19]'
  id: totrans-split-126
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE19]'
- en: The reason is that this will introduce a considerable amount of complexity for
    no real benefit. This is due to the fact that the `:` character normally separates
    the f-string format specification. This format specification is currently tokenized
    as a string. As the tokenizer MUST tokenize what’s on the right of the `:` as
    either a string or a stream of tokens, this won’t allow the parser to differentiate
    between the different semantics as that would require the tokenizer to backtrack
    and produce a different set of tokens (this is, first try as a stream of tokens,
    and if it fails, try as a string for a format specifier).
  id: totrans-split-127
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这是因为这样做会引入相当多的复杂性，而实际上并没有多大好处。原因在于`:`字符通常用于分隔f-string的格式说明。当前，这个格式说明被标记为一个字符串。因为标记器必须将`:`右边的内容标记为字符串或者一串标记，这将不允许解析器区分不同的语义，因为这将要求标记器回溯并生成不同的标记集（即，首先尝试作为一串标记，如果失败，则尝试作为格式说明的字符串）。
- en: 'As there is no fundamental advantage in being able to allow lambdas and similar
    expressions at the top level, we have decided to keep the restriction that these
    must be parenthesized if needed:'
  id: totrans-split-128
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 由于允许在顶层使用lambda和类似表达式并没有根本上的优势，我们决定保持这一限制，即如果需要的话，这些表达式必须使用括号括起来：
- en: '[PRE20]'
  id: totrans-split-129
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE20]'
- en: We have decided to disallow (for the time being) using escaped braces (`\{`
    and `\}`) in addition to the `{{` and `}}` syntax. Although the authors of the
    PEP believe that allowing escaped braces is a good idea, we have decided to not
    include it in this PEP, as it is not strictly necessary for the formalization
    of f-strings proposed here, and it can be added independently in a regular CPython
    issue.
  id: totrans-split-130
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们决定（目前）不允许使用转义大括号（`\{` 和 `\}`）以及`{{`和`}}`语法。尽管PEP的作者们认为允许转义大括号是个好主意，我们决定在这个PEP中不包括它，因为它对于这里提议的f-string的形式化并不是严格必要的，并且可以独立地在常规CPython问题中添加。
- en: This document is placed in the public domain or under the CC0-1.0-Universal
    license, whichever is more permissive.
  id: totrans-split-131
  prefs: []
  type: TYPE_NORMAL
  zh: 本文档位于公共领域或CC0-1.0-Universal许可证之下，以更加宽松的许可证为准。
