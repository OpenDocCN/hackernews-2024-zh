- en: <!--yml
  id: totrans-split-0
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  id: totrans-split-1
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-05-29 13:29:53'
  id: totrans-split-2
  prefs: []
  type: TYPE_NORMAL
- en: -->
  id: totrans-split-3
  prefs: []
  type: TYPE_NORMAL
- en: StarCoder 2 is a code-generating AI that runs on most GPUs | TechCrunch
  id: totrans-split-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://techcrunch.com/2024/02/28/starcoder-2-is-a-code-generating-ai-that-runs-on-most-gpus/](https://techcrunch.com/2024/02/28/starcoder-2-is-a-code-generating-ai-that-runs-on-most-gpus/)
  id: totrans-split-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Developers are adopting AI-powered code generators — services like [GitHub Copilot](https://techcrunch.com/tag/github-copilot/)
    and [Amazon CodeWhisperer](https://techcrunch.com/2022/06/23/amazon-launches-codewhisperer-its-ai-pair-programming-tool/),
    along with open access models such as Meta’s [Code Llama](https://techcrunch.com/2023/08/24/meta-releases-code-llama-a-code-generating-ai-model/)
    — at an [astonishing](https://www.csoonline.com/article/1249838/almost-all-developers-are-using-ai-despite-security-concerns-survey-suggests.html)
    rate. But the tools are far from ideal. Many aren’t free. Others are, but only
    under licenses that preclude them from being used in common commercial contexts.
  id: totrans-split-6
  prefs: []
  type: TYPE_NORMAL
- en: Perceiving the demand for alternatives, AI startup Hugging Face several years
    ago teamed up with ServiceNow, the workflow automation platform, to create [StarCoder](https://techcrunch.com/2023/05/04/hugging-face-and-servicenow-release-a-free-code-generating-model/),
    an open source code generator with a less restrictive license than some of the
    others out there. The original came online early last year, and work has been
    underway on a follow-up, StarCoder 2, ever since.
  id: totrans-split-7
  prefs: []
  type: TYPE_NORMAL
- en: 'StarCoder 2 isn’t a single code-generating model, but rather a family. Released
    today, it comes in three variants, the first two of which can run on most modern
    consumer GPUs:'
  id: totrans-split-8
  prefs: []
  type: TYPE_NORMAL
- en: A 3-billion-parameter (3B) model trained by ServiceNow
  id: totrans-split-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A 7-billion-parameter (7B) model trained by Hugging Face
  id: totrans-split-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A 15-billion-parameter (15B) model trained by Nvidia, the newest supporter of
    the StarCoder project
  id: totrans-split-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (Note that “parameters” are the parts of a model learned from training data
    and essentially define the skill of the model on a problem, in this case generating
    code.)
  id: totrans-split-12
  prefs: []
  type: TYPE_NORMAL
- en: Like most other code generators, StarCoder 2 can suggest ways to complete unfinished
    lines of code as well as summarize and retrieve snippets of code when asked in
    natural language. Trained with 4x more data than the original StarCoder (67.5
    terabytes versus 6.4 terabytes), StarCoder 2 delivers what Hugging Face, ServiceNow
    and Nvidia characterize as “significantly” improved performance at lower costs
    to operate.
  id: totrans-split-13
  prefs: []
  type: TYPE_NORMAL
- en: StarCoder 2 can be fine-tuned “in a few hours” using a GPU like the Nvidia A100
    on first- or third-party data to create apps such as chatbots and personal coding
    assistants. And, because it was trained on a larger and more diverse data set
    than the original StarCoder (~619 programming languages), StarCoder 2 can make
    more accurate, context-aware predictions — at least hypothetically.
  id: totrans-split-14
  prefs: []
  type: TYPE_NORMAL
- en: “StarCoder 2 was created especially for developers who need to build applications
    quickly,” Harm de Vries, head of ServiceNow’s StarCoder 2 development team, told
    TechCrunch in an interview. “With StarCoder2, developers can use its capabilities
    to make coding more efficient without sacrificing speed or quality.”
  id: totrans-split-15
  prefs: []
  type: TYPE_NORMAL
- en: Now, I’d venture to say that not every developer would agree with de Vries on
    the speed and quality points. Code generators promise to streamline certain coding
    tasks — but at a cost.
  id: totrans-split-16
  prefs: []
  type: TYPE_NORMAL
- en: A recent Stanford [study](https://techcrunch.com/2022/12/28/code-generating-ai-can-introduce-security-vulnerabilities-study-finds/)
    found that engineers who use code-generating systems are more likely to introduce
    security vulnerabilities in the apps they develop. Elsewhere, a [poll](https://www.sonatype.com/hubfs/The%20Risks%20and%20Rewards%20of%20Generative%20AI%20in%20Software%20Development.pdf)
    from Sonatype, the cybersecurity firm, shows that the majority of developers are
    concerned about the lack of insight into how code from code generators is produced
    and “code sprawl” from generators producing too much code to manage.
  id: totrans-split-17
  prefs: []
  type: TYPE_NORMAL
- en: StarCoder 2’s license might also prove to be a roadblock for some.
  id: totrans-split-18
  prefs: []
  type: TYPE_NORMAL
- en: StarCoder 2 is licensed under the BigCode Open RAIL-M 1.0, which aims to promote
    responsible use by imposing “light touch” restrictions on both model licensees
    and downstream users. While less constraining than many other licenses, RAIL-M
    isn’t truly “open” in the sense that it doesn’t [permit](https://about.gitlab.com/blog/2023/07/25/rail-m-is-an-imperfectly-good-start-for-ai-model-licenses/)
    developers to use StarCoder 2 for *every* conceivable application (medical advice-giving
    apps are strictly off limits, for example). Some commentators say RAIL-M’s requirements
    may be too vague to comply with in any case — and that RAIL-M could conflict with
    AI-related regulations like the EU AI Act.
  id: totrans-split-19
  prefs: []
  type: TYPE_NORMAL
- en: 'In response to the above criticism, a Hugging Face spokesperson had this to
    say via an emailed statement: “The license was carefully engineered to maximize
    compliance with current laws and regulations.”'
  id: totrans-split-20
  prefs: []
  type: TYPE_NORMAL
- en: Setting all this aside for a moment, is StarCoder 2 really superior to the other
    code generators out there — free or paid?
  id: totrans-split-21
  prefs: []
  type: TYPE_NORMAL
- en: Depending on the benchmark, it appears to be more efficient than one of the
    versions of Code Llama, Code Llama 33B. Hugging Face says that StarCoder 2 15B
    matches Code Llama 33B on a subset of code completion tasks at twice the speed.
    It’s not clear which tasks; Hugging Face didn’t specify.
  id: totrans-split-22
  prefs: []
  type: TYPE_NORMAL
- en: StarCoder 2, as an open source collection of models, also has the advantage
    of being able to deploy locally and “learn” a developer’s source code or codebase
    — an attractive prospect to devs and companies wary of exposing code to a cloud-hosted
    AI. In a 2023 [survey](https://chainstoreage.com/survey-companies-have-mixed-feelings-about-generative-ai)
    from Portal26 and CensusWide, 85% of businesses said that they were wary of adopting
    GenAI like code generators due to the privacy and security risks — like employees
    sharing sensitive information or vendors training on proprietary data.
  id: totrans-split-23
  prefs: []
  type: TYPE_NORMAL
- en: Hugging Face, ServiceNow and Nvidia also make the case that StarCoder 2 is more
    ethical — and less legally fraught — than its rivals.
  id: totrans-split-24
  prefs: []
  type: TYPE_NORMAL
- en: All GenAI models regurgitate — in other words, spit out a mirror copy of data
    they were trained on. It doesn’t take an active imagination to see why this might
    land a developer in trouble. With code generators trained on copyrighted code,
    it’s entirely possible that, even with filters and additional safeguards in place,
    the generators could unwittingly recommend copyrighted code and fail to label
    it as such.
  id: totrans-split-25
  prefs: []
  type: TYPE_NORMAL
- en: A few vendors, including GitHub, Microsoft (GitHub’s parent company) and Amazon,
    have [pledged](https://techcrunch.com/2023/10/06/some-gen-ai-vendors-say-theyll-defend-customers-from-ip-lawsuits-others-not-so-much/)
    to provide legal coverage in situations where a code generator customer is accused
    of violating copyright. But coverage varies vendor-to-vendor and is generally
    limited to corporate clientele.
  id: totrans-split-26
  prefs: []
  type: TYPE_NORMAL
- en: As opposed to code generators trained using copyrighted code (GitHub Copilot,
    among others), StarCoder 2 was trained only on data under license from the Software
    Heritage, the nonprofit organization providing archival services for code. Ahead
    of StarCoder 2’s training, [BigCode](https://techcrunch.com/2022/09/26/hugging-face-and-servicenow-launch-bigcode-a-project-to-open-source-code-generating-ai-systems/),
    the cross-organizational team behind much of StarCoder 2’s roadmap, gave code
    owners a chance to opt out of the training set if they wanted.
  id: totrans-split-27
  prefs: []
  type: TYPE_NORMAL
- en: As with the original StarCoder, StarCoder 2’s training data is available for
    developers to fork, reproduce or audit as they please.
  id: totrans-split-28
  prefs: []
  type: TYPE_NORMAL
- en: Leandro von Werra, a Hugging Face machine learning engineer and co-lead of BigCode,
    pointed out that while there’s been a proliferation of open code generators recently,
    few have been accompanied by information about the data that went into training
    them and, indeed, how they were trained.
  id: totrans-split-29
  prefs: []
  type: TYPE_NORMAL
- en: “From a scientific standpoint, an issue is that training is not reproducible,
    but also as a data producer (i.e. someone uploading their code to GitHub), you
    don’t know if and how your data was used,” von Werra said in an interview. “StarCoder
    2 addresses this issue by being fully transparent across the whole training pipeline
    from scraping pretraining data to the training itself.”
  id: totrans-split-30
  prefs: []
  type: TYPE_NORMAL
- en: StarCoder 2 isn’t perfect, that said. Like other code generators, it’s susceptible
    to bias. De Vries notes that it can generate code with elements that reflect stereotypes
    about gender and race. And because StarCoder 2 was trained on predominantly English-language
    comments, Python and Java code, it performs weaker on languages other than English
    and “lower-resource” code like Fortran and Haskell.
  id: totrans-split-31
  prefs: []
  type: TYPE_NORMAL
- en: Still, von Werra asserts it’s a step in the right direction.
  id: totrans-split-32
  prefs: []
  type: TYPE_NORMAL
- en: “We strongly believe that building trust and accountability with AI models requires
    transparency and auditability of the full model pipeline including training data
    and training recipe,” he said. “StarCoder 2 [showcases] how fully open models
    can deliver competitive performance.”
  id: totrans-split-33
  prefs: []
  type: TYPE_NORMAL
- en: You might be wondering — as was this writer — what incentive Hugging Face, ServiceNow
    and Nvidia have to invest in a project like StarCoder 2\. They’re businesses,
    after all — and training models isn’t cheap.
  id: totrans-split-34
  prefs: []
  type: TYPE_NORMAL
- en: 'So far as I can tell, it’s a tried-and-true strategy: foster goodwill and build
    paid services on top of the open source releases.'
  id: totrans-split-35
  prefs: []
  type: TYPE_NORMAL
- en: ServiceNow has already used StarCoder to create Now LLM, a product for code
    generation fine-tuned for ServiceNow workflow patterns, use cases and processes.
    Hugging Face, which offers model implementation consulting plans, is providing
    hosted versions of the StarCoder 2 models on its platform. So is Nvidia, which
    is making StarCoder 2 available through an API and web front-end.
  id: totrans-split-36
  prefs: []
  type: TYPE_NORMAL
- en: For devs expressly interested in the no-cost offline experience, StarCoder 2
    — the models, source code and more — can be downloaded from the project’s GitHub
    page.
  id: totrans-split-37
  prefs: []
  type: TYPE_NORMAL
