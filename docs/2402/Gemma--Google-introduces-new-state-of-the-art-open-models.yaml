- en: <!--yml
  id: totrans-split-0
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  id: totrans-split-1
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-05-27 15:03:41'
  id: totrans-split-2
  prefs: []
  type: TYPE_NORMAL
- en: -->
  id: totrans-split-3
  prefs: []
  type: TYPE_NORMAL
- en: 'Gemma: Google introduces new state-of-the-art open models'
  id: totrans-split-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://blog.google/technology/developers/gemma-open-models/](https://blog.google/technology/developers/gemma-open-models/)
  id: totrans-split-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Responsible by design
  id: totrans-split-6
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Gemma is designed with our [AI Principles](https://ai.google.dev/responsible?utm_source=agd&utm_medium=referral&utm_campaign=explore-responsible&utm_content)
    at the forefront. As part of making Gemma pre-trained models safe and reliable,
    we used automated techniques to filter out certain personal information and other
    sensitive data from training sets. Additionally, we used extensive fine-tuning
    and reinforcement learning from human feedback (RLHF) to align our instruction-tuned
    models with responsible behaviors. To understand and reduce the risk profile for
    Gemma models, we conducted robust evaluations including manual red-teaming, automated
    adversarial testing, and assessments of model capabilities for dangerous activities.
    These evaluations are outlined in our [Model Card](https://www.kaggle.com/models/google/gemma).
  id: totrans-split-7
  prefs: []
  type: TYPE_NORMAL
- en: 'We’re also releasing a new [Responsible Generative AI Toolkit](https://ai.google.dev/responsible?utm_source=agd&utm_medium=referral&utm_campaign=explore-responsible&utm_content)
    together with Gemma to help developers and researchers prioritize building safe
    and responsible AI applications. The toolkit includes:'
  id: totrans-split-8
  prefs: []
  type: TYPE_NORMAL
- en: '**Safety classification:** We provide a [novel methodology](https://codelabs.developers.google.com/codelabs/responsible-ai/agile-classifiers)
    for building robust safety classifiers with minimal examples.'
  id: totrans-split-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Debugging:** A model [debugging tool](https://codelabs.developers.google.com/codelabs/responsible-ai/lit-gemma)
    helps you investigate Gemma''s behavior and address potential issues.'
  id: totrans-split-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Guidance:** You can access best practices for model builders based on Google’s
    experience in developing and deploying large language models.'
  id: totrans-split-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Optimized across frameworks, tools and hardware
  id: totrans-split-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'You can fine-tune Gemma models on your own data to adapt to specific application
    needs, such as summarization or retrieval-augmented generation (RAG). Gemma supports
    a wide variety of tools and systems:'
  id: totrans-split-13
  prefs: []
  type: TYPE_NORMAL
- en: '**Multi-framework tools:** Bring your favorite framework, with reference implementations
    for inference and fine-tuning across multi-framework Keras 3.0, native PyTorch,
    JAX, and Hugging Face Transformers.'
  id: totrans-split-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Cross-device compatibility:** Gemma models run across popular device types,
    including laptop, desktop, IoT, mobile and cloud, enabling broadly accessible
    AI capabilities.'
  id: totrans-split-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Cutting-edge hardware platforms:** We’ve [partnered with NVIDIA to optimize
    Gemma for NVIDIA GPUs](https://blogs.nvidia.com/blog/google-gemma-llm-rtx-ai-pc),
    from data center to the cloud to local RTX AI PCs, ensuring industry-leading performance
    and integration with cutting-edge technology.'
  id: totrans-split-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Optimized for Google Cloud:** Vertex AI provides a broad MLOps toolset with
    a range of tuning options and one-click deployment using built-in inference optimizations.
    Advanced customization is available with fully-managed Vertex AI tools or with
    self-managed GKE, including deployment to cost-efficient infrastructure across
    GPU, TPU, and CPU from either platform.'
  id: totrans-split-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Free credits for research and development
  id: totrans-split-18
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Gemma is built for the open community of developers and researchers powering
    AI innovation. You can start working with Gemma today using free access in Kaggle,
    a free tier for Colab notebooks, and $300 in credits for first-time Google Cloud
    users. Researchers can also apply for [Google Cloud credits](https://docs.google.com/forms/d/e/1FAIpQLSe0grG6mRFW6dNF3Rb1h_YvKqUp2GaXiglZBgA2Os5iTLWlcg/viewform)
    of up to a collective $500,000 to accelerate their projects.
  id: totrans-split-19
  prefs: []
  type: TYPE_NORMAL
- en: Getting started
  id: totrans-split-20
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You can explore more about Gemma and access quickstart guides on [ai.google.dev/gemma](http://ai.google.dev/gemma).
  id: totrans-split-21
  prefs: []
  type: TYPE_NORMAL
- en: As we continue to expand the Gemma model family, we look forward to introducing
    new variants for diverse applications. Stay tuned for events and opportunities
    in the coming weeks to connect, learn and build with Gemma.
  id: totrans-split-22
  prefs: []
  type: TYPE_NORMAL
- en: We’re excited to see what you create!
  id: totrans-split-23
  prefs: []
  type: TYPE_NORMAL
