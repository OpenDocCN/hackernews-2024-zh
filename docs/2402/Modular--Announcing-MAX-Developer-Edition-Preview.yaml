- en: <!--yml
  id: totrans-split-0
  prefs: []
  type: TYPE_NORMAL
- en: 'category: Êú™ÂàÜÁ±ª'
  id: totrans-split-1
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-05-29 13:28:11'
  id: totrans-split-2
  prefs: []
  type: TYPE_NORMAL
- en: -->
  id: totrans-split-3
  prefs: []
  type: TYPE_NORMAL
- en: 'Modular: Announcing MAX Developer Edition Preview'
  id: totrans-split-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Êù•Ê∫êÔºö[https://www.modular.com/blog/announcing-max-developer-edition-preview](https://www.modular.com/blog/announcing-max-developer-edition-preview)
  id: totrans-split-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Modular was founded on the [vision](https://www.modular.com/vision) to enable
    AI to be used by anyone, anywhere. We have always believed that to achieve this
    vision, we must first fix the fragmented and disjoint infrastructure upon which
    AI is built today. As we said [2 years ago](https://www.modular.com/blog/the-case-for-a-next-generation-ai-developer-platform),
    we imagine a different future for AI software, one that rings truer now than ever
    before:'
  id: totrans-split-6
  prefs: []
  type: TYPE_NORMAL
- en: Our success means global developers will be empowered by access to AI software
    that is truly usable, portable, and scalable. A world where developers without
    unlimited budgets or access to top talent can be just as efficient as the world‚Äôs
    largest technology giants, where the efficiency and total cost of ownership of
    AI hardware are optimized, where organizations can easily plug in custom ASICs
    for their use cases, where deploying to the edge is as easy as deploying to a
    server, where organizations can use any AI framework that best suits their needs,
    and where AI programs seamlessly scale across hardware so that deploying the latest
    AI research into production ‚Äújust works‚Äù.
  id: totrans-split-7
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '**And today, we‚Äôre so excited to take a huge step towards that brighter future
    by announcing that the Modular Accelerated Xecution (MAX) Platform is** [**now
    globally available**](https://www.modular.com/max), starting first as a preview
    on Linux systems. MAX is a unified set of tools and libraries that unlock performance,
    programmability and portability for your AI inference pipelines. Each component
    of MAX is designed to simplify the process of deploying models to any hardware,
    delivering the best possible cost-performance ratio for your workloads.'
  id: totrans-split-8
  prefs: []
  type: TYPE_NORMAL
- en: '**What‚Äôs included with MAX?**'
  id: totrans-split-9
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**MAX is engineered to cater to the demanding needs of AI deployment, giving
    you everything you need to deploy low-latency, high-throughput, real-time inference
    pipelines into production**. This first release of MAX ships with three key components:'
  id: totrans-split-10
  prefs: []
  type: TYPE_NORMAL
- en: '[**MAX Engine**](https://www.modular.com/max/engine): A state-of-the-art AI
    compiler and runtime system supporting PyTorch, TensorFlow, and ONNX models like
    [Mistral](https://performance.modular.com/?task=language&model=mistral-7b-v0.1&instance=c7g.16xlarge&framework=stock-pytorch&view=chart),
    [Stable Diffusion](https://performance.modular.com/?task=computer_vision&model=stable-diffusion-v1.5-unet&instance=c5.9xlarge&framework=stock-pytorch&view=chart),
    [Llama2](https://performance.modular.com/?task=language&model=llama2-7B-MS-context-encoding&instance=c6a.16xlarge&framework=stock-pytorch&view=chart),
    [WavLM](https://performance.modular.com/?task=language&model=wavlm-large&instance=c6i.4xlarge&framework=stock-pytorch&view=chart),
    [DLMR](https://performance.modular.com/?task=recommenders&model=dlrm-rm2&instance=c5a.8xlarge&framework=stock-pytorch&view=chart),
    [ClipVit](https://performance.modular.com/?task=computer_vision&model=clip-vit-large-patch14&instance=c6i.4xlarge&framework=stock-pytorch&view=chart)
    and many more. It delivers unmatched inference speed across diverse hardware platforms,
    and we‚Äôre just getting started.'
  id: totrans-split-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[**MAX Serving**](https://www.modular.com/max/serving)**:** An efficient serving
    wrapper for the MAX Engine, ensuring seamless interoperability with current AI
    serving systems like NVIDIA Triton, and smooth deployment to container ecosystems
    such as Kubernetes.'
  id: totrans-split-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: ‚Äç[**Mojoüî•**](https://www.modular.com/max/mojo)**:** The world‚Äôs first programming
    language built from the ground up for AI development, with cutting-edge compiler
    technology that delivers unparalleled performance and programmability on any hardware.
  id: totrans-split-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: MAX gives developers superpowers, providing them with an array of tools and
    libraries for building high-performance AI applications that can be efficiently
    deployed across multiple hardware platforms.
  id: totrans-split-14
  prefs: []
  type: TYPE_NORMAL
- en: '**How do I use MAX?**'
  id: totrans-split-15
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: MAX is built to meet AI developers where they are today. It provides an instant
    boost in performance for your current models and seamlessly integrates with your
    existing toolchain and serving infrastructure, capturing the immediate value and
    performance improvements of MAX with minimal code changes. Then, when you're ready,
    you can extend MAX to support other parts of your AI pipeline and achieve even
    more performance, programmability, and portability.
  id: totrans-split-16
  prefs: []
  type: TYPE_NORMAL
- en: '**Quick performance & portability wins**'
  id: totrans-split-17
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '**MAX compiles and runs your existing AI models across a wide range of hardware,
    delivering immediate performance gains**. Getting started is as simple as using
    our Python or C API to replace your current PyTorch, TensorFlow, or ONNX inference
    calls with MAX Engine inference calls.'
  id: totrans-split-18
  prefs: []
  type: TYPE_NORMAL
- en: By changing only a few lines of code, [your models execute up to 5x faster](https://performance.modular.com/?task=recommenders&model=dlrm-rm1-multihot&instance=c7g.4xlarge&framework=stock-pytorch&view=chart),
    supercharging latency so you can productionize larger models while improving efficiency
    to significantly reduce compute costs compared to stock PyTorch, TensorFlow, or
    ONNX Runtime. And the MAX Engine provides full portability across a wide range
    of CPU architectures (Intel, AMD, ARM), with GPU support coming soon. This means
    you can seamlessly move to the best hardware for your use case without rewriting
    your model.
  id: totrans-split-19
  prefs: []
  type: TYPE_NORMAL
- en: Additionally, you can wrap the MAX Engine with MAX Serving as a backend to NVIDIA
    Triton Inference Server for a production-grade deployment stack. Triton provides
    gRPC and HTTP endpoints, efficiently handling incoming input requests.
  id: totrans-split-20
  prefs: []
  type: TYPE_NORMAL
- en: '**Extend & optimize your pipeline**'
  id: totrans-split-21
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '**Beyond executing existing models with MAX Engine, you can further optimize
    your performance with MAX‚Äôs unique extensibility and programmability capabilities**.
    MAX Engine is built using Mojo, and it is also fully extensible by you in Mojo.
    Today, the MAX Graph API enables you to build whole inference models in Mojo,
    consolidating the complexity that popular ‚Äúpoint solution‚Äù inference frameworks
    like llama.cpp provide - delivering better flexibility and better performance.
    [Coming soon](https://docs.modular.com/max/roadmap) you will even be able to write
    custom ops that can be natively fused by the MAX Engine compiler into your existing
    model graph.'
  id: totrans-split-22
  prefs: []
  type: TYPE_NORMAL
- en: Beyond model optimization with MAX Engine, you can further accelerate the rest
    of your AI pipeline by migrating your data pre/post-processing code and application
    code to Mojo, using native Mojo with the MAX Engine Mojo API. Our north star is
    to provide you with a massive amount of leverage, so you can get innovations in
    AI into your products faster.
  id: totrans-split-23
  prefs: []
  type: TYPE_NORMAL
- en: '**How does MAX work?**'
  id: totrans-split-24
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The foundation of MAX is [Mojo](https://www.modular.com/max/mojo), a common
    programming model for all AI hardware that aims to unify the entire AI stack,
    from graph kernels up to application layer, and provide a portable alternative
    to Python, C, and CUDA.
  id: totrans-split-25
  prefs: []
  type: TYPE_NORMAL
- en: '[MAX Engine](https://www.modular.com/max/engine) uses Mojo transparently to
    accelerate your AI models across a variety of AI hardware via a state-of-the-art
    compiler and runtime stack. Users can load and optimize existing models from PyTorch,
    TensorFlow, and ONNX with MAX Engine‚Äôs Python and C bindings, or build their own
    inference graphs using the Mojo Graph API. Additionally, we‚Äôve made it easy to
    use [MAX Serving](https://www.modular.com/max/serving) to quickly integrate and
    support your existing inference serving pipelines.'
  id: totrans-split-26
  prefs: []
  type: TYPE_NORMAL
- en: Because the internals of MAX are built in Mojo, you can always write native
    Mojo to further optimize the rest of your AI pipelines, including massively accelerating
    your pre- and post-processing logic and finally removing Python out of your production
    AI serving systems.
  id: totrans-split-27
  prefs: []
  type: TYPE_NORMAL
- en: '**A better developer experience**'
  id: totrans-split-28
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In addition to the preview release of the MAX Platform, we are also excited
    to announce the release of many enhancements to the MAX developer experience.
    These include:'
  id: totrans-split-29
  prefs: []
  type: TYPE_NORMAL
- en: '[**MAX Examples Repo**](https://github.com/modularml/max/tree/main/examples):
    The MAX GitHub repository provides plenty of rich examples of how to use MAX in
    practice. This includes inference examples for PyTorch and TensorFlow models with
    the MAX Engine Python and C APIs as well as a llama2.**üî•** model developed with
    the Graph API.'
  id: totrans-split-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[**New Documentation Site**](https://docs.modular.com/): Rebuilt from the ground
    up, our new documentation site features blazing fast search, better categorization
    and structure, richer code examples, and enhanced developer usability everywhere.
    Not to mention new tutorials for all the new MAX features!'
  id: totrans-split-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[**New ‚ÄúPlayground‚Äù experience for Mojo**](https://docs.modular.com/mojo/playground):
    Our new Mojo coding playground is perfect for early learnings and quick experimentation.
    You can start executing Mojo code within seconds and easily share it with your
    friends via Gist.'
  id: totrans-split-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[**New Developer Dashboard**](https://developer.modular.com/dashboard): We‚Äôve
    revamped and updated our Developer Dashboard, enabling you to access updates,
    the performance dashboard, and our MAX Enterprise features soon.'
  id: totrans-split-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**‚Ä¶ and much more**: There have been product improvements and fixes across
    almost every surface, and we‚Äôll continue rapidly iterating to deliver world class
    AI infrastructure for developers everywhere.'
  id: totrans-split-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**There‚Äôs more to come!**'
  id: totrans-split-35
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'All of this is available today in the [MAX Developer Edition preview](https://www.modular.com/max).
    We‚Äôre excited to launch this release, have MAX go global, and see what you build
    with it. While this is just our first preview release of MAX, we have many plans
    for subsequent releases in the near future: Mac support, enterprise features,
    and GPU support is all on the way. Please [see our roadmap](https://docs.modular.com/max/roadmap)
    for more information.'
  id: totrans-split-36
  prefs: []
  type: TYPE_NORMAL
- en: 'To learn more about MAX, and get involved with the Modular community:'
  id: totrans-split-37
  prefs: []
  type: TYPE_NORMAL
- en: We know this is just the beginning, and we‚Äôre excited to get out this release,
    listen and take in your feedback as we quickly iterate to improve MAX for the
    world. We‚Äôre on a mission to help push AI forward for the world, and enable AI
    to be used by anyone, anywhere - join us!
  id: totrans-split-38
  prefs: []
  type: TYPE_NORMAL
- en: Onwards!
  id: totrans-split-39
  prefs: []
  type: TYPE_NORMAL
- en: The Modular Team
  id: totrans-split-40
  prefs: []
  type: TYPE_NORMAL
