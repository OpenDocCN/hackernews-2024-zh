["```\nself_read, self_write = IO.pipe\nsigs = %w[INT TERM TTIN TSTP]\n# ...\nsigs.each do |sig|\n  old_handler = Signal.trap(sig) do\n    if old_handler.respond_to?(:call)\n      begin\n        old_handler.call\n      rescue Exception => exc\n        # signal handlers can't use Logger so puts only\n        puts [\"Error in #{sig} handler\", exc].inspect\n      end\n    end\n\n    self_write.puts(sig)\n  end\nrescue ArgumentError\n  puts \"Signal #{sig} not supported\"\nend \n```", "```\nint\nrb_cloexec_pipe(int descriptors[2])\n{\n#ifdef HAVE_PIPE2\n    int result = pipe2(descriptors, O_CLOEXEC | O_NONBLOCK);\n#else\n    int result = pipe(descriptors);\n#endif \n    if (result < 0)\n        return result;\n\n// ...\n} \n```", "```\nstatic sighandler_t\nruby_signal(int signum, sighandler_t handler)\n{\n    struct sigaction sigact, old;\n\n  // Code that populates the sigact struct is omitted\n\n  // Register the signal handler\n    if (sigaction(signum, &sigact, &old) < 0) {\n        return SIG_ERR;\n    }\n\n    if (old.sa_flags & SA_SIGINFO)\n        handler = (sighandler_t)old.sa_sigaction;\n    else\n        handler = old.sa_handler;\n    ASSUME(handler != SIG_ERR);\n\n    // Return the previous handler of the same signal\n    return handler;\n} \n```", "```\nold_handler = Signal.trap(sig) do\n  if old_handler.respond_to?(:call)\n    begin\n      old_handler.call\n    rescue Exception => exc\n      puts [\"Error in #{sig} handler\", exc].inspect\n    end\n  end\n  self_write.puts(sig)\nend \n```", "```\ndef launch(self_read)\n  @launcher = Sidekiq::Launcher.new(@config)\n\n  begin\n    launcher.run\n\n    while self_read.wait_readable\n      signal = self_read.gets.strip\n      handle_signal(signal)\n    end\n  rescue Interrupt\n    launcher.stop\n\n    exit(0)\n  end\nend \n```", "```\nSIGNAL_HANDLERS = {\n  \"INT\" => ->(cli) { raise Interrupt },\n  \"TERM\" => ->(cli) { raise Interrupt },\n  \"TSTP\" => ->(cli) {\n    cli.logger.info \"Received TSTP, no longer accepting new work\"\n    cli.launcher.quiet\n  },\n  \"TTIN\" => ->(cli) {\n    Thread.list.each do |thread|\n      cli.logger.warn \"Thread TID-#{(thread.object_id ^ ::Process.pid).to_s(36)}  #{thread.name}\"\n      if thread.backtrace\n        cli.logger.warn thread.backtrace.join(\"\\n\")\n      else\n        cli.logger.warn \"<no backtrace available>\"\n      end\n    end\n  }\n}\nUNHANDLED_SIGNAL_HANDLER = ->(cli) { cli.logger.info \"No signal handler registered, ignoring\" }\nSIGNAL_HANDLERS.default = UNHANDLED_SIGNAL_HANDLER\n\ndef handle_signal(sig)\n  logger.debug \"Got #{sig} signal\"\n  SIGNAL_HANDLERS[sig].call(self)\nend \n```", "```\ndef write(message)\n  begin\n    # ...\n    synchronize do # A convenience method that calls Monitor#synchronize\n      @dev.write(message)\n    end\n    # ...\n  rescue Exception => ignored\n    warn(\"log writing failed. #{ignored}\")\n  end\nend \n```", "```\n# Launcher\n\ndef initialize(config)\n  # ...\n  @managers = config.capsules.values.map do |cap|\n    Sidekiq::Manager.new(cap)\n  end\n  # ...\nend \n```", "```\n# Launcher\n\ndef run\n  @thread = safe_thread(\"heartbeat\", &method(:start_heartbeat))\n  @managers.each(&:start)\nend \n```", "```\n# Manager\n\ndef start\n  @workers.each(&:start)\nend \n```", "```\n# Processor\n\ndef start\n  @thread ||= safe_thread(\"#{config.name}/processor\", &method(:run))\nend \n```", "```\n# Launcher\n\ndef quiet\n  return if @done\n\n  @done = true\n  @managers.each(&:quiet)\nend \n```", "```\n# Manager\n\ndef quiet\n  return if @done\n  @done = true\n\n  @workers.each(&:terminate)\nend \n```", "```\n# Processor\n\ndef terminate\n  @done = true\nend \n```", "```\n# Processor\n\ndef run\n  process_one until @done\n  @callback.call(self)\nrescue Sidekiq::Shutdown\n  @callback.call(self)\nrescue Exception => ex\n  @callback.call(self, ex)\nend \n```", "```\nclass Manager\n  def initialize(capsule)\n    @done = false\n    @workers = Set.new\n    @plock = Mutex.new\n    @count.times do\n      @workers << Processor.new(@config, &method(:processor_result))\n    end\n  end\n\n  # ...\n\n  def processor_result(processor, reason = nil)\n    @plock.synchronize do\n      @workers.delete(processor)\n      unless @done\n        p = Processor.new(@config, &method(:processor_result))\n        @workers << p\n        p.start\n      end\n    end\n  end\nend \n```", "```\n# Launcher\n\ndef stop\n  deadline = ::Process.clock_gettime(::Process::CLOCK_MONOTONIC) + @config[:timeout]\n\n  quiet\n  stoppers = @managers.map do |mgr|\n    Thread.new do\n      mgr.stop(deadline)\n    end\n  end\n\n  stoppers.each(&:join)\nend \n```", "```\n# Manager\n\ndef stop(deadline)\n  quiet\n\n  return if @workers.empty?\n\n  logger.info { \"Pausing to allow jobs to finish...\" }\n  wait_for(deadline) { @workers.empty? }\n  return if @workers.empty?\n\n  hard_shutdown\nensure\n  capsule.stop\nend \n```", "```\n# Manager\n\ndef wait_for(deadline, &condblock)\n  remaining = deadline - ::Process.clock_gettime(::Process::CLOCK_MONOTONIC)\n  while remaining > PAUSE_TIME\n    return if condblock.call\n    sleep PAUSE_TIME\n    remaining = deadline - ::Process.clock_gettime(::Process::CLOCK_MONOTONIC)\n  end\nend \n```", "```\n# Manager\n\ndef hard_shutdown\n  # We've reached the timeout and we still have busy threads.\n  # They must die but their jobs shall live on.\n  cleanup = nil\n  @plock.synchronize do\n    cleanup = @workers.dup\n  end\n\n  if cleanup.size > 0\n    jobs = cleanup.map { |p| p.job }.compact\n    # ...\n    capsule.fetcher.bulk_requeue(jobs)\n  end\n\n  cleanup.each do |processor|\n    processor.kill\n  end\n\n  # when this method returns, we immediately call `exit` which may not give\n  # the remaining threads time to run `ensure` blocks, etc. We pause here up\n  # to 3 seconds to give threads a minimal amount of time to run `ensure` blocks.\n  deadline = ::Process.clock_gettime(::Process::CLOCK_MONOTONIC) + 3\n  wait_for(deadline) { @workers.empty? }\nend \n```", "```\n# Processor\n\ndef kill(wait = false)\n  @done = true\n  return unless @thread\n\n  @thread.raise ::Sidekiq::Shutdown\nend \n```", "```\n# Sidekiq checks queues in three modes:\n# - :strict - all queues have 0 weight and are checked strictly in order\n# - :weighted - queues have arbitrary weight between 1 and N\n# - :random - all queues have weight of 1\ndef queues=(val)\n  @weights = {}\n  @queues = Array(val).each_with_object([]) do |qstr, memo|\n    arr = qstr\n    arr = qstr.split(\",\") if qstr.is_a?(String)\n    name, weight = arr\n    @weights[name] = weight.to_i\n    [weight.to_i, 1].max.times do\n      memo << name\n    end\n  end\n  @mode = if @weights.values.all?(&:zero?)\n    :strict\n  elsif @weights.values.all? { |x| x == 1 }\n    :random\n  else\n    :weighted\n  end\nend \n```", "```\n# Processor\n\ndef get_one\n  uow = capsule.fetcher.retrieve_work\n  if @down\n    logger.info { \"Redis is online, #{::Process.clock_gettime(::Process::CLOCK_MONOTONIC) - @down} sec downtime\" }\n    @down = nil\n  end\n  uow\nrescue Sidekiq::Shutdown\nrescue => ex\n  handle_fetch_exception(ex)\nend \n```", "```\ndef fetcher\n  @fetcher ||= begin\n    inst = (config[:fetch_class] || Sidekiq::BasicFetch).new(self)\n    inst.setup(config[:fetch_setup]) if inst.respond_to?(:setup)\n    inst\n  end\nend \n```", "```\nclass BasicFetch\n  # We want the fetch operation to timeout every few seconds so the thread\n  # can check if the process is shutting down.\n  TIMEOUT = 2\n\n  UnitOfWork = Struct.new(:queue, :job, :config) {\n    def acknowledge\n      # nothing to do\n    end\n\n    def queue_name\n      queue.delete_prefix(\"queue:\")\n    end\n\n    def requeue\n      config.redis do |conn|\n        conn.rpush(queue, job)\n      end\n    end\n  }\n\n  # ...\n\n  def retrieve_work\n    qs = queues_cmd\n    # 4825 Sidekiq Pro with all queues paused will return an\n    # empty set of queues\n    if qs.size <= 0\n      sleep(TIMEOUT)\n      return nil\n    end\n\n    queue, job = redis { |conn| conn.blocking_call(conn.read_timeout + TIMEOUT, \"brpop\", *qs, TIMEOUT) }\n    UnitOfWork.new(queue, job, config) if queue\n  end\n\n  # ...\n\n  # Creating the Redis#brpop command takes into account any\n  # configured queue weights. By default Redis#brpop returns\n  # data from the first queue that has pending elements. We\n  # recreate the queue command each time we invoke Redis#brpop\n  # to honor weights and avoid queue starvation.\n  def queues_cmd\n    if @strictly_ordered_queues\n      @queues\n    else\n      permute = @queues.shuffle\n      permute.uniq!\n      permute\n    end\n  end\nend \n```", "```\n# Processor\n\ndef process(uow)\n  jobstr = uow.job\n  queue = uow.queue_name\n\n  job_hash = nil\n  begin\n    job_hash = JSON.parse(jobstr)\n  rescue => ex\n    handle_exception(ex, {context: \"Invalid JSON for job\", jobstr: jobstr})\n    now = Time.now.to_f\n    redis do |conn|\n      conn.multi do |xa|\n        xa.zadd(\"dead\", now.to_s, jobstr)\n        xa.zremrangebyscore(\"dead\", \"-inf\", now - @capsule.config[:dead_timeout_in_seconds])\n        xa.zremrangebyrank(\"dead\", 0, - @capsule.config[:dead_max_jobs])\n      end\n    end\n    return uow.acknowledge\n  end\n\n  # ...\nend \n```", "```\n# Processor\n\ndef process(uow)\n  # ...\n\n  ack = false\n  Thread.handle_interrupt(Sidekiq::Shutdown => :never) do\n    Thread.handle_interrupt(Sidekiq::Shutdown => :immediate) do\n      dispatch(job_hash, queue, jobstr) do |inst|\n        config.server_middleware.invoke(inst, job_hash, queue) do\n          execute_job(inst, job_hash[\"args\"])\n        end\n      end\n      ack = true\n    rescue Sidekiq::Shutdown\n      # Had to force kill this job because it didn't finish\n      # within the timeout.  Don't acknowledge the work since\n      # we didn't properly finish it.\n    rescue Sidekiq::JobRetry::Handled => h\n      # this is the common case: job raised error and Sidekiq::JobRetry::Handled\n      # signals that we created a retry successfully.  We can acknowlege the job.\n      ack = true\n      e = h.cause || h\n      handle_exception(e, {context: \"Job raised exception\", job: job_hash})\n      raise e\n    rescue Exception => ex\n      # Unexpected error!  This is very bad and indicates an exception that got past\n      # the retry subsystem (e.g. network partition).  We won't acknowledge the job\n      # so it can be rescued when using Sidekiq Pro.\n      handle_exception(ex, {context: \"Internal exception!\", job: job_hash, jobstr: jobstr})\n      raise ex\n    end\n  ensure\n    if ack\n      uow.acknowledge\n    end\n  end\nend \n```", "```\n# Processor\n# Relevant bits from #dispatch, #process & #execute_job combined together\n\n@job_logger.prepare(job_hash) do\n  @retrier.global(jobstr, queue) do\n    @job_logger.call(job_hash, queue) do\n      stats(jobstr, queue) do\n        @reloader.call do\n          klass = Object.const_get(job_hash[\"class\"])\n          inst = klass.new\n          inst.jid = job_hash[\"jid\"]\n          @retrier.local(inst, jobstr, queue) do\n            config.server_middleware.invoke(inst, job_hash, queue) do\n              inst.perform(*job_hash[\"args\"])\n            end\n          end\n        end\n      end\n    end\n  end\nend \n```", "```\nclass JobRetry\n  class Handled < ::RuntimeError; end\n  class Skip < Handled; end\n\n  # ...\n\n  def local(jobinst, jobstr, queue)\n    yield\n  rescue Handled => ex\n    raise ex\n  rescue Sidekiq::Shutdown => ey\n    raise ey\n  rescue Exception => e\n    raise Sidekiq::Shutdown if exception_caused_by_shutdown?(e)\n\n    msg = JSON.parse(jobstr)\n    if msg[\"retry\"].nil?\n      msg[\"retry\"] = jobinst.class.get_sidekiq_options[\"retry\"]\n    end\n\n    raise e unless msg[\"retry\"]\n    process_retry(jobinst, msg, queue, e)\n\n    raise Skip\n  end\nend \n```", "```\n# JobRetry\n\ndef process_retry(jobinst, msg, queue, exception)\n  max_retry_attempts = retry_attempts_from(msg[\"retry\"], @max_retries)\n\n  msg[\"queue\"] = (msg[\"retry_queue\"] || queue)\n\n  # Code that updates the `retry_count` job payload attribute, saves the `retried_at` and `failed_at` timestamps, filters and saves the backtrace, etc\n  count = msg[\"retry_count\"] # Increment logic of this value is omitted\n\n  return retries_exhausted(jobinst, msg, exception) if count >= max_retry_attempts\n\n  rf = msg[\"retry_for\"]\n  return retries_exhausted(jobinst, msg, exception) if rf && ((msg[\"failed_at\"] + rf) < Time.now.to_f)\n\n  strategy, delay = delay_for(jobinst, count, exception, msg)\n  case strategy\n  when :discard\n    return\n  when :kill\n    return retries_exhausted(jobinst, msg, exception)\n  end\n\n  jitter = rand(10) * (count + 1)\n  retry_at = Time.now.to_f + delay + jitter\n  payload = JSON.parse(msg)\n  redis do |conn|\n    conn.zadd(\"retry\", retry_at.to_s, payload)\n  end\nend \n```", "```\n# JobRetry\ndef delay_for(jobinst, count, exception, msg)\n  rv = begin\n    block = jobinst&.sidekiq_retry_in_block\n    # ...\n    block&.call(count, exception, msg)\n  rescue Exception => e\n    # ...\n    nil\n  end\n\n  rv = rv.to_i if rv.respond_to?(:to_i)\n  delay = (count**4) + 15\n  if Integer === rv && rv > 0\n    delay = rv\n  elsif rv == :discard\n    return [:discard, nil]\n  elsif rv == :kill\n    return [:kill, nil]\n  end\n\n  [:default, delay]\nend \n```", "```\n# JobRetry\n\ndef retries_exhausted(jobinst, msg, exception)\n  rv = begin\n    block = jobinst&.sidekiq_retries_exhausted_block\n    # ...\n    block&.call(msg, exception)\n  rescue => e\n    # Log the error and do not reraise it\n    # ...\n  end\n\n  return if rv == :discard\n  unless msg[\"dead\"] == false\n    payload = Sidekiq.dump_json(msg)\n    now = Time.now.to_f\n\n    redis do |conn|\n      conn.multi do |xa|\n        xa.zadd(\"dead\", now.to_s, payload)\n        xa.zremrangebyscore(\"dead\", \"-inf\", now - @capsule.config[:dead_timeout_in_seconds])\n        xa.zremrangebyrank(\"dead\", 0, - @capsule.config[:dead_max_jobs])\n      end\n    end\n  end\nend \n```", "```\n# JobRetry\n\ndef global(jobstr, queue)\n  yield\nrescue Handled => ex\n  raise ex\nrescue Sidekiq::Shutdown => ey\n  raise ey\nrescue Exception => e\n  raise Sidekiq::Shutdown if exception_caused_by_shutdown?(e)\n\n  msg = Sidekiq.load_json(jobstr)\n  if msg[\"retry\"]\n    process_retry(nil, msg, queue, e)\n  else\n    @capsule.config.death_handlers.each do |handler|\n      handler.call(msg, e)\n    rescue => handler_ex\n      handle_exception(handler_ex, {context: \"Error calling death handler\", job: msg})\n    end\n  end\n\n  raise Handled\nend \n```", "```\n# Client\n\ndef push(item)\n  normed = normalize_item(item)\n  payload = middleware.invoke(item[\"class\"], normed, normed[\"queue\"], @redis_pool) do\n    normed\n  end\n  if payload\n    verify_json(payload)\n    raw_push([payload])\n    payload[\"jid\"]\n  end\nend \n```", "```\ndef verify_json(item)\n  job_class = item[\"wrapped\"] || item[\"class\"]\n  args = item[\"args\"]\n  mode = Sidekiq::Config::DEFAULTS[:on_complex_arguments]\n\n  if mode == :raise || mode == :warn\n    if (unsafe_item = json_unsafe?(args))\n      msg = <<~EOM Job arguments to #{job_class} must be native JSON types, but #{unsafe_item.inspect} is a #{unsafe_item.class}.\n        See https://github.com/sidekiq/sidekiq/wiki/Best-Practices\n        To disable this error, add `Sidekiq.strict_args!(false)` to your initializer.\n EOM\n\n      if mode == :raise\n        raise(ArgumentError, msg)\n      else\n        warn(msg)\n      end\n    end\n  end\nend\n\n# ...\n\nRECURSIVE_JSON_UNSAFE = {\n  Integer => ->(val) {},\n  Float => ->(val) {},\n  TrueClass => ->(val) {},\n  FalseClass => ->(val) {},\n  NilClass => ->(val) {},\n  String => ->(val) {},\n  Array => ->(val) {\n    val.each do |e|\n      unsafe_item = RECURSIVE_JSON_UNSAFE[e.class].call(e)\n      return unsafe_item unless unsafe_item.nil?\n    end\n    nil\n  },\n  Hash => ->(val) {\n    val.each do |k, v|\n      return k unless String === k\n\n      unsafe_item = RECURSIVE_JSON_UNSAFE[v.class].call(v)\n      return unsafe_item unless unsafe_item.nil?\n    end\n    nil\n  }\n}\n\nRECURSIVE_JSON_UNSAFE.default = ->(val) { val }\nRECURSIVE_JSON_UNSAFE.compare_by_identity\nprivate_constant :RECURSIVE_JSON_UNSAFE\n\ndef json_unsafe?(item)\n  RECURSIVE_JSON_UNSAFE[item.class].call(item)\nend \n```", "```\n# Client\n\ndef raw_push(payloads)\n  @redis_pool.with do |conn|\n    retryable = true\n    begin\n      conn.pipelined do |pipeline|\n        atomic_push(pipeline, payloads)\n      end\n    rescue RedisClient::Error => ex\n      if retryable && ex.message =~ /READONLY|NOREPLICAS|UNBLOCKED/\n        conn.close\n        retryable = false\n        retry\n      end\n      raise\n    end\n  end\n  true\nend\n\ndef atomic_push(conn, payloads)\n  if payloads.first.key?(\"at\")\n    conn.zadd(\"schedule\", payloads.flat_map { |hash|\n      at = hash.delete(\"at\").to_s\n      # ...\n      [at, JSON.generate(hash)]\n    })\n  else\n    queue = payloads.first[\"queue\"]\n    now = Time.now.to_f\n    to_push = payloads.map { |entry|\n      entry[\"enqueued_at\"] = now\n      JSON.generate(entry)\n    }\n    conn.sadd(\"queues\", [queue])\n    conn.lpush(\"queue:#{queue}\", to_push)\n  end\nend \n```", "```\ndef start\n  @thread ||= safe_thread(\"scheduler\") {\n    initial_wait\n\n    until @done\n      enqueue\n      wait\n    end\n  }\nend \n```", "```\n# Scheduled::Poller\n\nINITIAL_WAIT = 10\n\n# ...\n\ndef initial_wait\n  # Have all processes sleep between 5-15 seconds. 10 seconds to give time for\n  # the heartbeat to register (if the poll interval is going to be calculated by the number\n  # of workers), and 5 random seconds to ensure they don't all hit Redis at the same time.\n  total = 0\n  total += INITIAL_WAIT unless @config[:poll_interval_average]\n  total += (5 * rand)\n\n  @sleeper.pop(total)\nrescue Timeout::Error\nensure\n  # periodically clean out the `processes` set in Redis which can collect\n  # references to dead processes over time. The process count affects how\n  # often we scan for scheduled jobs.\n  cleanup\nend \n```", "```\n# Scheduled::Poller\n\ndef cleanup\n  # dont run cleanup more than once per minute\n  return 0 unless redis { |conn| conn.set(\"process_cleanup\", \"1\", \"NX\", \"EX\", \"60\") }\n\n  count = 0\n  redis do |conn|\n    procs = conn.sscan(\"processes\").to_a\n    heartbeats = conn.pipelined { |pipeline|\n      procs.each do |key|\n        pipeline.hget(key, \"info\")\n      end\n    }\n\n    # the hash named key has an expiry of 60 seconds.\n    # if it's not found, that means the process has not reported\n    # in to Redis and probably died.\n    to_prune = procs.select.with_index { |proc, i|\n      heartbeats[i].nil?\n    }\n    count = conn.srem(\"processes\", to_prune) unless to_prune.empty?\n  end\n\n  count\nend \n```", "```\n# Scheduled::Poller\n\ndef wait\n  @sleeper.pop(random_poll_interval)\nrescue Timeout::Error\n  # expected\nrescue => ex\n  # if poll_interval_average hasn't been calculated yet, we can\n  # raise an error trying to reach Redis.\n  logger.error ex.message\n  handle_exception(ex)\n  sleep 5\nend\n\ndef random_poll_interval\n  # We want one Sidekiq process to schedule jobs every N seconds.  We have M processes\n  # and **don't** want to coordinate.\n  #\n  # So in N*M second timespan, we want each process to schedule once.  The basic loop is:\n  #\n  # * sleep a random amount within that N*M timespan\n  # * wake up and schedule\n  #\n  # We want to avoid one edge case: imagine a set of 2 processes, scheduling every 5 seconds,\n  # so N*M = 10\\.  Each process decides to randomly sleep 8 seconds, now we've failed to meet\n  # that 5 second average. Thankfully each schedule cycle will sleep randomly so the next\n  # iteration could see each process sleep for 1 second, undercutting our average.\n  #\n  # So below 10 processes, we special case and ensure the processes sleep closer to the average.\n  # In the example above, each process should schedule every 10 seconds on average. We special\n  # case smaller clusters to add 50% so they would sleep somewhere between 5 and 15 seconds.\n  # As we run more processes, the scheduling interval average will approach an even spread\n  # between 0 and poll interval so we don't need this artifical boost.\n  #\n  count = process_count\n  interval = poll_interval_average(count)\n\n  if count < 10\n    # For small clusters, calculate a random interval that is ±50% the desired average.\n    interval * rand + interval.to_f / 2\n  else\n    # With 10+ processes, we should have enough randomness to get decent polling\n    # across the entire timespan\n    interval * rand\n  end\nend\n\n# We do our best to tune the poll interval to the size of the active Sidekiq\n# cluster.  If you have 30 processes and poll every 15 seconds, that means one\n# Sidekiq is checking Redis every 0.5 seconds - way too often for most people\n# and really bad if the retry or scheduled sets are large.\n#\n# Instead try to avoid polling more than once every 15 seconds.  If you have\n# 30 Sidekiq processes, we'll poll every 30 * 15 or 450 seconds.\n# To keep things statistically random, we'll sleep a random amount between\n# 225 and 675 seconds for each poll or 450 seconds on average.  Otherwise restarting\n# all your Sidekiq processes at the same time will lead to them all polling at\n# the same time: the thundering herd problem.\n#\n# We only do this if poll_interval_average is unset (the default).\ndef poll_interval_average(count)\n  @config[:poll_interval_average] || scaled_poll_interval(count)\nend\n\n# Calculates an average poll interval based on the number of known Sidekiq processes.\n# This minimizes a single point of failure by dispersing check-ins but without taxing\n# Redis if you run many Sidekiq processes.\ndef scaled_poll_interval(process_count)\n  process_count * @config[:average_scheduled_poll_interval]\nend\n\ndef process_count\n  pcount = Sidekiq.redis { |conn| conn.scard(\"processes\") }\n  pcount = 1 if pcount == 0\n  pcount\nend \n```", "```\nclass Enq\n  LUA_ZPOPBYSCORE = <<~LUA local key, now = KEYS[1], ARGV[1]\n    local jobs = redis.call(\"zrange\", key, \"-inf\", now, \"byscore\", \"limit\", 0, 1)\n    if jobs[1] then\n      redis.call(\"zrem\", key, jobs[1])\n      return jobs[1]\n    end LUA\n\n  # ...\n\n  def enqueue_jobs(sorted_sets = %w[retry schedule])\n    # A job's \"score\" in Redis is the time at which it should be processed.\n    # Just check Redis for the set of jobs with a timestamp before now.\n    redis do |conn|\n      sorted_sets.each do |sorted_set|\n        # Get next item in the queue with score (time to execute) <= now.\n        # We need to go through the list one at a time to reduce the risk of something\n        # going wrong between the time jobs are popped from the scheduled queue and when\n        # they are pushed onto a work queue and losing the jobs.\n        while !@done && (job = zpopbyscore(conn, keys: [sorted_set], argv: [Time.now.to_f.to_s]))\n          @client.push(JSON.generate(job))\n        end\n      end\n    end\n  end\n\n  # ...\n\n  private\n\n  def zpopbyscore(conn, keys: nil, argv: nil)\n    if @lua_zpopbyscore_sha.nil?\n      @lua_zpopbyscore_sha = conn.script(:load, LUA_ZPOPBYSCORE)\n    end\n\n    conn.call(\"EVALSHA\", @lua_zpopbyscore_sha, keys.size, *keys, *argv)\n  rescue RedisClient::CommandError => e\n    raise unless e.message.start_with?(\"NOSCRIPT\")\n\n    @lua_zpopbyscore_sha = nil\n    retry\n  end\nend \n```", "```\n# Manager\n\ndef hard_shutdown\n  cleanup = nil\n  @plock.synchronize do\n    cleanup = @workers.dup\n  end\n\n  if cleanup.size > 0\n    jobs = cleanup.map { |p| p.job }.compact\n\n    capsule.fetcher.bulk_requeue(jobs)\n  end\n\n  cleanup.each do |processor|\n    processor.kill\n  end\n\n  # ...\nend \n```", "```\n# BasicFetch\n\ndef bulk_requeue(inprogress)\n  return if inprogress.empty?\n\n  # ...\n  jobs_to_requeue = {}\n  inprogress.each do |unit_of_work|\n    jobs_to_requeue[unit_of_work.queue] ||= []\n    jobs_to_requeue[unit_of_work.queue] << unit_of_work.job\n  end\n\n  redis do |conn|\n    conn.pipelined do |pipeline|\n      jobs_to_requeue.each do |queue, jobs|\n        pipeline.rpush(queue, jobs)\n      end\n    end\n  end\n  # ...\nend \n```"]