- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-05-27 15:03:37'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: AnyGPT
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://junzhan2000.github.io/AnyGPT.github.io/](https://junzhan2000.github.io/AnyGPT.github.io/)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: We introduce AnyGPT, an any-to-any multimodal language model that utilizes discrete
    representations for the unified processing of various modalities, including speech,
    text, images, and music. AnyGPT can be trained stably without any alterations
    to the current large language model (LLM) architecture or training paradigms.
    Instead, it relies exclusively on data-level preprocessing, facilitating the seamless
    integration of new modalities into LLMs, akin to the incorporation of new languages.
    We build a multimodal text-centric dataset for multimodal alignment pre-training.
    Utilizing generative models, we synthesize the first large-scale any-to-any multimodal
    instruction dataset. It consists of 108k samples of multi-turn conversations that
    intricately interweave various modalities, thus equipping the model to handle
    arbitrary combinations of multimodal inputs and outputs. Experimental results
    demonstrate that AnyGPT is capable of facilitating any-to-any multimodal conversation
    while achieving performance comparable to specialized models across all modalities,
    proving that discrete representations can effectively and conveniently unify multiple
    modalities within a language model.
  prefs: []
  type: TYPE_NORMAL
