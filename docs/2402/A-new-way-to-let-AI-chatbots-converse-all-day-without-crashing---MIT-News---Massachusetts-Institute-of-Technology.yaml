- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 类别：未分类
- en: 'date: 2024-05-27 14:49:51'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-05-27 14:49:51
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: A new way to let AI chatbots converse all day without crashing | MIT News |
    Massachusetts Institute of Technology
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 让 AI 聊天机器人全天候交谈的新方法 | 麻省理工学院新闻 | 麻省理工学院
- en: 来源：[https://news.mit.edu/2024/new-way-let-ai-chatbots-converse-all-day-without-crashing-0213](https://news.mit.edu/2024/new-way-let-ai-chatbots-converse-all-day-without-crashing-0213)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://news.mit.edu/2024/new-way-let-ai-chatbots-converse-all-day-without-crashing-0213](https://news.mit.edu/2024/new-way-let-ai-chatbots-converse-all-day-without-crashing-0213)
- en: When a human-AI conversation involves many rounds of continuous dialogue, the
    powerful large language machine-learning models that drive chatbots like ChatGPT
    sometimes start to collapse, causing the bots’ performance to rapidly deteriorate.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 当人类与人工智能进行多轮连续对话时，驱动 ChatGPT 等聊天机器人的强大大型语言机器学习模型有时会开始崩溃，导致机器人的性能迅速下降。
- en: A team of researchers from MIT and elsewhere has pinpointed a surprising cause
    of this problem and developed a simple solution that enables a chatbot to maintain
    a nonstop conversation without crashing or slowing down.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 麻省理工学院和其他地方的一个研究团队确定了这个问题的一个令人惊讶的原因，并开发了一个简单的解决方案，使聊天机器人能够保持持续对话而不崩溃或减慢速度。
- en: Their method involves a tweak to the key-value cache (which is like a conversation
    memory) at the core of many large language models. In some methods, when this
    cache needs to hold more information than it has capacity for, the first pieces
    of data are bumped out. This can cause the model to fail.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 他们的方法涉及对许多大型语言模型核心的键值缓存（类似于对话记忆）进行微调。在某些方法中，当此缓存需要容纳比其容量更多的信息时，将会挤出数据的第一部分。这可能会导致模型失败。
- en: By ensuring that these first few data points remain in memory, the researchers’
    method allows a chatbot to keep chatting no matter how long the conversation goes.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 通过确保这些最初的几个数据点保留在内存中，研究人员的方法允许聊天机器人继续聊天，无论对话持续多长时间。
- en: The method, called StreamingLLM, enables a model to remain efficient even when
    a conversation stretches on for more than 4 million words. When compared to another
    method that avoids crashing by constantly recomputing part of the past conversations,
    StreamingLLM performed more than 22 times faster.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法称为 StreamingLLM，使模型能够在对话超过 400 万字的情况下仍然保持效率。与另一种避免崩溃的方法相比，该方法通过不断重新计算过去对话的部分来保持性能，StreamingLLM
    的性能提高了 22 倍以上。
- en: This could allow a chatbot to conduct long conversations throughout the workday
    without needing to be continually rebooted, enabling efficient AI assistants for
    tasks like copywriting, editing, or generating code.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 这可以使聊天机器人在工作日整天进行长时间对话而无需持续重新启动，为诸如文案撰写、编辑或生成代码等任务提供高效的人工智能助手。
- en: “Now, with this method, we can persistently deploy these large language models.
    By making a chatbot that we can always chat with, and that can always respond
    to us based on our recent conversations, we could use these chatbots in some new
    applications,” says Guangxuan Xiao, an electrical engineering and computer science
    (EECS) graduate student and lead author of a paper on StreamingLLM.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: “现在，有了这种方法，我们可以持续部署这些大型语言模型。通过制作一个我们可以始终与之交谈，并且可以根据我们最近的对话始终对我们做出响应的聊天机器人，我们可以在一些新应用中使用这些聊天机器人，”电气工程和计算机科学（EECS）研究生、StreamingLLM
    论文的第一作者肖广选说道。
- en: Xiao’s co-authors include his advisor, Song Han, an associate professor in EECS,
    a member of the MIT-IBM Watson AI Lab, and a distinguished scientist of NVIDIA;
    as well as Yuandong Tian, a research scientist at Meta AI; Beidi Chen, an assistant
    professor at Carnegie Mellon University; and senior author Mike Lewis, a research
    scientist at Meta AI. The work will be presented at the International Conference
    on Learning Representations.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 肖的合作者包括他的导师、EECS 副教授、麻省理工学院-IBM 沃森人工智能实验室成员、NVIDIA杰出科学家 Song Han；以及 Meta AI
    研究科学家田原东；卡内基梅隆大学助理教授陈北迪；以及 Meta AI 研究科学家迈克·刘易斯。该工作将在国际学习表示会议上进行介绍。
- en: '**A puzzling phenomenon**'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: '**一个令人费解的现象**'
- en: Large language models encode data, like words in a user query, into representations
    called tokens. Many models employ what is known as an attention mechanism that
    uses these tokens to generate new text.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型将数据（如用户查询中的单词）编码为称为标记的表示形式。许多模型采用所谓的注意机制，使用这些标记生成新文本。
- en: Typically, an AI chatbot writes new text based on text it has just seen, so
    it stores recent tokens in memory, called a KV Cache, to use later. The attention
    mechanism builds a grid that includes all tokens in the cache, an “attention map”
    that maps out how strongly each token, or word, relates to each other token.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，人工智能聊天机器人根据其刚刚看到的文本编写新文本，因此它会将最近的标记存储在内存中，称为 KV 缓存，以便以后使用。注意机制构建了一个网格，其中包括缓存中的所有标记，一种“注意力图”将每个标记或单词与其他标记相关联的程度映射出来。
- en: Understanding these relationships is one feature that enables large language
    models to generate human-like text.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 理解这些关系是使大型语言模型能够生成类似人类文本的一个特征。
- en: But when the cache gets very large, the attention map can become even more massive,
    which slows down computation.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，当缓存变得非常大时，注意力图可能会变得更加庞大，从而减慢计算速度。
- en: Also, if encoding content requires more tokens than the cache can hold, the
    model’s performance drops. For instance, one popular model can store 4,096 tokens,
    yet there are about 10,000 tokens in an academic paper.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，如果编码内容所需的标记数量超过了缓存的容量，模型的性能将下降。例如，一个流行的模型可以存储 4,096 个标记，但学术论文中大约有 10,000
    个标记。
- en: To get around these problems, researchers employ a “sliding cache” that bumps
    out the oldest tokens to add new tokens. However, the model’s performance often
    plummets as soon as that first token is evicted, rapidly reducing the quality
    of newly generated words.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决这些问题，研究人员使用“滑动缓存”来将最老的标记挤出去以添加新的标记。然而，一旦第一个标记被清除，模型的性能往往会急剧下降，迅速降低新生成的单词的质量。
- en: In this new paper, researchers realized that if they keep the first token in
    the sliding cache, the model will maintain its performance even when the cache
    size is exceeded.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇新论文中，研究人员意识到，如果他们保留滑动缓存中的第一个标记，即使超出缓存大小，模型仍将保持其性能。
- en: But this didn’t make any sense. The first word in a novel likely has nothing
    to do with the last word, so why would the first word be so important for the
    model to generate the newest word?
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 但这毫无意义。小说中的第一个词很可能与最后一个词无关，那么为什么第一个词对于模型生成最新词语如此重要呢？
- en: In their new paper, the researchers also uncovered the cause of this phenomenon.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 在他们的新论文中，研究人员还揭示了这一现象的原因。
- en: '**Attention sinks**'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: '**注意力沉积**'
- en: Some models use a Softmax operation in their attention mechanism, which assigns
    a score to each token that represents how much it relates to each other token.
    The Softmax operation requires all attention scores to sum up to 1\. Since most
    tokens aren’t strongly related, their attention scores are very low. The model
    dumps any remaining attention score in the first token.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 一些模型在其注意机制中使用 Softmax 操作，该操作为每个标记分配一个分数，表示其与其他每个标记的相关程度。Softmax 操作要求所有注意力分数总和为
    1。由于大多数标记并不强相关，它们的注意力分数很低。模型将任何剩余的注意力分数都存储在第一个标记中。
- en: The researchers call this first token an “attention sink.”
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 研究人员称这个第一个标记为“注意力沉积”。
- en: “We need an attention sink, and the model decides to use the first token as
    the attention sink because it is globally visible — every other token can see
    it. We found that we must always keep the attention sink in the cache to maintain
    the model dynamics,” Han says.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: “我们需要一个注意力汇聚点，模型决定使用第一个标记作为注意力汇聚点，因为它是全局可见的 —— 每个其他标记都能看到它。我们发现我们必须始终保持缓存中的注意力汇聚点，以维持模型动态，”韩说。
- en: In building StreamingLLM, the researchers discovered that having four attention
    sink tokens at the beginning of the sliding cache leads to optimal performance.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 在构建 StreamingLLM 时，研究人员发现在滑动缓存的开头放置四个注意力汇聚点标记可以实现最佳性能。
- en: They also found that the positional encoding of each token must stay the same,
    even as new tokens are added and others are bumped out. If token 5 is bumped out,
    token 6 must stay encoded as 6, even though it is now the fifth token in the cache.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 他们还发现，每个标记的位置编码必须保持不变，即使添加了新标记并使其他标记被挤出。如果标记 5 被挤出，那么标记 6 必须保持编码为 6，即使它现在是缓存中的第五个标记。
- en: By combining these two ideas, they enabled StreamingLLM to maintain a continuous
    conversation while outperforming a popular method that uses recomputation.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 结合这两个想法，他们使 StreamingLLM 能够在胜过使用重新计算的流行方法的同时保持持续对话。
- en: For instance, when the cache has 256 tokens, the recomputation method takes
    63 milliseconds to decode a new token, while StreamingLLM takes 31 milliseconds.
    However, if the cache size grows to 4,096 tokens, recomputation requires 1,411
    milliseconds for a new token, while StreamingLLM needs just 65 milliseconds.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，当缓存有 256 个标记时，重新计算方法需要 63 毫秒来解码新标记，而 StreamingLLM 需要 31 毫秒。然而，如果缓存大小增长到 4,096
    个标记，重新计算需要 1,411 毫秒来解码新标记，而 StreamingLLM 只需 65 毫秒。
- en: “The innovative approach of StreamingLLM, centered around the attention sink
    mechanism, ensures stable memory usage and performance, even when processing texts
    up to 4 million tokens in length,” says Yang You, a presidential young professor
    of computer science at the National University of Singapore, who was not involved
    with this work. “This capability is not just impressive; it's transformative,
    enabling StreamingLLM to be applied across a wide array of AI applications. The
    performance and versatility of StreamingLLM mark it as a highly promising technology,
    poised to revolutionize how we approach AI-driven generation applications.”
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: “以关注沉积机制为中心的 StreamingLLM 创新方法，确保内存使用稳定，性能稳定，即使处理长度达到 400 万个标记的文本也是如此，”新加坡国立大学计算机科学青年教授杨友说，他并未参与此项工作。“这种能力不仅令人印象深刻，而且具有变革性，使
    StreamingLLM 能够应用于各种人工智能应用。StreamingLLM 的性能和多功能性使其成为一项极具前景的技术，有望彻底改变我们处理基于人工智能的生成应用的方式。”
- en: Tianqi Chen, an assistant professor in the machine learning and computer science
    departments at Carnegie Mellon University who also was not involved with this
    research, agreed, saying “Streaming LLM enables the smooth extension of the conversation
    length of large language models. We have been using it to enable the deployment
    of Mistral models on iPhones with great success.”
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 也没参与这项研究的卡内基梅隆大学机器学习和计算机科学系助理教授田奇琴表示同意，“Streaming LLM 可以平滑地扩展大型语言模型的对话长度。我们一直在使用它成功地将
    Mistral 模型部署到 iPhone 上。”
- en: The researchers also explored the use of attention sinks during model training
    by prepending several placeholder tokens in all training samples.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 研究人员还探索了在模型训练期间使用注意力汇聚点的方法，通过在所有训练样本中添加多个占位符标记。
- en: They found that training with attention sinks allowed a model to maintain performance
    with only one attention sink in its cache, rather than the four that are usually
    required to stabilize a pretrained model’s performance.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 他们发现，使用注意力汇聚点进行训练可以使模型在其缓存中只保留一个注意力汇聚点即可维持性能，而不是通常需要四个注意力汇聚点来稳定预训练模型的性能。
- en: But while StreamingLLM enables a model to conduct a continuous conversation,
    the model cannot remember words that aren’t stored in the cache. In the future,
    the researchers plan to target this limitation by investigating methods to retrieve
    tokens that have been evicted or enable the model to memorize previous conversations.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 但是虽然 StreamingLLM 可以使模型进行连续对话，但是模型无法记住未存储在缓存中的单词。未来，研究人员计划通过研究检索被驱逐的标记的方法或使模型记住先前对话的方法来解决这个限制。
- en: StreamingLLM has been incorporated into NVIDIA's large language model optimization
    library, [TensorRT-LLM](https://github.com/NVIDIA/TensorRT-LLM).
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: StreamingLLM 已经被整合到 NVIDIA 的大型语言模型优化库 [TensorRT-LLM](https://github.com/NVIDIA/TensorRT-LLM)
    中。
- en: This work is funded, in part, by the MIT-IBM Watson AI Lab, the MIT Science
    Hub, and the U.S. National Science Foundation.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 这项工作部分资助来自MIT-IBM沃森人工智能实验室、MIT科学中心和美国国家科学基金会。
