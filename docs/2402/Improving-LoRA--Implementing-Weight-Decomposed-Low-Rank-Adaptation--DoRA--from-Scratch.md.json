["```\nimport torch.nn as nn\n\nclass LoRALayer(nn.Module):\n    def __init__(self, in_dim, out_dim, rank, alpha):\n        super().__init__()\n        std_dev = 1 / torch.sqrt(torch.tensor(rank).float())\n        self.A = nn.Parameter(torch.randn(in_dim, rank) * std_dev)\n        self.B = nn.Parameter(torch.zeros(rank, out_dim))\n        self.alpha = alpha\n\n    def forward(self, x):\n        x = self.alpha * (x @ self.A @ self.B)\n        return x\n```", "```\nclass LinearWithLoRA(nn.Module):\n\n    def __init__(self, linear, rank, alpha):\n        super().__init__()\n        self.linear = linear\n        self.lora = LoRALayer(\n            linear.in_features, linear.out_features, rank, alpha\n        )\n\n    def forward(self, x):\n        return self.linear(x) + self.lora(x)\n```", "```\ntorch.manual_seed(123)\nlayer = nn.Linear(10, 2)\nx = torch.randn((1, 10))\n\nprint(\"Original output:\", layer(x))\n```", "```\nOriginal output: tensor([[0.6639, 0.4487]], grad_fn=<AddmmBackward0>)\n```", "```\nlayer_lora_1 = LinearWithLoRA(layer, rank=2, alpha=4)\nprint(\"LoRA output:\", layer_lora_1(x))\n```", "```\nLoRA output: tensor([[0.6639, 0.4487]], grad_fn=<AddmmBackward0>)\n```", "```\nclass LinearWithLoRAMerged(nn.Module):\n    def __init__(self, linear, rank, alpha):\n        super().__init__()\n        self.linear = linear\n        self.lora = LoRALayer(\n            linear.in_features, linear.out_features, rank, alpha\n        )\n\n    def forward(self, x):\n        lora = self.lora.A @ self.lora.B # Combine LoRA matrices\n        # Then combine LoRA with orig. weights\n        combined_weight = self.linear.weight + self.lora.alpha*lora.T \n        return F.linear(x, combined_weight, self.linear.bias)\n```", "```\nlayer_lora_2 = LinearWithLoRAMerged(layer, rank=2, alpha=4)\nprint(\"LoRA output:\", layer_lora_2(x))\n```", "```\nLoRA output: tensor([[0.6639, 0.4487]], grad_fn=<AddmmBackward0>)\n```", "```\nclass MultilayerPerceptron(nn.Module):\n    def __init__(self, num_features, \n        num_hidden_1, num_hidden_2, num_classes):\n        super().__init__()\n        self.layers = nn.Sequential(\n            nn.Linear(num_features, num_hidden_1),\n            nn.ReLU(),\n            nn.Linear(num_hidden_1, num_hidden_2),\n            nn.ReLU(),\n\n            nn.Linear(num_hidden_2, num_classes)\n        )\n\n    def forward(self, x):\n        x = self.layers(x)\n        return x\n\nmodel = MultilayerPerceptron(\n    num_features=num_features,\n    num_hidden_1=num_hidden_1,\n    num_hidden_2=num_hidden_2, \n    num_classes=num_classes\n)\n\nprint(model)\n```", "```\nMultilayerPerceptron(\n  (layers): Sequential(\n    (0): Linear(in_features=784, out_features=128, bias=True)\n    (1): ReLU()\n    (2): Linear(in_features=128, out_features=256, bias=True)\n    (3): ReLU()\n    (4): Linear(in_features=256, out_features=10, bias=True)\n  )\n)\n```", "```\nmodel.layers[0] = LinearWithLoRA(model.layers[0], rank=4, alpha=8)\nmodel.layers[2] = LinearWithLoRA(model.layers[2], rank=4, alpha=8)\nmodel.layers[4] = LinearWithLoRA(model.layers[4], rank=4, alpha=8)\n\nprint(model)\n```", "```\nMultilayerPerceptron(\n  (layers): Sequential(\n    (0): LinearWithLoRA(\n      (linear): Linear(in_features=784, out_features=128, bias=True)\n      (lora): LoRALayer()\n    )\n    (1): ReLU()\n    (2): LinearWithLoRA(\n      (linear): Linear(in_features=128, out_features=256, bias=True)\n      (lora): LoRALayer()\n    )\n    (3): ReLU()\n    (4): LinearWithLoRA(\n      (linear): Linear(in_features=256, out_features=10, bias=True)\n      (lora): LoRALayer()\n    )\n  )\n)\n```", "```\ndef freeze_linear_layers(model):\n    for child in model.children():\n        if isinstance(child, nn.Linear):\n            for param in child.parameters():\n                param.requires_grad = False\n        else:\n            # Recursively freeze linear layers in children modules\n            freeze_linear_layers(child)\n\nfreeze_linear_layers(model)\nfor name, param in model.named_parameters():\n    print(f\"{name}: {param.requires_grad}\")\n```", "```\nlayers.0.linear.weight: False\nlayers.0.linear.bias: False\nlayers.0.lora.A: True\nlayers.0.lora.B: True\nlayers.2.linear.weight: False\nlayers.2.linear.bias: False\nlayers.2.lora.A: True\nlayers.2.lora.B: True\nlayers.4.linear.weight: False\nlayers.4.linear.bias: False\nlayers.4.lora.A: True\nlayers.4.lora.B: True\n```", "```\nclass LinearWithDoRAMerged(nn.Module):\n\n    def __init__(self, linear, rank, alpha):\n        super().__init__()\n        self.linear = linear\n        self.lora = LoRALayer(\n            linear.in_features, linear.out_features, rank, alpha\n        )\n        self.m = nn.Parameter(\n            self.linear.weight.norm(p=2, dim=0, keepdim=True))\n\n  # Code loosely inspired by    \n  # https://github.com/catid/dora/blob/main/dora.py\n\n    def forward(self, x):\n        lora = self.lora.A @ self.lora.B\n        numerator = self.linear.weight + self.lora.alpha*lora.T\n        denominator = numerator.norm(p=2, dim=0, keepdim=True)\n        directional_component = numerator / denominator\n        new_weight = self.m * directional_component\n        return F.linear(x, new_weight, self.linear.bias)\n```", "```\nmodel.layers[0] = LinearWithDoRAMerged(model.layers[0], rank=4, alpha=8)\nmodel.layers[2] = LinearWithDoRAMerged(model.layers[2], rank=4, alpha=8)\nmodel.layers[4] = LinearWithDoRAMerged(model.layers[4], rank=4, alpha=8)\n\nprint(model)\n```", "```\nMultilayerPerceptron(\n  (layers): Sequential(\n    (0): LinearWithDoRAMerged(\n      (linear): Linear(in_features=784, out_features=128, bias=True)\n      (lora): LoRALayer()\n    )\n    (1): ReLU()\n    (2): LinearWithDoRAMerged(\n      (linear): Linear(in_features=128, out_features=256, bias=True)\n      (lora): LoRALayer()\n    )\n    (3): ReLU()\n    (4): LinearWithDoRAMerged(\n      (linear): Linear(in_features=256, out_features=10, bias=True)\n      (lora): LoRALayer()\n    )\n  )\n)\n```", "```\nfreeze_linear_layers(model)\nfor name, param in model.named_parameters():\n    print(f\"{name}: {param.requires_grad}\")\n```", "```\nlayers.0.m: True\nlayers.0.linear.weight: False\nlayers.0.linear.bias: False\nlayers.0.lora.A: True\nlayers.0.lora.B: True\nlayers.2.m: True\nlayers.2.linear.weight: False\nlayers.2.linear.bias: False\nlayers.2.lora.A: True\nlayers.2.lora.B: True\nlayers.4.m: True\nlayers.4.linear.weight: False\nlayers.4.linear.bias: False\nlayers.4.lora.A: True\nlayers.4.lora.B: True\n```"]