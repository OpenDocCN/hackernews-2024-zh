- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-05-27 14:45:26'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: Power of Small Optimizations
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://maksimkita.com/blog/power-of-small-optimizations.html](https://maksimkita.com/blog/power-of-small-optimizations.html)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Overview
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this post, I will talk about the importance of small optimizations that can
    provide a lot of performance improvements.
  prefs: []
  type: TYPE_NORMAL
- en: 'Generally, when I think about different optimizations, I place them in a hierarchy
    of different levels:'
  prefs: []
  type: TYPE_NORMAL
- en: Architecture level optimizations.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Algorithms and data structures level optimizations.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Source code level optimizations.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Typically, optimizations from higher levels of this hierarchy have more impact
    on your application than those from lower levels. It does not make a lot of sense
    to spend time on lower-level optimizations if, on the architecture level, your
    application is designed in a suboptimal way or the algorithms and data structures
    choices are wrong.
  prefs: []
  type: TYPE_NORMAL
- en: For lower-level optimizations, sometimes people think that those optimizations
    always require the usage of some hardcore algorithms or writing code in inline
    assembly, but a lot of them are fairly simple. In fact, from my practice, clear
    and simple code is generally fast, and optimizations do not always make code extremely
    hard to read or think about. It is also great if your application components are
    well isolated so you can change the internal implementation of some components
    without affecting other components or the rest of the application.
  prefs: []
  type: TYPE_NORMAL
- en: When I talk about “small” optimizations, they are most likely placed on a lower
    level of optimizations hierarchy, more often source code level optimizations,
    and if we measure them in lines of code, they are relatively small. It does not
    mean that such optimizations are easy because sometimes writing a “right” couple
    lines of code can be extremely hard and take a lot of time.
  prefs: []
  type: TYPE_NORMAL
- en: How to find places for optimizations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Generally, you need to have good introspection for your application and always
    profile your application, both in production and during development. You also
    need to be curious to explore every possible performance optimization opportunity.
    Even highly optimized places in your application can be optimized even further.
  prefs: []
  type: TYPE_NORMAL
- en: 'Examples of some optimizations for algorithms and data structures choice:'
  prefs: []
  type: TYPE_NORMAL
- en: Use hybrid algorithms. Some algorithms or data structures can work well when
    the amount of data is small, but when the amount of data grows, the underlying
    algorithm or data structure needs to be changed.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Use statistics for run-time optimizations. All algorithm’s performance is affected
    by data distribution. For example, if you know the cardinality of your data in
    advance, it is possible to choose a faster algorithm or data structure.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Use specializations. You can specialize your algorithms and data structures
    for specific data types. For example, if you know that you need to sort integers,
    it makes sense to use radix sort instead of general comparison-based sort algorithms
    like pdqsort. Another example is if you need to sort integers, but they are always
    sorted or almost sorted, radix sort will be much slower than pdqsort.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: There is no silver bullet or best algorithm for any task. You need to choose
    the fastest possible algorithm for your specific task.
  prefs: []
  type: TYPE_NORMAL
- en: 'Examples of some source code level optimizations:'
  prefs: []
  type: TYPE_NORMAL
- en: Avoid memory copying.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Avoid unnecessary allocations.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Change data layout to reduce memory usage and improve cache locality.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Use manual SIMD instructions or manual loop unrolling.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: It is also important to have performance tests as part of your CI/CD pipeline
    that can help you verify your optimization results or find regressions. Performance
    tests can also be very helpful in [finding places that you can optimize](cpu-dispatch-in-clickhouse.html#cpu-dispatch-optimization-places).
  prefs: []
  type: TYPE_NORMAL
- en: You need to constantly think about “How the performance of your application
    can be improved” and always check all available introspection for potential places
    that can be improved. For example, you can always check all hot functions in `perf-top`
    during development to understand if anything can be improved. It is also great
    to check the [flame graphs](https://www.brendangregg.com/flamegraphs.html) and
    try to understand the big picture of how your application works and where it spends
    time.
  prefs: []
  type: TYPE_NORMAL
- en: You always need to start optimization with places that take most of the time
    during application execution. But usually, after you optimize every such place,
    it is hard to understand which places to optimize next. For example, when you
    investigate some potential place, and you already know that this place takes 3-5%
    of the whole application execution time, it is important to still improve this
    place, even if you can improve this place’s performance only by a small amount.
    If you optimize many such places, the compound result of such optimizations will
    be visible for the whole application.
  prefs: []
  type: TYPE_NORMAL
- en: For multithreaded applications, it is also important to understand in which
    places your application processes data in a single thread. Such single-thread
    execution stage is common for many algorithms like sorting and aggregation. You
    sort/aggregate data by multiple threads, but the final merge stage is single-threaded.
    This also applies to the distributed execution of such algorithms. Even a small
    improvement in that single-thread merge stage is a big win because those places
    do not scale with an increased amount of threads or servers.
  prefs: []
  type: TYPE_NORMAL
- en: Example
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: I will provide an example of one of such optimizations, where [small source
    code level change](https://github.com/ClickHouse/ClickHouse/pull/57717) lead to
    great performance improvement.
  prefs: []
  type: TYPE_NORMAL
- en: In December 2023, during the development of some ClickHouse features, when I
    ran some queries that read a lot of String columns, I noticed in the `perf-top`
    and flame graphs that we can spend around 20-40% of query execution time on strings
    deserialization. I knew that string deserialization place was already heavily
    optimized in ClickHouse, but I decided to dig deeper.
  prefs: []
  type: TYPE_NORMAL
- en: 'In `perf-top`, I saw something like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Most of the time is spent in `DB::deserializeBinarySSE2`, and it is expected.
    What is not expected that we see `DB::PODArrayBase<1ul, 4096ul, Allocator<false,
    false>, 63ul, 64ul>::resize<>` and `DB::PODArrayDetails::byte_size` methods. If
    you check the `DB::deserializeBinarySSE2` assembly, you can notice that the PODArray
    `resize` function is called from it, and that function call is not inlined.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Now, if we check the `DB::PODArrayBase<1ul, 4096ul, Allocator<false, false>,
    63ul, 64ul>::resize<>` assembly, we will notice it calls `DB::PODArrayDetails::byte_size`
    function and we can also see that PODArray `resize` function call overhead is
    high.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'In C++ code `deserializeBinarySSE2` function looked like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'The solution was simple: in this specific deserialization place, we work with
    `PODArray` as just a characters buffer, and we can manually control the resize
    process and resize buffer with some constant resize factor, for example, 2\. We
    also use the `resize_exact` function to reduce memory allocation size. So inside
    the loop, we replace:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'with:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'As a result, we have such performance improvement for a query that I used for
    the optimization test:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Before:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'After:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Results of performance tests from ClickHouse CI:'
  prefs: []
  type: TYPE_NORMAL
- en: '| Query | Old (s) | New (s) | Ratio of speedup(-) or slowdown(+) | Relative
    difference (new - old) / old |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| SELECT count() FROM empty_strings WHERE NOT ignore(s) | 0.449 | 0.27 | -1.662x
    | -0.399 |'
  prefs: []
  type: TYPE_TB
- en: '| select anyHeavy(OpenstatSourceID) from hits_100m_single where OpenstatSourceID
    != '''' group by intHash32(UserID) % 1000000 FORMAT Null | 0.088 | 0.066 | -1.328x
    | -0.247 |'
  prefs: []
  type: TYPE_TB
- en: '| select anyHeavy(OpenstatCampaignID) from hits_100m_single where OpenstatCampaignID
    != '''' group by intHash32(UserID) % 1000000 FORMAT Null | 0.088 | 0.066 | -1.32x
    | -0.243 |'
  prefs: []
  type: TYPE_TB
- en: '| SELECT count() FROM hits_100m_single WHERE NOT ignore(format(''{}Hello{}'',
    MobilePhoneModel, PageCharset)) | 0.321 | 0.28 | -1.147x | -0.129 |'
  prefs: []
  type: TYPE_TB
- en: '| SELECT count() FROM hits_100m_single WHERE NOT ignore(concat(MobilePhoneModel,
    SearchPhrase)) | 0.337 | 0.298 | -1.131x | -0.117 |'
  prefs: []
  type: TYPE_TB
- en: '| SELECT str FROM test_full_10 FORMAT Null | 0.074 | 0.058 | -1.282x | -0.22
    |'
  prefs: []
  type: TYPE_TB
- en: '| SELECT count() FROM hits_100m_single WHERE NOT ignore(substring(PageCharset,
    1, 2)) | 0.135 | 0.116 | -1.16x | -0.138 |'
  prefs: []
  type: TYPE_TB
- en: As a result, this optimization improved performance for queries that spend a
    lot of execution time on string deserialization by 10-20% on average. For some
    queries, even for 60%.
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Sometimes, you can achieve significant performance improvement not only by using
    some high-level complex optimizations but also by using better algorithms and
    data structures for your specific tasks or by small source code level optimizations.
  prefs: []
  type: TYPE_NORMAL
