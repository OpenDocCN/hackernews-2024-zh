<!--yml

category: 未分类

date: 2024-05-27 14:34:34

-->

# 对比音频模型的不完整思考

> 来源：[https://zelalabs.substack.com/p/incomplete-thoughts-on-contrastive](https://zelalabs.substack.com/p/incomplete-thoughts-on-contrastive)

我最近花了很多时间准备各种基础设施，以便在文本-音频和音频-音频上对比训练音频模型。

我一直在专注于基础设施工作，没有花太多时间思考模型本身的潜力创新，所以这里是我完成的一些不完整的思考。

* * *

音频在模态列表中的位置有些奇怪。像文本一样，它是严格因果关系的，是模型的原始输入，波形严格是一维的。但与文本相反，音频具有极低的信息密度，通常以每秒24,000至48,000帧进行采样。因此，75个英文口语单词对应的序列长度为720k帧的音频，或者从严格的语义角度来看，大约是密度降低了10,000倍。在这一特点中，它更像是图像 - 单个像素不那么重要，即使在质量或精度大幅降低的情况下，你仍然可以保持高信息传输。可能它最接近的平行物体是视频（不出所料？） - 因果关系，大量的冗余等等。

大部分音频工作集中在SST或TTS应用程序上，每种应用都有处理这个问题独特形状的各种技术，并且目前都在努力寻找将其重新塑造以适应今天的LM和Diffusion范式的方法。对比模型（想想CLIP）在这里的代表性还是相当不足的。有一些，比如Betker的CLVP，但大多数其他人都集中在声音而不是人声上。

* * *

所以，当你将一个音频块总结成一个单一向量时，会发生什么？那个向量可能会编码什么？

与其他形式不同，我认为语音具有一种独特的特性，*风格*和*语义*差异很大。当然，像情感这样的特征跨越了这个界限，在语音合成世界的大多数情况下，处理语义（文本）和风格（3s语音克隆片段）作为大部分分开的组成部分是一个明显的缺陷。但值得注意的是，尽管传统上将这两者分开，模型在操作时效果显著，这在很大程度上是它们基本上是两个略有重叠的组成部分上具有很强的先验。

因此，我们可以预期，在一个音频段的最初块上，将会有大量的注意力（在字面上和实际上是变压器）放在风格特性，如说话者的声音特征，背景噪声，音频质量等上。然后，在其余的序列中，会存在更为分散的注意力，这里传播的是实际说出的话语。

另一个需要考虑的新事物是在存在两位发言者的样本中可能发生的情况。文本嵌入和图像嵌入通常在编码多个焦点和主题的信息方面表现出色，因此我们预期会有类似的情况发生。我们是否可以执行类似于经典的词向量演示，其中我们获取包含发言者 A 和发言者 B 的向量，减去仅包含发言者 A 的向量，然后使用结果向量找到严格属于发言者 B 的段落？

如何编码像背景噪音、音频质量等信息？从我们在 GAN 中看到的情况来看，我们可能期望对比模型努力实现对样本的最大消除歧义，考虑到我计划训练相邻的音频对，这个模型可能会通过特别强调人声频谱之外存在的声音环境来“作弊”。我希望能够尝试实验，看看这样的模型是否可以用于过滤出现音乐、背景噪音等内容的样本，所以期待能看到更多有趣的结果。

* * *

暂时就这些了。显然，这里的想法还非常不完整，早期的训练运行现在已经启动，希望能尽快报告一些发现。
