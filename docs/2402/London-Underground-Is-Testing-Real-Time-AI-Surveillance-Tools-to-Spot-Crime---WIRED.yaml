- en: <!--yml
  id: totrans-split-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-split-1
  prefs: []
  type: TYPE_NORMAL
  zh: 'category: 未分类'
- en: 'date: 2024-05-27 14:43:43'
  id: totrans-split-2
  prefs: []
  type: TYPE_NORMAL
  zh: 'date: 2024-05-27 14:43:43'
- en: -->
  id: totrans-split-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: London Underground Is Testing Real-Time AI Surveillance Tools to Spot Crime
    | WIRED
  id: totrans-split-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 伦敦地铁正在测试实时AI监控工具以便发现犯罪 | WIRED
- en: 来源：[https://www.wired.com/story/london-underground-ai-surveillance-documents/](https://www.wired.com/story/london-underground-ai-surveillance-documents/)
  id: totrans-split-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://www.wired.com/story/london-underground-ai-surveillance-documents/](https://www.wired.com/story/london-underground-ai-surveillance-documents/)
- en: In response to WIRED's Freedom of Information request, the TfL says it used
    existing CCTV images, AI algorithms, and “numerous detection models” to detect
    patterns of behavior. “By providing station staff with insights and notifications
    on customer movement and behaviour they will hopefully be able to respond to any
    situations more quickly,” the response says. It also says the trial has provided
    insight into fare evasion that will “assist us in our future approaches and interventions,”
    and the data gathered is in line with its [data policies](https://tfl.gov.uk/corporate/privacy-and-cookies/privacy-and-data-protection-policy).
  id: totrans-split-6
  prefs: []
  type: TYPE_NORMAL
  zh: 对于WIRED的信息自由请求，TfL表示他们使用现有的闭路电视图像、AI算法和“多种检测模型”来检测行为模式。“通过向车站工作人员提供关于客户移动和行为的见解和通知，他们希望能够更快地应对任何情况”，回应中说道。他还表示，该试验提供了有助于他们“未来方法和干预”的逃票信息，并且收集的数据符合其[数据政策](https://tfl.gov.uk/corporate/privacy-and-cookies/privacy-and-data-protection-policy)。
- en: In a statement sent after publication of this article, Mandy McGregor, TfL's
    head of policy and community safety, says the trial results are continuing to
    be analyzed and adds, “there was no evidence of bias” in the data collected from
    the trial. During the trial, McGregor says, there were no signs in place at the
    station that mentioned the tests of AI surveillance tools.
  id: totrans-split-7
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文发布后发送的一份声明中，TfL的政策与社区安全负责人曼迪·麦克格雷格表示，试验结果仍在继续分析，并补充道：“从试验收集的数据中没有证据表明存在偏见。”
    麦克格雷格表示，在试验期间，车站没有任何标志提及AI监控工具的测试。
- en: “We are currently considering the design and scope of a second phase of the
    trial. No other decisions have been taken about expanding the use of this technology,
    either to further stations or adding capability.” McGregor says. “Any wider roll
    out of the technology beyond a pilot would be dependent on a full consultation
    with local communities and other relevant stakeholders, including experts in the
    field.”
  id: totrans-split-8
  prefs: []
  type: TYPE_NORMAL
  zh: “我们目前正在考虑试验第二阶段的设计和范围。目前没有其他关于扩展此技术使用范围的决定，无论是扩展到其他车站还是增加功能。” 麦克格雷格说。“除了在当地社区和其他相关利益相关者，包括领域专家进行全面磋商之外，任何技术的更广泛推广都将取决于咨询。”
- en: Computer vision systems, such as those used in the test, work by trying to detect
    objects and people in images and videos. During the London trial, algorithms trained
    to detect certain behaviors or movements were combined with images from the Underground
    station’s 20-year-old CCTV cameras—analyzing imagery every tenth of a second.
    When the system detected one of 11 behaviors or events identified as problematic,
    it would issue an alert to station staff’s iPads or a computer. TfL staff received
    19,000 alerts to potentially act on and a further 25,000 kept for analytics purposes,
    the documents say.
  id: totrans-split-9
  prefs: []
  type: TYPE_NORMAL
  zh: 计算机视觉系统（如测试中所用的系统）的工作原理是通过尝试在图像和视频中检测物体和人物。在伦敦的试验中，训练用于检测特定行为或动作的算法与地铁站20年历史的闭路电视摄像头拍摄的图像结合——每0.1秒分析一次图像。当系统检测到被定义为问题的11种行为或事件之一时，它会向车站工作人员的iPad或计算机发出警报。文件显示，TfL工作人员共收到19,000个潜在行动警报，并进一步保留了25,000个以供分析。
- en: 'The categories the system tried to identify were: crowd movement, unauthorized
    access, safeguarding, mobility assistance, crime and antisocial behavior, person
    on the tracks, injured or unwell people, hazards such as litter or wet floors,
    unattended items, stranded customers, and fare evasion. Each has multiple subcategories.'
  id: totrans-split-10
  prefs: []
  type: TYPE_NORMAL
  zh: 系统试图识别的类别包括：人群移动、未经授权进入、保护、移动协助、犯罪和反社会行为、铁路上的人物、受伤或不适的人员、垃圾或地板湿滑等危险、无人看管物品、滞留客户和逃票行为。每种类别都有多个子类别。
- en: Daniel Leufer, a senior policy analyst at digital rights group Access Now, says
    whenever he sees any system doing this kind of monitoring, the first thing he
    looks for is whether it is attempting to pick out aggression or crime. “Cameras
    will do this by identifying the body language and behavior,” he says. “What kind
    of a data set are you going to have to train something on that?”
  id: totrans-split-11
  prefs: []
  type: TYPE_NORMAL
  zh: Daniel Leufer，数字权利组织Access Now的高级政策分析师，表示每当他看到任何进行此类监控的系统时，他首先要查看的是它是否试图识别攻击性或犯罪行为。“摄像头将通过识别身体语言和行为来实现这一点，”他说。“你会拥有怎样的数据集来训练这样的系统呢？”
- en: The TfL report on the trial says it “wanted to include acts of aggression” but
    found it was “unable to successfully detect” them. It adds that there was a lack
    of training data—other reasons for not including acts of aggression were blacked
    out. Instead, the system issued an alert when someone raised their arms, described
    as a “common behaviour linked to acts of aggression” in the documents.
  id: totrans-split-12
  prefs: []
  type: TYPE_NORMAL
  zh: TfL关于试验的报告称其“希望包括攻击行为”，但发现“无法成功检测”它们。报告补充说缺乏训练数据，其他不包括攻击行为的原因被涂黑。相反，当有人举起双臂时，系统会发出警报，文件中描述这种行为为“与攻击行为相关的常见行为”。
- en: “The training data is always insufficient because these things are arguably
    too complex and nuanced to be captured properly in data sets with the necessary
    nuances,” Leufer says, noting it is positive that TfL acknowledged it did not
    have enough training data. “I'm extremely skeptical about whether machine-learning
    systems can be used to reliably detect aggression in a way that isn’t simply replicating
    existing societal biases about what type of behavior is acceptable in public spaces.”
    There were a total of 66 alerts for aggressive behavior, including testing data,
    according to the documents WIRED received.
  id: totrans-split-13
  prefs: []
  type: TYPE_NORMAL
  zh: “训练数据总是不足的，因为这些事情可以说过于复杂和微妙，难以在必要的数据集中正确捕捉到其中的细微差别，”Leufer说道，并指出TfL承认其缺乏足够的训练数据是积极的。“我对机器学习系统能够可靠地检测攻击行为的能力非常怀疑，因为这种方式很可能只是在复制现有的社会偏见，即公共空间中哪种行为是可以接受的。”根据WIRED收到的文件，共有66次攻击性行为的警报，包括测试数据。
