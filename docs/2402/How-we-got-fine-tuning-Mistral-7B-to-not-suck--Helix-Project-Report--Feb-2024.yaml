- en: <!--yml
  id: totrans-split-0
  prefs: []
  type: TYPE_NORMAL
- en: 'category: Êú™ÂàÜÁ±ª'
  id: totrans-split-1
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-05-27 14:43:03'
  id: totrans-split-2
  prefs: []
  type: TYPE_NORMAL
- en: -->
  id: totrans-split-3
  prefs: []
  type: TYPE_NORMAL
- en: 'How we got fine-tuning Mistral-7B to not suck: Helix Project Report, Feb 2024'
  id: totrans-split-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Êù•Ê∫êÔºö[https://helixml.substack.com/p/how-we-got-fine-tuning-mistral-7b](https://helixml.substack.com/p/how-we-got-fine-tuning-mistral-7b)
  id: totrans-split-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Hi folks,
  id: totrans-split-6
  prefs: []
  type: TYPE_NORMAL
- en: 'It‚Äôs been just over a month since we [launched Helix v0.1](https://www.producthunt.com/products/helix-5),
    and today I‚Äôm happy to announce today the availability of [Helix v0.5](https://github.com/helixml/helix/releases).
    [Run it yourself on your own secure private infrastructure](https://docs.helix.ml/docs/controlplane)
    or [try it out on our SaaS](https://app.tryhelix.ai):'
  id: totrans-split-7
  prefs: []
  type: TYPE_NORMAL
- en: Along with a shiny new UI (you can a screenshot for comparison in our [first
    post](https://helixml.substack.com/p/building-a-generative-ai-platform)), we‚Äôve
    been extremely focused on improving the quality of the text fine-tuning.
  id: totrans-split-8
  prefs: []
  type: TYPE_NORMAL
- en: When we first launched Helix, the text fine tuning of the Mistral-7B-Instruct
    language model was based on [this LlamaIndex docs page](https://docs.llamaindex.ai/en/latest/examples/finetuning/knowledge/finetune_knowledge.html).
    Yeah, we basically decided to build a whole business around one this LlamaIndex
    notebook.
  id: totrans-split-9
  prefs: []
  type: TYPE_NORMAL
- en: 'At the time, unfortunately I hadn‚Äôt even seen the link from the parent page
    in the LlamaIndex docs that said ‚ÄúWIP: this isn‚Äôt very good yet‚Äù. Uh-oh.'
  id: totrans-split-10
  prefs: []
  type: TYPE_NORMAL
- en: 'Of course, because Helix is targeting being runnable fully on-prem, we‚Äôre using
    [Mistral-7B with axolotl](https://github.com/OpenAccess-AI-Collective/axolotl)
    for fine-tuning instead of the GPT-3.5 API, but it‚Äôs the same principle. The idea
    is that you chunk your documents up into pieces, then ask a language model to
    generate question-answer pairs (which is the shape of the training data you need
    to provide the fine-tuning process). You‚Äôre using an LLM to automate generating
    training data for fine-tuning another LLM. The original prompt looks like this:'
  id: totrans-split-11
  prefs: []
  type: TYPE_NORMAL
- en: Did it work? Sorta, but it was a bit crap. Just like LlamaIndex said it would
    be.
  id: totrans-split-12
  prefs: []
  type: TYPE_NORMAL
- en: We were able to feed it [complex technical papers](https://docs.helix.ml/docs/papers),
    and it was able to answer technical questions about them. But it failed at some
    much more basic tasks.
  id: totrans-split-13
  prefs: []
  type: TYPE_NORMAL
- en: 'This one news article for example: [Junior doctors in England to stage more
    strikes over pay](https://www.theguardian.com/society/2023/dec/05/junior-doctors-in-england-to-stage-more-strikes).
    This article became the bane of my life for a few weeks üòÇ'
  id: totrans-split-14
  prefs: []
  type: TYPE_NORMAL
- en: 'Why? Because when we first asked the fine tuned model the simple question:'
  id: totrans-split-15
  prefs: []
  type: TYPE_NORMAL
- en: What?! You were fine-tuned on information derived from the article. Why are
    you talking about fine-tuning?!
  id: totrans-split-16
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ‚Äìme
  id: totrans-split-17
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'OK, turned out this one was fairly straightforward. One of the text elements
    meant to tell the user ‚Äúfine tuning completed‚Äù was also being sent back to the
    model as if the user had said it. Now the only context the model had to go on
    was the idea of fine-tuning. We got that one out of the way. Cool, let‚Äôs try again:'
  id: totrans-split-18
  prefs: []
  type: TYPE_NORMAL
- en: OMG, seriously? Surely you should know that the doctors are threatening to go
    on strike. It‚Äôs right there in the title! Stupid fine-tuning, maybe it will never
    work. ü§¨
  id: totrans-split-19
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ‚Äìme
  id: totrans-split-20
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'But we persisted. OK, so why isn‚Äôt the model able to answer questions about
    the basic context of the article? Well, it‚Äôs because the question-answer pairs
    generated by the prompt aren‚Äôt in the form of simple questions about the title
    of the document. The solution, it turned out, was rather than a single qapair
    generating prompt, we had to implement a *suite* of them, to extract context from
    the document from all sorts of different perspectives: what are the entities in
    the document and how are they related? What‚Äôs a short, medium and long summary
    of the document? Who/what/where questions, etc. [See the full list here](https://github.com/helixml/helix/blob/main/api/pkg/dataprep/qapairs/qapair_config.yaml).'
  id: totrans-split-21
  prefs: []
  type: TYPE_NORMAL
- en: Turns out, when we carefully constructed this suite of prompts, we were then
    finally able to get the model to answer the basic question about ‚Äúwhat are the
    doctors going to do?‚Äù Phew! üòÖ
  id: totrans-split-22
  prefs: []
  type: TYPE_NORMAL
- en: Our other insight was that by generating a content-addressed hash for each document,
    we could also teach the model about the IDs of the individual documents, along
    with IDs for groups of documents.
  id: totrans-split-23
  prefs: []
  type: TYPE_NORMAL
- en: 'We can then map those IDs back onto the documents the model was fine-tuned
    on. For example: [in this session](https://app.tryhelix.ai/session/4d769c7c-b9a1-4fc4-b5c9-d277fc59ed1b)
    the model is able to tell you that what it‚Äôs learned was in a given document,
    even linking the user back to that document. I also found this exchange pretty
    hilarious:'
  id: totrans-split-24
  prefs: []
  type: TYPE_NORMAL
- en: Although maybe that says more about my sense of humour than anything else.
  id: totrans-split-25
  prefs: []
  type: TYPE_NORMAL
- en: We then added a system prompt telling the model to refer only to the specific
    document IDs it was trained on and not to refer to background knowledge. What
    do you know, it worked!
  id: totrans-split-26
  prefs: []
  type: TYPE_NORMAL
- en: So far we‚Äôre adjusting the prompts and system for this LLM app based on ‚Äúvibes‚Äù.
    That is, trying stuff, evaluating it by eye, and then changing it. Problem is,
    vibes don‚Äôt scale.
  id: totrans-split-27
  prefs: []
  type: TYPE_NORMAL
- en: Work is ongoing on an end-to-end ‚Äúevals‚Äù framework so we can automatically build
    up a library of good and bad sessions, and then every time we change the prompts,
    code, model etc re-run the fine-tuning across all of the sessions in the library,
    and then grade them. We might even use an LLM to grade them automatically :-)
  id: totrans-split-28
  prefs: []
  type: TYPE_NORMAL
- en: Please help us by clicking the new thumbs up and thumbs down buttons at the
    bottom of your sessions! We‚Äôll use these as input to improve the product.
  id: totrans-split-29
  prefs: []
  type: TYPE_NORMAL
- en: 'Oh believe me, we‚Äôve talked about it. We‚Äôre sticking with fine-tuning for now,
    because:'
  id: totrans-split-30
  prefs: []
  type: TYPE_NORMAL
- en: Fine tuning can memorize far more information than you can fit in a single prompt
  id: totrans-split-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: By not needing to cram any custom knowledge in the prompt, you can get much
    better latency
  id: totrans-split-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fine tuning is better at copying style (we have qapair prompts planned for this)
  id: totrans-split-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fine tuning is better at understanding a large corpus of background knowledge
    (a ‚Äúdomain‚Äù) and being able to draw on all of it when constructing an answer
  id: totrans-split-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fine tuned models are easier to run at the edge without needing the infrastructure
    of vector stores close to the model
  id: totrans-split-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You can use a much smaller fine-tuned model than general purpose model plus
    RAG. What can you do with a fine-tuned Phi-2 model that can run on any CPU?
  id: totrans-split-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We made it work!!
  id: totrans-split-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Are we wrong? [Come and roast us on Discord](https://discord.gg/VJftd844GE)!
  id: totrans-split-38
  prefs: []
  type: TYPE_NORMAL
- en: 'We now have an OpenAI compatible API. For example, here I am configuring Flowise
    to integrate with my private Helix deployment, for example:'
  id: totrans-split-39
  prefs: []
  type: TYPE_NORMAL
- en: 'Just set:'
  id: totrans-split-40
  prefs: []
  type: TYPE_NORMAL
- en: '**Model Name:** mistralai/Mistral-7B-Instruct-v0.1'
  id: totrans-split-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Connect Credential:** Your API key from [https://app.tryhelix.ai/account](https://app.tryhelix.ai/account)'
  id: totrans-split-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**BasePath (under additional parameters):** https://app.tryhelix.ai/v1'
  id: totrans-split-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: And that‚Äôs it! You‚Äôll notice your API calls to Helix show up as sessions in
    your account, so you get a free record of them :-)
  id: totrans-split-44
  prefs: []
  type: TYPE_NORMAL
- en: We‚Äôll automatically fine tune the model once the qapairs are extracted, then
    email you when it‚Äôs finished. We hope this will encourage more people to dive
    back into the app once you‚Äôve waited the 10-15 minutes it takes to train your
    own AI.
  id: totrans-split-45
  prefs: []
  type: TYPE_NORMAL
- en: Thanks for reading! Soon I‚Äôll blog more about our roadmap, on-prem use cases
    where we‚Äôre seeing significant commerial traction, and our dirty secret. Subscribe
    and stay in the loop :-)
  id: totrans-split-46
  prefs: []
  type: TYPE_NORMAL
