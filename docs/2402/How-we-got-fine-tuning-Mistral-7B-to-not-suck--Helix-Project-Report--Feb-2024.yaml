- en: <!--yml
  id: totrans-split-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: æœªåˆ†ç±»'
  id: totrans-split-1
  prefs: []
  type: TYPE_NORMAL
  zh: ç±»åˆ«ï¼šæœªåˆ†ç±»
- en: 'date: 2024-05-27 14:43:03'
  id: totrans-split-2
  prefs: []
  type: TYPE_NORMAL
  zh: æ—¥æœŸï¼š2024å¹´05æœˆ27æ—¥14:43:03
- en: -->
  id: totrans-split-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: 'How we got fine-tuning Mistral-7B to not suck: Helix Project Report, Feb 2024'
  id: totrans-split-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬æ˜¯å¦‚ä½•ç²¾è°ƒMistral-7Bä»¥é¿å…å¤±è´¥çš„ï¼šèºæ—‹è®¡åˆ’æŠ¥å‘Šï¼Œ2024å¹´2æœˆ
- en: æ¥æºï¼š[https://helixml.substack.com/p/how-we-got-fine-tuning-mistral-7b](https://helixml.substack.com/p/how-we-got-fine-tuning-mistral-7b)
  id: totrans-split-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: æ¥æºï¼š[https://helixml.substack.com/p/how-we-got-fine-tuning-mistral-7b](https://helixml.substack.com/p/how-we-got-fine-tuning-mistral-7b)
- en: Hi folks,
  id: totrans-split-6
  prefs: []
  type: TYPE_NORMAL
  zh: å¤§å®¶å¥½ï¼Œ
- en: 'Itâ€™s been just over a month since we [launched Helix v0.1](https://www.producthunt.com/products/helix-5),
    and today Iâ€™m happy to announce today the availability of [Helix v0.5](https://github.com/helixml/helix/releases).
    [Run it yourself on your own secure private infrastructure](https://docs.helix.ml/docs/controlplane)
    or [try it out on our SaaS](https://app.tryhelix.ai):'
  id: totrans-split-7
  prefs: []
  type: TYPE_NORMAL
  zh: è·[Helix v0.1å‘å¸ƒ](https://www.producthunt.com/products/helix-5)å·²ç»è¿‡å»ä¸€ä¸ªå¤šæœˆäº†ï¼Œä»Šå¤©æˆ‘å¾ˆé«˜å…´åœ°å®£å¸ƒ[Helix
    v0.5](https://github.com/helixml/helix/releases)çš„å¯ç”¨æ€§ã€‚[åœ¨æ‚¨è‡ªå·±çš„å®‰å…¨ç§äººåŸºç¡€è®¾æ–½ä¸Šè¿è¡Œ](https://docs.helix.ml/docs/controlplane)æˆ–è€…[åœ¨æˆ‘ä»¬çš„SaaSä¸Šè¯•ç”¨](https://app.tryhelix.ai)å§ï¼š
- en: Along with a shiny new UI (you can a screenshot for comparison in our [first
    post](https://helixml.substack.com/p/building-a-generative-ai-platform)), weâ€™ve
    been extremely focused on improving the quality of the text fine-tuning.
  id: totrans-split-8
  prefs: []
  type: TYPE_NORMAL
  zh: é™¤äº†å…¨æ–°çš„é—ªäº®UIï¼ˆä½ å¯ä»¥åœ¨æˆ‘ä»¬çš„[ç¬¬ä¸€ç¯‡æ–‡ç« ](https://helixml.substack.com/p/building-a-generative-ai-platform)ä¸­æ‰¾åˆ°æ¯”è¾ƒçš„æˆªå›¾ï¼‰ï¼Œæˆ‘ä»¬ä¸€ç›´åœ¨æåŠ›æé«˜æ–‡æœ¬ç²¾è°ƒçš„è´¨é‡ã€‚
- en: When we first launched Helix, the text fine tuning of the Mistral-7B-Instruct
    language model was based on [this LlamaIndex docs page](https://docs.llamaindex.ai/en/latest/examples/finetuning/knowledge/finetune_knowledge.html).
    Yeah, we basically decided to build a whole business around one this LlamaIndex
    notebook.
  id: totrans-split-9
  prefs: []
  type: TYPE_NORMAL
  zh: å½“æˆ‘ä»¬é¦–æ¬¡æ¨å‡ºHelixæ—¶ï¼ŒMistral-7B-Instructè¯­è¨€æ¨¡å‹çš„æ–‡æœ¬ç²¾è°ƒæ˜¯åŸºäº[è¿™ç¯‡LlamaIndexæ–‡æ¡£é¡µé¢](https://docs.llamaindex.ai/en/latest/examples/finetuning/knowledge/finetune_knowledge.html)ã€‚æ˜¯çš„ï¼Œæˆ‘ä»¬åŸºæœ¬ä¸Šå†³å®šå›´ç»•è¿™æœ¬LlamaIndexç¬”è®°å¼€å±•æ•´ä¸ªä¸šåŠ¡ã€‚
- en: 'At the time, unfortunately I hadnâ€™t even seen the link from the parent page
    in the LlamaIndex docs that said â€œWIP: this isnâ€™t very good yetâ€. Uh-oh.'
  id: totrans-split-10
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸å¹¸çš„æ˜¯ï¼Œå½“æ—¶æˆ‘ç”šè‡³æ²¡æœ‰çœ‹åˆ°æ¥è‡ªLlamaIndexæ–‡æ¡£çˆ¶é¡µé¢çš„é“¾æ¥ï¼Œä¸Šé¢å†™ç€â€œWIPï¼šè¿™è¿˜ä¸å¤ªå¥½â€ã€‚å“å‘€ã€‚
- en: 'Of course, because Helix is targeting being runnable fully on-prem, weâ€™re using
    [Mistral-7B with axolotl](https://github.com/OpenAccess-AI-Collective/axolotl)
    for fine-tuning instead of the GPT-3.5 API, but itâ€™s the same principle. The idea
    is that you chunk your documents up into pieces, then ask a language model to
    generate question-answer pairs (which is the shape of the training data you need
    to provide the fine-tuning process). Youâ€™re using an LLM to automate generating
    training data for fine-tuning another LLM. The original prompt looks like this:'
  id: totrans-split-11
  prefs: []
  type: TYPE_NORMAL
  zh: å½“ç„¶ï¼Œå› ä¸ºHelixçš„ç›®æ ‡æ˜¯å®Œå…¨åœ¨æœ¬åœ°è¿è¡Œï¼Œæ‰€ä»¥æˆ‘ä»¬åœ¨ç²¾è°ƒæ—¶ä½¿ç”¨[Mistral-7Bä¸axolotl](https://github.com/OpenAccess-AI-Collective/axolotl)ï¼Œè€Œä¸æ˜¯ä½¿ç”¨GPT-3.5
    APIï¼Œä½†åŸç†ç›¸åŒã€‚æ€è·¯æ˜¯å°†æ–‡æ¡£åˆ†å—ï¼Œç„¶åè¦æ±‚è¯­è¨€æ¨¡å‹ç”Ÿæˆé—®ç­”å¯¹ï¼ˆè¿™æ˜¯æ‚¨éœ€è¦æä¾›ç»™ç²¾è°ƒè¿‡ç¨‹çš„è®­ç»ƒæ•°æ®çš„å½¢å¼ï¼‰ã€‚æ‚¨ä½¿ç”¨LLMæ¥è‡ªåŠ¨åŒ–ç”Ÿæˆç”¨äºç²¾è°ƒå¦ä¸€ä¸ªLLMçš„è®­ç»ƒæ•°æ®ã€‚åŸå§‹æç¤ºçœ‹èµ·æ¥åƒè¿™æ ·ï¼š
- en: Did it work? Sorta, but it was a bit crap. Just like LlamaIndex said it would
    be.
  id: totrans-split-12
  prefs: []
  type: TYPE_NORMAL
  zh: è¡Œå¾—é€šå—ï¼Ÿæœ‰ç‚¹è¡Œï¼Œä½†æœ‰ç‚¹ç³Ÿç³•ã€‚å°±åƒLlamaIndexè¯´çš„é‚£æ ·ã€‚
- en: We were able to feed it [complex technical papers](https://docs.helix.ml/docs/papers),
    and it was able to answer technical questions about them. But it failed at some
    much more basic tasks.
  id: totrans-split-13
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬èƒ½å¤Ÿä¸ºå…¶æä¾›[å¤æ‚çš„æŠ€æœ¯è®ºæ–‡](https://docs.helix.ml/docs/papers)ï¼Œå®ƒèƒ½å¤Ÿå›ç­”ä¸å®ƒä»¬æœ‰å…³çš„æŠ€æœ¯é—®é¢˜ã€‚ä½†æ˜¯åœ¨ä¸€äº›æ›´åŸºæœ¬çš„ä»»åŠ¡ä¸Šå¤±è´¥äº†ã€‚
- en: 'This one news article for example: [Junior doctors in England to stage more
    strikes over pay](https://www.theguardian.com/society/2023/dec/05/junior-doctors-in-england-to-stage-more-strikes).
    This article became the bane of my life for a few weeks ğŸ˜‚'
  id: totrans-split-14
  prefs: []
  type: TYPE_NORMAL
  zh: ä¾‹å¦‚è¿™ç¯‡æ–°é—»æ–‡ç« ï¼š[è‹±æ ¼å…°çš„åˆçº§åŒ»ç”Ÿå› è–ªé…¬é—®é¢˜å°†å†æ¬¡ä¸¾è¡Œç½¢å·¥](https://www.theguardian.com/society/2023/dec/05/junior-doctors-in-england-to-stage-more-strikes)ã€‚è¿™ç¯‡æ–‡ç« åœ¨æˆ‘ç”Ÿæ´»ä¸­æˆä¸ºäº†ä¸€æ®µæ—¶é—´çš„å™©æ¢¦
    ğŸ˜‚
- en: 'Why? Because when we first asked the fine tuned model the simple question:'
  id: totrans-split-15
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºä»€ä¹ˆï¼Ÿå› ä¸ºå½“æˆ‘ä»¬é¦–æ¬¡è¯¢é—®æ–‡æœ¬ç²¾è°ƒæ¨¡å‹ä¸€ä¸ªç®€å•çš„é—®é¢˜æ—¶ï¼š
- en: What?! You were fine-tuned on information derived from the article. Why are
    you talking about fine-tuning?!
  id: totrans-split-16
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ä»€ä¹ˆï¼Ÿï¼ä½ æ˜¯ä»é‚£ç¯‡æ–‡ç« ä¸­è·å–çš„ä¿¡æ¯è¿›è¡Œç²¾è°ƒçš„ã€‚ä½ ä¸ºä»€ä¹ˆè¦è°ˆè®ºç²¾è°ƒï¼Ÿï¼
- en: â€“me
  id: totrans-split-17
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: â€“æˆ‘
- en: 'OK, turned out this one was fairly straightforward. One of the text elements
    meant to tell the user â€œfine tuning completedâ€ was also being sent back to the
    model as if the user had said it. Now the only context the model had to go on
    was the idea of fine-tuning. We got that one out of the way. Cool, letâ€™s try again:'
  id: totrans-split-18
  prefs: []
  type: TYPE_NORMAL
  zh: å¥½å§ï¼Œç»“æœè¿˜æ˜¯ç›¸å½“ç®€å•çš„ã€‚ä¸ºäº†å‘Šè¯‰ç”¨æˆ·â€œç²¾è°ƒå®Œæˆâ€ï¼Œå…¶ä¸­ä¸€ä¸ªæ–‡æœ¬å…ƒç´ ä¹Ÿè¢«å‘é€å›æ¨¡å‹ï¼Œå°±å¥½åƒç”¨æˆ·çœŸçš„è¯´äº†è¿™å¥è¯ã€‚ç°åœ¨ï¼Œæ¨¡å‹å¾—åˆ°çš„å”¯ä¸€ä¸Šä¸‹æ–‡å°±æ˜¯ç²¾è°ƒçš„æ¦‚å¿µã€‚æˆ‘ä»¬è§£å†³äº†è¿™ä¸ªé—®é¢˜ã€‚å¾ˆé…·ï¼Œè®©æˆ‘ä»¬å†è¯•ä¸€æ¬¡ï¼š
- en: OMG, seriously? Surely you should know that the doctors are threatening to go
    on strike. Itâ€™s right there in the title! Stupid fine-tuning, maybe it will never
    work. ğŸ¤¬
  id: totrans-split-19
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: å“¦æˆ‘çš„å¤©å•Šï¼Œè®¤çœŸçš„å—ï¼Ÿä½ ä¸€å®šçŸ¥é“åŒ»ç”Ÿä»¬æ­£åœ¨å¨èƒè¦ç½¢å·¥ã€‚æ ‡é¢˜ä¸Šä¸å°±å†™ç€å—ï¼æ„šè ¢çš„å¾®è°ƒï¼Œä¹Ÿè®¸æ°¸è¿œä¸ä¼šå¥æ•ˆã€‚ğŸ¤¬
- en: â€“me
  id: totrans-split-20
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: â€“æˆ‘
- en: 'But we persisted. OK, so why isnâ€™t the model able to answer questions about
    the basic context of the article? Well, itâ€™s because the question-answer pairs
    generated by the prompt arenâ€™t in the form of simple questions about the title
    of the document. The solution, it turned out, was rather than a single qapair
    generating prompt, we had to implement a *suite* of them, to extract context from
    the document from all sorts of different perspectives: what are the entities in
    the document and how are they related? Whatâ€™s a short, medium and long summary
    of the document? Who/what/where questions, etc. [See the full list here](https://github.com/helixml/helix/blob/main/api/pkg/dataprep/qapairs/qapair_config.yaml).'
  id: totrans-split-21
  prefs: []
  type: TYPE_NORMAL
  zh: ä½†æˆ‘ä»¬åšæŒä¸‹æ¥äº†ã€‚å¥½å§ï¼Œä¸ºä»€ä¹ˆæ¨¡å‹ä¸èƒ½å›ç­”å…³äºæ–‡ç« åŸºæœ¬èƒŒæ™¯çš„é—®é¢˜å‘¢ï¼Ÿå—¯ï¼Œè¿™æ˜¯å› ä¸ºæç¤ºç”Ÿæˆçš„é—®ç­”å¯¹ä¸æ˜¯å…³äºæ–‡æ¡£æ ‡é¢˜çš„ç®€å•é—®é¢˜å½¢å¼ã€‚è§£å†³æ–¹æ¡ˆæ˜¯ï¼Œä¸æ˜¯å•ä¸€çš„é—®ç­”å¯¹ç”Ÿæˆæç¤ºï¼Œæˆ‘ä»¬å¿…é¡»å®ç°ä¸€ä¸ª*å¥—ä»¶*ï¼Œä»å„ç§ä¸åŒçš„è§’åº¦æå–æ–‡æ¡£çš„ä¸Šä¸‹æ–‡ï¼šæ–‡æ¡£ä¸­çš„å®ä½“æ˜¯ä»€ä¹ˆï¼Œå®ƒä»¬ä¹‹é—´å¦‚ä½•å…³è”ï¼Ÿæ–‡æ¡£çš„ç®€çŸ­ã€ä¸­ç­‰å’Œé•¿ç¯‡æ‘˜è¦æ˜¯ä»€ä¹ˆï¼Ÿè°/ä»€ä¹ˆ/åœ¨å“ªé‡Œçš„é—®é¢˜ç­‰ç­‰ã€‚[åœ¨æ­¤å¤„æŸ¥çœ‹å®Œæ•´åˆ—è¡¨](https://github.com/helixml/helix/blob/main/api/pkg/dataprep/qapairs/qapair_config.yaml)ã€‚
- en: Turns out, when we carefully constructed this suite of prompts, we were then
    finally able to get the model to answer the basic question about â€œwhat are the
    doctors going to do?â€ Phew! ğŸ˜…
  id: totrans-split-22
  prefs: []
  type: TYPE_NORMAL
  zh: ç»“æœè¡¨æ˜ï¼Œå½“æˆ‘ä»¬ç²¾å¿ƒæ„å»ºäº†è¿™ä¸€ç³»åˆ—æç¤ºæ—¶ï¼Œæœ€ç»ˆæˆ‘ä»¬èƒ½å¤Ÿè®©æ¨¡å‹å›ç­”å…³äºâ€œåŒ»ç”Ÿä»¬å°†è¦åšä»€ä¹ˆï¼Ÿâ€çš„åŸºæœ¬é—®é¢˜ã€‚å“å‘€ï¼ğŸ˜…
- en: Our other insight was that by generating a content-addressed hash for each document,
    we could also teach the model about the IDs of the individual documents, along
    with IDs for groups of documents.
  id: totrans-split-23
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬çš„å¦ä¸€ä¸ªå‘ç°æ˜¯ï¼Œé€šè¿‡ä¸ºæ¯ä¸ªæ–‡æ¡£ç”Ÿæˆä¸€ä¸ªå†…å®¹å¯»å€çš„å“ˆå¸Œï¼Œæˆ‘ä»¬è¿˜å¯ä»¥æ•™ä¼šæ¨¡å‹å…³äºå•ä¸ªæ–‡æ¡£çš„IDï¼Œä»¥åŠæ–‡æ¡£ç»„çš„IDã€‚
- en: 'We can then map those IDs back onto the documents the model was fine-tuned
    on. For example: [in this session](https://app.tryhelix.ai/session/4d769c7c-b9a1-4fc4-b5c9-d277fc59ed1b)
    the model is able to tell you that what itâ€™s learned was in a given document,
    even linking the user back to that document. I also found this exchange pretty
    hilarious:'
  id: totrans-split-24
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åæˆ‘ä»¬å¯ä»¥å°†è¿™äº›IDæ˜ å°„å›æ¨¡å‹è¿›è¡Œå¾®è°ƒçš„æ–‡æ¡£ã€‚ä¾‹å¦‚ï¼š[åœ¨è¿™ä¸ªä¼šè¯ä¸­](https://app.tryhelix.ai/session/4d769c7c-b9a1-4fc4-b5c9-d277fc59ed1b)ï¼Œæ¨¡å‹èƒ½å¤Ÿå‘Šè¯‰æ‚¨å®ƒæ‰€å­¦åˆ°çš„æ˜¯åœ¨ç»™å®šçš„æ–‡æ¡£ä¸­ï¼Œå¹¶å°†ç”¨æˆ·é“¾æ¥å›è¯¥æ–‡æ¡£ã€‚æˆ‘è¿˜è§‰å¾—è¿™æ¬¡äº¤æµæŒºæç¬‘çš„ï¼š
- en: Although maybe that says more about my sense of humour than anything else.
  id: totrans-split-25
  prefs: []
  type: TYPE_NORMAL
  zh: å°½ç®¡ä¹Ÿè®¸è¿™æ›´å¤šåœ°åæ˜ äº†æˆ‘çš„å¹½é»˜æ„Ÿè€Œå·²ã€‚
- en: We then added a system prompt telling the model to refer only to the specific
    document IDs it was trained on and not to refer to background knowledge. What
    do you know, it worked!
  id: totrans-split-26
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åï¼Œæˆ‘ä»¬æ·»åŠ äº†ä¸€ä¸ªç³»ç»Ÿæç¤ºï¼Œå‘Šè¯‰æ¨¡å‹åªå‚è€ƒå®ƒè®­ç»ƒè¿‡çš„ç‰¹å®šæ–‡æ¡£IDï¼Œè€Œä¸å‚è€ƒèƒŒæ™¯çŸ¥è¯†ã€‚ä½ çŸ¥é“å—ï¼Œè¿™è¡Œå¾—é€šäº†ï¼
- en: So far weâ€™re adjusting the prompts and system for this LLM app based on â€œvibesâ€.
    That is, trying stuff, evaluating it by eye, and then changing it. Problem is,
    vibes donâ€™t scale.
  id: totrans-split-27
  prefs: []
  type: TYPE_NORMAL
  zh: åˆ°ç›®å‰ä¸ºæ­¢ï¼Œæˆ‘ä»¬æ ¹æ®â€œæ°›å›´â€è°ƒæ•´äº†è¿™ä¸ªLLMåº”ç”¨çš„æç¤ºå’Œç³»ç»Ÿã€‚ä¹Ÿå°±æ˜¯è¯´ï¼Œå°è¯•ä¸€äº›ä¸œè¥¿ï¼Œé€šè¿‡çœ¼ç›è¯„ä¼°ï¼Œç„¶åè¿›è¡Œä¿®æ”¹ã€‚é—®é¢˜æ˜¯ï¼Œæ°›å›´æ— æ³•æ‰©å±•ã€‚
- en: Work is ongoing on an end-to-end â€œevalsâ€ framework so we can automatically build
    up a library of good and bad sessions, and then every time we change the prompts,
    code, model etc re-run the fine-tuning across all of the sessions in the library,
    and then grade them. We might even use an LLM to grade them automatically :-)
  id: totrans-split-28
  prefs: []
  type: TYPE_NORMAL
  zh: æ­£åœ¨è¿›è¡Œä¸€ä¸ªç«¯åˆ°ç«¯çš„â€œè¯„ä¼°â€æ¡†æ¶ï¼Œä»¥ä¾¿æˆ‘ä»¬å¯ä»¥è‡ªåŠ¨å»ºç«‹ä¸€ä¸ªå¥½å’Œåä¼šè¯çš„åº“ï¼Œç„¶åæ¯æ¬¡æˆ‘ä»¬æ”¹å˜æç¤ºã€ä»£ç ã€æ¨¡å‹ç­‰ï¼Œé‡æ–°è¿è¡Œæ•´ä¸ªåº“ä¸­çš„æ‰€æœ‰ä¼šè¯çš„å¾®è°ƒï¼Œç„¶åè¿›è¡Œè¯„åˆ†ã€‚æˆ‘ä»¬ç”šè‡³å¯èƒ½ä¼šä½¿ç”¨LLMæ¥è‡ªåŠ¨è¯„åˆ†å®ƒä»¬
    :-)
- en: Please help us by clicking the new thumbs up and thumbs down buttons at the
    bottom of your sessions! Weâ€™ll use these as input to improve the product.
  id: totrans-split-29
  prefs: []
  type: TYPE_NORMAL
  zh: è¯·é€šè¿‡ç‚¹å‡»æ‚¨ä¼šè¯åº•éƒ¨çš„æ–°çš„ç‚¹èµå’Œç‚¹è¸©æŒ‰é’®æ¥å¸®åŠ©æˆ‘ä»¬ï¼æˆ‘ä»¬å°†ä½¿ç”¨è¿™äº›ä½œä¸ºæ”¹è¿›äº§å“çš„è¾“å…¥ã€‚
- en: 'Oh believe me, weâ€™ve talked about it. Weâ€™re sticking with fine-tuning for now,
    because:'
  id: totrans-split-30
  prefs: []
  type: TYPE_NORMAL
  zh: å“¦ï¼Œç›¸ä¿¡æˆ‘ï¼Œæˆ‘ä»¬å·²ç»è°ˆè®ºè¿‡äº†ã€‚æˆ‘ä»¬æš‚æ—¶åšæŒå¾®è°ƒï¼Œå› ä¸ºï¼š
- en: Fine tuning can memorize far more information than you can fit in a single prompt
  id: totrans-split-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å¾®è°ƒå¯ä»¥è®°å¿†çš„ä¿¡æ¯è¿œè¿œè¶…è¿‡ä½ å¯ä»¥åœ¨å•ä¸ªæç¤ºä¸­é€‚åˆçš„ä¿¡æ¯ã€‚
- en: By not needing to cram any custom knowledge in the prompt, you can get much
    better latency
  id: totrans-split-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: é€šè¿‡ä¸éœ€è¦åœ¨æç¤ºä¸­å¡å…¥ä»»ä½•è‡ªå®šä¹‰çŸ¥è¯†ï¼Œæ‚¨å¯ä»¥è·å¾—æ›´å¥½çš„å»¶è¿Ÿã€‚
- en: Fine tuning is better at copying style (we have qapair prompts planned for this)
  id: totrans-split-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å¾®è°ƒæ›´æ“…é•¿å¤åˆ¶é£æ ¼ï¼ˆæˆ‘ä»¬è®¡åˆ’ä¸ºæ­¤å‡†å¤‡é—®é¢˜å¯¹ï¼‰ã€‚
- en: Fine tuning is better at understanding a large corpus of background knowledge
    (a â€œdomainâ€) and being able to draw on all of it when constructing an answer
  id: totrans-split-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å¾®è°ƒæ›´æ“…é•¿ç†è§£å¤§é‡èƒŒæ™¯çŸ¥è¯†ï¼ˆâ€œé¢†åŸŸâ€ï¼‰ï¼Œå¹¶ä¸”åœ¨æ„å»ºç­”æ¡ˆæ—¶èƒ½å¤Ÿåˆ©ç”¨æ‰€æœ‰è¿™äº›çŸ¥è¯†ã€‚
- en: Fine tuned models are easier to run at the edge without needing the infrastructure
    of vector stores close to the model
  id: totrans-split-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ç»è¿‡å¾®è°ƒçš„æ¨¡å‹æ›´å®¹æ˜“åœ¨è¾¹ç¼˜è¿è¡Œï¼Œæ— éœ€å°†å‘é‡å­˜å‚¨çš„åŸºç¡€è®¾æ–½é è¿‘æ¨¡å‹
- en: You can use a much smaller fine-tuned model than general purpose model plus
    RAG. What can you do with a fine-tuned Phi-2 model that can run on any CPU?
  id: totrans-split-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æ‚¨å¯ä»¥ä½¿ç”¨æ¯”é€šç”¨æ¨¡å‹åŠ RAGæ›´å°çš„å¾®è°ƒæ¨¡å‹ã€‚å¯¹äºå¯ä»¥åœ¨ä»»ä½•CPUä¸Šè¿è¡Œçš„Phi-2å¾®è°ƒæ¨¡å‹ï¼Œæ‚¨å¯ä»¥åšä»€ä¹ˆï¼Ÿ
- en: We made it work!!
  id: totrans-split-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬è®©å®ƒå·¥ä½œäº†ï¼ï¼
- en: Are we wrong? [Come and roast us on Discord](https://discord.gg/VJftd844GE)!
  id: totrans-split-38
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬é”™äº†å—ï¼Ÿ[æ¥Discordä¸Šåæ§½æˆ‘ä»¬å§](https://discord.gg/VJftd844GE)!
- en: 'We now have an OpenAI compatible API. For example, here I am configuring Flowise
    to integrate with my private Helix deployment, for example:'
  id: totrans-split-39
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬ç°åœ¨æœ‰ä¸€ä¸ªå…¼å®¹OpenAIçš„APIã€‚ä¾‹å¦‚ï¼Œåœ¨è¿™é‡Œæˆ‘æ­£åœ¨é…ç½®Flowiseä¸æˆ‘çš„ç§æœ‰Helixéƒ¨ç½²é›†æˆï¼Œä¾‹å¦‚ï¼š
- en: 'Just set:'
  id: totrans-split-40
  prefs: []
  type: TYPE_NORMAL
  zh: åªéœ€è®¾ç½®ï¼š
- en: '**Model Name:** mistralai/Mistral-7B-Instruct-v0.1'
  id: totrans-split-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**æ¨¡å‹åç§°ï¼š** mistralai/Mistral-7B-Instruct-v0.1'
- en: '**Connect Credential:** Your API key from [https://app.tryhelix.ai/account](https://app.tryhelix.ai/account)'
  id: totrans-split-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**è¿æ¥å‡­è¯ï¼š** æ‚¨çš„APIå¯†é’¥æ¥è‡ª[https://app.tryhelix.ai/account](https://app.tryhelix.ai/account)'
- en: '**BasePath (under additional parameters):** https://app.tryhelix.ai/v1'
  id: totrans-split-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**BasePathï¼ˆé™„åŠ å‚æ•°ä¸‹ï¼‰ï¼š** https://app.tryhelix.ai/v1'
- en: And thatâ€™s it! Youâ€™ll notice your API calls to Helix show up as sessions in
    your account, so you get a free record of them :-)
  id: totrans-split-44
  prefs: []
  type: TYPE_NORMAL
  zh: å°±æ˜¯è¿™æ ·ï¼æ‚¨ä¼šæ³¨æ„åˆ°ï¼Œé€šè¿‡Helixè¿›è¡Œçš„APIè°ƒç”¨ä¼šæ˜¾ç¤ºä¸ºæ‚¨è´¦æˆ·ä¸­çš„ä¼šè¯ï¼Œè¿™æ ·æ‚¨å°±å¯ä»¥å…è´¹è®°å½•å®ƒä»¬ :-)
- en: Weâ€™ll automatically fine tune the model once the qapairs are extracted, then
    email you when itâ€™s finished. We hope this will encourage more people to dive
    back into the app once youâ€™ve waited the 10-15 minutes it takes to train your
    own AI.
  id: totrans-split-45
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€æ—¦æå–äº†é—®ç­”å¯¹ï¼Œæˆ‘ä»¬å°†è‡ªåŠ¨å¾®è°ƒæ¨¡å‹ï¼Œå®Œæˆåä¼šé€šè¿‡ç”µå­é‚®ä»¶é€šçŸ¥æ‚¨ã€‚æˆ‘ä»¬å¸Œæœ›è¿™æ ·åšèƒ½é¼“åŠ±æ›´å¤šäººï¼Œåœ¨ç­‰å¾…èŠ±è´¹10-15åˆ†é’Ÿæ¥è®­ç»ƒè‡ªå·±çš„AIåï¼Œé‡æ–°æŠ•å…¥åº”ç”¨ç¨‹åºã€‚
- en: Thanks for reading! Soon Iâ€™ll blog more about our roadmap, on-prem use cases
    where weâ€™re seeing significant commerial traction, and our dirty secret. Subscribe
    and stay in the loop :-)
  id: totrans-split-46
  prefs: []
  type: TYPE_NORMAL
  zh: æ„Ÿè°¢é˜…è¯»ï¼å¾ˆå¿«æˆ‘å°†åœ¨æˆ‘ä»¬çš„è·¯çº¿å›¾ä¸Šå†™æ›´å¤šï¼Œå…³äºæˆ‘ä»¬åœ¨çœ‹åˆ°æ˜¾è‘—å•†ä¸šå¸å¼•åŠ›çš„æœ¬åœ°ä½¿ç”¨æ¡ˆä¾‹ï¼Œä»¥åŠæˆ‘ä»¬çš„ç§˜å¯†ã€‚è®¢é˜…å¹¶ä¿æŒå…³æ³¨ :-)
