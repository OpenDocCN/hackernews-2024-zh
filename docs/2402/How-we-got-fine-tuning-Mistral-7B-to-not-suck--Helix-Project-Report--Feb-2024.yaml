- en: <!--yml
  id: totrans-split-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-split-1
  prefs: []
  type: TYPE_NORMAL
  zh: 类别：未分类
- en: 'date: 2024-05-27 14:43:03'
  id: totrans-split-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024年05月27日14:43:03
- en: -->
  id: totrans-split-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: 'How we got fine-tuning Mistral-7B to not suck: Helix Project Report, Feb 2024'
  id: totrans-split-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 我们是如何精调Mistral-7B以避免失败的：螺旋计划报告，2024年2月
- en: 来源：[https://helixml.substack.com/p/how-we-got-fine-tuning-mistral-7b](https://helixml.substack.com/p/how-we-got-fine-tuning-mistral-7b)
  id: totrans-split-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://helixml.substack.com/p/how-we-got-fine-tuning-mistral-7b](https://helixml.substack.com/p/how-we-got-fine-tuning-mistral-7b)
- en: Hi folks,
  id: totrans-split-6
  prefs: []
  type: TYPE_NORMAL
  zh: 大家好，
- en: 'It’s been just over a month since we [launched Helix v0.1](https://www.producthunt.com/products/helix-5),
    and today I’m happy to announce today the availability of [Helix v0.5](https://github.com/helixml/helix/releases).
    [Run it yourself on your own secure private infrastructure](https://docs.helix.ml/docs/controlplane)
    or [try it out on our SaaS](https://app.tryhelix.ai):'
  id: totrans-split-7
  prefs: []
  type: TYPE_NORMAL
  zh: 距[Helix v0.1发布](https://www.producthunt.com/products/helix-5)已经过去一个多月了，今天我很高兴地宣布[Helix
    v0.5](https://github.com/helixml/helix/releases)的可用性。[在您自己的安全私人基础设施上运行](https://docs.helix.ml/docs/controlplane)或者[在我们的SaaS上试用](https://app.tryhelix.ai)吧：
- en: Along with a shiny new UI (you can a screenshot for comparison in our [first
    post](https://helixml.substack.com/p/building-a-generative-ai-platform)), we’ve
    been extremely focused on improving the quality of the text fine-tuning.
  id: totrans-split-8
  prefs: []
  type: TYPE_NORMAL
  zh: 除了全新的闪亮UI（你可以在我们的[第一篇文章](https://helixml.substack.com/p/building-a-generative-ai-platform)中找到比较的截图），我们一直在极力提高文本精调的质量。
- en: When we first launched Helix, the text fine tuning of the Mistral-7B-Instruct
    language model was based on [this LlamaIndex docs page](https://docs.llamaindex.ai/en/latest/examples/finetuning/knowledge/finetune_knowledge.html).
    Yeah, we basically decided to build a whole business around one this LlamaIndex
    notebook.
  id: totrans-split-9
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们首次推出Helix时，Mistral-7B-Instruct语言模型的文本精调是基于[这篇LlamaIndex文档页面](https://docs.llamaindex.ai/en/latest/examples/finetuning/knowledge/finetune_knowledge.html)。是的，我们基本上决定围绕这本LlamaIndex笔记开展整个业务。
- en: 'At the time, unfortunately I hadn’t even seen the link from the parent page
    in the LlamaIndex docs that said “WIP: this isn’t very good yet”. Uh-oh.'
  id: totrans-split-10
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，当时我甚至没有看到来自LlamaIndex文档父页面的链接，上面写着“WIP：这还不太好”。哎呀。
- en: 'Of course, because Helix is targeting being runnable fully on-prem, we’re using
    [Mistral-7B with axolotl](https://github.com/OpenAccess-AI-Collective/axolotl)
    for fine-tuning instead of the GPT-3.5 API, but it’s the same principle. The idea
    is that you chunk your documents up into pieces, then ask a language model to
    generate question-answer pairs (which is the shape of the training data you need
    to provide the fine-tuning process). You’re using an LLM to automate generating
    training data for fine-tuning another LLM. The original prompt looks like this:'
  id: totrans-split-11
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，因为Helix的目标是完全在本地运行，所以我们在精调时使用[Mistral-7B与axolotl](https://github.com/OpenAccess-AI-Collective/axolotl)，而不是使用GPT-3.5
    API，但原理相同。思路是将文档分块，然后要求语言模型生成问答对（这是您需要提供给精调过程的训练数据的形式）。您使用LLM来自动化生成用于精调另一个LLM的训练数据。原始提示看起来像这样：
- en: Did it work? Sorta, but it was a bit crap. Just like LlamaIndex said it would
    be.
  id: totrans-split-12
  prefs: []
  type: TYPE_NORMAL
  zh: 行得通吗？有点行，但有点糟糕。就像LlamaIndex说的那样。
- en: We were able to feed it [complex technical papers](https://docs.helix.ml/docs/papers),
    and it was able to answer technical questions about them. But it failed at some
    much more basic tasks.
  id: totrans-split-13
  prefs: []
  type: TYPE_NORMAL
  zh: 我们能够为其提供[复杂的技术论文](https://docs.helix.ml/docs/papers)，它能够回答与它们有关的技术问题。但是在一些更基本的任务上失败了。
- en: 'This one news article for example: [Junior doctors in England to stage more
    strikes over pay](https://www.theguardian.com/society/2023/dec/05/junior-doctors-in-england-to-stage-more-strikes).
    This article became the bane of my life for a few weeks 😂'
  id: totrans-split-14
  prefs: []
  type: TYPE_NORMAL
  zh: 例如这篇新闻文章：[英格兰的初级医生因薪酬问题将再次举行罢工](https://www.theguardian.com/society/2023/dec/05/junior-doctors-in-england-to-stage-more-strikes)。这篇文章在我生活中成为了一段时间的噩梦
    😂
- en: 'Why? Because when we first asked the fine tuned model the simple question:'
  id: totrans-split-15
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么？因为当我们首次询问文本精调模型一个简单的问题时：
- en: What?! You were fine-tuned on information derived from the article. Why are
    you talking about fine-tuning?!
  id: totrans-split-16
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 什么？！你是从那篇文章中获取的信息进行精调的。你为什么要谈论精调？！
- en: –me
  id: totrans-split-17
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: –我
- en: 'OK, turned out this one was fairly straightforward. One of the text elements
    meant to tell the user “fine tuning completed” was also being sent back to the
    model as if the user had said it. Now the only context the model had to go on
    was the idea of fine-tuning. We got that one out of the way. Cool, let’s try again:'
  id: totrans-split-18
  prefs: []
  type: TYPE_NORMAL
  zh: 好吧，结果还是相当简单的。为了告诉用户“精调完成”，其中一个文本元素也被发送回模型，就好像用户真的说了这句话。现在，模型得到的唯一上下文就是精调的概念。我们解决了这个问题。很酷，让我们再试一次：
- en: OMG, seriously? Surely you should know that the doctors are threatening to go
    on strike. It’s right there in the title! Stupid fine-tuning, maybe it will never
    work. 🤬
  id: totrans-split-19
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 哦我的天啊，认真的吗？你一定知道医生们正在威胁要罢工。标题上不就写着吗！愚蠢的微调，也许永远不会奏效。🤬
- en: –me
  id: totrans-split-20
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: –我
- en: 'But we persisted. OK, so why isn’t the model able to answer questions about
    the basic context of the article? Well, it’s because the question-answer pairs
    generated by the prompt aren’t in the form of simple questions about the title
    of the document. The solution, it turned out, was rather than a single qapair
    generating prompt, we had to implement a *suite* of them, to extract context from
    the document from all sorts of different perspectives: what are the entities in
    the document and how are they related? What’s a short, medium and long summary
    of the document? Who/what/where questions, etc. [See the full list here](https://github.com/helixml/helix/blob/main/api/pkg/dataprep/qapairs/qapair_config.yaml).'
  id: totrans-split-21
  prefs: []
  type: TYPE_NORMAL
  zh: 但我们坚持下来了。好吧，为什么模型不能回答关于文章基本背景的问题呢？嗯，这是因为提示生成的问答对不是关于文档标题的简单问题形式。解决方案是，不是单一的问答对生成提示，我们必须实现一个*套件*，从各种不同的角度提取文档的上下文：文档中的实体是什么，它们之间如何关联？文档的简短、中等和长篇摘要是什么？谁/什么/在哪里的问题等等。[在此处查看完整列表](https://github.com/helixml/helix/blob/main/api/pkg/dataprep/qapairs/qapair_config.yaml)。
- en: Turns out, when we carefully constructed this suite of prompts, we were then
    finally able to get the model to answer the basic question about “what are the
    doctors going to do?” Phew! 😅
  id: totrans-split-22
  prefs: []
  type: TYPE_NORMAL
  zh: 结果表明，当我们精心构建了这一系列提示时，最终我们能够让模型回答关于“医生们将要做什么？”的基本问题。哎呀！😅
- en: Our other insight was that by generating a content-addressed hash for each document,
    we could also teach the model about the IDs of the individual documents, along
    with IDs for groups of documents.
  id: totrans-split-23
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的另一个发现是，通过为每个文档生成一个内容寻址的哈希，我们还可以教会模型关于单个文档的ID，以及文档组的ID。
- en: 'We can then map those IDs back onto the documents the model was fine-tuned
    on. For example: [in this session](https://app.tryhelix.ai/session/4d769c7c-b9a1-4fc4-b5c9-d277fc59ed1b)
    the model is able to tell you that what it’s learned was in a given document,
    even linking the user back to that document. I also found this exchange pretty
    hilarious:'
  id: totrans-split-24
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们可以将这些ID映射回模型进行微调的文档。例如：[在这个会话中](https://app.tryhelix.ai/session/4d769c7c-b9a1-4fc4-b5c9-d277fc59ed1b)，模型能够告诉您它所学到的是在给定的文档中，并将用户链接回该文档。我还觉得这次交流挺搞笑的：
- en: Although maybe that says more about my sense of humour than anything else.
  id: totrans-split-25
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管也许这更多地反映了我的幽默感而已。
- en: We then added a system prompt telling the model to refer only to the specific
    document IDs it was trained on and not to refer to background knowledge. What
    do you know, it worked!
  id: totrans-split-26
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们添加了一个系统提示，告诉模型只参考它训练过的特定文档ID，而不参考背景知识。你知道吗，这行得通了！
- en: So far we’re adjusting the prompts and system for this LLM app based on “vibes”.
    That is, trying stuff, evaluating it by eye, and then changing it. Problem is,
    vibes don’t scale.
  id: totrans-split-27
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们根据“氛围”调整了这个LLM应用的提示和系统。也就是说，尝试一些东西，通过眼睛评估，然后进行修改。问题是，氛围无法扩展。
- en: Work is ongoing on an end-to-end “evals” framework so we can automatically build
    up a library of good and bad sessions, and then every time we change the prompts,
    code, model etc re-run the fine-tuning across all of the sessions in the library,
    and then grade them. We might even use an LLM to grade them automatically :-)
  id: totrans-split-28
  prefs: []
  type: TYPE_NORMAL
  zh: 正在进行一个端到端的“评估”框架，以便我们可以自动建立一个好和坏会话的库，然后每次我们改变提示、代码、模型等，重新运行整个库中的所有会话的微调，然后进行评分。我们甚至可能会使用LLM来自动评分它们
    :-)
- en: Please help us by clicking the new thumbs up and thumbs down buttons at the
    bottom of your sessions! We’ll use these as input to improve the product.
  id: totrans-split-29
  prefs: []
  type: TYPE_NORMAL
  zh: 请通过点击您会话底部的新的点赞和点踩按钮来帮助我们！我们将使用这些作为改进产品的输入。
- en: 'Oh believe me, we’ve talked about it. We’re sticking with fine-tuning for now,
    because:'
  id: totrans-split-30
  prefs: []
  type: TYPE_NORMAL
  zh: 哦，相信我，我们已经谈论过了。我们暂时坚持微调，因为：
- en: Fine tuning can memorize far more information than you can fit in a single prompt
  id: totrans-split-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 微调可以记忆的信息远远超过你可以在单个提示中适合的信息。
- en: By not needing to cram any custom knowledge in the prompt, you can get much
    better latency
  id: totrans-split-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过不需要在提示中塞入任何自定义知识，您可以获得更好的延迟。
- en: Fine tuning is better at copying style (we have qapair prompts planned for this)
  id: totrans-split-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 微调更擅长复制风格（我们计划为此准备问题对）。
- en: Fine tuning is better at understanding a large corpus of background knowledge
    (a “domain”) and being able to draw on all of it when constructing an answer
  id: totrans-split-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 微调更擅长理解大量背景知识（“领域”），并且在构建答案时能够利用所有这些知识。
- en: Fine tuned models are easier to run at the edge without needing the infrastructure
    of vector stores close to the model
  id: totrans-split-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 经过微调的模型更容易在边缘运行，无需将向量存储的基础设施靠近模型
- en: You can use a much smaller fine-tuned model than general purpose model plus
    RAG. What can you do with a fine-tuned Phi-2 model that can run on any CPU?
  id: totrans-split-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您可以使用比通用模型加RAG更小的微调模型。对于可以在任何CPU上运行的Phi-2微调模型，您可以做什么？
- en: We made it work!!
  id: totrans-split-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们让它工作了！！
- en: Are we wrong? [Come and roast us on Discord](https://discord.gg/VJftd844GE)!
  id: totrans-split-38
  prefs: []
  type: TYPE_NORMAL
  zh: 我们错了吗？[来Discord上吐槽我们吧](https://discord.gg/VJftd844GE)!
- en: 'We now have an OpenAI compatible API. For example, here I am configuring Flowise
    to integrate with my private Helix deployment, for example:'
  id: totrans-split-39
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在有一个兼容OpenAI的API。例如，在这里我正在配置Flowise与我的私有Helix部署集成，例如：
- en: 'Just set:'
  id: totrans-split-40
  prefs: []
  type: TYPE_NORMAL
  zh: 只需设置：
- en: '**Model Name:** mistralai/Mistral-7B-Instruct-v0.1'
  id: totrans-split-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**模型名称：** mistralai/Mistral-7B-Instruct-v0.1'
- en: '**Connect Credential:** Your API key from [https://app.tryhelix.ai/account](https://app.tryhelix.ai/account)'
  id: totrans-split-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**连接凭证：** 您的API密钥来自[https://app.tryhelix.ai/account](https://app.tryhelix.ai/account)'
- en: '**BasePath (under additional parameters):** https://app.tryhelix.ai/v1'
  id: totrans-split-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**BasePath（附加参数下）：** https://app.tryhelix.ai/v1'
- en: And that’s it! You’ll notice your API calls to Helix show up as sessions in
    your account, so you get a free record of them :-)
  id: totrans-split-44
  prefs: []
  type: TYPE_NORMAL
  zh: 就是这样！您会注意到，通过Helix进行的API调用会显示为您账户中的会话，这样您就可以免费记录它们 :-)
- en: We’ll automatically fine tune the model once the qapairs are extracted, then
    email you when it’s finished. We hope this will encourage more people to dive
    back into the app once you’ve waited the 10-15 minutes it takes to train your
    own AI.
  id: totrans-split-45
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦提取了问答对，我们将自动微调模型，完成后会通过电子邮件通知您。我们希望这样做能鼓励更多人，在等待花费10-15分钟来训练自己的AI后，重新投入应用程序。
- en: Thanks for reading! Soon I’ll blog more about our roadmap, on-prem use cases
    where we’re seeing significant commerial traction, and our dirty secret. Subscribe
    and stay in the loop :-)
  id: totrans-split-46
  prefs: []
  type: TYPE_NORMAL
  zh: 感谢阅读！很快我将在我们的路线图上写更多，关于我们在看到显著商业吸引力的本地使用案例，以及我们的秘密。订阅并保持关注 :-)
