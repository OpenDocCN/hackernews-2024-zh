- en: <!--yml
  id: totrans-split-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-split-1
  prefs: []
  type: TYPE_NORMAL
  zh: 'category: 未分类'
- en: 'date: 2024-05-29 13:27:39'
  id: totrans-split-2
  prefs: []
  type: TYPE_NORMAL
  zh: 'date: 2024-05-29 13:27:39'
- en: -->
  id: totrans-split-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: Don't Mock Machine Learning Models In Unit Tests
  id: totrans-split-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 不要在单元测试中嘲笑机器学习模型
- en: 来源：[https://eugeneyan.com/writing/unit-testing-ml/](https://eugeneyan.com/writing/unit-testing-ml/)
  id: totrans-split-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://eugeneyan.com/writing/unit-testing-ml/](https://eugeneyan.com/writing/unit-testing-ml/)
- en: I’ve been applying typical unit testing practices to machine learning code and
    it hasn’t been straightforward. In software, units are small, isolated pieces
    of logic that we can test independently and quickly. In machine learning, models
    are blobs of logic learned from data, and machine learning code is the logic to
    learn and use these derived blobs of logic. This difference makes it necessary
    to rethink how we unit test machine learning code.
  id: totrans-split-6
  prefs: []
  type: TYPE_NORMAL
  zh: 我一直在将典型的单元测试实践应用于机器学习代码，但并不简单。在软件中，单元是小的、孤立的逻辑片段，我们可以独立快速地测试。而在机器学习中，模型是从数据中学习到的逻辑块，机器学习代码是用来学习和使用这些派生逻辑块的逻辑。这种差异使得有必要重新思考我们如何对机器学习代码进行单元测试。
- en: How ML code differs from regular software
  id: totrans-split-7
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 机器学习代码与常规软件的区别
- en: '**In software, we write code that *contains* logic; in ML, we write code that
    *learns* logic and then uses that learned logic.** Software code transforms input
    data + handcrafted logic into expected output. We can then test these outputs
    against asserts. In contrast, machine learning code transforms input data + expected
    output into learned logic (i.e., a model).'
  id: totrans-split-8
  prefs: []
  type: TYPE_NORMAL
  zh: '**在软件中，我们编写包含逻辑的代码；在机器学习中，我们编写学习逻辑并使用该学习逻辑的代码。** 软件代码将输入数据 + 手工逻辑转换为预期输出。然后，我们可以根据断言测试这些输出。相反，机器学习代码将输入数据
    + 预期输出转换为学习到的逻辑（即模型）。'
- en: '\[\text{Software}: \text{Input Data} + Handcrafted \text{ Logic} = \text{Expected
    Output}\] \[\text{Machine Learning}: \text{Input Data} + \text{Expected Output}
    = Learned \text{ Logic}\]'
  id: totrans-split-9
  prefs: []
  type: TYPE_NORMAL
  zh: '\[\text{软件}: \text{输入数据} + 手工逻辑 = \text{预期输出}\] \[\text{机器学习}: \text{输入数据}
    + \text{预期输出} = 学习到的逻辑\]'
- en: Thus, in machine learning, instead of writing code that contains logic, we write
    code to learn logic, such as via [building a decision tree](https://github.com/eugeneyan/testing-ml/blob/master/src/tree/decision_tree.py#L149)
    or [finetuning a hallucination classifier](https://github.com/eugeneyan/visualizing-finetunes/blob/main/3_ft_usb_then_fib.ipynb).
    Because the logic that acts on the input data is embedded within the model, if
    we want to test the learned logic, we’ll need to load the model, perform inference
    on some sample output, and then assert if the output matches the expected input.
  id: totrans-split-10
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，在机器学习中，我们不是写包含逻辑的代码，而是编写用于学习逻辑的代码，比如通过 [构建决策树](https://github.com/eugeneyan/testing-ml/blob/master/src/tree/decision_tree.py#L149)
    或 [微调幻觉分类器](https://github.com/eugeneyan/visualizing-finetunes/blob/main/3_ft_usb_then_fib.ipynb)。由于作用于输入数据的逻辑嵌入在模型内部，如果我们要测试学习到的逻辑，我们需要加载模型，在一些样本输出上执行推理，然后断言输出是否符合预期输入。
- en: '**In software, we typically mock dependencies like APIs; in ML, we want to
    test the actual model (sometimes).** When unit testing software, it’s good practice
    to mock database calls, filesystem access, sending emails/push notifications,
    etc. However, in ML, there are scenarios where we’ll want to test against the
    actual model.'
  id: totrans-split-11
  prefs: []
  type: TYPE_NORMAL
  zh: '**在软件中，我们通常模拟依赖关系如 API；在 ML 中，我们希望测试实际模型（有时）。** 在单元测试软件时，模拟数据库调用、文件系统访问、发送电子邮件/推送通知等是良好的实践。然而，在机器学习中，有些情况下我们希望针对实际模型进行测试。'
- en: 'For example, we want to test that loss decreases with each batch and the model
    can overfit (before wasting compute on an hopeless run.) If the model is a classifier,
    we want to check that the inference logic is correct. For instance, two models
    may have different output classes: [Google’s T5 NLI model](https://huggingface.co/google/t5_11b_trueteacher_and_anli)
    classifies factual consistency with class = 1 while [Meta’s BART NLI model](https://huggingface.co/facebook/bart-large-mnli)
    classifies it with class = 2!'
  id: totrans-split-12
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，我们希望测试每个批次的损失是否减少，以及模型是否可能过拟合（在浪费计算资源之前）。如果模型是一个分类器，我们希望检查推理逻辑是否正确。例如，两个模型可能具有不同的输出类别：[Google
    的 T5 NLI 模型](https://huggingface.co/google/t5_11b_trueteacher_and_anli) 将事实一致性分类为类别
    1，而 [Meta 的 BART NLI 模型](https://huggingface.co/facebook/bart-large-mnli) 则分类为类别
    2！
- en: '**Machine learning / language models can be large and unwieldy.** Some neural
    networks can be in the billions of parameters, exceeding what a laptop or standard
    dev environment can load. And even if we have the memory for smaller models, they
    are slow to load and perform inference on, testing our patience as we unit test
    while coding.'
  id: totrans-split-13
  prefs: []
  type: TYPE_NORMAL
  zh: '**机器学习/语言模型可能会很大且难以处理。** 一些神经网络可能有数十亿个参数，超出了笔记本电脑或标准开发环境可以加载的范围。即使我们有足够小型模型的内存，它们加载和推断速度也很慢，在编码过程中进行单元测试时会考验我们的耐心。'
- en: Some guidelines for unit testing ML code & models
  id: totrans-split-14
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 一些机器学习代码和模型单元测试的指导方针
- en: (These are a work in progress and my thinking’s still evolving—all feedback
    welcome!)
  id: totrans-split-15
  prefs: []
  type: TYPE_NORMAL
  zh: （这些仍在进行中，我的思路仍在演变中——欢迎所有反馈！）
- en: '**Use small, simple data samples.** Avoid loading CSVs or Parquet files as
    sample data. (It’s fine for integration tests and evals but not unit tests.) Define
    sample data directly in unit test code—so that the test is self-contained—to test
    key functionality such as:'
  id: totrans-split-16
  prefs: []
  type: TYPE_NORMAL
  zh: '**使用小型、简单的数据样本。** 避免加载CSV或Parquet文件作为样本数据。（对于集成测试和评估是可以的，但不适合单元测试。）直接在单元测试代码中定义样本数据，使测试自包含，以测试关键功能，如：'
- en: Splitting into train/test tests when you have custom logic
  id: totrans-split-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当存在自定义逻辑时进行训练/测试集分割
- en: Custom implementations, such as Cosine or Euclidean distance in Java
  id: totrans-split-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 自定义实现，例如Java中的余弦或欧几里得距离
- en: Preprocessing such as data augmentation or encoding
  id: totrans-split-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 预处理，例如数据增强或编码
- en: Postprocessing such as diversification or filtering recommendations
  id: totrans-split-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 后处理，例如多样化或筛选推荐
- en: Error handling for empty or malformed input
  id: totrans-split-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 处理空或格式错误输入的错误处理
- en: '**When viable, test against random or empty weights.** For example, we can
    initialize a model configuration with random weights to test output shape and
    device movement (from CPU to GPU and back). Here’s an example of how to initialize
    a model without having to download the weights and then assert the output shape:'
  id: totrans-split-22
  prefs: []
  type: TYPE_NORMAL
  zh: '**在可行时，对随机或空权重进行测试。** 例如，我们可以使用随机权重初始化模型配置，以测试输出形状和设备移动（从CPU到GPU再回来）。以下是初始化模型而不必下载权重并断言输出形状的示例：'
- en: '[PRE0]'
  id: totrans-split-23
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'The accelerate library also has [an example of initializing a model with empty
    weights](https://github.com/huggingface/accelerate/blob/main/tests/test_big_modeling.py#L955):'
  id: totrans-split-24
  prefs: []
  type: TYPE_NORMAL
  zh: 加速库还展示了[初始化空权重模型的示例](https://github.com/huggingface/accelerate/blob/main/tests/test_big_modeling.py#L955)：
- en: '[PRE1]'
  id: totrans-split-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '**Write critical tests against the actual model.** If they take a while to
    run, [mark them as slow](https://docs.pytest.org/en/latest/how-to/mark.html#registering-marks)
    and run only when needed (e.g., pre-commit and pre-merge). Some essentials include:'
  id: totrans-split-26
  prefs: []
  type: TYPE_NORMAL
  zh: '**对实际模型编写关键测试。** 如果运行时间较长，[标记为慢速](https://docs.pytest.org/en/latest/how-to/mark.html#registering-marks)，仅在需要时运行（例如，预提交和预合并）。一些要点包括：'
- en: Verify training is done correctly, such as loss going down, model overfitting,
    and training till convergence on a small sample of data
  id: totrans-split-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 验证训练是否正确完成，例如损失减少、模型过拟合和在少量数据上训练到收敛
- en: Verify model outputs match expectation, such as 0.99 = unsafe instead of safe
  id: totrans-split-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 验证模型输出是否符合期望，例如0.99 = 不安全而非安全
- en: Verify model server can start, take batch input, and return the expected output
  id: totrans-split-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 验证模型服务器能够启动，接受批量输入，并返回预期输出
- en: '**Don’t test external libraries.** We can assume that external libraries work.
    Thus, no need to test data loaders, tokenizers, optimizers, etc.'
  id: totrans-split-30
  prefs: []
  type: TYPE_NORMAL
  zh: '**不要测试外部库。** 我们可以假设外部库正常工作。因此，无需测试数据加载器、分词器、优化器等。'
- en: • • •
  id: totrans-split-31
  prefs: []
  type: TYPE_NORMAL
  zh: • • •
- en: What are your best practices for unit testing machine learning code and models?
    I would love to hear from you. [Please reach out!](https://twitter.com/eugeneyan)
  id: totrans-split-32
  prefs: []
  type: TYPE_NORMAL
  zh: 您在机器学习代码和模型单元测试中的最佳实践是什么？我很乐意听听您的意见。[请联系我！](https://twitter.com/eugeneyan)
- en: Further reading
  id: totrans-split-33
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 进一步阅读
- en: The problem when mocks happen when all your unit test passes and the program
    fails on integration. The mocks are a pristine place where your library unit test
    works like a champ. Bad mocks or bad library? Or both. Developers are then sent
    to debug the unit test… overhead.
  id: totrans-split-34
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 当所有单元测试通过而程序在集成中失败时，模拟发生的问题。模拟是一个理想的地方，其中你的库单元测试表现得像个冠军。是糟糕的模拟还是糟糕的库？还是两者都有问题。然后开发者们被派去调试单元测试…
    有点过头。
- en: 'Probablistic tests create an impossible problem: if you tighten your assertions
    you struggle with meaningless failing tests and normalize ignoring test failures,
    while if you loosen your assertions your tests aren’t really asserting anything
    any more. And there isn’t a happy balance: if you go somewhere in the middle,
    you end up having both problems.'
  id: totrans-split-35
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 概率测试创建了一个棘手的问题：如果你严格定义你的断言，你会苦于无意义的失败测试，而习惯性地忽略测试失败；但如果你放松定义，你的测试实际上就不再有所断言了。而且没有一个愉快的平衡点：如果你选择中间某个位置，你最终会遇到两个问题。
- en: How does it matter whether I inline my test data inside the unit test code,
    or have my unit test code load that same data from a checked-in file instead?
  id: totrans-split-36
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 为什么我将我的测试数据内联到单元测试代码中，或者让我的单元测试代码从一个已检入的文件中加载同样的数据，这有什么关系呢？
- en: It makes tests self-contained and easier to reason about. As a side-effect,
    random tests won’t accidentally break whenever you change some seemingly unrelated
    csv file. As a rule of thumb, I also only assert on input/output values that are
    explicitly defined as part of the test body. Saves a ton of time chasing down
    fixture definitions.
  id: totrans-split-37
  prefs:
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 这样做使得测试变得自包含且更易推理。作为副作用，随机测试在你改变看似无关的 csv 文件时不会意外地出错。按照经验法则，我也只在测试主体中显式定义的输入/输出值上进行断言。这样做节省了大量时间，不必追踪装置定义。
- en: I was expecting an article about side effects of hurting an LLM’s feelings in
    tests.
  id: totrans-split-38
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 我原以为会有一篇关于在测试中伤害 LLM 感情副作用的文章。
- en: 'If you found this useful, please cite this write-up as:'
  id: totrans-split-39
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您觉得这篇文章有用，请引用此文：
- en: Yan, Ziyou. (Feb 2024). Don't Mock Machine Learning Models In Unit Tests. eugeneyan.com.
    https://eugeneyan.com/writing/unit-testing-ml/.
  id: totrans-split-40
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: Yan, Ziyou. (2024年2月). 不要在单元测试中模拟机器学习模型。eugeneyan.com。https://eugeneyan.com/writing/unit-testing-ml/。
- en: or
  id: totrans-split-41
  prefs: []
  type: TYPE_NORMAL
  zh: 或者
- en: '[PRE2]'
  id: totrans-split-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Share on:'
  id: totrans-split-43
  prefs: []
  type: TYPE_NORMAL
  zh: 分享至：
