- en: <!--yml
  id: totrans-split-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-split-1
  prefs: []
  type: TYPE_NORMAL
  zh: 'category: 未分类'
- en: 'date: 2024-05-29 13:22:23'
  id: totrans-split-2
  prefs: []
  type: TYPE_NORMAL
  zh: 'date: 2024-05-29 13:22:23'
- en: -->
  id: totrans-split-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: Gemma, Ollama and LangChainGo - Eli Bendersky's website
  id: totrans-split-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Gemma，Ollama和LangChainGo - Eli Bendersky的网站
- en: 来源：[https://eli.thegreenplace.net/2024/gemma-ollama-and-langchaingo/](https://eli.thegreenplace.net/2024/gemma-ollama-and-langchaingo/)
  id: totrans-split-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://eli.thegreenplace.net/2024/gemma-ollama-and-langchaingo/](https://eli.thegreenplace.net/2024/gemma-ollama-and-langchaingo/)
- en: Yesterday Google released Gemma - an open LLM that folks can run locally on
    their machines (similarly to `llama2`). I was wondering how easy it would be to
    run Gemma on my computer, chat with it and interact with it from a Go program.
  id: totrans-split-6
  prefs: []
  type: TYPE_NORMAL
  zh: 昨天，Google发布了Gemma - 一个开放的LLM，用户可以在本地机器上运行（类似于`llama2`）。我想知道在我的计算机上运行Gemma，与之聊天并与之交互有多容易。
- en: 'Turns it - thanks to [Ollama](https://ollama.com/download) - it''s extremely
    easy! Gemma was already [added to Ollama](https://ollama.com/library/gemma), so
    all one has to do is run:'
  id: totrans-split-7
  prefs: []
  type: TYPE_NORMAL
  zh: 转而感谢[Ollama](https://ollama.com/download) - 这非常容易！Gemma已经被添加到了Ollama，因此所有人只需运行：
- en: And wait for a few minutes while the model downloads. From this point on, my
    previous post about [using Ollama locally in Go](https://eli.thegreenplace.net/2023/using-ollama-with-langchaingo/)
    applies with pretty much no changes. Gemma becomes available through a REST API
    locally, and can be accessed from ollama-aware libraries like [LangChainGo](https://github.com/tmc/langchaingo).
  id: totrans-split-8
  prefs: []
  type: TYPE_NORMAL
  zh: 等待几分钟，模型下载完成。从这一点开始，我之前关于[在Go中本地使用Ollama](https://eli.thegreenplace.net/2023/using-ollama-with-langchaingo/)的帖子几乎不需要更改，Gemma通过本地的REST
    API可用，并且可以通过像[LangChainGo](https://github.com/tmc/langchaingo)这样的ollama感知库访问。
- en: 'I went ahead and added a `--model` flag to all my [code samples from that post](https://github.com/eliben/code-for-blog/tree/main/2023/ollama-go-langchain),
    and they can all run with `--model gemma` now. It all just works, due to the magic
    of standard interfaces:'
  id: totrans-split-9
  prefs: []
  type: TYPE_NORMAL
  zh: 我已经为我所有的[来自该帖子的代码示例](https://github.com/eliben/code-for-blog/tree/main/2023/ollama-go-langchain)添加了一个`--model`标志，并且它们现在都可以使用`--model
    gemma`运行。这一切都很顺利，这要归功于标准接口的神奇：
- en: Gemma is packaged in a standard interface for inclusion in Ollama
  id: totrans-split-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gemma是一个标准接口包装，用于在Ollama中进行包含。
- en: Ollama then presents a standardized REST API for this model, just like it does
    for other compatible models
  id: totrans-split-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 然后，Ollama为该模型提供了标准化的REST API，就像它为其他兼容模型一样。
- en: LangChainGo has an Ollama provider that lets us write code to interact with
    any model running through Ollama
  id: totrans-split-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LangChainGo具有一个Ollama提供程序，让我们可以编写代码与通过Ollama运行的任何模型进行交互
- en: 'So we can write code like:'
  id: totrans-split-13
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们可以编写如下代码：
- en: '[PRE0]'
  id: totrans-split-14
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'And then run it as follows:'
  id: totrans-split-15
  prefs: []
  type: TYPE_NORMAL
  zh: 然后按以下方式运行它：
- en: '[PRE1]'
  id: totrans-split-16
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Gemma seems relatively fast for a model running on a CPU. I find that the default
    7B model, while much more capable than the default 7B llama2 based on published
    benchmarks - also runs about 30% faster on my machine.
  id: totrans-split-17
  prefs: []
  type: TYPE_NORMAL
  zh: 对于在CPU上运行的模型而言，Gemma似乎相对快速。我发现默认的7B模型，尽管在已发布的基准测试中比基于llama2的默认7B模型更强大，但在我的机器上也能运行快约30%。
- en: Without LangChainGo
  id: totrans-split-18
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 没有LangChainGo
- en: 'While LangChainGo offers a conveneint API that''s standardized across LLM providers,
    its use is by no means required for this sample. Ollama itself has a [Go API](https://pkg.go.dev/github.com/jmorganca/ollama/api)
    as part of its structure and it can be used externally as well. Here''s an equivalent
    sample that doesn''t require LangChainGo:'
  id: totrans-split-19
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然LangChainGo提供了一个跨LLM提供商标准化的便捷API，但并不是必须使用本示例。Ollama本身作为其结构的一部分具有一个[Go API](https://pkg.go.dev/github.com/jmorganca/ollama/api)，也可以外部使用。以下是一个不需要LangChainGo的等效示例：
- en: '[PRE2]'
  id: totrans-split-20
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
