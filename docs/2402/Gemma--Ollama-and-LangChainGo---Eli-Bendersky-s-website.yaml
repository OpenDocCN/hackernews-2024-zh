- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-05-29 13:22:23'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: Gemma, Ollama and LangChainGo - Eli Bendersky's website
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://eli.thegreenplace.net/2024/gemma-ollama-and-langchaingo/](https://eli.thegreenplace.net/2024/gemma-ollama-and-langchaingo/)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Yesterday Google released Gemma - an open LLM that folks can run locally on
    their machines (similarly to `llama2`). I was wondering how easy it would be to
    run Gemma on my computer, chat with it and interact with it from a Go program.
  prefs: []
  type: TYPE_NORMAL
- en: 'Turns it - thanks to [Ollama](https://ollama.com/download) - it''s extremely
    easy! Gemma was already [added to Ollama](https://ollama.com/library/gemma), so
    all one has to do is run:'
  prefs: []
  type: TYPE_NORMAL
- en: And wait for a few minutes while the model downloads. From this point on, my
    previous post about [using Ollama locally in Go](https://eli.thegreenplace.net/2023/using-ollama-with-langchaingo/)
    applies with pretty much no changes. Gemma becomes available through a REST API
    locally, and can be accessed from ollama-aware libraries like [LangChainGo](https://github.com/tmc/langchaingo).
  prefs: []
  type: TYPE_NORMAL
- en: 'I went ahead and added a `--model` flag to all my [code samples from that post](https://github.com/eliben/code-for-blog/tree/main/2023/ollama-go-langchain),
    and they can all run with `--model gemma` now. It all just works, due to the magic
    of standard interfaces:'
  prefs: []
  type: TYPE_NORMAL
- en: Gemma is packaged in a standard interface for inclusion in Ollama
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ollama then presents a standardized REST API for this model, just like it does
    for other compatible models
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: LangChainGo has an Ollama provider that lets us write code to interact with
    any model running through Ollama
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'So we can write code like:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'And then run it as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Gemma seems relatively fast for a model running on a CPU. I find that the default
    7B model, while much more capable than the default 7B llama2 based on published
    benchmarks - also runs about 30% faster on my machine.
  prefs: []
  type: TYPE_NORMAL
- en: Without LangChainGo
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'While LangChainGo offers a conveneint API that''s standardized across LLM providers,
    its use is by no means required for this sample. Ollama itself has a [Go API](https://pkg.go.dev/github.com/jmorganca/ollama/api)
    as part of its structure and it can be used externally as well. Here''s an equivalent
    sample that doesn''t require LangChainGo:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
