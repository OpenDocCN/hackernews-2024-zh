["```\nfrom transformers import AutoConfig, AutoModelForSequenceClassification\n\nmodel_name = \"valhalla/distilbart-mnli-12-1\"\nconfig = AutoConfig.from_pretrained(\"valhalla/distilbart-mnli-12-1\")\nmodel = AutoModelForSequenceClassification.from_config(config)\nassert model.classification_head.out_proj.out_features == 3 \n```", "```\ndef test_dispatch_model_bnb(self):\n    \"\"\"Tests that `dispatch_model` quantizes int8 layers\"\"\"\n    from huggingface_hub import hf_hub_download\n    from transformers import AutoConfig, AutoModel, BitsAndBytesConfig\n    from transformers.utils.bitsandbytes import replace_with_bnb_linear\n\n    with init_empty_weights():\n        model = AutoModel.from_config(AutoConfig.from_pretrained(\"bigscience/bloom-560m\"))\n\n    quantization_config = BitsAndBytesConfig(load_in_8bit=True)\n    model = replace_with_bnb_linear(\n        model, modules_to_not_convert=[\"lm_head\"], quantization_config=quantization_config\n    )\n\n    model_path = hf_hub_download(\"bigscience/bloom-560m\", \"pytorch_model.bin\")\n\n    model = load_checkpoint_and_dispatch(\n        model,\n        checkpoint=model_path,\n        device_map=\"balanced\",\n    )\n\n    assert model.h[0].self_attention.query_key_value.weight.dtype == torch.int8\n    assert model.h[0].self_attention.query_key_value.weight.device.index == 0\n\n    assert model.h[(-1)].self_attention.query_key_value.weight.dtype == torch.int8\n    assert model.h[(-1)].self_attention.query_key_value.weight.device.index == 1 \n```", "```\n@article{yan2024unit,\n  title   = {Don't Mock Machine Learning Models In Unit Tests},\n  author  = {Yan, Ziyou},\n  journal = {eugeneyan.com},\n  year    = {2024},\n  month   = {Feb},\n  url     = {https://eugeneyan.com/writing/unit-testing-ml/}\n}\n```"]