- en: <!--yml
  id: totrans-split-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: æœªåˆ†ç±»'
  id: totrans-split-1
  prefs: []
  type: TYPE_NORMAL
  zh: 'category: æœªåˆ†ç±»'
- en: 'date: 2024-05-29 13:18:46'
  id: totrans-split-2
  prefs: []
  type: TYPE_NORMAL
  zh: 'date: 2024-05-29 13:18:46'
- en: -->
  id: totrans-split-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: Meta's new LLM-based test generator is a sneak peek to the future of development
  id: totrans-split-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Metaæœ€æ–°åŸºäºLLMçš„æµ‹è¯•ç”Ÿæˆå™¨æ˜¯æœªæ¥å¼€å‘çš„ä¸€ä¸ªé¢„è§ˆã€‚
- en: æ¥æºï¼š[https://read.engineerscodex.com/p/metas-new-llm-based-test-generator](https://read.engineerscodex.com/p/metas-new-llm-based-test-generator)
  id: totrans-split-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: æ¥æºï¼š[https://read.engineerscodex.com/p/metas-new-llm-based-test-generator](https://read.engineerscodex.com/p/metas-new-llm-based-test-generator)
- en: '*Engineerâ€™s Codex is a publication about real-world software engineering.*'
  id: totrans-split-6
  prefs: []
  type: TYPE_NORMAL
  zh: '*Engineerâ€™s Codexæ˜¯ä¸€ä¸ªå…³äºç°å®ä¸–ç•Œè½¯ä»¶å·¥ç¨‹çš„å‡ºç‰ˆç‰©ã€‚'
- en: '* * *'
  id: totrans-split-7
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Meta recently released a [paper called â€œAutomated Unit Test Improvement using
    Large Language Models at Metaâ€](https://arxiv.org/abs/2402.09171). Itâ€™s a good
    look at how Big Tech is using AI internally to make development faster and software
    less buggy. For example, [Google is using AI to speed up code reviews](https://read.engineerscodex.com/i/139414745/critique-googles-code-review-tool).
  id: totrans-split-8
  prefs: []
  type: TYPE_NORMAL
  zh: Metaæœ€è¿‘å‘å¸ƒäº†ä¸€ç¯‡åä¸ºâ€œMetaåœ¨è‡ªåŠ¨åŒ–å•å…ƒæµ‹è¯•æ”¹è¿›ä¸­ä½¿ç”¨å¤§è¯­è¨€æ¨¡å‹â€çš„è®ºæ–‡ï¼ˆhttps://arxiv.org/abs/2402.09171ï¼‰ã€‚è¿™æ˜¯ä¸€ä¸ªæ·±å…¥äº†è§£å¤§å‹ç§‘æŠ€å…¬å¸å¦‚ä½•å†…éƒ¨ä½¿ç”¨AIæ¥åŠ å¿«å¼€å‘é€Ÿåº¦å’Œå‡å°‘è½¯ä»¶ç¼ºé™·çš„å¥½æœºä¼šã€‚ä¾‹å¦‚ï¼Œ[Googleæ­£åœ¨ä½¿ç”¨AIåŠ é€Ÿä»£ç å®¡æŸ¥](https://read.engineerscodex.com/i/139414745/critique-googles-code-review-tool)ã€‚
- en: A major win of this paper is that while it integrates LLMs into a developerâ€™s
    workflow, it also recommends fully-formed software improvements that are verified
    to be both correct and an improvement to current code coverage. This is **not
    a magic pill, but itâ€™s a good start on making LLMs more useful**. Compare this
    to ChatGPT, where suggestions still have to be manually verified to work - and
    we all know that [debugging code is twice as hard as writing it](https://read.engineerscodex.com/p/clever-code-is-probably-the-worst).
  id: totrans-split-9
  prefs: []
  type: TYPE_NORMAL
  zh: æœ¬æ–‡çš„ä¸€ä¸ªé‡è¦äº®ç‚¹æ˜¯ï¼Œå®ƒå°†LLMsé›†æˆåˆ°å¼€å‘è€…çš„å·¥ä½œæµç¨‹ä¸­ï¼ŒåŒæ—¶æ¨èç»è¿‡éªŒè¯çš„å®Œå…¨å½¢æˆçš„è½¯ä»¶æ”¹è¿›ï¼Œè¿™äº›æ”¹è¿›æ—¢æ­£ç¡®åˆèƒ½æé«˜å½“å‰ä»£ç è¦†ç›–ç‡ã€‚è¿™**ä¸æ˜¯çµä¸¹å¦™è¯ï¼Œä½†å®ƒæ˜¯ä½¿LLMsæ›´æœ‰ç”¨çš„ä¸€ä¸ªè‰¯å¥½å¼€ç«¯**ã€‚ä¸ChatGPTç›¸æ¯”ï¼Œå»ºè®®ä»ç„¶éœ€è¦æ‰‹åŠ¨éªŒè¯æ‰èƒ½ç”Ÿæ•ˆï¼Œæˆ‘ä»¬éƒ½çŸ¥é“ï¼Œ[è°ƒè¯•ä»£ç æ¯”ç¼–å†™ä»£ç éš¾ä¸Šä¸¤å€](https://read.engineerscodex.com/p/clever-code-is-probably-the-worst)ã€‚
- en: Meta claims that â€œthis is the first paper to report on LLM-generated code that
    has been developed independent of human intervention (other than final review
    sign off), and landed into large scale industrial production systems with guaranteed
    assurances for improvement over the existing code base.â€ (Bleh! That was a mouthful.)
  id: totrans-split-10
  prefs: []
  type: TYPE_NORMAL
  zh: Metaå£°ç§°â€œè¿™æ˜¯ç¬¬ä¸€ç¯‡æŠ¥å‘ŠLLMç”Ÿæˆçš„ä»£ç ï¼Œè¿™äº›ä»£ç åœ¨æ²¡æœ‰äººç±»å¹²é¢„ï¼ˆæœ€ç»ˆå®¡æŸ¥é™¤å¤–ï¼‰çš„æƒ…å†µä¸‹å¼€å‘ï¼Œå¹¶ä¸”å·²ç»è¿›å…¥äº†å¤§è§„æ¨¡å·¥ä¸šç”Ÿäº§ç³»ç»Ÿï¼Œå¹¶ä¿è¯æ¯”ç°æœ‰ä»£ç åº“æœ‰æ‰€æ”¹è¿›ã€‚â€ï¼ˆå“å‘€ï¼è¿™è¯è¯´èµ·æ¥çœŸè´¹å£èˆŒã€‚ï¼‰
- en: Furthermore, there are solid principles that developers can take away in order
    to use AI effectively themselves.
  id: totrans-split-11
  prefs: []
  type: TYPE_NORMAL
  zh: æ­¤å¤–ï¼Œå¼€å‘è€…å¯ä»¥æŒæ¡ä¸€äº›åšå®çš„åŸåˆ™ï¼Œä»¥ä¾¿è‡ªå·±æœ‰æ•ˆåœ°ä½¿ç”¨äººå·¥æ™ºèƒ½ã€‚
- en: '**Table of Contents:**'
  id: totrans-split-12
  prefs: []
  type: TYPE_NORMAL
  zh: '**ç›®å½•ï¼š**'
- en: '* * *'
  id: totrans-split-13
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: '***[SWE Quiz](http://swequiz.com)** is for ambitious developers who want to
    make sure their software fundamentals are rock solid. Reveal gaps in your knowledge
    and make sure you really know what youâ€™re doing both at work and while interviewing.*'
  id: totrans-split-14
  prefs: []
  type: TYPE_NORMAL
  zh: '***[SWE Quiz](http://swequiz.com)**æ˜¯ä¸ºæœ‰é›„å¿ƒçš„å¼€å‘è€…è®¾è®¡çš„ï¼Œä»–ä»¬æƒ³ç¡®ä¿è‡ªå·±çš„è½¯ä»¶åŸºç¡€ç‰¢å›ºã€‚æ­ç¤ºä½ çŸ¥è¯†ä¸­çš„ç©ºç™½ï¼Œå¹¶ç¡®ä¿ä½ åœ¨å·¥ä½œå’Œé¢è¯•æ—¶ç¡®å®çŸ¥é“è‡ªå·±åœ¨åšä»€ä¹ˆã€‚*'
- en: '*Take the tests for [authentication](https://www.swequiz.com/learn/authentication-roadmap),
    [caching](https://swequiz.com/learn/caching-roadmap), [databases](https://www.swequiz.com/learn/databases-roadmap),
    [API Design](https://www.swequiz.com/learn/api-design-roadmap), and [more](https://www.swequiz.com/learn).*'
  id: totrans-split-15
  prefs: []
  type: TYPE_NORMAL
  zh: '*ä¸º[èº«ä»½éªŒè¯](https://www.swequiz.com/learn/authentication-roadmap)ã€[ç¼“å­˜](https://swequiz.com/learn/caching-roadmap)ã€[æ•°æ®åº“](https://www.swequiz.com/learn/databases-roadmap)ã€[APIè®¾è®¡](https://www.swequiz.com/learn/api-design-roadmap)å’Œ[æ›´å¤š](https://www.swequiz.com/learn)è¿›è¡Œæµ‹è¯•ã€‚*'
- en: '[Check out SWE Quiz](https://swequiz.com)'
  id: totrans-split-16
  prefs: []
  type: TYPE_NORMAL
  zh: '[æŸ¥çœ‹SWE Quiz](https://swequiz.com)'
- en: '* * *'
  id: totrans-split-17
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: TestGen-LLM uses an approach called â€˜Assured LLM-based Software Engineeringâ€™
    (Assured LLMSE), using private, internal LLMs that are probably fine-tuned with
    Metaâ€™s codebase. This means that it uses LLMs to generate code improvements that
    are backed by â€œ**verifiable guaranteesâ€of improvement and non-regression.**
  id: totrans-split-18
  prefs: []
  type: TYPE_NORMAL
  zh: TestGen-LLMé‡‡ç”¨ä¸€ç§ç§°ä¸ºâ€˜ä¿è¯LLMåŸºç¡€è½¯ä»¶å·¥ç¨‹â€™ï¼ˆAssured LLMSEï¼‰çš„æ–¹æ³•ï¼Œä½¿ç”¨ç§æœ‰çš„ã€å†…éƒ¨çš„LLMsï¼Œè¿™äº›LLMså¯èƒ½ä¸Metaçš„ä»£ç åº“è¿›è¡Œäº†å¾®è°ƒã€‚è¿™æ„å‘³ç€å®ƒä½¿ç”¨LLMsç”Ÿæˆçš„ä»£ç æ”¹è¿›ï¼Œæ”¯æŒâ€œ**å¯éªŒè¯çš„ä¿è¯**ï¼Œç¡®ä¿æ”¹è¿›å’Œéå›å½’â€ã€‚
- en: TestGen-LLM uses an **ensemble approach** to generate code improvements. This
    means that it uses **â€œLLMs, prompts, and hyper-parametersâ€** to generate a set
    of candidate improvements, and then selects the best one. This approach can help
    to improve the quality of the generated improvements.
  id: totrans-split-19
  prefs: []
  type: TYPE_NORMAL
  zh: TestGen-LLMé‡‡ç”¨**åˆå¥æ–¹æ³•**ç”Ÿæˆä»£ç æ”¹è¿›ã€‚è¿™æ„å‘³ç€å®ƒä½¿ç”¨**â€œLLMsã€æç¤ºå’Œè¶…å‚æ•°â€**ç”Ÿæˆä¸€ç»„å€™é€‰æ”¹è¿›ï¼Œç„¶åé€‰æ‹©æœ€ä½³çš„ä¸€ä¸ªã€‚è¿™ç§æ–¹æ³•æœ‰åŠ©äºæé«˜ç”Ÿæˆæ”¹è¿›çš„è´¨é‡ã€‚
- en: 'TestGen-LLM is specifically designed to **improve existing human-written tests**
    rather than **generate code from scratch**.Â A good way to think about this LLM
    is: it''s a junior dev with the task of creating more comprehensive tests for
    existing code. Other devs have more important things to work on, so this LLM gets
    the fun task of improving unit tests.'
  id: totrans-split-20
  prefs: []
  type: TYPE_NORMAL
  zh: TestGen-LLMä¸“é—¨è®¾è®¡ç”¨äº**æ”¹è¿›ç°æœ‰çš„äººå·¥ç¼–å†™çš„æµ‹è¯•**ï¼Œè€Œä¸æ˜¯**ä»å¤´ç”Ÿæˆä»£ç **ã€‚å¯ä»¥è¿™æ ·ç†è§£è¿™ä¸ªLLMï¼šå®ƒæ˜¯ä¸€ä¸ªåˆçº§å¼€å‘äººå‘˜ï¼Œè´Ÿè´£ä¸ºç°æœ‰ä»£ç åˆ›å»ºæ›´å…¨é¢çš„æµ‹è¯•ã€‚å…¶ä»–å¼€å‘äººå‘˜æœ‰æ›´é‡è¦çš„äº‹æƒ…è¦åšï¼Œå› æ­¤è¿™ä¸ªLLMæœ‰å¹¸è´Ÿè´£æ”¹è¿›å•å…ƒæµ‹è¯•ã€‚
- en: The tests that it creates in its pull requests are often good, and sometimes
    trivial or pointless. Occasionally, a test it produces is really good or uncovers
    a bug inadvertently. Regardless, this work wouldn't have been done by humans anyways
    due to priorities. All of its pull requests require a human reviewer before pushed
    into the codebase.
  id: totrans-split-21
  prefs: []
  type: TYPE_NORMAL
  zh: å®ƒåœ¨æ‹‰å–è¯·æ±‚ä¸­åˆ›å»ºçš„æµ‹è¯•é€šå¸¸æ˜¯å¥½çš„ï¼Œæœ‰æ—¶æ˜¯å¾®ä¸è¶³é“æˆ–æ— æ„ä¹‰çš„ã€‚å¶å°”ï¼Œå®ƒç”Ÿæˆçš„æµ‹è¯•ç¡®å®å¾ˆå¥½ï¼Œæˆ–è€…æ— æ„ä¸­æ­ç¤ºäº†ä¸€ä¸ªbugã€‚æ— è®ºå¦‚ä½•ï¼Œè¿™é¡¹å·¥ä½œç”±äºä¼˜å…ˆäº‹é¡¹åŸå› ï¼Œäººç±»æœ¬æ¥ä¹Ÿä¸ä¼šåšã€‚æ‰€æœ‰çš„æ‹‰å–è¯·æ±‚åœ¨æ¨å…¥ä»£ç åº“ä¹‹å‰éƒ½éœ€è¦äººå·¥å®¡æŸ¥ã€‚
- en: TestGen-LLM has been **integrated into Meta's software engineering workflows**.
    This means that it can be used to automatically improve tests as part of the development
    process. It would be cool to see some screenshots of how exactly itâ€™s integrated,
    but the paper doesnâ€™t provide any.
  id: totrans-split-22
  prefs: []
  type: TYPE_NORMAL
  zh: TestGen-LLMå·²ç»**é›†æˆåˆ°Metaçš„è½¯ä»¶å·¥ç¨‹å·¥ä½œæµç¨‹**ä¸­ã€‚è¿™æ„å‘³ç€å®ƒå¯ä»¥ä½œä¸ºå¼€å‘è¿‡ç¨‹çš„ä¸€éƒ¨åˆ†è‡ªåŠ¨æ”¹è¿›æµ‹è¯•ã€‚çœ‹åˆ°å®ƒå¦‚ä½•ç¡®åˆ‡åœ°é›†æˆå°†ä¼šå¾ˆé…·ï¼Œä½†è®ºæ–‡æ²¡æœ‰æä¾›ä»»ä½•æˆªå›¾ã€‚
- en: These stats are either direct quotes or paraphrased quotes from the paper. A
    short reminder that all tests that were landed required a final sign-off by human
    reviewers.
  id: totrans-split-23
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™äº›ç»Ÿè®¡æ•°æ®è¦ä¹ˆæ˜¯ç›´æ¥å¼•ç”¨è¦ä¹ˆæ˜¯å¼•ç”¨çš„æ‘˜å½•ã€‚éœ€è¦æé†’ä¸€ä¸‹ï¼Œæ‰€æœ‰æˆåŠŸæŠ•å…¥çš„æµ‹è¯•éƒ½éœ€è¦äººå·¥å®¡æŸ¥æ‰¹å‡†ã€‚
- en: 'The image above shows that: in an evaluation on Reels and Stories products
    for Instagram, 75% of TestGen-LLM test cases that were generated built correctly,
    57% passed reliably, and *25% increased coverage*.'
  id: totrans-split-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä¸Šå›¾æ˜¾ç¤ºï¼šåœ¨Instagramçš„Reelså’ŒStoriesäº§å“çš„è¯„ä¼°ä¸­ï¼Œ75%çš„TestGen-LLMç”Ÿæˆçš„æµ‹è¯•ç”¨ä¾‹æ­£ç¡®æ„å»ºï¼Œ57%å¯é é€šè¿‡ï¼Œè€Œ*25%çš„è¦†ç›–ç‡å¢åŠ *ã€‚
- en: TestGen-LLM was able to improve **10% of all classes to which it was applied
    and 73% of its test improvements were accepted by developers**, and landed into
    production.
  id: totrans-split-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: TestGen-LLMèƒ½å¤Ÿæ”¹è¿›åº”ç”¨äºçš„æ‰€æœ‰ç±»çš„**10%**ï¼Œå…¶æµ‹è¯•æ”¹è¿›**73%**è¢«å¼€å‘äººå‘˜æ¥å—ï¼Œå¹¶æŠ•å…¥ç”Ÿäº§ã€‚
- en: In a â€œtest-a-thonâ€ between engineers, where various Meta engineers created tests
    in order to increase Instagramâ€™s test coverage, â€œ**the median number of lines
    of code added by a TestGen-LLM test was 2.5**.â€
  id: totrans-split-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: åœ¨Metaå·¥ç¨‹å¸ˆè¿›è¡Œçš„â€œæµ‹è¯•é©¬æ‹‰æ¾â€ä¸­ï¼Œå„ç§Metaå·¥ç¨‹å¸ˆåˆ›å»ºæµ‹è¯•ä»¥å¢åŠ Instagramçš„æµ‹è¯•è¦†ç›–ç‡ï¼Œâ€œ**TestGen-LLMæµ‹è¯•å¹³å‡å¢åŠ çš„ä»£ç è¡Œæ•°ä¸º2.5**ã€‚â€
- en: However, one test case â€œhit the jackpotâ€ and covered 1,326 lines.
  id: totrans-split-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ç„¶è€Œï¼Œæœ‰ä¸€ä¸ªæµ‹è¯•æ¡ˆä¾‹â€œå¤§è·æˆåŠŸâ€ï¼Œè¦†ç›–äº†1326è¡Œä»£ç ã€‚
- en: All improved cases generated during the â€œtest-a-thonâ€ did â€œcover at least one
    additional valid corner case, such as an early return and/or special processing
    for special values such as null and empty list.â€
  id: totrans-split-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: åœ¨â€œæµ‹è¯•é©¬æ‹‰æ¾â€æœŸé—´ç”Ÿæˆçš„æ‰€æœ‰æ”¹è¿›æ¡ˆä¾‹ï¼Œâ€œè‡³å°‘è¦†ç›–äº†ä¸€ä¸ªé¢å¤–çš„æœ‰æ•ˆè¾¹ç•Œæƒ…å†µï¼Œä¾‹å¦‚æå‰è¿”å›å’Œ/æˆ–ç‰¹æ®Šå€¼ï¼ˆå¦‚nullå’Œç©ºåˆ—è¡¨ï¼‰çš„ç‰¹æ®Šå¤„ç†ã€‚â€
- en: '**TestGen-LLM is a good example of how LLMs can be used to improve dev productivity
    and software reliability in a time-efficient manner.** (Note: many of these are
    my own personal opinions that Iâ€™ve taken away from the paper.)'
  id: totrans-split-29
  prefs: []
  type: TYPE_NORMAL
  zh: '**TestGen-LLMæ˜¯LLMså¦‚ä½•åœ¨æ—¶é—´é«˜æ•ˆçš„æƒ…å†µä¸‹ç”¨äºæé«˜å¼€å‘æ•ˆç‡å’Œè½¯ä»¶å¯é æ€§çš„è‰¯å¥½ç¤ºä¾‹ã€‚**ï¼ˆæ³¨ï¼šè¿™äº›å¤§éƒ¨åˆ†æ˜¯æˆ‘ä»è®ºæ–‡ä¸­å¾—å‡ºçš„ä¸ªäººè§‚ç‚¹ã€‚ï¼‰'
- en: Small context windows and scattered dependencies make LLMs nearly unusable for
    non-boilerplate solutions in large codebases. Aside from any privacy concerns,
    itâ€™s not feasible to paste in multiple files of code into an LLM when there could
    be 20+ dependencies from across a codebase in a C++ header file (as an example).
    Even if you do paste in multiple files, there is a time and cognitive cost to
    actually using and trying the code outputted by an LLM in a chat window or even
    in the code editor by GitHub Copilot.
  id: totrans-split-30
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨å¤§å‹ä»£ç åº“ä¸­ï¼Œå°çš„ä¸Šä¸‹æ–‡çª—å£å’Œåˆ†æ•£çš„ä¾èµ–ä½¿LLMså‡ ä¹æ— æ³•ç”¨äºéæ ·æ¿è§£å†³æ–¹æ¡ˆã€‚é™¤äº†éšç§é—®é¢˜å¤–ï¼Œå°†å¤šä¸ªä»£ç æ–‡ä»¶ç²˜è´´åˆ°LLMä¸­å¯èƒ½ä¸åˆ‡å®é™…ï¼Œä¾‹å¦‚C++å¤´æ–‡ä»¶ä¸­å¯èƒ½æœ‰20ä¸ªä»¥ä¸Šçš„ä¾èµ–å…³ç³»ã€‚å³ä½¿ç²˜è´´äº†å¤šä¸ªæ–‡ä»¶ï¼Œä½¿ç”¨LLMåœ¨èŠå¤©çª—å£æˆ–GitHub
    Copilotä»£ç ç¼–è¾‘å™¨ä¸­è¾“å‡ºçš„ä»£ç ä¹Ÿéœ€è¦æ—¶é—´å’Œè®¤çŸ¥æˆæœ¬ã€‚
- en: The price of extra cognitive load cannot be understated. [Hacker News commenters
    find the inaccuracies of GPT-based tooling exhausting and unreliable.](https://news.ycombinator.com/item?id=39460788)
    This is where the **verification of outputs being both valid and non-regressive
    is extremely important.** Each test by TestGen-LLM requires a human sign-off anyways,
    so any programmatic guarantees are useful here.
  id: totrans-split-31
  prefs: []
  type: TYPE_NORMAL
  zh: é¢å¤–çš„è®¤çŸ¥è´Ÿè·çš„ä»£ä»·ä¸å®¹å°è§‘ã€‚[Hacker Newsçš„è¯„è®ºè€…å‘ç°åŸºäºGPTçš„å·¥å…·çš„ä¸å‡†ç¡®æ€§ä»¤äººç­‹ç–²åŠ›å°½å’Œä¸å¯é ã€‚](https://news.ycombinator.com/item?id=39460788)
    è¿™æ­£æ˜¯**éªŒè¯è¾“å‡ºæ˜¯å¦æ—¢æœ‰æ•ˆåˆä¸å›é€€éå¸¸é‡è¦çš„åœ°æ–¹**ã€‚TestGen-LLMçš„æ¯ä¸ªæµ‹è¯•éƒ½éœ€è¦äººç±»ç­¾ç½²æ‰¹å‡†ï¼Œæ‰€ä»¥è¿™é‡Œçš„ä»»ä½•ç¨‹åºåŒ–ä¿è¯éƒ½æ˜¯æœ‰ç”¨çš„ã€‚
- en: This means that for a long-term productivity boost in large codebases, **improvements
    will probably come in incremental, specialized use cases**, like test generation
    and [automatic suggestions during code reviews](https://blog.research.google/2023/05/resolving-code-review-comments-with-ml.html).
    These are also low risk ways to save cumulative developer time. Basically, â€œGPT
    wrappersâ€ will continue to be useful ğŸ™‚.
  id: totrans-split-32
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹äºå¤§å‹ä»£ç åº“çš„é•¿æœŸç”Ÿäº§åŠ›æå‡æ¥è¯´ï¼Œ**æ”¹è¿›å¯èƒ½ä¼šé€æ­¥å‡ºç°åœ¨å¢é‡ã€ä¸“ä¸šåŒ–çš„ç”¨ä¾‹ä¸Š**ï¼Œæ¯”å¦‚æµ‹è¯•ç”Ÿæˆå’Œ[ä»£ç å®¡æŸ¥æœŸé—´çš„è‡ªåŠ¨å»ºè®®](https://blog.research.google/2023/05/resolving-code-review-comments-with-ml.html)ã€‚è¿™äº›ä¹Ÿæ˜¯èŠ‚çœç´¯ç§¯å¼€å‘æ—¶é—´çš„ä½é£é™©æ–¹å¼ã€‚åŸºæœ¬ä¸Šï¼Œâ€œGPTåŒ…è£…â€å°†ç»§ç»­å‘æŒ¥ä½œç”¨
    ğŸ™‚ã€‚
- en: '**The real value of LLMs here are displayed through the edge cases**. The paradox
    of writing good code is that [nobody ever gets credit for fixing problems that
    never happened](https://web.mit.edu/nelsonr/www/Repenning=Sterman_CMR_su01_.pdf).'
  id: totrans-split-33
  prefs: []
  type: TYPE_NORMAL
  zh: '**LLMçš„çœŸæ­£ä»·å€¼åœ¨äºæ˜¾ç¤ºå‡ºè¾¹ç¼˜æƒ…å†µ**ã€‚å†™å¥½ä»£ç çš„æ‚–è®ºåœ¨äºï¼Œ[ä»æœªå‘ç”Ÿè¿‡çš„é—®é¢˜ä¿®å¤å¾€å¾€ä¸ä¼šå¾—åˆ°ä»»ä½•è®¤å¯](https://web.mit.edu/nelsonr/www/Repenning=Sterman_CMR_su01_.pdf)ã€‚'
- en: '[Will Wilson writes](https://antithesis.com/blog/is_something_bugging_you/):'
  id: totrans-split-34
  prefs: []
  type: TYPE_NORMAL
  zh: '[å¨å°”Â·å¨å°”é€Šå†™é“](https://antithesis.com/blog/is_something_bugging_you/)ï¼š'
- en: '*â€œThe fundamental problem of software testingâ€¦ is that software has to handle
    many situations that the developer has never thought of or will never anticipate.
    This limits the value of testing, because if you had the foresight to write a
    test for a particular case, then you probably had the foresight to make the code
    handle that case too. This makes conventional testing great for catching regressions,
    but really terrible at catching all the â€œunknown unknownsâ€ that life, the universe,
    and your endlessly creative users will throw at you.â€*'
  id: totrans-split-35
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*â€œè½¯ä»¶æµ‹è¯•çš„æ ¹æœ¬é—®é¢˜åœ¨äºâ€¦â€¦è½¯ä»¶å¿…é¡»å¤„ç†å¼€å‘è€…ä»æœªè€ƒè™‘è¿‡æˆ–æ°¸è¿œæ— æ³•é¢„æ–™çš„è®¸å¤šæƒ…å†µã€‚è¿™é™åˆ¶äº†æµ‹è¯•çš„ä»·å€¼ï¼Œå› ä¸ºå¦‚æœä½ æœ‰é¢„è§æ€§åœ°ä¸ºç‰¹å®šæƒ…å†µç¼–å†™æµ‹è¯•ï¼Œé‚£ä¹ˆä½ å¯èƒ½ä¹Ÿæœ‰é¢„è§æ€§åœ°è®©ä»£ç å¤„ç†è¯¥æƒ…å†µã€‚è¿™ä½¿å¾—ä¼ ç»Ÿæµ‹è¯•åœ¨æ•æ‰å›å½’æ–¹é¢éå¸¸å‡ºè‰²ï¼Œä½†åœ¨æ•æ‰ç”Ÿæ´»ã€å®‡å®™å’Œä½ çš„æ— ç©·åˆ›æ„ç”¨æˆ·å°†å‘ä½ æŠ•æ·çš„æ‰€æœ‰â€œæœªçŸ¥æœªçŸ¥â€æ–¹é¢ç¡®å®éå¸¸ç³Ÿç³•ã€‚â€*'
- en: '**LLMs canâ€™t really â€œthink outside the boxâ€ because they only really know their
    training data. However, their box can be larger than a humanâ€™s, so they have the
    potential to think about cases that a human might miss.**'
  id: totrans-split-36
  prefs: []
  type: TYPE_NORMAL
  zh: '**LLMå®é™…ä¸Šæ— æ³•â€œè¶…å‡ºç›’å­â€çš„æƒ³æ³•ï¼Œå› ä¸ºå®ƒä»¬åªçœŸæ­£äº†è§£å®ƒä»¬çš„è®­ç»ƒæ•°æ®ã€‚ç„¶è€Œï¼Œå®ƒä»¬çš„ç›’å­å¯èƒ½æ¯”äººç±»çš„å¤§ï¼Œå› æ­¤å®ƒä»¬æœ‰æ½œåŠ›è€ƒè™‘åˆ°äººç±»å¯èƒ½ä¼šå¿½ç•¥çš„æƒ…å†µã€‚**'
- en: 'Most of the test cases created by Metaâ€™s TestGen-LLM only covered an extra
    2.5 lines. However, one test case covered 1326 lines! There are two ways to think
    about this case:'
  id: totrans-split-37
  prefs: []
  type: TYPE_NORMAL
  zh: Metaçš„TestGen-LLMåˆ›å»ºçš„å¤§å¤šæ•°æµ‹è¯•æ¡ˆä¾‹ä»…è¦†ç›–äº†é¢å¤–çš„2.5è¡Œã€‚ç„¶è€Œï¼Œä¸€ä¸ªæµ‹è¯•æ¡ˆä¾‹è¦†ç›–äº†1326è¡Œï¼æœ‰ä¸¤ç§æ–¹å¼æ¥æ€è€ƒè¿™ä¸ªæ¡ˆä¾‹ï¼š
- en: The optimistic case is that the LLM possibly caught an important edge case that
    was missed by humans when testing.
  id: totrans-split-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä¹è§‚çš„æƒ…å†µæ˜¯ï¼ŒLLMå¯èƒ½æ•æ‰åˆ°äº†åœ¨æµ‹è¯•ä¸­è¢«äººç±»å¿½ç•¥çš„é‡è¦è¾¹ç¼˜æƒ…å†µã€‚
- en: The more realistic case is that the LLM caught a codepath that was just missed
    altogether, rather than catching an edge case.
  id: totrans-split-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æ›´ç°å®çš„æƒ…å†µæ˜¯LLMæ•æ‰åˆ°äº†ä¸€ä¸ªå®Œå…¨è¢«å¿½ç•¥çš„ä»£ç è·¯å¾„ï¼Œè€Œä¸æ˜¯æ•æ‰åˆ°äº†ä¸€ä¸ªè¾¹ç¼˜æƒ…å†µã€‚
- en: '**More code coverage != better code**. We donâ€™t want to use lines covered as
    a chaseable metric, though it would be any bad engineering managerâ€™s dream. More
    code coverage is gameable, but the downsides of rote coverage created by LLMs
    is that bugs are also codified, which means they can go unnoticed until they appear
    in production.'
  id: totrans-split-40
  prefs: []
  type: TYPE_NORMAL
  zh: '**æ›´å¤šçš„ä»£ç è¦†ç›–ç‡å¹¶ä¸ç­‰äºæ›´å¥½çš„ä»£ç **ã€‚æˆ‘ä»¬ä¸å¸Œæœ›å°†è¦†ç›–çš„ä»£ç è¡Œæ•°ä½œä¸ºå¯ä»¥è¿½æ±‚çš„æŒ‡æ ‡ï¼Œå°½ç®¡è¿™å¯èƒ½æ˜¯ä»»ä½•ç³Ÿç³•çš„å·¥ç¨‹ç»ç†çš„æ¢¦æƒ³ã€‚æ›´å¤šçš„ä»£ç è¦†ç›–ç‡æ˜¯å¯ä»¥æ“çºµçš„ï¼Œä½†ç”±LLMç”Ÿæˆçš„æœºæ¢°è¦†ç›–çš„ç¼ºç‚¹æ˜¯ï¼Œç¼ºé™·ä¹Ÿè¢«ç¼–ç äº†ï¼Œè¿™æ„å‘³ç€å®ƒä»¬åœ¨ç”Ÿäº§ä¸­å¯èƒ½è¢«å¿½ç•¥ï¼Œç›´åˆ°å‡ºç°é—®é¢˜ã€‚'
- en: The value here is mostly bringing edge cases that lie outside of the developerâ€™s
    immediate context, but still within the LLMâ€™s training data, to the developerâ€™s
    attention.
  id: totrans-split-41
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™é‡Œçš„ä»·å€¼ä¸»è¦åœ¨äºå°†å¼€å‘è€…ä¸å¤ªå…³æ³¨ä½†ä»åœ¨LLMåŸ¹è®­æ•°æ®èŒƒå›´å†…çš„è¾¹ç¼˜æƒ…å†µå¸¦åˆ°å¼€å‘è€…çš„æ³¨æ„åŠ›ä¸­ã€‚
- en: The Blue Box signifies the space where LLMs can come in handy.
  id: totrans-split-42
  prefs: []
  type: TYPE_NORMAL
  zh: è“è‰²æ¡†è¡¨ç¤ºLLMåœ¨è¿™é‡Œå¯èƒ½ä¼šæœ‰ç”¨çš„ç©ºé—´ã€‚
- en: Like I mentioned earlier, a good way to think about this LLM is that it's a
    junior dev with the task of creating more comprehensive tests for existing code.
    **LLMs may not generate perfect code on the first try, but it at least provides
    options that otherwise may not have been thought of.** Other devs have more important
    things to work on, so this LLM gets the fun task of improving unit tests. The
    tests that it creates in its pull requests are often good, and sometimes trivial
    or pointless. Occasionally, a test it produces is really good or uncovers a bug
    inadvertently.
  id: totrans-split-43
  prefs: []
  type: TYPE_NORMAL
  zh: æ­£å¦‚æˆ‘ä¹‹å‰æåˆ°çš„ï¼Œç†è§£è¿™ä¸ªLLMçš„ä¸€ä¸ªå¥½æ–¹æ³•æ˜¯æŠŠå®ƒçœ‹ä½œæ˜¯ä¸€ä¸ªåˆçº§å¼€å‘äººå‘˜ï¼Œè´Ÿè´£ä¸ºç°æœ‰ä»£ç åˆ›å»ºæ›´å…¨é¢çš„æµ‹è¯•ã€‚**LLMå¯èƒ½ä¸ä¼šåœ¨ç¬¬ä¸€æ¬¡å°è¯•æ—¶ç”Ÿæˆå®Œç¾çš„ä»£ç ï¼Œä½†è‡³å°‘æä¾›äº†ä¸€äº›å¦åˆ™å¯èƒ½æ²¡æœ‰è¢«è€ƒè™‘åˆ°çš„é€‰é¡¹ã€‚**å…¶ä»–å¼€å‘äººå‘˜æœ‰æ›´é‡è¦çš„äº‹æƒ…è¦åšï¼Œæ‰€ä»¥è¿™ä¸ªLLMå¾—åˆ°äº†æ”¹è¿›å•å…ƒæµ‹è¯•çš„æœ‰è¶£ä»»åŠ¡ã€‚å®ƒåœ¨æ‹‰å–è¯·æ±‚ä¸­åˆ›å»ºçš„æµ‹è¯•é€šå¸¸å¾ˆå¥½ï¼Œæœ‰æ—¶æ˜¯çç¢æˆ–æ¯«æ— æ„ä¹‰çš„ã€‚å¶å°”ï¼Œå®ƒç”Ÿæˆçš„æµ‹è¯•éå¸¸å¥½ï¼Œæˆ–è€…æ— æ„ä¸­å‘ç°äº†ä¸€ä¸ªbugã€‚
- en: In fact, itâ€™s so high that the creator of FoundationDBâ€™s startup, [Antithesis](https://antithesis.com/),
    is entirely based on the fact that software testing edge cases are best found
    by continuously searching software for problems. [For reference, FoundationDB
    was acquired by Apple and is the basis for Apple iCloudâ€™s billions of databases.](https://read.engineerscodex.com/p/how-apple-built-icloud-to-store-billions)
  id: totrans-split-44
  prefs: []
  type: TYPE_NORMAL
  zh: å®é™…ä¸Šï¼Œè¿™ä¸€é«˜åº¦æ˜¯å¦‚æ­¤ä¹‹é«˜ï¼Œä»¥è‡³äºFoundationDBåˆ›å§‹äººçš„åˆ›ä¸šå…¬å¸[Antithesis](https://antithesis.com/)çš„åˆ›é€ è€…ï¼Œå®Œå…¨åŸºäºè½¯ä»¶æµ‹è¯•è¾¹ç¼˜æƒ…å†µæœ€å¥½é€šè¿‡ä¸æ–­æœç´¢è½¯ä»¶æ¥å‘ç°é—®é¢˜ã€‚[ä½œä¸ºå‚è€ƒï¼ŒFoundationDBè¢«è‹¹æœæ”¶è´­ï¼Œå¹¶æˆä¸ºè‹¹æœiCloudæ•°åäº¿æ•°æ®åº“çš„åŸºç¡€ã€‚](https://read.engineerscodex.com/p/how-apple-built-icloud-to-store-billions)
- en: Base model LLMs arenâ€™t â€œplug-n-play" and shouldnâ€™t reasonably ever be expected
    to. Sure, they might output pristine React and Tailwind CSS code, but thatâ€™s a
    narrow use case in most production codebases. They need a fair amount of processing
    and filtering for code generation tasks that require correctness. Part of this
    processing means grounding LLMs with examples. Google and Meta both make **suggestions
    based on existing code**, where the results are much, much better than raw generation.LLMs
    used in production should take ideas from how Meta processes and filters LLM outputs,
    and most outputs should be expected to be discarded.
  id: totrans-split-45
  prefs: []
  type: TYPE_NORMAL
  zh: åŸºç¡€æ¨¡å‹LLMå¹¶éâ€œå³æ’å³ç”¨â€ï¼Œåˆç†åœ°ä¸åº”æœŸæœ›å¦‚æ­¤ã€‚å½“ç„¶ï¼Œå®ƒä»¬å¯èƒ½ä¼šè¾“å‡ºå®Œç¾çš„Reactå’ŒTailwind CSSä»£ç ï¼Œä½†åœ¨å¤§å¤šæ•°ç”Ÿäº§ä»£ç åº“ä¸­ï¼Œè¿™åªæ˜¯ä¸€ä¸ªç‹­çª„çš„ç”¨ä¾‹ã€‚å®ƒä»¬éœ€è¦å¤§é‡çš„å¤„ç†å’Œè¿‡æ»¤ï¼Œä»¥å¤„ç†éœ€è¦æ­£ç¡®æ€§çš„ä»£ç ç”Ÿæˆä»»åŠ¡ã€‚è¿™ç§å¤„ç†çš„ä¸€éƒ¨åˆ†æ„å‘³ç€ç”¨ä¾‹åŒ–LLMä¸ç¤ºä¾‹ã€‚è°·æ­Œå’ŒMetaéƒ½åŸºäºç°æœ‰ä»£ç æå‡º**åŸºäºç°æœ‰ä»£ç çš„å»ºè®®**ï¼Œåœ¨è¿™äº›æƒ…å†µä¸‹ï¼Œç»“æœæ¯”åŸå§‹ç”Ÿæˆè¦å¥½å¾—å¤šã€‚åœ¨ç”Ÿäº§ä¸­ä½¿ç”¨çš„LLMåº”è¯¥å€Ÿé‰´Metaå¤„ç†å’Œè¿‡æ»¤LLMè¾“å‡ºçš„æ–¹æ³•ï¼Œå¹¶ä¸”å¤§å¤šæ•°è¾“å‡ºéƒ½åº”è¯¥é¢„æœŸè¢«ä¸¢å¼ƒã€‚
- en: LLMs do work best integrated into workflows. This is a reason why GitHub Copilot
    is so popular and another reason why Googleâ€™s Workspace integrations are a great
    idea. Asking a chatbot works great for certain use cases, like boilerplate code
    generation, but chatbots can often fail at more complex use cases.
  id: totrans-split-46
  prefs: []
  type: TYPE_NORMAL
  zh: LLMæœ€æœ‰æ•ˆçš„å·¥ä½œæ–¹å¼æ˜¯é›†æˆåˆ°å·¥ä½œæµç¨‹ä¸­ã€‚è¿™ä¹Ÿæ˜¯ä¸ºä»€ä¹ˆGitHub Copilotå¦‚æ­¤å—æ¬¢è¿ï¼Œä»¥åŠGoogle Workspaceé›†æˆçš„å¦ä¸€ä¸ªåŸå› ã€‚åƒèŠå¤©æœºå™¨äººè¯¢é—®ä¸€æ ·ï¼Œå¯¹äºæŸäº›ç”¨ä¾‹ï¼Œæ¯”å¦‚ç”Ÿæˆæ ·æ¿ä»£ç ï¼Œæ•ˆæœå¾ˆå¥½ï¼Œä½†åœ¨æ›´å¤æ‚çš„ç”¨ä¾‹ä¸­ï¼ŒèŠå¤©æœºå™¨äººç»å¸¸ä¼šå¤±è´¥ã€‚
- en: 'TestGen-LLM applies a series of semantic filters to candidate solutions generated
    by Metaâ€™s internal LLMs, making sure that only the most valuable tests are preserved.
    Hereâ€™s how it works:'
  id: totrans-split-47
  prefs: []
  type: TYPE_NORMAL
  zh: TestGen-LLMå¯¹Metaå†…éƒ¨LLMç”Ÿæˆçš„å€™é€‰è§£å†³æ–¹æ¡ˆåº”ç”¨ä¸€ç³»åˆ—è¯­ä¹‰ç­›é€‰å™¨ï¼Œç¡®ä¿åªä¿ç•™æœ€æœ‰ä»·å€¼çš„æµ‹è¯•ã€‚ä»¥ä¸‹æ˜¯å®ƒçš„å·¥ä½œåŸç†ï¼š
- en: '**Filter 1: Buildability:** Initially, TestGen-LLM checks if the generated
    code can be built within the app''s existing infrastructure. Any code that fails
    to build is immediately discarded.'
  id: totrans-split-48
  prefs: []
  type: TYPE_NORMAL
  zh: '**ç­›é€‰å™¨1ï¼šå¯æ„å»ºæ€§ï¼š** æœ€åˆï¼ŒTestGen-LLMæ£€æŸ¥ç”Ÿæˆçš„ä»£ç æ˜¯å¦å¯ä»¥åœ¨åº”ç”¨ç¨‹åºçš„ç°æœ‰åŸºç¡€è®¾æ–½ä¸­æ„å»ºã€‚ä»»ä½•ä¸èƒ½æ„å»ºçš„ä»£ç éƒ½ä¼šç«‹å³è¢«ä¸¢å¼ƒã€‚'
- en: '**Filter 2: Execution (does the test pass?):** Next, the system runs the tests
    that passed the buildability filter. Any test that doesn''t pass is discarded.
    This step is crucial because, without a way to automatically determine the validity
    of a failing test (whether it''s due to a bug or an incorrect assertion), TestGen-LLM
    opts to keep only those tests that can be used for regression testing (aka making
    sure they can protect current code against future regressions).'
  id: totrans-split-49
  prefs: []
  type: TYPE_NORMAL
  zh: '**ç­›é€‰å™¨2ï¼šæ‰§è¡Œï¼ˆæµ‹è¯•æ˜¯å¦é€šè¿‡ï¼Ÿï¼‰ï¼š** æ¥ä¸‹æ¥ï¼Œç³»ç»Ÿè¿è¡Œé€šè¿‡äº†æ„å»ºæ€§ç­›é€‰å™¨çš„æµ‹è¯•ã€‚ä»»ä½•ä¸èƒ½é€šè¿‡çš„æµ‹è¯•éƒ½ä¼šè¢«ä¸¢å¼ƒã€‚è¿™ä¸€æ­¥éª¤éå¸¸å…³é”®ï¼Œå› ä¸ºå¦‚æœæ²¡æœ‰ä¸€ç§è‡ªåŠ¨ç¡®å®šå¤±è´¥æµ‹è¯•çš„æœ‰æ•ˆæ€§çš„æ–¹æ³•ï¼ˆæ— è®ºæ˜¯ç”±äºbugè¿˜æ˜¯ä¸æ­£ç¡®çš„æ–­è¨€ï¼‰ï¼ŒTestGen-LLMé€‰æ‹©ä¿ç•™é‚£äº›å¯ä»¥ç”¨äºå›å½’æµ‹è¯•çš„æµ‹è¯•ï¼ˆå³ç¡®ä¿å®ƒä»¬å¯ä»¥ä¿æŠ¤å½“å‰ä»£ç å…å—æœªæ¥å›å½’çš„å½±å“ï¼‰ã€‚'
- en: '**Filter 3: Flakiness:** To address the issue of [flakiness](https://www.swequiz.com/learn/what-are-flaky-tests)
    (tests that pass or fail inconsistently under the same conditions), TestGen-LLM
    employs repeated execution. A test must pass consistently across multiple (five)
    executions to be considered non-flaky.'
  id: totrans-split-50
  prefs: []
  type: TYPE_NORMAL
  zh: '**è¿‡æ»¤å™¨ 3ï¼šæ˜“ç¢æ€§ï¼š** ä¸ºäº†è§£å†³[æ˜“ç¢æ€§](https://www.swequiz.com/learn/what-are-flaky-tests)ï¼ˆåœ¨ç›¸åŒæ¡ä»¶ä¸‹æµ‹è¯•ç»“æœä¸ä¸€è‡´çš„é—®é¢˜ï¼‰ï¼ŒTestGen-LLMé‡‡ç”¨é‡å¤æ‰§è¡Œã€‚æµ‹è¯•å¿…é¡»åœ¨å¤šæ¬¡ï¼ˆäº”æ¬¡ï¼‰æ‰§è¡Œä¸­å§‹ç»ˆé€šè¿‡æ‰èƒ½è¢«è§†ä¸ºéæ˜“ç¢ã€‚'
- en: '**Filter 3: Coverage Improvement:** Finally, to ensure that new tests actually
    add value, TestGen-LLM evaluates them for their contribution to test coverage.
    Tests that do not enhance coverage by exploring new code paths or conditions are
    discarded. Only tests that provide new insights or protect against regressions
    are kept.'
  id: totrans-split-51
  prefs: []
  type: TYPE_NORMAL
  zh: '**è¿‡æ»¤å™¨ 3ï¼šè¦†ç›–ç‡æ”¹è¿›ï¼š** æœ€åï¼Œä¸ºäº†ç¡®ä¿æ–°æµ‹è¯•å®é™…ä¸Šå¢åŠ äº†ä»·å€¼ï¼ŒTestGen-LLMè¯„ä¼°å®ƒä»¬å¯¹æµ‹è¯•è¦†ç›–ç‡çš„è´¡çŒ®ã€‚ä¸å¢åŠ é€šè¿‡æ¢ç´¢æ–°ä»£ç è·¯å¾„æˆ–æ¡ä»¶æ¥å¢å¼ºè¦†ç›–ç‡çš„æµ‹è¯•å°†è¢«ä¸¢å¼ƒã€‚åªæœ‰æä¾›æ–°è§è§£æˆ–é˜²æ­¢å›å½’çš„æµ‹è¯•æ‰ä¼šè¢«ä¿ç•™ã€‚'
- en: These processing filters are pretty important as they guarantee improvements
    to a test suite. It also shows that LLMs are very far from being â€œplug-and-play.â€
  id: totrans-split-52
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™äº›å¤„ç†è¿‡æ»¤å™¨éå¸¸é‡è¦ï¼Œå› ä¸ºå®ƒä»¬ä¿è¯äº†å¯¹æµ‹è¯•å¥—ä»¶çš„æ”¹è¿›ã€‚è¿™ä¹Ÿè¡¨æ˜LLMè¿œéâ€œå³æ’å³ç”¨â€ã€‚
- en: The tests that successfully pass through all these filters are guaranteed to
    enhance the existing test suite, offering reliable regression testing capabilities
    without duplicating effort or wasting resources. Pre- and post-processing steps
    in TestGen-LLM facilitate the extraction and reconstruction of test classes, streamlining
    the integration of new tests into the software development workflow.
  id: totrans-split-53
  prefs: []
  type: TYPE_NORMAL
  zh: é€šè¿‡æ‰€æœ‰è¿™äº›è¿‡æ»¤å™¨æˆåŠŸé€šè¿‡çš„æµ‹è¯•å¯ä»¥ä¿è¯å¢å¼ºç°æœ‰çš„æµ‹è¯•å¥—ä»¶ï¼Œæä¾›å¯é çš„å›å½’æµ‹è¯•èƒ½åŠ›ï¼Œè€Œä¸ä¼šé‡å¤åŠªåŠ›æˆ–æµªè´¹èµ„æºã€‚åœ¨TestGen-LLMä¸­ï¼Œæµ‹è¯•å‰åå¤„ç†æ­¥éª¤æœ‰åŠ©äºæå–å’Œé‡å»ºæµ‹è¯•ç±»ï¼Œç®€åŒ–æ–°æµ‹è¯•é›†æˆåˆ°è½¯ä»¶å¼€å‘å·¥ä½œæµç¨‹ä¸­ã€‚
- en: This paper is a good formalization of an use case that many devs probably already
    use LLMs like ChatGPT, Gemini, and Mistral/LLaMA for. Keeping it in writing is
    a good way of tracking the progress of future improvements on LLMs in the software
    reliability space. So far, we see that LLMs work best as an extension of the human,
    best thought of as a junior dev that needs assistance and prodding. As time goes
    on, weâ€™ll definitely see LLMs be able to catch and test for bugs in increasingly
    complex software systems.
  id: totrans-split-54
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ç¯‡è®ºæ–‡å¾ˆå¥½åœ°å½¢å¼åŒ–äº†ä¸€ä¸ªä½¿ç”¨æ¡ˆä¾‹ï¼Œè®¸å¤šå¼€å‘äººå‘˜å¯èƒ½å·²ç»åœ¨åƒChatGPTã€Geminiå’ŒMistral/LLaMAè¿™æ ·çš„LLMä¸­ä½¿ç”¨ã€‚å°†å…¶å†™ä¸‹æ¥æ˜¯è¿½è¸ªæœªæ¥åœ¨è½¯ä»¶å¯é æ€§ç©ºé—´ä¸­æ”¹è¿›LLMçš„è¿›å±•çš„å¥½æ–¹æ³•ã€‚åˆ°ç›®å‰ä¸ºæ­¢ï¼Œæˆ‘ä»¬çœ‹åˆ°LLMæœ€é€‚åˆä½œä¸ºäººç±»çš„å»¶ä¼¸ï¼Œæœ€å¥½è¢«è§†ä¸ºéœ€è¦å¸®åŠ©å’Œæ¨åŠ¨çš„åˆçº§å¼€å‘äººå‘˜ã€‚éšç€æ—¶é—´çš„æ¨ç§»ï¼Œæˆ‘ä»¬è‚¯å®šä¼šçœ‹åˆ°LLMèƒ½å¤Ÿåœ¨æ—¥ç›Šå¤æ‚çš„è½¯ä»¶ç³»ç»Ÿä¸­æ•æ‰å’Œæµ‹è¯•é”™è¯¯ã€‚
- en: The question is - will that make software easier to develop in the long run
    or will it lead to a proliferation of software complexity in the future?
  id: totrans-split-55
  prefs: []
  type: TYPE_NORMAL
  zh: é—®é¢˜æ˜¯ - é•¿è¿œæ¥çœ‹ï¼Œè¿™æ˜¯å¦ä¼šä½¿è½¯ä»¶å¼€å‘å˜å¾—æ›´åŠ å®¹æ˜“ï¼Œè¿˜æ˜¯ä¼šå¯¼è‡´æœªæ¥è½¯ä»¶å¤æ‚æ€§çš„è”“å»¶ï¼Ÿ
