- en: <!--yml
  id: totrans-split-0
  prefs: []
  type: TYPE_NORMAL
- en: 'category: Êú™ÂàÜÁ±ª'
  id: totrans-split-1
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-05-29 13:18:46'
  id: totrans-split-2
  prefs: []
  type: TYPE_NORMAL
- en: -->
  id: totrans-split-3
  prefs: []
  type: TYPE_NORMAL
- en: Meta's new LLM-based test generator is a sneak peek to the future of development
  id: totrans-split-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Êù•Ê∫êÔºö[https://read.engineerscodex.com/p/metas-new-llm-based-test-generator](https://read.engineerscodex.com/p/metas-new-llm-based-test-generator)
  id: totrans-split-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '*Engineer‚Äôs Codex is a publication about real-world software engineering.*'
  id: totrans-split-6
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  id: totrans-split-7
  prefs: []
  type: TYPE_NORMAL
- en: Meta recently released a [paper called ‚ÄúAutomated Unit Test Improvement using
    Large Language Models at Meta‚Äù](https://arxiv.org/abs/2402.09171). It‚Äôs a good
    look at how Big Tech is using AI internally to make development faster and software
    less buggy. For example, [Google is using AI to speed up code reviews](https://read.engineerscodex.com/i/139414745/critique-googles-code-review-tool).
  id: totrans-split-8
  prefs: []
  type: TYPE_NORMAL
- en: A major win of this paper is that while it integrates LLMs into a developer‚Äôs
    workflow, it also recommends fully-formed software improvements that are verified
    to be both correct and an improvement to current code coverage. This is **not
    a magic pill, but it‚Äôs a good start on making LLMs more useful**. Compare this
    to ChatGPT, where suggestions still have to be manually verified to work - and
    we all know that [debugging code is twice as hard as writing it](https://read.engineerscodex.com/p/clever-code-is-probably-the-worst).
  id: totrans-split-9
  prefs: []
  type: TYPE_NORMAL
- en: Meta claims that ‚Äúthis is the first paper to report on LLM-generated code that
    has been developed independent of human intervention (other than final review
    sign off), and landed into large scale industrial production systems with guaranteed
    assurances for improvement over the existing code base.‚Äù (Bleh! That was a mouthful.)
  id: totrans-split-10
  prefs: []
  type: TYPE_NORMAL
- en: Furthermore, there are solid principles that developers can take away in order
    to use AI effectively themselves.
  id: totrans-split-11
  prefs: []
  type: TYPE_NORMAL
- en: '**Table of Contents:**'
  id: totrans-split-12
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  id: totrans-split-13
  prefs: []
  type: TYPE_NORMAL
- en: '***[SWE Quiz](http://swequiz.com)** is for ambitious developers who want to
    make sure their software fundamentals are rock solid. Reveal gaps in your knowledge
    and make sure you really know what you‚Äôre doing both at work and while interviewing.*'
  id: totrans-split-14
  prefs: []
  type: TYPE_NORMAL
- en: '*Take the tests for [authentication](https://www.swequiz.com/learn/authentication-roadmap),
    [caching](https://swequiz.com/learn/caching-roadmap), [databases](https://www.swequiz.com/learn/databases-roadmap),
    [API Design](https://www.swequiz.com/learn/api-design-roadmap), and [more](https://www.swequiz.com/learn).*'
  id: totrans-split-15
  prefs: []
  type: TYPE_NORMAL
- en: '[Check out SWE Quiz](https://swequiz.com)'
  id: totrans-split-16
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  id: totrans-split-17
  prefs: []
  type: TYPE_NORMAL
- en: TestGen-LLM uses an approach called ‚ÄòAssured LLM-based Software Engineering‚Äô
    (Assured LLMSE), using private, internal LLMs that are probably fine-tuned with
    Meta‚Äôs codebase. This means that it uses LLMs to generate code improvements that
    are backed by ‚Äú**verifiable guarantees‚Äùof improvement and non-regression.**
  id: totrans-split-18
  prefs: []
  type: TYPE_NORMAL
- en: TestGen-LLM uses an **ensemble approach** to generate code improvements. This
    means that it uses **‚ÄúLLMs, prompts, and hyper-parameters‚Äù** to generate a set
    of candidate improvements, and then selects the best one. This approach can help
    to improve the quality of the generated improvements.
  id: totrans-split-19
  prefs: []
  type: TYPE_NORMAL
- en: 'TestGen-LLM is specifically designed to **improve existing human-written tests**
    rather than **generate code from scratch**.¬†A good way to think about this LLM
    is: it''s a junior dev with the task of creating more comprehensive tests for
    existing code. Other devs have more important things to work on, so this LLM gets
    the fun task of improving unit tests.'
  id: totrans-split-20
  prefs: []
  type: TYPE_NORMAL
- en: The tests that it creates in its pull requests are often good, and sometimes
    trivial or pointless. Occasionally, a test it produces is really good or uncovers
    a bug inadvertently. Regardless, this work wouldn't have been done by humans anyways
    due to priorities. All of its pull requests require a human reviewer before pushed
    into the codebase.
  id: totrans-split-21
  prefs: []
  type: TYPE_NORMAL
- en: TestGen-LLM has been **integrated into Meta's software engineering workflows**.
    This means that it can be used to automatically improve tests as part of the development
    process. It would be cool to see some screenshots of how exactly it‚Äôs integrated,
    but the paper doesn‚Äôt provide any.
  id: totrans-split-22
  prefs: []
  type: TYPE_NORMAL
- en: These stats are either direct quotes or paraphrased quotes from the paper. A
    short reminder that all tests that were landed required a final sign-off by human
    reviewers.
  id: totrans-split-23
  prefs: []
  type: TYPE_NORMAL
- en: 'The image above shows that: in an evaluation on Reels and Stories products
    for Instagram, 75% of TestGen-LLM test cases that were generated built correctly,
    57% passed reliably, and *25% increased coverage*.'
  id: totrans-split-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: TestGen-LLM was able to improve **10% of all classes to which it was applied
    and 73% of its test improvements were accepted by developers**, and landed into
    production.
  id: totrans-split-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In a ‚Äútest-a-thon‚Äù between engineers, where various Meta engineers created tests
    in order to increase Instagram‚Äôs test coverage, ‚Äú**the median number of lines
    of code added by a TestGen-LLM test was 2.5**.‚Äù
  id: totrans-split-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: However, one test case ‚Äúhit the jackpot‚Äù and covered 1,326 lines.
  id: totrans-split-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: All improved cases generated during the ‚Äútest-a-thon‚Äù did ‚Äúcover at least one
    additional valid corner case, such as an early return and/or special processing
    for special values such as null and empty list.‚Äù
  id: totrans-split-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**TestGen-LLM is a good example of how LLMs can be used to improve dev productivity
    and software reliability in a time-efficient manner.** (Note: many of these are
    my own personal opinions that I‚Äôve taken away from the paper.)'
  id: totrans-split-29
  prefs: []
  type: TYPE_NORMAL
- en: Small context windows and scattered dependencies make LLMs nearly unusable for
    non-boilerplate solutions in large codebases. Aside from any privacy concerns,
    it‚Äôs not feasible to paste in multiple files of code into an LLM when there could
    be 20+ dependencies from across a codebase in a C++ header file (as an example).
    Even if you do paste in multiple files, there is a time and cognitive cost to
    actually using and trying the code outputted by an LLM in a chat window or even
    in the code editor by GitHub Copilot.
  id: totrans-split-30
  prefs: []
  type: TYPE_NORMAL
- en: The price of extra cognitive load cannot be understated. [Hacker News commenters
    find the inaccuracies of GPT-based tooling exhausting and unreliable.](https://news.ycombinator.com/item?id=39460788)
    This is where the **verification of outputs being both valid and non-regressive
    is extremely important.** Each test by TestGen-LLM requires a human sign-off anyways,
    so any programmatic guarantees are useful here.
  id: totrans-split-31
  prefs: []
  type: TYPE_NORMAL
- en: This means that for a long-term productivity boost in large codebases, **improvements
    will probably come in incremental, specialized use cases**, like test generation
    and [automatic suggestions during code reviews](https://blog.research.google/2023/05/resolving-code-review-comments-with-ml.html).
    These are also low risk ways to save cumulative developer time. Basically, ‚ÄúGPT
    wrappers‚Äù will continue to be useful üôÇ.
  id: totrans-split-32
  prefs: []
  type: TYPE_NORMAL
- en: '**The real value of LLMs here are displayed through the edge cases**. The paradox
    of writing good code is that [nobody ever gets credit for fixing problems that
    never happened](https://web.mit.edu/nelsonr/www/Repenning=Sterman_CMR_su01_.pdf).'
  id: totrans-split-33
  prefs: []
  type: TYPE_NORMAL
- en: '[Will Wilson writes](https://antithesis.com/blog/is_something_bugging_you/):'
  id: totrans-split-34
  prefs: []
  type: TYPE_NORMAL
- en: '*‚ÄúThe fundamental problem of software testing‚Ä¶ is that software has to handle
    many situations that the developer has never thought of or will never anticipate.
    This limits the value of testing, because if you had the foresight to write a
    test for a particular case, then you probably had the foresight to make the code
    handle that case too. This makes conventional testing great for catching regressions,
    but really terrible at catching all the ‚Äúunknown unknowns‚Äù that life, the universe,
    and your endlessly creative users will throw at you.‚Äù*'
  id: totrans-split-35
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '**LLMs can‚Äôt really ‚Äúthink outside the box‚Äù because they only really know their
    training data. However, their box can be larger than a human‚Äôs, so they have the
    potential to think about cases that a human might miss.**'
  id: totrans-split-36
  prefs: []
  type: TYPE_NORMAL
- en: 'Most of the test cases created by Meta‚Äôs TestGen-LLM only covered an extra
    2.5 lines. However, one test case covered 1326 lines! There are two ways to think
    about this case:'
  id: totrans-split-37
  prefs: []
  type: TYPE_NORMAL
- en: The optimistic case is that the LLM possibly caught an important edge case that
    was missed by humans when testing.
  id: totrans-split-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The more realistic case is that the LLM caught a codepath that was just missed
    altogether, rather than catching an edge case.
  id: totrans-split-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**More code coverage != better code**. We don‚Äôt want to use lines covered as
    a chaseable metric, though it would be any bad engineering manager‚Äôs dream. More
    code coverage is gameable, but the downsides of rote coverage created by LLMs
    is that bugs are also codified, which means they can go unnoticed until they appear
    in production.'
  id: totrans-split-40
  prefs: []
  type: TYPE_NORMAL
- en: The value here is mostly bringing edge cases that lie outside of the developer‚Äôs
    immediate context, but still within the LLM‚Äôs training data, to the developer‚Äôs
    attention.
  id: totrans-split-41
  prefs: []
  type: TYPE_NORMAL
- en: The Blue Box signifies the space where LLMs can come in handy.
  id: totrans-split-42
  prefs: []
  type: TYPE_NORMAL
- en: Like I mentioned earlier, a good way to think about this LLM is that it's a
    junior dev with the task of creating more comprehensive tests for existing code.
    **LLMs may not generate perfect code on the first try, but it at least provides
    options that otherwise may not have been thought of.** Other devs have more important
    things to work on, so this LLM gets the fun task of improving unit tests. The
    tests that it creates in its pull requests are often good, and sometimes trivial
    or pointless. Occasionally, a test it produces is really good or uncovers a bug
    inadvertently.
  id: totrans-split-43
  prefs: []
  type: TYPE_NORMAL
- en: In fact, it‚Äôs so high that the creator of FoundationDB‚Äôs startup, [Antithesis](https://antithesis.com/),
    is entirely based on the fact that software testing edge cases are best found
    by continuously searching software for problems. [For reference, FoundationDB
    was acquired by Apple and is the basis for Apple iCloud‚Äôs billions of databases.](https://read.engineerscodex.com/p/how-apple-built-icloud-to-store-billions)
  id: totrans-split-44
  prefs: []
  type: TYPE_NORMAL
- en: Base model LLMs aren‚Äôt ‚Äúplug-n-play" and shouldn‚Äôt reasonably ever be expected
    to. Sure, they might output pristine React and Tailwind CSS code, but that‚Äôs a
    narrow use case in most production codebases. They need a fair amount of processing
    and filtering for code generation tasks that require correctness. Part of this
    processing means grounding LLMs with examples. Google and Meta both make **suggestions
    based on existing code**, where the results are much, much better than raw generation.LLMs
    used in production should take ideas from how Meta processes and filters LLM outputs,
    and most outputs should be expected to be discarded.
  id: totrans-split-45
  prefs: []
  type: TYPE_NORMAL
- en: LLMs do work best integrated into workflows. This is a reason why GitHub Copilot
    is so popular and another reason why Google‚Äôs Workspace integrations are a great
    idea. Asking a chatbot works great for certain use cases, like boilerplate code
    generation, but chatbots can often fail at more complex use cases.
  id: totrans-split-46
  prefs: []
  type: TYPE_NORMAL
- en: 'TestGen-LLM applies a series of semantic filters to candidate solutions generated
    by Meta‚Äôs internal LLMs, making sure that only the most valuable tests are preserved.
    Here‚Äôs how it works:'
  id: totrans-split-47
  prefs: []
  type: TYPE_NORMAL
- en: '**Filter 1: Buildability:** Initially, TestGen-LLM checks if the generated
    code can be built within the app''s existing infrastructure. Any code that fails
    to build is immediately discarded.'
  id: totrans-split-48
  prefs: []
  type: TYPE_NORMAL
- en: '**Filter 2: Execution (does the test pass?):** Next, the system runs the tests
    that passed the buildability filter. Any test that doesn''t pass is discarded.
    This step is crucial because, without a way to automatically determine the validity
    of a failing test (whether it''s due to a bug or an incorrect assertion), TestGen-LLM
    opts to keep only those tests that can be used for regression testing (aka making
    sure they can protect current code against future regressions).'
  id: totrans-split-49
  prefs: []
  type: TYPE_NORMAL
- en: '**Filter 3: Flakiness:** To address the issue of [flakiness](https://www.swequiz.com/learn/what-are-flaky-tests)
    (tests that pass or fail inconsistently under the same conditions), TestGen-LLM
    employs repeated execution. A test must pass consistently across multiple (five)
    executions to be considered non-flaky.'
  id: totrans-split-50
  prefs: []
  type: TYPE_NORMAL
- en: '**Filter 3: Coverage Improvement:** Finally, to ensure that new tests actually
    add value, TestGen-LLM evaluates them for their contribution to test coverage.
    Tests that do not enhance coverage by exploring new code paths or conditions are
    discarded. Only tests that provide new insights or protect against regressions
    are kept.'
  id: totrans-split-51
  prefs: []
  type: TYPE_NORMAL
- en: These processing filters are pretty important as they guarantee improvements
    to a test suite. It also shows that LLMs are very far from being ‚Äúplug-and-play.‚Äù
  id: totrans-split-52
  prefs: []
  type: TYPE_NORMAL
- en: The tests that successfully pass through all these filters are guaranteed to
    enhance the existing test suite, offering reliable regression testing capabilities
    without duplicating effort or wasting resources. Pre- and post-processing steps
    in TestGen-LLM facilitate the extraction and reconstruction of test classes, streamlining
    the integration of new tests into the software development workflow.
  id: totrans-split-53
  prefs: []
  type: TYPE_NORMAL
- en: This paper is a good formalization of an use case that many devs probably already
    use LLMs like ChatGPT, Gemini, and Mistral/LLaMA for. Keeping it in writing is
    a good way of tracking the progress of future improvements on LLMs in the software
    reliability space. So far, we see that LLMs work best as an extension of the human,
    best thought of as a junior dev that needs assistance and prodding. As time goes
    on, we‚Äôll definitely see LLMs be able to catch and test for bugs in increasingly
    complex software systems.
  id: totrans-split-54
  prefs: []
  type: TYPE_NORMAL
- en: The question is - will that make software easier to develop in the long run
    or will it lead to a proliferation of software complexity in the future?
  id: totrans-split-55
  prefs: []
  type: TYPE_NORMAL
