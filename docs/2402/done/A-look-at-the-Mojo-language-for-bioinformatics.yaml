- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: æœªåˆ†ç±»'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 'category: æœªåˆ†ç±»'
- en: 'date: 2024-05-27 14:46:14'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: æ—¥æœŸï¼š2024å¹´05æœˆ27æ—¥ 14:46:14
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: A look at the Mojo language for bioinformatics
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: äº†è§£Mojoè¯­è¨€åœ¨ç”Ÿç‰©ä¿¡æ¯å­¦ä¸­çš„åº”ç”¨
- en: æ¥æºï¼š[https://viralinstruction.com/posts/mojo/](https://viralinstruction.com/posts/mojo/)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: æ¥æºï¼š[https://viralinstruction.com/posts/mojo/](https://viralinstruction.com/posts/mojo/)
- en: ''
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '*Written 2024-02-09*'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '*å†™äº2024å¹´02æœˆ09æ—¥*'
- en: A few days ago, [a blog post](https://www.modular.com/blog/outperforming-rust-benchmarks-with-mojo)
    was posted on the website of [Modular](https://www.modular.com/blog/outperforming-rust-benchmarks-with-mojo),
    the company behind the new high-performance programming language [Mojo](https://www.modular.com/max/mojo).
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: å‡ å¤©å‰ï¼Œ[Modular](https://www.modular.com/blog/outperforming-rust-benchmarks-with-mojo)çš„ç½‘ç«™ä¸Šå‘å¸ƒäº†[ä¸€ç¯‡åšå®¢æ–‡ç« ](https://www.modular.com/blog/outperforming-rust-benchmarks-with-mojo)ï¼Œè¯¥å…¬å¸æ˜¯æ–°é«˜æ€§èƒ½ç¼–ç¨‹è¯­è¨€[Mojo](https://www.modular.com/max/mojo)èƒŒåçš„å…¬å¸ã€‚
- en: The post made the case for using Mojo in bioinformatics due to Mojo's dual features
    of being high-level language with high performance, and the blog author substantiated
    the case by presenting two benchmarks related to the processing of [FASTQ files](https://en.wikipedia.org/wiki/FASTQ_format),
    showing impressive speed.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: è¯¥å¸–å­ä¸ºåœ¨ç”Ÿç‰©ä¿¡æ¯å­¦ä¸­ä½¿ç”¨Mojoæä¾›äº†æ”¯æŒï¼Œå› ä¸ºMojoå…·æœ‰é«˜æ€§èƒ½çš„åŒé‡ç‰¹æ€§ï¼Œåšå®¢ä½œè€…é€šè¿‡æä¾›ä¸[FASTQæ–‡ä»¶](https://en.wikipedia.org/wiki/FASTQ_format)å¤„ç†ç›¸å…³çš„ä¸¤ä¸ªåŸºå‡†æ¥è¯å®è¿™ä¸€æ¡ˆä¾‹ï¼Œæ˜¾ç¤ºäº†ä»¤äººå°è±¡æ·±åˆ»çš„é€Ÿåº¦ã€‚
- en: As a bioinformatician who is obsessed with high-performance, high-level programming,
    that's right in my wheelhouse! I decided to dig deeper into the benchmark, and
    this post is about what I found out.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: ä½œä¸ºä¸€ä¸ªç—´è¿·äºé«˜æ€§èƒ½ã€é«˜çº§ç¼–ç¨‹çš„ç”Ÿç‰©ä¿¡æ¯å­¦å®¶ï¼Œè¿™æ­£æ˜¯æˆ‘çš„ä¸“é•¿æ‰€åœ¨ï¼æˆ‘å†³å®šæ·±å…¥ç ”ç©¶è¿™ä¸ªåŸºå‡†ï¼Œè¿™ç¯‡æ–‡ç« å°±æ˜¯æˆ‘å‘ç°çš„å†…å®¹ã€‚
- en: ''
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The basic premise of the post is that the field of bioinformatics is struggling
    to handle its increasingly large datasets. These datasets are so large that they
    have to be processed programmatically, but programming is the field is split between
    high-level dynamic languages used to do the actual data analysis, and the high-performance,
    static languages that Python calls into to do the computation underlying the analysis.
    As the post states:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: å¸–å­çš„åŸºæœ¬å‰ææ˜¯ï¼Œç”Ÿç‰©ä¿¡æ¯å­¦é¢†åŸŸæ­£åœ¨åŠªåŠ›å¤„ç†æ—¥ç›Šåºå¤§çš„æ•°æ®é›†ã€‚è¿™äº›æ•°æ®é›†éå¸¸åºå¤§ï¼Œä»¥è‡³äºå¿…é¡»é€šè¿‡ç¼–ç¨‹æ–¹å¼è¿›è¡Œå¤„ç†ï¼Œä½†æ˜¯è¯¥é¢†åŸŸçš„ç¼–ç¨‹åˆ†ä¸ºä¸¤ä¸ªå±‚æ¬¡ï¼šç”¨äºå®é™…æ•°æ®åˆ†æçš„é«˜çº§åŠ¨æ€è¯­è¨€ï¼Œä»¥åŠPythonè°ƒç”¨çš„é«˜æ€§èƒ½ã€é™æ€è¯­è¨€ï¼Œç”¨äºè¿›è¡Œåˆ†æåº•å±‚è®¡ç®—çš„è¯­è¨€ã€‚å¦‚å¸–å­æ‰€è¿°ï¼š
- en: This creates a two-world problem where bioinformaticians who are not skilled
    in low-level languages, are prohibited from understanding, customizing, and implementing
    low-level operations.
  id: totrans-13
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: è¿™é€ æˆäº†ä¸€ä¸ªåŒé‡ä¸–ç•Œçš„é—®é¢˜ï¼Œç”Ÿç‰©ä¿¡æ¯å­¦å®¶ä¸ç†Ÿæ‚‰ä½çº§è¯­è¨€ï¼Œæ— æ³•ç†è§£ã€å®šåˆ¶å’Œå®ç°ä½çº§æ“ä½œã€‚
- en: The blog post goes on to suggest Mojo could bridge the gap between the two worlds,
    by being a "Pythonic", but fast language.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: åšå®¢æ–‡ç« ç»§ç»­æš—ç¤ºMojoå¯ä»¥é€šè¿‡æˆä¸ºä¸€ç§â€œPythonicâ€ï¼Œä½†å¿«é€Ÿçš„è¯­è¨€æ¥å¼¥åˆè¿™ä¸¤ä¸ªä¸–ç•Œä¹‹é—´çš„å·®è·ã€‚
- en: I have a lot more to say on the topic of the two-language problem in bioinformatics,
    so much that I'll reserve it for another blog post. In summary, I wholeheartedly
    agree with that analysis, except that I'd encourage using Julia rather than Mojo.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘å¯¹ç”Ÿç‰©ä¿¡æ¯å­¦ä¸­çš„åŒè¯­é—®é¢˜æœ‰æ›´å¤šçš„çœ‹æ³•ï¼Œå¤ªå¤šäº†ï¼Œæˆ‘ä¼šæŠŠå®ƒä¿ç•™åˆ°å¦ä¸€ç¯‡åšå®¢æ–‡ç« ä¸­ã€‚æ€»ä¹‹ï¼Œæˆ‘å®Œå…¨åŒæ„é‚£ä¸ªåˆ†æï¼Œé™¤äº†æˆ‘ä¼šé¼“åŠ±ä½¿ç”¨Juliaè€Œä¸æ˜¯Mojoã€‚
- en: The post then describes how the author implemented a benchmark in Mojo and managed
    to beat a fairly optimised Rust library. That certainly made me curious, so I
    cloned [the git repo with the Mojo code](https://github.com/MoSafi2/MojoFastTrim)^([[1]](#fndef:1))
    and took a look myself.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: å¸–å­éšåæè¿°äº†ä½œè€…å¦‚ä½•åœ¨Mojoä¸­å®æ–½äº†ä¸€ä¸ªåŸºå‡†ï¼Œå¹¶æˆåŠŸå‡»è´¥äº†ä¸€ä¸ªç›¸å½“ä¼˜åŒ–çš„Ruståº“ã€‚è¿™ç¡®å®è®©æˆ‘æ„Ÿåˆ°å¥½å¥‡ï¼Œæ‰€ä»¥æˆ‘å…‹éš†äº†[å¸¦æœ‰Mojoä»£ç çš„gitå­˜å‚¨åº“](https://github.com/MoSafi2/MojoFastTrim)^([[1]](#fndef:1))å¹¶è‡ªå·±çœ‹äº†ä¸€çœ¼ã€‚
- en: ''
  id: totrans-17
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'After cloning the repo, the first step is to download and install Mojo:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: å…‹éš†å­˜å‚¨åº“åï¼Œç¬¬ä¸€æ­¥æ˜¯ä¸‹è½½å¹¶å®‰è£…Mojoï¼š
- en: '[PRE0]'
  id: totrans-19
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Well, that's disappointing. Apparently, Mojo currently only runs on Ubuntu and
    MacOS, and I run neither. So, I can't *run* any Mojo code, but must rely on just
    *reading* the code. Fortunately, the code is quite simple, and only a few hundred
    lines of code.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: å—¯ï¼Œè¿™ä»¤äººå¤±æœ›ã€‚æ˜¾ç„¶ï¼ŒMojoç›®å‰åªèƒ½åœ¨Ubuntuå’ŒMacOSä¸Šè¿è¡Œï¼Œè€Œæˆ‘éƒ½æ²¡æœ‰ã€‚æ‰€ä»¥ï¼Œæˆ‘ä¸èƒ½*è¿è¡Œ*ä»»ä½•Mojoä»£ç ï¼Œè€Œå¿…é¡»ä»…ä¾é *é˜…è¯»*ä»£ç ã€‚å¹¸è¿çš„æ˜¯ï¼Œä»£ç éå¸¸ç®€å•ï¼Œåªæœ‰å‡ ç™¾è¡Œä»£ç ã€‚
- en: ''
  id: totrans-21
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'At first glance, it''s clear that *Mojo isn''t anything like Python*. Here
    are some things that I found in the few hundred lines of the supposedly Pythonic
    language:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: ä¹ä¸€çœ‹ï¼Œå¾ˆæ˜æ˜¾*Mojoä¸Pythonå®Œå…¨ä¸åŒ*ã€‚ä»¥ä¸‹æ˜¯æˆ‘åœ¨å‡ ç™¾è¡Œæ‰€è°“çš„Pythonicè¯­è¨€ä¸­å‘ç°çš„ä¸€äº›äº‹ç‰©ï¼š
- en: 'Generic functions parameterized by type parameters: `fn foo[x: T](arg: Int)`'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'é€šè¿‡ç±»å‹å‚æ•°å‚æ•°åŒ–çš„é€šç”¨å‡½æ•°ï¼š`fn foo[x: T](arg: Int)`'
- en: 'Speaking of which, two distinct function definitions: `def foo` vs `fn foo`'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: è¯´åˆ°è¿™ä¸€ç‚¹ï¼Œæœ‰ä¸¤ä¸ªä¸åŒçš„å‡½æ•°å®šä¹‰ï¼š`def foo` vs `fn foo`
- en: '...and different kinds of integers, here `Int` as opposed to `int`'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '...è¿˜æœ‰ä¸åŒç±»å‹çš„æ•´æ•°ï¼Œè¿™é‡Œæ˜¯`Int`è€Œä¸æ˜¯`int`'
- en: 'Mutable vs immutable variables, initialized by `var x: T = y` vs `let x: T
    = y`'
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'å¯å˜vsä¸å¯å˜å˜é‡ï¼Œé€šè¿‡`var x: T = y` vs `let x: T = y`è¿›è¡Œåˆå§‹åŒ–'
- en: 'Also, type declarations before assignment, C-style: `let foo: T`'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'æ­¤å¤–ï¼Œç±»å‹å£°æ˜åœ¨èµ‹å€¼ä¹‹å‰ï¼ŒCé£æ ¼ï¼š`let foo: T`'
- en: '*Mandatory* type declarations in type signatures: `fn foo(x: Int)`'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'åœ¨ç±»å‹ç­¾åä¸­çš„*å¼ºåˆ¶*ç±»å‹å£°æ˜ï¼š`fn foo(x: Int)`'
- en: 'Mutability declaration of arguments via the `inout` keyword: `fn foo(inout
    self)`'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: é€šè¿‡`inout`å…³é”®å­—å£°æ˜å‚æ•°çš„å¯å˜æ€§ï¼š`fn foo(inout self)`
- en: The ability of a function to raise errors must be marked with the `raises` keyword
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å‡½æ•°å¼•å‘é”™è¯¯çš„èƒ½åŠ›å¿…é¡»ä½¿ç”¨`raises`å…³é”®å­—æ ‡è®°
- en: Data can be stored in `struct`s as well as `class`es.
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æ•°æ®å¯ä»¥å­˜å‚¨åœ¨`struct`ä¸­ï¼Œä¹Ÿå¯ä»¥å­˜å‚¨åœ¨`class`ä¸­ã€‚
- en: Compiler directives, notably `@always_inline` to control inlining heuristics
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ç¼–è¯‘å™¨æŒ‡ä»¤ï¼Œç‰¹åˆ«æ˜¯`@always_inline`æ¥æ§åˆ¶å†…è”å¯å‘å¼ç®—æ³•
- en: Mojo apparently even implements a [Rust-style borrow checker](https://docs.modular.com/mojo/programming-manual.html),
    though I couldn't tell from just reading the code.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: Mojoæ˜¾ç„¶ç”šè‡³å®ç°äº†ä¸€ç§[Rusté£æ ¼çš„å€Ÿç”¨æ£€æŸ¥å™¨](https://docs.modular.com/mojo/programming-manual.html)ï¼Œå°½ç®¡æˆ‘åªæ˜¯ä»ä»£ç ä¸­é˜…è¯»ï¼Œå¹¶æ— æ³•å¾—çŸ¥ã€‚
- en: Does this strike you as the features of a high-level, dynamic language?
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™è®©ä½ è§‰å¾—è¿™æ˜¯ä¸€ä¸ªé«˜çº§åŠ¨æ€è¯­è¨€çš„ç‰¹å¾å—ï¼Ÿ
- en: On one hand, the presence of low-level features is reassuring. When Mojo was
    initially announced, I didn't quite understand what Mojo was supposed to be. Was
    it supposed to be a faster Python implementation, like PyPy? Or perhaps a compiler
    to optimise selected parts of Python, like Numba? That left a lot of questions
    with me about how they were going to pull that off given that neither PyPy nor
    Numba can reliably produce fast code.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€æ–¹é¢ï¼Œä½çº§ç‰¹æ€§çš„å­˜åœ¨æ˜¯ä»¤äººæ”¾å¿ƒçš„ã€‚å½“Mojoæœ€åˆå®£å¸ƒæ—¶ï¼Œæˆ‘å¹¶ä¸å®Œå…¨ç†è§£Mojoåº”è¯¥æ˜¯ä»€ä¹ˆã€‚å®ƒæ˜¯åº”è¯¥æˆä¸ºä¸€ä¸ªæ›´å¿«çš„Pythonå®ç°ï¼Œå°±åƒPyPyå—ï¼Ÿè¿˜æ˜¯ä¸€ä¸ªç¼–è¯‘å™¨ï¼Œä¼˜åŒ–Pythonçš„éƒ¨åˆ†é€‰å®šéƒ¨åˆ†ï¼Œå°±åƒNumbaï¼Ÿè¿™è®©æˆ‘å¯¹ä»–ä»¬å¦‚ä½•å®Œæˆè¿™é¡¹å·¥ä½œæ„Ÿåˆ°å›°æƒ‘ï¼Œå› ä¸ºPyPyå’ŒNumbaéƒ½ä¸èƒ½å¯é åœ°ç”Ÿæˆå¿«é€Ÿçš„ä»£ç ã€‚
- en: Well, it looks like it's neither - instead, it's a *different, static language*
    that presumably aims to provide excellent interoperation with Python. That's a
    *much* more doable proposal! There is no reason to doubt that a static language
    can reliably generate fast code. And it could still provide great value for Pythonistas
    by essentially being a better version of Cython that they can selectively reach
    for when they have a need for speed. Especially so if Mojo can provide a kind
    of gradual performance where users from a Python background can gradually and
    selectively opt into each of these features as they get more familiar with lower
    level computing.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: å¥½å§ï¼Œçœ‹èµ·æ¥å®ƒæ—¢ä¸æ˜¯ - ç›¸åï¼Œå®ƒæ˜¯ä¸€ä¸ª*ä¸åŒçš„ã€é™æ€çš„è¯­è¨€*ï¼Œæ®æ¨æµ‹æ—¨åœ¨ä¸Pythonæä¾›å“è¶Šçš„äº’æ“ä½œæ€§ã€‚è¿™æ˜¯ä¸€ä¸ª*æ›´å¯è¡Œçš„æè®®*ï¼æ²¡æœ‰ç†ç”±æ€€ç–‘é™æ€è¯­è¨€èƒ½å¤Ÿå¯é åœ°ç”Ÿæˆå¿«é€Ÿä»£ç ã€‚è€Œä¸”å®ƒä»ç„¶å¯ä»¥ä¸ºPythonçˆ±å¥½è€…æä¾›å·¨å¤§çš„ä»·å€¼ï¼Œå› ä¸ºæœ¬è´¨ä¸Šå®ƒæ˜¯ä¸€ä¸ªæ›´å¥½çš„Cythonç‰ˆæœ¬ï¼Œå½“ä»–ä»¬éœ€è¦é€Ÿåº¦æ—¶å¯ä»¥æœ‰é€‰æ‹©åœ°ä½¿ç”¨ã€‚ç‰¹åˆ«æ˜¯å¦‚æœMojoå¯ä»¥æä¾›ä¸€ç§é€æ¸æ€§èƒ½æå‡çš„æ–¹å¼ï¼Œç”¨æˆ·å¯ä»¥ä»PythonèƒŒæ™¯é€æ¸å’Œæœ‰é€‰æ‹©åœ°é€‰æ‹©è¿™äº›ç‰¹æ€§ï¼Œéšç€ä»–ä»¬å¯¹ä½çº§è®¡ç®—çš„äº†è§£è¶Šæ¥è¶Šå¤šã€‚
- en: On the other hand, it's also a much less exciting vision to provide a fast static
    language with good interop, compared to smashing [Ousterhout's dichotomy](https://en.wikipedia.org/wiki/Ousterhout%27s_dichotomy)
    by providing a dynamic language that is also fast. It does make me question the
    use case somewhat. After all, static languages can already interoperate with Python
    relatively easily, e.g. with Rust's crate PyO3\. Presumably, Mojo's interop is
    going to be even easier. But is the improved interop going to outweigh the benefits
    that come from designing a language to be ergonomic on its own terms?
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: å¦ä¸€æ–¹é¢ï¼Œä¸é€šè¿‡æä¾›ä¸€ä¸ªåŒæ—¶åˆå¿«é€Ÿåˆå…·æœ‰è‰¯å¥½äº’æ“ä½œæ€§çš„é™æ€è¯­è¨€æ¥æ‰“ç ´[å¥¥æ–¯ç‰¹è±ªç‰¹çš„äºŒåˆ†æ³•](https://en.wikipedia.org/wiki/Ousterhout%27s_dichotomy)ç›¸æ¯”ï¼Œæä¾›ä¸€ä¸ªä»¤äººå…´å¥‹ç¨‹åº¦è¾ƒä½çš„æ„¿æ™¯ä¹Ÿè®¸å¹¶ä¸é‚£ä¹ˆä»¤äººå…´å¥‹ã€‚è¿™è®©æˆ‘å¯¹ä½¿ç”¨æƒ…å†µäº§ç”Ÿäº†ä¸€äº›è´¨ç–‘ã€‚æ¯•ç«Ÿï¼Œé™æ€è¯­è¨€å·²ç»å¯ä»¥ç›¸å¯¹å®¹æ˜“åœ°ä¸Pythonè¿›è¡Œäº’æ“ä½œï¼Œä¾‹å¦‚ä½¿ç”¨Rustçš„crate
    PyO3ã€‚æ®æ¨æµ‹ï¼ŒMojoçš„äº’æ“ä½œæ€§å°†ä¼šæ›´åŠ å®¹æ˜“ã€‚ä½†æ˜¯ï¼Œæ”¹è¿›çš„äº’æ“ä½œæ€§æ˜¯å¦ä¼šè¶…è¿‡è®¾è®¡ä¸€ç§ç¬¦åˆè‡ªèº«ç‰¹ç‚¹çš„è¯­è¨€æ‰€å¸¦æ¥çš„å¥½å¤„å‘¢ï¼Ÿ
- en: On that point, I don't really buy the idea that Mojo benefits terribly much
    from being "Pythonic" - which presumably means that its syntax is inspired by
    Python. What's the claim here, really? That it'd be *too hard* for people to learn
    the superficial syntax of a new language, while it'd simultaneously be *easy*
    for people to learn about function monomorphization, copy- vs borrow semantics,
    compiler directives and much more?
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™ä¸€ç‚¹ä¸Šï¼Œæˆ‘å¹¶ä¸è®¤åŒMojoä»â€œPythonicâ€ä¸­è·ç›Šå¾ˆå¤šçš„æƒ³æ³•-è¿™å¯èƒ½æ„å‘³ç€å®ƒçš„è¯­æ³•å—åˆ°Pythonçš„å¯å‘ã€‚è¿™é‡ŒçœŸçš„æœ‰ä»€ä¹ˆè¦æ±‚å—ï¼ŸçœŸçš„æœ‰äººè®¤ä¸ºå­¦ä¹ ä¸€ä¸ªæ–°è¯­è¨€çš„è¡¨é¢è¯­æ³•ä¼š*å¤ªéš¾*ï¼Œè€ŒåŒæ—¶åˆè®¤ä¸ºå­¦ä¹ å‡½æ•°å•æ€åŒ–ã€å¤åˆ¶ä¸å€Ÿç”¨è¯­ä¹‰ã€ç¼–è¯‘å™¨æŒ‡ä»¤ç­‰ä¼š*å¾ˆå®¹æ˜“*å—ï¼Ÿ
- en: The main intended application of Mojo appears to be deep learning, which has
    struggled with the same 'two-language problem' as bioinformatics, since models
    are prototyped in Python but all the tensor operations are written in C++ or CUDA.
    It's not clear to me how Mojo is going to change the game there, though. It doesn't
    seem like Mojo can replace a framework like PyTorch, since those are at entirely
    different levels of the stack. Can it integrate into PyTorch, such that tensor
    gradients are preserved across Mojo functions? That would allow users to keep
    using PyTorch while implementing a single custom kernel in Mojo. But it's seems
    unlikely Mojo is compatible with PyTorch's C++ interface. Perhaps Mojo is aimed
    at being a language suitable for developing new, future frameworks from scratch
    when people are ready to ditch the existing Python ecosystem? But if that's the
    goal, you might as well ditch Python entirely and all its 35-year old baggage
    and come to Julia for a clean start.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: Mojoçš„ä¸»è¦åº”ç”¨ä¼¼ä¹æ˜¯æ·±åº¦å­¦ä¹ ï¼Œä¸ç”Ÿç‰©ä¿¡æ¯å­¦ä¸€æ ·ï¼Œæ·±åº¦å­¦ä¹ ä¸€ç›´å—åˆ°åŒæ ·çš„â€œåŒè¯­è¨€é—®é¢˜â€çš„å›°æ‰°ï¼Œå› ä¸ºæ¨¡å‹åœ¨Pythonä¸­è¿›è¡ŒåŸå‹è®¾è®¡ï¼Œä½†æ‰€æœ‰å¼ é‡æ“ä½œéƒ½æ˜¯ç”¨C++æˆ–CUDAç¼–å†™çš„ã€‚æˆ‘ä¸å¤ªæ¸…æ¥šMojoå¦‚ä½•æ”¹å˜æ¸¸æˆè§„åˆ™ã€‚ä¸è¿‡ï¼ŒMojoä¼¼ä¹ä¸èƒ½å–ä»£PyTorchè¿™æ ·çš„æ¡†æ¶ï¼Œå› ä¸ºå®ƒä»¬åœ¨å®Œå…¨ä¸åŒçš„å †æ ˆå±‚æ¬¡ä¸Šã€‚å®ƒèƒ½å¤Ÿé›†æˆåˆ°PyTorchä¸­å—ï¼Œä»¥ä¾¿åœ¨Mojoå‡½æ•°ä¹‹é—´ä¿ç•™å¼ é‡æ¢¯åº¦ï¼Ÿè¿™å°†å…è®¸ç”¨æˆ·ç»§ç»­ä½¿ç”¨PyTorchï¼ŒåŒæ—¶åœ¨Mojoä¸­å®ç°å•ä¸ªè‡ªå®šä¹‰å†…æ ¸ã€‚ä½†Mojoä¼¼ä¹ä¸å¤ªå¯èƒ½ä¸PyTorchçš„C++æ¥å£å…¼å®¹ã€‚ä¹Ÿè®¸Mojoæ—¨åœ¨æˆä¸ºä¸€ç§é€‚åˆä»å¤´å¼€å‘æ–°çš„æœªæ¥æ¡†æ¶çš„è¯­è¨€ï¼Œå½“äººä»¬å‡†å¤‡æ”¾å¼ƒç°æœ‰çš„Pythonç”Ÿæ€ç³»ç»Ÿæ—¶ï¼Ÿä½†å¦‚æœè¿™æ˜¯ç›®æ ‡ï¼Œä½ å¯èƒ½ä¹Ÿå¯ä»¥å®Œå…¨æ”¾å¼ƒPythonåŠå…¶35å¹´çš„å†å²åŒ…è¢±ï¼Œå¹¶è½¬è€Œä½¿ç”¨Juliaæ¥è¿›è¡Œæ¸…æ´çš„å¼€å§‹ã€‚
- en: Let me also say some nice things about Mojo.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä¹Ÿè¯´ä¸€äº›å…³äºMojoçš„å¥½è¯ã€‚
- en: First, judging by the language features I listed above it should be clear that
    Mojo is extremely serious about performance. This is not some PyPy-like attempt
    to speed up vanilla Python *somewhat*, this is an attempt to make a language that
    is *actually fast*. Second, Mojo's built-in SIMD capabilities are enviable. It
    might make a big difference if developers are pushed towards writing SIMD-friendly
    code by default. Also, keep in mind I probably just don't understand the intended
    use case of Mojo. I haven't paid *that* close attention to how Mojo is intended
    to be used, and I probably won't, until I can get my hands on Mojo and run it
    on my own computer.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: é¦–å…ˆï¼Œä»æˆ‘ä¸Šé¢åˆ—å‡ºçš„è¯­è¨€ç‰¹æ€§æ¥åˆ¤æ–­ï¼Œåº”è¯¥æ¸…æ¥šMojoéå¸¸é‡è§†æ€§èƒ½ã€‚è¿™ä¸æ˜¯åƒPyPyé‚£æ ·è¯•å›¾ç¨å¾®åŠ é€ŸåŸå§‹Pythonçš„å°è¯•ï¼Œè¿™æ˜¯è¯•å›¾åˆ›å»ºä¸€ç§*å®é™…ä¸Šå¿«é€Ÿ*çš„è¯­è¨€ã€‚å…¶æ¬¡ï¼ŒMojoå†…ç½®çš„SIMDåŠŸèƒ½ä»¤äººç¾¡æ…•ã€‚å¦‚æœå¼€å‘äººå‘˜é»˜è®¤é‡‡ç”¨ç¼–å†™SIMDå‹å¥½ä»£ç ï¼Œå¯èƒ½ä¼šäº§ç”Ÿå¾ˆå¤§çš„å½±å“ã€‚æ­¤å¤–ï¼Œè¯·è®°ä½æˆ‘å¯èƒ½å¹¶ä¸ç†è§£Mojoçš„é¢„æœŸç”¨ä¾‹ã€‚æˆ‘å¯¹Mojoçš„é¢„æœŸä½¿ç”¨æ–¹å¼å¹¶æ²¡æœ‰ç‰¹åˆ«å…³æ³¨ï¼Œç›´åˆ°æˆ‘èƒ½å¤Ÿäº²è‡ªåœ¨æˆ‘çš„è®¡ç®—æœºä¸Šè¿è¡ŒMojoã€‚
- en: ''
  id: totrans-42
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Before we get back to the benchmark, we need to understand *why* the work done
    in the benchmark is meaningful. Well, we don't *need to*, but I find it interesting
    because it's my field of research, so let's take a detour into DNA sequencing.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨å›åˆ°åŸºå‡†ä¹‹å‰ï¼Œæˆ‘ä»¬éœ€è¦äº†è§£*ä¸ºä»€ä¹ˆ*åœ¨åŸºå‡†æµ‹è¯•ä¸­æ‰€åšçš„å·¥ä½œæ˜¯æœ‰æ„ä¹‰çš„ã€‚å—¯ï¼Œæˆ‘ä»¬ä¸*éœ€è¦*ï¼Œä½†æˆ‘è§‰å¾—è¿™å¾ˆæœ‰è¶£ï¼Œå› ä¸ºè¿™æ˜¯æˆ‘çš„ç ”ç©¶é¢†åŸŸï¼Œå› æ­¤è®©æˆ‘ä»¬åç¦»ä¸€ä¸‹ï¼Œäº†è§£DNAæµ‹åºã€‚
- en: Most people know that biological inheritance is controlled by DNA^([[2]](#fndef:2)).
    DNA is a linear polymer molecule of consisting of *nucleotides* stringed together
    in a chain, with each nucleotide containing one of four distinct *bases* which
    are abbreviated A, C, G or T. A DNA molecule can therefore be faithfully represented
    by a sequence of symbols, e.g. a string such as `TAGGCTATGCC`. Thus, DNA is a
    type of *digital* storage that controls much of how living organisms are built
    and how we behave. Reading the sequence of a physical sample containing DNA molecules
    is called *sequencing*, and is done by machines called *sequencers*.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: å¤§å¤šæ•°äººéƒ½çŸ¥é“ç”Ÿç‰©é—ä¼ æ˜¯ç”±DNAæ¥æ§åˆ¶çš„^([[2]](#fndef:2))ã€‚DNAæ˜¯ä¸€ä¸ªç”±çº¿æ€§èšåˆç‰©åˆ†å­ç»„æˆçš„é“¾çŠ¶åˆ†å­ï¼Œæ¯ä¸ªæ ¸è‹·é…¸é“¾ä¸­åŒ…å«ç€4ä¸ªä¸åŒçš„*ç¢±åŸº*ï¼Œè¿™äº›ç¢±åŸºç®€ç§°ä¸ºAã€Cã€Gå’ŒTã€‚å› æ­¤ï¼ŒDNAåˆ†å­å¯ä»¥è¢«ä¸€ä¸ªç¬¦å·åºåˆ—æ‰€è¡¨ç¤ºï¼Œæ¯”å¦‚ä¸€ä¸ªå­—ç¬¦ä¸²ï¼Œä¾‹å¦‚`TAGGCTATGCC`ã€‚å› æ­¤ï¼ŒDNAæ˜¯ä¸€ç§æ§åˆ¶ç”Ÿç‰©ä½“å¤§éƒ¨åˆ†æ„å»ºæ–¹å¼å’Œè¡Œä¸ºçš„*æ•°å­—*å­˜å‚¨æ–¹å¼ã€‚è¯»å–åŒ…å«DNAåˆ†å­çš„ç‰©ç†æ ·æœ¬çš„åºåˆ—è¢«ç§°ä¸º*æµ‹åº*ï¼Œè¿™æ˜¯ç”±å«åš*æµ‹åºä»ª*çš„æœºå™¨æ¥å®Œæˆçš„ã€‚
- en: 'Incidentally, the applicability of the field of bioinformatics stem from these
    facts: 1\. That much of molecular biology can be explained by the polymer molecule
    DNA (and RNA and protein), 2\. That these polymers are easily and faithfully represented
    in a computer, and 3\. That it''s possible to construct sequencers which can computerize
    massive amounts of these polymers cheaply. Biochemistry on Earth didn''t *have*
    to be this amenable to analysis, and we''re very lucky that it happened to be
    so.'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: é¡ºä¾¿æä¸€ä¸‹ï¼Œç”Ÿç‰©ä¿¡æ¯å­¦çš„åº”ç”¨èƒ½ä»è¿™äº›äº‹å®ä¸­å¾—ä»¥è§£é‡Šï¼š1. åˆ†å­ç”Ÿç‰©å­¦çš„å¾ˆå¤šå†…å®¹å¯ä»¥é€šè¿‡èšåˆç‰©åˆ†å­DNAï¼ˆè¿˜æœ‰RNAå’Œè›‹ç™½è´¨ï¼‰æ¥è§£é‡Šï¼Œ2. è¿™äº›èšåˆç‰©å¯ä»¥åœ¨è®¡ç®—æœºä¸Šå®¹æ˜“è€Œå‡†ç¡®åœ°è¡¨ç¤ºï¼Œ3.
    å¯ä»¥æ„å»ºèƒ½å¤Ÿå»‰ä»·è®¡ç®—å¤§é‡è¿™äº›èšåˆç‰©çš„æµ‹åºå™¨ã€‚åœ°çƒä¸Šçš„ç”Ÿç‰©åŒ–å­¦ä¸ä¸€å®š*è¦*é€‚åˆåˆ†æï¼Œæˆ‘ä»¬å¾ˆå¹¸è¿ï¼Œå®ƒæ°å¥½é€‚åˆåˆ†æã€‚
- en: There are different competing sequencers with different characteristics, but
    let's focus on the machines produced by the company Illumina, which currently
    dominate with around 80% market share. Illumina sequencers uses a chemical reaction
    to read DNA linearly from one end of the molecule. The output of reading one molecule
    of DNA is termed a *read*. Due to imperfections in the chemistry, the chemical
    reaction deteriorates to unreadability after around 150 bases, putting an upper
    limit on read length that is far too low to sequence full DNA molecules, which
    in humans are on the order of 100 million bases (100 Mbp) in length. To overcome
    this limitation, the DNA is broken apart to smaller fragments of around 500 bp,
    e.g. using ultrasound, and tens of millions of these fragments are then sequenced
    in parallel. Because we expect the sample to contain many near-identical DNA molecules
    that are fragmented independently and randomly, we can reconstruct the entire
    original sequence by merging partially overlapping reads, if only we sequence
    sufficiently many reads from each sample to ensure uniform coverage of the original
    sequence.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: æœ‰ä¸åŒçš„ç«äº‰æµ‹åºå™¨ï¼Œå…·æœ‰ä¸åŒçš„ç‰¹å¾ï¼Œä½†è®©æˆ‘ä»¬ç€é‡å…³æ³¨ç›®å‰å¸‚åœºä»½é¢å æ®äº†80%çš„Illuminaå…¬å¸ç”Ÿäº§çš„æœºå™¨ã€‚Illuminaæµ‹åºä»ªä½¿ç”¨åŒ–å­¦ååº”ä»åˆ†å­çš„ä¸€ç«¯çº¿æ€§è¯»å–DNAã€‚è¯»å–ä¸€åˆ†å­DNAçš„è¾“å‡ºè¢«ç§°ä¸º*è¯»å–*ã€‚ç”±äºåŒ–å­¦ååº”çš„ä¸å®Œç¾ï¼Œåœ¨çº¦150ä¸ªç¢±åŸºåï¼ŒåŒ–å­¦ååº”ä¼šå˜å¾—éš¾ä»¥è¯»å–ï¼Œè¿™å°±ç»™è¯»å–é•¿åº¦è®¾å®šäº†ä¸€ä¸ªä¸Šé™ï¼Œè¿™ä¸ªé•¿åº¦è¿œè¿œä¸è¶³ä»¥æµ‹åºå…¨é•¿çº¦1äº¿ç¢±åŸºï¼ˆ100
    Mbpï¼‰çš„DNAåˆ†å­ï¼Œè€Œåœ¨äººç±»ä¸­ï¼ŒDNAåˆ†å­çš„é•¿åº¦å¤§çº¦ä¸º1äº¿ç¢±åŸºã€‚ä¸ºäº†å…‹æœè¿™ä¸ªå±€é™ï¼ŒDNAè¢«åˆ†è§£ä¸ºå¤§çº¦500 bpçš„è¾ƒå°ç‰‡æ®µï¼Œä¾‹å¦‚ä½¿ç”¨è¶…å£°æ³¢ï¼Œç„¶åä»¥å¹¶è¡Œæ–¹å¼å¯¹æ•°åƒä¸‡ä¸ªè¿™äº›ç‰‡æ®µè¿›è¡Œæµ‹åºã€‚å› ä¸ºæˆ‘ä»¬æœŸæœ›æ ·å“ä¸­å«æœ‰è®¸å¤šè¿‘ä¼¼ç›¸åŒçš„DNAåˆ†å­ï¼Œè¿™äº›åˆ†å­æ˜¯ç‹¬ç«‹å’Œéšæœºåœ°æ–­è£‚çš„ï¼Œæˆ‘ä»¬å¯ä»¥é€šè¿‡åˆå¹¶éƒ¨åˆ†é‡å çš„è¯»å–æ¥é‡æ„æ•´ä¸ªåŸå§‹åºåˆ—ï¼Œåªè¦æˆ‘ä»¬å¯¹æ¯ä¸ªæ ·æœ¬æµ‹åºè¶³å¤Ÿå¤šçš„è¯»æ•°ï¼Œä»¥ç¡®ä¿å¯¹åŸå§‹åºåˆ—çš„å‡åŒ€è¦†ç›–ã€‚
- en: The number of reads is typically expressed in *depth of coverage* (or just *depth*),
    which is the average number of times each position in the original DNA molecule
    is present across all sequenced reads. A typical experiment might target ~2 %
    of the human genome's total size of 3 Gbp and aim for a depth of 100x, producing
    around 5 Gbp of data. With a read length of 150 bp, this is around 35 million
    reads.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: é€šå¸¸ï¼Œè¯»å–çš„æ•°é‡ä»¥*è¦†ç›–æ·±åº¦*ï¼ˆæˆ–è€…åªæ˜¯*æ·±åº¦*ï¼‰æ¥è¡¨ç¤ºï¼Œè¿™æ˜¯åŸå§‹DNAåˆ†å­çš„æ¯ä¸ªä½ç½®åœ¨æ‰€æœ‰æµ‹åºè¯»å–ä¸­çš„å¹³å‡å­˜åœ¨æ¬¡æ•°ã€‚ä¸€ä¸ªå…¸å‹çš„å®éªŒå¯èƒ½å®šä½äºäººç±»åŸºå› ç»„æ€»é•¿åº¦çš„çº¦2%ï¼Œç›®æ ‡æ˜¯100xçš„æ·±åº¦ï¼Œäº§ç”Ÿçº¦5
    Gbpçš„æ•°æ®ã€‚ä»¥150 bpçš„è¯»å–é•¿åº¦ï¼Œè¿™å¤§çº¦æ˜¯3500ä¸‡æ¡è¯»å–ã€‚
- en: 'Sequencers typically output the reads in the FASTQ format, which is a simple
    ASCII-encoded format^([[3]](#fndef:3)). One read in FASTQ format looks like this:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: æµ‹åºå™¨é€šå¸¸ä»¥FASTQæ ¼å¼è¾“å‡ºè¯»å–ï¼Œè¿™æ˜¯ä¸€ç§ç®€å•çš„ASCIIç¼–ç æ ¼å¼^([[3]](#fndef:3))ã€‚FASTQæ ¼å¼çš„ä¸€ä¸ªè¯»å–çœ‹èµ·æ¥åƒè¿™æ ·ï¼š
- en: '[PRE1]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'That is:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: å³ï¼š
- en: A read is always composed of four lines.
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä¸€æ¡è¯»å–å§‹ç»ˆç”±å››è¡Œç»„æˆã€‚
- en: The top line starts with `@` and contain a unique identifier of the read. It
    has no other restrictions. In the example read above, the name encodes a bunch
    of metadata about where the read originated.
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: é¡¶è¡Œä»¥`@`å¼€å¤´ï¼ŒåŒ…å«äº†è¯»å–çš„å”¯ä¸€æ ‡è¯†ç¬¦ã€‚å®ƒæ²¡æœ‰å…¶ä»–é™åˆ¶ã€‚åœ¨ä¸Šé¢çš„ä¾‹å­ä¸­ï¼Œæ ‡è¯†ç¬¦ç¼–ç äº†å…³äºè¯»å–æ¥æºçš„å¤§é‡å…ƒæ•°æ®ã€‚
- en: The next line contain the DNA sequence.
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æ¥ä¸‹æ¥çš„è¡ŒåŒ…å«äº†DNAåºåˆ—ã€‚
- en: The third line starts with a `+` and then may optionally repeat the same string
    as after the `@` on the first line
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ç¬¬ä¸‰è¡Œä»¥`+`å¼€å¤´ï¼Œéšåå¯èƒ½å¯é€‰åœ°é‡å¤ç¬¬ä¸€è¡Œä¸­`@`åé¢çš„ç›¸åŒå­—ç¬¦ä¸²
- en: 'The fourth header line contains the quality. This line must be the same length
    as the DNA sequence. It gives the estimated probability that the given DNA nucleotide
    is wrong. There are different encoding schemes, but by far the most common is
    Phred+33, where the error probability is:'
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ç¬¬å››è¡Œæ ‡é¢˜åŒ…å«è´¨é‡ä¿¡æ¯ã€‚è¿™ä¸€è¡Œå¿…é¡»å’ŒDNAåºåˆ—çš„é•¿åº¦ç›¸åŒã€‚å®ƒç»™å‡ºäº†ç»™å®šçš„DNAæ ¸è‹·é…¸æ˜¯é”™è¯¯çš„ä¼°è®¡æ¦‚ç‡ã€‚æœ‰ä¸åŒçš„ç¼–ç æ–¹æ¡ˆï¼Œä½†æœ€å¸¸è§çš„æ˜¯Phred+33ï¼Œå…¶ä¸­é”™è¯¯æ¦‚ç‡ä¸ºï¼š
- en: <math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>p</mi><mo>=</mo><mn>1</mn><msup><mn>0</mn><mfrac><mrow><mn>33</mn><mo>âˆ’</mo><mi>c</mi></mrow><mn>10</mn></mfrac></msup></mrow><annotation
    encoding="application/x-tex">p = 10^\frac{33 - c}{10}</annotation></semantics></math>p=101033âˆ’câ€‹
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: <math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>p</mi><mo>=</mo><mn>1</mn><msup><mn>0</mn><mfrac><mrow><mn>33</mn><mo>âˆ’</mo><mi>c</mi></mrow><mn>10</mn></mfrac></msup></mrow><annotation
    encoding="application/x-tex">p = 10^\frac{33 - c}{10}</annotation></semantics></math>p=101033âˆ’câ€‹
- en: Where <math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>c</mi></mrow><annotation
    encoding="application/x-tex">c</annotation></semantics></math>c is the ASCII value
    of the symbol in the quality line.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: å…¶ä¸­<math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>c</mi></mrow><annotation
    encoding="application/x-tex">c</annotation></semantics></math>cæ˜¯è´¨é‡è¡Œä¸­ç¬¦å·çš„ASCIIå€¼ã€‚
- en: A FASTQ file is then simply the concatenation of multiple reads like the one
    above. Since a research project may contain terabytes of FASTQ files, having a
    fast parser is important.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: FASTQæ–‡ä»¶å°±æ˜¯ç®€å•åœ°è¿æ¥ä¸Šé¢æ‰€ç¤ºçš„å¤šä¸ªè¯»å–ã€‚ç”±äºç ”ç©¶é¡¹ç›®å¯èƒ½åŒ…å«TBçº§åˆ«çš„FASTQæ–‡ä»¶ï¼Œå› æ­¤å…·æœ‰å¿«é€Ÿè§£æå™¨éå¸¸é‡è¦ã€‚
- en: ''
  id: totrans-59
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The Mojo blog post implements two benchmarks - I will only focus on one of
    them here. That''s the FASTQ parsing benchmark, which is taken from one of [the
    biofast benchmarks](https://github.com/lh3/biofast). The task is simple: Given
    a 1.4 GB FASTQ file with ~5.5M reads, count the number of reads, number of bases,
    and number of quality characters, by using a parser to loop over the individual
    reads in the file.'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: Mojoçš„åšå®¢æ–‡ç« å®ç°äº†ä¸¤ä¸ªåŸºå‡†æµ‹è¯•-åœ¨è¿™é‡Œæˆ‘åªå…³æ³¨å…¶ä¸­ä¸€ä¸ªã€‚é‚£å°±æ˜¯FASTQè§£æåŸºå‡†æµ‹è¯•ï¼Œå®ƒå–è‡ª[ç”Ÿç‰©å¿«é€ŸåŸºå‡†æµ‹è¯•](https://github.com/lh3/biofast)ä¹‹ä¸€ã€‚ä»»åŠ¡å¾ˆç®€å•ï¼šç»™å®šä¸€ä¸ªåŒ…å«çº¦550ä¸‡ä¸ªè¯»å–çš„1.4GB
    FASTQæ–‡ä»¶ï¼Œé€šè¿‡ä½¿ç”¨è§£æå™¨å¾ªç¯éå†æ–‡ä»¶ä¸­çš„å„ä¸ªè¯»å–ï¼Œç»Ÿè®¡è¯»å–æ•°ã€åŸºæ•°å’Œè´¨é‡å­—ç¬¦æ•°ã€‚
- en: Currently, the Needletail parser, written in Rust, tops the benchmark. On my
    four year old laptop, it rips through the file in 458 ms, about 3.05 GB/s. In
    comparison, my own `FASTX.jl` parser written in Julia is under half the speed,
    taking 986 ms (1.42 GB/s). I'll get back to discrepancy later.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: ç›®å‰ï¼Œç”¨Rustç¼–å†™çš„Needletailè§£æå™¨åœ¨åŸºå‡†æµ‹è¯•ä¸­ååˆ—å‰èŒ…ã€‚åœ¨æˆ‘å››å¹´å‰çš„ç¬”è®°æœ¬ä¸Šï¼Œå®ƒä»¥458æ¯«ç§’çš„é€Ÿåº¦å¿«é€Ÿè§£ææ–‡ä»¶ï¼Œçº¦ä¸º3.05 GB/sã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œæˆ‘è‡ªå·±ä½¿ç”¨Juliaç¼–å†™çš„`FASTX.jl`è§£æå™¨é€Ÿåº¦åªæœ‰å®ƒçš„ä¸€åŠï¼Œè€—æ—¶986æ¯«ç§’ï¼ˆ1.42
    GB/sï¼‰ã€‚æˆ‘ç¨åå†æ¥è°ˆè°ˆè¿™ä¸ªå·®å¼‚ã€‚
- en: ''
  id: totrans-62
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Since I can't time the Mojo implementation myself, I'll use the numbers from
    their git repo. It lists Needletail taking 0.27s versus 0.21s for Mojo on a more
    modern, faster machine than my own. If we assume Mojo ran with the same relative
    speed versus Rust on my machine, it'd clock in at 356 ms (3.92 GB/s).
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: ç”±äºæˆ‘æ— æ³•è‡ªå·±æµ‹è¯•Mojoçš„å®ç°ï¼Œæˆ‘å°†ä½¿ç”¨å®ƒä»¬gitåº“ä¸­çš„æ•°å­—ã€‚å®ƒåˆ—å‡ºNeedletailä»æ›´ç°ä»£ã€æ›´å¿«çš„æœºå™¨ä¸Šè·å–çš„0.27ç§’çš„é€Ÿåº¦ï¼Œè€ŒMojoè·å¾—äº†0.21ç§’ã€‚å¦‚æœå‡è®¾Mojoåœ¨æˆ‘çš„æœºå™¨ä¸Šä¸Rustæœ‰ç›¸åŒçš„ç›¸å¯¹é€Ÿåº¦ï¼Œå®ƒçš„é€Ÿåº¦å°†ä¸º356æ¯«ç§’ï¼ˆ3.92
    GB/sï¼‰ã€‚
- en: Nearly four GB/s is crazy fast. How does it do it? Let's dive into the Mojo
    code.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: å‡ ä¹å››GB/sçš„é€Ÿåº¦çœŸæ˜¯å¤ªå¿«äº†ã€‚å®ƒæ˜¯å¦‚ä½•åšåˆ°çš„å‘¢ï¼Ÿè®©æˆ‘ä»¬æ¥æ·±å…¥ç ”ç©¶Mojoçš„ä»£ç ã€‚
- en: 'The `main()` function is defined as:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '`main()`å‡½æ•°çš„å®šä¹‰å¦‚ä¸‹ï¼š'
- en: '[PRE2]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Where nearly all the work happens in `FastParser.parse_all`. That is defined
    as
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: å‡ ä¹æ‰€æœ‰å·¥ä½œéƒ½å‘ç”Ÿåœ¨`FastParser.parse_all`ä¸­ã€‚å®ƒçš„å®šä¹‰æ˜¯
- en: '[PRE3]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: The `fill_buffer` function seeks to the right location in the underlying file,
    then fills the internal buffer of `FastParser`. Either that or `self.check_EOF`
    can raise a (non-specific) `Error` on EOF, which breaks the loop in `parse_all`.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '`fill_buffer`å‡½æ•°å¯»æ‰¾åº•å±‚æ–‡ä»¶çš„æ­£ç¡®ä½ç½®ï¼Œç„¶åå¡«å……`FastParser`çš„å†…éƒ¨ç¼“å†²åŒºã€‚`fill_buffer`æˆ–è€…`self.check_EOF`åœ¨EOFæ—¶å¯èƒ½ä¼šå¼•å‘ï¼ˆéç‰¹å®šçš„ï¼‰`Error`ï¼Œè¿™ä¼šä¸­æ–­`parse_all`çš„å¾ªç¯ã€‚'
- en: I'm not crazy about the mandatory seeking of `fill_buffer`. This happens if
    there are extra unused bytes in the buffer. Instead of copying them to the beginning
    of the buffer, the reader rewinds the underlying stream and simply re-reads the
    bytes from the stream - but what if the parser wraps a non-seekable stream? In
    any case that's not important - it could probably be solved with almost no performance
    cost.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘å¯¹`fill_buffer`çš„å¼ºåˆ¶å¯»æ‰¾å¹¶ä¸æ»¡æ„ã€‚å¦‚æœç¼“å†²åŒºä¸­æœ‰é¢å¤–çš„æœªä½¿ç”¨å­—èŠ‚ï¼Œå°±ä¼šå‘ç”Ÿè¿™ç§æƒ…å†µã€‚è§£å†³æ–¹æ¡ˆåº”è¯¥æ˜¯å°†å®ƒä»¬å¤åˆ¶åˆ°ç¼“å†²åŒºçš„å¼€å¤´ï¼Œè€Œä¸æ˜¯å›ç»•åº•å±‚æµï¼Œç„¶åä»æµä¸­é‡æ–°è¯»å–å­—èŠ‚
    - ä½†å¦‚æœè§£æå™¨åŒ…è£…äº†ä¸€ä¸ªä¸å¯å¯»å€çš„æµä¼šæ€ä¹ˆæ ·ï¼Ÿåœ¨ä»»ä½•æƒ…å†µä¸‹ï¼Œè¿™éƒ½ä¸é‡è¦-å‡ ä¹ä¸ä¼šå¸¦æ¥æ€§èƒ½æˆæœ¬ã€‚
- en: 'The function `parse_chunk` parses all the reads in the current buffer. Its
    definition is:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: å‡½æ•°`parse_chunk`è§£æå½“å‰ç¼“å†²åŒºä¸­çš„æ‰€æœ‰è¯»å–ã€‚å®ƒçš„å®šä¹‰æ˜¯ï¼š
- en: '[PRE4]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Here, the `.tally(read)` method increments the read number and number of bases
    seen. The cost of that is insignificant. The exception is expected to be raised
    when the buffer reaches the end, such that the remaining part of the buffer only
    contains a partial read.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™é‡Œï¼Œ`.tally(read)`æ–¹æ³•é€’å¢è¯»å–ç¼–å·å’Œå·²æŸ¥çœ‹çš„åŸºæ•°ã€‚è¿™ä¸ªæˆæœ¬æ˜¯å¾®ä¸è¶³é“çš„ã€‚å¯ä»¥é¢„æ–™åˆ°ï¼Œå½“ç¼“å†²åŒºè¾¾åˆ°æœ«å°¾æ—¶ï¼Œä¼šå¼•å‘å¼‚å¸¸ï¼Œä½¿å¾—ç¼“å†²åŒºçš„å‰©ä½™éƒ¨åˆ†åªåŒ…å«éƒ¨åˆ†è¯»å–ã€‚
- en: Also note the lack of any kind of error handling here. No matter why `parse_chunk`
    throws, it's caught in `parse_all` and terminates the reading without propagating
    the error or examining what kind of error it is. The same issue is repeated further
    down the call chain.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: è¿˜è¦æ³¨æ„è¿™é‡Œå®Œå…¨ç¼ºå°‘ä»»ä½•å½¢å¼çš„é”™è¯¯å¤„ç†ã€‚æ— è®º`parse_chunk`å‡ºç°é—®é¢˜çš„åŸå› æ˜¯ä»€ä¹ˆï¼Œ`parse_all`éƒ½ä¼šæ•æ‰åˆ°å¹¶ç»ˆæ­¢è¯»å–ï¼Œè€Œä¸ä¼šä¼ æ’­é”™è¯¯æˆ–æ£€æŸ¥é”™è¯¯çš„ç±»å‹ã€‚è¿™ä¸ªé—®é¢˜åœ¨è°ƒç”¨é“¾çš„æ›´æ·±å¤„ä¹Ÿæ˜¯å¦‚æ­¤ã€‚
- en: 'Most work here happens in `parse_read` where the real parsing happens:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: å¤§éƒ¨åˆ†å·¥ä½œéƒ½å‘ç”Ÿåœ¨`parse_read`ä¸­ï¼ŒçœŸæ­£çš„è§£æå°±åœ¨è¿™é‡Œå‘ç”Ÿï¼š
- en: '[PRE5]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: That's the secret sauce, really. Each read is parsed by scanning four times
    to the next newline, then emitting the positions of the newlines with zero validation
    of any kind.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™å°±æ˜¯ç§˜å¯†æ­¦å™¨ã€‚æ¯ä¸ªè¯»å–éƒ½é€šè¿‡æ‰«æå››æ¬¡åˆ°ä¸‹ä¸€ä¸ªæ¢è¡Œç¬¦è¿›è¡Œè§£æï¼Œç„¶åå‘å‡ºæ¢è¡Œç¬¦çš„ä½ç½®ï¼Œè€Œéè¿›è¡Œä»»ä½•éªŒè¯ã€‚
- en: 'I''m sorry but this is not a serious parser. To be fair, the repo is pretty
    clear that:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: æŠ±æ­‰ï¼Œè¿™ä¸æ˜¯ä¸€ä¸ªä¸¥è‚ƒçš„è§£æå™¨ã€‚å¼€å±±å¼ï¼Œè¯¥ä»£ç åº“éå¸¸æ˜ç¡®åœ°æŒ‡å‡ºï¼š
- en: 'Disclaimer: MojoFastTrimğŸ”¥ is for demonstration purposes only and shouldn''t
    be used as part of bioinformatic pipelines'
  id: totrans-79
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: å…è´£å£°æ˜ï¼šMojoFastTrimğŸ”¥ä»…ç”¨äºæ¼”ç¤ºç›®çš„ï¼Œä¸åº”ä½œä¸ºç”Ÿç‰©ä¿¡æ¯å­¦ç®¡é“çš„ä¸€éƒ¨åˆ†ä½¿ç”¨ã€‚
- en: However, in my opinion, this lack of validation (really, lack of *parsing* in
    any meaningful sense) means that the performance between this parser and Needletail
    is incomparable. So what exactly does it demonstrate? You get to claim your implementation
    is faster than someone else if you do the same task in less time, but not if you
    skip half the job.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶è€Œï¼Œåœ¨æˆ‘çœ‹æ¥ï¼Œè¿™ç§ç¼ºä¹éªŒè¯ï¼ˆå®é™…ä¸Šï¼Œç¼ºä¹æœ‰æ„ä¹‰çš„ *è§£æ*ï¼‰æ„å‘³ç€è¿™ä¸ªè§£æå™¨å’Œ Needletail ä¹‹é—´çš„æ€§èƒ½æ˜¯æ— æ³•æ¯”è¾ƒçš„ã€‚é‚£åˆ°åº•å±•ç¤ºäº†ä»€ä¹ˆï¼Ÿå¦‚æœä½ åœ¨æ›´çŸ­çš„æ—¶é—´å†…å®Œæˆäº†ç›¸åŒçš„ä»»åŠ¡ï¼Œä½ å°±å¯ä»¥å£°ç§°ä½ çš„å®ç°æ¯”åˆ«äººæ›´å¿«ï¼Œä½†å¦‚æœä½ è·³è¿‡äº†ä¸€åŠçš„å·¥ä½œï¼Œå°±ä¸è¡Œã€‚
- en: 'Anyway, `get_next_line_index` is kind of neat. First, it statically checks
    if SIMD is enabled. If so, it calls `find_chr_next_occurance_simd`, which is essentially
    an implementation of [`memchr`](https://man7.org/linux/man-pages/man3/memchr.3.html).
    It''s implemented as:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: æ— è®ºå¦‚ä½•ï¼Œ`get_next_line_index` è¿˜æ˜¯æŒºä¸é”™çš„ã€‚é¦–å…ˆï¼Œå®ƒé™æ€åœ°æ£€æŸ¥ SIMD æ˜¯å¦å¯ç”¨ã€‚å¦‚æœæ˜¯çš„è¯ï¼Œå®ƒå°±è°ƒç”¨ `find_chr_next_occurance_simd`ï¼Œè¿™å®è´¨ä¸Šæ˜¯
    [`memchr`](https://man7.org/linux/man-pages/man3/memchr.3.html) çš„å®ç°ã€‚å®ƒçš„å®ç°å¦‚ä¸‹ï¼š
- en: '[PRE6]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Here we see Mojo's nice SIMD abstractions. First it uses `math.align_down` to
    get the last index from which it's safe to load a SIMD vector. The `simd_width`
    is automatically computed as `simdwidthof[DType.int8]()` and is presumably a compile
    time constant, so I assume its value is constant folded. I think that's pretty
    cool.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™é‡Œæˆ‘ä»¬çœ‹åˆ°äº† Mojo ä¸é”™çš„ SIMD æŠ½è±¡ã€‚é¦–å…ˆå®ƒä½¿ç”¨ `math.align_down` è·å–æœ€åä¸€ä¸ªç´¢å¼•ï¼Œä»è¿™ä¸ªç´¢å¼•å¼€å§‹åŠ è½½ SIMD å‘é‡æ˜¯å®‰å…¨çš„ã€‚`simd_width`
    è‡ªåŠ¨è®¡ç®—ä¸º `simdwidthof[DType.int8]()`ï¼Œåº”è¯¥æ˜¯ä¸€ä¸ªç¼–è¯‘æ—¶å¸¸é‡ï¼Œæ‰€ä»¥æˆ‘å‡è®¾å®ƒçš„å€¼æ˜¯è¢«æŠ˜å çš„ã€‚æˆ‘è§‰å¾—è¿™å¾ˆé…·ã€‚
- en: In the first loop, each vector (`mask`) is then loaded and compared to the byte
    `chr`. If any of the bytes are true, then `arg_true` is called (which loops over
    the vector to find the first true). I'm guessing this loops compiles effectively
    to a `vmovdqu` load instruction, and the reduction can be expressed as `vpcmpeqb`
    (compare vector to byte), `vpmovmskb` (extract upper bits of each byte in vector
    to a 32-bit integer), and then a comparison to zero.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ç¬¬ä¸€ä¸ªå¾ªç¯ä¸­ï¼Œæ¯ä¸ªå‘é‡ï¼ˆ`mask`ï¼‰éƒ½è¢«åŠ è½½å¹¶ä¸å­—èŠ‚ `chr` è¿›è¡Œæ¯”è¾ƒã€‚å¦‚æœä»»ä½•ä¸€ä¸ªå­—èŠ‚ä¸º trueï¼Œåˆ™è°ƒç”¨ `arg_true`ï¼ˆå®ƒå¾ªç¯éå†å‘é‡ä»¥æ‰¾åˆ°ç¬¬ä¸€ä¸ª
    trueï¼‰ã€‚æˆ‘çŒœè¿™ä¸ªå¾ªç¯æœ‰æ•ˆåœ°ç¼–è¯‘æˆäº†ä¸€ä¸ª `vmovdqu` åŠ è½½æŒ‡ä»¤ï¼Œå¹¶ä¸”çº¦ç®€å¯ä»¥è¡¨è¾¾ä¸º `vpcmpeqb`ï¼ˆå‘é‡ä¸å­—èŠ‚æ¯”è¾ƒï¼‰ã€`vpmovmskb`ï¼ˆå°†å‘é‡ä¸­çš„æ¯ä¸ªå­—èŠ‚çš„é«˜ä½æå–ä¸ºä¸€ä¸ª
    32 ä½æ•´æ•°ï¼‰ï¼Œç„¶åä¸é›¶æ¯”è¾ƒã€‚
- en: The function `arg_true` could be compiled to a single `tzcnt` instruction, but
    when I tried to emulate it in Julia I couldn't get the compiler to realise that,
    though that may just be Julia not having the right SIMD abstractions.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: å‡½æ•° `arg_true` å¯ä»¥ç¼–è¯‘æˆå•ä¸ª `tzcnt` æŒ‡ä»¤ï¼Œä½†å½“æˆ‘å°è¯•åœ¨ Julia ä¸­æ¨¡æ‹Ÿå®ƒæ—¶ï¼Œæˆ‘æ— æ³•è®©ç¼–è¯‘å™¨æ„è¯†åˆ°è¿™ä¸€ç‚¹ï¼Œå°½ç®¡è¿™å¯èƒ½åªæ˜¯ Julia
    æ²¡æœ‰æ­£ç¡®çš„ SIMD æŠ½è±¡ã€‚
- en: Finally, the last elements of the chunk which can't be safely SIMD loaded are
    handled in the last loop. This is much less frequently hit, around once every
    250 reads.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: æœ€åï¼Œä¸èƒ½å®‰å…¨ SIMD åŠ è½½çš„å—çš„æœ€åå‡ ä¸ªå…ƒç´ åœ¨æœ€åä¸€ä¸ªå¾ªç¯ä¸­å¤„ç†ã€‚è¿™ç§æƒ…å†µå‘ç”Ÿçš„é¢‘ç‡è¦å°‘å¾—å¤šï¼Œå¤§çº¦æ¯ 250 æ¬¡è¯»å–ä¸€æ¬¡ã€‚
- en: ''
  id: totrans-87
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The Mojo parser does zero validation and will even accept random bytes as input,
    which I think everyone can agree is not acceptable for real-life situations. But
    how much validation *should* a parser do? That's honestly a hard question to answer,
    and the performance you can expect from parsers hinges on the answer to that question.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: Mojo è§£æå™¨ä¸è¿›è¡Œä»»ä½•éªŒè¯ï¼Œç”šè‡³æ¥å—éšæœºå­—èŠ‚ä½œä¸ºè¾“å…¥ï¼Œæˆ‘è®¤ä¸ºè¿™å¯¹äºçœŸå®åœºæ™¯æ˜¯ä¸å¯æ¥å—çš„ã€‚ä½†è§£æå™¨åº”è¯¥åšå¤šå°‘éªŒè¯å‘¢ï¼Ÿè¿™å®åœ¨æ˜¯ä¸€ä¸ªéš¾ä»¥å›ç­”çš„é—®é¢˜ï¼Œè§£æå™¨çš„æ€§èƒ½å–å†³äºè¿™ä¸ªé—®é¢˜çš„ç­”æ¡ˆã€‚
- en: 'Let''s return to comparing Needletail and the FASTQ parser I maintain, FASTX.jl.
    Needletail uses a quite similar algorithm to the Mojo parser: It uses a buffered
    reader and memchr''s to find newlines before returning a record containing a view
    directly into the file buffer. However, it also handles `\r\n` newlines, and validates
    that the first and third lines begin with `@` and `+`, respectively, and that
    the seq and qual lines have the same length. That''s certainly more validation
    than the Mojo parser, but is it enough? If the quality line contains pure `\x00`
    bytes, how is this a valid FASTQ file? What if the header is `"@\r\v\r"`?'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬å›åˆ°æ¯”è¾ƒ Needletail å’Œæˆ‘ç»´æŠ¤çš„ FASTQ è§£æå™¨ FASTX.jlã€‚Needletail ä½¿ç”¨äº†ä¸ Mojo è§£æå™¨éå¸¸ç›¸ä¼¼çš„ç®—æ³•ï¼šå®ƒä½¿ç”¨äº†ä¸€ä¸ªç¼“å†²è¯»å–å™¨å’Œ
    memchr æ¥æ‰¾åˆ°æ¢è¡Œç¬¦ï¼Œç„¶åè¿”å›ä¸€ä¸ªåŒ…å«ç›´æ¥æŸ¥çœ‹æ–‡ä»¶ç¼“å†²åŒºçš„è®°å½•ã€‚ç„¶è€Œï¼Œå®ƒè¿˜å¤„ç†äº† `\r\n` æ¢è¡Œç¬¦ï¼Œå¹¶éªŒè¯ç¬¬ä¸€è¡Œå’Œç¬¬ä¸‰è¡Œåˆ†åˆ«ä»¥ `@` å’Œ `+`
    å¼€å¤´ï¼Œå¹¶ä¸”åºåˆ—å’Œè´¨é‡è¡Œçš„é•¿åº¦ç›¸åŒã€‚è¿™è‚¯å®šæ¯” Mojo è§£æå™¨å¤šäº†ä¸€äº›éªŒè¯ï¼Œä½†æ˜¯å¤Ÿå—ï¼Ÿå¦‚æœè´¨é‡è¡ŒåŒ…å«çº¯ `\x00` å­—èŠ‚ï¼Œè¿™æ€ä¹ˆæ˜¯ä¸€ä¸ªæœ‰æ•ˆçš„ FASTQ æ–‡ä»¶å‘¢ï¼Ÿå¦‚æœæ ‡å¤´æ˜¯
    `"@\r\v\r"` å‘¢ï¼Ÿ
- en: My own parser validates more - that the quality line is a printable ASCII character,
    that the sequence are ASCII letters, and that the second header is identical to
    the first. That's achieved through parsing the file with a state machine, which
    can therefore be much stricter. It's also partially the reason it's slower than
    Needletail^([[4]](#fndef:4)).
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘è‡ªå·±çš„è§£æå™¨éªŒè¯å¾—æ›´å¤š - è´¨é‡è¡Œæ˜¯å¯æ‰“å°çš„ ASCII å­—ç¬¦ï¼Œåºåˆ—æ˜¯ ASCII å­—æ¯ï¼Œå¹¶ä¸”ç¬¬äºŒä¸ªæ ‡å¤´ä¸ç¬¬ä¸€ä¸ªæ ‡å¤´ç›¸åŒã€‚è¿™æ˜¯é€šè¿‡ä½¿ç”¨çŠ¶æ€æœºè§£ææ–‡ä»¶å®ç°çš„ï¼Œå› æ­¤å¯ä»¥æ›´ä¸¥æ ¼ã€‚è¿™ä¹Ÿéƒ¨åˆ†æ˜¯å®ƒæ¯”
    Needletail æ…¢çš„åŸå› ^([[4]](#fndef:4))ã€‚
- en: On one side of the argument, one could say it's nice to provide as much validation
    as possible - suppose someone reads in a FASTQ file with non-ASCII sequences using
    Needletail, and the parser wrongly claims the seq and quality lines have a different
    number of symbols because they are encoded in a different number of bytes. That
    error is no good and will leave the user scratching their heads when they count
    the sequence and quality lengths and verifies that they match. Wouldn't it be
    nicer to instead have the parser check that the input is ASCII?
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: ä»æŸç§è§’åº¦æ¥çœ‹ï¼Œæˆ‘ä»¬å¯ä»¥è¯´å°½å¯èƒ½æä¾›å°½å¯èƒ½å¤šçš„éªŒè¯æ˜¯å¥½çš„ - å‡è®¾æœ‰äººä½¿ç”¨ Needletail è¯»å–å¸¦æœ‰é ASCII åºåˆ—çš„ FASTQ æ–‡ä»¶ï¼Œå¹¶ä¸”è§£æå™¨é”™è¯¯åœ°å£°ç§°åºåˆ—å’Œè´¨é‡è¡Œçš„ç¬¦å·æ•°ä¸åŒï¼Œå› ä¸ºå®ƒä»¬ä½¿ç”¨ä¸åŒæ•°é‡çš„å­—èŠ‚è¿›è¡Œç¼–ç ã€‚é‚£ç§é”™è¯¯æ˜¯ä¸å¥½çš„ï¼Œå½“ç”¨æˆ·æ•°ä¸€æ•°åºåˆ—å’Œè´¨é‡çš„é•¿åº¦å¹¶éªŒè¯å®ƒä»¬åŒ¹é…æ—¶ï¼Œä»–ä»¬ä¼šæƒ³ä¸æ˜ç™½ã€‚ç›¸åï¼Œè®©è§£æå™¨æ£€æŸ¥è¾“å…¥æ˜¯å¦ä¸º
    ASCII ä¸æ˜¯æ›´å¥½å—ï¼Ÿ
- en: Also in that favour - when do we ever need to parse files at 3 GB/s? What could
    we possibly *do* to the files that will be anywhere near that speed? Surely dropping
    to 2 or even 1 GB/s will have essentially no impact on the overall speed of a
    real life analysis.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: è¿˜æœ‰ä¸€ä¸ªæ–¹é¢ - æˆ‘ä»¬ä»€ä¹ˆæ—¶å€™éœ€è¦ä»¥ 3 GB/s çš„é€Ÿåº¦è§£ææ–‡ä»¶ï¼Ÿæˆ‘ä»¬èƒ½å¯¹æ–‡ä»¶åšçš„äº‹æƒ…ä¼šæœ‰å¤šå¿«ï¼Ÿé™åˆ° 2 GB/s ç”šè‡³ 1 GB/s å¯¹å®é™…åˆ†æçš„æ•´ä½“é€Ÿåº¦å‡ ä¹æ²¡æœ‰å½±å“ã€‚
- en: The other side of the argument is that parsers should do as *little* validation
    as possible. For example, my parser spends time checking that the first and third
    headers of FASTQ reads are identical, because the format says so. But what if
    a user has a record where they're not? Does it really help the user to have their
    program crash with otherwise perfectly fine records? After all, a sensible idiom
    of parsing goes "be liberal in what you accept, and conservative in what you send".
    Maybe parsers ought to do as little validation as they can get away with while
    still ensuring they don't give garbage answers.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: å¦ä¸€æ–¹é¢çš„è®ºç‚¹æ˜¯è§£æå™¨åº”å°½é‡å°‘è¿›è¡ŒéªŒè¯ã€‚ä¾‹å¦‚ï¼Œæˆ‘çš„è§£æå™¨èŠ±è´¹æ—¶é—´æ£€æŸ¥ FASTQ è¯»å–çš„ç¬¬ä¸€ä¸ªå’Œç¬¬ä¸‰ä¸ªå¤´æ˜¯å¦ç›¸åŒï¼Œå› ä¸ºæ ¼å¼æ˜¯è¿™æ ·è§„å®šçš„ã€‚ä½†å¦‚æœç”¨æˆ·æœ‰ä¸€æ¡è®°å½•ï¼Œå®ƒä»¬ä¸ç›¸åŒæ€ä¹ˆåŠï¼Ÿå°†ç¨‹åºå´©æºƒå¯¹ç”¨æˆ·çœŸçš„æœ‰å¸®åŠ©å—ï¼Ÿæ¯•ç«Ÿï¼Œè§£æçš„æ˜æ™ºæƒ¯ä¾‹æ˜¯â€œåœ¨æ¥å—æ–¹é¢è¦å®½å®¹ï¼Œåœ¨å‘é€æ–¹é¢è¦ä¿å®ˆâ€ã€‚ä¹Ÿè®¸è§£æå™¨åº”å°½å¯èƒ½å°‘åœ°è¿›è¡ŒéªŒè¯ï¼ŒåŒæ—¶ç¡®ä¿å®ƒä»¬ä¸æä¾›åƒåœ¾ç­”æ¡ˆã€‚
- en: One could also say that during the course of a project, the same file might
    be read tens of times, but it really only needs to be validated once. If the validation
    is a separate step from the parsing, it can be skipped all but the first time
    the file is read. In [a Reddit comment](https://www.reddit.com/r/rust/comments/1al8cuc/comment/kpgjkkd/?utm_source=share&utm_medium=web3x&utm_name=web3xcss&utm_term=1&utm_content=share_button)
    the maintainer of Needletail say they have an external tool to validate FASTQ
    files for this reason. That's also reasonable, but it does strike me as un-Rust
    like to opt-in to validation, especially when the cost is so low - after all,
    my parser still does more than 1 GB/s.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: ä¹Ÿå¯ä»¥è¯´ï¼Œåœ¨é¡¹ç›®è¿›è¡Œè¿‡ç¨‹ä¸­ï¼ŒåŒä¸€ä¸ªæ–‡ä»¶å¯èƒ½ä¼šè¢«è¯»å–æ•°åæ¬¡ï¼Œä½†å®é™…ä¸Šåªéœ€è¦éªŒè¯ä¸€æ¬¡ã€‚å¦‚æœéªŒè¯æ˜¯è§£æçš„ä¸€ä¸ªå•ç‹¬æ­¥éª¤ï¼Œåˆ™é™¤äº†ç¬¬ä¸€æ¬¡è¯»å–æ–‡ä»¶ä¹‹å¤–ï¼Œå®ƒå¯ä»¥è¢«è·³è¿‡ã€‚åœ¨[ä¸€æ¡
    Reddit è¯„è®ºä¸­](https://www.reddit.com/r/rust/comments/1al8cuc/comment/kpgjkkd/?utm_source=share&utm_medium=web3x&utm_name=web3xcss&utm_term=1&utm_content=share_button)
    ï¼ŒNeedletail çš„ç»´æŠ¤è€…è¡¨ç¤ºå‡ºäºè¿™ä¸ªåŸå› ä»–ä»¬æœ‰ä¸€ä¸ªå¤–éƒ¨å·¥å…·æ¥éªŒè¯ FASTQ æ–‡ä»¶ã€‚è¿™ä¹Ÿæ˜¯åˆç†çš„ï¼Œä½†æˆ‘è§‰å¾—é€‰æ‹©éªŒè¯çš„æ–¹å¼æœ‰äº›ä¸åƒ Rust é£æ ¼ï¼Œå°¤å…¶æ˜¯å½“æˆæœ¬å¦‚æ­¤ä¹‹ä½æ—¶
    - æ¯•ç«Ÿï¼Œæˆ‘çš„è§£æå™¨ä»ç„¶æ¯ç§’æ‰§è¡Œè¶…è¿‡ 1 GB çš„æ“ä½œã€‚
- en: ''
  id: totrans-95
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: My claim is that Mojo's high speed in this benchmarks comes from the implementation
    and not from Mojo being particularly fast. To back it up, I [ported the implementation
    to Julia](https://github.com/jakobnissen/MojoFQBenchmark) with all the same lack
    of error handling or validation. It's currently 78 lines of code, but to be fair,
    it does only the absolutely minimal necessary to complete the FASTQ benchmark.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘å£°ç§° Mojo åœ¨è¿™ä¸ªåŸºå‡†æµ‹è¯•ä¸­çš„é«˜é€Ÿåº¦æ¥è‡ªäºå®ç°ï¼Œè€Œä¸æ˜¯ Mojo ç‰¹åˆ«å¿«ã€‚ä¸ºäº†æ”¯æŒè¿™ä¸€ç‚¹ï¼Œæˆ‘å°†å®ç°[ç§»æ¤åˆ°äº† Julia](https://github.com/jakobnissen/MojoFQBenchmark)ï¼Œæ‰€æœ‰çš„é”™è¯¯å¤„ç†æˆ–éªŒè¯éƒ½æ˜¯ç›¸åŒçš„ã€‚ç›®å‰å®ƒåªæœ‰
    78 è¡Œä»£ç ï¼Œä½†å…¬å¹³åœ°è¯´ï¼Œå®ƒåªå®Œæˆäº†å®Œæˆ FASTQ åŸºå‡†æµ‹è¯•æ‰€éœ€çš„ç»å¯¹æœ€ä½é™åº¦ã€‚
- en: It "parses" the file in 200 ms (6.98 GB/s), 78% faster than Mojo's (imputed)
    speed. That's pretty fucking fast. `cat input.fq > /dev/null` takes 122 ms for
    comparison.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: å®ƒåœ¨ 200 msï¼ˆ6.98 GB/sï¼‰å†…â€œè§£æâ€æ–‡ä»¶ï¼Œæ¯” Mojo çš„ï¼ˆæ¨æµ‹ï¼‰é€Ÿåº¦å¿«äº† 78%ã€‚é‚£é€Ÿåº¦ç›¸å½“å¿«ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œ`cat input.fq >
    /dev/null` éœ€è¦ 122 msã€‚
- en: 'I think there is only one real conclusion here:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘è®¤ä¸ºè¿™é‡Œåªæœ‰ä¸€ä¸ªçœŸæ­£çš„ç»“è®ºï¼š
- en: ğŸ”¥ğŸ”¥ğŸ”¥JULIAğŸ”¥ğŸ”¥ğŸ”¥ IS FASTER THAN MOJOğŸ”¥!!!!111
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: ğŸ”¥ğŸ”¥ğŸ”¥JULIAğŸ”¥ğŸ”¥ğŸ”¥ æ¯” MOJO ğŸ”¥ğŸ”¥ å¿«ï¼ï¼ï¼111
- en: Just kidding. I don't know why my implementation is faster - I don't strictly
    *know* that it's even faster since I can't run Mojo on my own machine. Maybe it's
    the fact that my implementation doesn't seek the underlying file, or maybe 200
    ms is fast enough that Python's startup time begin to matter. If I include the
    time for Julia to start up and compile the script, my implementation takes 354
    ms total, on the same level as Mojo's.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: å¼€ç©ç¬‘çš„ã€‚æˆ‘ä¸çŸ¥é“ä¸ºä»€ä¹ˆæˆ‘çš„å®ç°æ›´å¿« - æˆ‘ç”šè‡³ä¸èƒ½ç¡®å®šå®ƒæ˜¯å¦æ›´å¿«ï¼Œå› ä¸ºæˆ‘æ— æ³•åœ¨è‡ªå·±çš„æœºå™¨ä¸Šè¿è¡Œ Mojoã€‚ä¹Ÿè®¸æ˜¯å› ä¸ºæˆ‘çš„å®ç°ä¸ä¼šæŸ¥æ‰¾åº•å±‚æ–‡ä»¶ï¼Œæˆ–è€…ä¹Ÿè®¸æ˜¯å› ä¸º
    200 ms è¶³å¤Ÿå¿«ï¼Œä»¥è‡³äº Python çš„å¯åŠ¨æ—¶é—´å¼€å§‹å˜å¾—é‡è¦ã€‚å¦‚æœå°† Julia å¯åŠ¨å’Œç¼–è¯‘è„šæœ¬çš„æ—¶é—´è®¡ç®—åœ¨å†…ï¼Œæˆ‘çš„å®ç°æ€»å…±éœ€è¦ 354 msï¼Œä¸ Mojo
    çš„æ°´å¹³ç›¸å½“ã€‚
- en: One interesting observation is that replacing the manual `memchr` implementation
    with a call to glibc's `memchr` slows it down by about 25%, despite glibc's `memchr`
    being around 70% faster when used on long haystacks. Julia's ccall has close to
    zero overhead, so I'm not sure what's up with that.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€ä¸ªæœ‰è¶£çš„è§‚å¯Ÿæ˜¯ï¼Œç”¨ glibc çš„ `memchr` æ›¿æ¢æ‰‹åŠ¨å®ç°çš„ `memchr` å®ç°ä¼šä½¿é€Ÿåº¦å‡æ…¢çº¦ 25%ï¼Œå°½ç®¡åœ¨é•¿çš„å­—ç¬¦é›†ä¸­ä½¿ç”¨æ—¶ï¼Œglibc
    çš„ `memchr` é€Ÿåº¦è¦å¿«çº¦ 70%ã€‚Julia çš„ ccall å‡ ä¹æ²¡æœ‰é¢å¤–å¼€é”€ï¼Œæ‰€ä»¥æˆ‘ä¸ç¡®å®šå…·ä½“åŸå› ã€‚
- en: Maybe it's that `memchr` doesn't inline, whereas the manual implementation is
    forcefully inlined into `parse_read`. If so, this might explain most of the performance
    difference to Needletail. Removing the `@inline` directive from my Julia code
    slows it down about 20%. Interestingly, setting `lto = "thin"` and `codegen-units
    = 1` in my Cargo.toml file reduces the runtime of Needletail to 357 ms, matching
    Mojo's imputed runtime nearly exactly.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: å¯èƒ½æ˜¯å› ä¸º `memchr` æ²¡æœ‰è¿›è¡Œå†…è”ï¼Œè€Œæ‰‹åŠ¨å®ç°è¢«å¼ºåˆ¶å†…è”åˆ° `parse_read` ä¸­ã€‚å¦‚æœæ˜¯è¿™æ ·ï¼Œè¿™å¯èƒ½è§£é‡Šäº†ä¸ Needletail ä¹‹é—´çš„æ€§èƒ½å·®å¼‚å¤§éƒ¨åˆ†ã€‚ä»æˆ‘çš„
    Julia ä»£ç ä¸­ç§»é™¤ `@inline` æŒ‡ä»¤ä¼šä½¿é€Ÿåº¦å‡æ…¢çº¦ 20%ã€‚æœ‰è¶£çš„æ˜¯ï¼Œåœ¨æˆ‘çš„ Cargo.toml æ–‡ä»¶ä¸­è®¾ç½® `lto = "thin"` å’Œ
    `codegen-units = 1` å¯ä»¥å°† Needletail çš„è¿è¡Œæ—¶é—´é™ä½åˆ° 357 msï¼Œå‡ ä¹ä¸ Mojo çš„æ¨æµ‹è¿è¡Œæ—¶é—´å®Œå…¨åŒ¹é…ã€‚
- en: These differences are trivialities. I don't know why my Julia implementation
    is twice as fast as Needletail, but subtracting the lack of validation, I doubt
    it's something substantial. There are often real important reasons why some languages
    are faster than others - whether they provide good zero-cost abstractions for
    high-level data types, whether they provide good multithreading and SIMD support,
    how well they support generics and how well libraries compose together, how defensive
    vs adventurous they make programmers, and much else. I don't think this Mojo implementation
    shows any of this.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™äº›å·®å¼‚éƒ½æ˜¯å¾®ä¸è¶³é“çš„ã€‚æˆ‘ä¸çŸ¥é“ä¸ºä»€ä¹ˆæˆ‘çš„ Julia å®ç°æ¯” Needletail å¿«ä¸¤å€ï¼Œä½†å‡å»ç¼ºä¹éªŒè¯ï¼Œæˆ‘æ€€ç–‘è¿™å¹¶ä¸æ˜¯ä»€ä¹ˆé‡è¦çš„äº‹æƒ…ã€‚æœ‰æ—¶å€™ï¼ŒæŸäº›è¯­è¨€æ¯”å…¶ä»–è¯­è¨€æ›´å¿«çš„åŸå› æ˜¯éå¸¸é‡è¦çš„
    - ä¾‹å¦‚å®ƒä»¬æ˜¯å¦ä¸ºé«˜çº§æ•°æ®ç±»å‹æä¾›äº†è‰¯å¥½çš„é›¶æˆæœ¬æŠ½è±¡ï¼Œå®ƒä»¬æ˜¯å¦æä¾›äº†è‰¯å¥½çš„å¤šçº¿ç¨‹å’Œ SIMD æ”¯æŒï¼Œå®ƒä»¬æ˜¯å¦æ”¯æŒæ³›å‹ä»¥åŠå®ƒä»¬æ˜¯å¦æ”¯æŒåº“çš„è‰¯å¥½ç»„åˆï¼Œä»¥åŠå®ƒä»¬æ˜¯å¦ä½¿ç¨‹åºå‘˜æ›´åŠ ä¿å®ˆè¿˜æ˜¯æ›´åŠ å†’é™©ï¼Œç­‰ç­‰ã€‚æˆ‘ä¸è®¤ä¸ºè¿™ä¸ª
    Mojo å®ç°å±•ç¤ºäº†å…¶ä¸­çš„ä»»ä½•å†…å®¹ã€‚
- en: ''
  id: totrans-104
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'I don''t want to coldly dismiss the Mojo blog post. After all, its two main
    points are essentially right: That bioinformatics needs a language to bridge high-level
    and high-performance programming, and that Mojo is capable of producing fast code.
    I don''t put too much value in the actual reported numbers in the benchmark, but
    they don''t matter in the big picture. It''s also feels a little like overkill
    to go to this length to tear apart a blog post from from a guy who is just excited
    about what Mojo could bring to bioinformatics. It''s just his bad luck that there
    are people like me out there - a bioinformatician who is passionate about high
    performance computing for science, maintain my own FASTQ parsing library, and
    is particularly sceptical about Mojo.'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä¸æƒ³å†·æ¼ åœ°å¦å®šæ²ƒéœåšå®¢æ–‡ç« ã€‚æ¯•ç«Ÿï¼Œå®ƒçš„ä¸¤ä¸ªä¸»è¦è§‚ç‚¹åŸºæœ¬æ˜¯æ­£ç¡®çš„ï¼šç”Ÿç‰©ä¿¡æ¯å­¦éœ€è¦ä¸€ç§è¯­è¨€æ¥è¿æ¥é«˜çº§å’Œé«˜æ€§èƒ½ç¼–ç¨‹ï¼Œå¹¶ä¸”æ²ƒéœèƒ½å¤Ÿäº§ç”Ÿå¿«é€Ÿçš„ä»£ç ã€‚æˆ‘å¯¹åŸºå‡†æµ‹è¯•ä¸­å®é™…æŠ¥å‘Šçš„æ•°å­—å¹¶ä¸æ˜¯å¾ˆçœ‹é‡ï¼Œä½†è¿™äº›æ•°æ®åœ¨å¤§å±€é¢ä¸Šå¹¶ä¸é‡è¦ã€‚æ‰¹è¯„ä¸€ä½å……æ»¡æ¿€æƒ…åœ°è®¨è®ºæ²ƒéœå¯èƒ½ç»™ç”Ÿç‰©ä¿¡æ¯å­¦å¸¦æ¥çš„æœªæ¥çš„äººçš„åšå®¢æ–‡ç« ä¼¼ä¹æœ‰ç‚¹è¿‡åˆ†äº†ã€‚å‘½è¿ä½¿æˆ‘é‡åˆ°åƒæˆ‘è¿™æ ·çš„äººâ€”â€”ä¸€ä¸ªå¯¹ç§‘å­¦çš„é«˜æ€§èƒ½è®¡ç®—å……æ»¡æ¿€æƒ…çš„ç”Ÿç‰©ä¿¡æ¯å­¦å®¶ï¼Œä¿æŒè‡ªå·±çš„FASTQè§£æåº“ï¼Œå¹¶å¯¹æ²ƒéœæŒæ€€ç–‘æ€åº¦ã€‚
- en: Introspecting, I think I'm a little oversensitive to Mojo's marketing hype.
    Ostensibly because the original Mojo announcements (and also this Mojo blog post),
    made a lot of bold claims that could be construed as hyperbolic, while keeping
    the compiler to themselves, giving it the smell of vaporware. But if I'm being
    honest with myself, it's probably because I'm so invested in the prospect of Julia
    for bioinformatics.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: åçœè‡ªå·±ï¼Œæˆ‘è§‰å¾—æˆ‘å¯¹æ²ƒéœçš„è¥é”€ç‚’ä½œæœ‰äº›è¿‡æ•ã€‚è¡¨é¢ä¸Šæ˜¯å› ä¸ºæœ€åˆçš„æ²ƒéœå…¬å‘Šï¼ˆä»¥åŠè¿™ç¯‡æ²ƒéœåšå®¢æ–‡ç« ï¼‰æå‡ºäº†å¾ˆå¤šå¯èƒ½è¢«è§£é‡Šä¸ºå¤¸å¼ çš„å¤§èƒ†å£°æ˜ï¼ŒåŒæ—¶ä»–ä»¬å¯¹ç¼–è¯‘å™¨å®ˆå£å¦‚ç“¶ï¼Œè¿™ç»™äººä¸€ç§æ²¡æœ‰å®è´¨çš„æ„Ÿè§‰ã€‚ä½†å®è¯å®è¯´ï¼Œå¯èƒ½æ˜¯å› ä¸ºæˆ‘å¯¹æœ±è‰å¨…åœ¨ç”Ÿç‰©ä¿¡æ¯å­¦é¢†åŸŸçš„å‰æ™¯éå¸¸æŠ•èµ„ã€‚
- en: To me, Julia seems like *such an obvious* solution to the two-language problem
    in bioinformatics (and in deep learning). All the hard problems with bridging
    speed and dynamism have essentially been solved in Julia. At the same time, the
    language remains niche, mostly because it still has too many rough edges and usability
    issues, such as latency, the inability to statically analyse Julia or compile
    executable binaries. But these issues are not fundamental to the language - they're
    rather in the category of ordinary engineering problems. Solving them is mostly
    "just" a matter of putting in tens of thousands of professional dev hours, which
    is a matter of getting tens of millions of euros to pay for hiring people to do
    the job.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹æˆ‘æ¥è¯´ï¼Œæœ±è‰å¨…ä¼¼ä¹æ˜¯åœ¨ç”Ÿç‰©ä¿¡æ¯å­¦ï¼ˆä»¥åŠæ·±åº¦å­¦ä¹ ï¼‰ä¸­è§£å†³åŒè¯­è¨€é—®é¢˜çš„*å¦‚æ­¤æ˜æ˜¾*çš„è§£å†³æ–¹æ¡ˆã€‚åœ¨æœ±è‰å¨…ä¸­ï¼Œè§£å†³äº†æ‰€æœ‰ä¸é€Ÿåº¦å’ŒåŠ¨åŠ›å­¦çš„è¿æ¥é—®é¢˜ï¼ŒåŒæ—¶ï¼Œè¯¥è¯­è¨€ä»ç„¶æ˜¯ä¸€ä¸ªå°ä¼—ï¼Œä¸»è¦æ˜¯å› ä¸ºå®ƒä»ç„¶å­˜åœ¨å¤ªå¤šç²—ç³™çš„è¾¹ç¼˜å’Œå¯ç”¨æ€§é—®é¢˜ï¼Œå¦‚å»¶è¿Ÿã€ä¸èƒ½é™æ€åˆ†ææœ±è‰å¨…æˆ–ç¼–è¯‘å¯æ‰§è¡ŒäºŒè¿›åˆ¶æ–‡ä»¶ã€‚ä½†è¿™äº›é—®é¢˜å¹¶ä¸æ˜¯è¯¥è¯­è¨€çš„æ ¹æœ¬é—®é¢˜â€”â€”å®ƒä»¬æ›´å¤šåœ°å±äºæ™®é€šå·¥ç¨‹é—®é¢˜çš„èŒƒç•´ã€‚è§£å†³å®ƒä»¬å¤§éƒ¨åˆ†æ˜¯â€œåªâ€éœ€è¦æŠ•å…¥æ•°ä¸‡å°æ—¶çš„ä¸“ä¸šå¼€å‘æ—¶é—´ï¼Œè¿™åªæ˜¯è·å–æ•°ç™¾ä¸‡æ¬§å…ƒæ¥æ”¯ä»˜é›‡ä½£äººå‘˜æ¥å®Œæˆå·¥ä½œçš„é—®é¢˜ã€‚
- en: It does grate me then, when *someone else* manages to raise 100M dollars on
    the premise of reinventing the wheel to solve the exact same problem, but from
    a worse starting point because they start from zero *and* they want to retain
    Python compatibility. Think of what money like that could do to Julia!
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: å½“*å…¶ä»–äºº*è®¾æ³•ç­¹é›†1äº¿ç¾å…ƒï¼Œä»¥é‡å¤å‘æ˜è½®å­æ¥è§£å†³å®Œå…¨ç›¸åŒçš„é—®é¢˜æ—¶ï¼Œè¿™è®©æˆ‘æ„Ÿåˆ°ä¸èˆ’æœï¼Œå› ä¸ºä»–ä»¬ä»é›¶å¼€å§‹*å¹¶ä¸”*ä»–ä»¬å¸Œæœ›ä¿ç•™Pythonçš„å…¼å®¹æ€§ã€‚æƒ³æƒ³è¿™æ ·çš„èµ„é‡‘å¯ä»¥ç»™æœ±è‰å¨…å¸¦æ¥ä»€ä¹ˆï¼
- en: A bigger person than me might have an attitude of 'let a thousand flowers bloom'
    to solve the two language problem, and, sure, it's probable that Julia will learn
    from Mojo as Mojo already has learned from Julia. But I can't help the feeling
    that the two languages compete in a zero-sum game, at least to some extent. When
    I talk to my colleagues, half of them have no interest in high performance computing,
    and most others have resigned themselves to only doing the analyses that existing
    C libraries allow them to do, believing that writing new low-level routines is
    the job of someone else, probably computer scientists. Because they're not programming
    language nerds like me, they will use the tools that are at hand, without caring
    about their technical merit. If more money is spent on sanding the edges off a
    technically worse solution, then they will stick with it until the end of time,
    and not demand something better.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: æ¯”æˆ‘æ›´å¤§åº¦çš„äººä¹Ÿè®¸ä¼šæŒæœ‰â€œè®©ä¸€åƒæœµé²œèŠ±ç»½æ”¾â€çš„æ€åº¦æ¥è§£å†³åŒè¯­é—®é¢˜ï¼Œå½“ç„¶ï¼Œæœ±è‰å¨…å¯èƒ½ä¼šå‘æ²ƒéœå­¦ä¹ ï¼Œå°±åƒæ²ƒéœå·²ç»ä»æœ±è‰å¨…å­¦ä¹ è¿‡ä¸€æ ·ã€‚ä½†æˆ‘ä¸èƒ½ä¸æ„Ÿåˆ°ï¼Œè¿™ä¸¤ç§è¯­è¨€åœ¨æŸç§ç¨‹åº¦ä¸Šç«äº‰ä¸ºé›¶å’Œåšå¼ˆã€‚å½“æˆ‘å’ŒåŒäº‹äº¤è°ˆæ—¶ï¼Œä¸€åŠçš„äººå¯¹é«˜æ€§èƒ½è®¡ç®—ä¸æ„Ÿå…´è¶£ï¼Œè€Œå…¶ä»–å¤§éƒ¨åˆ†äººå·²ç»æ¥å—äº†åªèƒ½ä½¿ç”¨ç°æœ‰çš„Cåº“æ¥è¿›è¡Œåˆ†æçš„äº‹å®ï¼Œä»–ä»¬ç›¸ä¿¡ç¼–å†™æ–°çš„ä½çº§ç¨‹åºæ˜¯åˆ«äººçš„ä»»åŠ¡ï¼Œå¯èƒ½æ˜¯è®¡ç®—æœºç§‘å­¦å®¶ã€‚å› ä¸ºä»–ä»¬ä¸åƒæˆ‘ä¸€æ ·æ˜¯ç¼–ç¨‹è¯­è¨€è¿·ï¼Œä»–ä»¬å°†ä½¿ç”¨æ‰‹å¤´çš„å·¥å…·ï¼Œè€Œä¸å…³å¿ƒå®ƒä»¬çš„æŠ€æœ¯ä»·å€¼ã€‚å¦‚æœæ›´å¤šçš„é’±èŠ±åœ¨ä¿®æ­£æŠ€æœ¯ä¸Šæ›´å·®çš„è§£å†³æ–¹æ¡ˆä¸Šï¼Œé‚£ä¹ˆä»–ä»¬ä¼šä¸€ç›´åšæŒä¸‹å»ï¼Œä¸ä¼šè¦æ±‚æ›´å¥½çš„ä¸œè¥¿ã€‚
- en: Does Mojo bring real value to the Python ecosystem? To me it's still too early
    to tell. I'm glad someone of the calibre of Chris Lattner is working on breaking
    the two-language barrier, but I wished he had joined forces with those who have
    been solving the problem the last decade in Julia-land.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: æ²ƒéœæ˜¯å¦ç»™Pythonç”Ÿæ€å¸¦æ¥äº†çœŸæ­£çš„ä»·å€¼ï¼Ÿå¯¹æˆ‘æ¥è¯´ï¼Œç°åœ¨è¿˜ä¸ºæ—¶è¿‡æ—©ã€‚æˆ‘å¾ˆé«˜å…´åƒå…‹é‡Œæ–¯Â·æ‹‰ç‰¹çº³è¿™æ ·çš„äººæ­£åœ¨åŠªåŠ›æ‰“ç ´åŒè¯­è¨€å£å’ï¼Œä½†æˆ‘å¸Œæœ›ä»–èƒ½ä¸é‚£äº›åœ¨è¿‡å»åå¹´é‡Œä¸€ç›´åœ¨æœ±è‰å¨…é¢†åŸŸè§£å†³è¿™ä¸€é—®é¢˜çš„äººåˆä½œã€‚
- en: ''
  id: totrans-111
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '*Added 2024-04-01*'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: '*æ·»åŠ äº2024-04-01*'
- en: After its original publication, this post has made the rounds on various forums.
    In this part, I want to address some responses.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æœ€åˆå‘å¸ƒåï¼Œè¿™ç¯‡æ–‡ç« åœ¨å„ç§è®ºå›ä¸Šå¹¿ä¸ºæµä¼ ã€‚åœ¨è¿™ä¸€éƒ¨åˆ†ï¼Œæˆ‘æƒ³å›åº”ä¸€äº›å›åº”ã€‚
- en: ''
  id: totrans-114
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The commit I read was 42ba5bc. This commit didn't do any validation.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘çœ‹çš„æäº¤æ˜¯42ba5bcã€‚è¿™ä¸ªæäº¤æ²¡æœ‰è¿›è¡Œä»»ä½•éªŒè¯ã€‚
- en: While the `record_coord.mojo` did contain a `.validate(self, chunk)` method,
    this method was not called by the `FastParser.parse_all` function. This is the
    function mentioned in the README.md under the "Usage" section, as well as the
    function called in `main.mojo`.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: å°½ç®¡`record_coord.mojo`åŒ…å«äº†ä¸€ä¸ª`.validate(self, chunk)`æ–¹æ³•ï¼Œä½†è¿™ä¸ªæ–¹æ³•å¹¶æ²¡æœ‰è¢«`FastParser.parse_all`å‡½æ•°è°ƒç”¨ã€‚è¿™æ˜¯README.mdä¸­â€œUsageâ€éƒ¨åˆ†æåˆ°çš„å‡½æ•°ï¼Œä¹Ÿæ˜¯`main.mojo`ä¸­è°ƒç”¨çš„å‡½æ•°ã€‚
- en: 'The benchmarking section said the following:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: åŸºå‡†æµ‹è¯•éƒ¨åˆ†å¦‚ä¸‹æ‰€è¿°ï¼š
- en: '[ content elided .... ]'
  id: totrans-118
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '[å†…å®¹å·²çœç•¥...]'
- en: It's entirely possible that the original Mojo post used timings obtained from
    running `parser.next()` in a loop, which *does* validate the records - the "Benchmarks"
    section does not say exactly *what function* from the `FastParser` module was
    used.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: å®Œå…¨æœ‰å¯èƒ½ï¼ŒåŸå§‹çš„ Mojo å¸–å­ä½¿ç”¨äº†åœ¨å¾ªç¯ä¸­è¿è¡Œ `parser.next()` è·å¾—çš„æ—¶é—´ï¼Œè¿™ä¼šéªŒè¯è®°å½• - "Benchmarks" éƒ¨åˆ†å¹¶æ²¡æœ‰å‡†ç¡®è¯´æ˜
    `FastParser` æ¨¡å—ä¸­ä½¿ç”¨äº†å“ªä¸ªå‡½æ•°ã€‚
- en: However, I feel it's unfair to blame me for looking at the published code and
    assuming that's the code that was being run.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸è¿‡ï¼Œæˆ‘è§‰å¾—æŠŠè´£ä»»æ¨ç»™æˆ‘ï¼Œè¯´æˆ‘åªçœ‹äº†å‘å¸ƒçš„ä»£ç ï¼Œå°±å‡å®šé‚£å°±æ˜¯æ­£åœ¨è¿è¡Œçš„ä»£ç ï¼Œè¿™æ˜¯ä¸å…¬å¹³çš„ã€‚
- en: Also, note that the MojoFastTrim repo, and the Mojo blog post has been *changed*
    since I wrote my post. The changes to the repo includes added validation to the
    `parse_all` function, and adding a new `benchmark` directory to make the benchmarking
    instructions clearer. The changes to the post include adding a link to the updated
    repo.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: æ­¤å¤–ï¼Œè¯·æ³¨æ„ï¼Œè‡ªä»æˆ‘å†™äº†æˆ‘çš„å¸–å­ä»¥æ¥ï¼ŒMojoFastTrim ä»“åº“å’Œ Mojo åšå®¢æ–‡ç« å·²ç» *æ›´æ”¹*ã€‚å¯¹ä»“åº“çš„æ›´æ”¹åŒ…æ‹¬ä¸º `parse_all` å‡½æ•°æ·»åŠ äº†éªŒè¯ï¼Œå¹¶æ·»åŠ äº†ä¸€ä¸ªæ–°çš„
    `benchmark` ç›®å½•ï¼Œä»¥ä½¿åŸºå‡†æµ‹è¯•è¯´æ˜æ›´æ¸…æ™°ã€‚å¯¹æ–‡ç« çš„æ›´æ”¹åŒ…æ‹¬æ·»åŠ äº†æŒ‡å‘æ›´æ–°åä»“åº“çš„é“¾æ¥ã€‚
- en: 'At any rate, it''s worth keeping in mind that:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: æ— è®ºå¦‚ä½•ï¼Œå€¼å¾—è®°ä½çš„æ˜¯ï¼š
- en: Even with the `validate` function, the Mojo implementation does less validation
    than Needletail. For example, it doesn't handle file IO errors at all, or handle
    reads longer than the buffer, or handle Windows line endings.
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å³ä½¿ä½¿ç”¨äº† `validate` å‡½æ•°ï¼ŒMojo çš„å®ç°ä¹Ÿæ¯” Needletail è¿›è¡Œçš„éªŒè¯è¦å°‘ã€‚ä¾‹å¦‚ï¼Œå®ƒæ ¹æœ¬ä¸å¤„ç†æ–‡ä»¶ IO é”™è¯¯ï¼Œä¹Ÿä¸å¤„ç†è¶…è¿‡ç¼“å†²åŒºçš„è¯»å–ï¼Œä¹Ÿä¸å¤„ç†
    Windows çš„è¡Œå°¾ã€‚
- en: 'However, Needletail doesn''t do a ton of validation, either. For example, it
    doesn''t check that the sequences and qualities are ASCII, which the code assumes.
    For example, this is an excerpt from the Needletail code, which where the check
    that is commented out is that the quality line must be bytes between `!` and `~`:'
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä½†æ˜¯ Needletail ä¹Ÿæ²¡æœ‰è¿›è¡Œå¤§é‡çš„éªŒè¯ã€‚ä¾‹å¦‚ï¼Œå®ƒä¸æ£€æŸ¥åºåˆ—å’Œè´¨é‡æ˜¯å¦æ˜¯ ASCIIï¼Œè€Œä»£ç å´å‡å®šäº†è¿™ä¸€ç‚¹ã€‚ä¾‹å¦‚ï¼Œè¿™æ˜¯ Needletail ä»£ç çš„ä¸€éƒ¨åˆ†ï¼Œè¢«æ³¨é‡Šæ‰çš„æ£€æŸ¥æ˜¯è´¨é‡è¡Œå¿…é¡»æ˜¯ä½äº
    `!` å’Œ `~` ä¹‹é—´çš„å­—èŠ‚ï¼š
- en: ''
  id: totrans-125
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: It's not necessary to run Mojo on my own machine to show that Mojo's speed is
    due to its implementation. This post 100% grants that Mojo's implementation is
    exactly as much faster than Needletail as is claimed on the MojoFastTrim repository.
    All I have to show is that I get at least as large an improvement over Needletail
    as they claim Mojo does, by implementing the same algorithm in Julia.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: å¹¶ä¸éœ€è¦åœ¨æˆ‘çš„æœºå™¨ä¸Šè¿è¡Œ Mojo æ¥è¯æ˜ Mojo çš„é€Ÿåº¦æ˜¯ç”±å…¶å®ç°å†³å®šçš„ã€‚æœ¬æ–‡ç™¾åˆ†ä¹‹ç™¾è‚¯å®šäº† Mojo çš„å®ç°ä¸ MojoFastTrim ä»“åº“ä¸Šæ‰€å£°ç§°çš„ä¸€æ ·å¿«ã€‚æˆ‘åªéœ€è¦è¯æ˜ï¼Œé€šè¿‡åœ¨
    Julia ä¸­å®ç°ç›¸åŒçš„ç®—æ³•ï¼Œæˆ‘è‡³å°‘å¯ä»¥è·å¾—ä¸ Mojo æ‰€å£°ç§°çš„ä¸€æ ·å¤§çš„æ”¹è¿›ï¼Œè¶…è¶Š Needletailã€‚
- en: But that's a different machine! How can we know your Julia implementation is
    fast on machines in general?
  id: totrans-127
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ä½†é‚£æ˜¯å¦ä¸€å°æœºå™¨ï¼æˆ‘ä»¬æ€ä¹ˆçŸ¥é“æ‚¨çš„ Julia å®ç°åœ¨ä¸€èˆ¬æƒ…å†µä¸‹è¿è¡Œå¾—å¿«å‘¢ï¼Ÿ
- en: If it all comes down to which machine it's run on, I might as well claim that
    the Mojo post doesn't prove anything because it's not run on my machine. See how
    it works?
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœä¸€åˆ‡éƒ½å–å†³äºåœ¨å“ªå°æœºå™¨ä¸Šè¿è¡Œï¼Œé‚£ä¹ˆæˆ‘å¯èƒ½ä¹Ÿä¼šå£°ç§° Mojo çš„å¸–å­å¹¶æ²¡æœ‰è¯æ˜ä»»ä½•äº‹æƒ…ï¼Œå› ä¸ºå®ƒæ²¡æœ‰åœ¨æˆ‘çš„æœºå™¨ä¸Šè¿è¡Œã€‚çœ‹çœ‹å®ƒæ˜¯å¦‚ä½•è¿ä½œçš„ï¼Ÿ
- en: At the *very least*, I've shown that the Mojo implementation is *insufficient
    evidence* that the reason the Mojo implementation is fast is due to features unique
    to Mojo which Julia (or Rust) doesn't have. If it's really due to Mojo's groundbreaking
    compiler advances, why is my Julia implementation relatively faster compared to
    Needletail?
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: è‡³å°‘ï¼Œæˆ‘å·²ç»è¯æ˜äº† Mojo å®ç°ä¸è¶³ä»¥è¯æ˜ Mojo å®ç°ä¹‹æ‰€ä»¥å¿«ï¼Œæ˜¯ç”±äº Mojo å…·æœ‰è€Œ Juliaï¼ˆæˆ– Rustï¼‰æ²¡æœ‰çš„ç‹¬ç‰¹ç‰¹æ€§ã€‚å¦‚æœçœŸçš„æ˜¯ç”±äº
    Mojo çš„çªç ´æ€§ç¼–è¯‘å™¨è¿›å±•ï¼Œä¸ºä»€ä¹ˆæˆ‘çš„ Julia å®ç°ä¸ Needletail ç›¸æ¯”ç›¸å¯¹æ›´å¿«å‘¢ï¼Ÿ
- en: 'But *fine.* I downloaded a VM and installed Julia and Mojo in it. Hmm, but
    what was the CLI for the Mojo program? Let''s see:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: ä½†æ˜¯ *å¥½å§*ã€‚æˆ‘ä¸‹è½½äº†ä¸€ä¸ªè™šæ‹Ÿæœºï¼Œå¹¶åœ¨å…¶ä¸­å®‰è£…äº† Julia å’Œ Mojoã€‚å—¯ï¼Œä½†æ˜¯ Mojo ç¨‹åºçš„å‘½ä»¤è¡Œæ˜¯ä»€ä¹ˆï¼Ÿè®©æˆ‘ä»¬çœ‹çœ‹ï¼š
- en: '[PRE7]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Fun fun fun. Anyway Here are my timings, all run in the same box on the later
    commit 38bb68:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: å¥½ç©å¥½ç©å¥½ç©ã€‚æ— è®ºå¦‚ä½•ï¼Œè¿™æ˜¯æˆ‘çš„æ—¶é—´è®°å½•ï¼Œéƒ½æ˜¯åœ¨åŒä¸€å°æœºå™¨ä¸Šçš„åæœŸæäº¤ 38bb68 ä¸Šè¿è¡Œçš„ï¼š
- en: 'My Julia code: 213 ms'
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æˆ‘çš„ Julia ä»£ç ï¼š213 æ¯«ç§’
- en: 'The provided Mojo code with validation: 332 ms'
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æä¾›çš„ Mojo ä»£ç éªŒè¯åï¼š332 æ¯«ç§’
- en: 'The provided Mojo code without validation: 320 ms'
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æä¾›çš„æ²¡æœ‰éªŒè¯çš„ Mojo ä»£ç ï¼š320 æ¯«ç§’
- en: 'Needletail + w. `lto` and `codegen_units=1`: 356 ms'
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Needletail + `lto` å’Œ `codegen_units=1`ï¼š356 æ¯«ç§’
- en: 'Needletail: 471 ms'
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Needletailï¼š471 æ¯«ç§’
- en: So yeah, the point stands.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥æ˜¯çš„ï¼Œè®ºç‚¹æˆç«‹ã€‚
- en: '| [[1]](#fnref:1) | I''ve looked at commit 42ba5bc. The repository has been
    updated since, so the code listed in this blog post might be out of date by the
    time you read this. |'
  id: totrans-139
  prefs: []
  type: TYPE_TB
  zh: '| [[1]](#fnref:1) | æˆ‘çœ‹äº†æäº¤ 42ba5bcã€‚ä»“åº“è‡ªé‚£æ—¶ä»¥æ¥å·²ç»æ›´æ–°ï¼Œå› æ­¤åœ¨æ‚¨é˜…è¯»æ­¤æ–‡ç« æ—¶ï¼Œæ­¤åšå®¢æ–‡ç« ä¸­åˆ—å‡ºçš„ä»£ç å¯èƒ½å·²è¿‡æ—¶ã€‚
    |'
- en: '| [[2]](#fnref:2) | I''ve found that when you mention that DNA is the basis
    of heritability, people will appear from thin air and argue about epigenetics.
    But I believe epigenetics is a rounding error compared to the DNA sequence when
    we talk about heritability and the medium of evolution. I don''t doubt that e.g.
    chromatin accessibility is an important parameter in cells, but let''s not conflate
    the biological state of a cell with a *heiritable signal* which is stable enough
    to be acted on over evolutionary time. |'
  id: totrans-140
  prefs: []
  type: TYPE_TB
  zh: '| [[2]](#fnref:2) | æˆ‘å‘ç°å½“ä½ æåˆ° DNA æ˜¯é—ä¼ æ€§çš„åŸºç¡€æ—¶ï¼Œäººä»¬ä¼šä»ç©ºä¸­å‡ºç°å¹¶äº‰è®ºè¡¨è§‚é—ä¼ å­¦ã€‚ä½†æˆ‘è®¤ä¸ºï¼Œä¸ DNA åºåˆ—ç›¸æ¯”ï¼Œè¡¨è§‚é—ä¼ å­¦åœ¨é—ä¼ æ€§å’Œè¿›åŒ–åª’ä»‹æ–¹é¢åªæ˜¯ä¸€ä¸ªèˆå…¥è¯¯å·®ã€‚æˆ‘ä¸æ€€ç–‘ä¾‹å¦‚æŸ“è‰²è´¨å¯åŠæ€§åœ¨ç»†èƒä¸­æ˜¯ä¸€ä¸ªé‡è¦å‚æ•°ï¼Œä½†è®©æˆ‘ä»¬ä¸è¦å°†ç»†èƒçš„ç”Ÿç‰©çŠ¶æ€ä¸
    *ç¨³å®šåˆ°å¯ä»¥åœ¨è¿›åŒ–æ—¶é—´å†…å‘æŒ¥ä½œç”¨çš„é—ä¼ ä¿¡å·* æ··ä¸ºä¸€è°ˆã€‚'
- en: '| [[3]](#fnref:3) | Some programmers wonder why DNA is usually saved encoded
    in plaintext. Isn''t that inefficient, considering the cost of storage for terabyte-sized
    DNA datasets? Nope. It''s usually stored gzip-compressed at decompressed on the
    fly when used. DNA compresses well, and the plaintext format allows extra metadata
    to be written directly into the file, as well as being much easier to parse. There
    are some more efficient formats, like CRAM, which are used in some large-scale
    projects, but in my subfield of microbial metagenomics, I can''t recall ever having
    worked with a CRAM file. |'
  id: totrans-141
  prefs: []
  type: TYPE_TB
  zh: '| [[3]](#fnref:3) | ä¸€äº›ç¨‹åºå‘˜æƒ³çŸ¥é“ä¸ºä»€ä¹ˆDNAé€šå¸¸ä»¥æ˜æ–‡ç¼–ç ä¿å­˜ã€‚è€ƒè™‘åˆ°TBçº§åˆ«çš„DNAæ•°æ®é›†çš„å­˜å‚¨æˆæœ¬ï¼Œè¿™ä¸æ˜¯ä½æ•ˆå—ï¼Ÿä¸æ˜¯çš„ã€‚é€šå¸¸å­˜å‚¨ä¸ºgzipå‹ç¼©æ ¼å¼ï¼Œåœ¨ä½¿ç”¨æ—¶åŠ¨æ€è§£å‹ç¼©ã€‚DNAå‹ç¼©æ•ˆæœå¾ˆå¥½ï¼Œå¹¶ä¸”æ˜æ–‡æ ¼å¼å…è®¸å°†é¢å¤–çš„å…ƒæ•°æ®ç›´æ¥å†™å…¥æ–‡ä»¶ï¼ŒåŒæ—¶æ›´å®¹æ˜“è§£æã€‚è¿˜æœ‰ä¸€äº›æ›´é«˜æ•ˆçš„æ ¼å¼ï¼Œæ¯”å¦‚CRAMï¼Œåœ¨ä¸€äº›å¤§å‹é¡¹ç›®ä¸­ä½¿ç”¨ï¼Œä½†åœ¨æˆ‘çš„å¾®ç”Ÿç‰©å®åŸºå› ç»„å­¦å­é¢†åŸŸï¼Œæˆ‘å‡ ä¹æ²¡æœ‰ä½¿ç”¨CRAMæ–‡ä»¶çš„ç»å†ã€‚
    |'
- en: '| [[4]](#fnref:4) | Only partially the reason - Needletail has two more reasons
    it''s faster. First, Rust''s `memchr` crate used by Needletail is much more optimised
    than Julia''s Automa.jl used by FASTX.jl, and Automa.jl probably can''t be optimised
    to the same level because Julia doesn''t support platform-specific SIMD code yet.
    Second, Rust''s borrowchecker makes it safe for Needletail to return a view into
    the active file buffer. This would be totally reckless in Julia, so we need to
    copy the bytes out to a separate buffer first (we actually need to do *two copies*
    of each byte, since Julia''s IO is buffered by default, using an inaccessible
    buffer). |'
  id: totrans-142
  prefs: []
  type: TYPE_TB
  zh: '| [[4]](#fnref:4) | åªæœ‰éƒ¨åˆ†åŸå›  - Needletail æ›´å¿«çš„ä¸¤ä¸ªåŸå› è¿˜åœ¨äºã€‚é¦–å…ˆï¼ŒRustçš„`memchr`åº“ç”¨äºNeedletailï¼Œæ¯”èµ·FASTX.jlä½¿ç”¨çš„Juliaçš„Automa.jlè¦ä¼˜åŒ–å¾—å¤šï¼Œè€Œä¸”Automa.jlå¯èƒ½æ— æ³•è¾¾åˆ°åŒæ ·çš„ä¼˜åŒ–æ°´å¹³ï¼Œå› ä¸ºJuliaç›®å‰è¿˜ä¸æ”¯æŒç‰¹å®šå¹³å°çš„SIMDä»£ç ã€‚å…¶æ¬¡ï¼ŒRustçš„å€Ÿç”¨æ£€æŸ¥å™¨ä½¿Needletailèƒ½å¤Ÿå®‰å…¨åœ°è¿”å›åˆ°æ´»åŠ¨æ–‡ä»¶ç¼“å†²åŒºçš„è§†å›¾ã€‚åœ¨Juliaä¸­è¿™æ ·åšå°†æ˜¯å®Œå…¨é²è½çš„ï¼Œæ‰€ä»¥æˆ‘ä»¬éœ€è¦å…ˆå°†å­—èŠ‚å¤åˆ¶åˆ°ä¸€ä¸ªå•ç‹¬çš„ç¼“å†²åŒºä¸­ï¼ˆå®é™…ä¸Šæˆ‘ä»¬éœ€è¦*å¤åˆ¶ä¸¤æ¬¡*æ¯ä¸ªå­—èŠ‚ï¼Œå› ä¸ºJuliaçš„IOé»˜è®¤ä½¿ç”¨ä¸å¯è®¿é—®çš„ç¼“å†²åŒºï¼‰ã€‚
    |'
