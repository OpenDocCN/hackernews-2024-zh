- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 类别：未分类
- en: 'date: 2024-05-27 15:03:37'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024年05月27日 15:03:37
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: AnyGPT
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: AnyGPT
- en: 来源：[https://junzhan2000.github.io/AnyGPT.github.io/](https://junzhan2000.github.io/AnyGPT.github.io/)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://junzhan2000.github.io/AnyGPT.github.io/](https://junzhan2000.github.io/AnyGPT.github.io/)
- en: We introduce AnyGPT, an any-to-any multimodal language model that utilizes discrete
    representations for the unified processing of various modalities, including speech,
    text, images, and music. AnyGPT can be trained stably without any alterations
    to the current large language model (LLM) architecture or training paradigms.
    Instead, it relies exclusively on data-level preprocessing, facilitating the seamless
    integration of new modalities into LLMs, akin to the incorporation of new languages.
    We build a multimodal text-centric dataset for multimodal alignment pre-training.
    Utilizing generative models, we synthesize the first large-scale any-to-any multimodal
    instruction dataset. It consists of 108k samples of multi-turn conversations that
    intricately interweave various modalities, thus equipping the model to handle
    arbitrary combinations of multimodal inputs and outputs. Experimental results
    demonstrate that AnyGPT is capable of facilitating any-to-any multimodal conversation
    while achieving performance comparable to specialized models across all modalities,
    proving that discrete representations can effectively and conveniently unify multiple
    modalities within a language model.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 我们介绍了 AnyGPT，这是一个任意到任意的多模态语言模型，利用离散表示统一处理各种模态，包括语音、文本、图像和音乐。AnyGPT 可以稳定训练，无需修改当前大语言模型（LLM）的架构或训练范例。相反，它完全依赖于数据级预处理，促进了新模态的无缝集成到
    LLM 中，类似于新语言的整合。我们为多模态对齐预训练构建了一个多模态文本中心的数据集。利用生成模型，我们合成了第一个大规模的任意到任意多模态指令数据集。它包含了108k个多轮对话样本，精细地交织了各种模态，从而使模型能够处理任意的多模态输入和输出组合。实验结果表明，AnyGPT
    能够促进任意到任意的多模态对话，同时在所有模态上达到了专用模型可比的性能，证明了离散表示能够有效而方便地统一语言模型中的多个模态。
