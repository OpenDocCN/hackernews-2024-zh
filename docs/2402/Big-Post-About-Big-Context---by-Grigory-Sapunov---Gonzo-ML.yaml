- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-05-29 13:28:12'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: Big Post About Big Context - by Grigory Sapunov - Gonzo ML
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://gonzoml.substack.com/p/big-post-about-big-context](https://gonzoml.substack.com/p/big-post-about-big-context)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: The context size in modern LLMs (that is, the maximum number of tokens they
    can process at once) is steadily increasing. Initially, moving from two or four
    thousand tokens to eight thousand seemed like a big achievement. Then came models
    with up to 32k tokens, but they were limited in availability for a long time.
    By the time they became widely available, they were already hopelessly outdated
    because one of the industry leaders, Anthropic, already had models with 100k tokens.
    Now, the limits of public models range from 128k (GPT-4 Turbo) to 200k (Anthropic).
    Google was lagging in this race, with its public models covering a maximum of
    32k (special versions of PaLM 2 and all versions of [Gemini 1.0](https://gonzoml.substack.com/p/google-gemini-a-family-of-highly)).
    A breakthrough appeared with [Gemini 1.5](https://blog.google/technology/ai/google-gemini-next-generation-model-february-2024/),
    which by default has the now typical 128k, but there's a non-public version with
    1M tokens, and a research version with 10M.
  prefs: []
  type: TYPE_NORMAL
- en: An interesting question is how exactly such a large context was achieved, and
    moreover, how it works efficiently. There are various fresh approaches from different
    angles, for example, [LongRoPE](https://arxiv.org/abs/2402.13753), [LongNet](https://arxiv.org/abs/2307.02486)
    with dilated attention, [RingAttention](https://arxiv.org/abs/2310.01889), or,
    say, [RMT-R](https://arxiv.org/abs/2402.10790). It's intriguing what exactly Google
    did.
  prefs: []
  type: TYPE_NORMAL
- en: These new limits will likely significantly change how we work with models. Let's
    speculate a bit about this near future.
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: '**1) First, old techniques like RAG, partly designed to circumvent the limitations
    of a small context window when working with long documents, should die out.**
    Or at least remain only for special cases, such as the need to pull in fresh or
    particularly relevant materials.'
  prefs: []
  type: TYPE_NORMAL
- en: Tools like [langchain's splitters](https://python.langchain.com/docs/modules/data_connection/document_transformers/),
    which mainly cut based on length (considering more suitable cutting points in
    some cases), were already problematic -- looking at those chopped paragraphs was
    hard, though somehow it worked.
  prefs: []
  type: TYPE_NORMAL
- en: Even with the ability to properly segment into reasonable pieces, all the different
    wrappers that match and select more suitable pieces, aggregate results, etc.,
    are needed. Now, potentially, there's no need to bother with this stuff, which
    is good.
  prefs: []
  type: TYPE_NORMAL
- en: In some cases, of course, it's still necessary and can improve solution quality,
    but that needs to be evaluated. I generally believe in end-to-end solutions and
    the eventual displacement of most of these workarounds.
  prefs: []
  type: TYPE_NORMAL
- en: It’s not that I’m against RAG or any other program-controlled LLMs. In fact,
    I’m a strong proponent of the [LLM Programs](https://gonzoml.substack.com/p/large-language-model-programs)
    approach (I wrote on it [here](https://gonzoml.substack.com/p/mindstorms-in-natural-language-based)
    and [there](https://gonzoml.substack.com/p/chain-of-thought-tree-of-thought)).
    My point is that these old-school and dumb RAG approaches are just poor man’s
    things. They must change.
  prefs: []
  type: TYPE_NORMAL
- en: '**2) 1M tokens is really a lot;** now, you can fit many articles, entire code
    repositories, or large books into the context. Considering the multimodality and
    the ability of modern models to also process images, videos, and audio (by converting
    them into special non-text tokens), you can load hours of video or audio recordings.'
  prefs: []
  type: TYPE_NORMAL
- en: Given that models perform well on [Needle In A Haystack tests](https://github.com/gkamradt/LLMTest_NeedleInAHaystack),
    you can get quite relevant answers when working with such lengths.
  prefs: []
  type: TYPE_NORMAL
- en: 'It''s really possible to find a specific frame in a video:'
  prefs: []
  type: TYPE_NORMAL
- en: 'or a moment in a book:'
  prefs: []
  type: TYPE_NORMAL
- en: Or solve entirely new classes of problems. [For example](https://www.facebook.com/DynamicWebPaige/videos/1422440318698615),
    cases where models were fed a video of a task solution (like house hunting on
    Zillow) and asked to generate Selenium code for solving the same task impress
    me. Or translating to/from the Kalamang language using a large grammar (not parallel
    sentences!) textbook. Jeff Dean also [wrote about this case](https://twitter.com/JeffDean/status/1758149033473020081).
  prefs: []
  type: TYPE_NORMAL
- en: Yes, there's also a dictionary and 400 parallel sentences, but still, in-context
    language learning is very cool. As are answers to questions about a long document.
  prefs: []
  type: TYPE_NORMAL
- en: Current models like GPT are still purely neural network-based, operating in
    a stimulus-response mode, without any clear place for System 2-like reasoning.
    The approaches [that exist](https://gonzoml.substack.com/p/system-2-attention-is-something-you)
    are mostly quite basic. But right now, various hybrid, including neuro-symbolic,
    models or models with planning elements are being developed. Hello to [the secret
    Q*](https://www.technologyreview.com/2023/11/27/1083886/unpacking-the-hype-around-openais-rumored-new-q-model/)
    or [other fresh approaches](https://arxiv.org/abs/2402.14083) in these areas.
    Even in the current mode, in-context learning of a new task from a textbook looks
    insanely cool (if it works). With full-fledged "System 2-like" capabilities, this
    could be a game-changer. One of the frontiers lies somewhere here.
  prefs: []
  type: TYPE_NORMAL
- en: '**3) An interesting question arises regarding the cost of such intelligence.**
    Existing [pricing for Gemini 1.0 Pro](https://ai.google.dev/pricing) (0.125$ per
    1M symbols) is significantly better than [OpenAI''s pricing](https://openai.com/pricing)
    for GPT-4 Turbo (10$/1M tokens), GPT-4 ($30/1M), and the significantly less cool
    GPT-3.5 Turbo (0.5$/1M). And better than [Anthropic''s Claude 2.1](https://www-cdn.anthropic.com/31021aea87c30ccaecbd2e966e49a03834bfd1d2/pricing.pdf)
    ($8/1M). [*] This discussion is about input tokens; output tokens are more expensive,
    but we usually don''t need to generate millions on the output, this is primarily
    important for tasks with a large input.'
  prefs: []
  type: TYPE_NORMAL
- en: If Gemini 1.5 Pro had the same pricing as 1.0, would you be willing to pay ten
    cents for an answer about a book? Or for generating code to automate a task you
    recorded on video?
  prefs: []
  type: TYPE_NORMAL
- en: My personal answer to the second question is yes, but to the first -- it depends.
    If you need to ask dozens of questions, it adds up to a few dollars. For analyzing
    a legal document or for a one-time book summary, okay, but if you need to do this
    regularly, it's a question. The economics need to be calculated. Services providing
    solutions based on such models need to explicitly account for usage to avoid going
    bankrupt.
  prefs: []
  type: TYPE_NORMAL
- en: '**4) Regardless of the economics, there must be ways to save and cache results.**
    If you need to ask a bunch of questions about the same set of documents, it''s
    strange to do it from scratch each time. If the prompt structure looks like {large
    text} + {question}, or {large grammar book} + {sentence to translate}, it would
    make sense to somehow cache the first part, since it''s constant. Technically,
    within a transformer, these input embeddings calculated by the multi-layer network
    could be saved somewhere, and for a new question, only calculate for this new
    addition, saving a lot of resources. But there''s no infrastructure for this yet
    (or I missed it), and even if you deploy the model yourself, you can''t do this
    right away; it requires programming.'
  prefs: []
  type: TYPE_NORMAL
- en: I expect something like this to appear both at the API level and infrastructure-wise
    for caching results of local models. Possibly, some convenient and lightweight
    integration with a vector database (startup founders, you get the idea).
  prefs: []
  type: TYPE_NORMAL
- en: '**5) When used correctly, this can significantly increase productivity in many
    tasks.** I personally wouldn''t be surprised if some individuals become 10 or
    100 times more productive, which is insanely cool. Obviously, this isn''t a panacea
    and won''t solve all problems, plus issues with confabulations (a better term
    than hallucinations). Result verification remains a highly relevant task.'
  prefs: []
  type: TYPE_NORMAL
- en: There are likely classes of tasks where verification is much cheaper than solving
    the task independently (we can jokingly call this class ***"cognitive NP"*** tasks),
    and there are definitely many of them -- writing letters or blog posts clearly
    falls here. I've long been writing in an English blog through direct translation
    of the entire text by GPT with subsequent editing, which is significantly faster
    than writing from scratch myself. I note that errors are comparatively rare, GPT-4
    Turbo often produces text that requires no changes at all. Sometimes -- one or
    two edits. I've never needed to rewrite not just the entire text, but even a single
    paragraph.
  prefs: []
  type: TYPE_NORMAL
- en: And these are just the surface-level tasks. If we dig deeper, there should be
    very many. I'm almost certain we'll see [Jevons paradox](https://en.wikipedia.org/wiki/Jevons_paradox)
    in full force here, with the use of all these models only increasing.
  prefs: []
  type: TYPE_NORMAL
- en: '**6) A very important and at the same time difficult class of solutions is
    model output validation.** As we can’t always rely on a model, and there is a
    risk of confabulations (which may sometimes be costly, see the [recent case with
    Air Canada chatbot](https://www.theguardian.com/world/2024/feb/16/air-canada-chatbot-lawsuit)
    or an older [court case with ChatGPT inventions](http://forbes.com/sites/mollybohannon/2023/06/08/lawyer-used-chatgpt-in-court-and-cited-fake-cases-a-judge-is-considering-sanctions/)),
    we need to lower such risks.'
  prefs: []
  type: TYPE_NORMAL
- en: In a similar and narrower field of machine translation (MT) there is a set of
    solutions called Machine translation quality estimation (MTQE), to estimate risk
    of bad translations, predict quality scores, detect specific types of errors,
    etc (the exact objective may vary). They are not perfect, and may not solve your
    particular task if it differs from the original objective, though they may help.
    We need something similar for LLMs, say LLMQE.
  prefs: []
  type: TYPE_NORMAL
- en: There will be solutions for which many companies will be willing to pay. There
    will be many different solutions suited for different cases. But reliably creating
    such solutions won't be easy. You all get this too.
  prefs: []
  type: TYPE_NORMAL
- en: '**7) It''s really unclear how the work for entry positions (juniors) will change
    in the near future.** And whether there will be any work for them at all. And
    if not, where the middles and seniors will come from. Not only and not so much
    in programming, but also in other areas. In content creation, in many tasks, models
    will surpass them or will be a significantly cheaper and faster alternative. What
    remains is the technically complex area of content validation -- probably where
    their activities will shift. But this is not certain. I expect a significant change
    in the nature of work and the emergence of entirely new tools, which do not yet
    exist (probably this is already being worked on by the likes of JetBrains).'
  prefs: []
  type: TYPE_NORMAL
- en: I don't know how much time OpenAI has until the creation of AGI, when they supposedly
    need to reconsider their relationship with Microsoft (*[“Such a system is excluded
    from IP licenses and other commercial terms with Microsoft, which only apply to
    pre-AGI technology.“](https://openai.com/our-structure)*) and generally decide
    how to properly monetize it. But even without that, they and Google are already
    acting as sellers of intelligence by the pound. It's unclear what will happen
    to the world next, but as some countries surged ahead of others during the industrial
    revolution, the same will happen here, but even faster.
  prefs: []
  type: TYPE_NORMAL
- en: What a time to be alive!
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Changelog
  prefs: []
  type: TYPE_NORMAL
- en: 02/03/2024 — added some clarifications based on questions and discussions at
    [HackerNews](https://news.ycombinator.com/item?id=39552705). Mostly in pp. 1 (LLM
    Programs) and 6 (MTQE).
  prefs: []
  type: TYPE_NORMAL
