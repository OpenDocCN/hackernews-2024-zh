- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-05-27 14:55:16'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: The fifth epoch of distributed computing | Google Cloud Blog
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://cloud.google.com/blog/topics/systems/the-fifth-epoch-of-distributed-computing](https://cloud.google.com/blog/topics/systems/the-fifth-epoch-of-distributed-computing)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Today, we have transitioned to the fifth Epoch, which is marked by a superposition
    of two opposing trends. First, while transistor count per ASIC continues to increase
    at exponential rates, clock rates are flat and the cost of each transistor is
    now nearly flat, both limited by the increasing complexity and investment required
    to achieve smaller feature sizes. The implication is that performance normalized
    to cost improvements, or performance efficiency, of all of compute, DRAM, storage,
    and network infrastructure, is flattening. At the same time, ubiquitous network
    coverage, broadly deployed sensors, and data-hungry machine learning applications
    are accelerating the demand for raw computing infrastructure exponentially.
  prefs: []
  type: TYPE_NORMAL
- en: '**Notable developments:** Machine learning, generative AI, privacy, sustainability,
    societal infrastructure **Interaction time among computers:** 10 microseconds
    **Featuring**: • 200Gbps–1+Tb/s networks • Ubiquitous, power-efficient, and high-speed
    wireless network coverage • Increasingly specialized accelerators: TPUs, GPUs,
    Smart NICs • Socket-level fabrics, optics, federated architectures • Connected
    spaces, vehicles, appliances, wearables, etc… **Breakthroughs:** Many coming...'
  prefs: []
  type: TYPE_NORMAL
- en: Without fundamental breakthroughs in computing design and organization, our
    ability as a community to meet societal demands for computing infrastructure will
    falter. Coming up with new architectures to overcome these limitations, new hardware
    and increasingly, software architectures, will define the fifth epoch of computing.
  prefs: []
  type: TYPE_NORMAL
- en: 'While we cannot predict the breakthroughs that will be delivered in this fifth
    epoch of computing, we do know that each previous epoch has been characterized
    by a factor of 100x improvement in scale, efficiency, and cost-performance, all
    while improving security and reliability. The demand for scale and capability
    is only increasing, so delivering such gains without the tailwinds of Moore’s
    Law and Dennard scaling at our backs will be daunting. We imagine, however, the
    broad strokes will involve:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Declarative programming models:** The [Von Neumann model](https://en.wikipedia.org/wiki/Von_Neumann_architecture)
    of sequential code execution on a dedicated processor has been incredibly useful
    for developers for decades. However, the rise of distributed and multi-threaded
    computing has broken the abstraction to the point where much of modern imperative
    code focuses on defensive, and often inefficient, constructs to manage asynchrony,
    heterogeneity, tail latency, optimistic concurrency, and failures. Complexity
    will only increase in the years ahead, essentially requiring new declarative programming
    models focused on intent, the user, and business logic. At the same time, managing
    execution flow and responding to shifting deployment conditions will need to be
    delegated to increasingly sophisticated compilers and [ML-powered runtimes](https://blog.google/technology/ai/introducing-pathways-next-generation-ai-architecture/).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Hardware segmentation:** In earlier epochs, a general-purpose server architecture
    with a system balance of CPU, memory, storage, and networking could efficiently
    meet workload needs throughout the data center. However, when designing for specialized
    computing needs, ML training, inference, video processing, the conflicting requirements
    for storage, memory capacity, latency, bandwidth and communication is causing
    a proliferation of heterogeneous designs. When general-purpose compute performance
    was improving at 1.5x/year, pursuing even a 5x improvement for 10% of workloads
    did not make sense given the complexity. Today, such improvements can no longer
    be ignored. Addressing this gap will require new approaches to designing, verifying,
    qualifying, and deploying composable hardware ASICs and memory units in months,
    not years.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Software-defined infrastructure:** As underlying infrastructure has become
    more complex and more distributed, multiple layers of virtualization from memory
    to CPU have maintained the single server abstraction for individual applications.
    This trend will continue in the coming epoch as infrastructure continues to scale
    out and become more heterogeneous. The corollary of hardware segmentation, declarative
    programming models and distributed computing environments comprised of thousands
    of servers, will stretch virtualization beyond the confines of individual servers
    to include distributed computing on a single server, multiple servers, storage/memory
    arrays, and clusters — in some cases bringing resources across an entire campus
    together to efficiently deliver end results.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Provably secure computation:** In the last epoch, the need to sustain compute
    efficiency inadvertently came at the cost of [security](https://dl.acm.org/doi/abs/10.1145/3399742)
    and [reliability](https://sigops.org/s/conferences/hotos/2021/papers/hotos21-s01-hochschild.pdf).
    However, as our lives move increasingly online, the need for privacy and confidentiality
    increases exponentially for individuals, for business, and governments. Data sovereignty,
    or the need to restrict the physical location of data, even derived, will become
    increasingly important to adhere to government policies, but also to transparently
    show the lineage of increasingly ML-generated content. Despite some cost in baseline
    performance, these needs must be first-class requirements and constraints.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Sustainability:** The first three epochs of computing delivered exponential
    improvements in performance for fixed power. With the end of Dennard scaling in
    the fourth epoch, global power consumption associated with computing has grown
    quickly, partially offset by the move to cloud-hosted infrastructure, which is
    2-3x more power-efficient relative to earlier, on-premises designs. Further, cloud
    providers have made broad commitments to move to first carbon-neutral and then
    carbon-free power sources. However, the demand for data and compute will continue
    to grow and even likely accelerate in the fifth epoch. This will turn power-efficiency
    and carbon emissions into primary systems-evaluation metrics. Of particular note,
    [embodied carbon](https://en.wikipedia.org/wiki/Embedded_emissions) over the entire
    lifecycle of infrastructure build and delivery will require both improved visibility
    and optimization.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Algorithmic innovation:** The tailwinds of exponentially increasing performance
    have allowed software efficiency improvements to often go neglected. As improvement
    in underlying hardware components slows, the focus will turn to software and algorithmic
    opportunities. [Studies](https://www.science.org/doi/10.1126/science.aam9744)
    indicate that opportunities for 2-10x improvement in software optimization abound
    in systems code. Efficiently identifying these software optimization opportunities
    and developing techniques to gracefully and reliably deliver these benefits to
    production systems at scale will be a critical opportunity. Leveraging recent
    breakthroughs in coding LLMs to partially automate this work would be a significant
    accelerant in the fifth epoch.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Integrating across the above, the fifth epoch will be ruled by measures of
    overall user-system efficiency (useful answers per second) rather than lower-level
    per-component measures such as cost per MIPS, cost per GB of DRAM, cost per Gb/s,
    etc. Further, the units of efficiency will not be simply measured in performance-per-unit-cost
    but will [explicitly account for power consumption and carbon emissions](https://www.youtube.com/watch?v=EFe7-WZMMhc),
    and will take security and privacy as primary metrics, all while enforcing reliability
    requirements for the infrastructure on which society increasingly depends. Taken
    together, there are many untapped opportunities to deliver the next generation
    of infrastructure:'
  prefs: []
  type: TYPE_NORMAL
- en: A greater than 10x opportunity in scale-out efficiency of our distributed infrastructure
    across hardware and software.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Another 10x opportunity in matching application balance points — that is, the
    ratio between different system resources such as compute, accelerators, memory,
    storage, and network — through software-defined infrastructure.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A more than 10x opportunity in next-generation accelerators and segment-specific
    hardware components relative to traditional one-size-fits-all, general-purpose
    computing architectures.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Finally, there is a hard-to-quantify but absolutely critical opportunity to
    improve developer productivity while simultaneously delivering substantially improved
    reliability and security.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Combining these trends, we are on the cusp of yet another dramatic 1000x efficiency
    gain over the next epoch that will define the next generation of infrastructure
    services and enable the next generation of computing services, likely centering
    around breakthroughs in multimodal models and generative AI. The opportunity to
    define, design, and deploy what computing means for the next generation does not
    come along very often, and the tectonic shifts in this fifth epoch promise perhaps
    the biggest technical transformations and challenges to date, requiring a level
    of responsibility, collaboration and vision perhaps not seen since the earliest
    days of computing.
  prefs: []
  type: TYPE_NORMAL
