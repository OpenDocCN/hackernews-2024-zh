<!--yml

类别：未分类

日期：2024年5月27日14:44:40

-->

# 无搜索的国际象棋大师级别 · GitHub

> 来源：[https://gist.github.com/yoavg/8b98bbd70eb187cf1852b3485b8cda4f](https://gist.github.com/yoavg/8b98bbd70eb187cf1852b3485b8cda4f)

# 无搜索的国际象棋大师级别：建模选择及其影响

[](#grand-master-level-chess-without-search-modeling-choices-and-their-implications)

Yoav Golderg，2024年2月。

* * *

谷歌DeepMind的研究人员发布了一篇关于一个学习系统的[论文](https://arxiv.org/pdf/2402.04494.pdf)，该系统能够在大师级别上玩闪电国际象棋，而不使用搜索。这很有趣，也很具有想象力，因为到目前为止，无论基于机器学习还是不是，都使用了搜索组件。^([1](#user-content-fn-1-1db3ccd16c3fe9d8fc359c6d4aaee58b))

事实上，当我阅读这篇论文时的第一反应是发推文 `哇，太疯狂和有趣了`。我仍然觉得这很疯狂和有趣，但在更仔细地阅读后，它可能并不像我最初想象的那么疯狂和有趣。Twitter、Reddit等的许多反应都非常印象深刻，涉及到AI系统预期学习能力的影响、神经网络能够从观察中学习语义等等，这些真的过于夸张了。论文并没有声称任何这些，但仍然被许多读者所感知和谈论。

另一方面，对于机器学习能力的深远断言而言，许多反应则转向于减少基于事实，即此成就仅限于快速移动、严格时间限制的闪电国际象棋，玩家没有太多时间考虑每一步棋，而非整场国际象棋比赛。

我也不是完全印象深刻，但我将从一个与关注闪电性质不同的角度来处理它。

简言之，一个天真的读者（我在第一次阅读时也是这样的天真读者）可能得出以下结论：

DeepMind展示了一种机器学习模型，它满足以下所有条件：

1.  学习并能够遵循国际象棋的规则。

1.  学习并能够以大师级别玩游戏。

1.  纯粹通过观察国际象棋比赛及其结果进行学习。

1.  使用纯ML系统学习，没有在训练过程中对游戏特定偏见进行编码。

特别是，认为1-4点导致人们对其他学习系统（例如大型语言模型）的结论，例如“如果一个变压器网络可以仅通过观察游戏来学习国际象棋的规则/语义，包括执行搜索的能力，那么这进一步证明我们可能能够仅通过观察书面文本来学习语言的真正语义”。

然而，**这些观点都不成立**！公平地说，论文并没有宣称它们。它写得很好，没有过度宣称（除了声称一个没有搜索的结果系统...我们会谈到这一点）。在更仔细地阅读和思考后，人们实际上可以意识到：

+   点(1)和(2)没有实现，并且实际上在论文中使用的学习设置中根本不可能实现。

+   此外，点(1)和(2)在*任何*遵循点(3)和(4)的设置中可能都不可行。

+   点(3)和(4)不受论文中使用的设置支持。

所以，这篇论文及其结果并不支持关于其他学习场景和变压器从形式中学习语义能力的任何进一步主张或假设。

尽管如此，在阅读论文或其摘要时，许多读者确实考虑到了上述(1-4)点（如果你没有，现在可以停止阅读）。接下来，我将深入探讨为什么(1-4)点不成立。在更好地理解当前论文的基础上，我认为这也是一个有用的演示，展示了看似中立和自然的建模选择以及数据表示实际上对实际学习内容以及其原因有重大影响。

研究人员从在线国际象棋竞技场获取了1000万场人类对弈棋局。这些对应大约500百万个棋盘位置。对于每个棋盘位置和合法移动（总计略高于150亿对），他们要求Stockfish 16引擎基于每步最多0.05秒的分析给出“获胜概率”的估计。然后训练了一个基于变压器的模型，其输入是棋盘位置+移动，目标输出是移动的获胜概率。^([2](#user-content-fn-2-1db3ccd16c3fe9d8fc359c6d4aaee58b)) 这被称为动作价值预测。^([3](#user-content-fn-3-1db3ccd16c3fe9d8fc359c6d4aaee58b))

从概念上讲^([4](#user-content-fn-4-1db3ccd16c3fe9d8fc359c6d4aaee58b))，随后使用了以下简单的过程来玩游戏：在每个回合，通过训练模型让其查看棋盘位置+每个可能的移动，并预测这个棋盘+移动对的获胜概率。然后选择具有最高获胜概率的移动。就是这样。仅仅是模型预测，没有向前搜索。

这一过程被证明非常强大，在与人类在闪电国际象棋中对战时达到了大师级水平。

这是令人印象深刻的。AI系统只能查看棋盘位置及其结果，并利用这一点学习策略，使其能够在应用时反复选择出强力着法，强到足以击败非常强大的人类选手。这不是单纯的记忆：与人类的比赛中包含了系统在训练期间未见过的许多棋盘位置。系统学会了从已见到的位置泛化到这些新颖的位置，并仍然建议出能导致胜利的强力着法。而且所有这些都是在"不预先查看"的情况下完成的。

这个结果的一个可能解释是，我在网上见过很多人提出的（而且我自己也曾有过类似的想法）：

```
The system learned to play chess from observations.
It learned the rules of the games and how to play strong moves
just by looking at (many) positions from chess games,
without any extra domain knowledge,
and without self-play or mechanisms for thinking-ahead. 
```

上述（1-4）点是同一解释的另一种变体。然而，这种解释*是错误的*。没有一点是真的。

在理解原因之前，让我们稍作偏离，讨论从观察1000万场比赛中学习棋局模式涉及的规模，以及它如何与人类学习玩棋的方式相关。

## 人类学习方式非常不同。

[](#humans-learn-very-differently)

系统从1000万场比赛中学习了。那很多。假设有一个假想的极其专注的职业人类棋手，每天学习国际象棋16小时，并能分析每一局棋，一年365天。这样的棋手将在大约28.5年内研究完这1000万场比赛。如果这位棋手不是1分钟分析一场，而是5分钟分析一场，他们将需要142年来研究这1000万场比赛。有13岁的国际象棋大师级别的棋手。因此很明显人类学习方式不同，并且效率要高得多。至少在学习过程中，他们可能对游戏有一种非常不同的知识。稍后我们会回到这个计算。

## 一切都是建模选择。建模选择影响可以学到的内容。

[](#everything-is-a-modeling-choice-modeling-choices-affect-what-can-be-learned)

作者们显然选择了一个中立的表征和任务：输入是棋盘位置，输出是移动的获胜概率。简单、自然、"中立"。然而请注意，这仍然*是一个选择*，而且，更重要的是，这个选择（正如本文第5节“讨论”中所讨论的）导致了一个系统，实际上并没有学习游戏的规则（点1），也*不能*学习游戏的规则，无论学习者多么强大。这种建模选择也阻止了*任何学习者*像国际象棋大师一样下棋（点2）。让我们看看为什么，使用论文中讨论的两种失败模式。第一种演示了无法学习游戏规则。第二种演示了无法像国际象棋大师一样学习下棋。这些都是建模选择后果的非常好的例子。

### 无法学习国际象棋的规则。

[](#inability-to-learn-the-rules-of-chess)

论文提到该模型无法遵循“连续三次不重复相同棋盘位置”的规则（即“三次重复规则”）。实际上，仅仅查看单个棋盘位置并预测下一步是不可能学会不连续重复过去的棋盘位置三次，因为你无法获取到之前的棋盘位置。这里面信号不足。

为了真正从观察中学习国际象棋，包括这个规则，你需要一个能够提供移动历史访问权限的输入表示方法。^([5](#user-content-fn-5-1db3ccd16c3fe9d8fc359c6d4aaee58b)) 要编码历史，使用“自然”的选项是*一系列棋盘状态*。^([6](#user-content-fn-6-1db3ccd16c3fe9d8fc359c6d4aaee58b)) 现在，比较通过查看单个棋盘状态学习国际象棋并学习下一步移动，与通过查看一系列棋盘位置学习国际象棋并学习预测下一步的差异。如果你不了解这些游戏，后者显然更难。至少在预测下一步时，你需要学会忽略所有先前的棋盘（除了一些特殊情况，例如三次重复规则或王车易位规则，其中历史记录非常重要）。这并不是一件简单的事情，一个没有任何游戏知识的学习模型很可能会捕捉到涉及先前棋盘位置且不正确的虚假模式。通过将输入限制为单个棋盘位置，*模型设计者*正在使用*他们对游戏的知识*向学习过程*注入游戏知识*：即在大多数情况下，先前的移动是无关紧要的知识。这看起来并不重要，但对于没有任何知识的学习来说，这是重要的。因此，专注于学习从单个棋盘位置到最佳移动的映射是*一个建模选择*，使问题*更容易学习*，但也*不可能学习其中的某些方面*。这是一个权衡，并且向学习模型注入了重要的知识和“归纳偏差”。这不是一个无辜的、中立的决定。没有决定是。

**观察是否可以学习？** 另一个有趣的问题是，即使你可以访问整个比赛历史（所以没有信号缺失），并且有一个完美的学习者（不会被虚假模式分散注意力），从观察人类对人类比赛中仅仅通过观察学习三次重复规则是非常具有挑战性的，甚至在限制自己只看强大的人类玩家的比赛时更加困难：玩家们知道这个规则，知道这样的重复会导致平局，因此他们在比赛中避免三次重复。很少有比赛示例会表现出这种行为，而少数存在的例子，是在两位玩家意识到比赛“僵局”并希望平局的情况下。在这种情况下，正确推断出通用的三次重复规则将非常困难，如果不是不可能的话，也学不会在强势局面下不连续重复相同的棋盘位置三次。（这意味着知道规则的有能力的玩家和这个模型的盲点，可以在模型处于非常强势领先位置时误导模型陷入平局）。

我发现这是一个很好的展示语义规则的例子，这种规则在这种情况下不能从观察中学习，因为语义影响了观察到的状态。值得注意的是，大型语言模型也仅仅通过观察学习，并反映可能由于各种原因无法学习的语义。

### 无法在真正的大师水平上进行比赛。

[](#inability-to-play-at-real-grandmaster-level)

大师级别的人类玩家对游戏了解很深。他们不会展示非常弱的玩家行为。从某种意义上说，我们已经看到这样的盲点：不知道三次重复规则，模型可能被操纵成平局，尽管它实际上处于领先位置。然而，下一个例子更加棘手，直接源于建模选择：

在一个玩家的位置比其他玩家强得多的情况下，“获胜概率”可以对几个不同的移动都达到100%：每个移动都会导致最终赢得比赛。然而，有些移动将使用比其他更少的未来移动来实现胜利。在我们的案例中，模型无法在这些移动之间做出选择：它只知道最大化单一移动的获胜概率，甚至不“知道”这个移动将跟随未来的移动，不知道如何赢得比赛，甚至不知道什么是赢。

因此，模型可能选择了一个糟糕的步骤，需要更多的未来步骤来导致胜利。在采取这样的步骤之后，它可能再次选择一个糟糕的步骤，这将需要很长时间才能取得胜利。事实上，它没有“计划”的概念，也没有使自己接近胜利的概念。在一些极端情况下，模型可能会永远卡在确保获胜的情况下，但模型永远也达不到这一点。例如，考虑一下，模型有两个战车，而对手有一个国王。这是一个经典的将军的例子，因此每一步都将有100%的获胜概率。然而，为了真正取胜，必须通过特定的战车移动系列，而不只是随意移动它们。但是棋盘足够大，可以让战车随意移动。这将是一种荒谬的行为，与非常弱的棋手相关联，这在大师级别永远不会发生。然而，按照本文的设置训练的模型无法避免这种情况，并且真正有可能展示出这种情况。

*建模决策*削弱了学习者，并*阻止了*它展示大师级别的表现。这个问题中的建模决策是学习将移动（或棋盘）与它们的“获胜概率”相关联，而不考虑达到胜利所需步骤的估计数。本文提出了一个特别的解决方案，实际上与其主要声明相矛盾：结果的象棋程序确实使用了搜索。^([7](#user-content-fn-7-1db3ccd16c3fe9d8fc359c6d4aaee58b))

建模选择很重要。

## 本文的设置并不是通过观察学习。

[](#the-papers-setup-is-not-learning-from-observations)

因此，我们看到建模选择阻止了模型学习游戏规则（而在任何设置下，有些规则将很难从观察比赛中学习）。这解决了上述的第（1）和第（2）点，并在某种程度上也处理了第（4）点：建模选择确实嵌入了游戏特定信息，而其他选择可能会使学习变得更加困难。

现在让我们来解决第三点：本文的设置**并不是通过观察比赛学习**。在我看来，这意味着，试图从本文中推断语言模型学习的任何尝试都是徒劳的。我现在将描述一些使从观察比赛学习变得非常困难的原因，以及本文实际上做了什么。

### 从观察学习意味着什么？

[](#what-does-it-mean-to-learn-from-observations)

考虑从观察中学习象棋的含义。你看到一场象棋比赛，在结束时被告知谁赢了。你可以看尽可能多的比赛，但你只能观看。从中你能学到关于游戏规则的什么？你能学到关于游戏的语义和策略的什么？学习玩得好需要什么？

学会玩家轮流行动很可能是一个容易捕捉的模式。^([8](#user-content-fn-8-1db3ccd16c3fe9d8fc359c6d4aaee58b)) 学习每个玩家的合法走法也很可能会比较容易（除了可能的王车易位可能会更难）。但是，如何学习游戏的语义呢？或者，让我们选择一个更容易的目标，学会做出好的走法，这将最大化你赢得比赛的机会？

为了学会选择好的走法，你可以例如，专注于每场比赛中获胜玩家的走法。现在，在每个棋盘位置上，你记录这位玩家做了什么走法。然后，你试图找到与棋盘状态和走法相关联的模式。然而，这里有一个大问题：这不一定（而且很可能不会）导致学会打得好和做出好的走法：并不是获胜玩家在他们的比赛中做的所有走法都是好走法。其中一些可能实际上是错误，但玩家成功从中恢复了。其他可能只是“一般般”的走法。通过观察序列及其结果来提取最终导致胜利的实际好走法是非常困难的。在强化学习（"RL"）术语中，这被称为“稀疏奖励”的情况，而识别导致稀疏奖励的实际好走法则被称为“信用分配问题”。信用分配可能是RL中*最*大的挑战之一，并且远未解决。而在RL中可能存在的任何解决方案，都假定你可以互动，提出走法并观察其结果。而仅允许观察而不允许互动的情况则要难得多，难得多得多。

忽略信用分配，只学习预测在每个棋盘位置上获胜玩家的走法，很可能会导致非常差和混乱的策略，尤其是当你对观察的游戏中的玩家没有任何了解时。（也许你观察到的玩家是一个弱者，只能因为他们对阵其他弱者而获胜？也许很多游戏都来自这样的弱者？在这种情况下，你将学会将棋盘位置与非常不起眼的走法关联起来。同样，也许游戏中大多数走法在未来可能导致胜利或失败，但是非常犹豫不决，而你学到的模式将被这种犹豫不决的走法而不是强势的获胜走法所主导？等等）。

所以，仅从观察中学习远非简单。当然，作者们意识到了这一点，并且甚至没有尝试从观察中学习。他们做了非常不同的事情。你注意到了吗？

### 论文实际做了什么？

[](#what-the-paper-actually-did)

本文中的设置并非通过观察学习。它没有查看游戏位置并试图学习和预测在某些位置下游戏的下一步。相反，它查看游戏位置，然后使用强大的国际象棋引擎，从这个位置开始扩展可能的游戏延续树，涵盖未来数十万甚至数百万个移动。游戏引擎然后利用其对国际象棋的内部知识评估可能延续中获胜局面的百分比，这就是该位置的“获胜概率”。然后，学习者试图学习*这个*数字。这不是通过观察学习，而是通过超人类专家学习。

知道一个位置中的“获胜百分比”或“获胜概率”是非常困难的。大师级玩家可以根据他们的过去经验估计某些棋盘位置的这个数字（以及他们与国际象棋引擎的互动历史，包括查询它们用于许多不同位置的精确数字，以及分析这些数字和某些选择的赢得和失去的路径）。但如果没有被暴露于其他移动的许多正确估计，不知道游戏规则并且不能“玩出来”，或者观察其他人这样做，这几乎不可能给出一个关于移动的“获胜概率”的良好估计。在任何情况下，这远远超出了仅仅观察已玩游戏可得的内容。

因此，学习者确实没有通过观察学习，而是通过一个显著更丰富的信号进行学习，这是无法在没有对游戏非常强的知识的情况下获得的（作者称其为“蒸馏Stockfish 16的知识”。这是一个合适的名称）。

## 模型学习 vs 人类学习。

[](#model-learning-vs-human-learning)

上文提到，人类学习者比本文的学习方法要高效得多，因为以每分钟观察和分析10M场比赛的速度，每天16小时，将需要28.5年，而人类的效率要远远高于此。

但事实上，情况更为极端。对于每场比赛，机器学习者不仅查看了游戏，还查看了这些游戏中15B个棋盘移动对的获胜概率估计，并试图学习和复制这些获胜概率。对于人类学习者来说，产生这些获胜概率估计不仅耗时，而且几乎不可能，除非具备游戏规则的强大知识（或者可以访问编码这些规则的国际象棋引擎）。人类学习者显然学习**非常**不同，并且在学习的中间部分要高效得多（一旦超过某个水平，他们可能也会学习获胜概率。但这将建立在一个与仅在这些概率上训练的系统完全不同的基础上，并且还将查看显著较少的棋盘位置）。

我们看到，问题的编码方式从未是自然的或中立的，它总是在各种选择中进行建模，而这些选择都有其利弊。

在本文中，显著的建模选择包括：

+   将输入表示为单个棋盘位置，而不是一系列位置或走法。

+   将输出表示为获胜概率。

虽然论文中没有明确讨论，但这两种选择编码了游戏特定的知识，并显著简化了学习问题。然而，第一种选择也限制了模型正确学习游戏规则的能力，而第二种选择则通过阻止在某些非常容易获胜的情况下简洁高效地取胜，从而阻止了模型达到真正的国际象棋大师水平。

此外，第二种选择在训练过程中引入了大量的游戏知识，并导致了一种与通过观察学习截然不同的过程。虽然在游戏时没有搜索过程，但在训练中进行了*大量*的搜索，以及*大量*的细粒度的棋类知识。 （尽管作者们没有否认它，甚至在开篇就提到了这一点，但这并不是重点，初次阅读时易被不经意的读者忽略。或者至少，我忽略了。）

尽管最终结果确实令人印象深刻，但它也比许多读者所描绘的要逊色得多：该模型学会了从给定位置到强力走法的模式匹配，以一种可推广的方式。但它并未从观察中学习，也没有学习到实际的游戏规则，而且其学习方式很可能与任何人类完全不同。在很多方面，这更多地反映了国际象棋游戏的特性，而不是机器学习模型的学习能力。我们无法仅凭这项工作支持语言模型潜在强大能力的主张。

但如果你从这篇文章中得到一个信息，我要强调的不是“这个结果一般般”或者学习无搜索下下棋的具体内容，而是更一般性的观点：“一切都是设计选择。设计选择具有重大影响，并且经常在论文中被忽视。寻找重要但隐藏的设计选择和影响是一项关键的论文阅读技能。”

* * *
