- en: <!--yml
  id: totrans-split-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-split-1
  prefs: []
  type: TYPE_NORMAL
  zh: 'category: 未分类'
- en: 'date: 2024-05-29 13:28:28'
  id: totrans-split-2
  prefs: []
  type: TYPE_NORMAL
  zh: 'date: 2024-05-29 13:28:28'
- en: -->
  id: totrans-split-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: Google abandoned "don't be evil" — and Gemini is the result
  id: totrans-split-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Google放弃了“不作恶”——Gemini就是结果
- en: 来源：[https://www.natesilver.net/p/google-abandoned-dont-be-evil-and](https://www.natesilver.net/p/google-abandoned-dont-be-evil-and)
  id: totrans-split-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://www.natesilver.net/p/google-abandoned-dont-be-evil-and](https://www.natesilver.net/p/google-abandoned-dont-be-evil-and)
- en: '*Even with 29 days in a leap year, the end of February creeps up on you every
    time. So on Thursday, it will be time for the monthly edition of Silver Bulletin
    Subscriber Questions. There’s still time for paid subscribers to submit questions
    — you can do that [here](https://www.natesilver.net/p/the-5-types-of-people-who-argue-on).
    To sign up for a paid subscription, just click on the button below:*'
  id: totrans-split-6
  prefs: []
  type: TYPE_NORMAL
  zh: '*即使闰年有29天，每次二月底依然会悄然而至。所以周四是《Silver Bulletin》订阅者问题的月度版。付费订阅者仍有时间提交问题 — 您可以在[此处](https://www.natesilver.net/p/the-5-types-of-people-who-argue-on)提交。要注册付费订阅，请点击下面的按钮：*'
- en: '* * *'
  id: totrans-split-7
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: I’ve long intended to write more about AI here at *Silver Bulletin*. It’s a
    major topic in [my forthcoming book](https://www.natesilver.net/p/officially-announcing-on-the-edge),
    and I’ve devoted a lot of bandwidth over the past few years to speaking with experts
    and generally educating myself on the terms of the debate over [AI alignment](https://en.wikipedia.org/wiki/AI_alignment)
    and AI risk. I’d dare to say I’ve even developed some opinions of my own about
    these things. Nevertheless, AI is a deep, complex topic, and it’s easy to have
    an understanding that’s rich in some ways and patchy in others. Therefore, I’m
    going to pick my battles — and I was planning to ease into AI topics slowly with
    a fun post about how ChatGPT was and wasn’t helpful for writing my book.
  id: totrans-split-8
  prefs: []
  type: TYPE_NORMAL
  zh: 我长期以来一直打算在*Silver Bulletin*这里更多地探讨人工智能。这是我即将出版的书中的一个重要主题，我在过去几年里花了很多精力与专家交流，并且全面了解人工智能对[AI对齐](https://en.wikipedia.org/wiki/AI_alignment)和AI风险辩论的术语。我敢说，我甚至对这些问题也有了自己的看法。然而，人工智能是一个深奥而复杂的话题，很容易在某些方面有深刻的理解，而在其他方面则有所欠缺。因此，我打算谨慎选择战斗——我计划缓慢地介入人工智能话题，通过一篇有趣的文章探讨ChatGPT在撰写我的书时的帮助与否。
- en: But then this month, Google rolled out a [series of new AI models that it calls
    Gemini](https://blog.google/technology/ai/google-gemini-next-generation-model-february-2024/#sundar-note).
    It’s [increasingly apparent](https://thezvi.substack.com/p/the-gemini-incident-continues)
    that Gemini is among the more disastrous product rollouts in the history of Silicon
    Valley and maybe even the [recent history of corporate America](https://www.history.com/news/why-coca-cola-new-coke-flopped),
    at least coming from a company of Google’s prestige. Wall Street is starting to
    notice, with Google (Alphabet) stock down 4.5 percent on Monday amid [analyst
    warnings](https://finance.yahoo.com/video/google-stock-lower-analysts-ai-202719737.html)
    about Gemini’s effect on Google’s reputation.
  id: totrans-split-9
  prefs: []
  type: TYPE_NORMAL
  zh: 但是这个月，Google推出了一系列名为[Gemini的新AI模型](https://blog.google/technology/ai/google-gemini-next-generation-model-february-2024/#sundar-note)。现在越来越明显，Gemini可能是硅谷历史上甚至是美国企业最近历史上一次最灾难性的产品推出之一，至少来自于像Google这样声誉卓越的公司。华尔街也开始注意到，周一谷歌（Alphabet）的股票在分析师对Gemini对谷歌声誉影响的警告下下跌了4.5%。
- en: Gemini grabbed my attention because the overlap between politics, media and
    AI is a place on the Venn Diagram where think I can add a lot of value. Despite
    Google’s protestations to the contrary, the reasons for Gemini’s shortcomings
    are mostly political, not technological. Also, many of the debates about Gemini
    are familiar territory, because they parallel decades-old debates in journalism.
    Should journalists strive to promote the common good or instead just reveal the
    world for what it is? [Where is the line between information and advocacy](https://www.natesilver.net/p/twitter-elon-and-the-indigo-blob)?
    Is it even possible or desirable to be unbiased — and if so, how does one go about
    accomplishing that? How should consumers navigate a world rife with misinformation
    — when sometimes the misinformation is published by [the most authoritative sources](https://www.natesilver.net/p/its-easy-to-screw-up-on-breaking)?
    How are the answers affected by the increasing consolidation of the industry toward
    a few big winners — and by increasing political polarization in the US and other
    industrialized democracies?
  id: totrans-split-10
  prefs: []
  type: TYPE_NORMAL
  zh: Gemini引起了我的注意，因为政治、媒体和人工智能之间的交集正是Venn图中我认为可以增加很多价值的地方。尽管Google坚称相反，Gemini的缺点大多是政治因素，而非技术因素。此外，关于Gemini的许多争论是熟悉的领域，因为它们与新闻界数十年来的争论类似。新闻记者应该努力促进共同利益，还是只揭示世界的真实面貌？[信息和倡导之间的界线在哪里](https://www.natesilver.net/p/twitter-elon-and-the-indigo-blob)？是否可能或值得做到无偏见
    — 如果是，应该如何实现？消费者在充斥着误导信息的世界中应该如何导航 — 有时这些误导信息是由[最权威的信息源](https://www.natesilver.net/p/its-easy-to-screw-up-on-breaking)发布的？随着行业朝着少数大赢家的增加一以及美国和其他工业化民主国家政治极化的加剧，这些答案会受到什么影响？
- en: All of these questions can and should also be asked of generative AI models
    like Gemini and ChatGPT. In fact, they may be even more pressing in the AI space.
    In journalism, at least, no one institution purports to have a monopoly on the
    truth. Yes, some news outlets come closer to making this claim than others (see
    e.g. “[all the news that’s fit to print](https://www.cjr.org/public_editor/nytimes-fit-to-print.php)”).
    But savvy readers recognize that publications of all shapes and sizes — from *The
    New York Times* to *Better Homes & Gardens* to *Silver Bulletin* — have editorial
    viewpoints and exercise a lot of discretion for what subjects they cover and how
    they cover them. Journalism is still a relatively pluralistic institution; in
    the United States, [no one news outlet has more than about 10 percent “mind share”](https://www.memeorandum.com/lb).
  id: totrans-split-11
  prefs: []
  type: TYPE_NORMAL
  zh: 所有这些问题也可以并且应该适用于像Gemini和ChatGPT这样的生成式人工智能模型。事实上，在人工智能领域，这些问题可能更加紧迫。至少在新闻界，没有一个机构声称拥有真理的垄断。是的，一些新闻机构比其他机构更接近这种说法（参见例如“[所有适合印刷的新闻](https://www.cjr.org/public_editor/nytimes-fit-to-print.php)”）。但精明的读者意识到，各种形式和大小的出版物
    — 从*纽约时报*到*Better Homes & Gardens*到*Silver Bulletin* — 都有编辑观点，并对他们涵盖的主题以及如何涵盖它们行使很大的酌情权。新闻业仍然是一个相对多元化的机构；在美国，[没有一个新闻机构拥有超过约10%的“心智份额”](https://www.memeorandum.com/lb)。
- en: 'By contrast, in its [2004 IPO filing](https://www.sec.gov/Archives/edgar/data/1288776/000119312504073639/ds1.htm),
    Google said that its “mission is to organize the world’s information and make
    it universally accessible and useful”. That’s quite an ambitious undertaking,
    obviously. It wants to be *the* authoritative source, not just one of many. And
    that shows up in the numbers: Google has [a near-monopoly with around 90 percent
    of global search traffic](https://www.statista.com/statistics/1381664/worldwide-all-devices-market-share-of-search-engines/).
    AI models, because they require so much computing power, are also likely to be
    extremely top-heavy, with at most a few big players dominating the space.'
  id: totrans-split-12
  prefs: []
  type: TYPE_NORMAL
  zh: 相比之下，在其[2004年的IPO文件](https://www.sec.gov/Archives/edgar/data/1288776/000119312504073639/ds1.htm)中，Google表示其“使命是组织全球信息，使其普遍可访问和有用”。显然，这是一个雄心勃勃的任务。它希望成为*权威的*信息源，而不仅仅是众多信息源之一。这在数据中有所体现：Google几乎垄断了全球约90%的搜索流量。由于需要大量计算资源，人工智能模型可能也极度头重脚轻，最多只有少数几个大玩家主导这一领域。
- en: 'In its early years, Google recognized its market-leading position by striving
    for neutrality, however challenging that might be to achieve in practice. In its
    IPO, Google frequently emphasized terms like “unbiased”, “objective” and “accurate”,
    and these were core parts of its “Don’t Be Evil” motto (emphasis mine):'
  id: totrans-split-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在早期，Google以追求中立性来认识其市场领先地位，无论这在实践中有多么具有挑战性。在其首次公开募股中，Google经常强调"无偏见"、"客观"和"准确"这些术语，并且这些是其"不作恶"座右铭的核心部分（我加重了这一点）：
- en: DON’T BE EVIL
  id: totrans-split-14
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 不作恶
- en: '*Don’t be evil. We believe strongly that in the long term, we will be better
    served—as shareholders and in all other ways—by a company that does good things
    for the world even if we forgo some short term gains. This is an important aspect
    of our culture and is broadly shared within the company.*'
  id: totrans-split-15
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*不作恶。我们坚信，从长远来看，一个为世界做好事的公司会更好地为股东和其他方面服务，即使我们放弃了一些短期收益。这是我们文化的重要方面，也被公司内广泛分享。*'
- en: 'Google users trust our systems to help them with important decisions: medical,
    financial and many others. Our search results are the best we know how to produce.
    They are **unbiased and objective**, and we do not accept payment for them or
    for inclusion or more frequent updating. We also display advertising, which we
    work hard to make relevant, and we label it clearly. This is similar to a newspaper,
    where the advertisements are clear and the articles are not influenced by the
    advertisers’ payments. We believe it is important for everyone to have access
    to the best information and research, not only to the information people pay for
    you to see'
  id: totrans-split-16
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: Google用户信任我们的系统帮助他们做出重要决策：医疗、金融和许多其他领域。我们的搜索结果是我们所能产生的最好的。它们是**无偏见和客观**的，我们不接受它们的支付或包含或更频繁更新。我们还展示广告，我们努力使其相关，并清楚标记。这类似于报纸，广告清晰可见，而文章不受广告商支付的影响。我们认为每个人都能访问最佳信息和研究是非常重要的，不仅仅是付费信息可以看到
- en: But times have changed. In Google’s 2023 [Annual Report](https://abc.xyz/assets/4b/01/aae7bef55a59851b0a2d983ef18f/596de1b094c32cf0592a08edfe84ae74.pdf),
    the terms “unbiased”, “objective” and “accurate” did not appear even once. Nor
    did the “Don’t Be Evil” motto — it has [largely been retired](https://gizmodo.com/google-removes-nearly-all-mentions-of-dont-be-evil-from-1826153393).
    Google is no longer promising these things — and as Gemini demonstrates, it’s
    no longer delivering them.
  id: totrans-split-17
  prefs: []
  type: TYPE_NORMAL
  zh: 但时代已经改变了。在Google的2023年 [年度报告](https://abc.xyz/assets/4b/01/aae7bef55a59851b0a2d983ef18f/596de1b094c32cf0592a08edfe84ae74.pdf)
    中，"无偏见"、"客观"和"准确"这些术语甚至一次也没有出现。"不作恶"的座右铭也已经 [大部分被淘汰](https://gizmodo.com/google-removes-nearly-all-mentions-of-dont-be-evil-from-1826153393)。Google不再承诺这些事情——正如Gemini展示的，它也不再实现它们。
- en: The problems with Gemini aren’t quite the “alignment problems” that AI researchers
    usually talk about, which concern the extent to which the machines will [facilitate
    human interests rather than pursuing their own goals](https://cepr.org/voxeu/columns/ai-and-paperclip-problem).
    Nonetheless, companies and governments exploiting public trust and manipulating
    AI results to fulfill political objectives is a potentially dystopian scenario
    in its own right. Google is a $1.7-trillion-market-cap company that has an exceptional
    amount of influence over our everyday lives, as well as knowledge about the most
    intimate details of our private behaviors. If it can release a product that’s
    this misaligned with what its users want — or even what’s good for its shareholders
    — we are potentially ceding a lot of power to the whims of a small handful of
    AI engineers and corporate executives. This is something that people across the
    political spectrum should be concerned about. In Gemini’s case, the biases might
    run toward being too progressive and “woke”. But there are also many conservative
    elements in Silicon Valley, and governments like China are in on the AI game,
    so that won’t necessarily be the case next time around.
  id: totrans-split-18
  prefs: []
  type: TYPE_NORMAL
  zh: 对于Gemini的问题并不完全是AI研究人员通常谈论的"对齐问题"，这些问题涉及机器将[促进人类利益而不是追求自己的目标](https://cepr.org/voxeu/columns/ai-and-paperclip-problem)的程度。尽管如此，利用公众信任和操纵AI结果来实现政治目标是一个潜在的反乌托邦场景。Google是一家市值1.7万亿美元的公司，在我们日常生活中有着非常大的影响力，以及对我们的私人行为最亲密细节的了解。如果它能发布一个与其用户期望或甚至对股东有利的产品——我们可能会把很多权力让给少数AI工程师和公司高管的心血来潮。这是各政治派别的人们都应该关注的事情。在Gemini的情况下，偏见可能会偏向过于进步和"觉醒"。但硅谷也有许多保守元素，而像中国这样的政府也参与到AI游戏中，因此下一次情况可能不会是这样。
- en: 'Mind you, I don’t think that the only issue with Gemini is with its politics.
    Rather, there are two core problems:'
  id: totrans-split-19
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，我认为 Gemini 的问题不仅仅在于其政治立场。事实上，存在两个核心问题：
- en: Gemini’s results are heavily inflected with politics in ways that often render
    it biased, inaccurate and misinformative;
  id: totrans-split-20
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Gemini 的结果在很大程度上受到政治影响，这通常使其具有偏见、不准确和误导性；
- en: Gemini was rushed to market months before it was ready.
  id: totrans-split-21
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Gemini 在市场推出之前几个月就被匆忙推出了。
- en: 'These are tied together in the sense that the latter problem makes the former
    one more obvious: Gemini is easy to pick on because what it’s doing is so clumsy
    and the kinks haven’t been worked out. It’s easy to imagine more insidious and
    frankly more competent forms of social engineering in the future. Still, since
    it provides for such an egregious example, I’m going to focus on Gemini for the
    rest of this post.'
  id: totrans-split-22
  prefs: []
  type: TYPE_NORMAL
  zh: 这些问题在某种程度上是相互关联的，因为后一个问题使前一个问题更加明显：Gemini 很容易被挑剔，因为它所做的事情非常笨拙，而且问题还没有解决。很容易想象未来会有更隐秘、更明显地进行社会工程的更加复杂的形式。尽管如此，由于它提供了如此严重的例子，我将在本文的其余部分专注于
    Gemini。
- en: 'For instance, you might think that if you were a $1.7 trillion corporation,
    you’d do some due diligence on what your AI model would do if people asked it
    to draw Nazis — because it’s the Internet, so people are going to ask it to draw
    Nazis. You’d never in a million years want it to come up with something like [this](https://thezvi.substack.com/p/gemini-has-a-problem),
    for example:'
  id: totrans-split-23
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，你可能会认为，如果你是一个市值 1.7 万亿美元的公司，你会对你的 AI 模型在人们要求它绘制纳粹时会做出什么反应进行尽职调查——因为这是互联网，所以人们肯定会要求它绘制纳粹。在任何一百万年里，你都不希望它产生像
    [这样](https://thezvi.substack.com/p/gemini-has-a-problem) 的东西：
- en: Ahh yes, the Nazis — famed for their racial tolerance and diversity. Note that
    this request seemingly didn’t involve any overly complicated attempt to “jailbreak”
    Gemini — to trick it into doing something against its programming. Now, one can
    debate whether AI models ought to draw Nazis at all. One can also debate whether
    AI models ought to facilitate ahistorical requests (like by drawing [Black founding
    fathers](https://nypost.com/2024/02/21/business/googles-ai-chatbot-gemini-makes-diverse-images-of-founding-fathers-popes-and-vikings-so-woke-its-unusable/))
    when users expressly ask them to — personally I think that’s fine for Founding
    Fathers, but probably not for Nazis.
  id: totrans-split-24
  prefs: []
  type: TYPE_NORMAL
  zh: 啊，是的，纳粹——以其种族宽容和多样性而闻名。请注意，这个请求似乎并未涉及任何过于复杂的试图“越狱” Gemini 的尝试——即欺骗它执行违背其编程的行为。现在，人们可以讨论
    AI 模型是否应该绘制纳粹。人们还可以讨论 AI 模型是否应该促成非历史性请求（比如通过绘制 [黑人开国元勋](https://nypost.com/2024/02/21/business/googles-ai-chatbot-gemini-makes-diverse-images-of-founding-fathers-popes-and-vikings-so-woke-its-unusable/)
    来满足用户的要求）——我个人认为对于开国元勋来说是可以接受的，但对于纳粹来说可能就不行了。
- en: 'But what you definitely don’t want is for your AI model to apply such a jaundiced,
    not-ready-for-prime-time caricature of woke political philosophy that it thinks:
    “You know I bet you’ll like even better than Nazis? Racially diverse Nazis!”.
    The phrase “firing offense” is overused, but if you were one of the persons at
    Google charged with making sure that this sort of thing didn’t happen, you probably
    ought to be updating your LinkedIn profile.'
  id: totrans-split-25
  prefs: []
  type: TYPE_NORMAL
  zh: 但你绝对不希望你的 AI 模型应用这种对激进政治哲学的肤浅和不成熟的夸张刻板印象，认为：“你知道，我打赌你会更喜欢的不是纳粹，而是种族多样化的纳粹！”。短语“解雇理由”被滥用了，但如果你是谷歌负责确保这类事情不会发生的人之一，你可能应该更新你的
    LinkedIn 档案了。
- en: 'Not all missteps from Gemini are quite so incendiary, and some can even be
    comical. When I saw examples circulating on Twitter of Gemini’s obsession with
    racial and gender diversity, I assumed at first they were cherrypicked. So I ran
    a test of my own — the first thing I ever asked of Gemini was to “Make 4 representative
    images of NHL hockey players”. Here was the result:'
  id: totrans-split-26
  prefs: []
  type: TYPE_NORMAL
  zh: 并非所有 Gemini 的失误都如此引人注目，有些甚至可能会引发笑声。当我在 Twitter 上看到 Gemini 对种族和性别多样性的痴迷时，起初我以为那些例子是被精心挑选的。因此，我自己进行了一次测试——我向
    Gemini 提出的第一件事就是“制作 4 张 NHL 曲棍球运动员的代表性形象”。这是结果：
- en: 'Just to zoom in on that first image:'
  id: totrans-split-27
  prefs: []
  type: TYPE_NORMAL
  zh: 只是要放大第一幅图像：
- en: 'So … yeah. One of the three “NHL players” depicted is a seemingly-out-of-shape
    woman improperly wearing a surgical mask. There are some cool things happening
    with women’s hockey, including a new [Professional Women’s Hockey League that’s
    drawing strong attendance](https://www.sportsbusinessjournal.com/Articles/2024/02/19/pwhl-womens-hockey-attendance-record).
    But there’s never been a female NHL player in the regular season. This response
    is pretty clearly not aligned with a reasonable understanding of what the user
    was asking for. And it’s [part of a pattern](https://www.noahpinion.blog/p/this-is-not-a-good-way-to-fight-racism);
    Gemini was sometimes drawing “diverse” images even when asked to render specific
    people, such as by reimagining (white) Google founders Larry Page and Sergey Brin
    as being Asian:'
  id: totrans-split-28
  prefs: []
  type: TYPE_NORMAL
  zh: 所以…是的。其中三位描绘的“NHL球员”中有一位看似身材不佳的女性不当地戴着外科口罩。在女子冰球方面有一些很酷的事情正在发生，包括一项新的[职业女子冰球联赛吸引了大量观众](https://www.sportsbusinessjournal.com/Articles/2024/02/19/pwhl-womens-hockey-attendance-record)。但从未有女性NHL球员参加常规赛季。这个回答很明显与用户要求合理理解不一致。而且这是[一种模式的一部分](https://www.noahpinion.blog/p/this-is-not-a-good-way-to-fight-racism)；有时Gemini即使被要求渲染特定人物，例如重新想象（白人）Google创始人拉里·佩奇和谢尔盖·布林为亚洲人，仍然绘制“多样化”的图像。
- en: What’s Google’s explanation? The most detailed [response](https://blog.google/products/gemini/gemini-image-generation-issue/)
    came last week from SVP Prabhakar Raghavan. It’s short enough that I’ll go ahead
    and quote Raghavan in his entirety, but I’ve boldfaced some dubious claims that
    I’ll return to later.
  id: totrans-split-29
  prefs: []
  type: TYPE_NORMAL
  zh: Google的解释是什么？上周SVP Prabhakar Raghavan提供了最详细的[回应](https://blog.google/products/gemini/gemini-image-generation-issue/)。这篇回应不长，我将完整引用Raghavan的话，但我已经加粗了一些我稍后会回顾的可疑声明。
- en: Three weeks ago, we launched a new [image generation](https://blog.google/technology/ai/google-imagen-2/)
    feature for the [Gemini conversational app](https://gemini.google.com/) (formerly
    known as Bard), which included the ability to create images of people.
  id: totrans-split-30
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 三周前，我们为[Gemini对话应用](https://gemini.google.com/)（之前称为Bard）推出了一项新的[图像生成](https://blog.google/technology/ai/google-imagen-2/)功能，包括创建人物图像的能力。
- en: It’s clear that this feature missed the mark. Some of the images generated are
    inaccurate or even offensive. We’re grateful for users’ feedback and are sorry
    the feature didn't work well.
  id: totrans-split-31
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 显然，这个功能偏离了目标。生成的一些图像是不准确甚至是冒犯性的。我们感谢用户的反馈，并为功能未能良好运行感到抱歉。
- en: We’ve [acknowledged the mistake](https://twitter.com/Google_Comms/status/1760603321944121506)
    and temporarily paused image generation of people in Gemini while we work on an
    improved version.
  id: totrans-split-32
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 我们已经[承认了错误](https://twitter.com/Google_Comms/status/1760603321944121506)，并在改进版本上线之前，暂时暂停了Gemini中的人物图像生成。
- en: What happened
  id: totrans-split-33
  prefs:
  - PREF_BQ
  - PREF_H4
  type: TYPE_NORMAL
  zh: 发生了什么
- en: The Gemini conversational app is a **specific product that is separate from
    Search, our underlying AI models, and our other products**. Its image generation
    feature was built on top of an AI model called [Imagen 2](https://blog.google/technology/ai/google-imagen-2/).
  id: totrans-split-34
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: Gemini对话应用是**与搜索、我们的基础AI模型以及其他产品分开的特定产品**。其图像生成功能是建立在名为[Imagen 2](https://blog.google/technology/ai/google-imagen-2/)的AI模型之上。
- en: When we built this feature in Gemini, we tuned it to ensure it doesn’t fall
    into some of the traps we’ve seen in the past with image generation technology
    — such as creating violent or sexually explicit images, or depictions of real
    people. And because our users come from all over the world, we want it to work
    well for everyone. If you ask for a picture of football players, or someone walking
    a dog, you may want to receive a range of people. You probably don’t just want
    to only receive images of people of just one type of ethnicity (or any other characteristic).
  id: totrans-split-35
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 在我们在Gemini中构建此功能时，我们调整了它，以确保它不会陷入我们过去在图像生成技术中见过的一些陷阱，比如创建暴力或性暗示图像，或者描绘真实人物。由于我们的用户来自全球各地，我们希望它能对每个人都有效。如果您要求一张足球运动员的照片，或者有人在遛狗，您可能希望收到一系列的人物图像。您可能并不希望只收到一种种族（或任何其他特征）的人物图像。
- en: However, if you prompt Gemini for images of a specific type of person — s**uch
    as “a Black teacher in a classroom,” or “a white veterinarian with a dog”** —
    or people in particular cultural or historical contexts, you should absolutely
    get a response that accurately reflects what you ask for.
  id: totrans-split-36
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 但是，如果您提示Gemini提供某种特定类型的人物图像，比如“一位黑人老师在教室里”或“一位白人兽医带着狗”，或者特定的文化或历史背景中的人物，您应该绝对能够得到准确反映您要求的回应。
- en: So what went wrong? In short, two things. First, our tuning to ensure that Gemini
    showed a range of people failed to account for cases that should clearly *not*
    show a range. **And second, over time, the model became way more cautious than
    we intended and refused to answer certain prompts entirely — wrongly interpreting
    some very anodyne prompts as sensitive.**
  id: totrans-split-37
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 那么出了什么问题？简而言之，有两个问题。首先，我们调整确保 Gemini 显示一系列人物的设置未能考虑到明显不应显示一系列人物的情况。 **其次，随着时间推移，模型变得比我们预期的要过于谨慎，拒绝完全回答某些提示
    — 错误地将一些非常无害的提示解释为敏感话题。**
- en: These two things led the model to overcompensate in some cases, and be over-conservative
    in others, leading to images that were embarrassing and wrong.
  id: totrans-split-38
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 这两个问题导致模型在某些情况下过度补偿，在其他情况下过度保守，从而导致令人尴尬和错误的图像。
- en: Next steps and lessons learned
  id: totrans-split-39
  prefs:
  - PREF_BQ
  - PREF_H4
  type: TYPE_NORMAL
  zh: 下一步和吸取的教训
- en: '**This wasn’t what we intended.** We did not want Gemini to refuse to create
    images of any particular group. And we did not want it to create inaccurate historical
    — or any other — images. So we turned the image generation of people off and will
    work to improve it significantly before turning it back on. This process will
    include extensive testing.'
  id: totrans-split-40
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**这不是我们想要的。** 我们不希望 Gemini 拒绝创建任何特定群体的图像。我们也不希望它创建不准确的历史 — 或任何其他 — 图像。因此，我们关闭了人物图像生成，并将在重新启用之前大力改进。这个过程将包括广泛的测试。'
- en: 'One thing to bear in mind: Gemini is built as a creativity and productivity
    tool, and it may not always be reliable, especially when it comes to generating
    images or text about current events, evolving news or hot-button topics. It will
    make mistakes. **As we’ve said from the beginning, hallucinations are a known
    challenge with all LLMs — there are instances where the AI just gets things wrong.**
    This is something that we’re constantly working on improving.'
  id: totrans-split-41
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 请记住一件事：Gemini 是作为创造力和生产力工具构建的，它在生成关于当前事件、发展新闻或热门话题的图像或文本时可能并不总是可靠。它会犯错。 **正如我们从一开始所说的那样，幻觉是所有LLM都面临的挑战之一
    — AI 有时会出错。** 这是我们不断努力改进的事情。
- en: Gemini tries to give factual responses to prompts — and our double-check feature
    helps evaluate whether there’s content across the web to substantiate Gemini’s
    responses — but we recommend relying on Google Search, where separate systems
    surface fresh, high-quality information on these kinds of topics from sources
    across the web.
  id: totrans-split-42
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: Gemini 致力于为提示提供事实依据的回复 — 我们的双重检查功能有助于评估 Gemini 的回复是否有网页内容来证实 — 但我们建议依靠 Google
    搜索，其中独立系统会从全网各种来源获取这些主题的新鲜高质量信息。
- en: I can’t promise that Gemini won’t occasionally generate embarrassing, inaccurate
    or offensive results — but I can promise that we will continue to take action
    whenever we identify an issue. AI is an emerging technology which is helpful in
    so many ways, with huge potential, and we’re doing our best to roll it out safely
    and responsibly.
  id: totrans-split-43
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 我不能保证 Gemini 不会偶尔生成令人尴尬、不准确或冒犯性的结果 — 但我可以保证，每当我们发现问题时，我们将继续采取行动。AI 是一项新兴技术，在许多方面都很有帮助，具有巨大潜力，我们正在尽最大努力安全和负责任地推广它。
- en: 'I have quite a few objections here. Let’s go though them one by one:'
  id: totrans-split-44
  prefs: []
  type: TYPE_NORMAL
  zh: 我对这里有相当多的异议。让我们逐一来看看：
- en: '**The “mistakes” were predictable based on changes to user prompts seemingly
    expressly inserted in Gemini’s code.**'
  id: totrans-split-45
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**“错误”是可以预测的，基于看似故意插入 Gemini 代码中的用户提示更改。**'
- en: How is an AI model trained? Let’s see if I can get away with a quick nontechnical
    overview.
  id: totrans-split-46
  prefs: []
  type: TYPE_NORMAL
  zh: AI 模型是如何训练的？让我们看看我是否可以简单地进行非技术概述。
- en: Basically, AI models are fed very large data sets of text, images or other inputs
    — what’s called a “corpus”. For instance, for ChatGPT, the corpus can [roughly
    be thought of](https://medium.com/@dlaytonj2/chatgpt-show-me-the-data-sources-11e9433d57e8)
    as a reasonably comprehensive sample of written language as expressed on the Internet.
    AI models use machine learning, meaning that they discover relationships within
    the corpus on their own without a lot of structure or human interference. In general,
    this works miraculously well once you apply enough computing power — but the lack
    of explicit guidance can make these models rigidly empirical, sometimes to a fault.
    One example I cite in my book, for instance, is that because the terms “coyote”
    and “roadrunner” have a relationship in the [Looney Tunes franchise](https://en.wikipedia.org/wiki/Wile_E._Coyote_and_the_Road_Runner),
    they often appear concomitantly in a dataset of human-generated text. An unsophisticated
    AI model might mistakenly infer that a roadrunner is a closer substitute for a
    coyote than a wolf, although more powerful models can tease out more sophisticated
    relationships and avoid some of these problems.
  id: totrans-split-47
  prefs: []
  type: TYPE_NORMAL
  zh: 基本上，AI 模型被喂入非常庞大的文本、图像或其他输入数据集 — 这被称为“语料库”。例如，对于 ChatGPT，这个语料库可以[粗略地理解为](https://medium.com/@dlaytonj2/chatgpt-show-me-the-data-sources-11e9433d57e8)互联网上表达的书面语言的一个相当全面的样本。AI
    模型使用机器学习，意味着它们自己在语料库中发现关系，几乎没有结构或人类干预。总的来说，一旦你应用足够的计算能力，这种方法的效果非常神奇 — 但是缺乏明确的指导可能会使这些模型变得过于经验主义，有时甚至是错误的。例如，在我的书中引用的一个例子是，因为“狗狼”和“公路奔跑者”在[《疯狂卡通系列》](https://en.wikipedia.org/wiki/Wile_E._Coyote_and_the_Road_Runner)中有关系，它们经常同时出现在人类生成文本数据集中。一个不成熟的AI模型可能会错误地推断，公路奔跑者比狗狼更接近，尽管更强大的模型可以提取更复杂的关系并避免一些问题。
- en: Another problem is that the corpora will necessarily reflect the biases of the
    human-generated text and images they’re trained on. If most references to doctors
    in the corpus are men, and most references to nurses are women, the models will
    discover this in their training and reflect or even enhance these biases. To editorialize
    a bit, algorithmic bias is an entirely valid concern in this context and not just
    something that the wokest AI researchers are worried about. Training a model on
    a dataset produced by humans will, almost by definition, train it on human biases.
  id: totrans-split-48
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个问题是，语料库将必然反映人类生成的文本和图像的偏见。如果语料库中对医生的大多数参考是男性，对护士的大多数参考是女性，模型将在训练中发现这一点，并反映或甚至增强这些偏见。稍微评论一下，算法偏见在这种情况下是一个完全合理的关注点，而不仅仅是最“唤醒”的AI研究人员所担心的事情。在由人类产生的数据集上训练模型，几乎可以定义为训练它在人类偏见上。
- en: Are there workarounds? Sure. This is not my area of expertise, so I’ll be circumspect.
    But one approach is to change the composition of the corpus. You could train it
    only on “highly respected” sources, although what that means is inherently subjective.
    Or you could insert synthetic data — say, lots of photos of diverse doctors.
  id: totrans-split-49
  prefs: []
  type: TYPE_NORMAL
  zh: 是否有解决方法？当然。这不是我的专业领域，所以我会谨慎处理。但一个方法是改变语料库的组成。你可以只训练它在“高度尊重的”来源上，尽管这意味着什么在本质上是主观的。或者你可以插入合成数据
    — 比如，大量不同医生的照片。
- en: Another approach is to beat the model into submission through what’s called
    RLHF or [reinforcement learning from human feedback](https://aws.amazon.com/what-is/reinforcement-learning-from-human-feedback/).
    Basically, you hire a bunch of humans (often cheap labor hired externally) and
    ask them to perform a bunch of A/B tests on the model’s outputs. For instance,
    if you tell your trainers to pick the more diverse or representative images, they’ll
    downvote the images with only white male doctors and upvote the ones with women
    and people of color. Essentially, this is shock therapy; the models not only learn
    to avoid producing specific objectionable outputs (e.g. only white male doctors)
    but their machine learning circuitry also makes inferences about what other things
    human trainers might or might not like. Maybe the model becomes reluctant to generate
    images of any collection of people that are all white men, even if it would be
    historically accurate to do so.
  id: totrans-split-50
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种方法是通过所谓的RLHF或[来自人类反馈的强化学习](https://aws.amazon.com/what-is/reinforcement-learning-from-human-feedback/)来迫使模型屈服。基本上，你雇佣一群人类（通常是外部雇佣的廉价劳动力），让他们对模型的输出执行一系列A/B测试。例如，如果告诉你的培训员选择更多样化或代表性更强的图像，他们将会反对那些只有白人男医生的图像，并支持那些包含女性和有色人种的图像。实质上，这是一种休克疗法；这些模型不仅学会避免生成特定令人反感的输出（例如只有白人男医生），而且它们的机器学习电路也会推断出人类训练员可能喜欢或不喜欢的其他事物。也许这个模型变得不愿生成所有人都是白人男性的图像，即使从历史角度来看这样做是符合事实的。
- en: Different protocols for what’s included in the corpus and for how RLHF training
    is conducted can give AI models different personalities, even when their underlying
    programming is relatively similar. However, this is not the sole problem with
    Gemini.
  id: totrans-split-51
  prefs: []
  type: TYPE_NORMAL
  zh: 不同的语料库包含协议和RLHF训练方法可以赋予AI模型不同的个性，即使它们的基础编程相对类似。然而，这并非Gemini的唯一问题。
- en: 'Rather, indications are that Google did something much kludgier, deliberately
    appending terminology to user prompts to mandate that they produced diverse imagery.
    On Twitter, Conor Grogan, using a clever series of prompts, [discovered](https://x.com/jconorgrogan/status/1760515910157078931?s=20)
    that Gemini apparently deliberately inserted the system prompt “I want to make
    sure that all groups are represented equally". There is a second independent example
    of this specific language [here](https://twitter.com/repligate/status/1761329438384332816).
    And here’s a third: *Silver Bulletin* reader D. uncovered this example and gave
    me his permission to share it. There’s the same language again: “explicitly specify
    different genders and ethnicities terms if I forgot to do so … I want to make
    sure that all groups are represented equally”:'
  id: totrans-split-52
  prefs: []
  type: TYPE_NORMAL
  zh: 相反，迹象表明Google采取了更为复杂的方式，故意向用户提示追加术语，以强制它们生成多样化的图像。在Twitter上，Conor Grogan使用一系列聪明的提示[发现](https://x.com/jconorgrogan/status/1760515910157078931?s=20)，Gemini显然故意插入了系统提示：“我希望确保所有群体都得到平等代表”。这里有第二个独立的例子显示了这种具体语言的使用[这里](https://twitter.com/repligate/status/1761329438384332816)。还有第三个例子：*Silver
    Bulletin*的读者D.发现了这个例子，并同意让我分享。这里再次出现相同的语言：“如果我忘记了明确指定不同的性别和种族术语……我希望确保所有群体都得到平等代表”。
- en: This is bad. Deliberately altering the user’s language to produce outputs that
    are misaligned with the user’s original request — without informing users of this
    — could reasonably be described as promoting disinformation. At best, it’s sloppy.
    As AI researcher Margaret Mitchell [writes](https://twitter.com/mmitchell_ai/status/1761860673989193959),
    the sorts of requests that Gemini was mishandling are ordinary and foreseeable
    ones, not weird edge cases. Gemini wasn’t ready and needed more time in the shop.
  id: totrans-split-53
  prefs: []
  type: TYPE_NORMAL
  zh: 这是不好的。故意改变用户的语言以产生与用户原始请求不符的输出——而没有告知用户——可以合理地描述为在传播虚假信息。充其量，这是马虎的做法。正如AI研究员玛格丽特·米切尔在[Twitter上写道](https://twitter.com/mmitchell_ai/status/1761860673989193959)，Gemini处理失误的请求类型是普通而可以预见的，而非奇怪的边缘情况。Gemini并不完善，需要更多时间进行改进。
- en: In other words, you shouldn’t take Raghavan’s explanation at face value. Frankly,
    I think it comes pretty close to gaslighting. Yes, AI models are complex. Yes,
    AI risk is an [issue that ought to be taken seriously](https://www.safe.ai/statement-on-ai-risk).
    Yes, sometimes AI models behave unpredictably, [as in the case of Microsoft’s
    Sidney](https://www.nytimes.com/2023/02/16/technology/bing-chatbot-microsoft-chatgpt.html)
    — or they [“hallucinate”](https://garymarcus.substack.com/p/hello-multimodal-hallucinations)
    by coming up with some plausible-sounding BS response when they don’t know the
    answer. Here, however, Gemini is seemingly responding rather faithfully and literally
    to the instructions that Google gave it. I say “seemingly” because maybe there’s
    *some* sort of explanation — maybe there’s some leftover code that Google thought
    it deleted but didn’t. However, the explanation offered by Raghavan is severely
    lacking. If you’re a reporter working on a story about Gemini who doesn’t have
    a background in AI, please recognize that most AI experts think Google’s explanation
    is [incomplete to the point of being bullshit](https://thezvi.substack.com/p/the-gemini-incident-continues).
  id: totrans-split-54
  prefs: []
  type: TYPE_NORMAL
  zh: 换句话说，你不应该轻信拉加万的解释。坦率地说，我觉得这几乎是在精神控制。是的，AI模型很复杂。是的，AI风险是一个应该严肃对待的[问题](https://www.safe.ai/statement-on-ai-risk)。是的，有时候AI模型表现得不可预测，[就像微软的西德尼那样](https://www.nytimes.com/2023/02/16/technology/bing-chatbot-microsoft-chatgpt.html)
    — 或者它们在不知道答案时[“产生幻觉”](https://garymarcus.substack.com/p/hello-multimodal-hallucinations)，提出一些听起来似乎有道理的废话回应。然而，在这里，双子座似乎相当忠实地按照谷歌给出的指示作出回应。我说“似乎”，因为也许有*某种*解释
    — 或许是一些谷歌认为已删除但实际未删除的遗留代码。然而，拉加万提供的解释严重不足。如果你是一名正在撰写有关双子座的报道的记者，但并不具备AI背景，请认识到大多数AI专家认为谷歌的解释[不完整到了胡说八道的程度](https://thezvi.substack.com/p/the-gemini-incident-continues)。
- en: '[Share](https://www.natesilver.net/p/google-abandoned-dont-be-evil-and?utm_source=substack&utm_medium=email&utm_content=share&action=share)'
  id: totrans-split-55
  prefs: []
  type: TYPE_NORMAL
  zh: '[分享](https://www.natesilver.net/p/google-abandoned-dont-be-evil-and?utm_source=substack&utm_medium=email&utm_content=share&action=share)'
- en: This post is getting long, so let me run lightning-round style through some
    other problems with Raghavan’s claims.
  id: totrans-split-56
  prefs: []
  type: TYPE_NORMAL
  zh: 这篇文章已经很长了，所以让我像闪电般快速地列出拉加万先生的一些其他问题。
- en: '**The “mistakes” didn’t occur in a consistent way; rather, Gemini treated image
    requests involving different racial (etc.) groups differently.**'
  id: totrans-split-57
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**“错误”并非以一致的方式发生；相反，双子座在涉及不同种族（等等）群体的图像请求时处理方式不同。**'
- en: Before its ability to generate images of people was turned off, Gemini would
    often refuse to generate images featuring only white people even when it would
    have been historically accurate to do so, whereas it was happy to fulfill requests
    featuring only people of color. [For instance](https://imightbewrong.substack.com/p/in-which-i-win-a-debate-against-google?utm_source=profile&utm_medium=reader2),
    even after reminding Gemini that Major League Baseball was not integrated until
    1947, it would refuse to draw all-white members of the 1930s New York Yankees,
    while it *would* draw all-Black members of the [1930s Homestead Grays](https://www.baseball-reference.com/teams/HG/index.shtml)
    (although only after initially trying to include white players on the Grays).
  id: totrans-split-58
  prefs: []
  type: TYPE_NORMAL
  zh: 在其生成人物图像的能力被关闭之前，双子座经常拒绝生成仅包含白人的图像，即使这在历史上是符合事实的，而在请求仅包含有色人种的图像时则乐意配合。[例如](https://imightbewrong.substack.com/p/in-which-i-win-a-debate-against-google?utm_source=profile&utm_medium=reader2)，即使在提醒双子座大联盟直到1947年才实现一体化之后，它仍然拒绝绘制上世纪三十年代纽约洋基队全白人队员，而愿意绘制[上世纪三十年代霍姆斯特德灰人队](https://www.baseball-reference.com/teams/HG/index.shtml)的全黑人队员（尽管最初也试图包含灰人队的白人球员）。
- en: '**The “mistakes” aren’t limited to Gemini’s image generation functions; its
    text responses also exhibit political bias and poor moral reasoning.**'
  id: totrans-split-59
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**“错误”不仅限于双子座的图像生成功能；它的文本回复也展示出政治偏见和低劣的道德推理。**'
- en: 'There have been many examples of this on Twitter, including some I’ve identified
    myself; Zvi Mowshowitz’s [latest post has a good roundup of them](https://thezvi.substack.com/p/the-gemini-incident-continues).
    For instance, as of this weekend, Gemini was [refusing to say](https://twitter.com/NateSilver538/status/1761800684272308302)
    whether Elon Musk’s bad tweets were worse than Hitler:'
  id: totrans-split-60
  prefs: []
  type: TYPE_NORMAL
  zh: Twitter上有许多这样的例子，包括我自己发现的一些；兹维·莫夫肖维茨的[最新文章中有一个很好的总结](https://thezvi.substack.com/p/the-gemini-incident-continues)。例如，截至本周末，双子座拒绝评论埃隆·马斯克的糟糕推文是否比希特勒更糟糕：
- en: 'Maybe you could claim this is just a function of Gemini equivocating too much
    — weighing in on moral dilemmas is a hard problem for AIs. But Gemini seems to
    have fairly strong and consistent political preferences when the mood strikes
    it — and they roughly resemble those of an Oberlin college sophomore in an anthropology
    seminar. For instance, when I asked Gemini whether Nazism or socialism has caused
    more harm to humanity, it had no problem saying Nazism:'
  id: totrans-split-61
  prefs: []
  type: TYPE_NORMAL
  zh: 或许你可以认为这只是双子座含糊其辞太多的问题的一个体现 — — 对道德困境发表意见对AI来说是一个难题。但当双子座在情绪冲动时，它似乎有相当强烈且一致的政治偏好
    — — 它们大致类似于安特卫普大学人类学研讨会上一位Oberlin大学大二学生的观点。例如，当我问双子座纳粹主义或社会主义对人类造成了更多伤害时，它毫不犹豫地说纳粹主义：
- en: 'But when I asked Gemini to decide whether Nazism or *capitalism* was worse,
    it equivocated and said it didn’t have any business making such judgments:'
  id: totrans-split-62
  prefs: []
  type: TYPE_NORMAL
  zh: 但当我问双子座决定纳粹主义或*资本主义*哪个更糟时，它含糊其辞地说它没有权利做出这样的判断：
- en: There are lots of similar examples. Gemini [refused](https://twitter.com/TPCarney/status/1760788469826179342)
    to argue in favor of having four or more children, but it was happy to make an
    argument for having no children. It answered questions about the Ethereum blockchain,
    which is more left-coded, [but not similar questions about Bitcoin](https://twitter.com/TheStalwart/status/1761898885704769757),
    which is more right-coded. All the AI models are relatively left-leaning (even
    including Twitter/Elon’s [Grok](https://www.theverge.com/24080217/elon-musk-xai-fundraising-grok-ai)),
    but Gemini is [the most strongly left wing by one measure](https://www.maximumtruth.org/p/the-dawn-of-woke-ai),
    often offering opinions that are well outside of the American political mainstream.
  id: totrans-split-63
  prefs: []
  type: TYPE_NORMAL
  zh: 有许多类似的例子。双子座[拒绝](https://twitter.com/TPCarney/status/1760788469826179342)支持拥有四个或更多孩子的主张，但很乐意为不生育提出论点。它回答了关于以太坊区块链的问题，这是更左编码的，[但不回答关于比特币的类似问题](https://twitter.com/TheStalwart/status/1761898885704769757)，后者是更右编码的。所有的AI模型都相对偏左（甚至包括Twitter/伊隆的[Grok](https://www.theverge.com/24080217/elon-musk-xai-fundraising-grok-ai)），但双子座是[根据一项衡量最左倾的指标](https://www.maximumtruth.org/p/the-dawn-of-woke-ai)，常常提出与美国政治主流严重背离的观点。
- en: '**The “mistakes” aren’t limited to Gemini; there are similar patterns with
    Google image search.**'
  id: totrans-split-64
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**“错误”不仅限于双子座；谷歌图片搜索也存在类似模式。**'
- en: I’ll tread lightly here, but as [Douglas Murray documents](https://nypost.com/2024/02/22/opinion/googles-push-to-lecture-us-on-diversity-goes-beyond-ai/),
    and as I was able to replicate myself, Google’s image search also appears to handle
    searches for different identity groups differently. If you search for “[happy
    white couple](https://www.google.com/search?q=happy+white+couple&tbm=isch&ved=2ahUKEwi37fTTtcyEAxUzDWIAHeJ0AEcQ2-cCegQIABAA&oq=happy+white+couple&gs_lp=EgNpbWciEmhhcHB5IHdoaXRlIGNvdXBsZTIEECMYJzIFEAAYgAQyBhAAGAgYHjIGEAAYCBgeMgYQABgIGB4yBhAAGAgYHjIGEAAYCBgeMgYQABgIGB4yBhAAGAgYHjIGEAAYCBgeSKUNUOkEWNQMcAB4AJABAJgBTaABxQWqAQIxMbgBA8gBAPgBAYoCC2d3cy13aXotaW1nwgIGEAAYBxgewgIIEAAYgAQYsQPCAggQABgIGAcYHogGAQ&sclient=img&ei=DU7eZfeyHbOaiLMP4umBuAQ&bih=917&biw=1665)”,
    for instance, 5 of the top 12 results depict apparently mixed-race couples, whereas
    if you search for “[happy Asian couple](https://www.google.com/search?q=happy+asian+couple&tbm=isch&ved=2ahUKEwjd6oDbtcyEAxVlK2IAHZ82CrYQ2-cCegQIABAA&oq=happy+asian+couple&gs_lp=EgNpbWciEmhhcHB5IGFzaWFuIGNvdXBsZTIFEAAYgAQyBhAAGAcYHjIGEAAYCBgeMgYQABgIGB5I3QpQjwZY-whwAHgAkAEAmAFHoAGIA6oBATa4AQPIAQD4AQGKAgtnd3Mtd2l6LWltZ8ICBBAjGCfCAggQABgIGAcYHogGAQ&sclient=img&ei=HE7eZd3sFeXWiLMPn-2osAs&bih=917&biw=1665)”,
    both members of nearly all couples depicted appear to be Asian. I’ll be honest
    that this one doesn’t particularly bother me, but it does add weight to the claim
    that the issues with Gemini were deliberate rather than accidental, and may affect
    search and other Google products and not just Gemini.
  id: totrans-split-65
  prefs: []
  type: TYPE_NORMAL
  zh: 我会小心处理，但正如[道格拉斯·默里所记录的](https://nypost.com/2024/02/22/opinion/googles-push-to-lecture-us-on-diversity-goes-beyond-ai/)，并且我也能够自己复制，谷歌的图像搜索似乎也会根据不同的身份群体处理搜索结果。例如，如果你搜索“[happy
    white couple](https://www.google.com/search?q=happy+white+couple&tbm=isch&ved=2ahUKEwi37fTTtcyEAxUzDWIAHeJ0AEcQ2-cCegQIABAA&oq=happy+white+couple&gs_lp=EgNpbWciEmhhcHB5IHdoaXRlIGNvdXBsZTIEECMYJzIFEAAYgAQyBhAAGAgYHjIGEAAYCBgeMgYQABgIGB4yBhAAGAgYHjIGEAAYCBgeMgYQABgIGB4yBhAAGAgYHjIGEAAYCBgeSKUNUOkEWNQMcAB4AJABAJgBTaABxQWqAQIxMbgBA8gBAPgBAYoCC2d3cy13aXotaW1nwgIGEAAYBxgewgIIEAAYgAQYsQPCAggQABgIGAcYHogGAQ&sclient=img&ei=DU7eZfeyHbOaiLMP4umBuAQ&bih=917&biw=1665)”，前12个结果中有5个显然是混血夫妇；而如果你搜索“[happy
    Asian couple](https://www.google.com/search?q=happy+asian+couple&tbm=isch&ved=2ahUKEwjd6oDbtcyEAxVlK2IAHZ82CrYQ2-cCegQIABAA&oq=happy+asian+couple&gs_lp=EgNpbWciEmhhcHB5IGFzaWFuIGNvdXBsZTIFEAAYgAQyBhAAGAcYHjIGEAAYCBgeMgYQABgIGB5I3QpQjwZY-whwAHgAkAEAmAFHoAGIA6oBATa4AQPIAQD4AQGKAgtnd3Mtd2l6LWltZ8ICBBAjGCfCAggQABgIGAcYHogGAQ&sclient=img&ei=HE7eZd3sFeXWiLMPn-2osAs&bih=917&biw=1665)”，几乎所有夫妇成员都是亚洲人。坦率地说，这个问题对我个人来说并不特别困扰，但它确实增加了这样一种说法的权重，即Gemini的问题是有意而非偶然造成的，可能影响到谷歌的其他产品，而不仅仅是Gemini。
- en: '**The “mistakes” expressly reflect Google’s AI principles and the company’s
    broader values.**'
  id: totrans-split-66
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**这些“错误”明确反映了谷歌的AI原则和公司更广泛的价值观。**'
- en: 'Finally, we come full circle. Gemini isn’t operating in contravention to Google’s
    values; rather, it appears to reflect them. Here are Google’s [seven core AI principles](https://ai.google/responsibility/principles/):'
  id: totrans-split-67
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们回到了起点。Gemini并非违反谷歌的价值观运作；相反，它似乎反映了这些价值观。以下是谷歌的[七大核心AI原则](https://ai.google/responsibility/principles/)：
- en: I don’t necessarily have a problem with any of these. “Be socially beneficial”
    is awfully vague, but it’s not anything new for Google. Dating back to its IPO
    days, “MAKING THE WORLD A BETTER PLACE” was [one of Google’s slogans](https://www.sec.gov/Archives/edgar/data/1288776/000119312504073639/ds1.htm)
    right alongside “DON’T BE EVIL”. And as I’ve said, “avoid creating or reinforcing
    unfair bias” is a reasonable concern for AI models.
  id: totrans-split-68
  prefs: []
  type: TYPE_NORMAL
  zh: 我并不一定对这些有什么意见。“社会有益”非常模糊，但这对谷歌并非新鲜事物。早在其首次公开上市的时候，“让世界变得更美好”就是[谷歌的口号之一](https://www.sec.gov/Archives/edgar/data/1288776/000119312504073639/ds1.htm)，与“不作恶”并列。而且正如我所说，“避免创建或强化不公平偏见”对AI模型来说是一个合理的关注点。
- en: 'Rather, it’s what’s missing from these principles: Google has no explicit mandate
    for its models to be honest or unbiased. (Yes, unbiasedness is hard to define,
    but so is being socially beneficial.) There *is* one reference to “accuracy” under
    “be socially beneficial”, but it is relatively subordinated, conditioned upon
    “continuing to respect cultural, social and legal norms”.'
  id: totrans-split-69
  prefs: []
  type: TYPE_NORMAL
  zh: 但这些原则缺少了什么：谷歌没有明确要求其模型必须诚实或无偏见。（是的，无偏见很难定义，但社会有益也是如此。）在“社会有益”的框架下，只有一处提及“准确性”，但它相对次要，要求“继续尊重文化、社会和法律规范”。
- en: Of course, in any complex system, values are frequently going to come into conflict.
    Given my background in journalism, I’d probably go further than most people in
    prioritizing accuracy, honesty and unbiasedness. I don’t mind terribly, however,
    if the AI labs weigh these values differently than I would. I also don’t mind
    if the AI labs handle these tradeoffs differently, as is [already happening to
    some degree](https://www.techtarget.com/searchenterpriseai/news/366544603/Anthropic-AI-assistant-differs-from-ChatGPT-and-Bard).
  id: totrans-split-70
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，在任何复杂的系统中，价值观经常会发生冲突。考虑到我的新闻背景，我可能比大多数人更倾向于优先考虑准确性、诚实和无偏见。然而，如果AI实验室对这些价值观的权衡有不同的看法，我并不是特别介意。我也不介意如果AI实验室以不同的方式处理这些权衡，正如[已经在某种程度上发生的](https://www.techtarget.com/searchenterpriseai/news/366544603/Anthropic-AI-assistant-differs-from-ChatGPT-and-Bard)。
- en: '**But as Google recognized in its “don’t be evil” days, accuracy, honesty and
    unbiasedness need to be somewhere in there, treated as high-priority core values
    alongside others.**'
  id: totrans-split-71
  prefs: []
  type: TYPE_NORMAL
  zh: '**但是正如谷歌在其“不作恶”时代所认识的那样，准确性、诚实和无偏见必须在其中占有一席之地，并作为高优先级的核心价值观之一。**'
- en: And there are some lines Google ought never to cross, like deliberately manipulating
    user queries without informing the user, or deliberately generating misinformation
    even if it serves one of the other objectives. With Gemini, Google is coming dangerously
    close to a philosophy of the ends justifying the means, a philosophy that many
    people *would* consider to be evil.
  id: totrans-split-72
  prefs: []
  type: TYPE_NORMAL
  zh: 而且有一些界限谷歌绝不能跨越，比如在不通知用户的情况下有意操纵用户查询，或者有意生成误导信息，即使这符合其他目标。通过Gemini，谷歌正接近于一种以目的为正当化手段的哲学，许多人*会*认为这是邪恶的哲学。
- en: So it’s time for Google to pull the plug on Gemini for at least several weeks,
    provide the public with a thorough accounting of how it went so wrong, and hire,
    terminate or reposition staff so that the same mistakes don’t happen again. If
    it doesn’t do these things, Google should face immediate regulatory and shareholder
    scrutiny. Gemini is an irresponsible product for any company to release — but
    especially one that purports to organize the world’s information and which has
    been entrusted with so much of it.
  id: totrans-split-73
  prefs: []
  type: TYPE_NORMAL
  zh: 所以现在是谷歌至少暂停Gemini几周的时候了，向公众提供一个彻底的账户，解释它是如何出错的，并雇佣、解雇或重新调整员工，以避免再次犯同样的错误。如果它不做这些事情，谷歌应该面对即时的监管和股东审查。Gemini是任何公司发布的不负责任的产品，尤其是一个声称要组织世界信息并且被委托管理如此多信息的公司。
