["```\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n66\n67\n68\n69\n70\n71\n72\n73\n74\n75\n76\n77\n78\n79\n80\n81\n82\n83\n84\n85\n86\n87\n88\n89\n90\n91\n92\n93\n94\n95\n96\n97\n98\n99\n100\n101\n102\n103\n104\n105\n106\n107\n108\n109\n110\n111\n112\n113\n114\n115\n116\n117\n118\n119\n120\n121\n122\n123\n124\n125\n126\n127\n128\n129\n130\n131\n132\n133\n134\n135\n136\n137\n138\n139\n140\n141\n142\n143\n144\n145\n146\n147\n148\n149\n150\n151\n152\n153\n154\n155\n156\n157\n158\n159\n160\n161\n162\n163\n164\n165\n166\n167\n168\n169\n170\n171\n172\n173\n174\n175\n176\n177\n178\n179\n180\n181\n182\n183\n184\n185\n186\n187\n188\n189\n190\n191\n192\n193\n194\n195\n196\n197\n198\n199\n200\n201\n202\n203\n204\n205\n206\n207\n208\n209\n210\n211\n212\n213\n214\n215\n216\n217\n218\n219\n220\n221\n222\n223\n224\n225\n226\n227\n228\n229\n230\n231\n232\n233\n234\n235\n236\n237\n238\n239\n240\n241\n242\n243\n244\n245\n246\n247\n248\n249\n250\n251\n252\n253\n254\n255\n256\n257\n258\n259\n260\n261\n262\n263\n264\n265\n266\n267\n268\n269\n270\n271\n272\n273\n274\n275\n276\n277\n278\n279\n280\n281\n282\n283\n284\n285\n286\n287\n288\n289\n290\n291\n292\n293\n294\n295\n296\n297\n298\n299\n300\n301\n302\n303\n304\n305\n306\n307\n308\n309\n310\n311\n312\n313\n314\n315\n316\n317\n318\n319\n320\n321\n322\n323\n324\n325\n326\n327\n328\n329\n330\n331\n332\n333\n334\n335\n336\n337\n338\n339\n340\n341\n342\n343\n344\n345\n346\n347\n348\n349\n350\n351\n352\n353\n354\n355\n356\n357\n358\n359\n360\n361\n362\n363\n364\n365\n366\n367\n368\n369\n370\n371\n372\n373\n374\n375\n376\n377\n378\n379\n380\n381\n382\n383\n384\n385\n386\n387\n388\n389\n390\n391\n392\n393\n394\n395\n396\n397\n398\n399\n400\n401\n402\n403\n404\n405\n406\n407\n408\n409\n410\n411\n412\n413\n414\n415\n416\n417\n418\n419\n420\n421\n422\n423\n424\n425\n426\n427\n428\n429\n430\n431\n432\n433\n434\n435\n436\n437\n438\n439\n440\n441\n442\n443\n444\n445\n446\n447\n448\n449\n450\n451\n452\n453\n454\n455\n456\n457\n458\n459\n460\n461\n462\n463\n464\n465\n466\n467\n468\n469\n470\n471\n472\n473\n474\n475\n476\n477\n478\n479\n480\n481\n482\n483\n484\n485\n486\n487\n488\n489\n490\n491\n492\n493\n494\n495\n496\n497\n498\n499\n500\n501\n502\n503\n504\n505\n506\n507\n508\n509\n510\n511\n512\n513\n514\n515\n516\n517\n518\n519\n520\n521\n522\n523\n524\n525\n526\n527\n528\n529\n530\n531\n532\n533\n534\n535\n536\n537\n538\n539\n540\n541\n542\n543\n544\n545\n546\n547\n548\n549\n550\n551\n552\n553\n554\n555\n556\n557\n558\n559\n560\n561\n562\n563\n564\n565\n566\n567\n568\n569\n570\n571\n572\n573\n574\n575\n576\n577\n578\n579\n580\n581\n582\n583\n584\n585\n586\n587\n588\n589\n590\n591\n592\n593\n594\n595\n596\n597\n598\n599\n600\n601\n602\n603\n604\n605\n606\n607\n608\n609\n610\n611\n612\n613\n614\n615\n616\n617\n618\n619\n620\n621\n622\n623\n624\n625\n626\n627\n628\n629\n630\n631\n632\n633\n634\n635\n636\n637\n638\n639\n640\n641\n642\n643\n644\n645\n646\n647\n648\n649\n650\n651\n652\n653\n654\n655\n656\n657\n658\n659\n660\n661\n662\n663\n664\n665\n666\n667\n668\n669\n670\n671\n672\n673\n674\n675\n676\n677\n678\n679\n680\n681\n682\n683\n684\n685\n686\n687\n688\n689\n690\n691\n692\n693\n694\n695\n696\n697\n698\n699\n700\n701\n702\n703\n704\n705\n706\n707\n708\n709\n710\n711\n712\n713\n714\n715\n716\n717\n718\n719\n720\n721\n722\n723\n724\n725\n726\n727\n728\n729\n730\n731\n732\n733\n734\n735\n736\n737\n738\n739\n740\n741\n742\n743\n744\n745\n746\n747\n748\n749\n750\n751\n752\n753\n754\n755\n756\n757\n758\n759\n760\n761\n762\n763\n764\n765\n766\n767\n768\n769\n770\n771\n772\n773\n774\n775\n776\n777\n778\n779\n780\n781\n782\n783\n784\n785\n786\n787\n788\n789\n790\n791\n792\n793\n794\n795\n796\n797\n798\n799\n800\n801\n802\n803\n804\n805\n806\n807\n808\n809\n810\n811\n812\n813\n814\n815\n816\n817\n818\n819\n820\n821\n822\n823\n824\n825\n826\n827\n828\n829\n830\n831\n832\n833\n834\n835\n836\n837\n838\n839\n840\n841\n842\n843\n844\n845\n846\n847\n848\n849\n850\n851\n852\n853\n854\n855\n856\n857\n858\n859\n860\n861\n862\n863\n864\n865\n866\n867\n868\n869\n870\n871\n872\n873\n874\n875\n876\n877\n878\n879\n880\n881\n882\n883\n884\n885\n886\n887\n888\n889\n890\n891\n892\n893\n894\n895\n896\n897\n898\n899\n900\n901\n902\n903\n904\n905\n906\n907\n908\n909\n910\n911\n912\n913\n914\n915\n916\n917\n918\n919\n920\n921\n922\n923\n924\n925\n926\n927\n928\n929\n930\n931\n932\n933\n934\n935\n936\n937\n938\n939\n940\n941\n942\n943\n944\n945\n946\n947\n948\n949\n950\n951\n952\n953\n954\n955\n956\n957\n958\n959\n960\n961\n962\n963\n964\n965\n966\n967\n968\n969\n970\n971\n972\n973\n974\n975\n976\n977\n978\n979\n980\n981\n982\n983\n984\n985\n986\n987\n988\n989\n990\n991\n992\n993\n994\n995\n996\n997\n998\n999\n1000\n1001\n1002\n1003\n1004\n1005\n1006\n1007\n1008\n1009\n1010\n1011\n1012\n1013\n1014\n1015\n1016\n1017\n1018\n1019\n1020\n1021\n1022\n1023\n1024\n1025\n1026\n1027\n1028\n1029\n1030\n1031\n1032\n1033\n1034\n1035\n1036\n1037\n1038\n1039\n1040\n1041\n1042\n1043\n1044\n1045\n1046\n1047\n1048\n1049\n1050\n1051\n1052\n1053\n1054\n1055\n1056\n1057\n1058\n1059\n1060\n1061\n1062\n1063\n1064\n1065\n1066\n1067\n1068\n1069\n1070\n1071\n1072\n1073\n1074\n1075\n1076\n1077\n1078\n1079\n1080\n1081\n1082\n1083\n1084\n1085\n1086\n1087\n1088\n1089\n1090\n1091\n1092\n1093\n1094\n1095\n1096\n1097\n1098\n1099\n1100\n1101\n1102\n1103\n1104\n1105\n1106\n1107\n1108\n1109\n1110\n1111\n1112\n1113\n1114\n1115\n1116\n1117\n1118\n1119\n1120\n1121\n1122\n1123\n1124\n1125\n1126\n1127\n1128\n1129\n1130\n1131\n1132\n1133\n1134\n1135\n1136\n1137\n1138\n1139\n1140\n1141\n1142\n1143\n1144\n1145\n1146\n1147\n1148\n1149\n1150\n1151\n1152\n1153\n1154\n1155\n1156\n1157\n1158\n1159\n1160\n1161\n1162\n1163\n1164\n1165\n1166\n1167\n1168\n1169\n1170\n1171\n1172\n1173\n1174\n1175\n1176\n1177\n1178\n1179\n1180\n1181\n1182\n1183\n1184\n1185\n1186\n1187\n1188\n1189\n1190\n1191\n1192\n1193\n1194\n1195\n1196\n1197\n1198\n1199\n1200\n1201\n1202\n1203\n1204\n1205\n1206\n1207\n1208\n1209\n1210\n1211\n1212\n1213\n1214\n1215\n1216\n1217\n1218\n1219\n1220\n1221\n1222\n1223\n1224\n1225\n1226\n1227\n1228\n1229\n1230\n1231\n1232\n1233\n1234\n1235\n1236\n1237\n1238\n1239\n1240\n1241\n1242\n1243\n1244\n1245\n1246\n1247\n1248\n1249\n1250\n1251\n1252\n1253\n1254\n1255\n1256\n1257\n1258\n1259\n1260\n1261\n1262\n1263\n1264\n1265\n1266\n1267\n1268\n1269\n1270\n1271\n1272\n1273\n1274\n1275\n1276\n1277\n1278\n1279\n1280\n1281\n1282\n1283\n1284\n1285\n1286\n1287\n1288\n1289\n1290\n1291\n1292\n1293\n1294\n1295\n1296\n1297\n1298\n1299\n1300\n1301\n1302\n1303\n1304\n1305\n1306\n1307\n1308\n1309\n1310\n1311\n1312\n1313\n1314\n1315\n1316\n1317\n1318\n1319\n1320\n1321\n1322\n1323\n1324\n1325\n1326\n1327\n1328\n1329\n1330\n1331\n1332\n1333\n1334\n1335\n1336\n1337\n1338\n1339\n1340\n1341\n1342\n1343\n1344\n1345\n1346\n1347\n1348\n1349\n1350\n1351\n1352\n1353\n1354\n1355\n1356\n1357\n1358\n1359\n1360\n1361\n1362\n1363\n1364\n1365\n1366\n1367\n1368\n1369\n1370\n1371\n1372\n1373\n1374\n1375\n1376\n1377\n1378\n1379\n1380\n1381\n1382\n1383\n1384\n1385\n1386\n1387\n1388\n1389\n1390\n1391\n1392\n1393\n1394\n1395\n1396\n1397\n1398\n1399\n1400\n1401\n1402\n1403\n1404\n1405\n1406\n1407\n1408\n1409\n1410\n1411\n1412\n1413\n1414\n1415\n1416\n1417\n1418\n1419\n1420\n1421\n1422\n1423\n1424\n1425\n1426\n1427\n1428\n1429\n1430\n1431\n1432\n1433\n1434\n1435\n1436\n1437\n1438\n1439\n1440\n1441\n1442\n1443\n1444\n1445\n1446\n1447\n1448\n1449\n1450\n1451\n1452\n1453\n1454\n1455\n1456\n1457\n1458\n1459\n1460\n1461\n1462\n1463\n1464\n1465\n1466\n1467\n1468\n1469\n1470\n1471\n1472\n1473\n1474\n1475\n1476\n1477\n1478\n1479\n1480\n1481\n1482\n1483\n1484\n1485\n1486\n1487\n1488\n1489\n1490\n1491\n1492\n1493\n1494\n1495\n1496\n1497\n1498\n1499\n1500\n1501\n1502\n1503\n1504\n1505\n1506\n1507\n1508\n1509\n1510\n1511\n1512\n1513\n1514\n1515\n1516\n1517\n1518\n1519\n1520\n1521\n1522\n1523\n1524\n1525\n1526\n1527\n1528\n1529\n1530\n1531\n1532\n1533\n1534\n1535\n1536\n1537\n1538\n1539\n1540\n1541\n1542\n1543\n1544\n1545\n1546\n1547\n1548\n1549\n1550\n1551\n1552\n1553\n1554\n1555\n1556\n1557\n1558\n1559\n1560\n1561\n1562\n1563\n1564\n1565\n1566\n1567\n1568\n1569\n1570\n1571\n1572\n1573\n1574\n1575\n1576\n1577\n1578\n1579\n1580\n1581\n1582\n1583\n1584\n1585\n1586\n1587\n1588\n1589\n1590\n1591\n1592\n1593\n1594\n1595\n1596\n1597\n1598\n1599\n1600\n1601\n1602\n1603\n1604\n1605\n1606\n1607\n1608\n1609\n1610\n1611\n1612\n1613\n1614\n1615\n1616\n1617\n1618\n1619\n1620\n1621\n1622\n1623\n1624\n1625\n1626\n1627\n1628\n1629\n1630\n1631\n1632\n1633\n1634\n1635\n1636\n1637\n1638\n1639\n1640\n1641\n1642\n1643\n1644\n1645\n1646\n1647\n1648\n1649\n1650\n1651\n1652\n1653\n1654\n1655\n1656\n1657\n1658\n1659\n1660\n1661\n1662\n1663\n1664\n1665\n1666\n1667\n1668\n1669\n1670\n1671\n1672\n1673\n1674\n1675\n1676\n1677\n1678\n1679\n1680\n1681\n1682\n1683\n1684\n1685\n1686\n1687\n1688\n1689\n1690\n1691\n1692\n1693\n1694\n1695\n1696\n1697\n1698\n1699\n1700\n1701\n1702\n1703\n1704\n1705\n1706\n1707\n1708\n1709\n1710\n1711\n1712\n1713\n1714\n1715\n1716\n1717\n1718\n1719\n1720\n1721\n1722\n1723\n1724\n1725\n1726\n1727\n1728\n1729\n1730\n1731\n1732\n1733\n1734\n1735\n1736\n1737\n1738\n1739\n1740\n1741\n1742\n1743\n1744\n1745\n1746\n1747\n1748\n1749\n1750\n1751\n1752\n1753\n1754\n1755\n1756\n1757\n1758\n1759\n1760\n1761\n1762\n1763\n1764\n1765\n1766\n1767\n1768\n1769\n1770\n1771\n1772\n1773\n1774\n1775\n1776\n1777\n1778\n1779\n1780\n1781\n1782\n1783\n1784\n1785\n1786\n1787\n1788\n1789\n1790\n1791\n1792\n1793\n1794\n1795\n1796\n1797\n1798\n1799\n1800\n1801\n1802\n1803\n1804\n1805\n1806\n1807\n1808\n1809\n1810\n1811\n1812\n1813\n1814\n1815\n1816\n1817\n1818\n1819\n1820\n1821\n1822\n1823\n1824\n1825\n1826\n1827\n1828\n1829\n1830\n1831\n1832\n1833\n1834\n1835\n1836\n1837\n1838\n1839\n1840\n1841\n1842\n1843\n1844\n1845\n1846\n1847\n1848\n1849\n1850\n1851\n1852\n1853\n1854\n1855\n1856\n1857\n1858\n1859\n1860\n1861\n1862\n1863\n1864\n1865\n1866\n1867\n1868\n1869\n1870\n1871\n1872\n1873\n1874\n1875\n1876\n1877\n1878\n1879\n1880\n1881\n1882\n1883\n1884\n1885\n1886\n1887\n1888\n1889\n1890\n1891\n1892\n1893\n1894\n1895\n1896\n1897\n1898\n1899\n1900\n1901\n1902\n1903\n1904\n1905\n1906\n1907\n1908\n1909\n1910\n1911\n1912\n1913\n1914\n1915\n1916\n1917\n1918\n1919\n1920\n1921\n1922\n1923\n1924\n1925\n1926\n1927\n1928\n1929\n1930\n1931\n1932\n1933\n1934\n1935\n1936\n1937\n1938\n1939\n1940\n1941\n1942\n1943\n1944\n1945\n1946\n1947\n1948\n1949\n1950\n1951\n1952\n1953\n1954\n1955\n1956\n1957\n1958\n1959\n1960\n1961\n1962\n1963\n1964\n1965\n1966\n1967\n1968\n1969\n1970\n1971\n1972\n1973\n1974\n1975\n1976\n1977\n1978\n1979\n1980\n1981\n1982\n1983\n1984\n1985\n1986\n1987\n1988\n1989\n1990\n1991\n1992\n1993\n1994\n1995\n1996\n1997\n1998\n1999\n2000\n2001\n2002\n2003\n2004\n2005\n2006\n2007\n2008\n2009\n2010\n2011\n2012\n2013\n2014\n2015\n2016\n2017\n2018\n2019\n2020\n2021\n2022\n2023\n2024\n2025\n2026\n2027\n2028\n2029\n2030\n2031\n2032\n2033\n2034\n2035\n2036\n2037\n2038\n2039\n2040\n2041\n2042\n2043\n2044\n2045\n2046\n2047\n2048\n2049\n2050\n2051\n2052\n2053\n2054\n2055\n2056\n2057\n2058\n2059\n2060\n2061\n2062\n2063\n2064\n2065\n2066\n2067\n2068\n2069\n2070\n2071\n2072\n2073\n2074\n2075\n2076\n2077\n2078\n2079\n2080\n2081\n2082\n2083\n2084\n2085\n2086\n2087\n2088\n2089\n2090\n2091\n2092\n2093\n2094\n2095\n2096\n2097\n2098\n2099\n2100\n2101\n2102\n2103\n2104\n2105\n2106\n2107\n2108\n2109\n2110\n2111\n2112\n2113\n2114\n2115\n2116\n2117\n2118\n2119\n2120\n2121\n2122\n2123\n2124\n2125\n2126\n2127\n2128\n2129\n2130\n2131\n2132\n2133\n2134\n2135\n2136\n2137\n2138\n2139\n2140\n2141\n2142\n2143\n2144\n2145\n2146\n2147\n2148\n2149\n2150\n2151\n2152\n2153\n2154\n2155\n2156\n2157\n2158\n2159\n2160\n2161\n2162\n2163\n2164\n2165\n2166\n2167\n2168\n2169\n2170\n2171\n2172\n2173\n2174\n2175\n2176\n2177\n2178\n2179\n2180\n2181\n2182\n2183\n2184\n2185\n2186\n2187\n2188\n2189\n2190\n2191\n2192\n2193\n2194\n2195\n2196\n2197\n2198\n2199\n2200\n2201\n2202\n2203\n2204\n2205\n2206\n2207\n2208\n2209\n2210\n2211\n2212\n2213\n2214\n2215\n2216\n2217\n2218\n2219\n2220\n2221\n2222\n2223\n2224\n2225\n2226\n2227\n2228\n2229\n2230\n2231\n2232\n2233\n2234\n2235\n2236\n2237\n2238\n2239\n2240\n2241\n2242\n2243\n2244\n2245\n2246\n2247\n2248\n2249\n2250\n2251\n2252\n2253\n2254\n2255\n2256\n2257\n2258\n2259\n2260\n2261\n2262\n2263\n2264\n2265\n2266\n2267\n2268\n2269\n2270\n2271\n2272\n2273\n2274\n2275\n2276\n2277\n2278\n2279\n2280\n2281\n2282\n2283\n2284\n2285\n2286\n2287\n2288\n2289\n2290\n2291\n2292\n2293\n2294\n2295\n2296\n2297\n2298\n2299\n2300\n2301\n2302\n2303\n2304\n2305\n2306\n2307\n2308\n2309\n2310\n2311\n2312\n2313\n2314\n2315\n2316\n2317\n2318\n2319\n2320\n2321\n2322\n2323\n2324\n2325\n2326\n2327\n2328\n2329\n2330\n2331\n2332\n2333\n2334\n2335\n2336\n2337\n2338\n2339\n2340\n2341\n2342\n2343\n2344\n2345\n2346\n2347\n2348\n2349\n2350\n2351\n2352\n2353\n2354\n2355\n2356\n2357\n2358\n2359\n2360\n2361\n2362\n2363\n2364\n2365\n2366\n2367\n2368\n2369\n2370\n2371\n2372\n2373\n2374\n2375\n2376\n2377\n2378\n2379\n2380\n2381\n2382\n2383\n2384\n2385\n2386\n2387\n2388\n2389\n2390\n2391\n2392\n2393\n2394\n2395\n2396\n2397\n2398\n2399\n2400\n2401\n2402\n2403\n2404\n2405\n2406\n2407\n2408\n2409\n2410\n\n```", "```\n/* SPDX-License-Identifier: MIT */\n/*\n * Sample program that can act either as a packet sink, where it just receives\n * packets and doesn't do anything with them, or it can act as a proxy where it\n * receives packets and then sends them to a new destination. The proxy can\n * be unidirectional (-B0), or bi-direction (-B1).\n * \n * Examples:\n *\n * Act as a proxy, listening on port 4444, and send data to 192.168.2.6 on port\n * 4445\\. Use multishot receive, DEFER_TASKRUN, and fixed files\n *\n * \t./proxy -m1 -r4444 -H 192.168.2.6 -p4445\n *\n * Same as above, but utilize send bundles (-C1, requires -u1 send_ring) as well\n * with ring provided send buffers, and recv bundles (-c1).\n *\n * \t./proxy -m1 -c1 -u1 -C1 -r4444 -H 192.168.2.6 -p4445\n *\n * Act as a bi-directional proxy, listening on port 8888, and send data back\n * and forth between host and 192.168.2.6 on port 22\\. Use multishot receive,\n * DEFER_TASKRUN, fixed files, and buffers of size 1500.\n *\n * \t./proxy -m1 -B1 -b1500 -r8888 -H 192.168.2.6 -p22\n *\n * Act a sink, listening on port 4445, using multishot receive, DEFER_TASKRUN,\n * and fixed files:\n *\n * \t./proxy -m1 -s1 -r4445\n *\n * Run with -h to see a list of options, and their defaults.\n *\n * (C) 2024 Jens Axboe <axboe@kernel.dk>\n *\n */\n#include <fcntl.h>\n#include <stdint.h>\n#include <netinet/in.h>\n#include <netinet/tcp.h>\n#include <arpa/inet.h>\n#include <stdio.h>\n#include <stdlib.h>\n#include <string.h>\n#include <sys/socket.h>\n#include <sys/time.h>\n#include <unistd.h>\n#include <sys/mman.h>\n#include <linux/mman.h>\n#include <locale.h>\n#include <assert.h>\n#include <pthread.h>\n#include <liburing.h>\n\n#include \"proxy.h\"\n#include \"helpers.h\"\n\n/*\n * Will go away once/if bundles are upstreamed and we put the generic\n * definitions in the kernel header.\n */\n#ifndef IORING_RECVSEND_BUNDLE\n#define IORING_RECVSEND_BUNDLE\t\t(1U << 4)\n#endif\n#ifndef IORING_FEAT_SEND_BUF_SELECT\n#define IORING_FEAT_SEND_BUF_SELECT\t(1U << 14)\n#endif\n\nstatic int cur_bgid = 1;\nstatic int nr_conns;\nstatic int open_conns;\nstatic long page_size;\n\nstatic unsigned long event_loops;\nstatic unsigned long events;\n\nstatic int recv_mshot = 1;\nstatic int sqpoll;\nstatic int defer_tw = 1;\nstatic int is_sink;\nstatic int fixed_files = 1;\nstatic char *host = \"192.168.3.2\";\nstatic int send_port = 4445;\nstatic int receive_port = 4444;\nstatic int buf_size = 32;\nstatic int bidi;\nstatic int ipv6;\nstatic int napi;\nstatic int napi_timeout;\nstatic int wait_batch = 1;\nstatic int wait_usec = 1000000;\nstatic int rcv_msg;\nstatic int snd_msg;\nstatic int snd_zc;\nstatic int send_ring = -1;\nstatic int snd_bundle;\nstatic int rcv_bundle;\nstatic int use_huge;\nstatic int ext_stat;\nstatic int verbose;\n\nstatic int nr_bufs = 256;\nstatic int br_mask;\n\nstatic int ring_size = 128;\n\nstatic pthread_mutex_t thread_lock;\nstatic struct timeval last_housekeeping;\n\n/*\n * For sendmsg/recvmsg. recvmsg just has a single vec, sendmsg will have\n * two vecs - one that is currently submitted and being sent, and one that\n * is being prepared. When a new sendmsg is issued, we'll swap which one we\n * use. For send, even though we don't pass in the iovec itself, we use the\n * vec to serialize the sends to avoid reordering.\n */\nstruct msg_vec {\n\tstruct iovec *iov;\n\t/* length of allocated vec */\n\tint vec_size;\n\t/* length currently being used */\n\tint iov_len;\n\t/* only for send, current index we're processing */\n\tint cur_iov;\n};\n\nstruct io_msg {\n\tstruct msghdr msg;\n\tstruct msg_vec vecs[2];\n\t/* current msg_vec being prepared */\n\tint vec_index;\n};\n\n/*\n * Per socket stats per connection. For bi-directional, we'll have both\n * sends and receives on each socket, this helps track them seperately.\n * For sink or one directional, each of the two stats will be only sends\n * or receives, not both.\n */\nstruct conn_dir {\n\tint index;\n\n\tint pending_shutdown;\n\tint pending_send;\n\tint pending_recv;\n\n\tint snd_notif;\n\n\tint out_buffers;\n\n\tint rcv, rcv_shrt, rcv_enobufs, rcv_mshot;\n\tint snd, snd_shrt, snd_enobufs, snd_busy, snd_mshot;\n\n\tint snd_next_bid;\n\tint rcv_next_bid;\n\n\tint *rcv_bucket;\n\tint *snd_bucket;\n\n\tunsigned long in_bytes, out_bytes;\n\n\t/* only ever have a single recv pending */\n\tstruct io_msg io_rcv_msg;\n\n\t/* one send that is inflight, and one being prepared for the next one */\n\tstruct io_msg io_snd_msg;\n};\n\nenum {\n\tCONN_F_STARTED\t\t= 1,\n\tCONN_F_DISCONNECTING\t= 2,\n\tCONN_F_DISCONNECTED\t= 4,\n\tCONN_F_PENDING_SHUTDOWN\t= 8,\n\tCONN_F_STATS_SHOWN\t= 16,\n\tCONN_F_END_TIME\t\t= 32,\n\tCONN_F_REAPED\t\t= 64,\n};\n\n/*\n * buffer ring belonging to a connection\n */\nstruct conn_buf_ring {\n\tstruct io_uring_buf_ring *br;\n\tvoid *buf;\n\tint bgid;\n};\n\nstruct conn {\n\tstruct io_uring ring;\n\n\t/* receive side buffer ring, new data arrives here */\n\tstruct conn_buf_ring in_br;\n\t/* if send_ring is used, outgoing data to send */\n\tstruct conn_buf_ring out_br;\n\n\tint tid;\n\tint in_fd, out_fd;\n\tint pending_cancels;\n\tint flags;\n\n\tstruct conn_dir cd[2];\n\n\tstruct timeval start_time, end_time;\n\n\tunion {\n\t\tstruct sockaddr_in addr;\n\t\tstruct sockaddr_in6 addr6;\n\t};\n\n\tpthread_t thread;\n\tpthread_barrier_t startup_barrier;\n};\n\n#define MAX_CONNS\t1024\nstatic struct conn conns[MAX_CONNS];\n\n#define vlog(str, ...) do {\t\t\t\t\t\t\\\n\tif (verbose)\t\t\t\t\t\t\t\\\n\t\tprintf(str, ##__VA_ARGS__);\t\t\t\t\\\n} while (0)\n\nstatic int prep_next_send(struct io_uring *ring, struct conn *c,\n\t\t\t  struct conn_dir *cd, int fd);\nstatic void *thread_main(void *data);\n\nstatic struct conn *cqe_to_conn(struct io_uring_cqe *cqe)\n{\n\tstruct userdata ud = { .val = cqe->user_data };\n\n\treturn &conns[ud.op_tid & TID_MASK];\n}\n\nstatic struct conn_dir *cqe_to_conn_dir(struct conn *c,\n\t\t\t\t\tstruct io_uring_cqe *cqe)\n{\n\tint fd = cqe_to_fd(cqe);\n\n\treturn &c->cd[fd != c->in_fd];\n}\n\nstatic int other_dir_fd(struct conn *c, int fd)\n{\n\tif (c->in_fd == fd)\n\t\treturn c->out_fd;\n\treturn c->in_fd;\n}\n\n/* currently active msg_vec */\nstatic struct msg_vec *msg_vec(struct io_msg *imsg)\n{\n\treturn &imsg->vecs[imsg->vec_index];\n}\n\nstatic struct msg_vec *snd_msg_vec(struct conn_dir *cd)\n{\n\treturn msg_vec(&cd->io_snd_msg);\n}\n\n/*\n * Goes from accept new connection -> create socket, connect to end\n * point, prepare recv, on receive do send (unless sink). If either ends\n * disconnects, we transition to shutdown and then close.\n */\nenum {\n\t__ACCEPT\t= 1,\n\t__SOCK\t\t= 2,\n\t__CONNECT\t= 3,\n\t__RECV\t\t= 4,\n\t__RECVMSG\t= 5,\n\t__SEND\t\t= 6,\n\t__SENDMSG\t= 7,\n\t__SHUTDOWN\t= 8,\n\t__CANCEL\t= 9,\n\t__CLOSE\t\t= 10,\n\t__FD_PASS\t= 11,\n\t__NOP\t\t= 12,\n\t__STOP\t\t= 13,\n};\n\nstruct error_handler {\n\tconst char *name;\n\tint (*error_fn)(struct error_handler *, struct io_uring *, struct io_uring_cqe *);\n};\n\nstatic int recv_error(struct error_handler *err, struct io_uring *ring,\n\t\t      struct io_uring_cqe *cqe);\nstatic int send_error(struct error_handler *err, struct io_uring *ring,\n\t\t      struct io_uring_cqe *cqe);\n\nstatic int default_error(struct error_handler *err,\n\t\t\t struct io_uring __attribute__((__unused__)) *ring,\n\t\t\t struct io_uring_cqe *cqe)\n{\n\tstruct conn *c = cqe_to_conn(cqe);\n\n\tfprintf(stderr, \"%d: %s error %s\\n\", c->tid, err->name, strerror(-cqe->res));\n\tfprintf(stderr, \"fd=%d, bid=%d\\n\", cqe_to_fd(cqe), cqe_to_bid(cqe));\n\treturn 1;\n}\n\n/*\n * Move error handling out of the normal handling path, cleanly seperating\n * them. If an opcode doesn't need any error handling, set it to NULL. If\n * it wants to stop the connection at that point and not do anything else,\n * then the default handler can be used. Only receive has proper error\n * handling, as we can get -ENOBUFS which is not a fatal condition. It just\n * means we need to wait on buffer replenishing before re-arming the receive.\n */\nstatic struct error_handler error_handlers[] = {\n\t{ .name = \"NULL\",\t.error_fn = NULL, },\n\t{ .name = \"ACCEPT\",\t.error_fn = default_error, },\n\t{ .name = \"SOCK\",\t.error_fn = default_error, },\n\t{ .name = \"CONNECT\",\t.error_fn = default_error, },\n\t{ .name = \"RECV\",\t.error_fn = recv_error, },\n\t{ .name = \"RECVMSG\",\t.error_fn = recv_error, },\n\t{ .name = \"SEND\",\t.error_fn = send_error, },\n\t{ .name = \"SENDMSG\",\t.error_fn = send_error, },\n\t{ .name = \"SHUTDOWN\",\t.error_fn = NULL, },\n\t{ .name = \"CANCEL\",\t.error_fn = NULL, },\n\t{ .name = \"CLOSE\",\t.error_fn = NULL, },\n\t{ .name = \"FD_PASS\",\t.error_fn = default_error, },\n\t{ .name = \"NOP\",\t.error_fn = NULL, },\n\t{ .name = \"STOP\",\t.error_fn = default_error, },\n};\n\nstatic void free_buffer_ring(struct io_uring *ring, struct conn_buf_ring *cbr)\n{\n\tif (!cbr->br)\n\t\treturn;\n\n\tio_uring_free_buf_ring(ring, cbr->br, nr_bufs, cbr->bgid);\n\tcbr->br = NULL;\n\tif (use_huge)\n\t\tmunmap(cbr->buf, buf_size * nr_bufs);\n\telse\n\t\tfree(cbr->buf);\n}\n\nstatic void free_buffer_rings(struct io_uring *ring, struct conn *c)\n{\n\tfree_buffer_ring(ring, &c->in_br);\n\tfree_buffer_ring(ring, &c->out_br);\n}\n\n/*\n * Setup a ring provided buffer ring for each connection. If we get -ENOBUFS\n * on receive, for multishot receive we'll wait for half the provided buffers\n * to be returned by pending sends, then re-arm the multishot receive. If\n * this happens too frequently (see enobufs= stat), then the ring size is\n * likely too small. Use -nXX to make it bigger. See recv_enobufs().\n *\n * The alternative here would be to use the older style provided buffers,\n * where you simply setup a buffer group and use SQEs with\n * io_urign_prep_provide_buffers() to add to the pool. But that approach is\n * slower and has been deprecated by using the faster ring provided buffers.\n */\nstatic int setup_recv_ring(struct io_uring *ring, struct conn *c)\n{\n\tstruct conn_buf_ring *cbr = &c->in_br;\n\tint ret, i;\n\tsize_t len;\n\tvoid *ptr;\n\n\tlen = buf_size * nr_bufs;\n\tif (use_huge) {\n\t\tcbr->buf = mmap(NULL, len, PROT_READ|PROT_WRITE,\n\t\t\t\tMAP_PRIVATE|MAP_HUGETLB|MAP_HUGE_2MB|MAP_ANONYMOUS,\n\t\t\t\t-1, 0);\n\t\tif (cbr->buf == MAP_FAILED) {\n\t\t\tperror(\"mmap\");\n\t\t\treturn 1;\n\t\t}\n\t} else {\n\t\tif (posix_memalign(&cbr->buf, page_size, len)) {\n\t\t\tperror(\"posix memalign\");\n\t\t\treturn 1;\n\t\t}\n\t}\n\tcbr->br = io_uring_setup_buf_ring(ring, nr_bufs, cbr->bgid, 0, &ret);\n\tif (!cbr->br) {\n\t\tfprintf(stderr, \"Buffer ring register failed %d\\n\", ret);\n\t\treturn 1;\n\t}\n\n\tptr = cbr->buf;\n\tfor (i = 0; i < nr_bufs; i++) {\n\t\tvlog(\"%d: add bid %d, data %p\\n\", c->tid, i, ptr);\n\t\tio_uring_buf_ring_add(cbr->br, ptr, buf_size, i, br_mask, i);\n\t\tptr += buf_size;\n\t}\n\tio_uring_buf_ring_advance(cbr->br, nr_bufs);\n\tprintf(\"%d: recv buffer ring bgid %d, bufs %d\\n\", c->tid, cbr->bgid, nr_bufs);\n\treturn 0;\n}\n\n/*\n * If 'send_ring' is used and the kernel supports it, we can skip serializing\n * sends as the data will be ordered regardless. This reduces the send handling\n * complexity, as buffers can always be added to the outgoing ring and will be\n * processed in the order in which they were added.\n */\nstatic int setup_send_ring(struct io_uring *ring, struct conn *c)\n{\n\tstruct conn_buf_ring *cbr = &c->out_br;\n\tint ret;\n\n\tcbr->br = io_uring_setup_buf_ring(ring, nr_bufs, cbr->bgid, 0, &ret);\n\tif (!cbr->br) {\n\t\tfprintf(stderr, \"Buffer ring register failed %d\\n\", ret);\n\t\treturn 1;\n\t}\n\n\tprintf(\"%d: send buffer ring bgid %d, bufs %d\\n\", c->tid, cbr->bgid, nr_bufs);\n\treturn 0;\n}\n\nstatic int setup_send_zc(struct io_uring *ring, struct conn *c)\n{\n\tstruct iovec *iovs;\n\tvoid *buf;\n\tint i, ret;\n\n\tif (snd_msg)\n\t\treturn 0;\n\n\tbuf = c->in_br.buf;\n\tiovs = calloc(nr_bufs, sizeof(struct iovec));\n\tfor (i = 0; i < nr_bufs; i++) {\n\t\tiovs[i].iov_base = buf;\n\t\tiovs[i].iov_len = buf_size;\n\t\tbuf += buf_size;\n\t}\n\n\tret = io_uring_register_buffers(ring, iovs, nr_bufs);\n\tif (ret) {\n\t\tfprintf(stderr, \"failed registering buffers: %d\\n\", ret);\n\t\tfree(iovs);\n\t\treturn ret;\n\t}\n\tfree(iovs);\n\treturn 0;\n}\n\n/*\n * Setup an input and output buffer ring.\n */\nstatic int setup_buffer_rings(struct io_uring *ring, struct conn *c)\n{\n\tint ret;\n\n\t/* no locking needed on cur_bgid, parent serializes setup */\n\tc->in_br.bgid = cur_bgid++;\n\tc->out_br.bgid = cur_bgid++;\n\tc->out_br.br = NULL;\n\n\tret = setup_recv_ring(ring, c);\n\tif (ret)\n\t\treturn ret;\n\tif (is_sink)\n\t\treturn 0;\n\tif (snd_zc) {\n\t\tret = setup_send_zc(ring, c);\n\t\tif (ret)\n\t\t\treturn ret;\n\t}\n\tif (send_ring) {\n\t\tret = setup_send_ring(ring, c);\n\t\tif (ret) {\n\t\t\tfree_buffer_ring(ring, &c->in_br);\n\t\t\treturn ret;\n\t\t}\n\t}\n\n\treturn 0;\n}\n\nstatic void show_buckets(struct conn_dir *cd)\n{\n\tint i;\n\n\tif (!cd->rcv_bucket || !cd->snd_bucket)\n\t\treturn;\n\n\tprintf(\"\\t Packets per recv/send:\\n\");\n\tfor (i = 0; i < nr_bufs; i++) {\n\t\tif (!cd->rcv_bucket[i] && !cd->snd_bucket[i])\n\t\t\tcontinue;\n\t\tprintf(\"\\t bucket(%3d): rcv=%u snd=%u\\n\", i, cd->rcv_bucket[i],\n\t\t\t\t\t\t\t     cd->snd_bucket[i]);\n\t}\n}\n\nstatic void __show_stats(struct conn *c)\n{\n\tunsigned long msec, qps;\n\tunsigned long bytes, bw;\n\tstruct conn_dir *cd;\n\tint i;\n\n\tif (c->flags & (CONN_F_STATS_SHOWN | CONN_F_REAPED))\n\t\treturn;\n\tif (!(c->flags & CONN_F_STARTED))\n\t\treturn;\n\n\tif (!(c->flags & CONN_F_END_TIME))\n\t\tgettimeofday(&c->end_time, NULL);\n\n\tmsec = (c->end_time.tv_sec - c->start_time.tv_sec) * 1000;\n\tmsec += (c->end_time.tv_usec - c->start_time.tv_usec) / 1000;\n\n\tqps = 0;\n\tfor (i = 0; i < 2; i++)\n\t\tqps += c->cd[i].rcv + c->cd[i].snd;\n\n\tif (!qps)\n\t\treturn;\n\n\tif (msec)\n\t\tqps = (qps * 1000) / msec;\n\n\tprintf(\"Conn %d/(in_fd=%d, out_fd=%d): qps=%lu, msec=%lu\\n\", c->tid,\n\t\t\t\t\tc->in_fd, c->out_fd, qps, msec);\n\n\tbytes = 0;\n\tfor (i = 0; i < 2; i++) {\n\t\tcd = &c->cd[i];\n\n\t\tif (!cd->in_bytes && !cd->out_bytes && !cd->snd && !cd->rcv)\n\t\t\tcontinue;\n\n\t\tbytes += cd->in_bytes;\n\t\tbytes += cd->out_bytes;\n\n\t\tprintf(\"\\t%3d: rcv=%u (short=%u, enobufs=%d), snd=%u (short=%u,\"\n\t\t\t\" busy=%u, enobufs=%d)\\n\", i, cd->rcv, cd->rcv_shrt,\n\t\t\tcd->rcv_enobufs, cd->snd, cd->snd_shrt, cd->snd_busy,\n\t\t\tcd->snd_enobufs);\n\t\tprintf(\"\\t   : in_bytes=%lu (Kb %lu), out_bytes=%lu (Kb %lu)\\n\",\n\t\t\tcd->in_bytes, cd->in_bytes >> 10,\n\t\t\tcd->out_bytes, cd->out_bytes >> 10);\n\t\tprintf(\"\\t   : mshot_rcv=%d, mshot_snd=%d\\n\", cd->rcv_mshot,\n\t\t\tcd->snd_mshot);\n\t\tshow_buckets(cd);\n\n\t}\n\tif (msec) {\n\t\tbytes *= 8UL;\n\t\tbw = bytes / 1000;\n\t\tbw /= msec;\n\t\tprintf(\"\\tBW=%'luMbit\\n\", bw);\n\t}\n\n\tc->flags |= CONN_F_STATS_SHOWN;\n}\n\nstatic void show_stats(void)\n{\n\tfloat events_per_loop = 0.0;\n\tstatic int stats_shown;\n\tint i;\n\n\tif (stats_shown)\n\t\treturn;\n\n\tif (events)\n\t\tevents_per_loop = (float) events / (float) event_loops;\n\n\tprintf(\"Event loops: %lu, events %lu, events per loop %.2f\\n\", event_loops,\n\t\t\t\t\t\t\tevents, events_per_loop);\n\n\tfor (i = 0; i < MAX_CONNS; i++) {\n\t\tstruct conn *c = &conns[i];\n\n\t\t__show_stats(c);\n\t}\n\tstats_shown = 1;\n}\n\nstatic void sig_int(int __attribute__((__unused__)) sig)\n{\n\tprintf(\"\\n\");\n\tshow_stats();\n\texit(1);\n}\n\n/*\n * Special cased for SQPOLL only, as we don't control when SQEs are consumed if\n * that is used. Hence we may need to wait for the SQPOLL thread to keep up\n * until we can get a new SQE. All other cases will break immediately, with a\n * fresh SQE.\n *\n * If we grossly undersized our SQ ring, getting a NULL sqe can happen even\n * for the !SQPOLL case if we're handling a lot of CQEs in our event loop\n * and multishot isn't used. We can do io_uring_submit() to flush what we\n * have here. Only caveat here is that if linked requests are used, SQEs\n * would need to be allocated upfront as a link chain is only valid within\n * a single submission cycle.\n */\nstatic struct io_uring_sqe *get_sqe(struct io_uring *ring)\n{\n\tstruct io_uring_sqe *sqe;\n\n\tdo {\n\t\tsqe = io_uring_get_sqe(ring);\n\t\tif (sqe)\n\t\t\tbreak;\n\t\tif (!sqpoll)\n\t\t\tio_uring_submit(ring);\n\t\telse\n\t\t\tio_uring_sqring_wait(ring);\n\t} while (1);\n\n\treturn sqe;\n}\n\n/*\n * See __encode_userdata() for how we encode sqe->user_data, which is passed\n * back as cqe->user_data at completion time.\n */\nstatic void encode_userdata(struct io_uring_sqe *sqe, struct conn *c, int op,\n\t\t\t    int bid, int fd)\n{\n\t__encode_userdata(sqe, c->tid, op, bid, fd);\n}\n\nstatic void __submit_receive(struct io_uring *ring, struct conn *c,\n\t\t\t     struct conn_dir *cd, int fd)\n{\n\tstruct conn_buf_ring *cbr = &c->in_br;\n\tstruct io_uring_sqe *sqe;\n\n\tvlog(\"%d: submit receive fd=%d\\n\", c->tid, fd);\n\n\tassert(!cd->pending_recv);\n\tcd->pending_recv = 1;\n\n\t/*\n\t * For both recv and multishot receive, we use the ring provided\n\t * buffers. These are handed to the application ahead of time, and\n\t * are consumed when a receive triggers. Note that the address and\n\t * length of the receive are set to NULL/0, and we assign the\n\t * sqe->buf_group to tell the kernel which buffer group ID to pick\n\t * a buffer from. Finally, IOSQE_BUFFER_SELECT is set to tell the\n\t * kernel that we want a buffer picked for this request, we are not\n\t * passing one in with the request.\n\t */\n\tsqe = get_sqe(ring);\n\tif (rcv_msg) {\n\t\tstruct io_msg *imsg = &cd->io_rcv_msg;\n\t\tstruct msghdr *msg = &imsg->msg;\n\n\t\tmemset(msg, 0, sizeof(*msg));\n\t\tmsg->msg_iov = msg_vec(imsg)->iov;\n\t\tmsg->msg_iovlen = msg_vec(imsg)->iov_len;\n\n\t\tif (recv_mshot) {\n\t\t\tcd->rcv_mshot++;\n\t\t\tio_uring_prep_recvmsg_multishot(sqe, fd, &imsg->msg, 0);\n\t\t} else {\n\t\t\tio_uring_prep_recvmsg(sqe, fd, &imsg->msg, 0);\n\t\t}\n\t} else {\n\t\tif (recv_mshot) {\n\t\t\tcd->rcv_mshot++;\n\t\t\tio_uring_prep_recv_multishot(sqe, fd, NULL, 0, 0);\n\t\t} else {\n\t\t\tio_uring_prep_recv(sqe, fd, NULL, 0, 0);\n\t\t}\n\t}\n\tencode_userdata(sqe, c, __RECV, 0, fd);\n\tsqe->buf_group = cbr->bgid;\n\tsqe->flags |= IOSQE_BUFFER_SELECT;\n\tif (fixed_files)\n\t\tsqe->flags |= IOSQE_FIXED_FILE;\n\tif (rcv_bundle)\n\t\tsqe->ioprio |= IORING_RECVSEND_BUNDLE;\n}\n\n/*\n * One directional just arms receive on our in_fd\n */\nstatic void submit_receive(struct io_uring *ring, struct conn *c)\n{\n\t__submit_receive(ring, c, &c->cd[0], c->in_fd);\n}\n\n/*\n * Bi-directional arms receive on both in and out fd\n */\nstatic void submit_bidi_receive(struct io_uring *ring, struct conn *c)\n{\n\t__submit_receive(ring, c, &c->cd[0], c->in_fd);\n\t__submit_receive(ring, c, &c->cd[1], c->out_fd);\n}\n\n/*\n * We hit -ENOBUFS, which means that we ran out of buffers in our current\n * provided buffer group. This can happen if there's an imbalance between the\n * receives coming in and the sends being processed, particularly with multishot\n * receive as they can trigger very quickly. If this happens, defer arming a\n * new receive until we've replenished half of the buffer pool by processing\n * pending sends.\n */\nstatic void recv_enobufs(struct io_uring *ring, struct conn *c,\n\t\t\t struct conn_dir *cd, int fd)\n{\n\tvlog(\"%d: enobufs hit\\n\", c->tid);\n\n\tcd->rcv_enobufs++;\n\n\t/*\n\t * If we're a sink, mark rcv as rearm. If we're not, then mark us as\n\t * needing a rearm for receive and send. The completing send will\n\t * kick the recv rearm.\n\t */\n\tif (!is_sink) {\n\t\tint do_recv_arm = 1;\n\n\t\tif (!cd->pending_send)\n\t\t\tdo_recv_arm = !prep_next_send(ring, c, cd, fd);\n\t\tif (do_recv_arm)\n\t\t\t__submit_receive(ring, c, &c->cd[0], c->in_fd);\n\t} else {\n\t\t__submit_receive(ring, c, &c->cd[0], c->in_fd);\n\t}\n}\n\n/*\n * Kill this socket - submit a shutdown and link a close to it. We don't\n * care about shutdown status, so mark it as not needing to post a CQE unless\n * it fails.\n */\nstatic void queue_shutdown_close(struct io_uring *ring, struct conn *c, int fd)\n{\n\tstruct io_uring_sqe *sqe1, *sqe2;\n\n\t/*\n\t * On the off chance that we run out of SQEs after the first one,\n\t * grab two upfront. This it to prevent our link not working if\n\t * get_sqe() ends up doing submissions to free up an SQE, as links\n\t * are not valid across separate submissions.\n\t */\n\tsqe1 = get_sqe(ring);\n\tsqe2 = get_sqe(ring);\n\n\tio_uring_prep_shutdown(sqe1, fd, SHUT_RDWR);\n\tif (fixed_files)\n\t\tsqe1->flags |= IOSQE_FIXED_FILE;\n\tsqe1->flags |= IOSQE_IO_LINK | IOSQE_CQE_SKIP_SUCCESS;\n\tencode_userdata(sqe1, c, __SHUTDOWN, 0, fd);\n\n\tif (fixed_files)\n\t\tio_uring_prep_close_direct(sqe2, fd);\n\telse\n\t\tio_uring_prep_close(sqe2, fd);\n\tencode_userdata(sqe2, c, __CLOSE, 0, fd);\n}\n\n/*\n * This connection is going away, queue a cancel for any pending recv, for\n * example, we have pending for this ring. For completeness, we issue a cancel\n * for any request we have pending for both in_fd and out_fd.\n */\nstatic void queue_cancel(struct io_uring *ring, struct conn *c)\n{\n\tstruct io_uring_sqe *sqe;\n\tint flags = 0;\n\n\tif (fixed_files)\n\t\tflags |= IORING_ASYNC_CANCEL_FD_FIXED;\n\n\tsqe = get_sqe(ring);\n\tio_uring_prep_cancel_fd(sqe, c->in_fd, flags);\n\tencode_userdata(sqe, c, __CANCEL, 0, c->in_fd);\n\tc->pending_cancels++;\n\n\tif (c->out_fd != -1) {\n\t\tsqe = get_sqe(ring);\n\t\tio_uring_prep_cancel_fd(sqe, c->out_fd, flags);\n\t\tencode_userdata(sqe, c, __CANCEL, 0, c->out_fd);\n\t\tc->pending_cancels++;\n\t}\n\n\tio_uring_submit(ring);\n}\n\nstatic int pending_shutdown(struct conn *c)\n{\n\treturn c->cd[0].pending_shutdown + c->cd[1].pending_shutdown;\n}\n\nstatic bool should_shutdown(struct conn *c)\n{\n\tint i;\n\n\tif (!pending_shutdown(c))\n\t\treturn false;\n\tif (is_sink)\n\t\treturn true;\n\tif (!bidi)\n\t\treturn c->cd[0].in_bytes == c->cd[1].out_bytes;\n\n\tfor (i = 0; i < 2; i++) {\n\t\tif (c->cd[0].rcv != c->cd[1].snd)\n\t\t\treturn false;\n\t\tif (c->cd[1].rcv != c->cd[0].snd)\n\t\t\treturn false;\n\t}\n\n\treturn true;\n}\n\n/*\n * Close this connection - send a ring message to the connection with intent\n * to stop. When the client gets the message, it will initiate the stop.\n */\nstatic void __close_conn(struct io_uring *ring, struct conn *c)\n{\n\tstruct io_uring_sqe *sqe;\n\tuint64_t user_data;\n\n\tprintf(\"Client %d: queueing stop\\n\", c->tid);\n\n\tuser_data = __raw_encode(c->tid, __STOP, 0, 0);\n\tsqe = io_uring_get_sqe(ring);\n\tio_uring_prep_msg_ring(sqe, c->ring.ring_fd, 0, user_data, 0);\n\tencode_userdata(sqe, c, __NOP, 0, 0);\n\tio_uring_submit(ring);\n}\n\nstatic void close_cd(struct conn *c, struct conn_dir *cd)\n{\n\tcd->pending_shutdown = 1;\n\n\tif (cd->pending_send)\n\t\treturn;\n\n\tif (!(c->flags & CONN_F_PENDING_SHUTDOWN)) {\n\t\tgettimeofday(&c->end_time, NULL);\n\t\tc->flags |= CONN_F_PENDING_SHUTDOWN | CONN_F_END_TIME;\n\t}\n}\n\n/*\n * We're done with this buffer, add it back to our pool so the kernel is\n * free to use it again.\n */\nstatic int replenish_buffer(struct conn_buf_ring *cbr, int bid, int offset)\n{\n\tvoid *this_buf = cbr->buf + bid * buf_size;\n\n\tassert(bid < nr_bufs);\n\n\tio_uring_buf_ring_add(cbr->br, this_buf, buf_size, bid, br_mask, offset);\n\treturn buf_size;\n}\n\n/*\n * Iterate buffers from '*bid' and with a total size of 'bytes' and add them\n * back to our receive ring so they can be reused for new receives.\n */\nstatic int replenish_buffers(struct conn *c, int *bid, int bytes)\n{\n\tstruct conn_buf_ring *cbr = &c->in_br;\n\tint nr_packets = 0;\n\n\twhile (bytes) {\n\t\tint this_len = replenish_buffer(cbr, *bid, nr_packets);\n\n\t\tif (this_len > bytes)\n\t\t\tthis_len = bytes;\n\t\tbytes -= this_len;\n\n\t\t*bid = (*bid + 1) & (nr_bufs - 1);\n\t\tnr_packets++;\n\t}\n\n\tio_uring_buf_ring_advance(cbr->br, nr_packets);\n\treturn nr_packets;\n}\n\nstatic void free_mvec(struct msg_vec *mvec)\n{\n\tfree(mvec->iov);\n\tmvec->iov = NULL;\n}\n\nstatic void init_mvec(struct msg_vec *mvec)\n{\n\tmemset(mvec, 0, sizeof(*mvec));\n\tmvec->iov = malloc(sizeof(struct iovec));\n\tmvec->vec_size = 1;\n}\n\nstatic void init_msgs(struct conn_dir *cd)\n{\n\tmemset(&cd->io_snd_msg, 0, sizeof(cd->io_snd_msg));\n\tmemset(&cd->io_rcv_msg, 0, sizeof(cd->io_rcv_msg));\n\tinit_mvec(&cd->io_snd_msg.vecs[0]);\n\tinit_mvec(&cd->io_snd_msg.vecs[1]);\n\tinit_mvec(&cd->io_rcv_msg.vecs[0]);\n}\n\nstatic void free_msgs(struct conn_dir *cd)\n{\n\tfree_mvec(&cd->io_snd_msg.vecs[0]);\n\tfree_mvec(&cd->io_snd_msg.vecs[1]);\n\tfree_mvec(&cd->io_rcv_msg.vecs[0]);\n}\n\n/*\n * Multishot accept completion triggered. If we're acting as a sink, we're\n * good to go. Just issue a receive for that case. If we're acting as a proxy,\n * then start opening a socket that we can use to connect to the other end.\n */\nstatic int handle_accept(struct io_uring *ring, struct io_uring_cqe *cqe)\n{\n\tstruct conn *c;\n\tint i;\n\n\tif (nr_conns == MAX_CONNS) {\n\t\tfprintf(stderr, \"max clients reached %d\\n\", nr_conns);\n\t\treturn 1;\n\t}\n\n\t/* main thread handles this, which is obviously serialized */\n\tc = &conns[nr_conns];\n\tc->tid = nr_conns++;\n\tc->in_fd = -1;\n\tc->out_fd = -1;\n\n\tfor (i = 0; i < 2; i++) {\n\t\tstruct conn_dir *cd = &c->cd[i];\n\n\t\tcd->index = i;\n\t\tcd->snd_next_bid = -1;\n\t\tcd->rcv_next_bid = -1;\n\t\tif (ext_stat) {\n\t\t\tcd->rcv_bucket = calloc(nr_bufs, sizeof(int));\n\t\t\tcd->snd_bucket = calloc(nr_bufs, sizeof(int));\n\t\t}\n\t\tinit_msgs(cd);\n\t}\n\n\tprintf(\"New client: id=%d, in=%d\\n\", c->tid, c->in_fd);\n\tgettimeofday(&c->start_time, NULL);\n\n\tpthread_barrier_init(&c->startup_barrier, NULL, 2);\n\tpthread_create(&c->thread, NULL, thread_main, c);\n\n\t/*\n\t * Wait for thread to have its ring setup, then either assign the fd\n\t * if it's non-fixed, or pass the fixed one\n\t */\n\tpthread_barrier_wait(&c->startup_barrier);\n\tif (!fixed_files) {\n\t\tc->in_fd = cqe->res;\n\t} else {\n\t\tstruct io_uring_sqe *sqe;\n\t\tuint64_t user_data;\n\n\t\t/*\n\t\t * Ring has just been setup, we'll use index 0 as the descriptor\n\t\t * value.\n\t\t */\n\t\tuser_data = __raw_encode(c->tid, __FD_PASS, 0, 0);\n\t\tsqe = io_uring_get_sqe(ring);\n\t\tio_uring_prep_msg_ring_fd(sqe, c->ring.ring_fd, cqe->res, 0,\n\t\t\t\t\t\tuser_data, 0);\n\t\tencode_userdata(sqe, c, __NOP, 0, cqe->res);\n\t}\n\n\treturn 0;\n}\n\n/*\n * Our socket request completed, issue a connect request to the other end.\n */\nstatic int handle_sock(struct io_uring *ring, struct io_uring_cqe *cqe)\n{\n\tstruct conn *c = cqe_to_conn(cqe);\n\tstruct io_uring_sqe *sqe;\n\tint ret;\n\n\tvlog(\"%d: sock: res=%d\\n\", c->tid, cqe->res);\n\n\tc->out_fd = cqe->res;\n\n\tif (ipv6) {\n\t\tmemset(&c->addr6, 0, sizeof(c->addr6));\n\t\tc->addr6.sin6_family = AF_INET6;\n\t\tc->addr6.sin6_port = htons(send_port);\n\t\tret = inet_pton(AF_INET6, host, &c->addr6.sin6_addr);\n\t} else {\n\t\tmemset(&c->addr, 0, sizeof(c->addr));\n\t\tc->addr.sin_family = AF_INET;\n\t\tc->addr.sin_port = htons(send_port);\n\t\tret = inet_pton(AF_INET, host, &c->addr.sin_addr);\n\t}\n\tif (ret <= 0) {\n\t\tif (!ret)\n\t\t\tfprintf(stderr, \"host not in right format\\n\");\n\t\telse\n\t\t\tperror(\"inet_pton\");\n\t\treturn 1;\n\t}\n\n\tsqe = get_sqe(ring);\n\tif (ipv6) {\n\t\tio_uring_prep_connect(sqe, c->out_fd,\n\t\t\t\t\t(struct sockaddr *) &c->addr6,\n\t\t\t\t\tsizeof(c->addr6));\n\t} else {\n\t\tio_uring_prep_connect(sqe, c->out_fd,\n\t\t\t\t\t(struct sockaddr *) &c->addr,\n\t\t\t\t\tsizeof(c->addr));\n\t}\n\tencode_userdata(sqe, c, __CONNECT, 0, c->out_fd);\n\tif (fixed_files)\n\t\tsqe->flags |= IOSQE_FIXED_FILE;\n\treturn 0;\n}\n\n/*\n * Connection to the other end is done, submit a receive to start receiving\n * data. If we're a bidirectional proxy, issue a receive on both ends. If not,\n * then just a single recv will do.\n */\nstatic int handle_connect(struct io_uring *ring, struct io_uring_cqe *cqe)\n{\n\tstruct conn *c = cqe_to_conn(cqe);\n\n\tpthread_mutex_lock(&thread_lock);\n\topen_conns++;\n\tpthread_mutex_unlock(&thread_lock);\n\n\tif (bidi)\n\t\tsubmit_bidi_receive(ring, c);\n\telse\n\t\tsubmit_receive(ring, c);\n\n\treturn 0;\n}\n\n/*\n * Append new segment to our currently active msg_vec. This will be submitted\n * as a sendmsg (with all of it), or as separate sends, later. If we're using\n * send_ring, then we won't hit this path. Instead, outgoing buffers are\n * added directly to our outgoing send buffer ring.\n */\nstatic void send_append_vec(struct conn_dir *cd, void *data, int len)\n{\n\tstruct msg_vec *mvec = snd_msg_vec(cd);\n\n\tif (mvec->iov_len == mvec->vec_size) {\n\t\tmvec->vec_size <<= 1;\n\t\tmvec->iov = realloc(mvec->iov, mvec->vec_size * sizeof(struct iovec));\n\t}\n\n\tmvec->iov[mvec->iov_len].iov_base = data;\n\tmvec->iov[mvec->iov_len].iov_len = len;\n\tmvec->iov_len++;\n}\n\n/*\n * Queue a send based on the data received in this cqe, which came from\n * a completed receive operation.\n */\nstatic void send_append(struct conn *c, struct conn_dir *cd, void *data,\n\t\t\tint bid, int len)\n{\n\tvlog(\"%d: send %d (%p, bid %d)\\n\", c->tid, len, data, bid);\n\n\tassert(bid < nr_bufs);\n\n\t/* if using provided buffers for send, add it upfront */\n\tif (send_ring) {\n\t\tstruct conn_buf_ring *cbr = &c->out_br;\n\n\t\tio_uring_buf_ring_add(cbr->br, data, len, bid, br_mask, 0);\n\t\tio_uring_buf_ring_advance(cbr->br, 1);\n\t} else {\n\t\tsend_append_vec(cd, data, len);\n\t}\n}\n\n/*\n * For non recvmsg && multishot, a zero receive marks the end. For recvmsg\n * with multishot, we always get the header regardless. Hence a \"zero receive\"\n * is the size of the header.\n */\nstatic int recv_done_res(int res)\n{\n\tif (!res)\n\t\treturn 1;\n\tif (rcv_msg && recv_mshot && res == sizeof(struct io_uring_recvmsg_out))\n\t\treturn 1;\n\treturn 0;\n}\n\n/*\n * Any receive that isn't recvmsg with multishot can be handled the same way.\n * Iterate from '*bid' and 'in_bytes' in total, and append the data to the\n * outgoing queue.\n */\nstatic int recv_bids(struct conn *c, struct conn_dir *cd, int *bid, int in_bytes)\n{\n\tstruct conn_buf_ring *cbr = &c->out_br;\n\tstruct conn_buf_ring *in_cbr = &c->in_br;\n\tstruct io_uring_buf *buf;\n\tint nr_packets = 0;\n\n\twhile (in_bytes) {\n\t\tint this_bytes;\n\t\tvoid *data;\n\n\t\tbuf = &in_cbr->br->bufs[*bid];\n\t\tdata = (void *) (unsigned long) buf->addr;\n\t\tthis_bytes = buf->len;\n\t\tif (this_bytes > in_bytes)\n\t\t\tthis_bytes = in_bytes;\n\n\t\tin_bytes -= this_bytes;\n\n\t\tif (send_ring)\n\t\t\tio_uring_buf_ring_add(cbr->br, data, this_bytes, *bid,\n\t\t\t\t\t\tbr_mask, nr_packets);\n\t\telse\n\t\t\tsend_append(c, cd, data, *bid, this_bytes);\n\n\t\t*bid = (*bid + 1) & (nr_bufs - 1);\n\t\tnr_packets++;\n\t}\n\n\tif (send_ring)\n\t\tio_uring_buf_ring_advance(cbr->br, nr_packets);\n\n\treturn nr_packets;\n}\n\n/*\n * Special handling of recvmsg with multishot\n */\nstatic int recv_mshot_msg(struct conn *c, struct conn_dir *cd, int *bid,\n\t\t\t  int in_bytes)\n{\n\tstruct conn_buf_ring *cbr = &c->out_br;\n\tstruct conn_buf_ring *in_cbr = &c->in_br;\n\tstruct io_uring_buf *buf;\n\tint nr_packets = 0;\n\n\twhile (in_bytes) {\n\t\tstruct io_uring_recvmsg_out *pdu;\n\t\tint this_bytes;\n\t\tvoid *data;\n\n\t\tbuf = &in_cbr->br->bufs[*bid];\n\n\t\t/*\n\t\t * multishot recvmsg puts a header in front of the data - we\n\t\t * have to take that into account for the send setup, and\n\t\t * adjust the actual data read to not take this metadata into\n\t\t * account. For this use case, namelen and controllen will not\n\t\t * be set. If they were, they would need to be factored in too.\n\t\t */\n\t\tbuf->len -= sizeof(struct io_uring_recvmsg_out);\n\t\tin_bytes -= sizeof(struct io_uring_recvmsg_out);\n\n\t\tpdu = (void *) (unsigned long) buf->addr;\n\t\tvlog(\"pdu namelen %d, controllen %d, payload %d flags %x\\n\",\n\t\t\t\tpdu->namelen, pdu->controllen, pdu->payloadlen,\n\t\t\t\tpdu->flags);\n\t\tdata = (void *) (pdu + 1);\n\n\t\tthis_bytes = pdu->payloadlen;\n\t\tif (this_bytes > in_bytes)\n\t\t\tthis_bytes = in_bytes;\n\n\t\tin_bytes -= this_bytes;\n\n\t\tif (send_ring)\n\t\t\tio_uring_buf_ring_add(cbr->br, data, this_bytes, *bid,\n\t\t\t\t\t\tbr_mask, nr_packets);\n\t\telse\n\t\t\tsend_append(c, cd, data, *bid, this_bytes);\n\n\t\t*bid = (*bid + 1) & (nr_bufs - 1);\n\t\tnr_packets++;\n\t}\n\n\tif (send_ring)\n\t\tio_uring_buf_ring_advance(cbr->br, nr_packets);\n\n\treturn nr_packets;\n}\n\nstatic int __handle_recv(struct io_uring *ring, struct conn *c,\n\t\t\t struct conn_dir *cd, struct io_uring_cqe *cqe)\n{\n\tstruct conn_dir *ocd = &c->cd[!cd->index];\n\tint bid, nr_packets;\n\n\t/*\n\t * Not having a buffer attached should only happen if we get a zero\n\t * sized receive, because the other end closed the connection. It\n\t * cannot happen otherwise, as all our receives are using provided\n\t * buffers and hence it's not possible to return a CQE with a non-zero\n\t * result and not have a buffer attached.\n\t */\n\tif (!(cqe->flags & IORING_CQE_F_BUFFER)) {\n\t\tcd->pending_recv = 0;\n\n\t\tif (!recv_done_res(cqe->res)) {\n\t\t\tfprintf(stderr, \"no buffer assigned, res=%d\\n\", cqe->res);\n\t\t\treturn 1;\n\t\t}\nstart_close:\n\t\tprep_next_send(ring, c, ocd, other_dir_fd(c, cqe_to_fd(cqe)));\n\t\tclose_cd(c, cd);\n\t\treturn 0;\n\t}\n\n\tif (cqe->res && cqe->res < buf_size)\n\t\tcd->rcv_shrt++;\n\n\tbid = cqe->flags >> IORING_CQE_BUFFER_SHIFT;\n\n\t/*\n\t * BIDI will use the same buffer pool and do receive on both CDs,\n\t * so can't reliably check. TODO.\n\t */\n\tif (!bidi && cd->rcv_next_bid != -1 && bid != cd->rcv_next_bid) {\n\t\tfprintf(stderr, \"recv bid %d, wanted %d\\n\", bid, cd->rcv_next_bid);\n\t\tgoto start_close;\n\t}\n\n\tvlog(\"%d: recv: bid=%d, res=%d, cflags=%x\\n\", c->tid, bid, cqe->res, cqe->flags);\n\t/*\n\t * If we're a sink, we're done here. Just replenish the buffer back\n\t * to the pool. For proxy mode, we will send the data to the other\n\t * end and the buffer will be replenished once the send is done with\n\t * it.\n\t */\n\tif (is_sink)\n\t\tnr_packets = replenish_buffers(c, &bid, cqe->res);\n\telse if (rcv_msg && recv_mshot)\n\t\tnr_packets = recv_mshot_msg(c, ocd, &bid, cqe->res);\n\telse\n\t\tnr_packets = recv_bids(c, ocd, &bid, cqe->res);\n\n\tif (cd->rcv_bucket)\n\t\tcd->rcv_bucket[nr_packets]++;\n\n\tif (!is_sink) {\n\t\tocd->out_buffers += nr_packets;\n\t\tassert(ocd->out_buffers <= nr_bufs);\n\t}\n\n\tcd->rcv++;\n\tcd->rcv_next_bid = bid;\n\n\t/*\n\t * If IORING_CQE_F_MORE isn't set, then this is either a normal recv\n\t * that needs rearming, or it's a multishot that won't post any further\n\t * completions. Setup a new one for these cases.\n\t */\n\tif (!(cqe->flags & IORING_CQE_F_MORE)) {\n\t\tcd->pending_recv = 0;\n\t\tif (recv_done_res(cqe->res))\n\t\t\tgoto start_close;\n\t\tif (is_sink)\n\t\t\t__submit_receive(ring, c, &c->cd[0], c->in_fd);\n\t}\n\n\t/*\n\t * Submit a send if we won't get anymore notifications from this\n\t * recv, or if we have nr_bufs / 2 queued up. If BIDI mode, send\n\t * every buffer. We assume this is interactive mode, and hence don't\n\t * delay anything.\n\t */\n\tif (((!ocd->pending_send && (bidi || (ocd->out_buffers >= nr_bufs / 2))) ||\n\t    !(cqe->flags & IORING_CQE_F_MORE)) && !is_sink)\n\t\tprep_next_send(ring, c, ocd, other_dir_fd(c, cqe_to_fd(cqe)));\n\n\tif (!recv_done_res(cqe->res))\n\t\tcd->in_bytes += cqe->res;\n\treturn 0;\n}\n\nstatic int handle_recv(struct io_uring *ring, struct io_uring_cqe *cqe)\n{\n\tstruct conn *c = cqe_to_conn(cqe);\n\tstruct conn_dir *cd = cqe_to_conn_dir(c, cqe);\n\n\treturn __handle_recv(ring, c, cd, cqe);\n}\n\nstatic int recv_error(struct error_handler *err, struct io_uring *ring,\n\t\t      struct io_uring_cqe *cqe)\n{\n\tstruct conn *c = cqe_to_conn(cqe);\n\tstruct conn_dir *cd = cqe_to_conn_dir(c, cqe);\n\n\tcd->pending_recv = 0;\n\n\tif (cqe->res != -ENOBUFS)\n\t\treturn default_error(err, ring, cqe);\n\n\trecv_enobufs(ring, c, cd, other_dir_fd(c, cqe_to_fd(cqe)));\n\treturn 0;\n}\n\nstatic void submit_send(struct io_uring *ring, struct conn *c,\n\t\t\tstruct conn_dir *cd, int fd, void *data, int len,\n\t\t\tint bid, int flags)\n{\n\tstruct io_uring_sqe *sqe;\n\tint bgid = c->out_br.bgid;\n\n\tif (cd->pending_send)\n\t\treturn;\n\tcd->pending_send = 1;\n\n\tflags |= MSG_WAITALL | MSG_NOSIGNAL;\n\n\tsqe = get_sqe(ring);\n\tif (snd_msg) {\n\t\tstruct io_msg *imsg = &cd->io_snd_msg;\n\n\t\tif (snd_zc) {\n\t\t\tio_uring_prep_sendmsg_zc(sqe, fd, &imsg->msg, flags);\n\t\t\tcd->snd_notif++;\n\t\t} else {\n\t\t\tio_uring_prep_sendmsg(sqe, fd, &imsg->msg, flags);\n\t\t}\n\t} else if (send_ring) {\n\t\tio_uring_prep_send(sqe, fd, NULL, 0, flags);\n\t} else if (!snd_zc) {\n\t\tio_uring_prep_send(sqe, fd, data, len, flags);\n\t} else {\n\t\tio_uring_prep_send_zc(sqe, fd, data, len, flags, 0);\n\t\tsqe->ioprio |= IORING_RECVSEND_FIXED_BUF;\n\t\tsqe->buf_index = bid;\n\t\tcd->snd_notif++;\n\t}\n\tencode_userdata(sqe, c, __SEND, bid, fd);\n\tif (fixed_files)\n\t\tsqe->flags |= IOSQE_FIXED_FILE;\n\tif (send_ring) {\n\t\tsqe->flags |= IOSQE_BUFFER_SELECT;\n\t\tsqe->buf_group = bgid;\n\t}\n\tif (snd_bundle) {\n\t\tsqe->ioprio |= IORING_RECVSEND_BUNDLE;\n\t\tcd->snd_mshot++;\n\t} else if (send_ring)\n\t\tcd->snd_mshot++;\n}\n\n/*\n * Prepare the next send request, if we need to. If one is already pending,\n * or if we're a sink and we don't need to do sends, then there's nothing\n * to do.\n *\n * Return 1 if another send completion is expected, 0 if not.\n */\nstatic int prep_next_send(struct io_uring *ring, struct conn *c,\n\t\t\t   struct conn_dir *cd, int fd)\n{\n\tint bid;\n\n\tif (cd->pending_send || is_sink)\n\t\treturn 0;\n\tif (!cd->out_buffers)\n\t\treturn 0;\n\n\tbid = cd->snd_next_bid;\n\tif (bid == -1)\n\t\tbid = 0;\n\n\tif (send_ring) {\n\t\t/*\n\t\t * send_ring mode is easy, there's nothing to do but submit\n\t\t * our next send request. That will empty the entire outgoing\n\t\t * queue.\n\t\t */\n\t\tsubmit_send(ring, c, cd, fd, NULL, 0, bid, 0);\n\t\treturn 1;\n\t} else if (snd_msg) {\n\t\t/*\n\t\t * For sendmsg mode, submit our currently prepared iovec, if\n\t\t * we have one, and swap our iovecs so that any further\n\t\t * receives will start preparing that one.\n\t\t */\n\t\tstruct io_msg *imsg = &cd->io_snd_msg;\n\n\t\tif (!msg_vec(imsg)->iov_len)\n\t\t\treturn 0;\n\t\timsg->msg.msg_iov = msg_vec(imsg)->iov;\n\t\timsg->msg.msg_iovlen = msg_vec(imsg)->iov_len;\n\t\tmsg_vec(imsg)->iov_len = 0;\n\t\timsg->vec_index = !imsg->vec_index;\n\t\tsubmit_send(ring, c, cd, fd, NULL, 0, bid, 0);\n\t\treturn 1;\n\t} else {\n\t\t/*\n\t\t * send without send_ring - submit the next available vec,\n\t\t * if any. If this vec is the last one in the current series,\n\t\t * then swap to the next vec. We flag each send with MSG_MORE,\n\t\t * unless this is the last part of the current vec.\n\t\t */\n\t\tstruct io_msg *imsg = &cd->io_snd_msg;\n\t\tstruct msg_vec *mvec = msg_vec(imsg);\n\t\tint flags = !snd_zc ? MSG_MORE : 0;\n\t\tstruct iovec *iov;\n\n\t\tif (mvec->iov_len == mvec->cur_iov)\n\t\t\treturn 0;\n\t\timsg->msg.msg_iov = msg_vec(imsg)->iov;\n\t\tiov = &mvec->iov[mvec->cur_iov];\n\t\tmvec->cur_iov++;\n\t\tif (mvec->cur_iov == mvec->iov_len) {\n\t\t\tmvec->iov_len = 0;\n\t\t\tmvec->cur_iov = 0;\n\t\t\timsg->vec_index = !imsg->vec_index;\n\t\t\tflags = 0;\n\t\t}\n\t\tsubmit_send(ring, c, cd, fd, iov->iov_base, iov->iov_len, bid, flags);\n\t\treturn 1;\n\t}\n}\n\n/*\n * Handling a send with an outgoing send ring. Get the buffers from the\n * receive side, and add them to the ingoing buffer ring again.\n */\nstatic int handle_send_ring(struct conn *c, struct conn_dir *cd,\n\t\t\t    int bid, int bytes)\n{\n\tstruct conn_buf_ring *in_cbr = &c->in_br;\n\tstruct conn_buf_ring *out_cbr = &c->out_br;\n\tint i = 0;\n\n\twhile (bytes) {\n\t\tstruct io_uring_buf *buf = &out_cbr->br->bufs[bid];\n\t\tint this_bytes;\n\t\tvoid *this_buf;\n\n\t\tthis_bytes = buf->len;\n\t\tif (this_bytes > bytes)\n\t\t\tthis_bytes = bytes;\n\n\t\tcd->out_bytes += this_bytes;\n\n\t\tvlog(\"%d: send: bid=%d, len=%d\\n\", c->tid, bid, this_bytes);\n\n\t\tthis_buf = in_cbr->buf + bid * buf_size;\n\t\tio_uring_buf_ring_add(in_cbr->br, this_buf, buf_size, bid, br_mask, i);\n\t\t/*\n\t\t * Find the provided buffer that the receive consumed, and\n\t\t * which we then used for the send, and add it back to the\n\t\t * pool so it can get picked by another receive. Once the send\n\t\t * is done, we're done with it.\n\t\t */\n\t\tbid = (bid + 1) & (nr_bufs - 1);\n\t\tbytes -= this_bytes;\n\t\ti++;\n\t}\n\tcd->snd_next_bid = bid;\n\tio_uring_buf_ring_advance(in_cbr->br, i);\n\n\tif (pending_shutdown(c))\n\t\tclose_cd(c, cd);\n\n\treturn i;\n}\n\n/*\n * sendmsg, or send without a ring. Just add buffers back to the ingoing\n * ring for receives.\n */\nstatic int handle_send_buf(struct conn *c, struct conn_dir *cd, int bid,\n\t\t\t   int bytes)\n{\n\tstruct conn_buf_ring *in_cbr = &c->in_br;\n\tint i = 0;\n\n\twhile (bytes) {\n\t\tstruct io_uring_buf *buf = &in_cbr->br->bufs[bid];\n\t\tint this_bytes;\n\n\t\tthis_bytes = bytes;\n\t\tif (this_bytes > buf->len)\n\t\t\tthis_bytes = buf->len;\n\n\t\tvlog(\"%d: send: bid=%d, len=%d\\n\", c->tid, bid, this_bytes);\n\n\t\tcd->out_bytes += this_bytes;\n\t\t/* each recvmsg mshot package has this overhead */\n\t\tif (rcv_msg && recv_mshot)\n\t\t\tcd->out_bytes += sizeof(struct io_uring_recvmsg_out);\n\t\treplenish_buffer(in_cbr, bid, i);\n\t\tbid = (bid + 1) & (nr_bufs - 1);\n\t\tbytes -= this_bytes;\n\t\ti++;\n\t}\n\tio_uring_buf_ring_advance(in_cbr->br, i);\n\tcd->snd_next_bid = bid;\n\treturn i;\n}\n\nstatic int __handle_send(struct io_uring *ring, struct conn *c,\n\t\t\t struct conn_dir *cd, struct io_uring_cqe *cqe)\n{\n\tstruct conn_dir *ocd;\n\tint bid, nr_packets;\n\n\tif (send_ring) {\n\t\tif (!(cqe->flags & IORING_CQE_F_BUFFER)) {\n\t\t\tfprintf(stderr, \"no buffer in send?! %d\\n\", cqe->res);\n\t\t\treturn 1;\n\t\t}\n\t\tbid = cqe->flags >> IORING_CQE_BUFFER_SHIFT;\n\t} else {\n\t\tbid = cqe_to_bid(cqe);\n\t}\n\n\t/*\n\t * CQE notifications only happen with send/sendmsg zerocopy. They\n\t * tell us that the data has been acked, and that hence the buffer\n\t * is now free to reuse. Waiting on an ACK for each packet will slow\n\t * us down tremendously, so do all of our sends and then wait for\n\t * the ACKs to come in. They tend to come in bundles anyway. Once\n\t * all acks are done (cd->snd_notif == 0), then fire off the next\n\t * receive.\n\t */\n\tif (cqe->flags & IORING_CQE_F_NOTIF) {\n\t\tcd->snd_notif--;\n\t} else {\n\t\tif (cqe->res && cqe->res < buf_size)\n\t\t\tcd->snd_shrt++;\n\n\t\t/*\n\t\t * BIDI will use the same buffer pool and do sends on both CDs,\n\t\t * so can't reliably check. TODO.\n\t\t */\n\t\tif (!bidi && send_ring && cd->snd_next_bid != -1 &&\n\t\t    bid != cd->snd_next_bid) {\n\t\t\tfprintf(stderr, \"send bid %d, wanted %d at %lu\\n\", bid,\n\t\t\t\t\tcd->snd_next_bid, cd->out_bytes);\n\t\t\tgoto out_close;\n\t\t}\n\n\t\tassert(bid <= nr_bufs);\n\n\t\tvlog(\"send: got %d, %lu\\n\", cqe->res, cd->out_bytes);\n\n\t\tif (send_ring)\n\t\t\tnr_packets = handle_send_ring(c, cd, bid, cqe->res);\n\t\telse\n\t\t\tnr_packets = handle_send_buf(c, cd, bid, cqe->res);\n\n\t\tif (cd->snd_bucket)\n\t\t\tcd->snd_bucket[nr_packets]++;\n\n\t\tcd->out_buffers -= nr_packets;\n\t\tassert(cd->out_buffers >= 0);\n\n\t\tcd->snd++;\n\t}\n\n\tif (!(cqe->flags & IORING_CQE_F_MORE)) {\n\t\tint do_recv_arm;\n\n\t\tcd->pending_send = 0;\n\n\t\t/*\n\t\t * send done - see if the current vec has data to submit, and\n\t\t * do so if it does. if it doesn't have data yet, nothing to\n\t\t * do.\n\t\t */\n\t\tdo_recv_arm = !prep_next_send(ring, c, cd, cqe_to_fd(cqe));\n\n\t\tocd = &c->cd[!cd->index];\n\t\tif (!cd->snd_notif && do_recv_arm && !ocd->pending_recv) {\n\t\t\tint fd = other_dir_fd(c, cqe_to_fd(cqe));\n\n\t\t\t__submit_receive(ring, c, ocd, fd);\n\t\t}\nout_close:\n\t\tif (pending_shutdown(c))\n\t\t\tclose_cd(c, cd);\n\t}\n\n\tvlog(\"%d: pending sends %d\\n\", c->tid, cd->pending_send);\n\treturn 0;\n}\n\nstatic int handle_send(struct io_uring *ring, struct io_uring_cqe *cqe)\n{\n\tstruct conn *c = cqe_to_conn(cqe);\n\tstruct conn_dir *cd = cqe_to_conn_dir(c, cqe);\n\n\treturn __handle_send(ring, c, cd, cqe);\n}\n\nstatic int send_error(struct error_handler *err, struct io_uring *ring,\n\t\t      struct io_uring_cqe *cqe)\n{\n\tstruct conn *c = cqe_to_conn(cqe);\n\tstruct conn_dir *cd = cqe_to_conn_dir(c, cqe);\n\n\tcd->pending_send = 0;\n\n\t/* res can have high bit set */\n\tif (cqe->flags & IORING_CQE_F_NOTIF)\n\t\treturn handle_send(ring, cqe);\n\tif (cqe->res != -ENOBUFS)\n\t\treturn default_error(err, ring, cqe);\n\n\tcd->snd_enobufs++;\n\treturn 0;\n}\n\n/*\n * We don't expect to get here, as we marked it with skipping posting a\n * CQE if it was successful. If it does trigger, than means it fails and\n * that our close has not been done. Log the shutdown error and issue a new\n * separate close.\n */\nstatic int handle_shutdown(struct io_uring *ring, struct io_uring_cqe *cqe)\n{\n\tstruct conn *c = cqe_to_conn(cqe);\n\tstruct io_uring_sqe *sqe;\n\tint fd = cqe_to_fd(cqe);\n\n\tfprintf(stderr, \"Got shutdown notication on fd %d\\n\", fd);\n\n\tif (!cqe->res)\n\t\tfprintf(stderr, \"Unexpected success shutdown CQE\\n\");\n\telse if (cqe->res < 0)\n\t\tfprintf(stderr, \"Shutdown got %s\\n\", strerror(-cqe->res));\n\n\tsqe = get_sqe(ring);\n\tif (fixed_files)\n\t\tio_uring_prep_close_direct(sqe, fd);\n\telse\n\t\tio_uring_prep_close(sqe, fd);\n\tencode_userdata(sqe, c, __CLOSE, 0, fd);\n\treturn 0;\n}\n\n/*\n * Final stage of a connection, the shutdown and close has finished. Mark\n * it as disconnected and let the main loop reap it.\n */\nstatic int handle_close(struct io_uring *ring, struct io_uring_cqe *cqe)\n{\n\tstruct conn *c = cqe_to_conn(cqe);\n\tint fd = cqe_to_fd(cqe);\n\n\tprintf(\"Closed client: id=%d, in_fd=%d, out_fd=%d\\n\", c->tid, c->in_fd, c->out_fd);\n\tif (fd == c->in_fd)\n\t\tc->in_fd = -1;\n\telse if (fd == c->out_fd)\n\t\tc->out_fd = -1;\n\n\tif (c->in_fd == -1 && c->out_fd == -1) {\n\t\tc->flags |= CONN_F_DISCONNECTED;\n\n\t\tpthread_mutex_lock(&thread_lock);\n\t\t__show_stats(c);\n\t\topen_conns--;\n\t\tpthread_mutex_unlock(&thread_lock);\n\t\tfree_buffer_rings(ring, c);\n\t\tfree_msgs(&c->cd[0]);\n\t\tfree_msgs(&c->cd[1]);\n\t\tfree(c->cd[0].rcv_bucket);\n\t\tfree(c->cd[0].snd_bucket);\n\t}\n\n\treturn 0;\n}\n\nstatic int handle_cancel(struct io_uring *ring, struct io_uring_cqe *cqe)\n{\n\tstruct conn *c = cqe_to_conn(cqe);\n\tint fd = cqe_to_fd(cqe);\n\n\tc->pending_cancels--;\n\n\tvlog(\"%d: got cancel fd %d, refs %d\\n\", c->tid, fd, c->pending_cancels);\n\n\tif (!c->pending_cancels) {\n\t\tqueue_shutdown_close(ring, c, c->in_fd);\n\t\tif (c->out_fd != -1)\n\t\t\tqueue_shutdown_close(ring, c, c->out_fd);\n\t\tio_uring_submit(ring);\n\t}\n\n\treturn 0;\n}\n\nstatic void open_socket(struct conn *c)\n{\n\tif (is_sink) {\n\t\tpthread_mutex_lock(&thread_lock);\n\t\topen_conns++;\n\t\tpthread_mutex_unlock(&thread_lock);\n\n\t\tsubmit_receive(&c->ring, c);\n\t} else {\n\t\tstruct io_uring_sqe *sqe;\n\t\tint domain;\n\n\t\tif (ipv6)\n\t\t\tdomain = AF_INET6;\n\t\telse\n\t\t\tdomain = AF_INET;\n\n\t\t/*\n\t\t * If fixed_files is set, proxy will use fixed files for any new\n\t\t * file descriptors it instantiates. Fixd files, or fixed\n\t\t * descriptors, are io_uring private file descriptors. They\n\t\t * cannot be accessed outside of io_uring. io_uring holds a\n\t\t * fixed reference to them, which means that we do not need to\n\t\t * grab per-request references to them. Particularly for\n\t\t * threaded applications, grabbing and dropping file references\n\t\t * for each operation can be costly as the file table is shared.\n\t\t * This generally shows up as fget/fput related overhead in any\n\t\t * workload profiles.\n\t\t *\n\t\t * Fixed descriptors are passed in via the 'fd' field just like\n\t\t * regular descriptors, and then marked as such by setting the\n\t\t * IOSQE_FIXED_FILE flag in the sqe->flags field. Some helpers\n\t\t * do that automatically, like the below, others will need it\n\t\t * set manually if they don't have a *direct*() helper.\n\t\t *\n\t\t * For operations that instantiate them, like the opening of a\n\t\t * direct socket, the application may either ask the kernel to\n\t\t * find a free one (as is done below), or the application may\n\t\t * manage the space itself and pass in an index for a currently\n\t\t * free slot in the table. If the kernel is asked to allocate a\n\t\t * free direct descriptor, note that io_uring does not abide by\n\t\t * the POSIX mandated \"lowest free must be returned\". It may\n\t\t * return any free descriptor of its choosing.\n\t\t */\n\t\tsqe = get_sqe(&c->ring);\n\t\tif (fixed_files)\n\t\t\tio_uring_prep_socket_direct_alloc(sqe, domain, SOCK_STREAM, 0, 0);\n\t\telse\n\t\t\tio_uring_prep_socket(sqe, domain, SOCK_STREAM, 0, 0);\n\t\tencode_userdata(sqe, c, __SOCK, 0, 0);\n\t}\n}\n\n/*\n * Start of connection, we got our in descriptor.\n */\nstatic int handle_fd_pass(struct io_uring_cqe *cqe)\n{\n\tstruct conn *c = cqe_to_conn(cqe);\n\tint fd = cqe_to_fd(cqe);\n\n\tvlog(\"%d: got fd pass %d\\n\", c->tid, fd);\n\tc->in_fd = fd;\n\topen_socket(c);\n\treturn 0;\n}\n\nstatic int handle_stop(struct io_uring_cqe *cqe)\n{\n\tstruct conn *c = cqe_to_conn(cqe);\n\n\tprintf(\"Client %d: queueing shutdown\\n\", c->tid);\n\tqueue_cancel(&c->ring, c);\n\treturn 0;\n}\n\n/*\n * Called for each CQE that we receive. Decode the request type that it\n * came from, and call the appropriate handler.\n */\nstatic int handle_cqe(struct io_uring *ring, struct io_uring_cqe *cqe)\n{\n\tint ret;\n\n\t/*\n\t * Unlikely, but there's an error in this CQE. If an error handler\n\t * is defined, call it, and that will deal with it. If no error\n\t * handler is defined, the opcode handler either doesn't care or will\n\t * handle it on its own.\n\t */\n\tif (cqe->res < 0) {\n\t\tstruct error_handler *err = &error_handlers[cqe_to_op(cqe)];\n\n\t\tif (err->error_fn)\n\t\t\treturn err->error_fn(err, ring, cqe);\n\t}\n\n\tswitch (cqe_to_op(cqe)) {\n\tcase __ACCEPT:\n\t\tret = handle_accept(ring, cqe);\n\t\tbreak;\n\tcase __SOCK:\n\t\tret = handle_sock(ring, cqe);\n\t\tbreak;\n\tcase __CONNECT:\n\t\tret = handle_connect(ring, cqe);\n\t\tbreak;\n\tcase __RECV:\n\tcase __RECVMSG:\n\t\tret = handle_recv(ring, cqe);\n\t\tbreak;\n\tcase __SEND:\n\tcase __SENDMSG:\n\t\tret = handle_send(ring, cqe);\n\t\tbreak;\n\tcase __CANCEL:\n\t\tret = handle_cancel(ring, cqe);\n\t\tbreak;\n\tcase __SHUTDOWN:\n\t\tret = handle_shutdown(ring, cqe);\n\t\tbreak;\n\tcase __CLOSE:\n\t\tret = handle_close(ring, cqe);\n\t\tbreak;\n\tcase __FD_PASS:\n\t\tret = handle_fd_pass(cqe);\n\t\tbreak;\n\tcase __STOP:\n\t\tret = handle_stop(cqe);\n\t\tbreak;\n\tcase __NOP:\n\t\tret = 0;\n\t\tbreak;\n\tdefault:\n\t\tfprintf(stderr, \"bad user data %lx\\n\", (long) cqe->user_data);\n\t\treturn 1;\n\t}\n\n\treturn ret;\n}\n\nstatic void house_keeping(struct io_uring *ring)\n{\n\tstatic unsigned long last_bytes;\n\tunsigned long bytes, elapsed;\n\tstruct conn *c;\n\tint i, j;\n\n\tvlog(\"House keeping entered\\n\");\n\n\tbytes = 0;\n\tfor (i = 0; i < nr_conns; i++) {\n\t\tc = &conns[i];\n\n\t\tfor (j = 0; j < 2; j++) {\n\t\t\tstruct conn_dir *cd = &c->cd[j];\n\n\t\t\tbytes += cd->in_bytes + cd->out_bytes;\n\t\t}\n\t\tif (c->flags & CONN_F_DISCONNECTED) {\n\t\t\tvlog(\"%d: disconnected\\n\", i);\n\n\t\t\tif (!(c->flags & CONN_F_REAPED)) {\n\t\t\t\tvoid *ret;\n\n\t\t\t\tpthread_join(c->thread, &ret);\n\t\t\t\tc->flags |= CONN_F_REAPED;\n\t\t\t}\n\t\t\tcontinue;\n\t\t}\n\t\tif (c->flags & CONN_F_DISCONNECTING)\n\t\t\tcontinue;\n\n\t\tif (should_shutdown(c)) {\n\t\t\t__close_conn(ring, c);\n\t\t\tc->flags |= CONN_F_DISCONNECTING;\n\t\t}\n\t}\n\n\telapsed = mtime_since_now(&last_housekeeping);\n\tif (bytes && elapsed >= 900) {\n\t\tunsigned long bw;\n\n\t\tbw = (8 * (bytes - last_bytes) / 1000UL) / elapsed;\n\t\tif (bw) {\n\t\t\tif (open_conns)\n\t\t\t\tprintf(\"Bandwidth (threads=%d): %'luMbit\\n\", open_conns, bw);\n\t\t\tgettimeofday(&last_housekeeping, NULL);\n\t\t\tlast_bytes = bytes;\n\t\t}\n\t}\n}\n\n/*\n * Event loop shared between the parent, and the connections. Could be\n * split in two, as they don't handle the same types of events. For the per\n * connection loop, 'c' is valid. For the main loop, it's NULL.\n */\nstatic int __event_loop(struct io_uring *ring, struct conn *c)\n{\n\tstruct __kernel_timespec active_ts, idle_ts;\n\tint flags;\n\n\tidle_ts.tv_sec = 0;\n\tidle_ts.tv_nsec = 100000000LL;\n\tactive_ts = idle_ts;\n\tif (wait_usec > 1000000) {\n\t\tactive_ts.tv_sec = wait_usec / 1000000;\n\t\twait_usec -= active_ts.tv_sec * 1000000;\n\t}\n\tactive_ts.tv_nsec = wait_usec * 1000;\n\n\tgettimeofday(&last_housekeeping, NULL);\n\n\tflags = 0;\n\twhile (1) {\n\t\tstruct __kernel_timespec *ts = &idle_ts;\n\t\tstruct io_uring_cqe *cqe;\n\t\tunsigned int head;\n\t\tint ret, i, to_wait;\n\n\t\t/*\n\t\t * If wait_batch is set higher than 1, then we'll wait on\n\t\t * that amount of CQEs to be posted each loop. If used with\n\t\t * DEFER_TASKRUN, this can provide a substantial reduction\n\t\t * in context switch rate as the task isn't woken until the\n\t\t * requested number of events can be returned.\n\t\t *\n\t\t * Can be used with -t to set a wait_usec timeout as well.\n\t\t * For example, if an application can deal with 250 usec\n\t\t * of wait latencies, it can set -w8 -t250 which will cause\n\t\t * io_uring to return when either 8 events have been received,\n\t\t * or if 250 usec of waiting has passed.\n\t\t *\n\t\t * If we don't have any open connections, wait on just 1\n\t\t * always.\n\t\t */\n\t\tto_wait = 1;\n\t\tif (open_conns && !flags) {\n\t\t\tts = &active_ts;\n\t\t\tto_wait = wait_batch;\n\t\t}\n\n\t\tvlog(\"Submit and wait for %d\\n\", to_wait);\n\t\tret = io_uring_submit_and_wait_timeout(ring, &cqe, to_wait, ts, NULL);\n\n\t\tif (*ring->cq.koverflow)\n\t\t\tprintf(\"overflow %u\\n\", *ring->cq.koverflow);\n\t\tif (*ring->sq.kflags &  IORING_SQ_CQ_OVERFLOW)\n\t\t\tprintf(\"saw overflow\\n\");\n\n\t\tvlog(\"Submit and wait: %d\\n\", ret);\n\n\t\ti = flags = 0;\n\t\tio_uring_for_each_cqe(ring, head, cqe) {\n\t\t\tif (handle_cqe(ring, cqe))\n\t\t\t\treturn 1;\n\t\t\tflags |= cqe_to_conn(cqe)->flags;\n\t\t\t++i;\n\t\t}\n\n\t\tvlog(\"Handled %d events\\n\", i);\n\n\t\t/*\n\t\t * Advance the CQ ring for seen events when we've processed\n\t\t * all of them in this loop. This can also be done with\n\t\t * io_uring_cqe_seen() in each handler above, which just marks\n\t\t * that single CQE as seen. However, it's more efficient to\n\t\t * mark a batch as seen when we're done with that batch.\n\t\t */\n\t\tif (i) {\n\t\t\tio_uring_cq_advance(ring, i);\n\t\t\tevents += i;\n\t\t}\n\n\t\tevent_loops++;\n\t\tif (c) {\n\t\t\tif (c->flags & CONN_F_DISCONNECTED)\n\t\t\t\tbreak;\n\t\t} else {\n\t\t\thouse_keeping(ring);\n\t\t}\n\t}\n\n\treturn 0;\n}\n\n/*\n * Main event loop, Submit our multishot accept request, and then just loop\n * around handling incoming connections.\n */\nstatic int parent_loop(struct io_uring *ring, int fd)\n{\n\tstruct io_uring_sqe *sqe;\n\n\t/*\n\t * proxy provides a way to use either multishot receive or not, but\n\t * for accept, we always use multishot. A multishot accept request\n\t * needs only be armed once, and then it'll trigger a completion and\n\t * post a CQE whenever a new connection is accepted. No need to do\n\t * anything else, unless the multishot accept terminates. This happens\n\t * if it encounters an error. Applications should check for\n\t * IORING_CQE_F_MORE in cqe->flags - this tells you if more completions\n\t * are expected from this request or not. Non-multishot never have\n\t * this set, where multishot will always have this set unless an error\n\t * occurs.\n\t */\n\tsqe = get_sqe(ring);\n\tif (fixed_files)\n\t\tio_uring_prep_multishot_accept_direct(sqe, fd, NULL, NULL, 0);\n\telse\n\t\tio_uring_prep_multishot_accept(sqe, fd, NULL, NULL, 0);\n\t__encode_userdata(sqe, 0, __ACCEPT, 0, fd);\n\n\treturn __event_loop(ring, NULL);\n}\n\nstatic int init_ring(struct io_uring *ring, int nr_files)\n{\n\tstruct io_uring_params params;\n\tint ret;\n\n\t/*\n\t * By default, set us up with a big CQ ring. Not strictly needed\n\t * here, but it's very important to never overflow the CQ ring.\n\t * Events will not be dropped if this happens, but it does slow\n\t * the application down in dealing with overflown events.\n\t *\n\t * Set SINGLE_ISSUER, which tells the kernel that only one thread\n\t * is doing IO submissions. This enables certain optimizations in\n\t * the kernel.\n\t */\n\tmemset(&params, 0, sizeof(params));\n\tparams.flags |= IORING_SETUP_SINGLE_ISSUER | IORING_SETUP_CLAMP;\n\tparams.flags |= IORING_SETUP_CQSIZE;\n\tparams.cq_entries = 1024;\n\n\t/*\n\t * If use_huge is set, setup the ring with IORING_SETUP_NO_MMAP. This\n\t * means that the application allocates the memory for the ring, and\n\t * the kernel maps it. The alternative is having the kernel allocate\n\t * the memory, and then liburing will mmap it. But we can't really\n\t * support huge pages that way. If this fails, then ensure that the\n\t * system has huge pages set aside upfront.\n\t */\n\tif (use_huge)\n\t\tparams.flags |= IORING_SETUP_NO_MMAP;\n\n\t/*\n\t * DEFER_TASKRUN decouples async event reaping and retrying from\n\t * regular system calls. If this isn't set, then io_uring uses\n\t * normal task_work for this. task_work is always being run on any\n\t * exit to userspace. Real applications do more than just call IO\n\t * related system calls, and hence we can be running this work way\n\t * too often. Using DEFER_TASKRUN defers any task_work running to\n\t * when the application enters the kernel anyway to wait on new\n\t * events. It's generally the preferred and recommended way to setup\n\t * a ring.\n\t */\n\tif (defer_tw) {\n\t\tparams.flags |= IORING_SETUP_DEFER_TASKRUN;\n\t\tsqpoll = 0;\n\t}\n\n\t/*\n\t * SQPOLL offloads any request submission and retry operations to a\n\t * dedicated thread. This enables an application to do IO without\n\t * ever having to enter the kernel itself. The SQPOLL thread will\n\t * stay busy as long as there's work to do, and go to sleep if\n\t * sq_thread_idle msecs have passed. If it's running, submitting new\n\t * IO just needs to make them visible to the SQPOLL thread, it needs\n\t * not enter the kernel. For submission, the application will only\n\t * enter the kernel if the SQPOLL has been idle long enough that it\n\t * has gone to sleep.\n\t *\n\t * Waiting on events still need to enter the kernel, if none are\n\t * available. The application may also use io_uring_peek_cqe() to\n\t * check for new events without entering the kernel, as completions\n\t * will be continually produced to the CQ ring by the SQPOLL thread\n\t * as they occur.\n\t */\n\tif (sqpoll) {\n\t\tparams.flags |= IORING_SETUP_SQPOLL;\n\t\tparams.sq_thread_idle = 1000;\n\t\tdefer_tw = 0;\n\t}\n\n\t/*\n\t * If neither DEFER_TASKRUN or SQPOLL is used, set COOP_TASKRUN. This\n\t * avoids heavy signal based notifications, which can force an\n\t * application to enter the kernel and process it as soon as they\n\t * occur.\n\t */\n\tif (!sqpoll && !defer_tw)\n\t\tparams.flags |= IORING_SETUP_COOP_TASKRUN;\n\n\t/*\n\t * The SQ ring size need not be larger than any batch of requests\n\t * that need to be prepared before submit. Normally in a loop we'd\n\t * only need a few, if any, particularly if multishot is used.\n\t */\n\tret = io_uring_queue_init_params(ring_size, ring, &params);\n\tif (ret) {\n\t\tfprintf(stderr, \"%s\\n\", strerror(-ret));\n\t\treturn 1;\n\t}\n\n\t/*\n\t * If send serialization is available and no option was given to use\n\t * it or not, default it to on. If it was turned on and the kernel\n\t * doesn't support it, turn it off.\n\t */\n\tif (params.features & IORING_FEAT_SEND_BUF_SELECT) {\n\t\tif (send_ring == -1)\n\t\t\tsend_ring = 1;\n\t} else {\n\t\tif (send_ring == 1) {\n\t\t\tfprintf(stderr, \"Kernel doesn't support ring provided \"\n\t\t\t\t\"buffers for sends, disabled\\n\");\n\t\t}\n\t\tsend_ring = 0;\n\t}\n\n\tif (!send_ring && snd_bundle) {\n\t\tfprintf(stderr, \"Can't use send bundle without send_ring\\n\");\n\t\tsnd_bundle = 0;\n\t}\n\n\tif (fixed_files) {\n\t\t/*\n\t\t * If fixed files are used, we need to allocate a fixed file\n\t\t * table upfront where new direct descriptors can be managed.\n\t\t */\n\t\tret = io_uring_register_files_sparse(ring, nr_files);\n\t\tif (ret) {\n\t\t\tfprintf(stderr, \"file register: %d\\n\", ret);\n\t\t\treturn 1;\n\t\t}\n\n\t\t/*\n\t\t * If fixed files are used, we also register the ring fd. See\n\t\t * comment near io_uring_prep_socket_direct_alloc() further\n\t\t * down. This avoids the fget/fput overhead associated with\n\t\t * the io_uring_enter(2) system call itself, which is used to\n\t\t * submit and wait on events.\n\t\t */\n\t\tret = io_uring_register_ring_fd(ring);\n\t\tif (ret != 1) {\n\t\t\tfprintf(stderr, \"ring register: %d\\n\", ret);\n\t\t\treturn 1;\n\t\t}\n\t}\n\n\tif (napi) {\n\t\tstruct io_uring_napi n = {\n\t\t\t.prefer_busy_poll = napi > 1 ? 1 : 0,\n\t\t\t.busy_poll_to = napi_timeout,\n\t\t};\n\n\t\tret = io_uring_register_napi(ring, &n);\n\t\tif (ret) {\n\t\t\tfprintf(stderr, \"io_uring_register_napi: %d\\n\", ret);\n\t\t\tif (ret != -EINVAL)\n\t\t\t\treturn 1;\n\t\t\tfprintf(stderr, \"NAPI not available, turned off\\n\");\n\t\t}\n\t}\n\n\treturn 0;\n}\n\nstatic void *thread_main(void *data)\n{\n\tstruct conn *c = data;\n\tint ret;\n\n\tc->flags |= CONN_F_STARTED;\n\n\t/* we need a max of 4 descriptors for each client */\n\tret = init_ring(&c->ring, 4);\n\tif (ret)\n\t\tgoto done;\n\n\tif (setup_buffer_rings(&c->ring, c))\n\t\tgoto done;\n\n\t/*\n\t * If we're using fixed files, then we need to wait for the parent\n\t * to install the c->in_fd into our direct descriptor table. When\n\t * that happens, we'll set things up. If we're not using fixed files,\n\t * we can set up the receive or connect now.\n\t */\n\tif (!fixed_files)\n\t\topen_socket(c);\n\n\t/* we're ready */\n\tpthread_barrier_wait(&c->startup_barrier);\n\n\t__event_loop(&c->ring, c);\ndone:\n\treturn NULL;\n}\n\nstatic void usage(const char *name)\n{\n\tprintf(\"%s:\\n\", name);\n\tprintf(\"\\t-m:\\t\\tUse multishot receive (%d)\\n\", recv_mshot);\n\tprintf(\"\\t-d:\\t\\tUse DEFER_TASKRUN (%d)\\n\", defer_tw);\n\tprintf(\"\\t-S:\\t\\tUse SQPOLL (%d)\\n\", sqpoll);\n\tprintf(\"\\t-f:\\t\\tUse only fixed files (%d)\\n\", fixed_files);\n\tprintf(\"\\t-a:\\t\\tUse huge pages for the ring (%d)\\n\", use_huge);\n\tprintf(\"\\t-t:\\t\\tTimeout for waiting on CQEs (usec) (%d)\\n\", wait_usec);\n\tprintf(\"\\t-w:\\t\\tNumber of CQEs to wait for each loop (%d)\\n\", wait_batch);\n\tprintf(\"\\t-B:\\t\\tUse bi-directional mode (%d)\\n\", bidi);\n\tprintf(\"\\t-s:\\t\\tAct only as a sink (%d)\\n\", is_sink);\n\tprintf(\"\\t-q:\\t\\tRing size to use (%d)\\n\", ring_size);\n\tprintf(\"\\t-H:\\t\\tHost to connect to (%s)\\n\", host);\n\tprintf(\"\\t-r:\\t\\tPort to receive on (%d)\\n\", receive_port);\n\tprintf(\"\\t-p:\\t\\tPort to connect to (%d)\\n\", send_port);\n\tprintf(\"\\t-6:\\t\\tUse IPv6 (%d)\\n\", ipv6);\n\tprintf(\"\\t-N:\\t\\tUse NAPI polling (%d)\\n\", napi);\n\tprintf(\"\\t-T:\\t\\tNAPI timeout (usec) (%d)\\n\", napi_timeout);\n\tprintf(\"\\t-b:\\t\\tSend/receive buf size (%d)\\n\", buf_size);\n\tprintf(\"\\t-n:\\t\\tNumber of provided buffers (pow2) (%d)\\n\", nr_bufs);\n\tprintf(\"\\t-u:\\t\\tUse provided buffers for send (%d)\\n\", send_ring);\n\tprintf(\"\\t-C:\\t\\tUse bundles for send (%d)\\n\", snd_bundle);\n\tprintf(\"\\t-z:\\t\\tUse zerocopy send (%d)\\n\", snd_zc);\n\tprintf(\"\\t-c:\\t\\tUse bundles for recv (%d)\\n\", snd_bundle);\n\tprintf(\"\\t-M:\\t\\tUse sendmsg (%d)\\n\", snd_msg);\n\tprintf(\"\\t-M:\\t\\tUse recvmsg (%d)\\n\", rcv_msg);\n\tprintf(\"\\t-x:\\t\\tShow extended stats (%d)\\n\", ext_stat);\n\tprintf(\"\\t-V:\\t\\tIncrease verbosity (%d)\\n\", verbose);\n}\n\n/*\n * Options parsing the ring / net setup\n */\nint main(int argc, char *argv[])\n{\n\tstruct io_uring ring;\n\tstruct sigaction sa = { };\n\tconst char *optstring;\n\tint opt, ret, fd;\n\n\tsetlocale(LC_NUMERIC, \"en_US\");\n\n\tpage_size = sysconf(_SC_PAGESIZE);\n\tif (page_size < 0) {\n\t\tperror(\"sysconf(_SC_PAGESIZE)\");\n\t\treturn 1;\n\t}\n\n\tpthread_mutex_init(&thread_lock, NULL);\n\n\toptstring = \"m:d:S:s:b:f:H:r:p:n:B:N:T:w:t:M:R:u:c:C:q:a:x:z:6Vh?\";\n\twhile ((opt = getopt(argc, argv, optstring)) != -1) {\n\t\tswitch (opt) {\n\t\tcase 'm':\n\t\t\trecv_mshot = !!atoi(optarg);\n\t\t\tbreak;\n\t\tcase 'S':\n\t\t\tsqpoll = !!atoi(optarg);\n\t\t\tbreak;\n\t\tcase 'd':\n\t\t\tdefer_tw = !!atoi(optarg);\n\t\t\tbreak;\n\t\tcase 'b':\n\t\t\tbuf_size = atoi(optarg);\n\t\t\tbreak;\n\t\tcase 'n':\n\t\t\tnr_bufs = atoi(optarg);\n\t\t\tbreak;\n\t\tcase 'u':\n\t\t\tsend_ring = !!atoi(optarg);\n\t\t\tbreak;\n\t\tcase 'c':\n\t\t\trcv_bundle = !!atoi(optarg);\n\t\t\tbreak;\n\t\tcase 'C':\n\t\t\tsnd_bundle = !!atoi(optarg);\n\t\t\tbreak;\n\t\tcase 'w':\n\t\t\twait_batch = atoi(optarg);\n\t\t\tbreak;\n\t\tcase 't':\n\t\t\twait_usec = atoi(optarg);\n\t\t\tbreak;\n\t\tcase 's':\n\t\t\tis_sink = !!atoi(optarg);\n\t\t\tbreak;\n\t\tcase 'f':\n\t\t\tfixed_files = !!atoi(optarg);\n\t\t\tbreak;\n\t\tcase 'H':\n\t\t\thost = strdup(optarg);\n\t\t\tbreak;\n\t\tcase 'r':\n\t\t\treceive_port = atoi(optarg);\n\t\t\tbreak;\n\t\tcase 'p':\n\t\t\tsend_port = atoi(optarg);\n\t\t\tbreak;\n\t\tcase 'B':\n\t\t\tbidi = !!atoi(optarg);\n\t\t\tbreak;\n\t\tcase 'N':\n\t\t\tnapi = !!atoi(optarg);\n\t\t\tbreak;\n\t\tcase 'T':\n\t\t\tnapi_timeout = atoi(optarg);\n\t\t\tbreak;\n\t\tcase '6':\n\t\t\tipv6 = true;\n\t\t\tbreak;\n\t\tcase 'M':\n\t\t\tsnd_msg = !!atoi(optarg);\n\t\t\tbreak;\n\t\tcase 'z':\n\t\t\tsnd_zc = !!atoi(optarg);\n\t\t\tbreak;\n\t\tcase 'R':\n\t\t\trcv_msg = !!atoi(optarg);\n\t\t\tbreak;\n\t\tcase 'q':\n\t\t\tring_size = atoi(optarg);\n\t\t\tbreak;\n\t\tcase 'a':\n\t\t\tuse_huge = !!atoi(optarg);\n\t\t\tbreak;\n\t\tcase 'x':\n\t\t\text_stat = !!atoi(optarg);\n\t\t\tbreak;\n\t\tcase 'V':\n\t\t\tverbose++;\n\t\t\tbreak;\n\t\tcase 'h':\n\t\tdefault:\n\t\t\tusage(argv[0]);\n\t\t\treturn 1;\n\t\t}\n\t}\n\n\tif (bidi && is_sink) {\n\t\tfprintf(stderr, \"Can't be both bidi proxy and sink\\n\");\n\t\treturn 1;\n\t}\n\tif (snd_msg && sqpoll) {\n\t\tfprintf(stderr, \"SQPOLL with msg variants disabled\\n\");\n\t\tsnd_msg = 0;\n\t}\n\tif (rcv_msg && rcv_bundle) {\n\t\tfprintf(stderr, \"Can't use bundles with recvmsg\\n\");\n\t\trcv_msg = 0;\n\t}\n\tif (snd_msg && snd_bundle) {\n\t\tfprintf(stderr, \"Can't use bundles with sendmsg\\n\");\n\t\tsnd_msg = 0;\n\t}\n\tif (snd_msg && send_ring) {\n\t\tfprintf(stderr, \"Can't use send ring sendmsg\\n\");\n\t\tsnd_msg = 0;\n\t}\n\tif (snd_zc && (send_ring || snd_bundle)) {\n\t\tfprintf(stderr, \"Can't use send zc with bundles or ring\\n\");\n\t\tsend_ring = snd_bundle = 0;\n\t}\n\t/*\n\t * For recvmsg w/multishot, we waste some data at the head of the\n\t * packet every time. Adjust the buffer size to account for that,\n\t * so we're still handing 'buf_size' actual payload of data.\n\t */\n\tif (rcv_msg && recv_mshot) {\n\t\tfprintf(stderr, \"Adjusted buf size for recvmsg w/multishot\\n\");\n\t\tbuf_size += sizeof(struct io_uring_recvmsg_out);\n\t}\n\n\tbr_mask = nr_bufs - 1;\n\n\tfd = setup_listening_socket(receive_port, ipv6);\n\tif (is_sink)\n\t\tsend_port = -1;\n\n\tif (fd == -1)\n\t\treturn 1;\n\n\tatexit(show_stats);\n\tsa.sa_handler = sig_int;\n\tsa.sa_flags = SA_RESTART;\n\tsigaction(SIGINT, &sa, NULL);\n\n\tret = init_ring(&ring, MAX_CONNS * 3);\n\tif (ret)\n\t\treturn ret;\n\n\tprintf(\"Backend: sqpoll=%d, defer_tw=%d, fixed_files=%d, \"\n\t\t\"is_sink=%d, buf_size=%d, nr_bufs=%d, host=%s, send_port=%d, \"\n\t\t\"receive_port=%d, napi=%d, napi_timeout=%d, huge_page=%d\\n\",\n\t\t\tsqpoll, defer_tw, fixed_files, is_sink,\n\t\t\tbuf_size, nr_bufs, host, send_port, receive_port,\n\t\t\tnapi, napi_timeout, use_huge);\n\tprintf(\" recv options: recvmsg=%d, recv_mshot=%d, recv_bundle=%d\\n\",\n\t\t\trcv_msg, recv_mshot, rcv_bundle);\n\tprintf(\" send options: sendmsg=%d, send_ring=%d, send_bundle=%d, \"\n\t\t\"send_zerocopy=%d\\n\", snd_msg, send_ring, snd_bundle,\n\t\t\tsnd_zc);\n\n\treturn parent_loop(&ring, fd);\n} \n```"]