<!--yml
category: Êú™ÂàÜÁ±ª
date: 2024-05-27 14:43:03
-->

# How we got fine-tuning Mistral-7B to not suck: Helix Project Report, Feb 2024

> Êù•Ê∫êÔºö[https://helixml.substack.com/p/how-we-got-fine-tuning-mistral-7b](https://helixml.substack.com/p/how-we-got-fine-tuning-mistral-7b)

Hi folks,

It‚Äôs been just over a month since we [launched Helix v0.1](https://www.producthunt.com/products/helix-5), and today I‚Äôm happy to announce today the availability of [Helix v0.5](https://github.com/helixml/helix/releases). [Run it yourself on your own secure private infrastructure](https://docs.helix.ml/docs/controlplane) or [try it out on our SaaS](https://app.tryhelix.ai):

Along with a shiny new UI (you can a screenshot for comparison in our [first post](https://helixml.substack.com/p/building-a-generative-ai-platform)), we‚Äôve been extremely focused on improving the quality of the text fine-tuning.

When we first launched Helix, the text fine tuning of the Mistral-7B-Instruct language model was based on [this LlamaIndex docs page](https://docs.llamaindex.ai/en/latest/examples/finetuning/knowledge/finetune_knowledge.html). Yeah, we basically decided to build a whole business around one this LlamaIndex notebook.

At the time, unfortunately I hadn‚Äôt even seen the link from the parent page in the LlamaIndex docs that said ‚ÄúWIP: this isn‚Äôt very good yet‚Äù. Uh-oh.

Of course, because Helix is targeting being runnable fully on-prem, we‚Äôre using [Mistral-7B with axolotl](https://github.com/OpenAccess-AI-Collective/axolotl) for fine-tuning instead of the GPT-3.5 API, but it‚Äôs the same principle. The idea is that you chunk your documents up into pieces, then ask a language model to generate question-answer pairs (which is the shape of the training data you need to provide the fine-tuning process). You‚Äôre using an LLM to automate generating training data for fine-tuning another LLM. The original prompt looks like this:

Did it work? Sorta, but it was a bit crap. Just like LlamaIndex said it would be.

We were able to feed it [complex technical papers](https://docs.helix.ml/docs/papers), and it was able to answer technical questions about them. But it failed at some much more basic tasks.

This one news article for example: [Junior doctors in England to stage more strikes over pay](https://www.theguardian.com/society/2023/dec/05/junior-doctors-in-england-to-stage-more-strikes). This article became the bane of my life for a few weeks üòÇ

Why? Because when we first asked the fine tuned model the simple question:

> What?! You were fine-tuned on information derived from the article. Why are you talking about fine-tuning?!
> 
> ‚Äìme

OK, turned out this one was fairly straightforward. One of the text elements meant to tell the user ‚Äúfine tuning completed‚Äù was also being sent back to the model as if the user had said it. Now the only context the model had to go on was the idea of fine-tuning. We got that one out of the way. Cool, let‚Äôs try again:

> OMG, seriously? Surely you should know that the doctors are threatening to go on strike. It‚Äôs right there in the title! Stupid fine-tuning, maybe it will never work. ü§¨
> 
> ‚Äìme

But we persisted. OK, so why isn‚Äôt the model able to answer questions about the basic context of the article? Well, it‚Äôs because the question-answer pairs generated by the prompt aren‚Äôt in the form of simple questions about the title of the document. The solution, it turned out, was rather than a single qapair generating prompt, we had to implement a *suite* of them, to extract context from the document from all sorts of different perspectives: what are the entities in the document and how are they related? What‚Äôs a short, medium and long summary of the document? Who/what/where questions, etc. [See the full list here](https://github.com/helixml/helix/blob/main/api/pkg/dataprep/qapairs/qapair_config.yaml).

Turns out, when we carefully constructed this suite of prompts, we were then finally able to get the model to answer the basic question about ‚Äúwhat are the doctors going to do?‚Äù Phew! üòÖ

Our other insight was that by generating a content-addressed hash for each document, we could also teach the model about the IDs of the individual documents, along with IDs for groups of documents.

We can then map those IDs back onto the documents the model was fine-tuned on. For example: [in this session](https://app.tryhelix.ai/session/4d769c7c-b9a1-4fc4-b5c9-d277fc59ed1b) the model is able to tell you that what it‚Äôs learned was in a given document, even linking the user back to that document. I also found this exchange pretty hilarious:

Although maybe that says more about my sense of humour than anything else.

We then added a system prompt telling the model to refer only to the specific document IDs it was trained on and not to refer to background knowledge. What do you know, it worked!

So far we‚Äôre adjusting the prompts and system for this LLM app based on ‚Äúvibes‚Äù. That is, trying stuff, evaluating it by eye, and then changing it. Problem is, vibes don‚Äôt scale.

Work is ongoing on an end-to-end ‚Äúevals‚Äù framework so we can automatically build up a library of good and bad sessions, and then every time we change the prompts, code, model etc re-run the fine-tuning across all of the sessions in the library, and then grade them. We might even use an LLM to grade them automatically :-)

Please help us by clicking the new thumbs up and thumbs down buttons at the bottom of your sessions! We‚Äôll use these as input to improve the product.

Oh believe me, we‚Äôve talked about it. We‚Äôre sticking with fine-tuning for now, because:

*   Fine tuning can memorize far more information than you can fit in a single prompt

*   By not needing to cram any custom knowledge in the prompt, you can get much better latency

*   Fine tuning is better at copying style (we have qapair prompts planned for this)

*   Fine tuning is better at understanding a large corpus of background knowledge (a ‚Äúdomain‚Äù) and being able to draw on all of it when constructing an answer

*   Fine tuned models are easier to run at the edge without needing the infrastructure of vector stores close to the model

*   You can use a much smaller fine-tuned model than general purpose model plus RAG. What can you do with a fine-tuned Phi-2 model that can run on any CPU?

*   We made it work!!

Are we wrong? [Come and roast us on Discord](https://discord.gg/VJftd844GE)!

We now have an OpenAI compatible API. For example, here I am configuring Flowise to integrate with my private Helix deployment, for example:

Just set:

*   **Model Name:** mistralai/Mistral-7B-Instruct-v0.1

*   **Connect Credential:** Your API key from [https://app.tryhelix.ai/account](https://app.tryhelix.ai/account)

*   **BasePath (under additional parameters):** https://app.tryhelix.ai/v1

And that‚Äôs it! You‚Äôll notice your API calls to Helix show up as sessions in your account, so you get a free record of them :-)

We‚Äôll automatically fine tune the model once the qapairs are extracted, then email you when it‚Äôs finished. We hope this will encourage more people to dive back into the app once you‚Äôve waited the 10-15 minutes it takes to train your own AI.

Thanks for reading! Soon I‚Äôll blog more about our roadmap, on-prem use cases where we‚Äôre seeing significant commerial traction, and our dirty secret. Subscribe and stay in the loop :-)