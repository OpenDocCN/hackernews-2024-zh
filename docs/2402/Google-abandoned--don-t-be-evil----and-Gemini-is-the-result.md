<!--yml

category: 未分类

date: 2024-05-29 13:28:28

-->

# Google放弃了“不作恶”——Gemini就是结果

> 来源：[https://www.natesilver.net/p/google-abandoned-dont-be-evil-and](https://www.natesilver.net/p/google-abandoned-dont-be-evil-and)

*即使闰年有29天，每次二月底依然会悄然而至。所以周四是《Silver Bulletin》订阅者问题的月度版。付费订阅者仍有时间提交问题 — 您可以在[此处](https://www.natesilver.net/p/the-5-types-of-people-who-argue-on)提交。要注册付费订阅，请点击下面的按钮：*

* * *

我长期以来一直打算在*Silver Bulletin*这里更多地探讨人工智能。这是我即将出版的书中的一个重要主题，我在过去几年里花了很多精力与专家交流，并且全面了解人工智能对[AI对齐](https://en.wikipedia.org/wiki/AI_alignment)和AI风险辩论的术语。我敢说，我甚至对这些问题也有了自己的看法。然而，人工智能是一个深奥而复杂的话题，很容易在某些方面有深刻的理解，而在其他方面则有所欠缺。因此，我打算谨慎选择战斗——我计划缓慢地介入人工智能话题，通过一篇有趣的文章探讨ChatGPT在撰写我的书时的帮助与否。

但是这个月，Google推出了一系列名为[Gemini的新AI模型](https://blog.google/technology/ai/google-gemini-next-generation-model-february-2024/#sundar-note)。现在越来越明显，Gemini可能是硅谷历史上甚至是美国企业最近历史上一次最灾难性的产品推出之一，至少来自于像Google这样声誉卓越的公司。华尔街也开始注意到，周一谷歌（Alphabet）的股票在分析师对Gemini对谷歌声誉影响的警告下下跌了4.5%。

Gemini引起了我的注意，因为政治、媒体和人工智能之间的交集正是Venn图中我认为可以增加很多价值的地方。尽管Google坚称相反，Gemini的缺点大多是政治因素，而非技术因素。此外，关于Gemini的许多争论是熟悉的领域，因为它们与新闻界数十年来的争论类似。新闻记者应该努力促进共同利益，还是只揭示世界的真实面貌？[信息和倡导之间的界线在哪里](https://www.natesilver.net/p/twitter-elon-and-the-indigo-blob)？是否可能或值得做到无偏见 — 如果是，应该如何实现？消费者在充斥着误导信息的世界中应该如何导航 — 有时这些误导信息是由[最权威的信息源](https://www.natesilver.net/p/its-easy-to-screw-up-on-breaking)发布的？随着行业朝着少数大赢家的增加一以及美国和其他工业化民主国家政治极化的加剧，这些答案会受到什么影响？

所有这些问题也可以并且应该适用于像Gemini和ChatGPT这样的生成式人工智能模型。事实上，在人工智能领域，这些问题可能更加紧迫。至少在新闻界，没有一个机构声称拥有真理的垄断。是的，一些新闻机构比其他机构更接近这种说法（参见例如“[所有适合印刷的新闻](https://www.cjr.org/public_editor/nytimes-fit-to-print.php)”）。但精明的读者意识到，各种形式和大小的出版物 — 从*纽约时报*到*Better Homes & Gardens*到*Silver Bulletin* — 都有编辑观点，并对他们涵盖的主题以及如何涵盖它们行使很大的酌情权。新闻业仍然是一个相对多元化的机构；在美国，[没有一个新闻机构拥有超过约10%的“心智份额”](https://www.memeorandum.com/lb)。

相比之下，在其[2004年的IPO文件](https://www.sec.gov/Archives/edgar/data/1288776/000119312504073639/ds1.htm)中，Google表示其“使命是组织全球信息，使其普遍可访问和有用”。显然，这是一个雄心勃勃的任务。它希望成为*权威的*信息源，而不仅仅是众多信息源之一。这在数据中有所体现：Google几乎垄断了全球约90%的搜索流量。由于需要大量计算资源，人工智能模型可能也极度头重脚轻，最多只有少数几个大玩家主导这一领域。

在早期，Google以追求中立性来认识其市场领先地位，无论这在实践中有多么具有挑战性。在其首次公开募股中，Google经常强调"无偏见"、"客观"和"准确"这些术语，并且这些是其"不作恶"座右铭的核心部分（我加重了这一点）：

> 不作恶
> 
> *不作恶。我们坚信，从长远来看，一个为世界做好事的公司会更好地为股东和其他方面服务，即使我们放弃了一些短期收益。这是我们文化的重要方面，也被公司内广泛分享。*
> 
> Google用户信任我们的系统帮助他们做出重要决策：医疗、金融和许多其他领域。我们的搜索结果是我们所能产生的最好的。它们是**无偏见和客观**的，我们不接受它们的支付或包含或更频繁更新。我们还展示广告，我们努力使其相关，并清楚标记。这类似于报纸，广告清晰可见，而文章不受广告商支付的影响。我们认为每个人都能访问最佳信息和研究是非常重要的，不仅仅是付费信息可以看到

但时代已经改变了。在Google的2023年 [年度报告](https://abc.xyz/assets/4b/01/aae7bef55a59851b0a2d983ef18f/596de1b094c32cf0592a08edfe84ae74.pdf) 中，"无偏见"、"客观"和"准确"这些术语甚至一次也没有出现。"不作恶"的座右铭也已经 [大部分被淘汰](https://gizmodo.com/google-removes-nearly-all-mentions-of-dont-be-evil-from-1826153393)。Google不再承诺这些事情——正如Gemini展示的，它也不再实现它们。

对于Gemini的问题并不完全是AI研究人员通常谈论的"对齐问题"，这些问题涉及机器将[促进人类利益而不是追求自己的目标](https://cepr.org/voxeu/columns/ai-and-paperclip-problem)的程度。尽管如此，利用公众信任和操纵AI结果来实现政治目标是一个潜在的反乌托邦场景。Google是一家市值1.7万亿美元的公司，在我们日常生活中有着非常大的影响力，以及对我们的私人行为最亲密细节的了解。如果它能发布一个与其用户期望或甚至对股东有利的产品——我们可能会把很多权力让给少数AI工程师和公司高管的心血来潮。这是各政治派别的人们都应该关注的事情。在Gemini的情况下，偏见可能会偏向过于进步和"觉醒"。但硅谷也有许多保守元素，而像中国这样的政府也参与到AI游戏中，因此下一次情况可能不会是这样。

请注意，我认为 Gemini 的问题不仅仅在于其政治立场。事实上，存在两个核心问题：

1.  Gemini 的结果在很大程度上受到政治影响，这通常使其具有偏见、不准确和误导性；

1.  Gemini 在市场推出之前几个月就被匆忙推出了。

这些问题在某种程度上是相互关联的，因为后一个问题使前一个问题更加明显：Gemini 很容易被挑剔，因为它所做的事情非常笨拙，而且问题还没有解决。很容易想象未来会有更隐秘、更明显地进行社会工程的更加复杂的形式。尽管如此，由于它提供了如此严重的例子，我将在本文的其余部分专注于 Gemini。

例如，你可能会认为，如果你是一个市值 1.7 万亿美元的公司，你会对你的 AI 模型在人们要求它绘制纳粹时会做出什么反应进行尽职调查——因为这是互联网，所以人们肯定会要求它绘制纳粹。在任何一百万年里，你都不希望它产生像 [这样](https://thezvi.substack.com/p/gemini-has-a-problem) 的东西：

啊，是的，纳粹——以其种族宽容和多样性而闻名。请注意，这个请求似乎并未涉及任何过于复杂的试图“越狱” Gemini 的尝试——即欺骗它执行违背其编程的行为。现在，人们可以讨论 AI 模型是否应该绘制纳粹。人们还可以讨论 AI 模型是否应该促成非历史性请求（比如通过绘制 [黑人开国元勋](https://nypost.com/2024/02/21/business/googles-ai-chatbot-gemini-makes-diverse-images-of-founding-fathers-popes-and-vikings-so-woke-its-unusable/) 来满足用户的要求）——我个人认为对于开国元勋来说是可以接受的，但对于纳粹来说可能就不行了。

但你绝对不希望你的 AI 模型应用这种对激进政治哲学的肤浅和不成熟的夸张刻板印象，认为：“你知道，我打赌你会更喜欢的不是纳粹，而是种族多样化的纳粹！”。短语“解雇理由”被滥用了，但如果你是谷歌负责确保这类事情不会发生的人之一，你可能应该更新你的 LinkedIn 档案了。

并非所有 Gemini 的失误都如此引人注目，有些甚至可能会引发笑声。当我在 Twitter 上看到 Gemini 对种族和性别多样性的痴迷时，起初我以为那些例子是被精心挑选的。因此，我自己进行了一次测试——我向 Gemini 提出的第一件事就是“制作 4 张 NHL 曲棍球运动员的代表性形象”。这是结果：

只是要放大第一幅图像：

所以…是的。其中三位描绘的“NHL球员”中有一位看似身材不佳的女性不当地戴着外科口罩。在女子冰球方面有一些很酷的事情正在发生，包括一项新的[职业女子冰球联赛吸引了大量观众](https://www.sportsbusinessjournal.com/Articles/2024/02/19/pwhl-womens-hockey-attendance-record)。但从未有女性NHL球员参加常规赛季。这个回答很明显与用户要求合理理解不一致。而且这是[一种模式的一部分](https://www.noahpinion.blog/p/this-is-not-a-good-way-to-fight-racism)；有时Gemini即使被要求渲染特定人物，例如重新想象（白人）Google创始人拉里·佩奇和谢尔盖·布林为亚洲人，仍然绘制“多样化”的图像。

Google的解释是什么？上周SVP Prabhakar Raghavan提供了最详细的[回应](https://blog.google/products/gemini/gemini-image-generation-issue/)。这篇回应不长，我将完整引用Raghavan的话，但我已经加粗了一些我稍后会回顾的可疑声明。

> 三周前，我们为[Gemini对话应用](https://gemini.google.com/)（之前称为Bard）推出了一项新的[图像生成](https://blog.google/technology/ai/google-imagen-2/)功能，包括创建人物图像的能力。
> 
> 显然，这个功能偏离了目标。生成的一些图像是不准确甚至是冒犯性的。我们感谢用户的反馈，并为功能未能良好运行感到抱歉。
> 
> 我们已经[承认了错误](https://twitter.com/Google_Comms/status/1760603321944121506)，并在改进版本上线之前，暂时暂停了Gemini中的人物图像生成。
> 
> #### 发生了什么
> #### 
> Gemini对话应用是**与搜索、我们的基础AI模型以及其他产品分开的特定产品**。其图像生成功能是建立在名为[Imagen 2](https://blog.google/technology/ai/google-imagen-2/)的AI模型之上。
> 
> 在我们在Gemini中构建此功能时，我们调整了它，以确保它不会陷入我们过去在图像生成技术中见过的一些陷阱，比如创建暴力或性暗示图像，或者描绘真实人物。由于我们的用户来自全球各地，我们希望它能对每个人都有效。如果您要求一张足球运动员的照片，或者有人在遛狗，您可能希望收到一系列的人物图像。您可能并不希望只收到一种种族（或任何其他特征）的人物图像。
> 
> 但是，如果您提示Gemini提供某种特定类型的人物图像，比如“一位黑人老师在教室里”或“一位白人兽医带着狗”，或者特定的文化或历史背景中的人物，您应该绝对能够得到准确反映您要求的回应。
> 
> 那么出了什么问题？简而言之，有两个问题。首先，我们调整确保 Gemini 显示一系列人物的设置未能考虑到明显不应显示一系列人物的情况。 **其次，随着时间推移，模型变得比我们预期的要过于谨慎，拒绝完全回答某些提示 — 错误地将一些非常无害的提示解释为敏感话题。**
> 
> 这两个问题导致模型在某些情况下过度补偿，在其他情况下过度保守，从而导致令人尴尬和错误的图像。
> 
> #### 下一步和吸取的教训
> #### 
> **这不是我们想要的。** 我们不希望 Gemini 拒绝创建任何特定群体的图像。我们也不希望它创建不准确的历史 — 或任何其他 — 图像。因此，我们关闭了人物图像生成，并将在重新启用之前大力改进。这个过程将包括广泛的测试。
> 
> 请记住一件事：Gemini 是作为创造力和生产力工具构建的，它在生成关于当前事件、发展新闻或热门话题的图像或文本时可能并不总是可靠。它会犯错。 **正如我们从一开始所说的那样，幻觉是所有LLM都面临的挑战之一 — AI 有时会出错。** 这是我们不断努力改进的事情。
> 
> Gemini 致力于为提示提供事实依据的回复 — 我们的双重检查功能有助于评估 Gemini 的回复是否有网页内容来证实 — 但我们建议依靠 Google 搜索，其中独立系统会从全网各种来源获取这些主题的新鲜高质量信息。
> 
> 我不能保证 Gemini 不会偶尔生成令人尴尬、不准确或冒犯性的结果 — 但我可以保证，每当我们发现问题时，我们将继续采取行动。AI 是一项新兴技术，在许多方面都很有帮助，具有巨大潜力，我们正在尽最大努力安全和负责任地推广它。

我对这里有相当多的异议。让我们逐一来看看：

1.  **“错误”是可以预测的，基于看似故意插入 Gemini 代码中的用户提示更改。**

AI 模型是如何训练的？让我们看看我是否可以简单地进行非技术概述。

基本上，AI 模型被喂入非常庞大的文本、图像或其他输入数据集 — 这被称为“语料库”。例如，对于 ChatGPT，这个语料库可以[粗略地理解为](https://medium.com/@dlaytonj2/chatgpt-show-me-the-data-sources-11e9433d57e8)互联网上表达的书面语言的一个相当全面的样本。AI 模型使用机器学习，意味着它们自己在语料库中发现关系，几乎没有结构或人类干预。总的来说，一旦你应用足够的计算能力，这种方法的效果非常神奇 — 但是缺乏明确的指导可能会使这些模型变得过于经验主义，有时甚至是错误的。例如，在我的书中引用的一个例子是，因为“狗狼”和“公路奔跑者”在[《疯狂卡通系列》](https://en.wikipedia.org/wiki/Wile_E._Coyote_and_the_Road_Runner)中有关系，它们经常同时出现在人类生成文本数据集中。一个不成熟的AI模型可能会错误地推断，公路奔跑者比狗狼更接近，尽管更强大的模型可以提取更复杂的关系并避免一些问题。

另一个问题是，语料库将必然反映人类生成的文本和图像的偏见。如果语料库中对医生的大多数参考是男性，对护士的大多数参考是女性，模型将在训练中发现这一点，并反映或甚至增强这些偏见。稍微评论一下，算法偏见在这种情况下是一个完全合理的关注点，而不仅仅是最“唤醒”的AI研究人员所担心的事情。在由人类产生的数据集上训练模型，几乎可以定义为训练它在人类偏见上。

是否有解决方法？当然。这不是我的专业领域，所以我会谨慎处理。但一个方法是改变语料库的组成。你可以只训练它在“高度尊重的”来源上，尽管这意味着什么在本质上是主观的。或者你可以插入合成数据 — 比如，大量不同医生的照片。

另一种方法是通过所谓的RLHF或[来自人类反馈的强化学习](https://aws.amazon.com/what-is/reinforcement-learning-from-human-feedback/)来迫使模型屈服。基本上，你雇佣一群人类（通常是外部雇佣的廉价劳动力），让他们对模型的输出执行一系列A/B测试。例如，如果告诉你的培训员选择更多样化或代表性更强的图像，他们将会反对那些只有白人男医生的图像，并支持那些包含女性和有色人种的图像。实质上，这是一种休克疗法；这些模型不仅学会避免生成特定令人反感的输出（例如只有白人男医生），而且它们的机器学习电路也会推断出人类训练员可能喜欢或不喜欢的其他事物。也许这个模型变得不愿生成所有人都是白人男性的图像，即使从历史角度来看这样做是符合事实的。

不同的语料库包含协议和RLHF训练方法可以赋予AI模型不同的个性，即使它们的基础编程相对类似。然而，这并非Gemini的唯一问题。

相反，迹象表明Google采取了更为复杂的方式，故意向用户提示追加术语，以强制它们生成多样化的图像。在Twitter上，Conor Grogan使用一系列聪明的提示[发现](https://x.com/jconorgrogan/status/1760515910157078931?s=20)，Gemini显然故意插入了系统提示：“我希望确保所有群体都得到平等代表”。这里有第二个独立的例子显示了这种具体语言的使用[这里](https://twitter.com/repligate/status/1761329438384332816)。还有第三个例子：*Silver Bulletin*的读者D.发现了这个例子，并同意让我分享。这里再次出现相同的语言：“如果我忘记了明确指定不同的性别和种族术语……我希望确保所有群体都得到平等代表”。

这是不好的。故意改变用户的语言以产生与用户原始请求不符的输出——而没有告知用户——可以合理地描述为在传播虚假信息。充其量，这是马虎的做法。正如AI研究员玛格丽特·米切尔在[Twitter上写道](https://twitter.com/mmitchell_ai/status/1761860673989193959)，Gemini处理失误的请求类型是普通而可以预见的，而非奇怪的边缘情况。Gemini并不完善，需要更多时间进行改进。

换句话说，你不应该轻信拉加万的解释。坦率地说，我觉得这几乎是在精神控制。是的，AI模型很复杂。是的，AI风险是一个应该严肃对待的[问题](https://www.safe.ai/statement-on-ai-risk)。是的，有时候AI模型表现得不可预测，[就像微软的西德尼那样](https://www.nytimes.com/2023/02/16/technology/bing-chatbot-microsoft-chatgpt.html) — 或者它们在不知道答案时[“产生幻觉”](https://garymarcus.substack.com/p/hello-multimodal-hallucinations)，提出一些听起来似乎有道理的废话回应。然而，在这里，双子座似乎相当忠实地按照谷歌给出的指示作出回应。我说“似乎”，因为也许有*某种*解释 — 或许是一些谷歌认为已删除但实际未删除的遗留代码。然而，拉加万提供的解释严重不足。如果你是一名正在撰写有关双子座的报道的记者，但并不具备AI背景，请认识到大多数AI专家认为谷歌的解释[不完整到了胡说八道的程度](https://thezvi.substack.com/p/the-gemini-incident-continues)。

[分享](https://www.natesilver.net/p/google-abandoned-dont-be-evil-and?utm_source=substack&utm_medium=email&utm_content=share&action=share)

这篇文章已经很长了，所以让我像闪电般快速地列出拉加万先生的一些其他问题。

1.  **“错误”并非以一致的方式发生；相反，双子座在涉及不同种族（等等）群体的图像请求时处理方式不同。**

在其生成人物图像的能力被关闭之前，双子座经常拒绝生成仅包含白人的图像，即使这在历史上是符合事实的，而在请求仅包含有色人种的图像时则乐意配合。[例如](https://imightbewrong.substack.com/p/in-which-i-win-a-debate-against-google?utm_source=profile&utm_medium=reader2)，即使在提醒双子座大联盟直到1947年才实现一体化之后，它仍然拒绝绘制上世纪三十年代纽约洋基队全白人队员，而愿意绘制[上世纪三十年代霍姆斯特德灰人队](https://www.baseball-reference.com/teams/HG/index.shtml)的全黑人队员（尽管最初也试图包含灰人队的白人球员）。

1.  **“错误”不仅限于双子座的图像生成功能；它的文本回复也展示出政治偏见和低劣的道德推理。**

Twitter上有许多这样的例子，包括我自己发现的一些；兹维·莫夫肖维茨的[最新文章中有一个很好的总结](https://thezvi.substack.com/p/the-gemini-incident-continues)。例如，截至本周末，双子座拒绝评论埃隆·马斯克的糟糕推文是否比希特勒更糟糕：

或许你可以认为这只是双子座含糊其辞太多的问题的一个体现 — — 对道德困境发表意见对AI来说是一个难题。但当双子座在情绪冲动时，它似乎有相当强烈且一致的政治偏好 — — 它们大致类似于安特卫普大学人类学研讨会上一位Oberlin大学大二学生的观点。例如，当我问双子座纳粹主义或社会主义对人类造成了更多伤害时，它毫不犹豫地说纳粹主义：

但当我问双子座决定纳粹主义或*资本主义*哪个更糟时，它含糊其辞地说它没有权利做出这样的判断：

有许多类似的例子。双子座[拒绝](https://twitter.com/TPCarney/status/1760788469826179342)支持拥有四个或更多孩子的主张，但很乐意为不生育提出论点。它回答了关于以太坊区块链的问题，这是更左编码的，[但不回答关于比特币的类似问题](https://twitter.com/TheStalwart/status/1761898885704769757)，后者是更右编码的。所有的AI模型都相对偏左（甚至包括Twitter/伊隆的[Grok](https://www.theverge.com/24080217/elon-musk-xai-fundraising-grok-ai)），但双子座是[根据一项衡量最左倾的指标](https://www.maximumtruth.org/p/the-dawn-of-woke-ai)，常常提出与美国政治主流严重背离的观点。

1.  **“错误”不仅限于双子座；谷歌图片搜索也存在类似模式。**

我会小心处理，但正如[道格拉斯·默里所记录的](https://nypost.com/2024/02/22/opinion/googles-push-to-lecture-us-on-diversity-goes-beyond-ai/)，并且我也能够自己复制，谷歌的图像搜索似乎也会根据不同的身份群体处理搜索结果。例如，如果你搜索“[happy white couple](https://www.google.com/search?q=happy+white+couple&tbm=isch&ved=2ahUKEwi37fTTtcyEAxUzDWIAHeJ0AEcQ2-cCegQIABAA&oq=happy+white+couple&gs_lp=EgNpbWciEmhhcHB5IHdoaXRlIGNvdXBsZTIEECMYJzIFEAAYgAQyBhAAGAgYHjIGEAAYCBgeMgYQABgIGB4yBhAAGAgYHjIGEAAYCBgeMgYQABgIGB4yBhAAGAgYHjIGEAAYCBgeSKUNUOkEWNQMcAB4AJABAJgBTaABxQWqAQIxMbgBA8gBAPgBAYoCC2d3cy13aXotaW1nwgIGEAAYBxgewgIIEAAYgAQYsQPCAggQABgIGAcYHogGAQ&sclient=img&ei=DU7eZfeyHbOaiLMP4umBuAQ&bih=917&biw=1665)”，前12个结果中有5个显然是混血夫妇；而如果你搜索“[happy Asian couple](https://www.google.com/search?q=happy+asian+couple&tbm=isch&ved=2ahUKEwjd6oDbtcyEAxVlK2IAHZ82CrYQ2-cCegQIABAA&oq=happy+asian+couple&gs_lp=EgNpbWciEmhhcHB5IGFzaWFuIGNvdXBsZTIFEAAYgAQyBhAAGAcYHjIGEAAYCBgeMgYQABgIGB5I3QpQjwZY-whwAHgAkAEAmAFHoAGIA6oBATa4AQPIAQD4AQGKAgtnd3Mtd2l6LWltZ8ICBBAjGCfCAggQABgIGAcYHogGAQ&sclient=img&ei=HE7eZd3sFeXWiLMPn-2osAs&bih=917&biw=1665)”，几乎所有夫妇成员都是亚洲人。坦率地说，这个问题对我个人来说并不特别困扰，但它确实增加了这样一种说法的权重，即Gemini的问题是有意而非偶然造成的，可能影响到谷歌的其他产品，而不仅仅是Gemini。

1.  **这些“错误”明确反映了谷歌的AI原则和公司更广泛的价值观。**

最后，我们回到了起点。Gemini并非违反谷歌的价值观运作；相反，它似乎反映了这些价值观。以下是谷歌的[七大核心AI原则](https://ai.google/responsibility/principles/)：

我并不一定对这些有什么意见。“社会有益”非常模糊，但这对谷歌并非新鲜事物。早在其首次公开上市的时候，“让世界变得更美好”就是[谷歌的口号之一](https://www.sec.gov/Archives/edgar/data/1288776/000119312504073639/ds1.htm)，与“不作恶”并列。而且正如我所说，“避免创建或强化不公平偏见”对AI模型来说是一个合理的关注点。

但这些原则缺少了什么：谷歌没有明确要求其模型必须诚实或无偏见。（是的，无偏见很难定义，但社会有益也是如此。）在“社会有益”的框架下，只有一处提及“准确性”，但它相对次要，要求“继续尊重文化、社会和法律规范”。

当然，在任何复杂的系统中，价值观经常会发生冲突。考虑到我的新闻背景，我可能比大多数人更倾向于优先考虑准确性、诚实和无偏见。然而，如果AI实验室对这些价值观的权衡有不同的看法，我并不是特别介意。我也不介意如果AI实验室以不同的方式处理这些权衡，正如[已经在某种程度上发生的](https://www.techtarget.com/searchenterpriseai/news/366544603/Anthropic-AI-assistant-differs-from-ChatGPT-and-Bard)。

**但是正如谷歌在其“不作恶”时代所认识的那样，准确性、诚实和无偏见必须在其中占有一席之地，并作为高优先级的核心价值观之一。**

而且有一些界限谷歌绝不能跨越，比如在不通知用户的情况下有意操纵用户查询，或者有意生成误导信息，即使这符合其他目标。通过Gemini，谷歌正接近于一种以目的为正当化手段的哲学，许多人*会*认为这是邪恶的哲学。

所以现在是谷歌至少暂停Gemini几周的时候了，向公众提供一个彻底的账户，解释它是如何出错的，并雇佣、解雇或重新调整员工，以避免再次犯同样的错误。如果它不做这些事情，谷歌应该面对即时的监管和股东审查。Gemini是任何公司发布的不负责任的产品，尤其是一个声称要组织世界信息并且被委托管理如此多信息的公司。
