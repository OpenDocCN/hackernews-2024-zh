- en: <!--yml
  id: totrans-split-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-split-1
  prefs: []
  type: TYPE_NORMAL
  zh: 'category: 未分类'
- en: 'date: 2024-05-27 14:37:48'
  id: totrans-split-2
  prefs: []
  type: TYPE_NORMAL
  zh: 'date: 2024-05-27 14:37:48'
- en: -->
  id: totrans-split-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: Demystifying GPU Compute Architectures - by Babbage
  id: totrans-split-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 解析GPU计算架构 - 作者：巴贝奇
- en: 来源：[https://thechipletter.substack.com/p/demystifying-gpu-compute-architectures](https://thechipletter.substack.com/p/demystifying-gpu-compute-architectures)
  id: totrans-split-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://thechipletter.substack.com/p/demystifying-gpu-compute-architectures](https://thechipletter.substack.com/p/demystifying-gpu-compute-architectures)
- en: NVIDIA_GeForce_8800GTX_G80___DSC01200 via Fritzchens Fritz https://commons.wikimedia.org/w/index.php?curid=65926359
  id: totrans-split-6
  prefs: []
  type: TYPE_NORMAL
  zh: NVIDIA_GeForce_8800GTX_G80___DSC01200 via Fritzchens Fritz https://commons.wikimedia.org/w/index.php?curid=65926359
- en: The complexity of modern GPUs needn’t obscure the essence of how these machines
    work. If we are to make the most of the advanced technology in these GPUs then
    we need a little less mystique.
  id: totrans-split-7
  prefs: []
  type: TYPE_NORMAL
  zh: 现代GPU的复杂性不应该掩盖这些机器运行的本质。如果我们要充分利用这些GPU的先进技术，那么我们需要少一点神秘感。
- en: I think this is one of the most exciting times in computer architecture for
    many years. In CPUs we have x86, Arm and RISC-V, three increasingly well-supported
    application architectures.
  id: totrans-split-8
  prefs: []
  type: TYPE_NORMAL
  zh: 我认为这是计算机架构领域中令人激动的时刻之一。在CPU中，我们有x86、Arm和RISC-V，这三种逐渐得到越来越多支持的应用架构。
- en: And GPU architectures are, of course, becoming much more important. GPUs are
    obviously a critical piece of machine learning infrastructure. They are also making
    massively parallel computing available to help solve a wider range of computational
    problems outside of machine learning.
  id: totrans-split-9
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，GPU架构变得越来越重要。显然，GPU是机器学习基础设施的关键组成部分。它们还使得大规模并行计算可用于帮助解决更广泛的计算问题，超越了机器学习范畴。
- en: There is one problem though. How GPU architectures work is much less widely
    understood than is the case for CPUs. CPUs are often complex, but much of that
    complexity (pipelines, instruction and data caches, branch prediction, out-of-order
    execution, register renaming, virtual memory and so on) is largely hidden from
    the user, and can often be ignored. Doing things in parallel, though, is inherently
    more complex and GPUs necessarily expose a lot of complexity.
  id: totrans-split-10
  prefs: []
  type: TYPE_NORMAL
  zh: 不过有一个问题。相比于CPU，人们对GPU架构的理解远不如后者广泛。CPU通常很复杂，但其中大部分复杂性（如流水线、指令和数据缓存、分支预测、乱序执行、寄存器重命名、虚拟内存等）大多数时候对用户来说是隐藏的，甚至可以忽略不计。然而，并行处理本质上更复杂，而GPU必然暴露了很多复杂性。
- en: '* * *'
  id: totrans-split-11
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: 'There is also a degree of mystique about GPUs. The recent (and excellent) Acquired
    podcast [episode](https://www.acquired.fm/episodes/nvidia-the-dawn-of-the-ai-era)
    on Nvidia stated that for GPUs:'
  id: totrans-split-12
  prefs: []
  type: TYPE_NORMAL
  zh: 关于GPU也存在一定程度的神秘感。最近（而且非常出色的）Acquired播客[集数](https://www.acquired.fm/episodes/nvidia-the-dawn-of-the-ai-era)中提到，对于GPU：
- en: The magical unlock, of course, is to make a computer that is not a von Neumann
    architecture.
  id: totrans-split-13
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 当然，神奇的突破在于制造一台不是冯·诺伊曼体系结构的计算机。
- en: I know the Acquired hosts didn't mean to imply that GPUs are ‘magical’ but still
    the use of the word in connection with GPUs somehow seems appropriate. GPUs don’t
    use a ‘von Neumann’ architecture. Instead they use an unspecified, unnamed architecture,
    and if we don’t know what it is then its necessarily mysterious.
  id: totrans-split-14
  prefs: []
  type: TYPE_NORMAL
  zh: 我知道Acquired的主持人并不是想暗示GPU是‘神奇’的，但在与GPU相关的语境中使用这个词似乎很合适。GPU不使用‘冯·诺伊曼’架构。相反，它们使用一种未指定的、未命名的架构，如果我们不知道它是什么，那它必然是神秘的。
- en: 'To make matters worse, there are several barriers to learning how GPUs work:'
  id: totrans-split-15
  prefs: []
  type: TYPE_NORMAL
  zh: 更糟糕的是，了解GPU工作原理存在几个障碍：
- en: '**Architectural Variations:** Each of the major GPU architectures has a somewhat
    different implementation.'
  id: totrans-split-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**架构变化：** 每个主要的GPU架构都有稍微不同的实现。'
- en: '**Inconsistent Terminology:** The major GPU designers use inconsistent terminology,
    both between themselves and with terms commonly used for CPUs. So a CPU ‘core’
    is very different to Nvidia’s ‘CUDA core’.'
  id: totrans-split-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**术语不一致：** 主要的GPU设计者在术语使用上存在不一致，他们之间以及与通常用于CPU的术语之间都不同。因此，CPU的‘核心’与Nvidia的‘CUDA核心’有很大不同。'
- en: '**Legacy Terminology:** Some of the terms used are carried forward from the
    graphics origins of the GPU designs (e.g. ‘shaders’).'
  id: totrans-split-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**传统术语：** 一些术语是从GPU设计的图形起源中继承而来的（例如‘着色器’）。'
- en: '**Software Abstraction:** Some GPU complexity can be hidden from the user behind
    the abstractions used by software packages used to program GPUs. This helps when
    implementing programs, but sometimes it prevents users from developing an adequate
    picture of how the architecture works.'
  id: totrans-split-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**软件抽象：** 一些GPU的复杂性可以通过软件包使用的抽象来隐藏，用于编程GPU。这有助于实现程序，但有时会阻止用户形成对架构运作方式的足够清晰的理解。'
- en: '**CUDA’s Dominance:** Because Nvidia’s CUDA has been the market leader in GPU
    computing for many years now, the CUDA terminology tends to be used in many explanations
    making it hard to read across to other architectures.'
  id: totrans-split-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**CUDA的主导地位：** 由于Nvidia的CUDA多年来一直是GPU计算的市场领导者，许多解释中使用了CUDA术语，使得理解其他架构变得困难。'
- en: No wonder it all seems a bit mysterious.
  id: totrans-split-21
  prefs: []
  type: TYPE_NORMAL
  zh: 难怪这一切看起来有点神秘。
- en: But as GPUs become more important, this seems like an unsatisfactory state of
    affairs. Let’s try to cut through the mystery!
  id: totrans-split-22
  prefs: []
  type: TYPE_NORMAL
  zh: 但随着GPU的重要性日益增加，这似乎是一个不令人满意的状态。让我们试图揭开这个神秘的面纱！
- en: '* * *'
  id: totrans-split-23
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: 'This post started out as a history of the architectures used for general purpose
    computing on Nvidia GPUs. However, I quickly changed course for two reasons:'
  id: totrans-split-24
  prefs: []
  type: TYPE_NORMAL
  zh: 这篇帖子最初是关于Nvidia GPU通用计算架构历史的。然而，由于两个原因，我很快改变了方向：
- en: Just explaining one generation of architectures is complex. Attempting to describe
    a whole series of generations of architectures is complexity squared, and probably
    just too much!
  id: totrans-split-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 解释一个架构的代际已经很复杂了。试图描述一系列架构的代际，就像复杂度的平方，可能就太多了！
- en: There is already a awesome, detailed post by Fabien Sangland called [A HISTORY
    OF NVIDIA STREAM MULTIPROCESSOR](https://fabiensanglard.net/cuda/index.html) that
    sets out the history of the development of Nvidia’s GPU general purpose computing
    architecture over the period 2006 to 2020.
  id: totrans-split-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 已经有一篇非常详细的帖子，法比安·桑格兰的《[NVIDIA流多处理器的历史](https://fabiensanglard.net/cuda/index.html)》，详细描述了Nvidia通用GPU计算架构从2006年到2020年的发展历史。
- en: Just a word of warning about Fabien’s post though. It assumes a lot of prior
    knowledge about GPUs. Don’t worry if you don’t know what a ‘Streaming Multiprocessor’
    is at this stage. All will, hopefully, become clear later! It’s worth scanning
    Fabien’s post, even if you’re not familiar with GPUs, to see how far Nvidia’s
    architectures have developed over this period.
  id: totrans-split-27
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，关于法比安的帖子，需要提个小心。它假设你对GPU有很多先验知识。如果你目前还不知道什么是“流多处理器”，不要担心。一切将在稍后变得清晰！即使你对GPU不熟悉，浏览法比安的帖子也是值得的，可以看到Nvidia架构在这段时间内的发展。
- en: Rather than repeat much of what Fabien has written, I decided that it would
    be more useful to provide a ‘ground up’ basic explanation of two recent architectures
    from Nvidia and AMD.
  id: totrans-split-28
  prefs: []
  type: TYPE_NORMAL
  zh: 我决定不重复法比安写的大部分内容，而是提供一个“从基础开始”的最新Nvidia和AMD两款架构的基本解释，这可能更有用。
- en: 'Before we move on though, I want to quote from Fabien’s post on how Nvidia’s
    general purpose compute architecture emerged:'
  id: totrans-split-29
  prefs: []
  type: TYPE_NORMAL
  zh: 不过，在我们继续之前，我想引用法比安在他的帖子中关于Nvidia通用计算架构如何出现的一段话：
- en: Up to 2006, Nvidia's GPU design was correlated to the logical stages in the
    rendering API. The GeForce 7900 GTX, powered by a G71 die is made of three sections
    dedicated to vertex processing (8 units), fragment generation (24 units), and
    fragment merging (16 units).
  id: totrans-split-30
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 直到2006年，Nvidia的GPU设计与渲染API中的逻辑阶段相一致。由G71芯片驱动的GeForce 7900 GTX由三个部分组成，专用于顶点处理（8个单元）、片段生成（24个单元）和片段合并（16个单元）。
- en: This correlation forced designers to guess the location of bottlenecks in order
    to properly balance each layers. With the emergence of yet another stage in DirectX
    10, the geometry shader, Nvidia engineers found themselves faced with the difficult
    task of balancing a die without knowing how much a stage was going to be adopted.
    It was time for a change.
  id: totrans-split-31
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 这种相关性迫使设计者们去猜测瓶颈的位置，以便正确平衡每个层次。随着DirectX 10引入了几何着色器的另一个阶段，Nvidia的工程师们面临着在不知道一个阶段会被采用多少的情况下平衡芯片的困难任务。是时候做出改变了。
- en: Nvidia solved the problem of escalating complexity with its "unified" Tesla
    architecture, released in 2006\.
  id: totrans-split-32
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: Nvidia通过其于2006年发布的“统一”特斯拉架构解决了复杂性不断上升的问题。
- en: In the G80 die, there is no more distinction between layers. The Stream Multiprocessor
    (SM) replaces all previous units thanks to its ability to run vertex, fragment
    and geometry "kernel" without distinction. The load balancing happens automatically
    by swapping the "kernel" run by each SM depending on the need of the pipeline.
  id: totrans-split-33
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 在G80芯片中，不再区分层次。流多处理器（SM）替代了以往的所有单元，因其能够在不区分的情况下运行顶点、片段和几何“核心”。通过每个SM运行的“核心”根据管道需求自动进行负载平衡。
- en: And so the first of Nvidia’s modern ‘general purpose compute’ architectures
    was born.
  id: totrans-split-34
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，Nvidia现代“通用计算”架构的第一代就此诞生。
- en: '*We pretty much threw out the entire shader architecture from NV30/NV40 and
    made a new one from scratch with a new general processor architecture (SIMT),
    that also introduced new processor design methodologies.'
  id: totrans-split-35
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*我们基本上抛弃了 NV30/NV40 的整个着色器架构，并从头开始采用了新的通用处理器架构（SIMT），还引入了新的处理器设计方法。*'
- en: '- Jonah Alben (Interview with extremetech.com)*'
  id: totrans-split-36
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '- Jonah Alben（与 extremetech.com 的采访）*'
- en: 'And quoting Fabien again:'
  id: totrans-split-37
  prefs: []
  type: TYPE_NORMAL
  zh: 再次引用 Fabien 的话：
- en: Tesla was a risky move which turned out to be a very Good Thing. It was so successful
    that it became the foundation of NVidia GPUs for the next two decades.
  id: totrans-split-38
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 特斯拉是一次冒险的举动，结果证明非常成功。它如此成功，以至于成为了 Nvidia GPU 在接下来二十年的基础。
- en: 'And we now how much of a ‘Good Thing’ this has been for Nvidia, which as I
    write this, has a $1.6 trillion market cap:'
  id: totrans-split-39
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我在撰写本文时所述，这对 Nvidia 真的是一件“大好事”，其市值已达到 1.6 万亿美元：
- en: 'Nvidia has, by now, put in almost 18 years of development and investment, starting
    with the Tesla architecture, and including work on Nvidia’s famous ‘Compute Unified
    Device Architecture’ or ‘CUDA’ introduced in 2007\. We’ve discussed some aspects
    of Nvidia’s strategy before (the first is now free to read):'
  id: totrans-split-40
  prefs: []
  type: TYPE_NORMAL
  zh: Nvidia 已经投入了将近 18 年的开发和投资，从特斯拉架构开始，包括对 Nvidia 著名的“统一计算设备架构”或“CUDA”的工作，该架构于 2007
    年引入。我们之前已经讨论过 Nvidia 的一些战略方面（第一个现在可免费阅读）：
- en: '* * *'
  id: totrans-split-41
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: '[Share](https://thechipletter.substack.com/p/demystifying-gpu-compute-architectures?utm_source=substack&utm_medium=email&utm_content=share&action=share)'
  id: totrans-split-42
  prefs: []
  type: TYPE_NORMAL
  zh: '[分享](https://thechipletter.substack.com/p/demystifying-gpu-compute-architectures?utm_source=substack&utm_medium=email&utm_content=share&action=share)'
- en: '**The rest of this post explains how general-purpose computing on modern GPUs
    works ‘from the bottom up’. We’ll start at the lowest level, with two major architectures,
    and then build up from there.**'
  id: totrans-split-43
  prefs: []
  type: TYPE_NORMAL
  zh: '**本文剩余部分将详细解释现代 GPU 上的通用计算是如何从底层逐步构建起来的。我们将从最低层级开始，介绍两种主要架构，然后逐步展开讨论。**'
- en: I’ve chosen to look at both Nvidia (Hopper used in the H100 GPU) and AMD’s CDNA
    2 (used in the Instinct MI200 GPU a version of which is used in [Frontier Supercomputer](https://en.wikipedia.org/wiki/Frontier_(supercomputer)))
    architectures and explain the common ground and the differences between these
    recent designs from each firm. We’ll focus particularly on clarifying some of
    the terminology used. We won’t consider the latest features built to accelerate
    machine learning, such as tensor cores. Instead, we’ll concentrate on the most
    ‘general purpose’ computing capabilities.
  id: totrans-split-44
  prefs: []
  type: TYPE_NORMAL
  zh: 我选择了同时观察 Nvidia（用于 H100 GPU 的 Hopper 架构）和 AMD 的 CDNA 2（用于 Instinct MI200 GPU
    的版本，其中的一个版本用于 [Frontier 超级计算机](https://en.wikipedia.org/wiki/Frontier_(supercomputer))），并解释了每家公司最新设计之间的共同点和差异。我们特别关注澄清使用的一些术语。我们不会考虑用于加速机器学习的最新功能，如张量核心。相反，我们将集中讨论最“通用”的计算能力。
- en: This is not a tutorial that will enable readers to get started writing programs
    on GPUs. Instead, if you want to do that, it’s intended as a supplement to help
    gain a better understanding of what is going on under the surface and the abstractions
    of a GPU program.
  id: totrans-split-45
  prefs: []
  type: TYPE_NORMAL
  zh: 这不是一个让读者能够开始在 GPU 上编写程序的教程。相反，如果你想做到这一点，它旨在作为一个补充，帮助更好地理解在表面下发生的事情和 GPU 程序的抽象。
- en: In any event, to properly understand a complex topic like how GPUs work, it’s
    really necessary to read lots of sources. To help with this, the end of this post
    has lots of references and links to further articles and presentations for readers
    who would like to learn more about GPU computing.
  id: totrans-split-46
  prefs: []
  type: TYPE_NORMAL
  zh: 无论如何，要正确理解像 GPU 如何工作这样复杂的主题，真的有必要阅读大量资料。为了帮助这一点，本文末尾提供了大量参考文献和链接，供希望了解更多 GPU
    计算的读者阅读。
- en: '* * *'
  id: totrans-split-47
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: We’ll start by looking at GPU compute hardware. We’ll group into three ‘Levels’
    - my terminology, in the absence of a common naming convention across architectures,
    and nothing to do with ‘Cache Levels’.
  id: totrans-split-48
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将从 GPU 计算硬件开始。我们将分成三个“级别” - 这是我自己的术语，因为不同架构之间没有共同的命名惯例，与“缓存级别”无关。
- en: We’ll start at the lowest level, with the basic building block of GPU architectures.
    Perhaps, surprisingly, this doesn’t seen to be given a memorable name by either
    Nvidia or AMD.
  id: totrans-split-49
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将从 GPU 架构的最低级别开始，介绍其基本构建模块。也许有些意外的是，这个名字在 Nvidia 或 AMD 那里似乎没有被赋予一个令人难忘的名字。
- en: 'It contains a collection of compute resources that can perform a range of operations
    : integer, 32-bit or 64-bit floating point arithmetic, data loads and stores from
    or to memory and more.'
  id: totrans-split-50
  prefs: []
  type: TYPE_NORMAL
  zh: 它包含了一系列计算资源，可以执行一系列操作：整数、32位或64位浮点运算、从内存加载和存储数据等等。
- en: We’ll start by focusing on Nvidia’s hardware. Let’s have a look at a schematic
    of what this basic building block looks like. Here is a diagram from Nvidia’s
    Hopper H100 Architecture White Paper.
  id: totrans-split-51
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将从Nvidia的硬件开始。让我们看一下这个基本构建块的示意图。这是Nvidia Hopper H100架构白皮书中的一个图表。
- en: The smallest boxes here with “INT32'“ or “FP32” or “FP64” represent the hardware
    or ‘execution units’ to perform single 32-bit integer, 32-bit floating point or
    64-bit floating point operations. So in this case we have thirty-two 32-bit floating
    point execution units, and sixteen each of the 32-bit integer and 64-bit float
    execution units.
  id: totrans-split-52
  prefs: []
  type: TYPE_NORMAL
  zh: 这里最小的方框标注着“INT32”、“FP32”或“FP64”，代表着执行单元或者单个32位整数、32位浮点或64位浮点操作。所以在这种情况下，我们有三十二个32位浮点执行单元，以及分别有十六个32位整数和64位浮点执行单元。
- en: As can be seen on the right there is also a ‘Tensor Core’ for the ‘tensor’ or
    ‘matrix’ operations required by many machine learning programs. We won’t discuss
    Tensor Cores further in this post.
  id: totrans-split-53
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 如右侧所示，还有一个“Tensor Core”用于许多机器学习程序所需的“张量”或“矩阵”运算。我们在本文中不会进一步讨论Tensor Core。
- en: At the bottom of this schematic are execution units (‘LD/ST’) to perform ‘loads’
    and ‘stores’ from and to memory.
  id: totrans-split-54
  prefs: []
  type: TYPE_NORMAL
  zh: 在此原理图的底部是执行单元（“LD/ST”），用于执行从内存到内存的“加载”和“存储”操作。
- en: We need to consider how these execution units are controlled. Let’s focus on
    the integer and floating point hardware. The first thing to emphasise is that
    multiple execution units are grouped together and all execute a single instruction
    at the same time, so that their operation as a group is similar to how ‘Single
    Instruction Multiple Data’ or ‘SIMD’ operations work on a CPU. In fact GPU makers
    will sometimes refer to their architectures as ‘SIMT’ for ‘Single Instruction
    Multiple Thread’.
  id: totrans-split-55
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要考虑如何控制这些执行单元。让我们关注整数和浮点硬件。首先要强调的是，多个执行单元被组合在一起，并且同时执行单条指令，因此它们作为一个组的操作类似于CPU上的“单指令多数据”或“SIMD”操作。事实上，GPU制造商有时会将他们的架构称为“SIMT”，即“单指令多线程”。
- en: An instruction might, for example, perform a single operation on, for example,
    a ‘vector’ (or array) of sixteen 64-bit floating point numbers or thirty-two 32-bit
    floating point numbers or thirty-two 32-bit integers.
  id: totrans-split-56
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，一条指令可能会在一个“向量”（或数组）上执行单个操作，例如十六个64位浮点数、三十二个32位浮点数或三十二个32位整数。
- en: There are some differences when compared to SIMD on a CPU though. Unlike a modern
    CPU though, there are lots of features, such as branch prediction or out-of-order
    execution, that these processors don’t implement. This makes them poor as ‘general
    purpose’ processors. However, avoiding the complexity of these features frees
    up lots of space to squeeze in more of these processors on a single silicon die.
  id: totrans-split-57
  prefs: []
  type: TYPE_NORMAL
  zh: 与CPU上的SIMD相比，这里也有一些不同之处。不像现代CPU那样，这些处理器不实现诸如分支预测或乱序执行等许多功能。这使它们作为“通用目的”处理器表现不佳。然而，避免这些功能的复杂性可以腾出大量空间，以在单个硅片上容纳更多的这些处理器。
- en: 'If they aren’t as efficient as CPUs at some things, they are much better at
    one task: switching between different programs. Without branch prediction, for
    example, programs running on this hardware are much more likely to have to wait
    for either program code or data to be fetched from memory. If this happens, then
    to keep the processor busy it can quickly switch to another program that is waiting
    and ready to take over. The ‘retired’ program can then be quickly restarted when
    it has the data that it needs.'
  id: totrans-split-58
  prefs: []
  type: TYPE_NORMAL
  zh: 如果它们在某些方面不如CPU高效，它们在一项任务上要好得多：在不同程序之间进行切换。例如，没有分支预测，运行在这种硬件上的程序更有可能需要等待程序代码或数据从内存中获取。如果发生这种情况，为了保持处理器的忙碌状态，它可以快速切换到另一个等待并准备接管的程序。当‘退役’程序获取到所需数据后，它可以快速重新启动。
- en: We need a name for each of the elements of this hardware that operates on one
    item of data out of the ‘Multiple Data’ that it’s presented with. We’ll call this
    a ‘SIMT Lane’, analogous to a lane in a motorway, with multiple lanes laid out
    in parallel to allow multiple streams of traffic to progress at the same time.
  id: totrans-split-59
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要为硬件中每个处理一个数据项的元素取一个名字。我们将其称为“SIMT Lane”，类似于高速公路上的车道，多条并行铺设的车道使多条流量能够同时前进。
- en: '* * *'
  id: totrans-split-60
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Let’s cover the remaining features of this basic building block of GPU architectures.
  id: totrans-split-61
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来介绍GPU架构的这个基本构建块的其余特性。
- en: There is some memory in the form of a ‘Register File’ and some ‘L0 Instruction
    Cache’ memory. We’ll come back to registers and memory on the GPU later on.
  id: totrans-split-62
  prefs: []
  type: TYPE_NORMAL
  zh: 存在一些‘Register File’和一些‘L0 Instruction Cache’内存。稍后我们会回顾 GPU 上的寄存器和内存。
- en: There is what is called a ‘Warp Scheduler’ and a ‘Dispatch Unit’. We’ll explain
    what a ‘Warp’ is precisely later on, but essentially this is hardware that organises
    the running of multiple programs on the execution units.
  id: totrans-split-63
  prefs: []
  type: TYPE_NORMAL
  zh: 还有所谓的‘Warp Scheduler’和‘Dispatch Unit’。稍后我们将详细解释‘Warp’是什么，但基本上这是一种组织在执行单元上运行多个程序的硬件。
- en: Finally, there is the ‘Special Function Units’ or ‘SFUs’ that can be seen at
    the bottom right. These execute transcendental instructions such as sin, cosine,
    reciprocal, and square root.
  id: totrans-split-64
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，还有‘Special Function Units’或‘SFUs’，可以看到它们位于右下角。这些执行诸如正弦、余弦、倒数和平方根等超越函数指令。
- en: '* * *'
  id: totrans-split-65
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: AMD’s CDNA 2 architecture hardware at this level is quite similar to Nvidia’s
    but with some differences. For example it only has sixteen 32-bit floating point
    execution units. In most respects it operates in a similar way though. We’re not
    going to look at it in more detail at this point.
  id: totrans-split-66
  prefs: []
  type: TYPE_NORMAL
  zh: AMD 的 CDNA 2 架构在这个层次上与 Nvidia 的相似，但也有一些不同之处。例如，它只有十六个 32 位浮点执行单元。在大多数方面，它的运作方式是类似的。在这一点上，我们不打算深入研究它。
- en: The next stage in building the GPU is to group a number of these ‘Level 1’ hardware
    units together. Both Nvidia and AMD group four of these together in what they
    call either a ’Streaming Multiprocessor’ (Nvidia) or ‘Compute Unit’ (AMD) respectively.
  id: totrans-split-67
  prefs: []
  type: TYPE_NORMAL
  zh: 构建 GPU 的下一个阶段是将若干这些‘Level 1’硬件单元组合在一起。Nvidia 和 AMD 各自将四个这样的单元组合在一起，称之为‘流处理多处理器’（Nvidia）或‘计算单元’（AMD）。
- en: Here is Nvidia’s schematic for a Streaming Multiprocessor. Nvidia also adds
    some ‘L1 Instruction Cache’ and ‘L1 Data Cache’. There is more on ‘shared memory’
    later. We can ignore the ‘Tensor Memory Accelerator’ and ‘Tex’.
  id: totrans-split-68
  prefs: []
  type: TYPE_NORMAL
  zh: 这是 Nvidia 的流处理多处理器示意图。Nvidia 还添加了一些‘L1 Instruction Cache’和‘L1 Data Cache’。稍后我们会详细讨论‘共享内存’。我们可以忽略‘Tensor
    Memory Accelerator’和‘Tex’。
- en: And here is a representation of AMD’s Compute Unit. AMD adds ‘Matrix Core Units’,
    more memory and a ‘Scalar Unit’, a scalar processor, which is distinct and runs
    independently of the other execution units.
  id: totrans-split-69
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是 AMD 计算单元的示意图。AMD 添加了‘Matrix Core Units’、更多内存和‘Scalar Unit’，这是一个独立运行的标量处理器。
- en: ‘Matrix Cores’ play a similar role to Nvidia’s Tensor Cores and are included
    to accelerate machine learning operations. We won’t discuss Matrix Cores further
    in this post.
  id: totrans-split-70
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ‘Matrix Cores’ 的作用类似于 Nvidia 的 Tensor Cores，用于加速机器学习操作。我们不会在本文中进一步讨论 Matrix
    Cores。
- en: Our final stage is to bring together multiple Streaming Multiprocessors (Nvidia)
    / Compute Units (AMD) together. The latest GPUs from the two architectures each
    bring together more than 100 Streaming Multiprocessors / Compute Units.
  id: totrans-split-71
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 我们的最终阶段是将多个流处理多处理器（Nvidia）/计算单元（AMD）组合在一起。来自两种架构的最新 GPU 每个都组合了超过 100 个流处理多处理器/计算单元。
- en: 'Here is a schematic representation of the Nvidia H100 with 144 Streaming Multiprocessors
    per GPU (click for an easier-to-read version of this image):'
  id: totrans-split-72
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 这里是 Nvidia H100 的示意图，每个 GPU 有 144 个流处理多处理器（点击以查看此图像的更易读版本）：
- en: And here is the AMD M200 with 112 Compute Units per GPU.
  id: totrans-split-73
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 这里是 AMD M200 的示意图，每个 GPU 有 112 个计算单元。
- en: In each case the Streaming Multiprocessors / Compute Units are accompanied by
    ‘L2 Cache’ and by hardware to facilitate fast communication with other parts of
    the system (including memory). This hardware may use HBM, PCIe or proprietary
    technology such as NVLink (Nvidia) or Infinity Fabric (AMD).
  id: totrans-split-74
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 在每种情况下，流处理多处理器/计算单元都配备了‘L2 Cache’和用于与系统其他部分（包括内存）快速通信的硬件。这些硬件可能使用 HBM、PCIe 或专有技术，如
    NVLink（Nvidia）或 Infinity Fabric（AMD）。
- en: '* * *'
  id: totrans-split-75
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '* * *'
- en: '**Let’s briefly step back and consider just how much computational hardware
    one of these GPUs packs when compared to even the most powerful modern CPUs.**'
  id: totrans-split-76
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**让我们简要地退后一步，考虑一下这些 GPU 在计算硬件方面与甚至最强大的现代 CPU 相比的差距。**'
- en: '**A leading AMD x86 CPU has 128 processor cores and has the floating-point
    execution units to perform (via AVX512 instructions) eight 32-bit floating-point
    calculations at a time. That’s 1,024 execution units per CPU. Contrast this with
    the 18,432 32-bit floating point execution units in Nvidia’s H100.**'
  id: totrans-split-77
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**领先的 AMD x86 CPU 拥有 128 个处理器核心，并且具有执行浮点运算单元（通过 AVX512 指令）以每次执行八个 32 位浮点运算的能力。这是每个
    CPU 1,024 个执行单元。与之形成对比的是 Nvidia H100 的 18,432 个 32 位浮点执行单元。**'
- en: After the break we’ll discuss GPU instruction sets and registers, memory and
    software.
  id: totrans-split-78
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 休息结束后，我们将讨论 GPU 指令集和寄存器、内存和软件。
