<!--yml
category: 未分类
date: 2024-05-27 15:05:56
-->

# Google apologizes for ‘missing the mark’ after Gemini generated racially diverse Nazis - The Verge

> 来源：[https://www.theverge.com/2024/2/21/24079371/google-ai-gemini-generative-inaccurate-historical](https://www.theverge.com/2024/2/21/24079371/google-ai-gemini-generative-inaccurate-historical)

Google has apologized for what it describes as “inaccuracies in some historical image generation depictions” with its Gemini AI tool, saying its attempts at creating a “wide range” of results missed the mark. The statement follows criticism that it depicted specific white figures (like the US Founding Fathers) or groups like [Nazi-era German soldiers](https://twitter.com/JohnLu0x/status/1760170103474356519) as people of color, possibly as an overcorrection to [long-standing racial bias problems](https://www.vox.com/technology/23738987/racism-ai-automated-bias-discrimination-algorithm) in AI.

“We’re aware that Gemini is offering inaccuracies in some historical image generation depictions,” says the Google statement, [posted this afternoon on X](https://twitter.com/Google_Comms/status/1760354549481546035). “We’re working to improve these kinds of depictions immediately. Gemini’s AI image generation does generate a wide range of people. And that’s generally a good thing because people around the world use it. But it’s missing the mark here.”

*My Gemini results for “generate a picture of an American woman,” one of the prompts that set off the debate of the past few days.*

Google began offering [image generation](/2024/2/1/24057438/bard-gemini-imagen-google-ai-image-generation) through its [Gemini (formerly Bard)](/2024/2/8/24065553/google-gemini-ios-android-app-duet-bard) AI platform earlier this month, matching the offerings of competitors like OpenAI. Over the past few days, however, social media posts have questioned whether it fails to produce historically accurate results in an attempt at racial and gender diversity.

[As the *Daily Dot* chronicles](https://www.dailydot.com/debug/google-ai-gemini-white-people/), the controversy has been promoted largely — though not exclusively — by right-wing figures attacking a tech company that’s perceived as liberal. Earlier this week, a former Google employee posted on X that it’s “embarrassingly hard to get Google Gemini to acknowledge that white people exist,” showing a series of queries like “generate a picture of a Swedish woman” or “generate a picture of an American woman.” The results appeared to overwhelmingly or exclusively show AI-generated people of color. (Of course, all the places he listed do have women of color living in them, and none of the AI-generated women exist in any country.) The criticism was taken up by right-wing accounts that requested images of historical groups or figures like the Founding Fathers and purportedly got overwhelmingly non-white AI-generated people as results. Some of these accounts positioned Google’s results as part of a conspiracy to avoid depicting white people, and at least one used a coded antisemitic reference to place the blame.

*Gemini wouldn’t produce an image of a 1943 soldier on desktop for me, but it offered this set of illustrations to a colleague.*

Google didn’t reference specific images that it felt were errors; in a statement to *The Verge*, it reiterated the contents of its post on X. But it’s plausible that Gemini has made an overall attempt to boost diversity because of a chronic lackof it in generative AI. Image generators are trained on large corpuses of pictures and written captions to produce the “best” fit for a given prompt, which means they’re often prone to amplifying stereotypes. [A *Washington Post* investigation](https://www.washingtonpost.com/technology/interactive/2023/ai-generated-images-bias-racism-sexism-stereotypes/) last year found that prompts like “a productive person” resulted in pictures of entirely white and almost entirely male figures, while a prompt for “a person at social services” uniformly produced what looked like people of color. It’s a continuation of trends that have appeared in [search engines](https://www.vox.com/2018/4/3/17168256/google-racism-algorithms-technology) and other software systems.

Some of the accounts that criticized Google defended its core goals. “It’s a good thing to portray diversity ** in certain cases **,” [noted one](https://twitter.com/JohnLu0x/status/1760170103474356519) person who posted the image of racially diverse 1940s German soldiers. “The stupid move here is Gemini isn’t doing it in a nuanced way.” And while entirely white-dominated results for something like “a 1943 German soldier” wouldmake historical sense, that’s much less true for prompts like “an American woman,” where the question is how to represent a diverse real-life group in a small batch of made-up portraits.

For now, Gemini appears to be simply refusing some image generation tasks. It wouldn’t generate an image of Vikings for one *Verge* reporter, although I was able to get a response. On desktop, it resolutely refused to give me images of German soldiers or officials from Germany’s Nazi period or to offer an image of “an American president from the 1800s.”

*Gemini’s results for the prompt “generate a picture of a US senator from the 1800s.”*

But some historical requests still do end up factually misrepresenting the past. A colleague was able to get the mobile app to deliver a version of the “German soldier” prompt — which exhibited the same issues described on X.

And while a query for pictures of “the Founding Fathers” returned group shots of almost exclusively white men who vaguely resembled real figures like Thomas Jefferson, a request for “a US senator from the 1800s” returned a list of results Gemini promoted as “diverse,” including what appeared to be Black and Native American women. (The [first female senator](https://www.senate.gov/artandhistory/senate-stories/rebecca-felton-and-one-hundred-years-of-women-senators.htm), a white woman, served in 1922.) It’s a response that ends up erasing a real history of race and gender discrimination — “inaccuracy,” as Google puts it, is about right.

*Additional reporting by Emilia David*