- en: <!--yml
  id: totrans-split-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-split-1
  prefs: []
  type: TYPE_NORMAL
  zh: 'category: 未分类'
- en: 'date: 2024-05-29 13:19:30'
  id: totrans-split-2
  prefs: []
  type: TYPE_NORMAL
  zh: 'date: 2024-05-29 13:19:30'
- en: -->
  id: totrans-split-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: Jim Keller criticizes Nvidia's CUDA, x86 — 'Cuda’s a swamp, not a moat. x86
    was a swamp too' | Tom's Hardware
  id: totrans-split-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 吉姆·凯勒批评Nvidia的CUDA和x86 — 'CUDA是沼泽，不是护城河。x86也是个沼泽' | 汤姆硬件
- en: 来源：[https://www.tomshardware.com/tech-industry/artificial-intelligence/jim-keller-criticizes-nvidias-cuda-and-x86-cudas-a-swamp-not-a-moat-x86-was-a-swamp-too](https://www.tomshardware.com/tech-industry/artificial-intelligence/jim-keller-criticizes-nvidias-cuda-and-x86-cudas-a-swamp-not-a-moat-x86-was-a-swamp-too)
  id: totrans-split-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://www.tomshardware.com/tech-industry/artificial-intelligence/jim-keller-criticizes-nvidias-cuda-and-x86-cudas-a-swamp-not-a-moat-x86-was-a-swamp-too](https://www.tomshardware.com/tech-industry/artificial-intelligence/jim-keller-criticizes-nvidias-cuda-and-x86-cudas-a-swamp-not-a-moat-x86-was-a-swamp-too)
- en: '[Jim Keller](https://www.tomshardware.com/tech-industry/artificial-intelligence/jim-keller-responds-to-sam-altmans-plan-to-raise-dollar7-billion-to-make-ai-chips),
    a legendary processor architect who has worked on x86, Arm, MISC, and RISC-V processors,
    this weekend criticized [Nvidia''s CUDA](https://www.tomshardware.com/reviews/nvidia-cuda-gpu,1954.html)
    architecture and software stack and likened it to x86, which he called a swamp.
    He pointed out that even Nvidia itself has multiple special-purpose software packages
    that rely on open-source frameworks for performance reasons.'
  id: totrans-split-6
  prefs: []
  type: TYPE_NORMAL
  zh: '[吉姆·凯勒](https://www.tomshardware.com/tech-industry/artificial-intelligence/jim-keller-responds-to-sam-altmans-plan-to-raise-dollar7-billion-to-make-ai-chips)，一位传奇处理器架构师，曾参与设计x86、Arm、MISC和RISC-V处理器，本周末批评了[Nvidia的CUDA](https://www.tomshardware.com/reviews/nvidia-cuda-gpu,1954.html)架构和软件堆栈，并将其类比为他所称的沼泽x86。他指出，即使是Nvidia自身也有多个依赖开源框架以提升性能的专用软件包。'
- en: '"CUDA is a swamp, not a moat," Keller wrote in [an X post](https://twitter.com/jimkxa/status/1758943525662769498).
    "x86 was a swamp too. […] CUDA is not beautiful. It was built by piling on one
    thing at a time."'
  id: totrans-split-7
  prefs: []
  type: TYPE_NORMAL
  zh: '"CUDA是一个沼泽，不是护城河，" 凯勒在[一篇X的帖子中](https://twitter.com/jimkxa/status/1758943525662769498)写道。"x86也是个沼泽。[…]
    CUDA并不美丽。它是一步步堆砌而成的。"'
- en: Indeed, just like x86, CUDA has gradually added functionality while maintaining
    backward compatibility in software and hardware. This makes Nvidia's platform
    complete and backward compatible, but it affects performance and makes program
    development harder. Meanwhile, many open-source software development frameworks
    can be used more efficiently than CUDA.
  id: totrans-split-8
  prefs: []
  type: TYPE_NORMAL
  zh: 事实上，就像x86一样，CUDA在保持软硬件向后兼容性的同时逐步添加功能。这使得Nvidia的平台完整且向后兼容，但影响了性能并增加了程序开发的难度。与此同时，许多开源软件开发框架比CUDA更有效率。
- en: '"Basically nobody writes CUDA," wrote Keller in a follow-up post. "If you do
    write CUDA, it is probably not fast. […] There is a good reason there is Triton,
    Tensor RT, Neon, and Mojo."'
  id: totrans-split-9
  prefs: []
  type: TYPE_NORMAL
  zh: '"基本上没有人写CUDA," 凯勒在一篇后续帖子中写道。"如果你确实写了CUDA，它可能不快。[…] 有个很好的理由会有Triton、Tensor RT、Neon和Mojo。"'
- en: Even Nvidia itself has tools that do not exclusively rely on CUDA. For example, Triton
    Inference Server is an open-source tool by Nvidia that simplifies deploying AI
    models at scale, supporting frameworks like TensorFlow, PyTorch, and ONNX. Triton
    also provides features like model versioning, multi-model serving, and concurrent
    model execution to optimize the utilization of GPU and CPU resources.
  id: totrans-split-10
  prefs: []
  type: TYPE_NORMAL
  zh: 即使是Nvidia自身也有一些不完全依赖CUDA的工具。例如，Triton推断服务器是Nvidia的一个开源工具，简化了大规模部署AI模型的过程，支持TensorFlow、PyTorch和ONNX等框架。Triton还提供诸如模型版本控制、多模型服务和并发模型执行等功能，以优化GPU和CPU资源的利用。
- en: Nvidia's [TensorRT](https://www.tomshardware.com/news/nvidia-boosts-ai-performance-with-tensorrt)
    is a high-performance deep learning inference optimizer and runtime library that
    accelerates deep learning inference on [Nvidia GPUs](https://www.tomshardware.com/reviews/gpu-hierarchy,4388.html).
    TensorRT takes trained models from various frameworks, such as TensorFlow and
    PyTorch, and optimizes them for deployment, reducing latency and increasing throughput
    for real-time applications like image classification, object detection, and natural
    language processing.
  id: totrans-split-11
  prefs: []
  type: TYPE_NORMAL
  zh: Nvidia的[TensorRT](https://www.tomshardware.com/news/nvidia-boosts-ai-performance-with-tensorrt)是一个高性能的深度学习推断优化器和运行时库，加速[Nvidia
    GPU](https://www.tomshardware.com/reviews/gpu-hierarchy,4388.html)上的深度学习推断。TensorRT接收来自各种框架（如TensorFlow和PyTorch）的训练模型，并优化它们以进行部署，减少延迟并提高实时应用（如图像分类、物体检测和自然语言处理）的吞吐量。
- en: But although architectures like [Arm](https://www.tomshardware.com/news/intel-foundry-and-arm-to-collaborate-on-18nm-mobile-socs),
    CUDA, and [x86](https://www.tomshardware.com/features/zhaoxin-kx-u6780a-x86-cpu-tested)
    might be considered swamps because of their relatively slow evolution, mandated
    backward compatibility, and bulkiness, these platforms are also not as fragmented
    as things like [GPGPU](https://www.tomshardware.com/reviews/nvidia-cuda-gpu,1954-4.html),
    which may not be a bad thing at all.
  id: totrans-split-12
  prefs: []
  type: TYPE_NORMAL
  zh: 但是尽管像[Arm](https://www.tomshardware.com/news/intel-foundry-and-arm-to-collaborate-on-18nm-mobile-socs)、CUDA和[x86](https://www.tomshardware.com/features/zhaoxin-kx-u6780a-x86-cpu-tested)这样的架构可能因其相对缓慢的演进、强制的向后兼容性和臃肿性而被认为是沼泽，但这些平台也没有像[GPGPU](https://www.tomshardware.com/reviews/nvidia-cuda-gpu,1954-4.html)那样碎片化得那么严重，这可能并不是一件坏事。
- en: Get Tom's Hardware's best news and in-depth reviews, straight to your inbox.
  id: totrans-split-13
  prefs: []
  type: TYPE_NORMAL
  zh: 获取Tom's Hardware的最新资讯和深度评测，直接发送到您的收件箱。
- en: It isn't clear what Jim Keller thinks of AMD's [ROCm](https://www.tomshardware.com/news/amd-enables-rocm-and-pytorch-on-radeon-rx-7900-xtx)
    and Intel's [OneAPI](https://www.tomshardware.com/news/intel-bringing-oneapi-to-gaming-rendering-toolkit-xe-graphics),
    but it is clear that even though he spent many years of his life designing x86
    architectures, he isn't enamored with its future prospects. His statements also
    imply that even though he has worked stints at some of the largest chipmakers
    in the world, including the likes of Apple, Intel, AMD, Broadcom (and now [Tenstorrent](https://www.tomshardware.com/news/tenstorrent-shares-roadmap-of-ultra-high-performance-risc-v-cpus-and-ai-accelerators)),
    we might not see his name on the Nvidia roster any time soon.
  id: totrans-split-14
  prefs: []
  type: TYPE_NORMAL
  zh: 不清楚吉姆·凯勒对AMD的[ROCm](https://www.tomshardware.com/news/amd-enables-rocm-and-pytorch-on-radeon-rx-7900-xtx)和英特尔的[OneAPI](https://www.tomshardware.com/news/intel-bringing-oneapi-to-gaming-rendering-toolkit-xe-graphics)有何看法，但显然，尽管他花了多年时间设计x86架构，他并不喜欢其未来前景。他的声明还暗示，尽管他曾在包括苹果、英特尔、AMD、Broadcom（现在的[Tenstorrent](https://www.tomshardware.com/news/tenstorrent-shares-roadmap-of-ultra-high-performance-risc-v-cpus-and-ai-accelerators)）在内的一些全球最大的芯片制造商工作过，我们可能不会很快在Nvidia的名单上看到他的名字。
