<!--yml

category: 未分类

date: 2024-05-27 14:37:52

-->

# 从我们在生产中使用 Kubernetes 的 8 年经验中学到的教训 —— 两次主要集群崩溃，放弃自管理，降低集群成本，工具化，以及更多 | 作者：Anders Jönsson | Medium

> 来源：[https://medium.com/@.anders/learnings-from-our-8-years-of-kubernetes-in-production-two-major-cluster-crashes-ditching-self-0257c09d36cd](https://medium.com/@.anders/learnings-from-our-8-years-of-kubernetes-in-production-two-major-cluster-crashes-ditching-self-0257c09d36cd)

# 从我们在生产中使用 Kubernetes 的 8 年经验中学到的教训 —— 两次主要集群崩溃，放弃自管理，降低集群成本，工具化，以及更多

在 Urb-it 的早期，我加入之前，我们决定将 Kubernetes 作为云原生战略的核心。这一选择的背后是我们预期的快速扩展，以及利用容器编排能力为我们的应用程序提供更动态、更弹性和更高效的环境。而且，考虑到我们的微服务架构，Kubernetes 很好地适配了我们的需求。

# 早期决策

我们早早地做出了这个决策，当然，这个决策应该受到质疑，因为它代表了对一个初创公司（或者任何公司）的重大依赖和大量的知识负担。此外，在那个阶段，我们是否真正面临 Kubernetes 可以解决的问题？有人可能会认为，最初我们可以选择一个规模较大的单体应用，并依靠它直到扩展和其他问题变得痛苦时，再考虑迁移到 Kubernetes（或其他方案）。此外，Kubernetes 当时仍处于早期开发阶段。但是，让我们另外深入探讨这个问题。

# **8 Years In Production**

在生产环境中运行 Kubernetes 已经超过八年的时间（每个环境单独一个集群），我们做出了一些好的和不那么好的决策。有些错误仅仅是“otur när vi tänkte”（我们决策时的不好运气）的结果，而另一些则源于我们并没有完全（甚至根本没有）理解这项技术本身。Kubernetes 强大，但也非常复杂。

> 我们毫无经验地迎头而上，试图在规模上运行它。

# **从AWS自管理迁移到Azure托管（AKS）**

在最初的几年里，我们在AWS上运行了一个自管理的集群。如果我没记错的话，最开始我们并没有选择使用 [Azure Kubernetes Service (AKS)](https://azure.microsoft.com/en-us/products/kubernetes-service)，[Google Kubernetes Engine (GKE)](https://cloud.google.com/kubernetes-engine)，[Amazon Elastic Kubernetes Service (EKS)](https://aws.amazon.com/eks/)，因为它们还没有提供官方的托管解决方案。我们的第一个也是最严重的集群崩溃发生在 [Amazon Web Services (AWS)](https://aws.amazon.com/) 的自托管环境中，稍后我们会详细讲述这件事情。

由于我们是一个小团队，跟上我们所需的所有新能力是具有挑战性的。同时，管理自托管集群需要不断的关注和维护，这增加了我们的工作量。

当托管解决方案普遍可用时，我们花了一些时间评估[Azure Kubernetes 服务 (AKS)](https://azure.microsoft.com/en-us/products/kubernetes-service)、[Google Kubernetes Engine (GKE)](https://cloud.google.com/kubernetes-engine)和[Amazon EKS](https://aws.amazon.com/eks/)。对于我们来说，它们都比自己管理好几倍，并且我们可以很容易地看到通过迁移带来的快速投资回报率。

当时，我们的平台是50% 的 .Net 和 50% 的 Python，我们已经在使用 Azure Service Bus、Azure SQL Server 和其他 Azure 服务。因此，将我们的集群迁移到 Azure 不仅可以更轻松地集成使用它们，还可以通过利用 Azure 主干网络基础设施来使我们受益，避免与离开/进入外部网络和 VNETs 相关的成本，这些成本是我们在混合 AWS 和 Azure 设置中存在的。此外，我们的许多工程师都熟悉 Azure 及其生态系统。

我们还应该提到，对于我们在 AKS 上的初始设置，我们不必为控制平面节点（主节点）付费 — 这是一个额外的好处（节省节点费用）。

*我们在2018年冬天迁移了，尽管多年来我们遇到了一些 AKS 的问题，但我们从未后悔过这次迁移。*

# **集群崩溃 #1**

在 AWS 上自我管理期间，我们遇到了一个严重的集群崩溃，导致我们大多数系统和产品宕机。根 CA 证书、etcd 证书和 API 服务器证书过期，导致集群停止工作并阻止我们对其进行管理。那时，在 kube-aws 中解决这个问题的支持是有限的。我们请来了一位专家，但最终我们不得不从头开始重建整个集群。

我们以为所有的值和 Helm charts 都在每个 git 仓库里，但是，出乎意料的是，并非所有服务都是这样。更重要的是，用于创建集群的所有配置都没有存储。这变成了一场与时间赛跑，要重新设置集群并填充我们拥有的所有服务和产品。其中一些需要重新发明 Helm charts 来创建缺失的配置。像 Dev1 对 Dev2 说：“你还记得这个服务应该有多少 CPU 或内存，或者它应该有什么网络和端口访问权限吗？”。更不用说所有随风而逝的秘密了。

> 我们花了几天时间让它重新运行起来。这绝不是我们最自豪的时刻。

多亏了我们的积极沟通，通过保持透明、诚实和培养我们的关系，我们没有失去任何业务或客户。

# **集群崩溃 #2**

现在你可能会说：第二次崩溃不可能是因为证书问题，因为你应该从第一次崩溃中吸取了教训，对吧？是和否。重新创建从第一次崩溃中的集群时，不幸的是，我们使用的 kube-aws 版本具有一个 [问题](https://github.com/kubernetes-retired/kube-aws/issues/892)。当它创建新集群时，它没有将 etcd 证书的过期日期设置为提供的到期日期；它默认为一年。因此，在第一次集群崩溃后的一年，证书过期了，我们经历了另一次集群崩溃。然而，这次更容易恢复；我们不必重新构建一切。但仍然是一个糟糕的周末。

*附注 1：其他公司也受到了这个* [*bug*](https://github.com/kubernetes-incubator/kube-aws/issues/892) *的影响，就像我们一样，这并没有帮助我们的客户……*

*附注 2：我们的计划是在一年后更新所有证书，但为了给自己留有余地，我们将到期时间设置为两年（如果我记得没错的话）。因此，我们有更新证书的计划，但是 bug 比我们还快一步。*

> *自 2018 年以来，我们再也没有遇到过集群崩溃…… 这是在自吹自擂吗？是的。*

# 学到的东西

+   **Kubernetes Is Complex** 您需要对 Kubernetes 基础设施和运维方面感兴趣并愿意从事相关工作的工程师。在我们的情况下，除了正常职责外，我们需要几名工程师随时担任 Kubernetes 的“问题专家”。Kubernetes 特定任务的工作量各不相同，这一点您可能已经可以想象到了。有些周几乎没有什么要做的，而在其他时候，例如在集群升级期间，需要更多的关注。

    对我们来说，不可能轮换和分配整个团队的工作；技术过于复杂，无法每两周“跳进和跳出”。当然，每个人都需要知道如何使用它（部署、调试等） — 但要在更具挑战性的方面出色，需要专门的时间。此外，重要的是要有人以远见领导，并制定演化集群的策略。

+   **Kubernetes 证书**

    经历了两次集群崩溃，都是因为证书过期，深入了解 Kubernetes 内部证书及其过期日期的细节至关重要。

+   **保持 Kubernetes 和 Helm 的最新状态** 落后一点就变得既昂贵又乏味。我们总是等待几个月才跳到最新版本，以确保其他人先面对任何新版本的问题。但即使保持更新，由于 Kubernetes 和 Helm 的新版本（例如 Kubernetes API 从 alpha 到 beta，beta 到 1.0 等），我们仍然面临许多耗时的配置文件和图表重写。我知道 Simon 和 Martin 都喜欢所有的 Ingress 更改。

+   **集中管理的 Helm 图表**

    当涉及到Helm图表时，我们厌倦了为每个版本更改更新所有70个以上的图表，因此我们采用了更通用的*“一张图表统治它们所有”*的方法。集中式Helm图表方法有许多利弊，但最终，这更符合我们的需求。

+   **灾难恢复计划**

    我再也不能强调这一点了：务必确保有方法可以在需要时重新创建集群。是的，你可以在用户界面中点击来创建新的集群，但这种方法永远不适用于大规模或及时地工作。

    有不同的方法来处理这个问题，从简单的shell脚本到更高级的方法，如使用Terraform（或类似的）。Crossplane还可以用来管理基础设施即代码（IaC）等。

    对于我们来说，由于团队的带宽有限，我们决定存储和使用shell脚本。

    **无论你选择的方法如何，请务必定期测试流程，以确保在需要时可以重新创建集群。**

+   **备份密钥**

    有一个备份和存储密钥的策略。如果你的集群消失了，你所有的密钥也会消失。相信我，我们亲身经历过这一点；当你拥有多个不同的微服务和外部依赖时，要再次正确地获取所有内容需要花费很多时间。

+   **与供应商无关与“全面采用”**

    刚开始时，我们在转移到AKS后，试图使我们的集群与供应商无关，意味着我们将继续使用其他服务进行容器注册表、认证、密钥保管库等功能。我们的想法是，我们将来可以轻松地转移到其他的托管解决方案。尽管保持与供应商无关是一个很好的想法，但对我们来说，它带来了很高的机会成本。过了一段时间，我们决定全面使用与AKS相关的Azure产品，比如容器注册表、安全扫描、认证等。对我们来说，这带来了更好的开发体验，简化的安全性（与Azure Entra Id集中式访问管理）等，从而导致了更快的上市时间和降低成本（量产效益）。

+   **客户资源定义**是的，我们全面使用了Azure产品，但我们的指导要点是尽可能少地使用自定义资源定义，而是使用内置的Kubernetes资源。然而，我们也有一些例外，比如Traefik，因为Ingress API并未满足我们所有的需求。

+   **安全**见下文。

+   **可观察性**

    见下文。

+   **已知高峰期间的预扩展**即使使用了自动扩展器，我们有时也扩展得太慢。通过使用流量数据和常识（我们是一个物流公司，节日期间有高峰），我们手动扩展了集群（ReplicaSet）前一天到达高峰之前，然后在高峰后一天将其缩减（慢慢地来应对可能发生的第二波高峰）。

+   **集群内的飞行器**

    我们将构建系统[Drone](https://www.drone.io/)保留在舞台集群中；它有一些好处，但也有一些缺点。由于它在同一集群中，很容易进行扩展和使用。然而，当同时构建太多时，它几乎消耗了所有资源，导致Kubernetes急于启动新节点。最好的解决方案可能是将其作为纯SaaS解决方案，无需担心托管和维护产品本身。

+   **选择正确的节点类型** 这非常依赖于具体的上下文，但根据节点类型不同，AKS会为可用内存的大约~10-30%预留（用于内部AKS服务）。因此，对我们来说，使用较少但更大的节点类型是有益的。另外，由于我们在许多服务上运行.Net，我们需要选择具有高效和可靠IO的节点类型。（.Net经常对JIT和日志进行磁盘写入，如果这需要网络访问，则会变慢。我们还确保节点磁盘/缓存至少与总配置的节点磁盘大小相同，以避免需要进行网络跳跃。）

+   **保留实例**

    你可以争论说这种方法在某种程度上违背了云的灵活性，但对我们来说，将关键实例保留一年或两年实现了巨大的成本节约。在许多情况下，与“按需付费”方式相比，我们可以节省50-60%的费用。是的，这对团队来说是丰厚的蛋糕。

+   **k9s**

    [https://k9scli.io/](https://k9scli.io/) 是一个很棒的工具，适合任何希望比纯*kubectl*更高抽象层次的人。

# **可观测性**

## **监控**

确保随时间跟踪内存、CPU等的使用情况，以便观察您的集群的性能如何，并确定新功能是否改善或恶化了其性能。通过这样做，更容易找到并设置不同Pod的“正确”限制（找到正确的平衡非常重要，因为如果Pod内存不足时会被杀死）。

## **警报**

优化我们的警报系统是一个过程，但最终，我们将所有警报都指向了我们的Slack频道。这种方法使得在集群出现预期外问题或发生任何意外问题时接收通知变得非常方便。

## **日志**

将所有日志集中在一个地方，并配备健壮的跟踪ID策略（例如[OpenTelemetry](https://opentelemetry.io/)或类似的），对于任何微服务架构都至关重要。我们花了2-3年时间才做到这一点。如果我们早些实施它，将会节省大量时间。

# **安全性**

Kubernetes中的安全性是一个广阔的主题，我强烈建议彻底研究以理解所有细微差别（例如参见[NSA，CISA发布的Kubernetes加固指南](https://www.nsa.gov/Press-Room/News-Highlights/Article/Article/2716980/nsa-cisa-release-kubernetes-hardening-guidance/)）。以下是我们经验中的一些关键点，但请注意，这绝不是挑战的完整图景。

## 访问控制

简而言之，Kubernetes默认情况下并不过于严格。因此，我们花了大量时间 tightening access，为pod和容器实施最小权限原则。此外，由于特定漏洞，未授权的攻击者有可能将其权限提升至root，绕过Linux命名空间限制，并且在某些情况下甚至逃逸容器以获取主机节点的root访问权限。这是非常不好的。

您应该设置只读根文件系统，禁用服务账户令牌自动挂载，禁用特权升级，放弃所有不必要的能力，等等。在我们特定的设置中，我们使用Azure Policy和Gatekeeper来确保我们没有部署不安全的容器。

在我们的AKS Kubernetes设置中，我们利用了基于角色的访问控制（RBAC）的强大性能，进一步增强了安全性和访问管理。

## 容器漏洞

有许多优秀的工具可以扫描和验证Kubernetes中的容器和其他部分。我们使用Azure Defender和Azure Defender for Containers来满足部分需求。

*注：与其在寻找完美的工具时“*[*分析麻痹*](https://en.wikipedia.org/wiki/Analysis_paralysis)*”，试图找到带有所有“花哨功能”的工具，不如选择一个，让学习开始吧。*

# **我们多年来的设置**

+   **部署** 与许多其他人一样，我们使用Helm来管理和简化我们在Kubernetes上的应用部署和打包。自从很久以前开始使用Helm以来，我们最初混合了.Net/Go/Java/Python/PHP，我们重写了Helm图表的次数多到我不敢记数。

+   **可观测性** 我们最开始使用[Loggly](https://www.loggly.com/)与[FluentD](https://www.fluentd.org/)进行集中日志管理，但几年后，我们转向了Elastic和Kibana（ELK堆栈）。对我们来说，使用Elastic和Kibana更容易，因为它们被广泛使用，并且在我们的设置中更加经济实惠。

+   **容器注册表** 我们最初使用[Quay](https://quay.io/)，这是一个不错的产品。但随着迁移到Azure，自然而然地开始使用[Azure Container Registry](https://azure.microsoft.com/en-us/products/container-registry)，因为它集成度高，对我们而言更“本地化”。（我们也因此将容器纳入Azure安全顾问的管理）。

+   **流水线** 从一开始，我们就使用[Drone](https://www.drone.io/)构建我们的容器。当我们开始时，没有多少CI系统支持容器和Docker，也没有提供配置作为代码的选项。多年来，Drone一直为我们提供良好的服务。在Harness收购它后变得有些混乱，但是在我们妥协并迁移到高级版本之后，我们得到了所有所需的功能。

# **游戏改变者**

在过去几年中，Kubernetes对我们来说是一个**革命性的变革者**。它解锁了能力，使我们能够更高效地**扩展（**波动的流量量**）**，优化我们的基础设施成本，改进我们的开发者体验，使测试新思路更加容易，从而显著缩短新产品和服务的上市时间/投产时间。

我们在早期就开始使用Kubernetes，可能比我们真正面临它可以解决的问题要早一些。但从长远来看，特别是在最近几年，**它已被证明为我们带来了巨大的价值。**

# **最后的话**

回顾八年的经验，有许多故事可以分享，很多已经在记忆中淡去。希望您喜欢了解我们的设置，我们犯过的错误，以及我们在这条路上学到的教训。

谢谢您的阅读。
