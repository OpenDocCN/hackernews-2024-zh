<!--yml

category: 未分类

date: 2024-05-27 14:47:29

-->

# 对于提议的加利福尼亚SB 1047 - 由Zvi Mowshowitz所述

> 来源：[https://thezvi.substack.com/p/on-the-proposed-california-sb-1047](https://thezvi.substack.com/p/on-the-proposed-california-sb-1047)

[旧金山的加利福尼亚参议员斯科特·维纳](https://twitter.com/Scott_Wiener/status/1755650108287578585)介绍了[SB 1047](https://t.co/JWaOLP44Iu) [来监管人工智能](https://www.washingtonpost.com/technology/2024/02/08/california-legislation-artificial-intelligence-regulation/)。我已经在[市场上提出了关于其成为法律的可能性](https://manifold.markets/ZviMowshowitz/will-california-bill-sb-1047-become)。

> “如果国会某时能够通过一项强有力的支持创新、保障人工智能安全的法律，我将会第一个为此欢呼，但我不抱太大希望，”维纳在一次采访中说道。“我们需要走在前面，以保持公众对人工智能的信任。”

国会显然高度dysfunctional。我仍然普遍反对加利福尼亚试图表现得像联邦政府一样，即使事业是好的，但我理解。

加利福尼亚能否有效地在这里施加其意愿？

在当前最大的参与者中，目前很可能是这样。

长期来看，当情况变得活跃危险时，我的假设是不。

这里有一个潜在的陷阱。如果我们把我们的规则放在一个足够有上升空间的人可以无视它们的地方，而我们从未在国会通过任何东西。

那么，根据法案的作者，它是怎么做的呢？

> 加利福尼亚参议员斯科特·维纳：SB 1047有几个作用：
> 
> 1.  为开发最大和最强大人工智能系统制定明确、可预测、常识性的安全标准。这些标准仅适用于最大的模型，不适用于初创企业。
> 1.  
> 1.  成立CalCompute，一个公共人工智能云计算集群。CalCompute将是研究人员、初创企业和社区团体推动加利福尼亚创新、提供多样化视角参与人工智能开发并确保我们在人工智能领域持续领先的资源。
> 1.  
> 1.  防止价格歧视和反竞争行为
> 1.  
> 1.  建立了了解客户的要求
> 1.  
> 1.  保护大型人工智能公司的告密者
> 1.  
> @geoffreyhinton称SB 1047是“平衡这些需求的非常明智的方法”。代表广泛人工智能社区的领导人表示支持。
> 
> 人们理所当然地担心人工智能模型的巨大力量可能带来严重风险。为了这些模型能够成功达到我们需要的方式，用户必须相信人工智能模型是安全的，并与核心价值观一致。履行基本的安全职责是一个很好的起点。
> 
> 通过人工智能，我们有机会应用过去二十年来学到的艰难经验。在理解风险之前，允许社交媒体无约束地增长已经带来了灾难性后果，这一次我们应该采取合理的预防措施。

一如既往，RTFC（阅读卡片，或在[这里查看法案](https://leginfo.legislature.ca.gov/faces/billNavClient.xhtml?bill_id=202320240SB1047)）适用。

第 1 节命名了该法案。

第 2 节说加利福尼亚州在人工智能方面处于领先地位（[请参见这首歌](https://www.youtube.com/watch?v=BATf_eUcb8M&ab_channel=ThePresidentsoftheUnitedStatesofAmerica)），人工智能有很大的潜力，但可能会造成伤害。错失了提及存在风险的机会。

第 3 节 22602 提供了定义。我有一些笔记。

1.  对AI广泛定义的通常关注点。

1.  奇怪的是，“一个模型在未经用户请求的情况下自主进行持续序列的不安全行为”只有在“用户的请求之外”才算作“AI安全事件”。如果用户请求了，难道不应该确保模型不会这样做吗？听起来对我来说像是一个安全事件。

1.  覆盖的模型主要通过计算来定义，不确定为什么这不是一个“基础”模型，我喜欢次要扩展条款：“2024 年使用超过 10^26 个整数或浮点操作进行训练的人工智能模型，或者在常用于量化最先进基础模型性能的基准测试中合理预期具有类似性能的模型，如行业最佳实践和相关标准制定组织所确定的 OR 人工智能模型在特定基准测试上具有相关阈值以下的能力，但在其他方面具有类似的一般能力。”

1.  严重伤害要么是大规模伤亡，要么是 5 亿美元的损害，或者可比的情况。

1.  完全关闭意味着完全关闭，但仅在您的控制和控制范围内。所以当我们真正需要完全关闭时，这个定义就行不通了。关闭的整个意义在于，无论您是否控制它，它都会发生。

1.  开源人工智能模型的定义仅包括“可能被自由修改和重新分发”的模型，因此引发了这样一个问题，即这是否合法或实际可行。这样的定义需要实际可行，如果我可以非法做到但显然仍然可以做到，那就需要计入。

1.  定义（s）：“积极的安全判定”意味着根据第 22603 条的分项（a）或（c），针对一个非派生模型的覆盖模型的确定，开发者可以合理地排除覆盖模型具有危险能力或在考虑到合理安全保障和后期培训修改的可能性时可能接近拥有危险能力的可能性。

    1.  非常高兴看到后期修改的提及，稍后还指出包括访问工具和数据，因此支架明确计入。

第 3 节 22603 (a) 规定，在训练新的非派生模型之前，您需要确定是否可以做出积极的安全判定。

我喜欢这在您开始训练之前发生。但是，这当然引发了一个问题，即您如何知道它将在基准测试中得分如何？

我担心的一件事是，如果你在各种基准测试中得分低于另一个模型，那么这算作积极的安全认定。至少有四种显而易见的失败模式。

1.  开发者可能选择通过排除相关数据和训练或其他方式来破坏基准测试的性能。或者，之前的开发者可能已经操纵了基准测试，这种情况经常发生，因此你只需不自己操纵这些基准测试就能得分更低。

1.  模型可能具有情境感知，并选择获得较低的分数。这可能是开发者有意为之的各种程度。

1.  模型可能不会遵循你的预测或缩放定律。所以也许你说它在基准测试上得分较低，但谁能说你是对的？

1.  这些基准可能根本不能很好地衡量我们关心的内容。

同样，最好在开始训练之前做出安全认定，但如果模型值得训练，那么你很可能实际上无法事先知道它的安全性，尤其是因为这不仅涉及生存安全。

第3 22603（b）节涵盖了如果你无法作出积极的安全认定时你必须做的事情。以下是主要规定：

1.  你必须防止未经授权的访问。

1.  你必须能够进行全面关闭。

1.  你必须实施所有涵盖的指导。好啦，好啦。

1.  你必须制定书面和单独的安全和安全协议，提供‘合理保证’，确保模型将具有防止关键危害的保障。这必须包括明确的测试，以验证你是否成功。

1.  你必须说明你打算如何做到这一切，你会如何改变你的做法，并且是什么将触发关闭。

1.  提供你的协议的副本并保持其更新。

在训练和测试后，你可以做出‘积极的安全认定’，但必须遵守安全协议。

第（d）节规定，如果你的模型‘不受积极安全认定的约束’，为了部署它（你究竟能否部署它？！），你需要实施‘合理的保障和要求’，使你能够防止危害并追溯任何发生的危害。我担心这一部分没有认真对待这样的情景。为了不受此类认定的约束，模型需要在能力上开辟新的领域，并且你无法保证它不会危险。那么，什么是那些‘合理的保障和要求’，使得部署它是可以接受的？也许我在这里有误解。

第（g）节规定必须报告安全事故。

第（h）节说，如果你的积极安全认定是不合理的，那么它就不算数，并且为了合理，你需要考虑已在其他地方识别的任何风险。

总体来看，这似乎是一个很好的开端，但我担心它存在漏洞，并且我担心它没有考虑到模型可能存在潜在的存在性危险，或可能表现出意外的能力或情境意识等。整个法案仍然采用了华盛顿特区风格的‘预期和检查特定危害’方法。

Section 22604 是关于 KYC 的，大型计算集群必须收集信息并检查客户是否试图训练一个涵盖模型。

Section 22605 要求推理或计算集群的销售者提供透明、统一、公开的价格计划，禁止价格歧视，并禁止在确定价格或获取途径上进行‘非法歧视或非竞争活动’。

我总是想知道那些说‘你不能做已经非法的事情’的法律，我以为这正是它们已经非法的全部意义所在。

我不确定这条规则在实践中有多大影响力，以及它是否有效地意味着任何销售这类服务的人都必须成为一种无法挑选谁获得其有限服务的普通运营者，并且不能进行任何形式的交易。我理解其吸引力，但同时也看到迫使此举可能带来的明显经济下降。

Section 22606 涵盖了处罚事项。罚款的范围相对有限，主要的救济是禁令以及可能删除模型。我担心实际操作中这里的执行力度不够。

Section 2207 是举报者保护条款。奇怪的是，这是必要的，人们可能会认为现在普遍应该有这种保护措施？这里没有意外强烈的条款，只有正常的内容。

Section 4 11547.6 赋予新的 Frontier Model Division 其官方业务，包括收集报告和发布指导意见。

Section 5 11547.7 是关于 CalCompute 公共云计算集群的。这看起来像是一个糟糕的想法，这里没有公众参与的理由，而且也没有说明或分配预算。假设它很小，那就不太重要了。

Sections 6-9 是标准的免责声明和规则。

我们应该如何看待所有这些？

它看起来像是一个诚意满满的努力，提出一个有帮助的法案。它里面有很多好主意。我相信这将是有益的。特别是，它的结构使得如果你的模型不接近前沿，那么你在这里的负担将会很小。

我担心这在各个地方都有潜在的漏洞，并且还没有强力解决未来更多存在性威胁的性质。如果你想无视这个法律，你很可能可以做到。

但这似乎是一个好的开始，特别是在处理相对乏味但仍然潜在灾难性威胁方面，而不会给开发者造成不必要的负担。这可以进一步发展。

啊，泰勒·考恩在这个链接上有一个……[加利福尼亚对人工智能的束缚](https://hyperdimensional.substack.com/p/californias-effort-to-strangle-ai?utm_source=post-email-title&publication_id=2244049&post_id=141516001&utm_campaign=email-post-title&isFreemail=false&r=3j06n&utm_medium=email)。

当然了。我们每次都这样做。人们不停地说，“这项法律将禁止讽刺”或电子表格或可爱小狗的图片或其他什么，基于如果它在最佳情况下直接实施且每个人都严格执行的话，那么这个提案将成为极端反现实主义者的最大主义阅读。

> 代表：本周，加利福尼亚州立法机构引入了[SB 1047：前沿人工智能系统安全创新和安全法](https://leginfo.legislature.ca.gov/faces/billTextClient.xhtml?bill_id=202320240SB1047)。该法案由国家参议员斯科特·维纳（许多人包括我在内，因为他支持住房）提出，将为人工智能创建一个全面的监管制度，将预防原则应用于所有人工智能开发，并有效地禁止所有新的开源人工智能模型，可能覆盖整个美国。

这是每当有人提议任何形式的人工智能受任何监管制度管辖时被拿出来的一条线，即使没有任何一种形式的牙齿。当有人说，某人可能被法律要求写电子邮件时。

至少我和迪恩·鲍尔中的一人极其错误地理解了这项法案。

对我而言，涵盖模型的定义显然只意味着那些在模型能力前沿有效的模型。

让我们再次看看确切的定义：

> (1) 人工智能模型在2024年使用了超过10^26整数或浮点运算的计算能力，**或者在常用于量化最先进基础模型性能的基准测试上可能具有类似表现的模型**，由行业最佳实践和相关标准制定组织决定。
> 
> (2) **人工智能模型在特定基准测试上具有低于相关阈值的能力，但在其他方面具有类似的一般能力。**

这似乎是一目了然的含义，它的含义是这样的：

1.  如果你的模型超过10^26，我们认为它算数。

1.  如果不是这样，但它与最先进当前模型一样好，它就算数。

1.  “和……一样好”是一种普遍的能力，而不是达到特定基准测试的事情。

根据这一定义，如果没有人在积极作弊基准测试，最多只有三个现有模型有可能符合这一定义：GPT-4、Gemini Ultra和Claude。我甚至不确定Claude是否符合。

如果开源模型在游戏基准测试方面作弊得如此之多，以至于有些模型与GPT-4在基准测试上表现相当，那我能说什么呢，也许应该停止这种基准测试的作弊？

或者相当合理地指出真正的基准是用户偏好，从这些方面来看，你的产品不尽如人意，所以没问题。不管怎样。

> 但请注意，这并不是法案的内容。该法案适用于大型模型*以及*无论需要多少计算预算来达到相同性能的模型。这意味着该法案适用于初创企业和大公司。

呃，不，因为开放模型权重模型远远达不到OpenAI的性能水平？

或许将来会有一些人这么做。

但这很明显并不是“禁止所有开源”。目前没有任何开放模型权重模型受此禁止。

未来可能有少数几家公司可能会对此感到担忧，如果OpenAI不发布GPT-5一段时间的话，但我们在谈论的是Mistral和Meta，而不是小型初创公司。我们之所以谈论它们，正是因为它们将在这种情况下完全与大公司竞争。

Bell也错误地认为在训练之前会实施预防原则。

我在这里没有看到任何这样的规定。我看到的是，如果您不能在训练之前显示您的模型绝对安全，那么您必须等到训练运行之后才能证明它是安全的。

换句话说，这是一个逃避条款。我们真的反对这个吗？

然后，如果您在训练后还不能证明其安全性，那么我们谈谈预防措施。但没有人说您不能进行训练，除非我漏掉了什么？

和往常一样，像Ball这样的人想象一个“我的产品绝不会被用来造成伤害”的标准，这在任何方面都不适用。这就是为什么任何未达到前沿水平的模型都可以自动获得正面的安全评估，这与这一理论完全相悖。然后，如果你处于前沿水平，你必须遵守行业标准的安全程序，并告知加利福尼亚你正在遵循哪些程序。真是糟糕。当然，一旦有其他人拥有大幅度更好的模型，猜猜现在谁又会被认为是绝对安全？

Ball声称引起警惕的“覆盖指南”并不意味着“执行所有安全组织说的一切，如果它们互相矛盾，那么你就被禁止。”法律并非如此运作。这是它实际上的内容：

> (e) “覆盖指南”指以下任何内容：
> 
> (1) 由国家标准与技术研究院和前沿模型部门发布的适用指南。
> 
> (2) 行业最佳实践，包括开发类似模型的开发者采取的相关安全实践、预防措施或测试程序，以及学术界或非营利部门相关专家普遍认可的任何安全标准或最佳实践。
> 
> (3) 由标准制定组织设定的适用于增强安全性的标准。

那么这意味着，我们将以NIST的扩展为基础制定我们的标准，而且我们也期望您有责任实施任何被认为是‘行业最佳实践’的东西，即使我们没有将其包括在要求中。但显然，如果违法的话，那就不会是最佳实践。接着我们有第三条规则，只计算‘适用’标准。加利福尼亚州将对它们进行审查，并决定什么是适用的，这就意味着他们将寻求外部帮助。

还要注意在谈论所有模型时的‘非衍生’术语。如果您的模型是衍生模型，那么您默认就没有问题。而且几乎所有带有开放权重的模型都是衍生模型，因为这当然是关键，蒸馏和精炼，而不是一直从头开始。

那么根据我所知，这个法律实际上会做到以下几点：

1.  如果您的模型没有预计到达最先进的水平，而且也没有超过迄今为止尚未达到的10^26限制，并且除了大三家之外，没有其他任何人处于接近状态，这个法律对您的影响微乎其微，仅仅是一些琐碎的文书工作。美国的其他每一个企业，特别是加利福尼亚州，都会嫉妒。

1.  如果您的模型是现有模型的衍生物，那么您就没问题，就这样。

1.  如果您想要训练的模型预计到达最先进的水平，但您可以在训练之前就证明它是安全的，那么干得好，您又是金光闪闪了。

1.  如果您的模型预计到达最先进的水平，并且在训练之前不能证明它是安全的，您仍然可以训练它，只要不发布它，并确保它不被他人窃取或发布。然后，如果您证明它是安全的或者证明它不是最先进的，那么您又是金光闪闪了。

1.  如果您的模型是最先进的，并且您训练了它但仍不知道它是否‘安全’，这里的安全不是指‘从未发生过任何错误’，而是更像‘从未造成5亿美元的损失或大规模伤亡’，那么您必须实施一系列由加利福尼亚州确定的安全协议（监管要求），并告知他们您正在采取什么措施确保安全。

1.  您必须具备‘关闭我控制下运行的计算机上的AI’和‘合理防止未经授权的人员访问模型’等能力，即使这不适用于您不再控制的程序副本。这会成为问题吗？

1.  您还必须报告发生的任何‘安全事故’。

1.  还有一些未知大小和重要性的‘支持创新’的东西。

SB 1047不仅没有试图‘扼杀AI’，也没有试图实施监管困境或针对初创企业，除非它们有活跃的安全事故。如果存在活跃的安全事故，那么我们会了解到这些事故，这可能会引发责任或公开担忧，这似乎是主要的缺点？人们可能会了解到你的失败，现行法律有时可能会适用？

反对这些规则的论点往往源于这样一个假设：我们按照书面法律可靠地、无差错地实施我们的法律。但我们并没有。如果，正如埃利泽最近开玩笑说的那样，法律实际上按照批评这些法规的人所宣称的方式运作，会发生什么？如果每一条法律都严格按照书面法规执行，不使用常识，正如他们警告的那样会发生的？法院能处理涉及的案件负担吗？每个人都会在一周内被关进监狱。

当人们看到将AI稍微像其他任何东西一样对待，并对其施加相当普通的规定，有明确而有意识的努力，仅针对全面封闭的前沿模型，他们说这‘禁止开源’是什么意思？

他们说开放模型权重是不安全的，没有任何方法可以修复这一点，我们希望做的事情显然是不安全的，所以问任何形式的‘这安全吗？’并且对答案‘不’有异议就是对开放模型权重的禁令。或者，换句话说，他们说他们的商业模式和分发计划与任何规则完全不兼容，因此我们不应该通过任何规则，或者他们应该免于遵守任何规则。

认为这将‘标志着美国在AI领导地位的终结’是可笑的。如果你认为美国的技术产业不能承受一点监管，我是说，他们了解美国或加利福尼亚吗？他们见过另一位吗？他们见过在几乎完全适用规则次序下的各行各业的美国创新吗？这简直是荒谬的。

但是，那些批评者何时让这些成为阻碍呢？无论什么情况下，他们的论调总是一样的。有些人似乎愿意放大这些声音，而不去问他们的话是否讲得通。

如果真的有只狼出现会发生什么？
