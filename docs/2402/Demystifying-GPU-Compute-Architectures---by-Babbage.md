<!--yml

category: 未分类

date: 2024-05-27 14:37:48

-->

# 解析GPU计算架构 - 作者：巴贝奇

> 来源：[https://thechipletter.substack.com/p/demystifying-gpu-compute-architectures](https://thechipletter.substack.com/p/demystifying-gpu-compute-architectures)

NVIDIA_GeForce_8800GTX_G80___DSC01200 via Fritzchens Fritz https://commons.wikimedia.org/w/index.php?curid=65926359

现代GPU的复杂性不应该掩盖这些机器运行的本质。如果我们要充分利用这些GPU的先进技术，那么我们需要少一点神秘感。

我认为这是计算机架构领域中令人激动的时刻之一。在CPU中，我们有x86、Arm和RISC-V，这三种逐渐得到越来越多支持的应用架构。

当然，GPU架构变得越来越重要。显然，GPU是机器学习基础设施的关键组成部分。它们还使得大规模并行计算可用于帮助解决更广泛的计算问题，超越了机器学习范畴。

不过有一个问题。相比于CPU，人们对GPU架构的理解远不如后者广泛。CPU通常很复杂，但其中大部分复杂性（如流水线、指令和数据缓存、分支预测、乱序执行、寄存器重命名、虚拟内存等）大多数时候对用户来说是隐藏的，甚至可以忽略不计。然而，并行处理本质上更复杂，而GPU必然暴露了很多复杂性。

* * *

关于GPU也存在一定程度的神秘感。最近（而且非常出色的）Acquired播客[集数](https://www.acquired.fm/episodes/nvidia-the-dawn-of-the-ai-era)中提到，对于GPU：

> 当然，神奇的突破在于制造一台不是冯·诺伊曼体系结构的计算机。

我知道Acquired的主持人并不是想暗示GPU是‘神奇’的，但在与GPU相关的语境中使用这个词似乎很合适。GPU不使用‘冯·诺伊曼’架构。相反，它们使用一种未指定的、未命名的架构，如果我们不知道它是什么，那它必然是神秘的。

更糟糕的是，了解GPU工作原理存在几个障碍：

+   **架构变化：** 每个主要的GPU架构都有稍微不同的实现。

+   **术语不一致：** 主要的GPU设计者在术语使用上存在不一致，他们之间以及与通常用于CPU的术语之间都不同。因此，CPU的‘核心’与Nvidia的‘CUDA核心’有很大不同。

+   **传统术语：** 一些术语是从GPU设计的图形起源中继承而来的（例如‘着色器’）。

+   **软件抽象：** 一些GPU的复杂性可以通过软件包使用的抽象来隐藏，用于编程GPU。这有助于实现程序，但有时会阻止用户形成对架构运作方式的足够清晰的理解。

+   **CUDA的主导地位：** 由于Nvidia的CUDA多年来一直是GPU计算的市场领导者，许多解释中使用了CUDA术语，使得理解其他架构变得困难。

难怪这一切看起来有点神秘。

但随着GPU的重要性日益增加，这似乎是一个不令人满意的状态。让我们试图揭开这个神秘的面纱！

* * *

这篇帖子最初是关于Nvidia GPU通用计算架构历史的。然而，由于两个原因，我很快改变了方向：

+   解释一个架构的代际已经很复杂了。试图描述一系列架构的代际，就像复杂度的平方，可能就太多了！

+   已经有一篇非常详细的帖子，法比安·桑格兰的《[NVIDIA流多处理器的历史](https://fabiensanglard.net/cuda/index.html)》，详细描述了Nvidia通用GPU计算架构从2006年到2020年的发展历史。

但是，关于法比安的帖子，需要提个小心。它假设你对GPU有很多先验知识。如果你目前还不知道什么是“流多处理器”，不要担心。一切将在稍后变得清晰！即使你对GPU不熟悉，浏览法比安的帖子也是值得的，可以看到Nvidia架构在这段时间内的发展。

我决定不重复法比安写的大部分内容，而是提供一个“从基础开始”的最新Nvidia和AMD两款架构的基本解释，这可能更有用。

不过，在我们继续之前，我想引用法比安在他的帖子中关于Nvidia通用计算架构如何出现的一段话：

> 直到2006年，Nvidia的GPU设计与渲染API中的逻辑阶段相一致。由G71芯片驱动的GeForce 7900 GTX由三个部分组成，专用于顶点处理（8个单元）、片段生成（24个单元）和片段合并（16个单元）。
> 
> 这种相关性迫使设计者们去猜测瓶颈的位置，以便正确平衡每个层次。随着DirectX 10引入了几何着色器的另一个阶段，Nvidia的工程师们面临着在不知道一个阶段会被采用多少的情况下平衡芯片的困难任务。是时候做出改变了。
> 
> Nvidia通过其于2006年发布的“统一”特斯拉架构解决了复杂性不断上升的问题。
> 
> 在G80芯片中，不再区分层次。流多处理器（SM）替代了以往的所有单元，因其能够在不区分的情况下运行顶点、片段和几何“核心”。通过每个SM运行的“核心”根据管道需求自动进行负载平衡。

因此，Nvidia现代“通用计算”架构的第一代就此诞生。

> *我们基本上抛弃了 NV30/NV40 的整个着色器架构，并从头开始采用了新的通用处理器架构（SIMT），还引入了新的处理器设计方法。*
> 
> - Jonah Alben（与 extremetech.com 的采访）*

再次引用 Fabien 的话：

> 特斯拉是一次冒险的举动，结果证明非常成功。它如此成功，以至于成为了 Nvidia GPU 在接下来二十年的基础。

正如我在撰写本文时所述，这对 Nvidia 真的是一件“大好事”，其市值已达到 1.6 万亿美元：

Nvidia 已经投入了将近 18 年的开发和投资，从特斯拉架构开始，包括对 Nvidia 著名的“统一计算设备架构”或“CUDA”的工作，该架构于 2007 年引入。我们之前已经讨论过 Nvidia 的一些战略方面（第一个现在可免费阅读）：

* * *

[分享](https://thechipletter.substack.com/p/demystifying-gpu-compute-architectures?utm_source=substack&utm_medium=email&utm_content=share&action=share)

**本文剩余部分将详细解释现代 GPU 上的通用计算是如何从底层逐步构建起来的。我们将从最低层级开始，介绍两种主要架构，然后逐步展开讨论。**

我选择了同时观察 Nvidia（用于 H100 GPU 的 Hopper 架构）和 AMD 的 CDNA 2（用于 Instinct MI200 GPU 的版本，其中的一个版本用于 [Frontier 超级计算机](https://en.wikipedia.org/wiki/Frontier_(supercomputer))），并解释了每家公司最新设计之间的共同点和差异。我们特别关注澄清使用的一些术语。我们不会考虑用于加速机器学习的最新功能，如张量核心。相反，我们将集中讨论最“通用”的计算能力。

这不是一个让读者能够开始在 GPU 上编写程序的教程。相反，如果你想做到这一点，它旨在作为一个补充，帮助更好地理解在表面下发生的事情和 GPU 程序的抽象。

无论如何，要正确理解像 GPU 如何工作这样复杂的主题，真的有必要阅读大量资料。为了帮助这一点，本文末尾提供了大量参考文献和链接，供希望了解更多 GPU 计算的读者阅读。

* * *

我们将从 GPU 计算硬件开始。我们将分成三个“级别” - 这是我自己的术语，因为不同架构之间没有共同的命名惯例，与“缓存级别”无关。

我们将从 GPU 架构的最低级别开始，介绍其基本构建模块。也许有些意外的是，这个名字在 Nvidia 或 AMD 那里似乎没有被赋予一个令人难忘的名字。

它包含了一系列计算资源，可以执行一系列操作：整数、32位或64位浮点运算、从内存加载和存储数据等等。

我们将从Nvidia的硬件开始。让我们看一下这个基本构建块的示意图。这是Nvidia Hopper H100架构白皮书中的一个图表。

这里最小的方框标注着“INT32”、“FP32”或“FP64”，代表着执行单元或者单个32位整数、32位浮点或64位浮点操作。所以在这种情况下，我们有三十二个32位浮点执行单元，以及分别有十六个32位整数和64位浮点执行单元。

> 如右侧所示，还有一个“Tensor Core”用于许多机器学习程序所需的“张量”或“矩阵”运算。我们在本文中不会进一步讨论Tensor Core。

在此原理图的底部是执行单元（“LD/ST”），用于执行从内存到内存的“加载”和“存储”操作。

我们需要考虑如何控制这些执行单元。让我们关注整数和浮点硬件。首先要强调的是，多个执行单元被组合在一起，并且同时执行单条指令，因此它们作为一个组的操作类似于CPU上的“单指令多数据”或“SIMD”操作。事实上，GPU制造商有时会将他们的架构称为“SIMT”，即“单指令多线程”。

例如，一条指令可能会在一个“向量”（或数组）上执行单个操作，例如十六个64位浮点数、三十二个32位浮点数或三十二个32位整数。

与CPU上的SIMD相比，这里也有一些不同之处。不像现代CPU那样，这些处理器不实现诸如分支预测或乱序执行等许多功能。这使它们作为“通用目的”处理器表现不佳。然而，避免这些功能的复杂性可以腾出大量空间，以在单个硅片上容纳更多的这些处理器。

如果它们在某些方面不如CPU高效，它们在一项任务上要好得多：在不同程序之间进行切换。例如，没有分支预测，运行在这种硬件上的程序更有可能需要等待程序代码或数据从内存中获取。如果发生这种情况，为了保持处理器的忙碌状态，它可以快速切换到另一个等待并准备接管的程序。当‘退役’程序获取到所需数据后，它可以快速重新启动。

我们需要为硬件中每个处理一个数据项的元素取一个名字。我们将其称为“SIMT Lane”，类似于高速公路上的车道，多条并行铺设的车道使多条流量能够同时前进。

* * *

让我们来介绍GPU架构的这个基本构建块的其余特性。

存在一些‘Register File’和一些‘L0 Instruction Cache’内存。稍后我们会回顾 GPU 上的寄存器和内存。

还有所谓的‘Warp Scheduler’和‘Dispatch Unit’。稍后我们将详细解释‘Warp’是什么，但基本上这是一种组织在执行单元上运行多个程序的硬件。

最后，还有‘Special Function Units’或‘SFUs’，可以看到它们位于右下角。这些执行诸如正弦、余弦、倒数和平方根等超越函数指令。

* * *

AMD 的 CDNA 2 架构在这个层次上与 Nvidia 的相似，但也有一些不同之处。例如，它只有十六个 32 位浮点执行单元。在大多数方面，它的运作方式是类似的。在这一点上，我们不打算深入研究它。

构建 GPU 的下一个阶段是将若干这些‘Level 1’硬件单元组合在一起。Nvidia 和 AMD 各自将四个这样的单元组合在一起，称之为‘流处理多处理器’（Nvidia）或‘计算单元’（AMD）。

这是 Nvidia 的流处理多处理器示意图。Nvidia 还添加了一些‘L1 Instruction Cache’和‘L1 Data Cache’。稍后我们会详细讨论‘共享内存’。我们可以忽略‘Tensor Memory Accelerator’和‘Tex’。

这里是 AMD 计算单元的示意图。AMD 添加了‘Matrix Core Units’、更多内存和‘Scalar Unit’，这是一个独立运行的标量处理器。

> ‘Matrix Cores’ 的作用类似于 Nvidia 的 Tensor Cores，用于加速机器学习操作。我们不会在本文中进一步讨论 Matrix Cores。
> 
> 我们的最终阶段是将多个流处理多处理器（Nvidia）/计算单元（AMD）组合在一起。来自两种架构的最新 GPU 每个都组合了超过 100 个流处理多处理器/计算单元。
> 
> 这里是 Nvidia H100 的示意图，每个 GPU 有 144 个流处理多处理器（点击以查看此图像的更易读版本）：
> 
> 这里是 AMD M200 的示意图，每个 GPU 有 112 个计算单元。
> 
> 在每种情况下，流处理多处理器/计算单元都配备了‘L2 Cache’和用于与系统其他部分（包括内存）快速通信的硬件。这些硬件可能使用 HBM、PCIe 或专有技术，如 NVLink（Nvidia）或 Infinity Fabric（AMD）。
> 
> * * *
> 
> **让我们简要地退后一步，考虑一下这些 GPU 在计算硬件方面与甚至最强大的现代 CPU 相比的差距。**
> 
> **领先的 AMD x86 CPU 拥有 128 个处理器核心，并且具有执行浮点运算单元（通过 AVX512 指令）以每次执行八个 32 位浮点运算的能力。这是每个 CPU 1,024 个执行单元。与之形成对比的是 Nvidia H100 的 18,432 个 32 位浮点执行单元。**
> 
> 休息结束后，我们将讨论 GPU 指令集和寄存器、内存和软件。
