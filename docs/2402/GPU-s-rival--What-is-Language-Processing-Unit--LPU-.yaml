- en: <!--yml
  id: totrans-split-0
  prefs: []
  type: TYPE_NORMAL
- en: 'category: Êú™ÂàÜÁ±ª'
  id: totrans-split-1
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-05-27 15:01:09'
  id: totrans-split-2
  prefs: []
  type: TYPE_NORMAL
- en: -->
  id: totrans-split-3
  prefs: []
  type: TYPE_NORMAL
- en: GPU's rival? What is Language Processing Unit (LPU)
  id: totrans-split-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Êù•Ê∫êÔºö[https://www.turingpost.com/p/fod41](https://www.turingpost.com/p/fod41)
  id: totrans-split-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Next Week in Turing Post:'
  id: totrans-split-6
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Wednesday, Token 1.21: **Model Safety and Data Privacy**'
  id: totrans-split-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Friday, AI Unicorns: **Scale AI**'
  id: totrans-split-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Turing Post is a reader-supported publication. To have full access to our
    most interesting articles and investigations, become a paid subscriber ‚Üí*'
  id: totrans-split-9
  prefs: []
  type: TYPE_NORMAL
- en: This week, a largely unknown company, [Groq](https://groq.com/?utm_source=www.turingpost.com&utm_medium=referral&utm_campaign=fod-41-gpu-s-rival-what-is-language-processing-unit-lpu),
    demonstrated unprecedented speed running open-source LLMs such as Llama-2 (70
    billion parameters) at more than 100 tokens per second, and Mixtral at nearly
    500 tokens per second per user on a Groq‚Äôs Language Processing Unit (LPU).
  id: totrans-split-10
  prefs: []
  type: TYPE_NORMAL
- en: ‚ÄúAccording to Groq, in similar tests, ChatGPT loads at 40-50 tokens per second,
    and Bard at 70 tokens per second on typical GPU-based computing systems.
  id: totrans-split-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Context for 100 tokens per second per user ‚Äì A user could generate a 4,000-word
    essay in just over a minute.‚Äù
  id: totrans-split-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'So: **What is LPU, how does it work**, and where is Groq (such an unfortunate
    name, given Musk''s Grok is all over the media) coming from?'
  id: totrans-split-13
  prefs: []
  type: TYPE_NORMAL
- en: Remember that game of Go in 2016 when AlphaGo played against the world champion
    Lee Sedol and won? Well, about a month before the competition, there was a test
    game which AlphaGo lost. The researchers from DeepMind ported AlphaGo to Tensor
    Processing Unit (TPU) and then the computer program was able to win by a wide
    margin.
  id: totrans-split-14
  prefs: []
  type: TYPE_NORMAL
- en: The realization that computational power was a bottleneck for AI's potential
    led to the inception of Groq and the creation of the LPU. This realization came
    to [Jonathan Ross](https://www.linkedin.com/in/ross-jonathan/?utm_source=www.turingpost.com&utm_medium=referral&utm_campaign=fod-41-gpu-s-rival-what-is-language-processing-unit-lpu)
    who initially began what became TPU project in Google. He started Groq in 2016\.
  id: totrans-split-15
  prefs: []
  type: TYPE_NORMAL
- en: '**The LPU is a special kind of computer brain designed to handle language tasks
    very quickly.** Unlike other computer chips that do many things at once (parallel
    processing), the LPU works on tasks one after the other (sequential processing),
    which is perfect for understanding and generating language. Imagine it like a
    relay race where each runner (chip) passes the baton (data) to the next, making
    everything run super fast. The LPU is designed to overcome the two LLM bottlenecks:
    compute density and memory bandwidth.'
  id: totrans-split-16
  prefs: []
  type: TYPE_NORMAL
- en: Groq took a novel approach right from the start, **focusing on software and
    compiler development** before even thinking about the hardware. They made sure
    the software could guide how the chips talk to each other, ensuring they work
    together seamlessly like a team in a factory. This makes the LPU really good at
    processing language efficiently and at high speed, ideal for AI tasks that involve
    understanding or creating text.
  id: totrans-split-17
  prefs: []
  type: TYPE_NORMAL
- en: This led to a highly optimized system that not only runs circles around traditional
    setups in terms of speed but does so with greater cost efficiency and lower energy
    consumption. This is big news for industries like finance, government, and tech,
    where quick and accurate data processing is key.
  id: totrans-split-18
  prefs: []
  type: TYPE_NORMAL
- en: Now, don't go tossing out your GPUs just yet! While the LPU is a beast when
    it comes to inference, making light work of applying trained models to new data,
    GPUs still reign supreme in the training arena. The LPU and GPU might become the
    dynamic duo of AI hardware, each excelling in their respective roles.
  id: totrans-split-19
  prefs: []
  type: TYPE_NORMAL
- en: 'As Elvis Saravia [put](https://www.linkedin.com/feed/update/urn:li:activity:7165371713378074624/?utm_source=www.turingpost.com&utm_medium=referral&utm_campaign=fod-41-gpu-s-rival-what-is-language-processing-unit-lpu)
    it: ‚Äú*With breakthroughs in inference and long context understanding, we are officially
    entering a new era in LLMs.*‚Äù'
  id: totrans-split-20
  prefs: []
  type: TYPE_NORMAL
- en: 'To better understand architecture, Groq offers two papers: from [2020](https://wow.groq.com/groq-isca-paper-2020/?utm_source=www.turingpost.com&utm_medium=referral&utm_campaign=fod-41-gpu-s-rival-what-is-language-processing-unit-lpu)
    (*Think Fast: A Tensor Streaming Processor (TSP) for Accelerating Deep Learning
    Workloads*) and [2022](https://wow.groq.com/isca-2022-paper/?utm_source=www.turingpost.com&utm_medium=referral&utm_campaign=fod-41-gpu-s-rival-what-is-language-processing-unit-lpu)
    (*A Soware-defined Tensor Streaming Multiprocessor for Large-scale Machine Learning*).¬†The
    term ‚ÄúLPU‚Äù must be a recent addition to Groq‚Äôs narrative, since it‚Äôs never mentioned
    in the papers.'
  id: totrans-split-21
  prefs: []
  type: TYPE_NORMAL
- en: 'Compute is also a part of this paper: [Computing Power and the Governance of
    Artificial Intelligence](https://arxiv.org/pdf/2402.08797.pdf?utm_source=www.turingpost.com&utm_medium=referral&utm_campaign=fod-41-gpu-s-rival-what-is-language-processing-unit-lpu)
    that discusses managing AI development through compute control, focusing on its
    potential for regulation, benefits, and risks, and suggests balanced governance
    approaches.'
  id: totrans-split-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Meanwhile, The U.S. [awards](https://www.reuters.com/technology/us-awards-15-bln-globalfoundries-domestic-semiconductor-production-2024-02-19?utm_source=www.turingpost.com&utm_medium=referral&utm_campaign=fod-41-gpu-s-rival-what-is-language-processing-unit-lpu)
    GlobalFoundries, the world's third-largest contract chipmaker, $1.5 billion to
    boost semiconductor production, enhancing domestic supply chains, with expansions
    in New York and Vermont.
  id: totrans-split-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The paper [published](https://bair.berkeley.edu/blog/2024/02/18/compound-ai-systems/?utm_source=www.turingpost.com&utm_medium=referral&utm_campaign=fod-41-gpu-s-rival-what-is-language-processing-unit-lpu)
    by Berkeley Artificial Intelligence Research (BAIR) argues that ‚Äú**compound AI
    systems will likely be the best way to maximize AI results in the future**, and
    might be one of the most impactful trends in AI in 2024.‚Äù
  id: totrans-split-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: News from The Usual Suspects ¬©
  id: totrans-split-25
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Y Combinator
  id: totrans-split-26
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Since 2009, Y Combinator has published **[Request for Startups](https://www.ycombinator.com/rfs?utm_source=www.turingpost.com&utm_medium=referral&utm_campaign=fod-41-gpu-s-rival-what-is-language-processing-unit-lpu)**
    which hints at what ‚Äúideas we‚Äôd want to see made real, in spaces that we believe
    will be important in the coming decades‚Äù. This year, the list contains 20 categories:'
  id: totrans-split-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 20 big names
  id: totrans-split-28
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Twenty tech giants, including Adobe, Amazon, Google, IBM, Meta, Microsoft, OpenAI,
    and TikTok, have [agreed](https://apnews.com/article/ai-generated-election-deepfakes-munich-accord-meta-google-microsoft-tiktok-x-c40924ffc68c94fac74fa994c520fc06?utm_source=www.turingpost.com&utm_medium=referral&utm_campaign=fod-41-gpu-s-rival-what-is-language-processing-unit-lpu)
    to take "reasonable precautions" to prevent the misuse of AI in disrupting elections
    worldwide.
  id: totrans-split-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: OpenAI
  id: totrans-split-30
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Models making headlines:'
  id: totrans-split-31
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Introducing Sora**: This paper introduces Sora, a breakthrough in video generation
    technology by OpenAI, capable of producing high-fidelity videos. It leverages
    spacetime patches to handle videos of varying durations and resolutions, making
    strides toward simulating the physical world with impressive 3D consistency and
    long-range coherence. It represents a leap in the ability to create detailed simulations
    that could be used for a myriad of applications, from entertainment to virtual
    testing environments [‚Üíread the paper](https://openai.com/research/video-generation-models-as-world-simulators?utm_source=www.turingpost.com&utm_medium=referral&utm_campaign=fod-41-gpu-s-rival-what-is-language-processing-unit-lpu)'
  id: totrans-split-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Additional read:**'
  id: totrans-split-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Introducing V-Jepa** (Yann LeCun‚Äôs vision of advanced machine intelligence
    (AMI): Meta''s V-JEPA model revolutionizes unsupervised learning from videos by
    using feature prediction as its sole objective. This approach bypasses the need
    for pre-trained image encoders or text annotations, relying instead on the intrinsic
    dynamics of video data to learn versatile visual representations. It''s a significant
    contribution to the field of unsupervised visual learning, promising advancements
    in how machines understand motion and appearance without explicit guidance [‚Üíread
    the paper](https://scontent-lga3-1.xx.fbcdn.net/v/t39.2365-6/427986745_768441298640104_1604906292521363076_n.pdf?_nc_cat=103&ccb=1-7&_nc_sid=3c67a6&_nc_ohc=Lpq5IeF5ftUAX_QVyw0&_nc_ht=scontent-lga3-1.xx&oh=00_AfCxzl-IotQM2_EGXbgWfRe66yPVffGfxGm5oSY74v1Slw&oe=65D898F1&utm_source=www.turingpost.com&utm_medium=referral&utm_campaign=fod-41-gpu-s-rival-what-is-language-processing-unit-lpu)'
  id: totrans-split-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Introducing Gemini 1.5**: Google DeepMind''s Gemini 1.5 introduces a Mixture-of-Experts
    architecture, enhancing the model''s performance across a broader array of tasks.
    Notably, it expands the context window to 1 million tokens, enabling deep analysis
    over large datasets. Gemini 1.5 represents a significant step forward in AI''s
    capability to process and understand extensive contexts, marking a milestone in
    the development of multimodal models [‚Üíread the paper](https://blog.google/technology/ai/google-gemini-next-generation-model-february-2024/?utm_source=www.turingpost.com&utm_medium=referral&utm_campaign=fod-41-gpu-s-rival-what-is-language-processing-unit-lpu#build-experiment)'
  id: totrans-split-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Introducing Stable Cascade**: Stable Cascade from Stability AI introduces
    a novel text-to-image generation framework that prioritizes efficiency, ease of
    training, and fine-tuning on consumer-grade hardware. The model''s hierarchical
    compression technique represents a significant reduction in the resources required
    for training high-quality generative models, providing a pathway for wider accessibility
    and experimentation in the AI community [‚Üíread the paper](https://stability.ai/news/introducing-stable-cascade?utm_source=www.turingpost.com&utm_medium=referral&utm_campaign=fod-41-gpu-s-rival-what-is-language-processing-unit-lpu)'
  id: totrans-split-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The freshest research papers, categorized for your convenience
  id: totrans-split-37
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Language Understanding and Generation
  id: totrans-split-38
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**OpenToM**: Explores evaluating Theory-of-Mind reasoning in LLMs, addressing
    their capability to understand complex social and psychological narratives. [Read
    the paper](https://arxiv.org/pdf/2402.06044.pdf?utm_source=www.turingpost.com&utm_medium=referral&utm_campaign=fod-41-gpu-s-rival-what-is-language-processing-unit-lpu)'
  id: totrans-split-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**In Search of Needles in a 10M Haystack**: Demonstrates the capability of
    NLP models to process exceptionally long documents, pushing the boundaries of
    document length comprehension. [Read the paper](https://arxiv.org/pdf/2402.10790.pdf?utm_source=www.turingpost.com&utm_medium=referral&utm_campaign=fod-41-gpu-s-rival-what-is-language-processing-unit-lpu)'
  id: totrans-split-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Premise Order Matters in Reasoning with LLMs**: Investigates the sensitivity
    of LLMs to the order of premises, revealing implications for reasoning tasks.
    [Read the paper](https://arxiv.org/pdf/2402.08939.pdf?utm_source=www.turingpost.com&utm_medium=referral&utm_campaign=fod-41-gpu-s-rival-what-is-language-processing-unit-lpu)'
  id: totrans-split-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Chain-of-Thought Reasoning Without Prompting**: Uncovers the inherent ability
    of LLMs to generate reasoning paths, suggesting an alternative to explicit prompting.
    [Read the paper](https://arxiv.org/pdf/2402.10200.pdf?utm_source=www.turingpost.com&utm_medium=referral&utm_campaign=fod-41-gpu-s-rival-what-is-language-processing-unit-lpu)'
  id: totrans-split-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Suppressing Pink Elephants with Direct Principle Feedback**: Addresses the
    challenge of topic avoidance in LLMs, proposing a novel fine-tuning method for
    enhanced controllability. [Read the paper](https://arxiv.org/pdf/2402.07896.pdf?utm_source=www.turingpost.com&utm_medium=referral&utm_campaign=fod-41-gpu-s-rival-what-is-language-processing-unit-lpu)'
  id: totrans-split-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**GhostWriter**: Develops an AI-powered writing environment focusing on personalization
    and increased user control in collaborative writing. [Read the paper](https://arxiv.org/pdf/2402.08855.pdf?utm_source=www.turingpost.com&utm_medium=referral&utm_campaign=fod-41-gpu-s-rival-what-is-language-processing-unit-lpu)'
  id: totrans-split-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Speech and Text-to-Speech Technologies
  id: totrans-split-45
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Mathematical and Scientific Reasoning
  id: totrans-split-46
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**OpenMathInstruct-1**: Develops a dataset for math instruction tuning, aiming
    to improve LLMs'' mathematical reasoning capabilities. [Read the paper](https://arxiv.org/pdf/2402.10176.pdf?utm_source=www.turingpost.com&utm_medium=referral&utm_campaign=fod-41-gpu-s-rival-what-is-language-processing-unit-lpu)'
  id: totrans-split-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**InternLM-Math**: Introduces a specialized LLM for math reasoning, incorporating
    various techniques for enhanced problem-solving in mathematics. [Read the paper](https://arxiv.org/pdf/2402.06332.pdf?utm_source=www.turingpost.com&utm_medium=referral&utm_campaign=fod-41-gpu-s-rival-what-is-language-processing-unit-lpu)'
  id: totrans-split-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**ChemLLM**: Creates the first LLM dedicated to chemistry, transforming structured
    chemical data into dialogue for diverse chemical tasks. [Read the paper](https://arxiv.org/pdf/2402.06852.pdf?utm_source=www.turingpost.com&utm_medium=referral&utm_campaign=fod-41-gpu-s-rival-what-is-language-processing-unit-lpu)'
  id: totrans-split-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Efficiency and Data Utilization in AI
  id: totrans-split-50
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**How to Train Data-Efficient LLMs**: Proposes sampling methods for enhancing
    data efficiency in LLM training, optimizing example selection. [Read the paper](https://arxiv.org/pdf/2402.09668.pdf?utm_source=www.turingpost.com&utm_medium=referral&utm_campaign=fod-41-gpu-s-rival-what-is-language-processing-unit-lpu)'
  id: totrans-split-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**FIDDLER**: Introduces a system for efficient inference of MoE models, leveraging
    CPU-GPU orchestration for improved performance in resource-limited settings. [Read
    the paper](https://arxiv.org/pdf/2402.07033.pdf?utm_source=www.turingpost.com&utm_medium=referral&utm_campaign=fod-41-gpu-s-rival-what-is-language-processing-unit-lpu)'
  id: totrans-split-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Tandem Transformers**: Presents an architecture for improving inference efficiency
    of LLMs, utilizing a dual-model system for faster and accurate predictions. [Read
    the paper](https://arxiv.org/pdf/2402.08644.pdf?utm_source=www.turingpost.com&utm_medium=referral&utm_campaign=fod-41-gpu-s-rival-what-is-language-processing-unit-lpu)'
  id: totrans-split-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Towards Next-Level Post-Training Quantization of Hyper-Scale Transformers**:
    Proposes an advanced PTQ algorithm for efficient deployment of large Transformer
    models on edge devices. [Read the paper](https://arxiv.org/pdf/2402.08958.pdf?utm_source=www.turingpost.com&utm_medium=referral&utm_campaign=fod-41-gpu-s-rival-what-is-language-processing-unit-lpu)'
  id: totrans-split-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Multimodal and Vision-Language Models
  id: totrans-split-55
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Reinforcement Learning and Model Behavior
  id: totrans-split-56
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**ODIN**: Addresses reward hacking in RLHF, proposing a method to mitigate
    verbosity bias in LLMs for more concise and content-focused responses. [Read the
    paper](https://arxiv.org/pdf/2402.07319.pdf?utm_source=www.turingpost.com&utm_medium=referral&utm_campaign=fod-41-gpu-s-rival-what-is-language-processing-unit-lpu)'
  id: totrans-split-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Mixtures of Experts Unlock Parameter Scaling for Deep RL**: Shows the impact
    of MoE modules on deep RL networks, enhancing parameter scalability and performance.
    [Read the paper](https://arxiv.org/pdf/2402.08609.pdf?utm_source=www.turingpost.com&utm_medium=referral&utm_campaign=fod-41-gpu-s-rival-what-is-language-processing-unit-lpu)'
  id: totrans-split-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Operating Systems and Generalist Agents
  id: totrans-split-59
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Graph Learning and State Space Models
  id: totrans-split-60
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Challenges and Innovations in AI
  id: totrans-split-61
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**A Tale of Tails**: Explores the effects of synthetic data on neural model
    performance, theorizing potential risks of model collapse with synthetic data
    reliance. [Read the paper](https://arxiv.org/pdf/2402.07043.pdf?utm_source=www.turingpost.com&utm_medium=referral&utm_campaign=fod-41-gpu-s-rival-what-is-language-processing-unit-lpu)'
  id: totrans-split-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Transformers Can Achieve Length Generalization But Not Robustly**: Investigates
    Transformers'' ability to generalize to longer sequences, highlighting the challenge
    of maintaining robust performance. [Read the paper](https://arxiv.org/pdf/2402.09371.pdf?utm_source=www.turingpost.com&utm_medium=referral&utm_campaign=fod-41-gpu-s-rival-what-is-language-processing-unit-lpu)'
  id: totrans-split-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Become our Premium subscriber today! In most cases,* ***you can expense this
    subscription through your company!***ü§ç'
  id: totrans-split-64
  prefs: []
  type: TYPE_NORMAL
