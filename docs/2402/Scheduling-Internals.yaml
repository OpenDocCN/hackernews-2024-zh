- en: <!--yml
  id: totrans-split-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: æœªåˆ†ç±»'
  id: totrans-split-1
  prefs: []
  type: TYPE_NORMAL
  zh: ç±»åˆ«ï¼šæœªåˆ†ç±»
- en: 'date: 2024-05-29 13:29:40'
  id: totrans-split-2
  prefs: []
  type: TYPE_NORMAL
  zh: æ—¥æœŸï¼š2024-05-29 13:29:40
- en: -->
  id: totrans-split-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: Scheduling Internals
  id: totrans-split-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: è°ƒåº¦å†…éƒ¨
- en: æ¥æºï¼š[https://tontinton.com/posts/scheduling-internals/](https://tontinton.com/posts/scheduling-internals/)
  id: totrans-split-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: æ¥æºï¼š[https://tontinton.com/posts/scheduling-internals/](https://tontinton.com/posts/scheduling-internals/)
- en: A sneak peek to what's coming!
  id: totrans-split-6
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: å…ˆç¹ä¸ºå¿«ï¼
- en: I remember when I first learned that you can write a server handling millions
    of clients running on just a single thread, my mind was simply blown away ğŸ¤¯
  id: totrans-split-7
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘è®°å¾—å½“æˆ‘ç¬¬ä¸€æ¬¡å¾—çŸ¥ä½ å¯ä»¥åœ¨ä»…ä¸€ä¸ªçº¿ç¨‹ä¸Šè¿è¡Œå¤„ç†æ•°ç™¾ä¸‡å®¢æˆ·ç«¯çš„æœåŠ¡å™¨æ—¶ï¼Œæˆ‘çš„è„‘è¢‹ç®€ç›´æ˜¯è¢«éœ‡æƒŠåˆ°äº† ğŸ¤¯
- en: I used Node.js while knowing it is single threaded, I used `async` / `await`
    in Python, and I used threads, but never asked myself *"How is any of this possible?"*.
  id: totrans-split-8
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä½¿ç”¨ Node.jsï¼Œè™½ç„¶çŸ¥é“å®ƒæ˜¯å•çº¿ç¨‹çš„ï¼Œæˆ‘åœ¨ Python ä¸­ä½¿ç”¨äº†`async` / `await`ï¼Œä¹Ÿä½¿ç”¨äº†çº¿ç¨‹ï¼Œä½†ä»æœªé—®è¿‡è‡ªå·±â€œ*è¿™ä¸€åˆ‡æ€ä¹ˆå¯èƒ½å‘¢ï¼Ÿ*â€ã€‚
- en: This post is written to spread the genius of concurrency and hopefully getting
    you excited about it too.
  id: totrans-split-9
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ç¯‡æ–‡ç« çš„ç›®çš„æ˜¯ä¼ æ’­å¹¶å‘çš„å¤©æ‰ï¼Œå¹¶å¸Œæœ›ä½ ä¹Ÿå¯¹æ­¤æ„Ÿåˆ°å…´å¥‹ã€‚
- en: My goal is for you to want to send a link to this post to an engineer in your
    team asking out loud *"Wait, but how does async even work?"*.
  id: totrans-split-10
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘çš„ç›®æ ‡æ˜¯è®©ä½ æƒ³æŠŠè¿™ç¯‡æ–‡ç« çš„é“¾æ¥å‘é€ç»™å›¢é˜Ÿä¸­çš„å·¥ç¨‹å¸ˆï¼Œå¹¶å¤§å£°é—®é“â€œ*ç­‰ç­‰ï¼Œå¼‚æ­¥å·¥ä½œæ˜¯æ€ä¹ˆå›äº‹ï¼Ÿ*â€ã€‚
- en: 'Questions I''m going to answer:'
  id: totrans-split-11
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘è¦å›ç­”çš„é—®é¢˜ï¼š
- en: Why not create a thread per client?
  id: totrans-split-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä¸ºä»€ä¹ˆä¸ä¸ºæ¯ä¸ªå®¢æˆ·ç«¯åˆ›å»ºä¸€ä¸ªçº¿ç¨‹ï¼Ÿ
- en: How to sleep when waiting on I/O?
  id: totrans-split-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: åœ¨ç­‰å¾… I/O æ—¶å¦‚ä½•è¿›è¡Œä¼‘çœ ï¼Ÿ
- en: How does Node.js achieve concurrency?
  id: totrans-split-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Node.js å¦‚ä½•å®ç°å¹¶å‘ï¼Ÿ
- en: What's concurrency? What's parallelism?
  id: totrans-split-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä»€ä¹ˆæ˜¯å¹¶å‘ï¼Ÿä»€ä¹ˆæ˜¯å¹¶è¡Œï¼Ÿ
- en: What are coroutines?
  id: totrans-split-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä»€ä¹ˆæ˜¯åç¨‹ï¼Ÿ
- en: With an implementation we'll build piece by piece.
  id: totrans-split-17
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: é€šè¿‡ä¸€ä¸ªå®ç°ï¼Œæˆ‘ä»¬å°†é€æ­¥æ„å»ºèµ·æ¥ã€‚
- en: What are preemptive and non-preemptive schedulers?
  id: totrans-split-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä»€ä¹ˆæ˜¯æŠ¢å å¼å’ŒéæŠ¢å å¼è°ƒåº¦å™¨ï¼Ÿ
- en: How does Go and Rust implement concurrency in the language (stackful vs stackless)?
  id: totrans-split-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Go å’Œ Rust å¦‚ä½•åœ¨è¯­è¨€ä¸­å®ç°å¹¶å‘ï¼ˆæœ‰æ ˆ vs æ— æ ˆï¼‰ï¼Ÿ
- en: What scheduling algorithms are used by linux, Go and Rust's tokio?
  id: totrans-split-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Linuxã€Go å’Œ Rust çš„ Tokio ä½¿ç”¨ä»€ä¹ˆè°ƒåº¦ç®—æ³•ï¼Ÿ
- en: I assume proficiency in reading code and OS internals at an intermediate level,
    but don't stress over details you don't understand, try to get the bigger picture!
  id: totrans-split-21
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘å‡è®¾æ‚¨åœ¨ä¸­çº§æ°´å¹³ä¸Šå…·å¤‡é˜…è¯»ä»£ç å’Œæ“ä½œç³»ç»Ÿå†…éƒ¨çš„ç†Ÿç»ƒèƒ½åŠ›ï¼Œä½†ä¸è¦è¿‡äºå¼ºè°ƒæ‚¨ä¸ç†è§£çš„ç»†èŠ‚ï¼Œè¯•ç€æŠŠæ¡æ•´ä½“å›¾æ™¯ï¼
- en: With all of that out of the way, let us begin.
  id: totrans-split-22
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€åˆ‡æå®šåï¼Œè®©æˆ‘ä»¬å¼€å§‹å§ã€‚
- en: Just create a thread, bro
  id: totrans-split-23
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: åªéœ€åˆ›å»ºä¸€ä¸ªçº¿ç¨‹ï¼Œä¼™è®¡
- en: 'Let''s try to write a simple echo server (whatever we receive, we send back)
    in C code, we''ll call it `echod`:'
  id: totrans-split-24
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬å°è¯•åœ¨ C ä»£ç ä¸­ç¼–å†™ä¸€ä¸ªç®€å•çš„å›æ˜¾æœåŠ¡å™¨ï¼ˆæ— è®ºæ”¶åˆ°ä»€ä¹ˆï¼Œæˆ‘ä»¬éƒ½è¿”å›ï¼‰ï¼Œæˆ‘ä»¬ç§°ä¹‹ä¸º`echod`ï¼š
- en: '[PRE0]'
  id: totrans-split-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Cool, now what should we do if want to handle multiple clients concurrently?
    While one client is being handled, another tries to `connect` to our server, without
    ever succeeding, as our server reaches the `accept` call only once it is done
    handling the current client.
  id: totrans-split-26
  prefs: []
  type: TYPE_NORMAL
  zh: å¾ˆé…·ï¼Œç°åœ¨å¦‚æœæˆ‘ä»¬æƒ³è¦åŒæ—¶å¤„ç†å¤šä¸ªå®¢æˆ·ç«¯åº”è¯¥æ€ä¹ˆåŠï¼Ÿå½“ä¸€ä¸ªå®¢æˆ·ç«¯æ­£åœ¨è¢«å¤„ç†æ—¶ï¼Œå¦ä¸€ä¸ªè¯•å›¾è¿æ¥åˆ°æˆ‘ä»¬çš„æœåŠ¡å™¨ï¼Œä½†æ°¸è¿œä¸æˆåŠŸï¼Œå› ä¸ºæˆ‘ä»¬çš„æœåŠ¡å™¨åªæœ‰åœ¨å¤„ç†å½“å‰å®¢æˆ·ç«¯åæ‰ä¼šè°ƒç”¨`accept`ã€‚
- en: How can we fix that?
  id: totrans-split-27
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å¦‚ä½•ä¿®å¤è¿™ä¸ªé—®é¢˜ï¼Ÿ
- en: 'The first thing most people will think is *"Can''t you just create a thread
    for each client?"*, something that looks like this:'
  id: totrans-split-28
  prefs: []
  type: TYPE_NORMAL
  zh: å¤§å¤šæ•°äººä¼šé¦–å…ˆæƒ³åˆ°â€œ*éš¾é“ä½ ä¸èƒ½ä¸ºæ¯ä¸ªå®¢æˆ·ç«¯åˆ›å»ºä¸€ä¸ªçº¿ç¨‹å—ï¼Ÿ*â€ï¼Œçœ‹èµ·æ¥åƒè¿™æ ·ï¼š
- en: '[PRE1]'
  id: totrans-split-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: The first problem with threads is how the OS allocates a stack for a new thread.
    The stack is allocated virtual memory (10mb on linux by default), and physical
    pages are only commited once the pages are actually written to. This is really
    nice as it means that you don't really reserve 10mb of RAM for each thread right
    out of the gate, **but** it does mean the granularity of allocation is at least
    that of a page (run `getconf PAGESIZE`, my machine is 4kb). Using `pthread_attr_setstacksize`
    won't fix the problem, you still must provide a value that is a multiple of a
    page size. A page might be a lot more that what you actually use, depending on
    the application.
  id: totrans-split-30
  prefs: []
  type: TYPE_NORMAL
  zh: ä½¿ç”¨çº¿ç¨‹çš„ç¬¬ä¸€ä¸ªé—®é¢˜æ˜¯æ“ä½œç³»ç»Ÿä¸ºæ–°çº¿ç¨‹åˆ†é…å †æ ˆçš„æ–¹å¼ã€‚å †æ ˆåˆ†é…äº†è™šæ‹Ÿå†…å­˜ï¼ˆLinux é»˜è®¤ä¸º 10MBï¼‰ï¼Œç‰©ç†é¡µé¢ä»…åœ¨å®é™…å†™å…¥é¡µé¢æ—¶æ‰ä¼šåˆ†é…ã€‚è¿™éå¸¸å¥½ï¼Œå› ä¸ºè¿™æ„å‘³ç€ä½ ä¸ä¼šåœ¨ä¸€å¼€å§‹å°±ä¸ºæ¯ä¸ªçº¿ç¨‹é¢„ç•™10MBçš„RAMï¼Œ**ä½†**å®ƒç¡®å®æ„å‘³ç€åˆ†é…çš„ç²’åº¦è‡³å°‘æ˜¯é¡µé¢å¤§å°ï¼ˆè¿è¡Œ`getconf
    PAGESIZE`ï¼Œæˆ‘çš„æœºå™¨æ˜¯4KBï¼‰ã€‚ä½¿ç”¨`pthread_attr_setstacksize`ä¸ä¼šè§£å†³é—®é¢˜ï¼Œä½ ä»ç„¶å¿…é¡»æä¾›ä¸€ä¸ªé¡µé¢å¤§å°çš„å€æ•°ã€‚é¡µé¢å¯èƒ½æ¯”ä½ å®é™…ä½¿ç”¨çš„è¦å¤§å¾—å¤šï¼Œè¿™å–å†³äºåº”ç”¨ç¨‹åºã€‚
- en: I also think that relying on overcommitment of memory is pretty annoying. We
    are getting killed by the OOM killer instead of having an opportunity cleaning
    up resources when an allocation fails indicating we are out of memory.
  id: totrans-split-31
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: æˆ‘ä¹Ÿè®¤ä¸ºä¾èµ–å†…å­˜è¶…é¢åˆ†é…éå¸¸æ¼äººã€‚æˆ‘ä»¬è¢«å†…å­˜è€—å°½æ€æ‰‹ï¼ˆOOM killerï¼‰æ€æ­»ï¼Œè€Œä¸æ˜¯åœ¨åˆ†é…å¤±è´¥æ—¶æœ‰æœºä¼šæ¸…ç†èµ„æºã€‚
- en: 'The second problem we need to fix when creating a bunch of OS threads is to
    change all the relevant limits:'
  id: totrans-split-32
  prefs: []
  type: TYPE_NORMAL
  zh: åˆ›å»ºä¸€å †æ“ä½œç³»ç»Ÿçº¿ç¨‹æ—¶éœ€è¦è§£å†³çš„ç¬¬äºŒä¸ªé—®é¢˜æ˜¯æ›´æ”¹æ‰€æœ‰ç›¸å…³çš„é™åˆ¶ï¼š
- en: '[PRE2]'
  id: totrans-split-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Just the files I showed you might not be enough for your system, for example
    `systemd` also sets maximums.
  id: totrans-split-34
  prefs: []
  type: TYPE_NORMAL
  zh: åªæœ‰æˆ‘å‘æ‚¨å±•ç¤ºçš„æ–‡ä»¶å¯èƒ½å¯¹æ‚¨çš„ç³»ç»Ÿä¸å¤Ÿï¼Œä¾‹å¦‚`systemd`ä¹Ÿè®¾ç½®äº†æœ€å¤§é™åˆ¶ã€‚
- en: The third problem is performance. Context switching between kernel and user
    mode is expensive in terms of CPU cycles. A single context switch isn't that expensive
    on its own, but doing a lot of them adds up.
  id: totrans-split-35
  prefs: []
  type: TYPE_NORMAL
  zh: ç¬¬ä¸‰ä¸ªé—®é¢˜æ˜¯æ€§èƒ½ã€‚å†…æ ¸å’Œç”¨æˆ·æ¨¡å¼ä¹‹é—´çš„ä¸Šä¸‹æ–‡åˆ‡æ¢åœ¨ CPU å¾ªç¯æ–¹é¢æ˜¯æ˜‚è´µçš„ã€‚å•ä¸ªä¸Šä¸‹æ–‡åˆ‡æ¢æœ¬èº«å¹¶ä¸æ˜‚è´µï¼Œä½†å¦‚æœé¢‘ç¹è¿›è¡Œå¤§é‡åˆ‡æ¢ï¼Œæˆæœ¬å°±ä¼šç´¯ç§¯èµ·æ¥ã€‚
- en: The fourth problem is that the stack allocation is static, we can't modify the
    stack size (grow) or free up commited physical pages in the stack once they are
    unused (shrink).
  id: totrans-split-36
  prefs: []
  type: TYPE_NORMAL
  zh: ç¬¬å››ä¸ªé—®é¢˜æ˜¯å †æ ˆåˆ†é…æ˜¯é™æ€çš„ï¼Œæˆ‘ä»¬æ— æ³•ä¿®æ”¹å †æ ˆå¤§å°ï¼ˆå¢åŠ ï¼‰æˆ–åœ¨æœªä½¿ç”¨æ—¶é‡Šæ”¾å·²åˆ†é…çš„ç‰©ç†é¡µï¼ˆç¼©å‡ï¼‰ã€‚
- en: Because of all these problems, threads should not be your go-to solution for
    running a lot of tasks concurrently (especially for I/O bound tasks like in `echod`).
  id: totrans-split-37
  prefs: []
  type: TYPE_NORMAL
  zh: å› ä¸ºæ‰€æœ‰è¿™äº›é—®é¢˜ï¼Œçº¿ç¨‹ä¸åº”è¯¥æˆä¸ºåŒæ—¶è¿è¡Œå¤§é‡ä»»åŠ¡çš„é¦–é€‰è§£å†³æ–¹æ¡ˆï¼ˆç‰¹åˆ«æ˜¯åœ¨åƒ`echod`ä¸­çš„ I/O ç»‘å®šä»»åŠ¡ä¸­ï¼‰ã€‚
- en: How else can we make `echod` serve millions of clients concurrently?
  id: totrans-split-38
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬è¿˜èƒ½ä»¥ä½•ç§æ–¹å¼ä½¿`echod`åŒæ—¶ä¸ºæ•°ç™¾ä¸‡ä¸ªå®¢æˆ·ç«¯æä¾›æœåŠ¡ï¼Ÿ
- en: Async I/O
  id: totrans-split-39
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: å¼‚æ­¥ I/O
- en: Why block an entire thread from running, when calling `read` / `write` / `accept`?
    If you think about it, we waste a precious resource (CPU) from doing anything
    while the application waits for I/O.
  id: totrans-split-40
  prefs: []
  type: TYPE_NORMAL
  zh: å½“è°ƒç”¨`read` / `write` / `accept`æ—¶ï¼Œä¸ºä»€ä¹ˆè¦é˜»å¡æ•´ä¸ªçº¿ç¨‹ï¼Ÿå¦‚æœä»”ç»†è€ƒè™‘ï¼Œæˆ‘ä»¬ä¼šæµªè´¹ä¸€ç§å®è´µçš„èµ„æºï¼ˆCPUï¼‰ï¼Œè®©åº”ç”¨ç¨‹åºç­‰å¾…
    I/Oã€‚
- en: When calling `read` for example, the kernel waits for a network packet to be
    received from the network interface card or `NIC`. The CPU is free to run something
    else meanwhile.
  id: totrans-split-41
  prefs: []
  type: TYPE_NORMAL
  zh: ä¾‹å¦‚ï¼Œåœ¨è°ƒç”¨`read`æ—¶ï¼Œå†…æ ¸ä¼šç­‰å¾…ä»ç½‘ç»œæ¥å£å¡æˆ–`NIC`æ¥æ”¶åˆ°ç½‘ç»œæ•°æ®åŒ…ã€‚CPU å¯ä»¥åœ¨æ­¤æœŸé—´è¿è¡Œå…¶ä»–ä»»åŠ¡ã€‚
- en: In linux, you can mark a socket as non-blocking by either using `ioctl(fd, FIONBIO)`
    or `fcntl & O_NONBLOCK` (posix). A `read` call on that same socket will return
    immediately. If there's a packet written by the `NIC` we haven't read yet, `read`
    will copy the buffer like usual, otherwise it will return an error, with `errno`
    equal to `EWOULDBLOCK`.
  id: totrans-split-42
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ Linux ä¸­ï¼Œæ‚¨å¯ä»¥é€šè¿‡ä½¿ç”¨`ioctl(fd, FIONBIO)`æˆ–`fcntl & O_NONBLOCK`ï¼ˆPOSIXï¼‰å°†å¥—æ¥å­—æ ‡è®°ä¸ºéé˜»å¡ã€‚å¯¹åŒä¸€å¥—æ¥å­—çš„`read`è°ƒç”¨å°†ç«‹å³è¿”å›ã€‚å¦‚æœ`NIC`å†™å…¥äº†æˆ‘ä»¬å°šæœªè¯»å–çš„æ•°æ®åŒ…ï¼Œ`read`å°†åƒå¾€å¸¸ä¸€æ ·å¤åˆ¶ç¼“å†²åŒºï¼Œå¦åˆ™å°†è¿”å›é”™è¯¯ï¼Œ`errno`ç­‰äº`EWOULDBLOCK`ã€‚
- en: 'Let''s patch `echod` to be single threaded again, but this time, supporting
    multiple concurrent clients using non-blocking sockets:'
  id: totrans-split-43
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬å†æ¬¡å°†`echod`ä¿®è¡¥ä¸ºå•çº¿ç¨‹ï¼Œä½†è¿™æ¬¡ä½¿ç”¨éé˜»å¡å¥—æ¥å­—æ”¯æŒå¤šä¸ªå¹¶å‘å®¢æˆ·ç«¯ï¼š
- en: '[PRE3]'
  id: totrans-split-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: A bit lengthy, don't try to understand everything, just that we are dealing
    with a lot of different tasks "at once". For a compilable version, click [here](https://github.com/tontinton/echod-hog/blob/master/main.c).
  id: totrans-split-45
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: æœ‰ç‚¹å†—é•¿ï¼Œä¸è¦è¯•å›¾ç†è§£ä¸€åˆ‡ï¼Œåªéœ€çŸ¥é“æˆ‘ä»¬åŒæ—¶å¤„ç†è®¸å¤šä¸åŒçš„ä»»åŠ¡ã€‚è¦è·å–å¯ç¼–è¯‘ç‰ˆæœ¬ï¼Œè¯·ç‚¹å‡»[è¿™é‡Œ](https://github.com/tontinton/echod-hog/blob/master/main.c)ã€‚
- en: The main problem with this solution is that we are now always busy doing something,
    the CPU runs at 100%, even when most loop iterations will result in `EWOULDBLOCK`.
  id: totrans-split-46
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ç§è§£å†³æ–¹æ¡ˆçš„ä¸»è¦é—®é¢˜æ˜¯æˆ‘ä»¬ç°åœ¨æ€»æ˜¯åœ¨å¿™äºåšæŸäº‹ï¼ŒCPU çš„åˆ©ç”¨ç‡ä¸º 100%ï¼Œå³ä½¿å¤§å¤šæ•°å¾ªç¯è¿­ä»£å°†å¯¼è‡´`EWOULDBLOCK`ã€‚
- en: Another problem is code complexity, we are now prohibited from running code
    that will block, to not block our entire server application.
  id: totrans-split-47
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: å¦ä¸€ä¸ªé—®é¢˜æ˜¯ä»£ç å¤æ‚æ€§ï¼Œç°åœ¨æˆ‘ä»¬è¢«ç¦æ­¢è¿è¡Œå¯èƒ½ä¼šé˜»å¡æ•´ä¸ªæœåŠ¡å™¨åº”ç”¨ç¨‹åºçš„ä»£ç ã€‚
- en: What we really want is to sleep when we have nothing *useful* to do. I.e. When
    there is no client waiting to connect, no client has sent any packet and we can't
    yet send a packet to the client for whatever reason (maybe the client is busy
    doing something of its own).
  id: totrans-split-48
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬çœŸæ­£æƒ³è¦çš„æ˜¯åœ¨æ²¡æœ‰æœ‰ç”¨äº‹æƒ…å¯åšæ—¶ä¼‘çœ ã€‚å³å½“æ²¡æœ‰å®¢æˆ·ç«¯ç­‰å¾…è¿æ¥ï¼Œæ²¡æœ‰å®¢æˆ·ç«¯å‘é€ä»»ä½•æ•°æ®åŒ…ï¼Œå¹¶ä¸”ç”±äºæŸç§åŸå› ï¼ˆä¹Ÿè®¸å®¢æˆ·ç«¯æ­£åœ¨å¿™äºè‡ªå·±çš„äº‹æƒ…ï¼‰ï¼Œæˆ‘ä»¬å°šæœªèƒ½å¤Ÿå‘å®¢æˆ·ç«¯å‘é€æ•°æ®åŒ…æ—¶ã€‚
- en: 'The reasons we want this are:'
  id: totrans-split-49
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å¸Œæœ›è¿™æ ·åšçš„åŸå› æœ‰ï¼š
- en: To be good neighbours to other applications running on the same machine, and
    not take CPU cycles they might want to utilize.
  id: totrans-split-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä¸ºäº†æˆä¸ºåœ¨åŒä¸€å°æœºå™¨ä¸Šè¿è¡Œçš„å…¶ä»–åº”ç”¨ç¨‹åºçš„è‰¯å¥½é‚»å±…ï¼Œå¹¶ä¸”ä¸å ç”¨å®ƒä»¬å¯èƒ½æƒ³è¦åˆ©ç”¨çš„ CPU å¾ªç¯ã€‚
- en: 'The more the CPU runs, the more energy it takes:'
  id: totrans-split-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: CPU è¿è¡Œå¾—è¶Šå¤šï¼Œæ¶ˆè€—çš„èƒ½é‡å°±è¶Šå¤šï¼š
- en: Worse battery life.
  id: totrans-split-52
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: ç”µæ± å¯¿å‘½æ›´çŸ­ã€‚
- en: More expensive.
  id: totrans-split-53
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: æ›´æ˜‚è´µã€‚
- en: Less environmental friendly ğŸŒ²ğŸŒ³ğŸŒ¿
  id: totrans-split-54
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: æ›´å°‘ç¯ä¿ ğŸŒ²ğŸŒ³ğŸŒ¿
- en: 'Good news though, most operating systems provide an API to do just that. Maybe
    even too many APIs:'
  id: totrans-split-55
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸è¿‡å¥½æ¶ˆæ¯æ˜¯ï¼Œå¤§å¤šæ•°æ“ä½œç³»ç»Ÿæä¾›äº†é€‚åˆæ­¤ç±»æ“ä½œçš„ APIã€‚ç”šè‡³å¯èƒ½æä¾›äº†å¤ªå¤šçš„ APIï¼š
- en: '**select(2)** - A posix API. The man page is excellent, so let''s copy the
    important bits:'
  id: totrans-split-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**select(2)** - ä¸€ä¸ª POSIX APIã€‚æ‰‹å†Œéå¸¸æ£’ï¼Œè®©æˆ‘ä»¬å¤åˆ¶é‡è¦éƒ¨åˆ†ï¼š'
- en: '[PRE4]'
  id: totrans-split-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Main takeaway is that select is limited to `FD_SETSIZE` number of fds to monitor,
    which is usually 1024 (glibc). Another thing to note is that when a FD becomes
    ready, it scans all its registered FDs (`O(n)`), so when you have a lot of FDs,
    you can spend a lot of time just on this.
  id: totrans-split-58
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ä¸»è¦çš„è¦ç‚¹æ˜¯selectåªèƒ½ç›‘è§†`FD_SETSIZE`æ•°é‡çš„FDsï¼Œé€šå¸¸ä¸º1024ï¼ˆglibcï¼‰ã€‚å¦ä¸€ä¸ªéœ€è¦æ³¨æ„çš„æ˜¯ï¼Œå½“FDå‡†å¤‡å°±ç»ªæ—¶ï¼Œå®ƒä¼šæ‰«ææ‰€æœ‰æ³¨å†Œçš„FDsï¼ˆ`O(n)`ï¼‰ï¼Œå› æ­¤å½“ä½ æœ‰å¤§é‡çš„FDsæ—¶ï¼Œå¯èƒ½ä¼šèŠ±è´¹å¤§é‡æ—¶é—´åœ¨è¿™ä¸Šé¢ã€‚
- en: '**poll(2)** - A posix API. Not limited to `FD_SETSIZE` but still `O(n)` like
    `select`.'
  id: totrans-split-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**poll(2)** - ä¸€ä¸ªposix APIã€‚ä¸é™äº`FD_SETSIZE`ï¼Œä½†ä»ç„¶åƒ`select`ä¸€æ ·æ˜¯`O(n)`ã€‚'
- en: '**epoll(7)** - A linux API. A non portable `poll` but at least it scales better
    as it''s `O(1)`.'
  id: totrans-split-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**epoll(7)** - ä¸€ä¸ªLinux APIã€‚éå¯ç§»æ¤çš„`poll`ï¼Œä½†è‡³å°‘å®ƒçš„æ‰©å±•æ€§æ›´å¥½ï¼Œå› ä¸ºå®ƒæ˜¯`O(1)`çš„ã€‚'
- en: '**aio(7)** - A linux API. Unlike previous APIs, it supports both sockets and
    files, but with some major disadvantages:'
  id: totrans-split-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**aio(7)** - ä¸€ä¸ªLinux APIã€‚ä¸ä¹‹å‰çš„APIä¸åŒï¼Œå®ƒæ”¯æŒå¥—æ¥å­—å’Œæ–‡ä»¶ï¼Œä½†æœ‰ä¸€äº›ä¸»è¦çš„ç¼ºç‚¹ï¼š'
- en: Supports only files opened with `O_DIRECT`, which are complex to work with.
  id: totrans-split-62
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä»…æ”¯æŒä½¿ç”¨`O_DIRECT`æ‰“å¼€çš„æ–‡ä»¶ï¼Œè¿™äº›æ–‡ä»¶çš„å¤„ç†éå¸¸å¤æ‚ã€‚
- en: Blocks if file metadata isn't available until it becomes available.
  id: totrans-split-63
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: å¦‚æœæ–‡ä»¶çš„å…ƒæ•°æ®ä¸å¯ç”¨ï¼Œä¼šé˜»å¡ç›´åˆ°å¯ç”¨ã€‚
- en: Blocks when the storage device is out of request slots (each storage device
    has a fixed number of slots).
  id: totrans-split-64
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: å½“å­˜å‚¨è®¾å¤‡çš„è¯·æ±‚æ§½ä½ç”¨å®Œæ—¶ä¼šé˜»å¡ï¼ˆæ¯ä¸ªå­˜å‚¨è®¾å¤‡éƒ½æœ‰å›ºå®šæ•°é‡çš„æ§½ä½ï¼‰ã€‚
- en: Each IO submission copies 72 bytes and each completion copies 32 bytes. 104
    bytes copied for each IO operation using 2 syscalls.
  id: totrans-split-65
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: æ¯æ¬¡IOæäº¤éƒ½ä¼šå¤åˆ¶72å­—èŠ‚ï¼Œæ¯æ¬¡å®Œæˆéƒ½ä¼šå¤åˆ¶32å­—èŠ‚ã€‚æ¯æ¬¡IOæ“ä½œä½¿ç”¨2ä¸ªç³»ç»Ÿè°ƒç”¨å¤åˆ¶104å­—èŠ‚ã€‚
- en: '**io_uring(7)** - A linux API (since 5.1). Like `aio`, it unifies disk and
    network operations under a single API, but without `aio`''s shortcomings. It is
    designed to be fast, creating 2 queues that live in shared memory (between user
    and kernel space), one for submission of I/O operations, the other is populated
    with the results of the I/O operations once they are ready. For more info head
    over to ["What is io_uring?"](https://unixism.net/loti/what_is_io_uring.html).'
  id: totrans-split-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**io_uring(7)** - è‡ªLinux 5.1èµ·çš„ä¸€ä¸ªLinux APIã€‚åƒ`aio`ä¸€æ ·ï¼Œå®ƒå°†ç£ç›˜å’Œç½‘ç»œæ“ä½œç»Ÿä¸€åˆ°ä¸€ä¸ªAPIä¸‹ï¼Œä½†æ²¡æœ‰`aio`çš„ç¼ºç‚¹ã€‚å®ƒè¢«è®¾è®¡ä¸ºå¿«é€Ÿçš„ï¼Œåˆ›å»ºäº†ä¸¤ä¸ªé˜Ÿåˆ—ï¼Œè¿™ä¸¤ä¸ªé˜Ÿåˆ—å­˜åœ¨äºå…±äº«å†…å­˜ä¸­ï¼ˆç”¨æˆ·ç©ºé—´å’Œå†…æ ¸ç©ºé—´ä¹‹é—´ï¼‰ï¼Œä¸€ä¸ªç”¨äºæäº¤I/Oæ“ä½œï¼Œå¦ä¸€ä¸ªç”¨äºå­˜æ”¾I/Oæ“ä½œçš„ç»“æœä¸€æ—¦å‡†å¤‡å¥½ã€‚æ›´å¤šä¿¡æ¯è¯·è®¿é—®["ä»€ä¹ˆæ˜¯io_uringï¼Ÿ"](https://unixism.net/loti/what_is_io_uring.html)ã€‚'
- en: '`ScyllaDB` have successfully implemented their database using `aio` in [seastar](https://seastar.io/),
    you can read more on async disk I/O on [their blog](https://scylladb.com/2017/10/05/io-access-methods-scylla/).'
  id: totrans-split-67
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '`ScyllaDB`æˆåŠŸåœ°åœ¨[seastar](https://seastar.io/)ä¸­ä½¿ç”¨`aio`å®ç°äº†ä»–ä»¬çš„æ•°æ®åº“ï¼Œæ‚¨å¯ä»¥åœ¨[ä»–ä»¬çš„åšå®¢](https://scylladb.com/2017/10/05/io-access-methods-scylla/)ä¸Šäº†è§£æ›´å¤šå…³äºå¼‚æ­¥ç£ç›˜I/Oçš„ä¿¡æ¯ã€‚'
- en: If you're interested about platforms other than linux, windows has [I/O Completion
    Ports](https://learn.microsoft.com/en-us/windows/win32/fileio/i-o-completion-ports),
    with FreeBSD and MacOS both using [kqueue](https://man.freebsd.org/cgi/man.cgi?kqueue).
  id: totrans-split-68
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: å¦‚æœæ‚¨å¯¹é™¤äº†Linuxä»¥å¤–çš„å¹³å°æ„Ÿå…´è¶£ï¼ŒWindowsæœ‰[I/O Completion Ports](https://learn.microsoft.com/en-us/windows/win32/fileio/i-o-completion-ports)ï¼ŒFreeBSDå’ŒMacOSéƒ½ä½¿ç”¨[kqueue](https://man.freebsd.org/cgi/man.cgi?kqueue)ã€‚
- en: Prior to `io_uring`, async I/O abstraction libraries used a thread pool to run
    disk I/O in a non-blocking manner.
  id: totrans-split-69
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨`io_uring`ä¹‹å‰ï¼Œå¼‚æ­¥I/OæŠ½è±¡åº“ä½¿ç”¨çº¿ç¨‹æ± ä»¥éé˜»å¡æ–¹å¼è¿è¡Œç£ç›˜I/Oã€‚
- en: There are libraries like [libuv](https://libuv.org/) (what powers Node.js) that
    you can use to run highly concurrent servers while using just a single thread
    (they finally changed it to [use io_uring](https://github.com/libuv/libuv/pull/3952)).
    These kind of libraries are often called `Event Loops`, let's talk about them
    a bit.
  id: totrans-split-70
  prefs: []
  type: TYPE_NORMAL
  zh: ç±»ä¼¼äº[libuv](https://libuv.org/)ï¼ˆNode.jsçš„åŠ¨åŠ›æ¥æºï¼‰çš„åº“å¯ä»¥åœ¨ä»…ä½¿ç”¨ä¸€ä¸ªçº¿ç¨‹çš„æƒ…å†µä¸‹è¿è¡Œé«˜å¹¶å‘æœåŠ¡å™¨ï¼ˆæœ€ç»ˆæ”¹ä¸º[ä½¿ç”¨
    io_uring](https://github.com/libuv/libuv/pull/3952)ï¼‰ã€‚è¿™ç±»åº“é€šå¸¸è¢«ç§°ä¸º`äº‹ä»¶å¾ªç¯`ï¼Œè®©æˆ‘ä»¬ç¨å¾®è°ˆè°ˆå®ƒä»¬ã€‚
- en: Event Loop
  id: totrans-split-71
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: äº‹ä»¶å¾ªç¯
- en: 'At its essence an event loop (also sometimes called [reactor pattern](https://en.wikipedia.org/wiki/Reactor_pattern))
    is basically this:'
  id: totrans-split-72
  prefs: []
  type: TYPE_NORMAL
  zh: æœ¬è´¨ä¸Šï¼Œäº‹ä»¶å¾ªç¯ï¼ˆæœ‰æ—¶ä¹Ÿç§°ä¸º[ååº”å™¨æ¨¡å¼](https://en.wikipedia.org/wiki/Reactor_pattern)ï¼‰åŸºæœ¬ä¸Šæ˜¯è¿™æ ·çš„ï¼š
- en: '[PRE5]'
  id: totrans-split-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Yeah, that''s it, just look at `libuv`''s `uv_run`:'
  id: totrans-split-74
  prefs: []
  type: TYPE_NORMAL
  zh: æ˜¯çš„ï¼Œå°±æ˜¯è¿™æ ·ï¼Œçœ‹çœ‹`libuv`çš„`uv_run`ï¼š
- en: '[PRE6]'
  id: totrans-split-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'And here''s `uv__run_pending` without omitting any details:'
  id: totrans-split-76
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™é‡Œæ˜¯`uv__run_pending`ï¼Œä¸é—æ¼ä»»ä½•ç»†èŠ‚ï¼š
- en: '[PRE7]'
  id: totrans-split-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Pretty awesome huh? This is what all Node.js applications are running on.
  id: totrans-split-78
  prefs: []
  type: TYPE_NORMAL
  zh: ç›¸å½“ä»¤äººå°è±¡æ·±åˆ»ï¼Œå¯¹å§ï¼Ÿè¿™å°±æ˜¯æ‰€æœ‰Node.jsåº”ç”¨ç¨‹åºæ­£åœ¨è¿è¡Œçš„å†…å®¹ã€‚
- en: What if you need to call a blocking 3rd party library function? For that, most
    event loop libraries have a thread pool you can run arbitrary code on, for example
    in `libuv`, you can use [uv_queue_work()](https://docs.libuv.org/en/v1.x/threadpool.html).
  id: totrans-split-79
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœéœ€è¦è°ƒç”¨é˜»å¡çš„ç¬¬ä¸‰æ–¹åº“å‡½æ•°æ€ä¹ˆåŠï¼Ÿä¸ºæ­¤ï¼Œå¤§å¤šæ•°äº‹ä»¶å¾ªç¯åº“éƒ½æœ‰ä¸€ä¸ªçº¿ç¨‹æ± ï¼Œæ‚¨å¯ä»¥åœ¨å…¶ä¸­è¿è¡Œä»»æ„ä»£ç ï¼Œä¾‹å¦‚åœ¨`libuv`ä¸­ï¼Œæ‚¨å¯ä»¥ä½¿ç”¨[uv_queue_work()](https://docs.libuv.org/en/v1.x/threadpool.html)ã€‚
- en: 'Pop quiz: how would something like `setTimeout` be implemented in an event
    loop? If nothing comes to mind, try cloning `libuv` and reading the implementation
    of `uv__run_timers` in `src/timer.c`.'
  id: totrans-split-80
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: è€ƒéªŒæ—¶é—´ï¼šåƒ`setTimeout`è¿™æ ·çš„ä¸œè¥¿åœ¨äº‹ä»¶å¾ªç¯ä¸­æ˜¯å¦‚ä½•å®ç°çš„ï¼Ÿå¦‚æœè„‘æµ·ä¸­ä»€ä¹ˆä¹Ÿæ²¡æœ‰ï¼Œè¯·å°è¯•å…‹éš†`libuv`å¹¶é˜…è¯»`src/timer.c`ä¸­`uv__run_timers`çš„å®ç°ã€‚
- en: Event Driven Development
  id: totrans-split-81
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: äº‹ä»¶é©±åŠ¨å¼€å‘
- en: The programming model when using an event loop is inherently event driven, with
    each registered event having a callback to execute once it is ready.
  id: totrans-split-82
  prefs: []
  type: TYPE_NORMAL
  zh: ä½¿ç”¨äº‹ä»¶å¾ªç¯æ—¶çš„ç¼–ç¨‹æ¨¡å‹æœ¬è´¨ä¸Šæ˜¯äº‹ä»¶é©±åŠ¨çš„ï¼Œæ¯ä¸ªæ³¨å†Œäº‹ä»¶éƒ½æœ‰ä¸€ä¸ªå‡†å¤‡å¥½æ—¶æ‰§è¡Œçš„å›è°ƒå‡½æ•°ã€‚
- en: 'Let''s see how `echod` would look like using an imaginary event loop library
    instead (start with `serve` from the bottom):'
  id: totrans-split-83
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬çœ‹çœ‹ä½¿ç”¨è™šæ„çš„äº‹ä»¶å¾ªç¯åº“æ—¶`echod`ä¼šæ˜¯ä»€ä¹ˆæ ·å­ï¼ˆä»åº•éƒ¨çš„`serve`å¼€å§‹ï¼‰ï¼š
- en: '[PRE8]'
  id: totrans-split-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: If you worked with javascript before, this should look familiar to you.
  id: totrans-split-85
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœä½ ä¹‹å‰ä½¿ç”¨è¿‡ JavaScriptï¼Œè¿™çœ‹èµ·æ¥åº”è¯¥å¾ˆç†Ÿæ‚‰ã€‚
- en: 'The code will be very lightweight in terms of performance, and highly concurrent,
    but is often criticized for being:'
  id: totrans-split-86
  prefs: []
  type: TYPE_NORMAL
  zh: ä»æ€§èƒ½ä¸Šæ¥è¯´ï¼Œä»£ç å°†éå¸¸è½»é‡åŒ–ï¼Œå¹¶ä¸”é«˜åº¦å¹¶å‘ï¼Œä½†é€šå¸¸å› ä»¥ä¸‹åŸå› è€Œå—åˆ°æ‰¹è¯„ï¼š
- en: '**Not intuitive** - Complex for most programmers who are used to reading and
    writing synchronous code, see ["Callback Hell"](http://callbackhell.com).'
  id: totrans-split-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**ä¸ç›´è§‚** - å¯¹äºå¤§å¤šæ•°ä¹ æƒ¯äºé˜…è¯»å’Œç¼–å†™åŒæ­¥ä»£ç çš„ç¨‹åºå‘˜è€Œè¨€ï¼Œçœ‹èµ·æ¥å¾ˆå¤æ‚ï¼Œå‚è§["å›è°ƒåœ°ç‹±"](http://callbackhell.com)ã€‚'
- en: '**Hard to debug** - The call stack is very short and will not show you the
    flow of how you got to a specific breakpoint. The caller of each callback will
    always be `uv_run` in `libuv`, or `run_event_loop` in our example.'
  id: totrans-split-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**éš¾ä»¥è°ƒè¯•** - è°ƒç”¨å †æ ˆéå¸¸çŸ­ï¼Œä¸ä¼šæ˜¾ç¤ºä½ å¦‚ä½•åˆ°è¾¾ç‰¹å®šæ–­ç‚¹çš„æµç¨‹ã€‚æ¯ä¸ªå›è°ƒçš„è°ƒç”¨è€…å§‹ç»ˆæ˜¯`libuv`ä¸­çš„`uv_run`ï¼Œæˆ–è€…æˆ‘ä»¬ç¤ºä¾‹ä¸­çš„`run_event_loop`ã€‚'
- en: A lot of modern programming languages and runtimes try to solve these problems
    by letting you write code that looks synchronous while being fully asynchronous.
    In the next chapter, we're gonna learn how.
  id: totrans-split-89
  prefs: []
  type: TYPE_NORMAL
  zh: è®¸å¤šç°ä»£ç¼–ç¨‹è¯­è¨€å’Œè¿è¡Œæ—¶è¯•å›¾é€šè¿‡è®©ä½ ç¼–å†™çœ‹èµ·æ¥æ˜¯åŒæ­¥çš„ä»£ç ï¼ŒåŒæ—¶å®é™…ä¸Šæ˜¯å®Œå…¨å¼‚æ­¥çš„æ¥è§£å†³è¿™äº›é—®é¢˜ã€‚åœ¨ä¸‹ä¸€ç« ä¸­ï¼Œæˆ‘ä»¬å°†å­¦ä¹ å¦‚ä½•å®ç°ã€‚
- en: Preemption
  id: totrans-split-90
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: æŠ¢å 
- en: Have you ever wondered how is it that you can run more than 1 thread on a computer
    with just a single CPU core?
  id: totrans-split-91
  prefs: []
  type: TYPE_NORMAL
  zh: ä½ æœ‰æ²¡æœ‰æƒ³è¿‡ï¼Œåªæœ‰ä¸€ä¸ªCPUæ ¸å¿ƒçš„è®¡ç®—æœºä¸Šå¦‚ä½•è¿è¡Œå¤šä¸ªçº¿ç¨‹ï¼Ÿ
- en: In this section, we'll go over the secret technique that enables this magic,
    called **preemption**.
  id: totrans-split-92
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æœ¬èŠ‚ä¸­ï¼Œæˆ‘ä»¬å°†ä»‹ç»ä¸€ç§ç§°ä¸º**æŠ¢å **çš„ç§˜å¯†æŠ€æœ¯ï¼Œå®ƒä½¿è¿™ç§é­”æœ¯æˆä¸ºå¯èƒ½ã€‚
- en: 'Let''s say we have the following two tasks we would like to execute concurrently:'
  id: totrans-split-93
  prefs: []
  type: TYPE_NORMAL
  zh: å‡è®¾æˆ‘ä»¬è¦åŒæ—¶æ‰§è¡Œä»¥ä¸‹ä¸¤ä¸ªä»»åŠ¡ï¼š
- en: '[PRE9]'
  id: totrans-split-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: What should `run_all` do to make sure that both tasks run **concurrently**?
  id: totrans-split-95
  prefs: []
  type: TYPE_NORMAL
  zh: '`run_all`åº”è¯¥åšä»€ä¹ˆä»¥ç¡®ä¿è¿™ä¸¤ä¸ªä»»åŠ¡**å¹¶å‘**è¿è¡Œï¼Ÿ'
- en: If we had 2 CPU cores, we could have simply run 1 task on 1 core, which would
    mean we would run the two tasks **parallelly**, or in other words the two tasks
    run at the same time in the *real* world (at least the one we base physics on).
  id: totrans-split-96
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœæˆ‘ä»¬æœ‰ä¸¤ä¸ªCPUæ ¸å¿ƒï¼Œæˆ‘ä»¬å¯ä»¥ç®€å•åœ°åœ¨ä¸€ä¸ªæ ¸å¿ƒä¸Šè¿è¡Œä¸€ä¸ªä»»åŠ¡ï¼Œè¿™æ„å‘³ç€æˆ‘ä»¬å¯ä»¥**å¹¶è¡Œ**è¿è¡Œä¸¤ä¸ªä»»åŠ¡ï¼Œæˆ–è€…æ¢å¥è¯è¯´ï¼Œåœ¨*çœŸå®*ä¸–ç•Œä¸­ï¼ˆè‡³å°‘æ˜¯æˆ‘ä»¬åŸºäºç‰©ç†å­¦çš„ä¸–ç•Œï¼‰è¿™ä¸¤ä¸ªä»»åŠ¡åŒæ—¶è¿è¡Œã€‚
- en: A **concurrent** program deals with running multiple things at once, just not
    at the same time, thus they may seem **parallel** even if they're really not.
  id: totrans-split-97
  prefs: []
  type: TYPE_NORMAL
  zh: '**å¹¶å‘**ç¨‹åºå¤„ç†åŒæ—¶è¿è¡Œå¤šä¸ªäº‹ç‰©ï¼Œä½†å¹¶éåŒæ—¶è¿›è¡Œï¼Œå› æ­¤å®ƒä»¬å¯èƒ½çœ‹èµ·æ¥æ˜¯**å¹¶è¡Œ**çš„ï¼Œå³ä½¿å®é™…ä¸Šå¹¶éå¦‚æ­¤ã€‚'
- en: One way `run_all` can achieve **concurrency** is by running 1 task for some
    amount of time, pause, and then resume running the next task, forever in a loop
    until all tasks exit for example. To magically make it appear as if it's **parallel**,
    you simply need to configure the amount of time before pausing to be really small
    relative to the human experience (e.g. 100Î¼s?).
  id: totrans-split-98
  prefs: []
  type: TYPE_NORMAL
  zh: '`run_all`å®ç°**å¹¶å‘**çš„ä¸€ç§æ–¹å¼æ˜¯åœ¨æŸæ®µæ—¶é—´å†…è¿è¡Œä¸€ä¸ªä»»åŠ¡ï¼Œæš‚åœï¼Œç„¶åç»§ç»­è¿è¡Œä¸‹ä¸€ä¸ªä»»åŠ¡ï¼Œå¹¶ä¸”æ°¸è¿œå¾ªç¯ï¼Œç›´åˆ°æ‰€æœ‰ä»»åŠ¡é€€å‡ºä¸ºæ­¢ã€‚ä¸ºäº†åƒ**å¹¶è¡Œ**ä¸€æ ·ç¥å¥‡åœ°å±•ç°å‡ºæ¥ï¼Œä½ åªéœ€é…ç½®æš‚åœä¹‹å‰çš„æ—¶é—´ç›¸å¯¹äºäººç±»ç»éªŒæ¥è¯´éå¸¸çŸ­ï¼ˆä¾‹å¦‚100Î¼sï¼Ÿï¼‰å³å¯ã€‚'
- en: '[PRE10]'
  id: totrans-split-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: But how do you pause and resume execution of code?
  id: totrans-split-100
  prefs: []
  type: TYPE_NORMAL
  zh: é‚£ä¹ˆï¼Œå¦‚ä½•æš‚åœå’Œæ¢å¤ä»£ç çš„æ‰§è¡Œå‘¢ï¼Ÿ
- en: The answer lies in programs being deterministic state machines, as long as you
    give a program's executor (e.g. CPU for native code) the same inputs (e.g. registers,
    memory, etc...), it doesn't matter if it executes today or in a few years, the
    output will be the same.
  id: totrans-split-101
  prefs: []
  type: TYPE_NORMAL
  zh: ç­”æ¡ˆåœ¨äºç¨‹åºæ˜¯ç¡®å®šæ€§çŠ¶æ€æœºï¼Œåªè¦å‘ç¨‹åºçš„æ‰§è¡Œè€…ï¼ˆä¾‹å¦‚æœ¬æœºä»£ç çš„CPUï¼‰æä¾›ç›¸åŒçš„è¾“å…¥ï¼ˆä¾‹å¦‚å¯„å­˜å™¨ã€å†…å­˜ç­‰ï¼‰ï¼Œå®ƒæ— è®ºæ˜¯ä»Šå¤©æ‰§è¡Œè¿˜æ˜¯å‡ å¹´åæ‰§è¡Œï¼Œè¾“å‡ºéƒ½å°†ç›¸åŒã€‚
- en: Basically, pausing a task can be implemented as copying the current state of
    the program, and resuming a task can be implemented by loading that saved state.
  id: totrans-split-102
  prefs: []
  type: TYPE_NORMAL
  zh: åŸºæœ¬ä¸Šï¼Œæš‚åœä»»åŠ¡å¯ä»¥é€šè¿‡å¤åˆ¶ç¨‹åºçš„å½“å‰çŠ¶æ€æ¥å®ç°ï¼Œæ¢å¤ä»»åŠ¡å¯ä»¥é€šè¿‡åŠ è½½ä¿å­˜çš„çŠ¶æ€æ¥å®ç°ã€‚
- en: It doesn't matter if the program runs on a real CPU or a virtual one like in
    `python`'s bytecode or on the `JVM` for example, they are all deterministic state
    machines. As long as you copy all the necessary state, the task will resume as
    if it was never even paused.
  id: totrans-split-103
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ç¨‹åºè¿è¡Œåœ¨çœŸå® CPU ä¸Šæˆ–åƒ `python` çš„å­—èŠ‚ç æˆ– `JVM` è¿™æ ·çš„è™šæ‹Ÿ CPU ä¸Šéƒ½æ²¡æœ‰å…³ç³»ï¼Œå®ƒä»¬éƒ½æ˜¯ç¡®å®šæ€§çŠ¶æ€æœºã€‚åªè¦å¤åˆ¶æ‰€æœ‰å¿…è¦çš„çŠ¶æ€ï¼Œä»»åŠ¡å°†ä¼šç»§ç»­è¿›è¡Œï¼Œå°±åƒä»æœªæš‚åœè¿‡ä¸€æ ·ã€‚
- en: To make it easy to understand how you might implement preemption (saving and
    loading of program state), let's look at `setjmp.h`, which implements saving and
    loading program state in a lot of different CPU architectures, and is part of
    `libc`.
  id: totrans-split-104
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†ä¾¿äºç†è§£å¦‚ä½•å®ç°æŠ¢å ï¼ˆä¿å­˜å’ŒåŠ è½½ç¨‹åºçŠ¶æ€ï¼‰ï¼Œè®©æˆ‘ä»¬çœ‹çœ‹ `setjmp.h`ï¼Œå®ƒåœ¨è®¸å¤šä¸åŒçš„ CPU æ¶æ„ä¸­å®ç°äº†ä¿å­˜å’ŒåŠ è½½ç¨‹åºçŠ¶æ€ï¼Œå¹¶ä¸”æ˜¯ `libc`
    çš„ä¸€éƒ¨åˆ†ã€‚
- en: 'See the following example (copied directly from [wikipedia](https://en.wikipedia.org/wiki/Setjmp.h)):'
  id: totrans-split-105
  prefs: []
  type: TYPE_NORMAL
  zh: å‚è§ä»¥ä¸‹ç¤ºä¾‹ï¼ˆç›´æ¥ä» [wikipedia](https://en.wikipedia.org/wiki/Setjmp.h) å¤åˆ¶ï¼‰ï¼š
- en: '[PRE11]'
  id: totrans-split-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Running it will output:'
  id: totrans-split-107
  prefs: []
  type: TYPE_NORMAL
  zh: è¿è¡Œå®ƒå°†è¾“å‡ºï¼š
- en: '[PRE12]'
  id: totrans-split-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '`setjmp` saves the program state (in `buf`), and `longjmp` loads whatever is
    in `buf` to the CPU.'
  id: totrans-split-109
  prefs: []
  type: TYPE_NORMAL
  zh: '`setjmp` ä¿å­˜ç¨‹åºçŠ¶æ€ï¼ˆåœ¨ `buf` ä¸­ï¼‰ï¼Œè€Œ `longjmp` å°† `buf` ä¸­çš„ä»»ä½•å†…å®¹åŠ è½½åˆ° CPU ä¸­ã€‚'
- en: 'Let''s look behind the curtains, the following is the `x86_64` assembly code
    for `setjmp` and `longjmp` in [musl](https://musl.libc.org/) (a popular `libc`
    implementation):'
  id: totrans-split-110
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬çœ‹çœ‹å¹•åçš„æƒ…å†µï¼Œä»¥ä¸‹æ˜¯ `musl`ï¼ˆä¸€ä¸ªæµè¡Œçš„ `libc` å®ç°ï¼‰ä¸­ `x86_64` æ±‡ç¼–ä»£ç çš„ `setjmp` å’Œ `longjmp`ï¼š
- en: '[PRE13]'
  id: totrans-split-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Don't stress it if you don't understand assembly. The point is that saving and
    loading program state is pretty short and simple.
  id: totrans-split-112
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: å¦‚æœä½ ä¸ç†è§£æ±‡ç¼–è¯­è¨€ä¹Ÿä¸è¦ç´§ã€‚å…³é”®æ˜¯ä¿å­˜å’ŒåŠ è½½ç¨‹åºçŠ¶æ€éå¸¸ç®€çŸ­å’Œç®€å•ã€‚
- en: '`setjmp` saves all callee-saved registers into `jmp_buf`. Callee-saved registers
    are registers used to hold long-lived values that should be preserved across function
    calls. `longjmp` restores the callee-saved registers stored inside a `jmp_buf`
    directly to the CPU registers.'
  id: totrans-split-113
  prefs: []
  type: TYPE_NORMAL
  zh: '`setjmp` å°†æ‰€æœ‰è¢«è°ƒç”¨ä¿å­˜çš„å¯„å­˜å™¨ä¿å­˜åˆ° `jmp_buf` ä¸­ã€‚è¢«è°ƒç”¨ä¿å­˜çš„å¯„å­˜å™¨ç”¨äºä¿å­˜é•¿æœŸå­˜åœ¨çš„å€¼ï¼Œåœ¨å‡½æ•°è°ƒç”¨ä¹‹é—´åº”è¯¥ä¿ç•™ã€‚`longjmp`
    ç›´æ¥å°†ä¿å­˜åœ¨ `jmp_buf` ä¸­çš„è¢«è°ƒç”¨ä¿å­˜çš„å¯„å­˜å™¨æ¢å¤åˆ° CPU å¯„å­˜å™¨ä¸­ã€‚'
- en: To the curious, the reason caller-saved registers (like `rcx` for example) are
    not saved, is because to the compiler `setjmp` is just another function call,
    meaning it will not use caller-saved registers to hold state. It assumes just
    like with any function call, that these registers might be changed.
  id: totrans-split-114
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: å¯¹äºå¥½å¥‡çš„äººï¼Œä¸ä¿å­˜è°ƒç”¨è€…ä¿å­˜çš„å¯„å­˜å™¨ï¼ˆä¾‹å¦‚ `rcx`ï¼‰çš„åŸå› æ˜¯å› ä¸ºå¯¹ç¼–è¯‘å™¨è€Œè¨€ `setjmp` åªæ˜¯å¦ä¸€ä¸ªå‡½æ•°è°ƒç”¨ï¼Œè¿™æ„å‘³ç€å®ƒä¸ä¼šä½¿ç”¨è°ƒç”¨è€…ä¿å­˜çš„å¯„å­˜å™¨æ¥ä¿å­˜çŠ¶æ€ã€‚å®ƒå‡å®šåƒä»»ä½•å‡½æ•°è°ƒç”¨ä¸€æ ·ï¼Œè¿™äº›å¯„å­˜å™¨å¯èƒ½ä¼šè¢«ä¿®æ”¹ã€‚
- en: Non-Preemptive Schedulers
  id: totrans-split-115
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: éæŠ¢å è°ƒåº¦ç¨‹åº
- en: Already, we have a solid foundation to start running multiple tasks concurrently.
  id: totrans-split-116
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨ï¼Œæˆ‘ä»¬å·²ç»æœ‰äº†ä¸€ä¸ªåšå®çš„åŸºç¡€ï¼Œå¯ä»¥å¼€å§‹åŒæ—¶è¿è¡Œå¤šä¸ªä»»åŠ¡äº†ã€‚
- en: 'Instead of relying on time to pause execution of a running task, we can instead
    assume the programmer manually inserts calls to `longjmp`, see example (this time
    in C for `setjmp.h`):'
  id: totrans-split-117
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ä¾èµ–æ—¶é—´æ¥æš‚åœæ­£åœ¨è¿è¡Œçš„ä»»åŠ¡ï¼Œæˆ‘ä»¬å¯ä»¥å‡è®¾ç¨‹åºå‘˜æ‰‹åŠ¨æ’å…¥è°ƒç”¨ `longjmp`ï¼Œå‚è§ç¤ºä¾‹ï¼ˆè¿™æ¬¡æ˜¯ C è¯­è¨€çš„ `setjmp.h`ï¼‰ï¼š
- en: '[PRE14]'
  id: totrans-split-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Let''s run it:'
  id: totrans-split-119
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬è¿è¡Œå®ƒï¼š
- en: '[PRE15]'
  id: totrans-split-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Cool, but... There's actually a hidden bug (can you find it?).
  id: totrans-split-121
  prefs: []
  type: TYPE_NORMAL
  zh: å¾ˆé…·ï¼Œä½†æ˜¯... å®é™…ä¸Šæœ‰ä¸€ä¸ªéšè—çš„ bugï¼ˆä½ èƒ½æ‰¾åˆ°å—ï¼Ÿï¼‰ã€‚
- en: 'Let''s change `task_0` to hold some state on the stack:'
  id: totrans-split-122
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬ä¿®æ”¹ `task_0`ï¼Œåœ¨å †æ ˆä¸Šä¿å­˜ä¸€äº›çŠ¶æ€ï¼š
- en: '[PRE16]'
  id: totrans-split-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Run it again:'
  id: totrans-split-124
  prefs: []
  type: TYPE_NORMAL
  zh: å†æ¬¡è¿è¡Œå®ƒï¼š
- en: '[PRE17]'
  id: totrans-split-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Whoops... Because all our tasks share the same stack, each task (including
    our `main` function) may overwrite whatever is in the stack. See the following
    illustration:'
  id: totrans-split-126
  prefs: []
  type: TYPE_NORMAL
  zh: ç³Ÿç³•... å› ä¸ºæ‰€æœ‰ä»»åŠ¡å…±äº«åŒä¸€ä¸ªå †æ ˆï¼Œæ¯ä¸ªä»»åŠ¡ï¼ˆåŒ…æ‹¬æˆ‘ä»¬çš„ `main` å‡½æ•°ï¼‰å¯èƒ½ä¼šè¦†ç›–å †æ ˆä¸­çš„ä»»ä½•å†…å®¹ã€‚è¯·å‚è§ä¸‹é¢çš„ç¤ºä¾‹ï¼š
- en: '[PRE18]'
  id: totrans-split-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: '[PRE19]'
  id: totrans-split-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: '[PRE20]'
  id: totrans-split-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: '[PRE21]'
  id: totrans-split-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'The fix is to create a stack for each task, and switch to it right before calling
    the task function:'
  id: totrans-split-131
  prefs: []
  type: TYPE_NORMAL
  zh: è§£å†³æ–¹æ³•æ˜¯ä¸ºæ¯ä¸ªä»»åŠ¡åˆ›å»ºä¸€ä¸ªå †æ ˆï¼Œå¹¶åœ¨è°ƒç”¨ä»»åŠ¡å‡½æ•°ä¹‹å‰åˆ‡æ¢åˆ°è¯¥å †æ ˆï¼š
- en: '[PRE22]'
  id: totrans-split-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: The reason for saving the task function in the register `rax`, was to not lookup
    `tasks[i]` inside the stack, as we just changed the stack to some other memory
    location. The `asm` syntax is fully documented [here](https://gcc.gnu.org/onlinedocs/gcc/extensions-to-the-c-language-family/how-to-use-inline-assembly-language-in-c-code.html).
  id: totrans-split-133
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ä¿å­˜ä»»åŠ¡å‡½æ•°åœ¨å¯„å­˜å™¨ `rax` ä¸­çš„åŸå› æ˜¯ä¸ºäº†é¿å…åœ¨å †æ ˆå†…æŸ¥æ‰¾ `tasks[i]`ï¼Œå› ä¸ºæˆ‘ä»¬åˆšåˆšå°†å †æ ˆæ›´æ”¹ä¸ºå…¶ä»–å†…å­˜ä½ç½®ã€‚`asm` è¯­æ³•åœ¨ [è¿™é‡Œ](https://gcc.gnu.org/onlinedocs/gcc/extensions-to-the-c-language-family/how-to-use-inline-assembly-language-in-c-code.html)
    å…¨é¢è®°å½•ã€‚
- en: 'Run it one last time:'
  id: totrans-split-134
  prefs: []
  type: TYPE_NORMAL
  zh: æœ€åå†è¿è¡Œä¸€æ¬¡ï¼š
- en: '[PRE23]'
  id: totrans-split-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: We have successfully implemented a user mode non-preemptive scheduler!
  id: totrans-split-136
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬æˆåŠŸå®ç°äº†ç”¨æˆ·æ¨¡å¼éæŠ¢å è°ƒåº¦ç¨‹åºï¼
- en: In real non-preemptive (also called cooperative) systems, the runtime should
    yield when it knows that the CPU has nothing useful to do anymore in the current
    task, for example waiting on I/O. They do that by registering for I/O and move
    the task to a different queue that holds blocked tasks (which the scheduler skips
    from running). Once there's I/O, they move the task from the blocked queue back
    to the regular queue for execution. This can be done for example by integrating
    with an event loop.
  id: totrans-split-137
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨çœŸæ­£çš„éæŠ¢å å¼ï¼ˆä¹Ÿç§°ä¸ºåä½œå¼ï¼‰ç³»ç»Ÿä¸­ï¼Œè¿è¡Œæ—¶åº”åœ¨çŸ¥é“ CPU åœ¨å½“å‰ä»»åŠ¡ä¸­æ²¡æœ‰æ›´å¤šæœ‰ç”¨å·¥ä½œæ—¶è®©å‡ºï¼Œä¾‹å¦‚ç­‰å¾… I/Oã€‚ä»–ä»¬é€šè¿‡æ³¨å†Œ I/O å¹¶å°†ä»»åŠ¡ç§»åŠ¨åˆ°ä¸åŒçš„é˜Ÿåˆ—æ¥åšåˆ°è¿™ä¸€ç‚¹ï¼Œè¯¥é˜Ÿåˆ—ä¿å­˜é˜»å¡ä»»åŠ¡ï¼ˆè°ƒåº¦å™¨è·³è¿‡è¿è¡Œï¼‰ã€‚ä¸€æ—¦æœ‰äº†
    I/Oï¼Œä»–ä»¬å°†ä»»åŠ¡ä»é˜»å¡é˜Ÿåˆ—ç§»å›å¸¸è§„é˜Ÿåˆ—ä»¥æ‰§è¡Œã€‚ä¾‹å¦‚ï¼Œå¯ä»¥é€šè¿‡ä¸äº‹ä»¶å¾ªç¯é›†æˆæ¥å®Œæˆæ­¤æ“ä½œã€‚
- en: 'Here are some examples of non-preemptive schedulers in popular mainstream runtimes:'
  id: totrans-split-138
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™é‡Œæœ‰ä¸€äº›æµè¡Œä¸»æµè¿è¡Œæ—¶ä¸­çš„éæŠ¢å å¼è°ƒåº¦å™¨ç¤ºä¾‹ï¼š
- en: '**Rust''s [tokio](https://tokio.rs/)** - To yield, you either call `tokio::task::yield_now()`,
    or run until blocking (e.g. waiting on I/O or `tokio::time::sleep()`). In version
    0.3.1 they introduced an [automatic yield](https://tokio.rs/blog/2020-04-preemption).'
  id: totrans-split-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Rust çš„ [tokio](https://tokio.rs/)** - è¦è¿›è¡Œè®©æ­¥ï¼Œä½ å¯ä»¥è°ƒç”¨`tokio::task::yield_now()`ï¼Œæˆ–è€…è¿è¡Œç›´åˆ°é˜»å¡ï¼ˆä¾‹å¦‚ç­‰å¾…
    I/O æˆ– `tokio::time::sleep()`ï¼‰ã€‚åœ¨ç‰ˆæœ¬ 0.3.1 ä¸­ï¼Œä»–ä»¬å¼•å…¥äº†[è‡ªåŠ¨è®©æ­¥](https://tokio.rs/blog/2020-04-preemption)ã€‚'
- en: '**Go (prior to 1.14)** - At release (version 1.0), to yield, you would either
    call `runtime.Gosched()`, or run until blocking. In version 1.2 the scheduler
    is also invoked occasionally upon entry to a function.'
  id: totrans-split-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Goï¼ˆ1.14 ç‰ˆæœ¬ä¹‹å‰ï¼‰** - åœ¨å‘å¸ƒï¼ˆç‰ˆæœ¬ 1.0ï¼‰æ—¶ï¼Œè¦è¿›è¡Œè®©æ­¥ï¼Œä½ å¯ä»¥è°ƒç”¨`runtime.Gosched()`ï¼Œæˆ–è€…è¿è¡Œç›´åˆ°é˜»å¡ã€‚åœ¨ç‰ˆæœ¬
    1.2 ä¸­ï¼Œè°ƒåº¦å™¨ä¹Ÿå¶å°”ä¼šåœ¨å‡½æ•°å…¥å£å¤„è¢«è°ƒç”¨ã€‚'
- en: '**Erlang** - In [BEAM](https://blog.stenmans.org/theBeamBook/) (erlang''s awesome
    runtime), the scheduler is invoked at function calls. Since there are no other
    loop constructs than recursion and list comprehensions, there is no way to loop
    forever without doing a function call. You can cheat though by running native
    C code using a `NIF` (native implemented function).'
  id: totrans-split-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Erlang** - åœ¨[BEAM](https://blog.stenmans.org/theBeamBook/)ï¼ˆerlang çš„å¼ºå¤§è¿è¡Œæ—¶ï¼‰ä¸­ï¼Œè°ƒåº¦å™¨åœ¨å‡½æ•°è°ƒç”¨æ—¶è¢«è°ƒç”¨ã€‚ç”±äºé™¤äº†é€’å½’å’Œåˆ—è¡¨æ¨å¯¼æ²¡æœ‰å…¶ä»–å¾ªç¯ç»“æ„ï¼Œæ²¡æœ‰åŠæ³•æ— é™å¾ªç¯è€Œä¸è¿›è¡Œå‡½æ•°è°ƒç”¨ã€‚ä¸è¿‡ï¼Œä½ å¯ä»¥é€šè¿‡è¿è¡Œæœ¬åœ°çš„
    C ä»£ç æ¥ä½œå¼Šï¼Œä½¿ç”¨`NIF`ï¼ˆæœ¬åœ°å®ç°å‡½æ•°ï¼‰ã€‚'
- en: 'Non-preemptive schedulers are risky, as we assume developers remember to put
    `yield` calls when doing long computations:'
  id: totrans-split-142
  prefs: []
  type: TYPE_NORMAL
  zh: éæŠ¢å å¼è°ƒåº¦å™¨æ˜¯æœ‰é£é™©çš„ï¼Œå› ä¸ºæˆ‘ä»¬å‡è®¾å¼€å‘äººå‘˜åœ¨è¿›è¡Œé•¿æ—¶é—´è®¡ç®—æ—¶è®°å¾—æ”¾ç½®`yield`è°ƒç”¨ï¼š
- en: '[PRE24]'
  id: totrans-split-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: Preemptive Schedulers
  id: totrans-split-144
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æŠ¢å å¼è°ƒåº¦å™¨
- en: A preemptive scheduler context switches (yields) once in a while, even without
    a developer inserting yield calls.
  id: totrans-split-145
  prefs: []
  type: TYPE_NORMAL
  zh: æŠ¢å å¼è°ƒåº¦å™¨å¶å°”è¿›è¡Œä¸Šä¸‹æ–‡åˆ‡æ¢ï¼ˆè®©å‡ºï¼‰ï¼Œå³ä½¿æ²¡æœ‰å¼€å‘äººå‘˜æ’å…¥è®©å‡ºè°ƒç”¨ã€‚
- en: Most modern operating systems utilize timer interrupts. The CPU receives an
    interrupt once every X amount of time is passed. The interrupt stops execution
    of whatever is currently running, and the interrupt handler calls the scheduler
    which decides whether to context switch.
  id: totrans-split-146
  prefs: []
  type: TYPE_NORMAL
  zh: å¤§å¤šæ•°ç°ä»£æ“ä½œç³»ç»Ÿä½¿ç”¨å®šæ—¶å™¨ä¸­æ–­ã€‚CPU æ¯éš”ä¸€æ®µæ—¶é—´æ”¶åˆ°ä¸€ä¸ªä¸­æ–­ã€‚ä¸­æ–­åœæ­¢å½“å‰æ­£åœ¨è¿è¡Œçš„ä»»ä½•å†…å®¹ï¼Œå¹¶ä¸”ä¸­æ–­å¤„ç†ç¨‹åºè°ƒç”¨è°ƒåº¦å™¨ï¼Œå†³å®šæ˜¯å¦è¿›è¡Œä¸Šä¸‹æ–‡åˆ‡æ¢ã€‚
- en: That's cool and all, but user mode applications can't register to interrupts,
    so what can we do if we want to implement a preemptive scheduler in user mode?
  id: totrans-split-147
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™å¾ˆé…·ï¼Œä½†ç”¨æˆ·æ¨¡å¼åº”ç”¨ç¨‹åºä¸èƒ½æ³¨å†Œä¸­æ–­ï¼Œé‚£ä¹ˆå¦‚æœæˆ‘ä»¬æƒ³åœ¨ç”¨æˆ·æ¨¡å¼ä¸­å®ç°æŠ¢å å¼è°ƒåº¦å™¨ï¼Œæˆ‘ä»¬è¯¥æ€ä¹ˆåšå‘¢ï¼Ÿ
- en: One simple solution would be to utilize the kernel's preemptive scheduler. Create
    a thread that periodically sends a signal to threads running our scheduler.
  id: totrans-split-148
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€ä¸ªç®€å•çš„è§£å†³æ–¹æ¡ˆæ˜¯åˆ©ç”¨å†…æ ¸çš„æŠ¢å å¼è°ƒåº¦å™¨ã€‚åˆ›å»ºä¸€ä¸ªçº¿ç¨‹ï¼Œå®šæœŸå‘è¿è¡Œæˆ‘ä»¬çš„è°ƒåº¦å™¨çš„çº¿ç¨‹å‘é€ä¿¡å·ã€‚
- en: This is exactly how Go made their scheduler preemptive in version 1.14\. By
    periodically sending signals from their monitoring thread ([runtime.sysmon](https://sobyte.net/post/2021-12/golang-sysmon/))
    to the scheduler threads running goroutines.
  id: totrans-split-149
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ­£æ˜¯ Go åœ¨ç‰ˆæœ¬ 1.14 ä¸­ä½¿ä»–ä»¬çš„è°ƒåº¦å™¨å˜ä¸ºæŠ¢å å¼çš„æ–¹å¼ã€‚é€šè¿‡å‘¨æœŸæ€§åœ°ä»ä»–ä»¬çš„ç›‘æ§çº¿ç¨‹ï¼ˆ[runtime.sysmon](https://sobyte.net/post/2021-12/golang-sysmon/)ï¼‰å‘è¿è¡Œ
    goroutines çš„è°ƒåº¦å™¨çº¿ç¨‹å‘é€ä¿¡å·ã€‚
- en: 'For more info on their solution, I recommend you watch ["Pardon the Interruption:
    Loop Preemption in Go 1.14"](https://youtube.com/watch?v=1I1WmeSjRSw).'
  id: totrans-split-150
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 'æƒ³äº†è§£æ›´å¤šå…³äºä»–ä»¬è§£å†³æ–¹æ¡ˆçš„ä¿¡æ¯ï¼Œæˆ‘å»ºè®®ä½ è§‚çœ‹["Pardon the Interruption: Loop Preemption in Go 1.14"](https://youtube.com/watch?v=1I1WmeSjRSw)ã€‚'
- en: Stackful vs Stackless
  id: totrans-split-151
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: å †æ ˆå¼ vs æ— å †æ ˆå¼
- en: Up until now, I have been calling them tasks to not confuse you, but they have
    many different names like fibers, greenlets, user mode threads, green threads,
    virtual threads, coroutines and goroutines.
  id: totrans-split-152
  prefs: []
  type: TYPE_NORMAL
  zh: ç›´åˆ°ç°åœ¨ï¼Œæˆ‘ä¸€ç›´ç§°å®ƒä»¬ä¸ºä»»åŠ¡ï¼Œä»¥å…è®©ä½ å›°æƒ‘ï¼Œä½†å®ƒä»¬æœ‰è®¸å¤šä¸åŒçš„åç§°ï¼Œå¦‚çº¤ç¨‹ï¼ˆfibersï¼‰ã€greenletsã€ç”¨æˆ·æ¨¡å¼çº¿ç¨‹ã€ç»¿è‰²çº¿ç¨‹ã€è™šæ‹Ÿçº¿ç¨‹ã€åç¨‹å’Œ
    goroutinesã€‚
- en: When people say threads, they usually mean OS threads (managed by the kernel
    scheduler).
  id: totrans-split-153
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: å½“äººä»¬è¯´çº¿ç¨‹æ—¶ï¼Œé€šå¸¸æŒ‡çš„æ˜¯æ“ä½œç³»ç»Ÿçº¿ç¨‹ï¼ˆç”±å†…æ ¸è°ƒåº¦ç¨‹åºç®¡ç†ï¼‰ã€‚
- en: 'A coroutine is simply a program that can be paused and resumed. There are mainly
    two ways to implement them: either you allocate a stack for each coroutine (stackful),
    or you make each function marked as `async` return an object that can hold all
    the state needed to pause and resume that function (stackless).'
  id: totrans-split-154
  prefs: []
  type: TYPE_NORMAL
  zh: åç¨‹ç®€å•æ¥è¯´æ˜¯å¯ä»¥æš‚åœå’Œæ¢å¤çš„ç¨‹åºã€‚ä¸»è¦æœ‰ä¸¤ç§å®ç°æ–¹å¼ï¼šä¸ºæ¯ä¸ªåç¨‹åˆ†é…ä¸€ä¸ªå †æ ˆï¼ˆåŸºäºå †æ ˆï¼‰ï¼Œæˆ–è€…ä½¿æ¯ä¸ªæ ‡è®°ä¸º`async`çš„å‡½æ•°è¿”å›ä¸€ä¸ªå¯ä»¥æš‚åœå’Œæ¢å¤è¯¥å‡½æ•°æ‰€éœ€çŠ¶æ€çš„å¯¹è±¡ï¼ˆæ— å †æ ˆï¼‰ã€‚
- en: 'Stackful and stackless impact the API greatly, each with its own advantages
    and disadvantages. Here''s an overview:'
  id: totrans-split-155
  prefs: []
  type: TYPE_NORMAL
  zh: åŸºäºå †æ ˆå’Œæ— å †æ ˆæå¤§åœ°å½±å“APIï¼Œå„æœ‰å…¶ä¼˜ç¼ºç‚¹ã€‚ä»¥ä¸‹æ˜¯æ¦‚è¿°ï¼š
- en: '**Stackful** - Coroutines have the exact same API and semantics as OS threads,
    which makes sense, as they both allocate a stack at runtime. Our example scheduler
    using `setjmp` is stackful. Go is another example of a stackful implementation.
    Just like Go needs to periodically context switch, it also needs to periodically
    check whether there is enough free stack space to continue running, if not, it
    reallocates the stack to have more memory, copies what it had before and fixes
    all pointers that pointed to the old stack to now point to the new stack. Just
    like the stack can grow dynamically, it can also shrink if needed. The real beauty
    is that you can choose to run any function either synchronously or asynchronously
    in the background, without affecting the code around it:'
  id: totrans-split-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**åŸºäºå †æ ˆ** - åç¨‹ä¸æ“ä½œç³»ç»Ÿçº¿ç¨‹å…·æœ‰å®Œå…¨ç›¸åŒçš„APIå’Œè¯­ä¹‰ï¼Œè¿™æ˜¯æœ‰é“ç†çš„ï¼Œå› ä¸ºå®ƒä»¬éƒ½åœ¨è¿è¡Œæ—¶åˆ†é…å †æ ˆã€‚æˆ‘ä»¬ä½¿ç”¨`setjmp`çš„ç¤ºä¾‹è°ƒåº¦ç¨‹åºæ˜¯åŸºäºå †æ ˆçš„ã€‚Goè¯­è¨€æ˜¯å¦ä¸€ä¸ªåŸºäºå †æ ˆçš„å®ç°çš„ç¤ºä¾‹ã€‚å°±åƒGoéœ€è¦å®šæœŸä¸Šä¸‹æ–‡åˆ‡æ¢ä¸€æ ·ï¼Œå®ƒè¿˜éœ€è¦å®šæœŸæ£€æŸ¥æ˜¯å¦æœ‰è¶³å¤Ÿçš„ç©ºé—²å †æ ˆç©ºé—´ç»§ç»­è¿è¡Œï¼Œå¦‚æœæ²¡æœ‰ï¼Œå®ƒé‡æ–°åˆ†é…å †æ ˆä»¥è·å¾—æ›´å¤šå†…å­˜ï¼Œå¤åˆ¶ä¹‹å‰çš„å†…å®¹å¹¶ä¿®å¤æ‰€æœ‰æŒ‡å‘æ—§å †æ ˆçš„æŒ‡é’ˆï¼Œä½¿å…¶æŒ‡å‘æ–°å †æ ˆã€‚å †æ ˆä¸ä»…å¯ä»¥åŠ¨æ€å¢é•¿ï¼Œè¿˜å¯ä»¥åœ¨éœ€è¦æ—¶ç¼©å°ã€‚çœŸæ­£çš„ç¾å¦™ä¹‹å¤„åœ¨äºï¼Œæ‚¨å¯ä»¥é€‰æ‹©åœ¨åå°åŒæ­¥æˆ–å¼‚æ­¥è¿è¡Œä»»ä½•å‡½æ•°ï¼Œè€Œä¸å½±å“å…¶å‘¨å›´çš„ä»£ç ï¼š'
- en: '[PRE25]'
  id: totrans-split-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: '**Stackless** - If you have ever used a language with `async` & `await`, you
    used a stackless implementation. Examples include Rust and Python''s `asyncio`.
    Rust''s `async` transforms a block of code into a state machine that is not run
    until you `await` it. The biggest advantage of this approach is how [lightweight
    it is at runtime](https://pkolaczk.github.io/memory-consumption-of-async/), memory
    is allocated exactly as needed, which served well for Rust''s embedded use case
    as well. The main problem with this approach is "function coloring". An `async`
    function can only be called inside another `async` function:'
  id: totrans-split-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**æ— å †æ ˆ** - å¦‚æœæ‚¨æ›¾ç»ä½¿ç”¨è¿‡å…·æœ‰`async`å’Œ`await`çš„è¯­è¨€ï¼Œæ‚¨å°±ä½¿ç”¨è¿‡æ— å †æ ˆçš„å®ç°ã€‚ä¾‹å¦‚ï¼ŒRustå’ŒPythonçš„`asyncio`ã€‚Rustçš„`async`å°†ä¸€æ®µä»£ç å—è½¬æ¢ä¸ºä¸€ä¸ªçŠ¶æ€æœºï¼Œåœ¨æ‚¨`await`å®ƒä¹‹å‰ä¸ä¼šè¿è¡Œã€‚è¿™ç§æ–¹æ³•çš„æœ€å¤§ä¼˜åŠ¿åœ¨äºè¿è¡Œæ—¶çš„è½»é‡çº§ï¼Œå†…å­˜åˆ†é…æ°å¥½åœ¨éœ€è¦æ—¶è¿›è¡Œï¼Œè¿™ä¹Ÿéå¸¸é€‚åˆRustçš„åµŒå…¥å¼ç”¨ä¾‹ã€‚è¿™ç§æ–¹æ³•çš„ä¸»è¦é—®é¢˜æ˜¯â€œå‡½æ•°ç€è‰²â€ã€‚`async`å‡½æ•°åªèƒ½åœ¨å¦ä¸€ä¸ª`async`å‡½æ•°å†…éƒ¨è°ƒç”¨ï¼š'
- en: '[PRE26]'
  id: totrans-split-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Rust started with stackful prior to release, but ultimately ended up switching
    to stackless: ["Why async rust?"](https://without.boats/blog/why-async-rust/).'
  id: totrans-split-160
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: Ruståœ¨å‘å¸ƒä¹‹å‰é‡‡ç”¨äº†åŸºäºå †æ ˆçš„æ–¹æ³•ï¼Œä½†æœ€ç»ˆåˆ‡æ¢åˆ°äº†æ— æ ˆï¼š["ä¸ºä»€ä¹ˆé€‰æ‹©å¼‚æ­¥ Rustï¼Ÿ"](https://without.boats/blog/why-async-rust/)ã€‚
- en: Scheduler Algorithms
  id: totrans-split-161
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: è°ƒåº¦ç®—æ³•
- en: A scheduler is also responsible for deciding which task it should run next once
    one finishes.
  id: totrans-split-162
  prefs: []
  type: TYPE_NORMAL
  zh: è°ƒåº¦ç¨‹åºè¿˜è´Ÿè´£å†³å®šä¸€æ—¦ä¸€ä¸ªä»»åŠ¡å®Œæˆååº”è¯¥è¿è¡Œå“ªä¸ªä»»åŠ¡ã€‚
- en: One of the simplest methods is one we have already seen before in the event
    loop section, and that is to run tasks in the order that they are added to the
    task queue.
  id: totrans-split-163
  prefs: []
  type: TYPE_NORMAL
  zh: æœ€ç®€å•çš„æ–¹æ³•ä¹‹ä¸€æ˜¯æˆ‘ä»¬å·²ç»åœ¨äº‹ä»¶å¾ªç¯éƒ¨åˆ†çœ‹åˆ°çš„ï¼Œåœ¨ä»»åŠ¡é˜Ÿåˆ—ä¸­æŒ‰ç…§å®ƒä»¬è¢«æ·»åŠ çš„é¡ºåºè¿è¡Œä»»åŠ¡ã€‚
- en: 'Linux''s `SCHED_FIFO` scheduler does exactly this:'
  id: totrans-split-164
  prefs: []
  type: TYPE_NORMAL
  zh: Linuxçš„`SCHED_FIFO`è°ƒåº¦ç¨‹åºæ­£æ˜¯è¿™æ ·åšçš„ï¼š
- en: Each circle is a task. The white progress circle around tasks is the time left
    to run until the task is blocked.
  id: totrans-split-165
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: æ¯ä¸ªåœ†åœˆéƒ½æ˜¯ä¸€ä¸ªä»»åŠ¡ã€‚å›´ç»•ä»»åŠ¡çš„ç™½è‰²è¿›åº¦åœ†åœˆæ˜¾ç¤ºä»»åŠ¡è¢«é˜»å¡çš„å‰©ä½™è¿è¡Œæ—¶é—´ã€‚
- en: '**Purple box** - The queue holding tasks ready to run.'
  id: totrans-split-166
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**ç´«è‰²æ¡†** - åŒ…å«å‡†å¤‡è¿è¡Œä»»åŠ¡çš„é˜Ÿåˆ—ã€‚'
- en: '**Green box** - The CPU.'
  id: totrans-split-167
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**ç»¿è‰²æ¡†** - CPUã€‚'
- en: '**Gray box** - Tasks blocked on something (e.g. I/O).'
  id: totrans-split-168
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**ç°è‰²æ¡†** - è¢«æŸäº›ä¸œè¥¿é˜»å¡çš„ä»»åŠ¡ï¼ˆä¾‹å¦‚I/Oï¼‰ã€‚'
- en: 'Taking `SCHED_FIFO` and adding a task runtime limit is what `SCHED_RR` does,
    allowing the CPU to be shared in a more uniform manner:'
  id: totrans-split-169
  prefs: []
  type: TYPE_NORMAL
  zh: ä½¿ç”¨`SCHED_FIFO`å¹¶æ·»åŠ ä»»åŠ¡è¿è¡Œæ—¶é™æ˜¯`SCHED_RR`æ‰€åšçš„ï¼Œå…è®¸CPUä»¥æ›´å‡åŒ€çš„æ–¹å¼å…±äº«ï¼š
- en: What if you have a task that *must* run once every 5ms, even if for a really
    short amount of time? For example in audio programming, you have a buffer to fill
    with a signal in time (e.g. `sin(x)`) that the audio device reads from at some
    interval. Missing out on filling this buffer, will result in a random signal which
    sounds like crackling noise, potentially ruining a recording of an entire orchestra.
  id: totrans-split-170
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœä½ æœ‰ä¸€ä¸ªå¿…é¡»æ¯5æ¯«ç§’è¿è¡Œä¸€æ¬¡çš„ä»»åŠ¡ï¼Œå³ä½¿æ—¶é—´å¾ˆçŸ­ä¹Ÿå¿…é¡»è¿è¡Œæ€ä¹ˆåŠï¼Ÿä¾‹å¦‚ï¼Œåœ¨éŸ³é¢‘ç¼–ç¨‹ä¸­ï¼Œä½ éœ€è¦å¡«å……ä¸€ä¸ªä¿¡å·ï¼ˆå¦‚`sin(x)`ï¼‰åˆ°ç¼“å†²åŒºä¸­ï¼ŒéŸ³é¢‘è®¾å¤‡ä»¥æŸä¸ªé—´éš”ä»ä¸­è¯»å–ã€‚å¦‚æœé”™è¿‡äº†å¡«å……æ­¤ç¼“å†²åŒºï¼Œå°†å¯¼è‡´å¬èµ·æ¥åƒçˆ†è£‚å™ªéŸ³çš„éšæœºä¿¡å·ï¼Œå¯èƒ½ä¼šç ´åæ•´ä¸ªä¹å›¢çš„å½•éŸ³ã€‚
- en: These kind of programs are usually called soft real time programs. Hard real
    time means missing a deadline will result in the whole system failing, for example
    autopilot and spacecrafts.
  id: totrans-split-171
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ç§ç±»å‹çš„ç¨‹åºé€šå¸¸ç§°ä¸ºè½¯å®æ—¶ç¨‹åºã€‚ç¡¬å®æ—¶æ„å‘³ç€é”™è¿‡æˆªæ­¢æ—¥æœŸå°†å¯¼è‡´æ•´ä¸ªç³»ç»Ÿå¤±è´¥ï¼Œä¾‹å¦‚è‡ªåŠ¨é©¾é©¶å’Œèˆªå¤©å™¨ã€‚
- en: 'Linux has a nice answer for soft real time systems called `SCHED_DEADLINE`,
    where each thread sets the amount of time until their deadline, and the scheduler
    always runs the task that is closest to reaching the deadline:'
  id: totrans-split-172
  prefs: []
  type: TYPE_NORMAL
  zh: Linux å¯¹è½¯å®æ—¶ç³»ç»Ÿæœ‰ä¸€ä¸ªå¾ˆå¥½çš„è§£å†³æ–¹æ¡ˆï¼Œç§°ä¸º`SCHED_DEADLINE`ï¼Œå…¶ä¸­æ¯ä¸ªçº¿ç¨‹è®¾ç½®åˆ°å…¶æˆªæ­¢æ—¶é—´çš„æ—¶é—´é‡ï¼Œè°ƒåº¦å™¨å§‹ç»ˆè¿è¡Œæ¥è¿‘è¾¾åˆ°æˆªæ­¢æ—¶é—´çš„ä»»åŠ¡ï¼š
- en: The **green** progress circle is how much time is left until the deadline.
  id: totrans-split-173
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**ç»¿è‰²**è¿›åº¦åœ†åœˆæ˜¾ç¤ºè·ç¦»æˆªæ­¢æ—¶é—´è¿˜å‰©å¤šå°‘æ—¶é—´ã€‚'
- en: Follow the **pink** circle, it has a short deadline, making it run a lot more
    than others.
  id: totrans-split-174
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: å…³æ³¨**ç²‰è‰²**åœ†åœˆï¼Œå®ƒæœ‰ä¸€ä¸ªçŸ­æœŸé™ï¼Œä½¿å…¶æ¯”å…¶ä»–åœ†åœˆè¿è¡Œæ›´é¢‘ç¹ã€‚
- en: '`SCHED_FIFO` and `SCHED_RR` can also be used in soft real time systems because
    of their deterministic nature, depending on the problem you need to solve.'
  id: totrans-split-175
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '`SCHED_FIFO`å’Œ`SCHED_RR`ä¹Ÿå¯ç”¨äºè½¯å®æ—¶ç³»ç»Ÿï¼Œå› ä¸ºå®ƒä»¬å…·æœ‰ç¡®å®šæ€§è´¨ï¼Œå–å†³äºæ‚¨éœ€è¦è§£å†³çš„é—®é¢˜ã€‚'
- en: To guarantee all tasks are able to run according to their configured deadline,
    `SCHED_DEADLINE` calculates and rejects threads with a configuration that will
    steal too much run time. You can learn more about it on lwn's ["Deadline scheduling"](https://lwn.net/Articles/743740/).
  id: totrans-split-176
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†ä¿è¯æ‰€æœ‰ä»»åŠ¡èƒ½å¤ŸæŒ‰å…¶é…ç½®çš„æˆªæ­¢æ—¶é—´è¿è¡Œï¼Œ`SCHED_DEADLINE`è®¡ç®—å¹¶æ‹’ç»å…·æœ‰ä¼šå¤ºå–è¿‡å¤šè¿è¡Œæ—¶é—´çš„é…ç½®çš„çº¿ç¨‹ã€‚æ‚¨å¯ä»¥åœ¨lwnçš„["æˆªæ­¢æ—¥æœŸè°ƒåº¦"](https://lwn.net/Articles/743740/)ä¸Šäº†è§£æ›´å¤šä¿¡æ¯ã€‚
- en: For general purpose workloads, like a laptop running arbitrary processes, you
    usually want fairness. Fairness can be achieved by continuously tracking which
    processes have gotten less CPU time than others, and always run the task with
    the lowest tracked runtime. Linux's default scheduler `SCHED_OTHER`, also known
    as `CFS` (Completely Fair Scheduler), does exactly this. You can also configure
    priorities to processes by setting a `nice` value, where processes with a lower
    `nice` value will be scheduled more.
  id: totrans-split-177
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹äºåƒç¬”è®°æœ¬ç”µè„‘è¿è¡Œä»»æ„è¿›ç¨‹è¿™æ ·çš„é€šç”¨å·¥ä½œè´Ÿè½½ï¼Œé€šå¸¸å¸Œæœ›å…¬å¹³æ€§ã€‚å…¬å¹³æ€§å¯ä»¥é€šè¿‡æŒç»­è·Ÿè¸ªå“ªäº›è¿›ç¨‹çš„ CPU æ—¶é—´æ¯”å…¶ä»–è¿›ç¨‹å°‘ï¼Œå¹¶å§‹ç»ˆè¿è¡Œå…·æœ‰æœ€ä½å·²è·Ÿè¸ªè¿è¡Œæ—¶é—´çš„ä»»åŠ¡æ¥å®ç°ã€‚Linux
    çš„é»˜è®¤è°ƒåº¦å™¨`SCHED_OTHER`ï¼Œä¹Ÿç§°ä¸º`CFS`ï¼ˆå®Œå…¨å…¬å¹³è°ƒåº¦å™¨ï¼‰ï¼Œæ­£æ˜¯è¿™æ ·åšçš„ã€‚æ‚¨è¿˜å¯ä»¥é€šè¿‡è®¾ç½®`nice`å€¼ä¸ºè¿›ç¨‹é…ç½®ä¼˜å…ˆçº§ï¼Œå…¶ä¸­`nice`å€¼è¾ƒä½çš„è¿›ç¨‹å°†è¢«æ›´é¢‘ç¹åœ°è°ƒåº¦ã€‚
- en: '`CFS` has served well for the last 26 years, but in v6.6, the new default scheduling
    algorithm is [EEVDF](https://lwn.net/Articles/925371/).'
  id: totrans-split-178
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '`CFS`å·²ç»æœåŠ¡äº†26å¹´ï¼Œä½†åœ¨v6.6ä¸­ï¼Œæ–°çš„é»˜è®¤è°ƒåº¦ç®—æ³•æ˜¯[EEVDF](https://lwn.net/Articles/925371/)ã€‚'
- en: Multi-Core
  id: totrans-split-179
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: å¤šæ ¸
- en: So far, I have pretty much ignored the fact that modern machines have more than
    1 CPU core.
  id: totrans-split-180
  prefs: []
  type: TYPE_NORMAL
  zh: åˆ°ç›®å‰ä¸ºæ­¢ï¼Œæˆ‘åŸºæœ¬ä¸Šå¿½ç•¥äº†ç°ä»£è®¡ç®—æœºæ‹¥æœ‰å¤šä¸ª CPU æ ¸å¿ƒçš„äº‹å®ã€‚
- en: 'The simplest way to achieve multi-core scheduling, is to do exactly as before.
    Having a global queue of tasks that are ready to run, and run them once a core
    is ready:'
  id: totrans-split-181
  prefs: []
  type: TYPE_NORMAL
  zh: å®ç°å¤šæ ¸è°ƒåº¦çš„æœ€ç®€å•æ–¹æ³•æ˜¯ç»§ç»­åƒä»¥å‰ä¸€æ ·ã€‚æ‹¥æœ‰ä¸€ä¸ªå…¨å±€ä»»åŠ¡é˜Ÿåˆ—ï¼Œå…¶ä¸­åŒ…å«å‡†å¤‡è¿è¡Œçš„ä»»åŠ¡ï¼Œå¹¶åœ¨æ ¸å¿ƒå‡†å¤‡å°±ç»ªåè¿è¡Œå®ƒä»¬ï¼š
- en: You just need to ensure that the task queue is thread-safe for `MPMC` operations
    (multi-producer multi-consumer), by using atomics or locks.
  id: totrans-split-182
  prefs: []
  type: TYPE_NORMAL
  zh: æ‚¨åªéœ€ç¡®ä¿ä»»åŠ¡é˜Ÿåˆ—åœ¨`MPMC`æ“ä½œï¼ˆå¤šç”Ÿäº§è€…å¤šæ¶ˆè´¹è€…ï¼‰æ—¶æ˜¯çº¿ç¨‹å®‰å…¨çš„ï¼Œå¯ä»¥é€šè¿‡åŸå­æ“ä½œæˆ–é”æ¥å®ç°ã€‚
- en: '`MPMC` queues are a lot slower than the more restrictive `SPMC` (single-producer
    multi-consumer) queues, which is why Go decided to have a fixed size `SPMC` queue
    for each scheduler (Go runs a scheduler per core configured by `GOMAXPROCS`),
    with a global `MPSC` queue to push to when the `SPMC` queue is full.'
  id: totrans-split-183
  prefs: []
  type: TYPE_NORMAL
  zh: '`MPMC`é˜Ÿåˆ—æ¯”æ›´ä¸¥æ ¼çš„`SPMC`ï¼ˆå•ç”Ÿäº§è€…å¤šæ¶ˆè´¹è€…ï¼‰é˜Ÿåˆ—æ…¢å¾—å¤šï¼Œè¿™å°±æ˜¯ä¸ºä»€ä¹ˆ Go é€‰æ‹©ä¸ºæ¯ä¸ªè°ƒåº¦å™¨é…ç½®ä¸€ä¸ªå›ºå®šå¤§å°çš„`SPMC`é˜Ÿåˆ—ï¼ˆGo ä½¿ç”¨`GOMAXPROCS`é…ç½®æ¯ä¸ªæ ¸å¿ƒè¿è¡Œä¸€ä¸ªè°ƒåº¦å™¨ï¼‰ï¼Œå¹¶åœ¨`SPMC`é˜Ÿåˆ—æ»¡æ—¶æ¨é€åˆ°å…¨å±€`MPSC`é˜Ÿåˆ—ã€‚'
- en: To ensure all cores are fully utilized, when a core is free to run but has nothing
    in its local queue and there are no tasks in the global queue, it **steals** tasks
    from other local queues (which is why they are multi-consumer).
  id: totrans-split-184
  prefs: []
  type: TYPE_NORMAL
  zh: è¦ç¡®ä¿æ‰€æœ‰æ ¸å¿ƒå……åˆ†åˆ©ç”¨ï¼Œå½“ä¸€ä¸ªæ ¸å¿ƒå¤„äºç©ºé—²çŠ¶æ€ä½†æœ¬åœ°é˜Ÿåˆ—ä¸­æ²¡æœ‰ä»»åŠ¡å¹¶ä¸”å…¨å±€é˜Ÿåˆ—ä¸­ä¹Ÿæ²¡æœ‰ä»»åŠ¡æ—¶ï¼Œå®ƒä¼š**çªƒå–**å…¶ä»–æœ¬åœ°é˜Ÿåˆ—çš„ä»»åŠ¡ï¼ˆè¿™å°±æ˜¯å®ƒä»¬ä¸ºä»€ä¹ˆæ˜¯å¤šæ¶ˆè´¹è€…çš„åŸå› ï¼‰ã€‚
- en: 'Go''s solution is so good, tokio borrowed a lot from it. I highly recommend
    reading it on their blog: ["Making the Tokio scheduler 10x faster"](https://tokio.rs/blog/2019-10-scheduler).'
  id: totrans-split-185
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: Goçš„è§£å†³æ–¹æ¡ˆéå¸¸å‡ºè‰²ï¼Œtokioä»ä¸­å€Ÿé‰´äº†å¾ˆå¤šã€‚æˆ‘å¼ºçƒˆæ¨èåœ¨ä»–ä»¬çš„åšå®¢ä¸Šé˜…è¯»ç›¸å…³å†…å®¹ï¼š["ä½¿Tokioè°ƒåº¦å™¨å¿«10å€"](https://tokio.rs/blog/2019-10-scheduler)ã€‚
- en: Conclusion
  id: totrans-split-186
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ç»“è®º
- en: Congratulations ğŸ¥³! You are a real hero reaching the end, hopefully you have
    learned a thing or two.
  id: totrans-split-187
  prefs: []
  type: TYPE_NORMAL
  zh: ç¥è´º ğŸ¥³ï¼ä½ æ˜¯ä¸€ä¸ªçœŸæ­£çš„è‹±é›„ï¼Œåˆ°è¾¾äº†ç»ˆç‚¹ï¼Œå¸Œæœ›ä½ æœ‰æ‰€æ”¶è·ã€‚
- en: The topic has a lot more to cover, the links left throughout this post are a
    great place to start exploring the endless rabbit hole of concurrency and parallelism.
  id: totrans-split-188
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¸ªä¸»é¢˜è¿˜æœ‰å¾ˆå¤šå†…å®¹éœ€è¦æ¶µç›–ï¼Œæœ¬æ–‡ä¸­ç•™ä¸‹çš„é“¾æ¥æ˜¯æ¢ç´¢å¹¶å‘å’Œå¹¶è¡Œæ€§æ— å°½å…”å­æ´çš„ç»ä½³èµ·ç‚¹ã€‚
- en: If you want to play around with the animations yourself, here's a link to the
    [code](https://github.com/tontinton/sched_animation).
  id: totrans-split-189
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœä½ æƒ³è‡ªå·±ç©å¼„è¿™äº›åŠ¨ç”»ï¼Œè¯·ç‚¹å‡»è¿™é‡ŒæŸ¥çœ‹[ä»£ç ](https://github.com/tontinton/sched_animation)ã€‚
- en: Click here to scroll back to the animation at the top.
  id: totrans-split-190
  prefs: []
  type: TYPE_NORMAL
  zh: ç‚¹å‡»è¿™é‡Œè¿”å›é¡¶éƒ¨çš„åŠ¨ç”»ã€‚
