<!--yml

category: 未分类

date: 2024-05-27 15:02:18

-->

# 为了规范AI，从硬件入手，研究人员主张 • The Register

> 来源：[https://www.theregister.com/2024/02/16/boffins_propose_regulating_ai_hardware/](https://www.theregister.com/2024/02/16/boffins_propose_regulating_ai_hardware/)

在我们努力限制人工智能的破坏潜力时，剑桥大学的一篇论文建议在驱动它的硬件中加入遥控杀死开关和锁定功能，类似于用于阻止未经授权发射核武器的技术。

该论文[[PDF](https://www.cser.ac.uk/media/uploads/files/Computing-Power-and-the-Governance-of-AI.pdf)]包括来自多所学术机构和几家开放AI机构的声音，论证了规范这些模型依赖的硬件可能是防止其滥用的最佳途径。

“与AI相关的计算是干预的一个特别有效点：它是可检测的、可排除的和可量化的，并且是通过极度集中的供应链生产的，”研究人员辩称。

训练被认为超过一万亿参数的最常见模型需要庞大的物理基础设施：数以万计的GPU或加速器以及数周甚至数月的处理时间。研究人员表示，这使得这些资源的存在和相对性能难以隐藏。

此外，用于训练这些模型的最先进芯片由少数公司生产，例如英伟达、AMD和英特尔，这使得政策制定者可以限制将这些产品销售给关注对象的个人或国家。

这些因素，与半导体制造供应链限制等因素一起，为政策制定者提供了更好地了解人工智能基础设施部署情况的手段，以及谁有权访问以及谁无权访问它，并对其滥用实施处罚的途径，该论文提出。

### 控制基础设施

该论文强调了政策制定者可能采取的多种方法来处理AI硬件的规范。许多建议（包括旨在改善可见性并限制AI加速器销售的建议）已在国家层面得到实施。

去年，美国总统乔·拜登提出了一项旨在识别开发大型双用途AI模型的公司以及能够对其进行训练的基础设施供应商的行政命令。如果您不熟悉，“双用途”指的是在民用和军事应用中都能发挥作用的技术。

更近期，美国商务部提出了一项[建议](https://www.theregister.com/2024/01/29/us_raimondo_ai_cloud_kyc/)的规定，要求美国云服务提供商实施更严格的“了解您的客户”政策，以防止关注对象绕过出口限制。

研究人员指出，这种可见性是有价值的，因为它可以帮助避免另一场类似于导弹缺口争议引发的军备竞赛，当时错误的报告导致大规模的弹道导弹建造。尽管有价值，他们警告称执行这些报告要求会侵犯客户隐私，甚至可能导致敏感数据泄露。

与此同时，在贸易方面，商务部继续加强了[限制措施](https://www.theregister.com/2023/10/19/china_biden_ai/)，限制向中国出售的加速器的性能。但正如我们之前报道的，尽管这些努力使得像中国这样的国家更难获取美国芯片，它们远非完美。

为了解决这些限制，研究人员建议实施一个全球AI芯片销售注册表，跟踪它们在生命周期中的使用情况，即使它们已经离开其原产国。他们建议这样的注册表可以在每个芯片中包含一个唯一标识符，这可以帮助打击[走私](https://www.theregister.com/2024/02/05/smuggling_ai_chips/)的组件。

在极端情况下，研究人员建议在硅中嵌入“杀死开关”，以防止它们在恶意应用中被使用。

他们这样表达：

他们进一步扩展了这种机制，对我们来说，听起来他们建议加速器可以自行失效或由监管机构远程禁用：

学术界在其研究中在其他地方表达得更清楚，建议处理器功能可以通过数字许可证远程关闭或减弱：

理论上，这可以让监管机构更快地响应对敏感技术的滥用，通过远程切断对芯片的访问，但作者警告说这样做并非没有风险。这意味着，如果实施不当，这样的杀死开关可能成为网络犯罪分子利用的目标。

另一个提议是，在大规模部署潜在风险的AI训练任务之前，需要多方签署批准。他们写道：“核武器使用类似的机制称为许可行动链”。

对于核武器，这些安全锁设计旨在防止个人单方面发动第一次袭击。然而对于人工智能而言，想法是如果个人或公司希望在云中训练一个模型超过某个阈值，他们首先需要获得授权才能这样做。

尽管这是一个强大的工具，研究人员观察到这可能会适得其反，阻碍了可期望的人工智能的发展。论点似乎是，虽然使用核武器有一个相当明确的结果，但人工智能并非总是那么黑白分明。

但如果这让你感觉有点过于反乌托邦，那么该论文专门为将AI资源重新配置以促进整个社会的改善而设立了一个完整的章节。其核心思想是决策者可以联合起来，使AI计算能力更易于那些不太可能用于恶意目的的群体使用，这一概念被描述为“分配”。

### 对AI发展进行监管有什么问题呢？

为什么要费这么大劲呢？好吧，文章的作者认为，物理硬件本质上更容易控制。

与硬件相比，“AI发展的其他输入和输出——数据、算法和训练模型——是易于共享的非竞争性无形商品，这使得它们本质上难以控制，”文章中写道。

论点是，一旦模型发布，无论是公开还是泄露，就无法将精灵重新装回瓶子，阻止其在网络上的传播。

研究人员还指出，防止模型被滥用的努力已经被证明是不可靠的。举一个例子，作者们突出了研究人员如何轻松地拆除Meta的Llama 2中旨在防止模型生成冒犯性语言的保障措施。

如果被极端利用，有人担心，一个足够先进的双用途模型可能被用来加速化学或生物武器的开发。

文章承认，AI硬件监管并非灵丹妙药，并不能消除对行业其他方面监管的需求。

然而，考虑到OpenAI几位研究人员的参与，很难忽视CEO Sam Altman在AI监管话题上试图控制叙述的努力。 ®
