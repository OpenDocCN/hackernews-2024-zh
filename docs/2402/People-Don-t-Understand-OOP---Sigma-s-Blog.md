<!--yml
category: æœªåˆ†ç±»
date: 2024-05-27 14:31:42
-->

# People Donâ€™t Understand OOP â€“ Sigma's Blog

> æ¥æºï¼š[https://blog.sigma-star.io/2024/01/people-dont-understand-oop/](https://blog.sigma-star.io/2024/01/people-dont-understand-oop/)

> â€œOOP to me means only messaging, local retention and protection and hiding of state-process, and extreme late-binding of all things.â€
> 
> Alan Kay (the guy who coined the term â€œObject Oriented Programmingâ€)

It seems like a lot of people dislike Object Oriented Programming. The first things that come to mind when hearing these three letters are cars, inheritance, getters, setters, and ObjectFactoryFactorySingleton.

This always seemed kinda odd to me. Not only do I like OOP, I even feel that it often is the best/most obvious way to model a problem. Here is why I think that is:

## OO Who?

I think before doing anything further we should probably define what we are talking about. Unfortunately OOP is not that well defined. So for the sake of coherence letâ€™s settle on a clear and unambiguous definition first.

We will be talking about â€œobjectsâ€ a lot. So what are they? Most introduction texts to OOP use physical things like cars and animals to illustrate what objects are. And while thatâ€™s not wrong (it is literally where the object metaphor comes from; Alan Kay was thinking in terms of biological cells and networks.) itâ€™s certainly misleading, because objects are much more than that.

Peter Wegner writes: â€œObjects are collections of operations that share a state.â€

Mark Stefik & Daniel Bobrow define objects as follows:
â€œObjects are entities that combine the properties of procedures and data since they perform computations and save local state. Uniform use of objects contrasts with the use of separate procedures and data in conventional programming.â€

Hereâ€™s another definition by The Gang of Four: â€œObject-Oriented Programs are made up of objects. An object packages both data and procedures that can operate on data. Procedures are typically called methods or operations.â€

Okay, thatâ€™s a good start, but I still think an important characteristic of objects is missing. Maybe Tim Rentsch can help: Objects are units of state, that are generally opaque to the outside (This, I think, is the important part. We will see later why that is.). An object can however provide the possibility to interact with its state by the means of message-passing (= â€œmethodsâ€).

Wait a second. â€œ[â€¦] collections of operations that share stateâ€? â€œ[â€¦] entities that combine [â€¦] procedures and data [â€¦]â€? â€œ[â€¦] units of stateâ€?
The hell is that supposed to mean?
Well, it means that â€œobjectâ€ is an abstract term. An object can potentially be anything â€“ anything with state that is. It can be a physical item like car, an abstract concept, it can be any random piece of data with some sort of behavior attached to it. OOP just means we model our problem using these objects. Thatâ€™s it.

### A Class Of Its Own

You might be thinking: â€œHold on, we defined OOP without even touching on classes. What gives?â€

The answer is simple: Classes are not strictly necessary for OOP. A shocker, I know.

Of course we need to be able to construct new objects, and class-based languages are admittedly way more prevalent. However, this is not the only way to achieve this goal.

Languages like JavaScript (although ES6 introduced classes to the language) or Lua use a concept called prototype-based or prototypal OOP. Instead of providing a schema for constructing new objects we use an existing object as a prototype. This approach can even have real-world benefits, as it reduces the language complexity.

(Just as a side note: Classes donâ€™t need to be called classes. Languages like Go or Rust â€“ also C++ to some extend â€“ call them structs for example.)

### Itâ€™s Hereditary

Another term that â€“ while not technically necessary â€“ is often associated with OOP is inheritance.

There are two reasons to use inheritance:

The first is to reuse existing code. However, in modern programming this is usually discouraged in favor of object composition (an object inside another object).

The second reason (to me the more important one) is for abstraction and polymorphism. The technical term for this is â€œsubtypingâ€.

### â€˜Sup, Typing?

(Yes, I think this topic is important enough to warrant its own heading. ğŸ˜…)

Subtyping is not exclusive to OOP, however itâ€™s of special significance here since itâ€™s the primary way of modelling polymorphism. The idea is to combine multiple different classes that share common messages (i.e. have methods with similar semantics) into a super type that defines those messages. Now, the super type can be used instead of specifying a subtype.

My favorite example for how subtyping can be used in practice is the Java collections framework. It defines interfaces (we will later talk about what exactly interfaces are) for common use-cases like Lists, Queues, Sets, Maps, â€¦ as well as different implementations with different characteristics which support those different use-cases.

wpg_div_collectionmap

(The graph was generated from JavaDocs by scrapping all known subclasses of [Collection](https://docs.oracle.com/en/java/javase/20/docs/api/java.base/java/util/Collection.html) and [Map](https://docs.oracle.com/en/java/javase/20/docs/api/java.base/java/util/Map.html) and removing non-relevant nodes.)

So letâ€™s say I want to process a list of data, Iâ€™d just use the List interface everywhere. At the point where I instantiate the List I choose ArrayList, since itâ€™s usually the more performant implementation. Later on it turns out that the program is doing a lot of inserts/deletes at the beginning of the list, which is pretty slow on arrays. To speed up the program I can switch to a LinkedList without changing any of the type signatures.

Side Note: When calling a method we need to know the actual class of the object, not just itâ€™s declared class, otherwise subtyping wonâ€™t work properly. This is called â€œlate/dynamic-bindingâ€. Itâ€™s technical execution is a bit tricky and is the main reason why in C++ objects and object-pointers behave differently (see vtables).

### Odd Behavior

I think we canâ€™t (and shouldnâ€™t) talk about subtyping without mentioning behavioral subtyping and Barbara Liskov. The basic idea of behavioral subtyping is that a subtype should behave in a way similar to the parent type.

Barbara Liskov (who later won a Turing award for her work on programming languages and OOP in particular) formalized this concept in 1987 into â€œstrong behavioral subtypingâ€: A subtype should be able to be used in every situation its parent type can be used in.

â€œSubtype Requirement: LetÂ Ï†(x)Â be a property provable about objectsÂ xÂ of typeÂ T. ThenÂ Ï†(y)Â should be true for objectsÂ yÂ of typeÂ SÂ whereÂ SÂ is a subtype ofÂ T.â€

This is called the Liskov substitution principle. I wonâ€™t to go into details here, but the basic idea is that any precondition (for types, data or state) for parameters can not be stronger than the super-type, while any postcondition for results can not be weaker than the super-type. The notion is related to the design-by-contract methodology that also started to pop up around the same time.

### Way Too Abstract

In some cases we donâ€™t care about the code-sharing aspect of inheritance, but still want to profit off of subtyping â€“ we might never actually use the super type implementation of the methods and can therefore omit it entirely. This is in fact so common, it even has a name: Virtual or abstract methods.

We might even end up removing all state from our abstract super type, and only use it as a stencil for defining methods. This is called an interface.

Some languages even go a step further and completely decouple interfaces from classes. There are two different schools of thought:

Structural typing (as opposed to the usual nominal typing) is when interface implementations are not declared at all. You can simply use the object as an implementation as long as all necessary methods are defined. This is statically checked at compile time. Examples of languages that support structural typing are Go (both for interfaces themself and type constraints) and C++ (for concepts).
Duck typing is similar but the existence of methods is only checked at runtime. Languages that use this pattern include Python and JavaScript. One disadvantage thatâ€™s often cited is that it is more complicated to figure out which classes can be expected at a particular point in the program.

The second pattern doesnâ€™t seem to have an established name yet. The idea is to declare that a class is implementing an interface after the class was already defined. An example of a language that does this is Rust with traits. Unfortunately, â€œtraitsâ€ is a horrible name for this concept, since â€œtraitsâ€ usually just refers to mix-ins. Iâ€™ve heard the term â€œextension traitsâ€ â€“ in reference to â€œextension methodsâ€ in C#/Kotlin â€“ but this doesnâ€™t seem to be very common either. Another language that support this feature is Haskell (they call it â€œtype classesâ€; but Haskell is arguably not object-oriented).

### Hide & Seek

One term thatâ€™s often used with OOP is â€œencapsulationâ€. There are actually two applicable definitions of the term. The first one refers to bundling data with behavior (= the object metaphor). And the second one refers to restricting access to the state only to the objects itself. I would like to focus a bit on the latter since I think a lot of people donâ€™t understand it properly.

> â€œEncapsulation is a technique for minimizing interdependencies among separately-written modules by defining strict external interfaces.â€
> 
> Alan Snyder, 1986

So why is it important to restrict access to state? Well, there are multiple reasons. We could argue that it would violate Liskovâ€™s history constraint. But I think itâ€™s much more practical to look at it from the perspective of a developer who wants to refactor the code base. Letâ€™s say we want to change the internal structure of an object (like in the List example earlier, maybe we want to switch from an ArrayList to a LinkedList). But if other components are reliant on the internal state (in the case of the ArrayList: this could be the internal primitive array) we can not easily change it. We would need to find all places outside of the class where the internal structure is referenced. The problem gets even worse when the class is exported and used by modules that we might not even control.

â€œ(Object) couplingâ€ and â€œ(class) cohesionâ€ are often used to talk about encapsulation. â€œObject couplingâ€ describes how much different objects depend on each other. High â€œobject couplingâ€ implies that the objects in question rely a lot on each other, which usually means they should be one single object instead. If objects rely on each others internal structure they are highly coupled. â€œClass cohesionâ€ describes the same characteristic but from a different perspective. Itâ€™s a measure of how coherent a classâ€™ responsibilities are. A class should ideally represent one idea and only do stuff related to that idea. Low â€œclass cohesionâ€ usually means high â€œobject couplingâ€ and vice versa.

Iâ€™m sure if you have done any object-oriented programming youâ€™ve heard something like â€œDonâ€™t use public properties [â€¦]â€ (properties in the sense of member variables) at some point. And this is true, because public properties expose the internal state, and can potentially cause high object coupling. However, as with any dogma, itâ€™s usually a good idea to question it. In this case the complete â€œguidelineâ€ is â€œDonâ€™t use public properties, use getters and setters instead.â€, which is completely wrong. In terms of encapsulation getters and setters are just as bad as public properties, as they do nothing to prevent object coupling. If you have a class without any methods (besides getters and setters) it doesnâ€™t really fit our object definition. A term that has been used for this is â€œrecordâ€.

### };

Okay, so what it OOP? OOP is when related state and behavior are bundled into units (= objects). Other properties object-oriented languages *may* have are: Classes, prototypes, encapsulation, subtyping, inheritance, â€¦

Letâ€™s take a look at some modern languages (these are the top 15 languages from the [StackOverflow Developer Survey 2023](https://survey.stackoverflow.co/2023/#section-most-popular-technologies-programming-scripting-and-markup-languages) â€“ excluding stuff like HTML, â€¦):

| Language | Objs | Obj Creation | Encapsulation | Subtyping |
| --- | --- | --- | --- | --- |
| JavaScript | âœ”ï¸ | Classes/ Prototypes
 | âœ”ï¸ (since ES2022) | Inheritance/ Duck typing |
| Python | âœ”ï¸ | Classes | âŒ (not on a language level) | Inheritance/ Duck typing |
| TypeScript | âœ”ï¸ | Classes/ Prototypes | âœ”ï¸ | Inheritance/ Structural typing/
Duck typing |
| ShellScript | âŒ | âŒ | âŒ | âŒ |
| Java | âœ”ï¸ | Classes | âœ”ï¸ | Inheritance/ Nominal typing |
| C# | âœ”ï¸ | Classes | âœ”ï¸ | Inheritance/ Nominal typing |
| C++ | âœ”ï¸ | Classes + Structs | âœ”ï¸ | Inheritance/ Nominal typing +
Structural typing (concepts) |
| C | âŒ (no methods) | Structs | âœ”ï¸ (kinda using incomplete types) | âŒ (single â€œinheritanceâ€ by nesting structs; no real subtyping) |
| PHP | âœ”ï¸ | Classes | âœ”ï¸ | Inheritance/ Duck typing |
| PowerShell | âœ”ï¸ | Classes | âŒ | Inheritance/ Duck typing* |
| Go | âœ”ï¸ | Structs | âœ”ï¸ (on package level) | Structural typing |
| Rust | âœ”ï¸ | Structs | âœ”ï¸ | Extension Traits/ Nominal typing |
| Kotlin | âœ”ï¸ | Classes | âœ”ï¸ | Inheritance/ Nominal typing |
| Ruby | âœ”ï¸ | Classes | âœ”ï¸ (enforced) | Inheritance/ Duck typing |
| Lua | âœ”ï¸ | Tables (Prototypes) | âŒ | Inheritance/ Duck typing |

*) not sure

## O-Oh, Noâ€¦

Okay. Now that we have a good understanding of what exactly OOP is and what we can expect from languages that implement the OOP paradigm, letâ€™s take a look at some common points of criticism. (I shamelessly crowdsourced most of the following part by asking my friends what they hate about OOP. ğŸ˜‹)

### But What ARE Objects?

So, objects can be anything, right? So how do I know what should be an object? When should I combine things, what should be separate?

Well, in the end itâ€™s just practice and experience. With time youâ€™ll get a feeling about what should and what shouldnâ€™t be an object. However, to get started there are some tricks that might help you.
Hereâ€™s what The Gang of Four has to say:

â€œObject-oriented design methodologies favor many different approaches. You can write a problem statement, single out the nouns and verbs, and create corresponding classes and operations. Or you can focus on the collaborations and responsibilities in your system. Or you can model the real world and translate the objects found during analysis into design. There will always be disagreement on which approach is best.â€

### Speedy Thing Goes In, Speedy Thing Comes Out

EDIT: It has been pointed out to me that a made a few mistakes when designing the benchmarks. Thanks to NoNaeAbC on Github for pointing out that Iâ€™m allocating and zeroing way too much memory in the OOP and SP tests, and u9vata on Youtube for critiquing my benchmark design. With regard to the latter, while I donâ€™t agree with everything they said, itâ€™s definitely true that I was making some unfounded assumptions about compiler optimisation.
Iâ€™m not sure when Iâ€™ll have the time to redesign the benchmarks, so for the time being, please take the following with a big grain of salt.
While we are at it, I also came up with another explanation on why the FP tests are so slow: The band is stored as increasingly nested closures, which have to store their arguments on the heap, while the OOP and SP version can exclusively work on the stack.

OOP is slow. Or so Iâ€™ve heard. The rationale is that vtable lookups are overhead compared to direct function calls. I donâ€™t actually know whether thatâ€™s true, so I decided to test it.

The test setup is as follows: I wrote the same program (a Turing machine checking for binary palindromes) 3 times, once using object-oriented programming, once using structured programming (only using functions, loops, tuples, arrays â€“ stuff like that), and once using functional programming for good measure.

I implemented everything in C++ so itâ€™s an equal playing field (also, C++ has first class functions/lambda expressions for the functional version). There is 100 000 test cases, the total time is measured. The compiler is clang 14.0.3, the target platform is Apple Silicon (M1). I ran each test with both -O0 and -O3.

For the OOP implementation I made sure to not rely on heap allocations, since the context switches would probably completely ruin the runtime. I did however use inheritance (the template pattern to be specific) to make the vtable lookups as realistic as possible.

The structured version also allocates everything on the stack. I build two different versions. The first one uses tuples in the transition table lookup, however I wasnâ€™t sure how tuples are implemented under the hood and I wanted to avoid using objects by accident if possible, so I wrote another version that only relies on functions. But it turned out the results were so close I couldnâ€™t tell the difference.

<canvas id="myiChartbenchmarks--o0-1"></canvas>

<canvas id="myiChartbenchmarks--o3-2"></canvas>

As we can see, the structured version is marginally (~ 5 %) faster than the object-oriented one when not using any optimization (Although I should mention that I observed the values jumping quite a lot between runs). When using -O3 the performance is basically identical (~ 1 %), so my guess is that the C++ optimizer was able to get rid of whatever impacted the performance.

The functional implementation is not even remotely close. To a certain extend this is probably caused by the benchmark I chose. Turing machines are inherently stateful, which is pretty awkward to model in a functional way. Another aspect is that even though I used C++14 (which supports return type inference) I was forced to use the std::function template as a wrapper for lambda expressions (anonymous types are a pain in the backside) which (according to my tests) are quite a bit slower than native lambda expressions.

I should have probably done some rigorous statistical tests, or at least calculated the deviation. But honestly, I was too lazy. I may write an update with a proper analysis later on.

In case you want to do some tests on your own, feel free to send me the results afterwards. ğŸ˜› The source code is on [Github](https://github.com/overflowerror/oop-benchmarks/tree/blog-version) (also I should probably apologise for the horrible code, C++ is not my native language and I hacked it together in like an hour or so ğŸ˜…).

Anyway, without rigorous statistics my conclusion of the tests is that there is only a very small difference in performance. Adding more abstraction layers (or using different data structures) probably has a more significant impact.

However, other benchmarks on embedded systems have found a ~10 % performance penalty compared to a procedural implementation.

Another paper comparing the performance of different aspects of OOP as well as different design pattern has shown that virtual functions (which I used in my implementation) can impact the performance negatively (~ 5 %). The template pattern (which I also used) can also decrease the performance by about 3 â€“ 4 % (but this might also just be because it relies on virtual functions).

### Abstract Nonsense

For some reason OOP leads us to overcomplicate everything. We needlessly build abstractions on top of abstractions, seemingly for the sole purpose of making pretty UML diagrams.

The thing is: Itâ€™s caused by how we use the tools, not the tools themselves. My suspicion is that most of these issues arise from developers wanting to be clever and build generic solutions to cover every possible future development.

I think a lot of this can be avoided by adjusting the workflow. Specifically: If the end goal is not determined from the beginning, donâ€™t plan for every eventuality from the start, only plan for what you know youâ€™ll need. The requirements might change later, so your amazing, highly generic solution that you worked on for 4 weeks straight might not be used in the end â€“ a waste of time.

### The Threat of Get and Set

OOP is so verbose, there is so much boilerplate code. Getters and setters for example.

*sigh* This is a personal pet peeve of mine. We touched on this earlier but I would really like to hammer this part home: If you really need getters and setters for ever single member variable itâ€™s probably not a proper object to start with. Iâ€™d highly encourage reconsidering your object model, try to reduce coupling. If itâ€™s really a record class with no internal behavior, everything might as well be public â€“ there is hardly a point in using getters and setters. A similar thing (though admittedly itâ€™s a better) applies to properties in languages like C#, and of course code generators like the infamous Lombok.

The only real reason for the use of getters and setters over public members, is when there is some additional logic like validation of invariants for example.

Kinda related: If you have a value object with no setters but lots of getters, make sure to not accidentally expose a modifiable reference to internal state. Otherwise youâ€™ve got setters â€“ just not intentionally.

### ObjectFactoryFactorySingleton

I guess there are two topics that fit this heading. The first being the naming madness that has been established in enterprise software development. This is again not per se an issue with OOP although for some reason this seems to happen a lot more with OOP. I happen to be a Kevlin Henney fan, and he gave an [amazing talk on naming in programming](https://www.youtube.com/watch?v=CzJ94TMPcD8) at DevWeek 2015\. Among other things he talks about how naming can influence modelling. I highly recommend watching it.

The second topic is the rabbit hole of design patterns, that are often blindly applied, seemingly without any thought on why exactly. Specifically, the factory pattern has some valid uses, but because people overuse the pattern so much itâ€™s now synonymous with unnecessary abstractions.

There are of course also established patterns where you should really have a damn good reason to actually use it â€“ at least in a strictly object-oriented context. Singletons for example. â€œSingletonâ€ is in essence just a fancy name for a global variable â€“ great. Fun little side note: In the Spring framework Beans by default get the Singleton scope, meaning if not stated otherwise every single bean is global.

### A Dream of Spring

Another thing Iâ€™ve been noticing with modern â€œenterpriseâ€ applications is that they are not actually object-oriented. Entities, DTOs, â€¦ are records, not objects. Beans, Services, Repositories, â€¦ donâ€™t hold state and could just as well be plain functions in modules.

We are using languages that force us to think in classes with architectures that donâ€™t require objects â€“ Spring Boot could just as well be written in C.

## Conclusion

What a ride. I think this my longest blog post so far. Maybe even a bit too longâ€¦ Iâ€™ll make sure the next one is shorter. ğŸ˜…

I also found this really interesting [talk by Barbara Liskov about abstraction](https://www.youtube.com/watch?v=dtZ-o96bH9A), but I just wasnâ€™t sure where to put it, so here you go. (I particularly like the stab against Python for throwing encapsulation out the window. ğŸ˜†)

Anyway, I hope I could shed some light on the topic, maybe you learned something, or at the very least you found my ramblings somewhat entertaining.

See you soon,
Sigma

### Footnotes