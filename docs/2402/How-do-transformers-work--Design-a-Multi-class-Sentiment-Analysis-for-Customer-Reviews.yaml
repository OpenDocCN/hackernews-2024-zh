- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: Êú™ÂàÜÁ±ª'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-05-27 14:35:08'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: How do transformers work?+Design a Multi-class Sentiment Analysis for Customer
    Reviews
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Êù•Ê∫êÔºö[https://nintyzeros.substack.com/p/how-do-transformer-workdesign-a-multi](https://nintyzeros.substack.com/p/how-do-transformer-workdesign-a-multi)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '*üëã Hi, this is [](https://twitter.com/gergelyorosz) Venkat and here with a
    free, full issue of the The ZenMode Engineer Newsletter. In every issue, I cover
    one topic explained in a simpler terms in areas related to computer technologies
    and beyond.*'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Transformers have become synonymous with cutting-edge AI, particularly in the
    realm of natural language processing (NLP).
  prefs: []
  type: TYPE_NORMAL
- en: But what exactly makes them tick? How do these models navigate the intricacies
    of language with such remarkable efficiency and accuracy?
  prefs: []
  type: TYPE_NORMAL
- en: Buckle up, because we're about to learn the heart of the transformer architecture.
  prefs: []
  type: TYPE_NORMAL
- en: But.. Before we deep dive into it lets understand where its been used.. if you
    have used google translate/ ChatGPT both rely on these.
  prefs: []
  type: TYPE_NORMAL
- en: '***Google Translate:** This widely used platform relies heavily on transformers
    to achieve fast and accurate translations across over 100 languages. It considers
    the entire sentence context, not just individual words, leading to more natural-sounding
    translations.*'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '***Netflix Recommendation System:** Ever wondered how Netflix suggests shows
    and movies you might enjoy? Transformers analyze your viewing history and other
    users'' data to identify patterns and connections, ultimately recommending content
    tailored to your preferences.*'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '**The Big Picture: Encoder and Decoder Dance**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Imagine a factory, but instead of assembling physical objects, it processes
    language. This factory has two main departments:'
  prefs: []
  type: TYPE_NORMAL
- en: '**The Encoder:** This is the information extractor, meticulously dissecting
    the input text, understanding its individual elements, and uncovering the hidden
    connections between them.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**The Decoder:** Armed with the encoder''s insights, the decoder crafts the
    desired output, be it a translated sentence, a concise summary, or even a brand
    new poem.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Encoder: Decoding the Input Labyrinth**'
  prefs: []
  type: TYPE_NORMAL
- en: The encoder's journey begins with **Input Embedding**, where each word is transformed
    from its textual form into a numerical representation (vector). Think of it as
    assigning each word a unique identifier.
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Consider this example.
  prefs: []
  type: TYPE_NORMAL
- en: '**Input Text:** The process begins with the raw text sentence, such as "The
    cat sat on the mat."'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Input Embedding Layer:**'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: This layer acts as a translator, converting each word into a numerical vector.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Imagine a large dictionary where each word has a corresponding vector address.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'These vectors capture various aspects of word meaning:'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Semantic relationships (e.g., "cat" is closer to "pet" than "chair").
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Syntactic roles (e.g., "cat" is often a noun, while "sat" is a verb).
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Context within the sentence (e.g., "mat" here likely refers to a floor mat).
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Vector Representation:**'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '* * *'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'But the encoder doesn''t stop there. It employs the following key mechanisms
    to delve deeper:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Self-Attention Layer:** This is the game-changer. Imagine shining a spotlight
    on each word, but instead of illuminating it in isolation, you also highlight
    how it connects to all other words in the sentence. This allows the encoder to
    grasp the context, nuances, and relationships within the text, not just the individual
    words.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: ref from Raimi Karim blog (used only to refernce)
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '* * *'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Consider this example sentence again "***The quick brown fox jumps over the
    lazy dog.***"
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Word Embeddings:** First, each word is transformed into a numerical representation
    called a "word embedding." Think of it as assigning each word a unique identifier
    in a giant vocabulary map.'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Query, Key, Value:** Next, the Self-Attention mechanism creates three special
    vectors for each word:'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Query (Q):** This vector asks "What information do I need from other words?"'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Key (K):** This vector acts like a label, saying "This is the information
    I have to offer."'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Value (V):** This vector holds the actual information, like the word''s meaning
    and context.'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Attention Scores:** Now comes the interesting part. The Self-Attention layer
    compares the Query vector of each word with the Key vectors of all other words
    in the sentence.'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: This helps it understand how relevant each word is to the current word. Based
    on this comparison, it calculates an **attention score** for each pair of words.
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: Imagine shining a spotlight on each word. The brighter the spotlight on another
    word, the higher the attention score, meaning the more relevant that word is to
    the current word.
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Weighted Values:** Finally, the Self-Attention layer uses the attention scores
    to weigh the Value vectors of all other words. Words with higher attention scores
    get more weight, contributing more to the final representation of the current
    word.'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Think of it like taking a weighted average of the information from other words,
    where the weights are determined by how relevant they are.
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: '**New Word Representation:** By considering the context provided by other words,
    the Self-Attention layer creates a new, enriched representation of each word.
    This new representation captures not just the word''s own meaning, but also how
    it relates to and is influenced by other words in the sentence.'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '* * *'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Multi-Head Attention:** This is like having multiple teams of analysts, each
    focusing on different aspects of the connections between words. It allows the
    encoder to capture various facets of the relationships, enriching its understanding.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '* * *'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Sentence:** "The quick brown fox jumps over the lazy dog."'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Individual Heads:** Instead of one Self-Attention mechanism, Multi-Head Attention
    uses several independent "heads" (often 4-8). Each head has its own set of Query,
    Key, and Value vectors for each word.'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Diverse Attention:** Each head computes attention scores differently, focusing
    on various aspects of word relationships:'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: One head might attend to grammatical roles (e.g., "fox" and "jumps").
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Another might focus on word order (e.g., "the" and "quick").
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Another might capture synonyms or related concepts (e.g., "quick" and "fast").
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Combining Perspectives:** After each head generates its own weighted values,
    their outputs are concatenated. This combines the diverse insights from different
    attention mechanisms.'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Final Representation:** This combined representation holds a richer understanding
    of the sentence, incorporating various relationships between words, not just a
    single focus.'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '* * *'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Positional Encoding:** Since transformers don''t process word order directly,
    this layer injects information about each word''s position in the sentence. It''s
    like giving the analysts a map so they know the order in which to consider the
    words.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '* * *'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Sure, let''s delve into positional encoding using an example sentence:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Sentence:** "The quick brown fox jumps over the lazy dog."'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Here''s how positional encoding works step-by-step:**'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Word Embeddings:**'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Each word ("The", "quick", etc.) is converted into a numerical representation
    called a word embedding, like a unique identifier in a vast vocabulary map.
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Imagine these embeddings as vectors:'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '"The": [0.2, 0.5, -0.1, ...]'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '"quick": [0.8, -0.3, 0.4, ...]'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '"brown": [..., ...]'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '...'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Positional Information:**'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Each word's embedding is combined with additional values based on its position
    in the sentence.
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'These values are calculated using sine and cosine functions at different frequencies:'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Lower frequencies capture long-range dependencies (e.g., "quick" and "fox" are
    related).
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Higher frequencies encode short-range relationships (e.g., "jumps" and "over"
    are close).
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Think of these additional values as "position vectors":'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '"The": [position 1 vector]'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '"quick": [position 2 vector]'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '"brown": [position 3 vector]'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '...'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Combining Embeddings and Positions:**'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Understanding Order:**'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Even if the sentence order changes (e.g., "Dog lazy jumps..."), the position
    vectors ensure relative positions are maintained.
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: The model can still learn that "jumps" is more related to "over" than, say,
    "The".
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '* * *'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Feed Forward Network(FFN):** This adds a layer of non-linearity, enabling
    the model to learn more complex relationships that might not be easily captured
    by attention mechanisms alone.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '* * *'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: You've already delved into the sentence through previous layers. You understand
    individual words, their relationships, and their positions. Now, the FFN arrives
    like a detective magnifying glass, ready to uncover intricate details not immediately
    visible.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**The FFN does this through three key steps:**'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Non-linear Transformation:** Instead of straightforward calculations, the
    FFN uses non-linear functions like ReLU to add complexity. Think of it as applying
    a special filter to the existing information, revealing hidden patterns and connections
    that simple arithmetic might miss. This allows the FFN to capture more nuanced
    relationships between words.'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Multi-layered Analysis:** The FFN isn''t just one step; it''s typically a
    chain of two or more fully connected layers. Each layer builds upon the previous
    one, transforming the information step-by-step. Imagine you''re examining the
    sentence under increasing magnification, uncovering finer details with each layer.'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Dimensionality Shift:** The FFN expands the information''s size (e.g., from
    512 dimensions to 2048) in the first layer. This allows it to analyze a wider
    range of features and capture more complex patterns. Think of it as spreading
    out the information on a larger canvas for deeper examination. Then, it contracts
    it back to the original size (e.g., 512 again) in the final layer to ensure compatibility
    with subsequent layers.'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Applying this to our sentence:**'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Imagine the FFN helps identify that "quick" and "brown" not only describe the
    "fox" but also subtly connect to its perceived speed through their combined meaning.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Or, it might delve deeper into the relationship between "jumps" and "over,"
    understanding the action and spatial context beyond just their individual definitions.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '* * *'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Repeat, Refine, Repeat:** These layers (self-attention, multi-head attention,
    etc.) are stacked and repeated multiple times. With each iteration, the encoder
    refines its understanding, building a comprehensive representation of the input
    text.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'image source: pillow lab blog'
  prefs: []
  type: TYPE_NORMAL
- en: '**Decoder: Weaving the Output Tapestry**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, the decoder takes the baton. But unlike the encoder, it has an additional
    challenge: generating the output word by word without peeking at the future. To
    achieve this, it utilizes:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Masked Self-Attention:** Similar to the encoder''s self-attention, but with
    a twist. The decoder only attends to previously generated words, ensuring it doesn''t
    cheat and use future information. It''s like writing a story one sentence at a
    time, without knowing how it ends.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Encoder-Decoder Attention:** This mechanism allows the decoder to consult
    the encoded input, like referring back to a reference document while writing.
    It ensures the generated output stays coherent and aligned with the original text.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Multi-Head Attention and Feed Forward Network:** Just like the encoder, these
    layers help the decoder refine its understanding of the context and relationships
    within the text.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Output Layer:** Finally, the decoder translates its internal representation
    into the actual output word, one by one. It''s like the final assembly line, putting
    the pieces together to form the desired outcome.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Beyond the Basics:**'
  prefs: []
  type: TYPE_NORMAL
- en: Remember, this is just a glimpse into the fascinating world of transformers.
    The specific architecture can vary depending on the task and dataset, with different
    numbers of layers and configurations.
  prefs: []
  type: TYPE_NORMAL
- en: Additionally, each layer involves complex mathematical operations that go beyond
    the scope of this explanation.
  prefs: []
  type: TYPE_NORMAL
- en: But hopefully, this has equipped you with a fundamental understanding of how
    transformers work and why they have revolutionized the field of NLP.
  prefs: []
  type: TYPE_NORMAL
- en: So, the next time you encounter a seamless machine translation or marvel at
    the creativity of an AI-powered text generator, remember the intricate dance of
    the encoder and decoder within the transformer, weaving magic with the power of
    attention and parallel processing.
  prefs: []
  type: TYPE_NORMAL
- en: '*Paper: https://arxiv.org/abs/1706.03762*'
  prefs: []
  type: TYPE_NORMAL
- en: Thank you for reading The ZenMode. This post is public so feel free to share
    it.
  prefs: []
  type: TYPE_NORMAL
- en: '[Share](https://nintyzeros.substack.com/p/how-do-transformer-workdesign-a-multi?utm_source=substack&utm_medium=email&utm_content=share&action=share)'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
