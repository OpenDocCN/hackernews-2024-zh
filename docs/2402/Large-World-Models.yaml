- en: <!--yml
  id: totrans-split-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-split-1
  prefs: []
  type: TYPE_NORMAL
  zh: 'category: 未分类'
- en: 'date: 2024-05-27 14:51:46'
  id: totrans-split-2
  prefs: []
  type: TYPE_NORMAL
  zh: 'date: 2024-05-27 14:51:46'
- en: -->
  id: totrans-split-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: Large World Models
  id: totrans-split-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 大型世界模型
- en: 来源：[https://largeworldmodel.github.io/](https://largeworldmodel.github.io/)
  id: totrans-split-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://largeworldmodel.github.io/](https://largeworldmodel.github.io/)
- en: 'Current language models fall short in understanding aspects of the world not
    easily described in words, and struggle with complex, long-form tasks. Video sequences
    offer valuable temporal information absent in language and static images, making
    them attractive for joint modeling with language. Such models could develop a
    understanding of both human textual knowledge and the physical world, enabling
    broader AI capabilities for assisting humans. However, learning from millions
    of tokens of video and language sequences poses challenges due to memory constraints,
    computational complexity, and limited datasets. To address these challenges, we
    curate a large dataset of diverse videos and books, utilize the Blockwise RingAttention
    technique to scalably train on long sequences, and gradually increase context
    size from 4K to 1M tokens. This paper makes the following contributions: (a) Largest
    context size neural network: We train one of the largest context size transformers
    on long video and language sequences, setting new benchmarks in difficult retrieval
    tasks and long video understanding. (b) Solutions for overcoming vision-language
    training challenges, including using masked sequence packing for mixing different
    sequence lengths, loss weighting to balance language and vision, and model-generated
    QA dataset for long sequence chat. (c) A highly-optimized implementation with
    RingAttention, blockwise transformers, masked sequence packing, and other key
    features for training on millions-length multimodal sequences. (d) Fully open-sourced
    a family of 7B parameter models capable of processing long text documents (LWM-Text,
    LWM-Text-Chat) and videos (LWM, LWM-Chat) of over 1M tokens. This work paves the
    way for training on massive datasets of long video and language to develop understanding
    of both human knowledge and the multimodal world, and broader capabilities.'
  id: totrans-split-6
  prefs: []
  type: TYPE_NORMAL
  zh: 当前的语言模型在理解那些难以用语言描述的世界方面存在不足，并且在处理复杂的长篇任务时也面临困难。视频序列提供了语言和静态图像所没有的宝贵时间信息，因此与语言联合建模变得非常吸引人。这样的模型可以发展对人类文本知识和物理世界的理解，从而扩展AI在帮助人类方面的能力。然而，从数百万标记的视频和语言序列中学习面临记忆限制、计算复杂性和有限数据集等挑战。为了应对这些挑战，我们策划了一个包含各种视频和书籍的大型数据集，利用分块环注意力技术来可伸缩地训练长序列，并逐步增加上下文大小从4K到1M标记。本文的主要贡献如下：(a)
    最大上下文大小的神经网络：我们训练了一个在长视频和语言序列上拥有最大上下文大小的Transformer模型，在困难的检索任务和长视频理解方面设立了新的基准。(b)
    解决视觉语言训练挑战的方案，包括使用掩码序列包装以混合不同长度的序列、损失加权以平衡语言和视觉、以及使用模型生成的QA数据集进行长序列对话。(c) 通过RingAttention、分块Transformer、掩码序列包装和其他关键特性，在百万级多模态序列上进行训练的高度优化实现。(d)
    完全开源的一系列7B参数模型，能够处理超过1M标记的长文档（LWM-Text、LWM-Text-Chat）和视频（LWM、LWM-Chat）。这项工作为训练大规模视频和语言数据集，以发展对人类知识和多模态世界的理解以及更广泛能力铺平了道路。
