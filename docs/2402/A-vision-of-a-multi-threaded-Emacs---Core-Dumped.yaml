- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-05-27 14:58:27'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
- en: A vision of a multi-threaded Emacs • Core Dumped
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://coredumped.dev/2022/05/19/a-vision-of-a-multi-threaded-emacs/](https://coredumped.dev/2022/05/19/a-vision-of-a-multi-threaded-emacs/)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Starting in Emacs 26 some very ambitious changes were added. Basic thread support
    was enabled, laying the groundwork for a future concurrent emacs. The [docs](https://www.gnu.org/software/emacs/manual/html_node/elisp/Threads.html)
    layout this possibility:'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
- en: Emacs Lisp provides a limited form of concurrency, called threads. All the threads
    in a given instance of Emacs share the same memory. Concurrency in Emacs Lisp
    is “mostly cooperative”, meaning that Emacs will only switch execution between
    threads at well-defined times. However, the Emacs thread support has been designed
    in a way to later allow more fine-grained concurrency
  id: totrans-7
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: What would a future with fine-grained concurrency look like? Could we have an
    Emacs that uses more then 1 thread? This post sketches out some rough ideas of
    what that could look like from an elisp perspective. I am going to take the easy
    way out and completely ignore *how* to actually implement this, just speculating
    on the big what-ifs.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
- en: The `thread` feature is specifically trying to enable *concurrency*, which is
    the ability to interweave lines of execution to make progress on more then one
    program at a time. This is the model used by async/await and coroutines. Concurrency
    is useful when your application is IO bound. The library enables you to switch
    between concurrent programs at designated points, which is why it is call *cooperative*
    concurrency. *Parallelism* on the other hand is the ability to actually run multiple
    programs at the same time. This is useful when your application is CPU bound.
    See [this post](https://oxylabs.io/blog/concurrency-vs-parallelism) for a more
    detailed explanation.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
- en: There are two main scenarios where concurrency/parallelism are useful.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
- en: When I want some elisp task done in the background without blocking my main
    user thread, I can use concurrency. This includes things like handling filter
    output, indexing buffers, watching for changes, etc. The background task will
    “steal” idle time from my main thread to make incremental progress on its job.
  id: totrans-11
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: When I want to get the results of some task faster, I need parallelism. This
    includes things like updating or searching buffers, applying font lock, or loading
    code. In order to do these task faster I need multiple threads running at the
    same time.
  id: totrans-12
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Note that parallelism can service both use case 1 and 2, but concurrency can
    only deal with use case 1\. In some sense, parallelism is a superset of concurrency.
    All parallel code is concurrent, but concurrent code is not necessarily parallel.
    For this reason, I am much more interested in a parallel Emacs then just a concurrent
    one.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
- en: 'My oversimplification of parallel languages breaks them into three categories:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
- en: Level 1 - memory unsafe and data races allowed
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
- en: Languages where incorrect code can lead to corruption of the program state and
    segfaults. This includes C++ and Swift.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
- en: Level 2 - memory safe and data races allowed
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
- en: Languages where parallelism is memory safe, but can still lead to data races.
    This includes Java and Go.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
- en: Level 3 - memory safe and no data races
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
- en: Languages that enable [fearless concurrency](https://doc.rust-lang.org/book/ch16-00-concurrency.html)
    by eliminating unguarded access to shared-memory. This includes Clojure, Rust,
    and TCL.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
- en: Generally the closer you are Level 1 the more footguns there are, but the more
    performance you can squeeze out. The higher you go the easier concurrent code
    is to write, but you have less performance and control. The exception to this
    is Rust, which is a safe Level 3 language with the performance of a Level 1.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
- en: 'So where do we want Emacs to land on this spectrum? The creator of [nogil](https://github.com/colesbury/nogil)
    python, a multi-threaded python implementation, [said this](https://docs.google.com/document/d/18CXhDb1ygxg-YXNBJNzfzZsDFosB5e6BfnXLlejd9l0/edit):'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 那么我们希望 Emacs 在这个范围内达到什么标准？[nogil](https://github.com/colesbury/nogil) Python
    的创建者，一个多线程 Python 实现，[说过这个](https://docs.google.com/document/d/18CXhDb1ygxg-YXNBJNzfzZsDFosB5e6BfnXLlejd9l0/edit)：
- en: The project aims for a concurrency model that matches the threads + shared memory
    model implemented in common operating systems… The shared-memory model is notoriously
    difficult to program correctly, but provides a good base to build better abstractions
    because it closely matches the primitives provided by the operating system (e.g.
    threads, mutexes).
  id: totrans-23
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 该项目旨在实现与常见操作系统中实现的线程+共享内存模型相匹配的并发模型... 共享内存模型以正确编程而闻名，但提供了一个很好的基础，以构建更好的抽象，因为它与操作系统提供的原语（例如线程、互斥锁）密切匹配。
- en: The argument is that Level 2 is the right balance, because you avoid crazy bugs
    you get with unsafe languages but still have more flexibility then level 3\. You
    should just give programmers the tools they need to build safe abstractions.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 论点是级别 2 是正确的平衡，因为你避免了在不安全语言中遇到的疯狂 bug，但仍然比级别 3 有更多的灵活性。你只需给程序员提供构建安全抽象所需的工具。
- en: I, however, disagree with that take. As the author said, shared memory is “notoriously
    difficult” to do correctly. As Emacs pulls in hundreds of packages, the potential
    for data races grows exponentially. Even Emacs’ current threads library [suffers
    from data races](https://nullprogram.com/blog/2018/05/31/) which is one of the
    reasons I believe it has not seen much adoption. We need to make concurrency as
    pain free as possible if it is going to be usable. Therefore I am in the “Emacs
    parallelism should be level 3” boat.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，我不同意这种看法。正如作者所说，共享内存“很难”正确执行。随着 Emacs 引入数百个包，数据竞争的潜力呈指数级增长。即使是 Emacs 目前的线程库[也存在数据竞争问题](https://nullprogram.com/blog/2018/05/31/)，这也是我认为它没有得到广泛采用的原因之一。如果并发性要可用，我们需要尽量减少痛苦。因此，我支持“Emacs
    的并行性应该是级别 3”。
- en: 'So what are the standards we want for a multi-threaded Emacs implementation?
    Here is my short list:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，我们希望多线程 Emacs 实现达到什么标准？这是我的简短清单：
- en: No [function coloring](https://journal.stuffwithstuff.com/2015/02/01/what-color-is-your-function/)
    or special requirements on functions. One of Emacs’ big advantages is the huge
    bulk of existing lisp code. We want to reuse as much as we can. This is generally
    only a problem with concurrent schemes like async/wait.
  id: totrans-27
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 没有[函数着色](https://journal.stuffwithstuff.com/2015/02/01/what-color-is-your-function/)或对函数的特殊要求。Emacs
    的一个重要优势是庞大的现有 lisp 代码库。我们希望尽可能多地重用。这通常只在像 async/wait 这样的并发方案中才会出现问题。
- en: No data races. This will make programs significantly easier to write correctly,
    but is also going to make our code more limited (** *foreshadowing ***).
  id: totrans-28
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 没有数据竞争。这将使程序编写起来更加简单，但也将使我们的代码更加受限制（** *预示着 ***）。
- en: We want the behavior of multi-threaded code to be as close to single-threaded
    code as possible. More on this later.
  id: totrans-29
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们希望多线程代码的行为尽可能接近单线程代码。稍后详细介绍。
- en: Most people have never heard of TCL (or if they have they’ve never used it)
    but I find it has a [very simple approach](https://www.activestate.com/blog/threads-done-right-tcl/)
    to multi-threading. Essentially the interpreter can work in its own thread, and
    carries with it all of its state. This is the multi-interpreter approach; Every
    thread starts in a clean environment with its own interpreter. “Messages” can
    be passed to any thread and they can return a result. In elisp it could look something
    like this.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数人从未听说过 TCL（或者如果他们听说过，他们从未使用过），但我发现它对多线程有[非常简单的方法](https://www.activestate.com/blog/threads-done-right-tcl/)。本质上，解释器可以在自己的线程中工作，并携带所有状态。这就是多解释器方法；每个线程都在一个干净的环境中以自己的解释器开始。可以将“消息”传递给任何线程，它们可以返回一个结果。在
    elisp 中，它可能看起来像这样。
- en: '[PRE0]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: This is really simple and really effective, but it has some limitations. First
    is that since each thread starts a new interpreter, you need to load a bunch of
    lisp code to do almost anything useful. This means that thread overhead is significant
    and is not a good fit for small tasks. Second, since you are copying objects between
    threads when you pass them with `thread-send` you can’t modifying existing buffers^(.
    And buffers are probably the most important use case here. Let’s see if we can
    fix that.)
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 这真的很简单，也很有效，但它有一些限制。首先，由于每个线程都启动一个新的解释器，所以几乎无法做任何有用的事情，你需要加载一堆 lisp 代码。这意味着线程开销很大，不适合执行小任务。其次，由于在使用
    `thread-send` 传递对象时在线程之间复制对象，因此无法修改现有缓冲区^(。而缓冲区可能是这里最重要的用例。让我们看看我们是否可以解决这个问题。)
- en: What if instead of needing to copy the buffers between threads, they could be
    shared? I know, I know, shared-memory is a footgun, but we are going to use mutexes!
    So it’s more like sharing in pre-school where everyone gets a turn. Each buffer
    is guarded by a mutex, and only one thread can have access to a buffer at a given
    time. The way you acquire the mutex is by switching to that buffer (using `set-buffer`
    , `switch-to-buffer`, or `with-current-buffer`). Just as you can only have a single
    “current buffer”, you can only have the mutex for a single buffer at a time. A
    thread can switch to a buffer, do some operations, then release it. This is all
    well and good, but we have a major issue; shared-state.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 如果不需要在线程之间复制缓冲区，而是可以共享它们呢？我知道，我知道，共享内存是一把双刃剑，但我们将使用互斥锁！因此，这更像是在学前班分享，每个人都有机会。每个缓冲区都由一个互斥锁保护，每次只能有一个线程访问一个缓冲区。获取互斥锁的方法是切换到该缓冲区（使用
    `set-buffer` 、`switch-to-buffer` 或 `with-current-buffer`）。就像你只能有一个“当前缓冲区”一样，你一次只能拥有一个缓冲区的互斥锁。一个线程可以切换到一个缓冲区，做一些操作，然后释放它。这都很好，但我们有一个主要问题；共享状态。
- en: 'You see, for a buffer to really be useful you need have the buffer local variables.
    Without those you can’t even know the `major-mode`! But buffer local variable
    can share data with global variables, and each thread has its own set of globals.
    Consider the code below:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 你看，要使缓冲区真正有用，你需要有缓冲区本地变量。没有这些，你甚至都不知道`major-mode`！但缓冲区本地变量可以与全局变量共享数据，每个线程都有自己的全局变量集。考虑下面的代码：
- en: '[PRE1]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Here both `local` and `global` share the same cons cells. If I mutate one it
    will mutate the other. This obviously won’t work if I am sharing buffer local
    variables between threads. What we need to sever the ties between these. Buffer
    local variables can’t share any data with non-buffer local variables. You could
    setup a write barrier that would make copies when a thread releases the mutex.
    But I am not going to get into the *how* (I get to ignore implementation details
    remember?).
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 这里`local`和`global`共享相同的 cons 单元。如果我改变其中一个，它将改变另一个。显然，如果我在线程之间共享缓冲区本地变量，这是行不通的。我们需要切断它们之间的联系。缓冲区本地变量不能与非缓冲区本地变量共享任何数据。你可以设置一个写屏障，当线程释放互斥锁时进行复制。但我不会深入讨论
    *怎么做*（记住我可以忽略实现细节）。
- en: This would technically make multi-threaded emacs have different behavior then
    the old single threaded one (I told you we would talk about that later). But I
    would argue that if you are relying on sharing data between globals and buffer-locals
    for the correct operation of your code, it is in serious need of a refactor. I
    imagine that in real life this situation is very rare.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 从技术上讲，这会使多线程的 Emacs 与旧的单线程版本有不同的行为（我告诉过你我们会晚些时候讨论这个）。但我认为，如果你依赖于在全局变量和缓冲区本地变量之间共享数据来正确执行代码，那么它就需要进行严重的重构。我想在现实生活中，这种情况非常罕见。
- en: As it currently stands you basically need to load your entire init file in each
    new thread. Why can’t you just load the bare minimum elisp? Consider what would
    happen when a buffer local hook calls some function from another package? We need
    to make sure the code is loaded, and the only way we can do that is by loading
    the init file which contains all package initialization. There is also the problem
    that the state can get out sync. Each thread would start in a clean state, but
    this is not going to match the state of your main thread. This impedance mismatch
    is a clear source of bugs.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 目前情况是，你基本上需要在每个新线程中加载整个初始化文件。为什么不能只加载最少的 elisp 呢？想想看，当一个缓冲区本地钩子调用另一个包的某个函数时会发生什么？我们需要确保代码已加载，我们唯一能做到的就是加载包含所有包初始化的初始化文件。还有一个问题是状态可能会不同步。每个线程都会以干净的状态启动，但这与你的主线程的状态不匹配。这种阻抗不匹配是错误的明显来源。
- en: What if we did something crazy? What if we said that all functions are shared
    between threads? If you think about it, this is almost a perfect match. Function
    rarely change, and when they do, you can just replace the whole function atomically.
    However to do this you would need to address the *functional literal mutation
    problem* I talk about [here](https://coredumped.dev/2021/04/07/when-pure-function-lie/).
    Otherwise you could have multiple threads mutating the same function constants
    and potentially corrupting the VM state. But again, *implementation details*.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们做点疯狂的事呢？我们说所有的函数都在线程之间共享？如果你仔细想想，这几乎是完美的匹配。函数很少改变，而且当它们改变时，你可以原子地替换整个函数。但要做到这一点，你需要解决我在[这里](https://coredumped.dev/2021/04/07/when-pure-function-lie/)谈到的
    *函数文字变异问题*。否则，你可能会有多个线程改变相同的函数常量，并潜在地破坏 VM 状态。但同样，*实现细节*。
- en: Okay so that takes care of functions, but what about variables? Sharing variables
    would lead to data races, which is exactly what we are trying avoid. What if instead
    of sharing, we copied variables on demand? Hear me out! The first time a “sub-thread”
    does a lookup of a global variable, its value is copied from the main thread.
    This sets the initial value in that thread. From then on that copy of the value
    is “owned” by the thread and it can be mutated or read whenever. This would also
    help with the out-of-sync with the main thread problem we mentioned earlier. The
    state in your sub-thread would be much closer to the main global state. There
    could be a message queue internal to the VM that sends these requests back and
    forth. At certain points, the main thread would check the queue and send the values.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 好吧，这解决了函数的问题，但变量呢？共享变量会导致数据竞争，这正是我们试图避免的。如果我们不共享，而是按需复制变量呢？听我说！“子线程”第一次查找全局变量时，其值将从主线程复制。这设置了该线程中的初始值。从那时起，该值的副本就由该线程“拥有”，可以随时进行变异或读取。这也将有助于解决我们之前提到的与主线程不同步的问题。子线程中的状态将更接近主全局状态。虚拟机内部可能有一个消息队列，用于在这些请求之间发送值。在某些点上，主线程会检查队列并发送值。
- en: But this also means that sub-threads could spend a decent amount of time waiting
    for the main thread to be ready to send the values it needs, at least at the start.
    There could be a couple ways to alleviate this. The one that comes to mind is
    that you could cache the list of variables used at the call site of the thread
    creation. Then next time the thread is called you eagerly copy all the variables
    it had used before. This would make repeated initialization of the same thread
    much faster, but could also mean you get different variables the first time vs
    following times (if the calling function changed something immediately afterwards
    for example). As with everything; trade-offs.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 但这也意味着子线程可能会花费相当多的时间等待主线程准备好发送它所需的值，至少在开始时是这样。有几种缓解这种情况的方法。我想到的一个方法是，你可以缓存线程创建调用点使用的变量列表。然后，下次调用线程时，你可以急切地复制它之前使用过的所有变量。这将使相同线程的重复初始化速度更快，但也可能意
- en: 'There is one big reason for all this song and dance around buffers locals,
    functions, and variables: reusing existing elisp code. Languages that were created
    with concurrency in mind have designed their languages around these considerations.
    But Emacs is a giant ball of mutable state. Taming that to do something useful
    in a multi-threaded world while still reusing existing code is tricky.'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 在缓冲区局部、函数和变量周围进行所有这些繁琐的操作的一个重要原因是：重用现有的 elisp 代码。考虑到这些因素，为并发设计他们的语言。但 Emacs
    是一个巨大的可变状态球。在一个多线程的世界中驯服它以做一些有用的事情，同时仍然重用现有的代码是棘手的。
- en: So our threads are pretty cheap to create (as threads go), but not to keep around.
    Each elisp thread maps to an OS thread, and even when thread is idle it is still
    taking OS resources. Go and Clojure have solved this by creating so called green
    threads that are managed by the runtime. The green threads will be executed on
    OS threads, but they can be created, destroyed and managed by the VM. In Go you
    can create thousands of green threads and it will not impact the system. Don’t
    try at home with OS threads.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们的线程创建起来相当便宜（就线程而言），但维持下来就不是了。每个 elisp 线程都映射到一个 OS 线程，即使线程空闲时也仍在占用 OS 资源。Go
    和 Clojure 通过创建所谓的绿色线程来解决了这个问题，这些线程由运行时管理。绿色线程将在 OS 线程上执行，但它们可以由 VM 创建、销毁和管理。在
    Go 中，你可以创建成千上万的绿色线程，而不会影响系统。请不要尝试在家里使用 OS 线程。
- en: 'Now that we have green threads, the observant among you will notice that we
    have basically reinvented goroutines. All we need to do is add channels and we
    can have something close to [core.async](https://clojuredocs.org/clojure.core.async).
    I imagine usage looking something like this:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了绿色线程，你们中敏锐的人会注意到我们基本上重新发明了 goroutine。我们所需要做的就是添加通道，我们就可以拥有接近[core.async](https://clojuredocs.org/clojure.core.async)的东西了。我想象中的使用方式是这样的：
- en: '[PRE2]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'We have created some simple light weight green threads for Emacs. Well, we
    didn’t actually create anything, but we sure did talk about it a lot! The thing
    I like about this approach is that threads are easy to create and use to accomplish
    work in parallel. There are no data races and footguns have been minimized. But
    I can still see a few open problems that are not solved:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 我们为 Emacs 创建了一些简单的轻量级绿色线程。嗯，其实我们并没有创建任何东西，但我们确实谈论了很多！我喜欢这种方法的原因是线程很容易创建和使用，以并行方式完成工作。没有数据竞争，并且已经将不安全的情况最小化。但我仍然看到了一些未解决的问题：
- en: The most important buffer in Emacs is the one you are currently editing. But
    with the mutex scheme, you can’t use any other threads to work on that buffer!
    If you want to index or search or syntax-highlight the buffer, that still needs
    to use your main thread, meaning that the user is blocked. I don’t like it, but
    am not sure of a clean way to fix it.
  id: totrans-47
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Emacs 中最重要的缓冲区是你当前正在编辑的那个。但是在互斥方案中，你不能使用任何其他线程来处理该缓冲区！如果你想对缓冲区进行索引、搜索或语法高亮，仍然需要使用你的主线程，这意味着用户被阻塞了。我不喜欢这种情况，但我不确定是否有一种清晰的解决方法。
- en: What about when you need to iterate over all buffers (like with `ibuffer`)?
    Here you would need to acquire the mutex for each one in turn. If another thread
    is using a buffer the main thread will have to wait. Hopefully sub-threads would
    choose to do their work incrementally, giving time for the thread to yield the
    mutex.
  id: totrans-48
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 当你需要遍历所有缓冲区时（比如使用 `ibuffer` 时）会怎样？这里你需要依次获取每个缓冲区的互斥锁。如果另一个线程正在使用缓冲区，则主线程将不得不等待。希望子线程选择逐步执行他们的工作，从而给予主线程放弃互斥锁的时间。
- en: Since the sub-threads take their global variables from the main thread, you
    can’t load code in parallel. Only the main thread can load code that can be used
    by everyone.
  id: totrans-49
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 由于子线程从主线程获取全局变量，所以你不能并行加载代码。只有主线程才能加载可以被所有人使用的代码。
- en: I haven’t even mentioned a bunch of other multi-threading concerns like cancellation,
    atomics, garbage collection, message queue buffering, semaphores, signals, error
    reporting, debugging, concurrent data structures, C integration, deadlock, and
    livelock to name a few. Perhaps those are a topic for a future post.
  id: totrans-50
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我甚至还没有提到一堆其他多线程问题，比如取消、原子操作、垃圾回收、消息队列缓冲、信号、错误报告、调试、并发数据结构、C 集成、死锁和活锁等等。也许这些是未来帖子的主题。
- en: It has been fun to speculate about multi-threaded Emacs. But the real question
    is, would it be worth it? Emacs has gotten along just fine with a single thread;
    In fact many (most?) dynamic languages have. I would guess that threads would
    only be useful in about 10% of the programming tasks you would do in elisp. But
    when threads can be used, they would be big boon.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 思考多线程 Emacs 是很有趣的。但真正的问题是，这样做值得吗？Emacs 只使用一个线程就过得很好；事实上，许多（大部分？）动态语言也是如此。我猜想，在
    elisp 中，只有大约 10% 的编程任务会用到线程。但是当线程可以使用时，它们将是巨大的优势。
- en: As with everything in engineering, concurrency comes with trade-offs. Implementing
    a scheme like I described would be a monumental task. It would probably involve
    a complete rewrite of the core runtime^(. Also anytime you make an interpreter
    multi-threaded, you make single threaded code slower^(. There is no avoiding that.
    If 90% of code is still single-threaded, is that worth the cost?))
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 与工程中的一切一样，并发带来了权衡。实现我描述的方案将是一项巨大的任务。这可能涉及到核心运行时的完全重写。而且，每当你将解释器变成多线程时，你都会使单线程代码变慢。这是无法避免的。如果
    90% 的代码仍然是单线程的，这是否值得代价？
- en: Anyways, I would love to get some feedback on the ideas presented. Are there
    obvious holes that I missed? Would this scheme be useful? Do you know way that
    these could be implemented (or would not be possible to implement)? How does this
    compare to other dynamic languages? Do you prefer the more “thread-like” or “green-thread-like”
    approach? Is there a way to address some of the problems presented above?
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 无论如何，我很乐意听取对所提出的想法的反馈。我是否忽略了一些明显的漏洞？这个方案是否有用？你知道这些问题如何实现（或者无法实现）吗？这与其他动态语言相比如何？你更喜欢更“线程式”的还是更“绿色线程式”的方法？有没有办法解决上述一些问题？
- en: Join the [discussion](https://discu.eu/?q=https%3A%2F%2Fcoredumped.dev%2F2022%2F05%2F19%2Fa-vision-of-a-multi-threaded-emacs%2F&submit_title=A%20vision%20of%20a%20multi-threaded%20Emacs%20%E2%80%A2%20Core%20Dumped)
    or send me an [email](mailto:troy.hinckley@dabrev.com)
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 加入[讨论](https://discu.eu/?q=https%3A%2F%2Fcoredumped.dev%2F2022%2F05%2F19%2Fa-vision-of-a-multi-threaded-emacs%2F&submit_title=A%20vision%20of%20a%20multi-threaded%20Emacs%20%E2%80%A2%20Core%20Dumped)或发送电子邮件至[troy.hinckley@dabrev.com]。
