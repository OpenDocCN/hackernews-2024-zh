- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-05-27 14:58:00'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: The AI bullshit singularity | Successful Software
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://successfulsoftware.net/2024/02/18/the-ai-bullshit-singularity/](https://successfulsoftware.net/2024/02/18/the-ai-bullshit-singularity/)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: I’m sure we are all familiar with the idea of a technological singularity. Humans
    create an AI that is smart enough to create an even smarter successor. That successor
    then creates an even smarter successor. The process accelerates through a positive
    feedback loop, until we reach a technological singularity, where puny human intelligence
    is quickly left far behind.
  prefs: []
  type: TYPE_NORMAL
- en: Some people seem to think that Large Language Models could be the start of this
    process. We train the LLMs on vast corpuses of human knowledge. The LLMs then
    help humans create new knowledge, which is then used to train the next generation
    of LLMs. Singularity, here we come!
  prefs: []
  type: TYPE_NORMAL
- en: But I don’t think so. Human nature being what it is, LLMs are inevitably going
    to be used to churn out vast amount of low quality ‘content’ for SEO and other
    commercial purposes. LLM nature being what it is, a lot of this content is going
    to be hallucinated. In otherwords, bullshit. Given that LLMs can generate content
    vastly faster than humans can, we could quickly end up with an Internet that is
    mostly bullshit. Which will then be used to train the next generation of LLM.
    We will eventually reach a bullshit singularlity, where it is almost impossible
    to work out whether anything on the Internet is true. Enshittification at scale.
    Well done us.
  prefs: []
  type: TYPE_NORMAL
