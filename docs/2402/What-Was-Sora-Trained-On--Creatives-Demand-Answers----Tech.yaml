- en: <!--yml
  id: totrans-split-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-split-1
  prefs: []
  type: TYPE_NORMAL
  zh: 'category: 未分类'
- en: 'date: 2024-05-27 15:04:12'
  id: totrans-split-2
  prefs: []
  type: TYPE_NORMAL
  zh: 'date: 2024-05-27 15:04:12'
- en: -->
  id: totrans-split-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: What Was Sora Trained On? Creatives Demand Answers. - Tech
  id: totrans-split-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Sora受到训练的是什么？创意人士要求答案。- Tech
- en: 来源：[https://in.mashable.com/tech/69813/what-was-sora-trained-on-creatives-demand-answers](https://in.mashable.com/tech/69813/what-was-sora-trained-on-creatives-demand-answers)
  id: totrans-split-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://in.mashable.com/tech/69813/what-was-sora-trained-on-creatives-demand-answers](https://in.mashable.com/tech/69813/what-was-sora-trained-on-creatives-demand-answers)
- en: On Thursday, OpenAI once again shook up the AI world with a video generation
    model called Sora.
  id: totrans-split-6
  prefs: []
  type: TYPE_NORMAL
  zh: 上周四，OpenAI再次震动了AI界，推出了名为Sora的视频生成模型。
- en: The demos showed photorealistic videos with crisp detail and complexity, based
    off of simple text prompts. A [video](https://twitter.com/heyBarsee/status/1758377540870160442)
    based on the prompt "Reflections in the window of a train traveling through the
    Tokyo suburbs" looked like it was filmed on a phone, shaky camera work and reflections
    of train passengers included. No weird distorted hands in sight.
  id: totrans-split-7
  prefs: []
  type: TYPE_NORMAL
  zh: 展示的演示视频具有清晰的细节和复杂性，基于简单的文本提示。一部基于“透过东京郊区火车窗户的反射”这一提示的[视频](https://twitter.com/heyBarsee/status/1758377540870160442)看起来像是用手机拍摄的，摇晃的摄影作品中还包括了乘客的反射。一点奇怪扭曲的手部画面都没有。
- en: A video from the prompt, "A movie trailer featuring the adventures of the 30
    year old space man wearing a red wool knitted motorcycle helmet, blue sky, salt
    desert, cinematic style, shot on 35mm film, vivid colors" looked like a Christopher
    Nolan-Wes Anderson hybrid.
  id: totrans-split-8
  prefs: []
  type: TYPE_NORMAL
  zh: 基于提示“一个30岁的太空人穿着红色羊毛编织的摩托车头盔，蓝天，盐沙漠，电影风格，35mm胶片拍摄，生动的色彩”的视频看起来像是克里斯托弗·诺兰和韦斯·安德森的混合体。
- en: Another of golden retriever puppies playing in the snow rendered soft fur and
    fluffy snow so realistic you could reach out and touch it.
  id: totrans-split-9
  prefs: []
  type: TYPE_NORMAL
  zh: 另外，一群金毛幼犬在雪地里玩耍，柔软的毛发和蓬松的雪看起来如此逼真，你几乎能够伸手触摸它。
- en: The 7 trillion dollar question is, how did OpenAI achieve this? We don't actually
    know because OpenAI has barely shared anything about its training data. But in
    order to create a model this advanced, Sora needed lots of video data, so we can
    assume it was trained on video data scraped from all corners of the internet.
    And some are speculating that training data included copyrighted works. OpenAI
    did not immediately respond to request for comment on Sora's training data.
  id: totrans-split-10
  prefs: []
  type: TYPE_NORMAL
  zh: 7万亿美元的问题是，OpenAI是如何做到这一点的？我们实际上并不知道，因为OpenAI几乎没有分享关于其训练数据的任何信息。但是，为了创建这样一个先进的模型，Sora需要大量的视频数据，因此我们可以假设它是通过从互联网的各个角落抓取的视频数据进行训练的。有些人推测训练数据包括受版权保护的作品。OpenAI并未立即回复关于Sora训练数据的评论请求。
- en: 'In OpenAI''s [technical paper](https://openai.com/research/video-generation-models-as-world-simulators)
    it largely focuses on the method for achieving these results: Sora is a diffusion
    model that turns visual data into "patches" or pieces of data that the model can
    understand. But there''s scant mention of where the visual data came from.'
  id: totrans-split-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在OpenAI的[技术论文](https://openai.com/research/video-generation-models-as-world-simulators)中，它主要关注实现这些结果的方法：Sora是一个将视觉数据转换为“补丁”或模型可以理解的数据片段的扩散模型。但是关于视觉数据的来源几乎没有提及。
- en: OpenAI says it “take[s] inspiration from large language models which acquire
    generalist capabilities by training on internet-scale data.” The incredibly vague
    “taking inspiration” part is the only evasive reference to the source of Sora’s
    training data. Further down in the paper, OpenAI says, “training text-to-video
    generation systems requires a large amount of videos with corresponding text captions.”
    The only source of a massive amount of visual data can be found on the internet,
    another hint at where Sora comes from.
  id: totrans-split-12
  prefs: []
  type: TYPE_NORMAL
  zh: OpenAI表示，“我们受到大型语言模型的启发，这些模型通过训练在互联网规模的数据上获得了通用能力。”这个极为模糊的“受到启发”的部分是关于Sora训练数据来源的唯一闪烁其词的提及。在文章的后半部分，OpenAI表示，“训练文本到视频生成系统需要大量具有相应文本标题的视频。”互联网上的大量视觉数据来源可以找到，这是Sora来自何处的又一个暗示。
- en: The legal and ethical issue of how training data is acquired for AI models has
    been around ever since OpenAI launched ChatGPT. Both [OpenAI](https://mashable.com/article/openai-chatgpt-class-action-lawsuit)
    and [Google](/tech/56360/google-slapped-with-a-lawsuit-for-secretly-stealing-data-to-train-bard)
    have been accused of “stealing” data to train their language models, in other
    words using data scraped from social media, online forums like Reddit and Quora,
    Wikipedia, databases of private books, and news sites.
  id: totrans-split-13
  prefs: []
  type: TYPE_NORMAL
  zh: 如何获取 AI 模型的训练数据的法律和伦理问题自 OpenAI 推出 ChatGPT 以来就一直存在。OpenAI 和 Google 都被指控“偷取”数据来训练他们的语言模型，换句话说，使用从社交媒体、Reddit
    和 Quora 等在线论坛、维基百科、私人书籍数据库和新闻网站上抓取的数据。
- en: Until now the rationale for scraping the entirety of the internet for training
    data is that it's publicly-available. But publicly-available [doesn't always translate](https://x.com/ednewtonrex/status/1758259614414770626?s=20)
    to public domain. Case in point, the *New York Times* is [suing](/tech/66470/the-new-york-times-sues-openai-and-microsoft-for-copyright-infringement)
    OpenAI and Microsoft for copyright infringement, alleging OpenAI's models used
    the *Times*' works word for word or incorrectly cited the stories.
  id: totrans-split-14
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，从整个互联网上抓取训练数据的理由是它是公开可用的。但是公开可用并不总是等同于公共领域。以*纽约时报*为例，他们正在起诉 OpenAI 和微软侵犯版权，声称
    OpenAI 的模型直接使用了*时报*的文字，未经授权引用了他们的报道。
- en: Now it looks like OpenAI is doing the same thing, but with video. If this is
    the case, you can expect heavy-hitters in the entertainment industry to have something
    to say about it.
  id: totrans-split-15
  prefs: []
  type: TYPE_NORMAL
  zh: 现在看起来 OpenAI 正在做同样的事情，但是通过视频。如果是这样的话，你可以期待娱乐行业的重量级人物会对此有所表示。
- en: 'But the problem remains: We still don''t know the source of Sora''s training
    data. "The company (despite its name) has been characteristically close-lipped
    about what they have trained the models on," [wrote](https://garymarcus.substack.com/p/soras-surreal-physics)
    Gary Marcus, an AI expert who testified at the U.S. Senate AI Oversight Committee
    hearing. " Many people have [speculated] that there’s probably a lot of stuff
    in there that is generated from game engines like Unreal. I would not at all be
    surprised if there also had been lots of training on YouTube visited, and various
    copyrighted materials," said Marcus, before adding, "Artists are presumably getting
    really screwed here."'
  id: totrans-split-16
  prefs: []
  type: TYPE_NORMAL
  zh: 但问题依然存在：我们仍然不知道 Sora 的训练数据来源。“尽管公司（尽管其名称）一直对他们对模型进行了训练的内容保持缄口不谈，” 美国参议院人工智能监管委员会听证会上作证的
    AI 专家 Gary Marcus 写道。“许多人猜测其中可能有很多来自像虚幻引擎这样的游戏引擎生成的东西。如果在 YouTube 上也有大量的训练材料，以及各种受版权保护的材料，我一点也不会感到惊讶，”
    Marcus 补充道。“这里的艺术家们可能正在遭受重创。”
- en: Despite OpenAI's refusal to divulge its secrets, artists and creatives are assuming
    the worst. Justine Bateman, a filmmaker and SAG-AFTRA generative AI advisor didn't
    mince words. "Every nanosecond of this [#AI](https://twitter.com/hashtag/AI?src=hashtag_click)
    garbage is trained on stolen work by real artists," [posted](https://x.com/JustineBateman/status/1758220737675145553?s=20)
    Bateman on X. "Repulsive," she added.
  id: totrans-split-17
  prefs: []
  type: TYPE_NORMAL
  zh: '尽管 OpenAI 拒绝透露其秘密，艺术家和创作者们却想象最坏的情况。电影制片人和 SAG-AFTRA 生成 AI 顾问 Justine Bateman
    毫不客气地说道：“这些 #AI 垃圾的每一纳秒都是由真正艺术家的作品训练而成的。” Bateman 在 X 上发布说。她还补充道：“令人厌恶。”'
- en: Others in creative industries are concerned about how the rise of Sora and video
    generating models will affect their jobs. "I work in film vfx, practically everyone
    I know is doom and gloom, panicking about what to do now," [posted](https://twitter.com/jimmylanceworth/status/1758445030736285961)
    @jimmylanceworth.
  id: totrans-split-18
  prefs: []
  type: TYPE_NORMAL
  zh: 创意产业中的其他人担心 Sora 和视频生成模型的崛起将如何影响他们的工作。“我从事电影视觉特效工作，我认识的几乎所有人都对现状感到悲观，担心接下来该怎么办。”
    @jimmylanceworth 发布说。
- en: OpenAI didn't completely ignore the explosive impact Sora might have. But that's
    largely focused on potential harms involving deepfakes and misinformation. It
    is currently in red-teaming phase, which means it's being stress-tested for inappropriate
    and harmful content. Towards the end of its announcement, OpenAI said it will
    be "engaging policymakers, educators and artists around the world to understand
    their concerns and to identify positive use cases for this new technology."
  id: totrans-split-19
  prefs: []
  type: TYPE_NORMAL
  zh: OpenAI 并未完全忽视 Sora 可能带来的爆炸性影响。但主要集中在涉及深度伪造和虚假信息可能造成的潜在危害上。目前正处于红队测试阶段，这意味着正在对不当和有害内容进行压力测试。在公告的最后，OpenAI
    表示将“与全球的政策制定者、教育工作者和艺术家进行交流，了解他们的关切，并确定这项新技术的积极应用案例。”
- en: But that doesn't address the harms that may have already occurred by making
    Sora in the first place.
  id: totrans-split-20
  prefs: []
  type: TYPE_NORMAL
  zh: 但这并没有解决制造 Sora 可能已经造成的伤害。
